#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
post_clean_sections.py (global switches version)

åœ¨ batch_label_sections.py çš„è§£æç»“æœåŸºç¡€ä¸Šåšè§„åˆ™åå¤„ç†ï¼š

ç”±ä¸¤ä¸ªå…¨å±€å˜é‡å†³å®šæ˜¯å¦å¯ç”¨ç‰¹æ€§ï¼š

    STRIP_HEADER_FOOTER = True/False   åˆ é™¤é¡µçœ‰é¡µè„š
    DROP_CHECKLIST = True/False       åˆ é™¤ Checklist æ¨¡å—

è¾“å…¥æ ¼å¼ä¸ºï¼š
    paper_parse.jsonï¼ˆå†…å« content + section_labelsï¼‰

è¾“å‡ºæ ¼å¼ä¸ºï¼š
    paper_clean.json
"""

from pathlib import Path
import json
import re
from typing import List, Dict, Any
from collections import Counter
from tqdm import tqdm
from utils import load_json, save_json

# ===========================================================
# å…¨å±€å˜é‡ â€”â€” åœ¨è¿™é‡Œæ”¹å¼€å…³
# ===========================================================
STRIP_HEADER_FOOTER = True      # åˆ é™¤é¡µçœ‰ã€é¡µè„š
DROP_CHECKLIST = True           # åˆ é™¤ Checklist æ¨¡å—
INPUT_NAME = "paper_parse_add_section.json"
OUTPUT_NAME = "paper_final.json"
ROOT_DIR = "/mnt/parallel_ssd/home/zdhs0006/mlrbench/download/data/ICLR_30"
# ===========================================================


def find_jsons(root: Path, name: str) -> List[Path]:
    return sorted([p for p in root.rglob(name) if p.is_file()])


# ======= è§„åˆ™ï¼šé¡µçœ‰/é¡µè„šæ¸…æ´— ======= #

def _normalize_line(line: str) -> str:
    """
    ç”¨äºåˆ¤æ–­â€œæ˜¯ä¸æ˜¯åŒä¸€è¡Œâ€çš„å½’ä¸€åŒ–è§„åˆ™ï¼š
    - å»é¦–å°¾ç©ºæ ¼
    - å»æ‰å¼€å¤´çš„ Markdown æ ‡é¢˜ç¬¦å· (#, *, ä¹‹ç±»)ï¼Œä¿è¯ "# VersaPRM ..." å’Œ "VersaPRM ..." å½’åˆ°ä¸€èµ·
    - å¤šç©ºæ ¼åˆå¹¶
    - å»æ‰å‰åå¸¸è§ç¬¦å·ï¼Œè½¬å°å†™
    - å¤ªçŸ­çš„è¡Œï¼ˆæ¯”å¦‚å•ä¸ªé¡µç ï¼‰ç›´æ¥è§†ä¸ºæ— æ•ˆ
    """
    if not line:
        return ""
    s = line.strip()
    # å»æ‰ markdown æ ‡é¢˜ã€åˆ—è¡¨ç¬¦å·å‰ç¼€ï¼š#ã€##ã€* ç­‰
    s = re.sub(r"^[#*]+\s*", "", s)
    # å¤šä¸ªç©ºæ ¼åˆå¹¶
    s = re.sub(r"\s+", " ", s)
    # å»æ‰å‰åç‚¹å·/ç«–çº¿/ç ´æŠ˜å·ç­‰
    s = s.strip(" .Â·â€¢|-â€“â€”~")
    s = s.lower()
    # å‡€é•¿åº¦å¤ªçŸ­çš„ä¸ç®—ï¼ˆå¦‚ "3"ã€"p.5"ï¼‰
    if len(s.replace(" ", "")) < 5:
        return ""
    return s

def detect_header_footer_patterns(
    content: List[Dict[str, Any]],
    min_occurs: int = 3,
    min_ratio: float = 0.15,
) -> List[str]:
    """
    ä»æ‰€æœ‰ text block ä¸­ç»Ÿè®¡é¦–è¡Œ & æœ«è¡Œçš„é‡å¤æƒ…å†µï¼Œ
    è¿”å›å¯èƒ½å±äºé¡µçœ‰/é¡µè„šçš„â€œå½’ä¸€åŒ–è¡Œæ–‡æœ¬â€åˆ—è¡¨ã€‚
    """
    counter = Counter()
    text_count = 0

    for block in content:
        if block.get("type") != "text":
            continue
        text = block.get("text") or ""
        lines = text.splitlines()
        if not lines:
            continue
        text_count += 1

        first = _normalize_line(lines[0])
        last = _normalize_line(lines[-1])

        if first:
            counter[first] += 1
        if last and last != first:
            counter[last] += 1

    if text_count == 0:
        return []

    patterns = []
    for norm_line, cnt in counter.items():
        if cnt >= min_occurs and (cnt / text_count) >= min_ratio:
            patterns.append(norm_line)

    return patterns

def strip_header_footer(content: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    æ ¹æ® detect_header_footer_patterns æ‰¾åˆ°çš„æ¨¡å¼ï¼Œåˆ é™¤é¡µçœ‰/é¡µè„šè¡Œã€‚
    ğŸ”¹ æ–°è¦æ±‚ï¼š
        - æ¯ä¸ª pattern çš„ç¬¬ä¸€æ¬¡å‡ºç°ä¼šä¿ç•™ï¼ˆé€šå¸¸æ˜¯è®ºæ–‡æ ‡é¢˜/ç¬¬ä¸€æ¬¡å‡ºç°çš„ headerï¼‰
        - åç»­é‡å¤è¡Œä¼šè¢«åˆ é™¤
    å¦‚æœä¸€ä¸ª text block å…¨éƒ¨è¢«åˆ ç©ºï¼Œåˆ™æ•´ä½“åˆ é™¤ã€‚
    """
    patterns = detect_header_footer_patterns(content)
    if not patterns:
        return content

    pattern_set = set(patterns)
    # è®°å½•æ¯ç§ pattern å·²ç»å‡ºç°è¿‡å‡ æ¬¡ï¼šç”¨äºâ€œä¿ç•™ç¬¬ä¸€æ¬¡â€
    pattern_seen: Dict[str, int] = {p: 0 for p in pattern_set}

    new_content: List[Dict[str, Any]] = []

    for block in content:
        if block.get("type") != "text":
            new_content.append(block)
            continue

        text = block.get("text") or ""
        lines = text.splitlines()
        kept_lines: List[str] = []

        for line in lines:
            norm = _normalize_line(line)
            if norm and norm in pattern_set:
                # å‘½ä¸­ header/footer æ¨¡å¼
                if pattern_seen[norm] == 0:
                    # ç¬¬ä¸€æ¬¡å‡ºç°ï¼šä¿ç•™ï¼Œå¹¶æ‰“æ ‡â€œå·²è§è¿‡â€
                    pattern_seen[norm] += 1
                    kept_lines.append(line)
                else:
                    # åç»­å‡ºç°ï¼šè§†ä¸ºé¡µçœ‰/é¡µè„šè¡Œï¼Œåˆ é™¤
                    continue
            else:
                kept_lines.append(line)

        joined = "\n".join(kept_lines).strip()
        if not joined:
            # æ•´å—éƒ½æ˜¯ header/footerï¼Œç›´æ¥åˆ æ‰
            continue

        nb = dict(block)
        nb["text"] = joined
        new_content.append(nb)

    return new_content


# ======= åˆ é™¤ Checklist ======= #

def drop_checklist(content: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    new = []
    for blk in content:
        sec = (blk.get("section") or "").strip().lower()
        if sec == "checklist":
            continue
        new.append(blk)
    return new


# ======= é‡å»º section_labels ======= #

def rebuild_section_labels(content: List[Dict[str, Any]]) -> Dict[str, Any]:
    labels = []

    def push(start: int, end: int, section: str):
        if start is None or end is None:
            return
        if start == end:
            idx_expr = str(start)
        else:
            idx_expr = f"{start}-{end}"
        labels.append({"content_index": idx_expr, "section": section})

    current_sec = None
    rstart = None
    last_idx = None

    for blk in content:
        idx = blk["index"]
        sec = blk.get("section") or "Introduction"

        if current_sec is None:
            current_sec = sec
            rstart = idx
            last_idx = idx
            continue

        if sec == current_sec and idx == last_idx + 1:
            last_idx = idx
        else:
            push(rstart, last_idx, current_sec)
            current_sec = sec
            rstart = idx
            last_idx = idx

    push(rstart, last_idx, current_sec)
    return {"labels": labels, "model_used": "post_clean"}


# ======= ä¸»é€»è¾‘ ======= #

def process_one(path_in: Path, path_out: Path):
    data = load_json(path_in)
    content = data.get("content", [])

    # 1) åˆ é™¤ Checklist
    if DROP_CHECKLIST:
        content = drop_checklist(content)

    # 2) åˆ é™¤é¡µçœ‰/é¡µè„šï¼ˆåªåˆ é‡å¤ï¼Œä¿ç•™ç¬¬ä¸€æ¬¡ï¼‰
    if STRIP_HEADER_FOOTER:
        content = strip_header_footer(content)

    # 3) é‡æ–°ç¼–å· index
    new_content = []
    for i, blk in enumerate(content, start=1):
        nb = dict(blk)
        nb["index"] = i
        new_content.append(nb)

    # 4) é‡å»º section_labels
    section_labels = rebuild_section_labels(new_content)

    out_obj = dict(data)
    out_obj["content"] = new_content
    out_obj["section_labels"] = section_labels

    save_json(out_obj, path_out)


def main():
    root = Path(ROOT_DIR).expanduser().resolve()
    files = find_jsons(root, INPUT_NAME)

    print(f"[INFO] root={root}")
    print(f"[INFO] STRIP_HEADER_FOOTER={STRIP_HEADER_FOOTER}  DROP_CHECKLIST={DROP_CHECKLIST}")
    print(f"[INFO] Found {len(files)} files to process.")

    for p in tqdm(files, desc="Post-cleaning"):
        out = p.parent / OUTPUT_NAME
        process_one(p, out)

    print("[DONE]")

if __name__ == "__main__":
    main()