Summary
The paper proposes Q-FSRU, a medical VQA framework that transforms both image and text features into the frequency domain via FFT, compresses spectra with learnable filter banks, and applies cross-modal gated enhancement before answering. It augments reasoning with a quantum-inspired retrieval module that selects external medical knowledge using Uhlmann fidelity applied to normalized “quantum state” embeddings. Training combines cross-entropy with dual contrastive objectives to align modalities. The method reports strong results on VQA-RAD and zero-shot transfer to PathVQA, with ablations indicating that FFT-based spectral processing contributes the largest gains, while quantum-inspired retrieval and contrastive learning add further improvements.

Strengths
- Well-motivated use of frequency-domain processing for medical imaging, with a coherent pipeline that applies FFT-based spectral representations to both modalities and compresses them with learnable filters.
- Integration of a theoretically grounded similarity (Uhlmann fidelity) for knowledge retrieval is novel in the Med-VQA context and bridges quantum-inspired IR with multimodal clinical QA.
- Dual contrastive objectives to align modalities are appropriate and plausibly contribute to better cross-modal representations.
- Strong empirical results on VQA-RAD and improved zero-shot cross-dataset generalization to PathVQA support the practicality of the approach.
- Ablation studies indicate component importance, with spectral processing yielding the largest boost and additional gains from the quantum-inspired retrieval and contrastive training.
- Thoughtful data splitting to reduce leakage at the image/patient level enhances experimental rigor.
- Clear high-level narrative and figures (e.g., system diagram, spectral visualizations) help communicate the overall design.

Weaknesses
- Architectural inconsistencies: The paper conflicts on whether the aggregated knowledge embedding (k_agg) is included in the classifier input (z_concat) for prediction (§4.5.1 includes it, §4.5.2 excludes it). Inputs to the retrieval module are also inconsistent (§4.4.3 uses t_enhanced/v_enhanced, whereas §4.5.1 passes t_freq/v_freq). These contradictions undermine clarity about the final model used for reported results.
- Incomplete description of the external knowledge base: The corpus’s sources, size, curation, licensing, indexing method, and update policy are unspecified, making it difficult to assess or reproduce the retrieval component’s impact. There is no evaluation of retrieval quality or grounding (e.g., clinical verification).
- Mischaracterized and under-specified operations in the frequency pipeline: The “parameterized convolution” in spectrum compression is written as a linear weighted sum rather than a true convolution; kernel shapes, strides, and padding are not provided. Gate matrices and tensor dimensions in cross-modal gating are not clearly specified.
- FFT design choices are insufficiently justified: Applying a 1D FFT to static 256-d embeddings is unconventional, and discarding phase information may lose useful alignment cues. No sensitivity analysis or rationale is provided for using magnitudes only.
- Loss formulation inconsistencies and hyperparameter rationale: The loss mixture changes between sections (e.g., general α/β weights vs. fixed 0.3/0.7), with no justification or sensitivity analysis. Temperatures and other training hyperparameters appear chosen ad hoc without reported tuning protocol.
- Fidelity versus cosine similarity: For normalized pure states, fidelity reduces to the squared inner product, making it closely related to cosine similarity. The paper does not explain why fidelity outperforms cosine in ablations, nor does it analyze numerical stability, normalization, or computational/memory costs. The indexing/storage backend for retrieval (e.g., FAISS) is not described.
- Statistical reporting gaps: p-values are presented without specifying the statistical test (paired t-test, Wilcoxon, etc.), the unit of analysis (folds, images, questions), or whether corrections for multiple comparisons were applied. The protocol for multi-class AUC (macro/micro, one-vs-rest) is not detailed, and confidence intervals are absent.
- Baseline comparability concerns: It is unclear how large LLM-based baselines (e.g., LLaVA-Med, STLLaVA-Med) were trained or adapted (full fine-tuning, LoRA, inference-only), and parameter counts are ambiguously reported (e.g., “7000 M”). Adaptation details for non-medical baselines (e.g., FSRU from rumor detection) are missing, raising fairness and reproducibility concerns.
- Presentation inconsistencies and ambiguity: Conflicting definitions of z_concat, inconsistent retrieval inputs, and misuse of “convolution” cause confusion. Several dimensions and axes are unclear or contradictory (e.g., Appendix sequence length up to 120 vs. L ≤ 50 in the main text). These issues impede independent reimplementation.
- Limited analysis of failure modes and clinical relevance: No error analysis stratified by question types or imaging modalities is provided, and there is no clinician assessment of answer quality or retrieval faithfulness, limiting insight into real-world robustness and safety.
