# Global Summary
The paper introduces Q-FSRU, a novel framework for Medical Visual Question Answering (Med-VQA). The core problem addressed is the limitation of existing models that operate primarily in the spatial domain and use classical knowledge retrieval methods, which can miss subtle diagnostic patterns and struggle with nuanced clinical reasoning. Q-FSRU's approach combines two main components: 1) Frequency Spectrum Representation and Fusion (FSRU), which transforms both image (ViT-B/16) and text features into the frequency domain using Fast Fourier Transform (FFT) to capture global contextual patterns, and 2) a Quantum-inspired Retrieval-Augmented Generation (Quantum RAG) mechanism that uses Uhlmann fidelity to measure similarity and retrieve relevant medical knowledge. The model is evaluated primarily on the VQA-RAD dataset, with generalization tested on PathVQA. The key finding is that Q-FSRU achieves state-of-the-art performance on VQA-RAD, reaching 90.0% accuracy, a 2.9% absolute improvement over the strongest baseline (FSRU). Ablation studies confirm that the frequency processing component provides the most significant performance gain (-4.9% accuracy when removed).

# Introduction
- Med-VQA is an interdisciplinary challenge combining computer vision, NLP, and clinical decision-making.
- Key challenges in Med-VQA include data scarcity, specialized terminology, complex imaging modalities, and the critical nature of medical decisions.
- Existing transformer-based models (e.g., LLaVA-Med) have shown progress but are limited by operating in the spatial domain, potentially missing frequency-based patterns relevant to pathology.
- Retrieval-augmented methods improve factual grounding but often use classical similarity measures (e.g., cosine similarity) that may not capture complex semantic relationships.
- The paper proposes Q-FSRU, which combines Frequency Spectrum Representation and Fusion (FSRU) with Quantum-inspired Retrieval-Augmented Generation (Quantum RAG).
- The core motivations are: 1) frequency domain transformation can capture global contextual patterns missed by spatial processing, and 2) quantum-inspired similarity measures may better capture nuanced semantic relationships for knowledge retrieval.
- Contributions are: 1) a novel frequency domain fusion framework for Med-VQA using FFT; 2) a quantum-inspired retrieval mechanism using amplitude similarity; 3) demonstrating competitive performance on the VQA-RAD dataset; 4) analysis showing the combination improves performance and interpretability.

# Abstract
- The paper proposes Q-FSRU, a model for medical VQA that combines Frequency Spectrum Representation and Fusion (FSRU) with Quantum Retrieval-Augmented Generation (Quantum RAG).
- The model transforms image and text features into the frequency domain using Fast Fourier Transform (FFT) to focus on meaningful data and reduce noise.
- A quantum-inspired retrieval system fetches external medical facts using quantum-based similarity techniques to improve accuracy and grounding.
- Evaluation on the VQA-RAD dataset shows Q-FSRU outperforms previous models, particularly on complex reasoning cases.
- The combination of frequency and quantum-inspired methods is claimed to improve both performance and explainability.

# Related Work
- **Medical VQA:** Early models struggled with clinical data. Recent models like STLLaVA-Med have improved domain adaptation but operate in the spatial domain and use conventional retrieval.
- **Frequency-Domain Representations:** Prior work like FDTrans (Cai et al., 2023) and FreqU-FNet (Xing, 2025) has shown the value of frequency analysis in medical imaging for diagnostics and segmentation. Lao et al. (2024) used frequency analysis for multimodal rumor detection, but not for Med-VQA or with knowledge retrieval.
- **Quantum-Inspired Methods in IR:** This is an established research area. The paper cites Kankeu et al. (2025) for using quantum-inspired similarity metrics for representation learning and Nguyen et al. (2025) for applying quantum principles in a vision-brain understanding neural network. These works establish the viability of quantum principles for similarity and representation.
- **Knowledge Retrieval in VQA:** Foundational work includes Retrieval-Augmented Generation (Lewis et al., 2021). The paper notes that standard retrieval struggles with nuanced clinical reasoning.
- **Research Contributions:** The paper claims its novelty lies in the specific combination of frequency-domain processing and quantum-inspired retrieval within a unified framework (Q-FSRU) for the Med-VQA task.

# Preliminaries
- The problem is formulated as a multimodal classification task on the VQA-RAD dataset, which contains image-question-answer triplets `(I_i, Q_i, y_i)`.
- The model learns a mapping `f: (I_i, Q_i) -> y_hat_i`.
- The model has two main components: frequency-spectral fusion `z_i^freq = f_FSRU(I_i, Q_i)` and knowledge retrieval `k_i`.
- The final prediction is made by an MLP that concatenates the frequency-fused features and the retrieved knowledge: `y_hat_i = MLP([z_i^freq || k_i])`.
- The model is trained with a combined loss function: `L = L_CE + alpha * L_intra + beta * L_cross`, where `L_CE` is cross-entropy loss, and `L_intra` and `L_cross` are intra-modal and cross-modal contrastive losses.

# Method
- **Architecture Overview:** The model has four components: 1) multimodal feature extraction, 2) frequency-domain processing with FFT, 3) quantum-inspired knowledge retrieval, and 4) multimodal fusion with contrastive learning.
- **Feature Extraction:**
    - Text: Questions are tokenized and mapped to 300-dim domain-specific embeddings, then mean-pooled and projected to a 256-dim vector.
    - Image: A ViT-B/16 backbone pretrained on ImageNet processes 224x224 images, producing a 768-dim vector which is then projected to 256 dimensions.
- **Frequency Spectrum Representation and Fusion (FSRU):**
    - A 1D FFT is applied to the 256-dim image and text feature vectors. The real-valued magnitude spectrum is retained.
    - Unimodal Spectrum Compression uses learnable filter banks (K=4) to compress the frequency representations.
    - Cross-Modal Co-selection uses a gated attention mechanism to mutually enhance the text and image features.
- **Quantum-Inspired Retrieval Augmentation:**
    - Features are represented as pure quantum states `|psi(x)> = x / ||x||_2` and corresponding density matrices `rho(x)`.
    - Similarity is computed using the Uhlmann fidelity measure between the query's and knowledge base entries' density matrices.
    - The retrieval pipeline forms a multimodal query `q_multi` by averaging the enhanced text and image features, computes fidelity scores, retrieves the Top-3 knowledge entries, and aggregates them using a weighted sum with softmax temperature `tau = 0.1`.
- **Multimodal Fusion and Classification:**
    - The feature integration pipeline is sequential: frequency processing is followed by quantum RAG.
    - The final concatenated feature vector for classification is `z_concat = [t_freq || v_freq || k_agg]`.
    - However, the MLP classifier section specifies the input as `z_concat = [t_freq || v_freq]`, a 512-dim vector.
    - The MLP has a `512 -> 1024 -> 256 -> C` architecture with LayerNorm, GELU, and Dropout (p=0.1).
- **Optimization Objective:**
    - A dual contrastive learning framework is used. Intra-modal contrastive loss uses temperature `tau = 0.07`. Cross-modal contrastive loss uses `tau = 0.05`.
    - The total loss is `L_total = L_CE + (0.3 * L_intra_combined + 0.7 * L_cross)`.

# Experiments
- **Datasets:**
    - VQA-RAD: 3,515 question-answer pairs from radiology images (X-ray, CT, MRI).
    - PathVQA: 32,799 question-answer pairs from 4,998 pathology images, used for generalization testing.
- **Implementation Details:**
    - Implemented in PyTorch with Adam optimizer (LR 5e-5, L2 weight 1e-5).
    - Trained with 5-fold cross-validation for up to 50 epochs, batch size 32, and early stopping (patience 10).
    - Images resized to 224x224, questions padded/truncated to 50 tokens.
- **Baselines:** MCAN, LXMERT (general VQA); LLaVA-Med, STLLaVA-Med (medical VQA); LaPA (knowledge-augmented); FSRU (frequency-domain).
- **Main Results on VQA-RAD (Table 1):**
    - Q-FSRU achieves 90.0 ± 0.5% accuracy, 85.2 ± 0.6% F1-score, 88.3 ± 0.4% precision, 83.1 ± 0.8% recall, and 0.954 ± 0.01 AUC.
    - This represents a +2.9% improvement in accuracy, +2.9% in F1-score, and +0.033 in AUC over the strongest baseline (FSRU).
    - All improvements are statistically significant (p < 0.01).
    - Q-FSRU has 92.4M parameters.
- **Cross-Dataset Generalization (Table 2):**
    - VQA-RAD → PathVQA: Q-FSRU achieves 81.7 ± 0.9% accuracy (+3.3% over FSRU).
    - PathVQA → VQA-RAD: Q-FSRU achieves 80.3 ± 1.0% accuracy (+3.4% over FSRU).
- **Ablation Studies (Table 3):**
    - Removing Frequency Processing causes the largest drop in accuracy: -4.9% (p < 0.001).
    - Removing Quantum Retrieval drops accuracy by -3.2% (p < 0.01).
    - Removing Contrastive Learning drops accuracy by -2.7% (p < 0.01).
    - Using only spatial fusion drops accuracy by -5.8% (p < 0.001).
    - Replacing quantum fidelity with cosine similarity drops accuracy by -1.9% (p < 0.05).
- **Qualitative Analysis:** The paper claims frequency processing helps detect subtle patterns and quantum retrieval provides relevant knowledge for ambiguous cases.

# Conclusion
- The paper introduced Q-FSRU, a Med-VQA framework combining frequency-domain processing and quantum-inspired knowledge retrieval.
- The frequency transformation helps capture global contextual patterns. The quantum retrieval enhances reasoning with external knowledge.
- Experiments on VQA-RAD show Q-FSRU outperforms SOTA models across multiple metrics (accuracy, F1, AUC).
- The model also demonstrates robust generalization in cross-dataset evaluations.
- Ablation studies confirm the significant contributions of each component, especially frequency processing.
- Future work includes scaling to larger datasets and refining the retrieval mechanism.

# References
This section lists the references cited in the paper, including works on VQA, frequency-domain methods, quantum-inspired IR, and retrieval-augmented generation.

# Appendix
- This section provides links to the VQA-RAD and PathVQA datasets on Kaggle for reproducibility.
- It includes Figure 2, which shows example frequency spectrograms for a medical image (axial MRI brain scan) and a clinical question ("Is there evidence of pneumonia in the lung?") from the VQA-RAD dataset.