Summary
- The paper proposes Q-FSRU, a medical visual question answering (Med-VQA) framework that fuses frequency-domain features with a quantum-inspired retrieval module. The method applies 1D FFT to text and image embeddings, compresses spectra with learnable filters, and performs cross-modal gated co-selection before retrieval via quantum fidelity over density matrices (Sections 4.3–4.4; Equations in 4.3.1, 4.3.2, 4.4.1–4.4.2; Figure 1). The training objective combines cross-entropy with intra- and cross-modal contrastive losses (Section 4.5.3, 4.5.4). Experiments on VQA-RAD and cross-dataset evaluation to PathVQA report superior accuracy and F1, with statistically significant gains over baselines (Tables 1–3; Sections 7.1, 7.1.1, 7.2). The authors claim improved performance and interpretability from spectral processing and quantum retrieval (Abstract; Section 8; Figure 2).Strengths
- Bolded title: **Novel integration of frequency-domain fusion with quantum-inspired retrieval**
  - The architecture combines FFT-based feature transformations with a quantum fidelity-based retrieval pipeline (Sections 4.3, 4.4; Equations in 4.3.1–4.3.3, 4.4.2; Figure 1), which is a distinctive composition relative to standard spatial-domain VQA and cosine-similarity retrieval; this matters for novelty and potential impact.
  - Explicit use of density matrices and Uhlmann fidelity (Section 4.4.1–4.4.2) provides a concrete quantum-inspired similarity formulation, improving technical soundness beyond vague “quantum” claims.
  - Cross-modal co-selection after spectral compression (Section 4.3.3) ties the two modalities in the frequency domain, contributing a coherent design component for multimodal fusion.- Bolded title: **Clear mathematical formulation of core components**
  - The FFT-based transformation and magnitude spectrum selection are defined with equations (Section 4.3.1), aiding reproducibility and technical clarity.
  - Spectrum compression via learnable filter banks is parameterized (Section 4.3.2), specifying Wfilter shapes and indexing; this improves implementability.
  - Quantum state normalization and density matrix construction (Section 4.4.1) and fidelity computation (Section 4.4.2) are precisely stated; this matters for rigor.- Bolded title: **Contrastive learning to align intra- and cross-modal representations**
  - The dual contrastive objectives with modality-specific temperatures are formalized (Section 4.5.3), showing a thoughtful training design for feature alignment.
  - The complete loss combines CE and contrastive terms with explicit weights and temperatures (Section 4.5.4), clarifying optimization and enhancing technical soundness.
  - Ablation indicates contrastive learning contributes measurable gains (+2.7% accuracy, p < 0.01) (Table 3; Section 7.2), supporting empirical value.- Bolded title: **Empirical gains with statistical reporting**
  - On VQA-RAD, Q-FSRU achieves 90.0% accuracy and +2.9% improvement over the strongest baseline (FSRU) (Table 1; Section 7.1), substantiating effectiveness.
  - Improvements are consistent across F1, precision, recall, and AUC (+0.033) (Table 1; Section 7.1), indicating robust performance, which matters for impact.
  - Statistical significance (p < 0.01) is reported for main comparisons (Table 1; Sections 7.1), strengthening the claim of non-random gains.- Bolded title: **Cross-dataset generalization to PathVQA**
  - Zero-shot transfer from VQA-RAD to PathVQA yields +3.3% accuracy over baselines, and +3.4% in the reverse direction (Table 2; Section 7.1.1), suggesting transferability.
  - The method outperforms frequency-only FSRU by notable margins in both directions (Table 2), implying the retrieval mechanism and spectral fusion contribute generalization.
  - Inclusion of two datasets (Section 5.1) broadens evaluation scope beyond a single domain, supporting claims of robustness.- Bolded title: **Ablation study disentangles component contributions**
  - Removing frequency processing causes the largest drop (−4.9% accuracy; p < 0.001) (Table 3; Section 7.2), demonstrating the centrality of spectral features.
  - Quantum fidelity outperforms cosine similarity (+1.9% accuracy; p < 0.05) (Table 3), empirically validating the retrieval choice.
  - Cross-modal co-selection and contrastive learning also show positive effects (Table 3), clarifying each module’s role and enhancing interpretability of results.- Bolded title: **Implementation details and training protocol are described**
  - The paper reports optimizer, learning rate, regularization, batch size, epochs, and LR decay (Section 5.1), improving reproducibility.
  - 5-fold cross-validation with patient-level separation per image’s questions is stated (Section 5.1), indicating attention to data leakage risks.
  - Data preprocessing includes image resizing and tokenization details (Section 5.1), increasing transparency.- Bolded title: **Architectural clarity supported by figures**
  - Figure 1 outlines the overall pipeline integrating FSRU and Quantum RAG (Section 4; Figure 1), aiding reader comprehension.
  - Figure 2 and Appendix visuals depict frequency spectrograms for image and text (Appendix; Figure 2; Blocks 44–46), supporting the spectral processing narrative.
  - Equations across Sections 4.2–4.5 connect components with shapes and flows, improving clarity.Weaknesses
- Bolded title: **Inconsistency about whether knowledge embeddings enter classification**
  - Section 4.5.1 Step 3 concatenates [t_freq | v_freq | k_agg] ∈ R^{3d_model}, implying knowledge embeddings are used in classification, but Section 4.5.2 states “excluding the quantum knowledge embeddings” and defines z_concat = [t_freq | v_freq] ∈ R^{2d_model}; this is a direct contradiction that affects technical soundness.
  - The Abstract and Introduction claim that retrieved knowledge is “merged with the frequency-based features for stronger reasoning” (Abstract; Section 1), but Section 4.5.2 contradicts this; this matters for clarity and validity of the claimed mechanism.
  - Dimensionality progression in Section 4.5.2 (512→1024→256→C) aligns with excluding k_agg, conflicting with Section 4.5.1’s 3d_model concatenation; this undermines reproducibility.- Bolded title: **Insufficient detail on the knowledge base and retrieval construction**
  - Section 4.4.3 defines Top-3 retrieval over {Sim_i}_{i=1}^N but does not specify what k_i are (source corpora, size N, preprocessing, indexing), limiting reproducibility and fairness.
  - Section 3 mentions “external corpora” without identifying sources, licensing, or domain filtering (“k_i represents relevant medical knowledge retrieved from external corpora”), which matters for grounding and potential leakage.
  - There is no description of how k_i embeddings are produced (models, training, dimensionality) before forming quantum states ρ(k_i) (Sections 4.4.1–4.4.3); No direct evidence found in the manuscript.- Bolded title: **Questionable justification for 1D FFT along feature dimension instead of spatial frequency for images**
  - Section 4.3.1 applies a 1D FFT to a 256-d pooled image vector (v_proj), rather than performing a 2D FFT over spatial dimensions typical for capturing image-wide frequency patterns; the rationale is not provided, affecting technical soundness.
  - Claims that spectral processing “captures global contextual patterns” (Section 1; Section 8) are not supported by controlled analyses that compare 1D feature-FFT to 2D spatial-FFT; No direct evidence found in the manuscript.
  - Figure 2 shows spectrograms (Appendix; Figure 2), but no quantitative assessment of which frequency bands contribute to performance or pathology detection is provided; No direct evidence found in the manuscript.- Bolded title: **Cross-dataset evaluation lacks clarity about categorical label alignment**
  - The problem is formulated as categorical classification (Section 3), and training focuses on discrete answer classes; PathVQA has different answer distributions and open-ended answers, yet Table 2 reports accuracy for zero-shot transfer without explaining label-space mapping or normalization, impacting validity.
  - Section 5.1 states zero-shot transfer without fine-tuning, but does not describe how predictions are evaluated when label sets differ (e.g., mapping, canonicalization); No direct evidence found in the manuscript.
  - There is no disclosure of answer vocabulary construction or harmonization across datasets (Section 5.1), which affects experimental rigor.- Bolded title: **Baselines and fairness of comparison are under-specified**
  - Section 6 claims comparisons with large models (e.g., LLaVA-Med, STLLaVA-Med) and reports parameters (Table 1), but Section 5.1 does not describe how these baselines were trained or evaluated (fine-tuning vs. zero-shot, prompts, decoding), impacting fairness.
  - The paper reports statistically significant improvements (Table 1) without detailing the baselines’ hyperparameters, compute budgets, or adaptation procedures; No direct evidence found in the manuscript.
  - It is unclear whether FSRU (Lao et al., 2024) was re-implemented under the same training regime (Section 6; Table 1), which matters for reproducibility.- Bolded title: **Statistical testing methodology is missing**
  - p-values are reported (Table 1, Table 3; Sections 7.1, 7.2), but the statistical test (e.g., paired t-test, Wilcoxon), the unit of analysis (folds vs. runs), and assumptions are not provided, weakening the robustness of significance claims.
  - Confidence intervals or effect sizes are not reported (Sections 7.1, 7.2), limiting interpretability beyond p-values; No direct evidence found in the manuscript.
  - There is no correction for multiple comparisons across metrics (Table 1, Table 3), affecting statistical rigor; No direct evidence found in the manuscript.- Bolded title: **Potentially problematic augmentations and absence of clinical risk analysis**
  - Random horizontal flipping is applied to medical images (Section 5.1) without discussion of laterality-sensitive questions (e.g., left/right pathology), which can induce label inconsistency; this affects technical soundness.
  - There is no bias or safety analysis regarding training on limited data (Section 5.1) and evaluation on clinical questions (Section 1), which matters for real-world deployment; No direct evidence found in the manuscript.
  - Ethical considerations or failure mode analysis are absent (Sections 7–8), despite claims of clinical applicability; No direct evidence found in the manuscript.- Bolded title: **Limited qualitative evidence for interpretability and retrieval grounding**
  - Section 7.2.1 claims “illustrative cases” and that quantum retrieval provides explanatory evidence, but no retrieved passages, attention maps, or case studies are shown; No direct evidence found in the manuscript.
  - Figure 2 shows frequency spectrograms but does not connect them to decision rationales or clinical features (Appendix; Figure 2), limiting interpretability.
  - The Abstract and Conclusion assert improved explainability (Abstract; Section 8), yet the paper does not provide qualitative or user-study evidence; No direct evidence found in the manuscript.Suggestions for Improvement
- Bolded title: **Resolve and document the knowledge integration pathway**
  - Unify Sections 4.5.1 and 4.5.2 by clearly specifying whether k_agg is concatenated for classification; update equations and dimensionalities to be consistent across both sections.
  - If knowledge influences prediction indirectly (e.g., via feature refinement), revise Step 3 in Section 4.5.1 to reflect the actual mechanism and provide an end-to-end computational graph (Figure 1 update).
  - Provide an ablation comparing “with k_agg in classifier” vs. “without” under identical settings to substantiate the intended design (Table 3 extension).- Bolded title: **Detail the knowledge base and retrieval construction pipeline**
  - Specify sources for k_i (e.g., PubMed abstracts, radiology textbooks), corpus size N, preprocessing, and licensing (Section 4.4.3); include a data card for the KB in Appendix.
  - Describe how k_i embeddings are computed (model, dimensionality, training/fine-tuning), and how ρ(k_i) is formed; add shapes and normalization steps analogous to Section 4.2 and 4.4.1.
  - Report retrieval metrics (recall@K, MRR) and latency, and include a sensitivity analysis for K and τ (Section 4.4.3) to assess the retrieval module’s behavior.- Bolded title: **Justify the 1D FFT choice and analyze spectral contributions**
  - Provide rationale and controlled experiments comparing 1D FFT on pooled features vs. 2D FFT on spatial grids; add a table quantifying gains/losses in both settings (Section 4.3).
  - Include a frequency-band ablation (e.g., low/mid/high-pass filters) showing which bands drive performance, connected to clinical patterns (Table added to Section 7.2).
  - Augment Figure 2 with per-band attribution maps or integrate spectral attention visualization to relate spectral components to predicted answers.- Bolded title: **Clarify cross-dataset evaluation and label alignment**
  - Describe the answer vocabulary harmonization strategy (e.g., mapping both datasets to a shared categorical set) and how out-of-vocabulary answers are handled (Section 5.1; Table 2 notes).
  - Provide evaluation protocol details: whether accuracy is computed on overlapping classes only or via a normalization step; include a confusion matrix per transfer direction.
  - Add a robustness analysis to show zero-shot behavior versus few-shot fine-tuning baselines to contextualize the reported gains (Table 2 extension).- Bolded title: **Strengthen baseline comparisons and reproducibility**
  - Document training/evaluation settings for each baseline (fine-tuning data, epochs, LR, prompts, decoding strategies), and ensure the same folds/splits and data augmentations; add a Baselines Appendix.
  - Where re-implementations are infeasible (e.g., very large LLMs), explicitly state whether numbers are from prior work and under what conditions; adjust fairness claims accordingly (Section 6).
  - Provide parameter counts, FLOPs, and wall-clock times uniformly across methods to contextualize performance vs. efficiency (Table 1 extension).- Bolded title: **Report rigorous statistical testing procedures**
  - State the statistical test used (e.g., paired t-test over folds), assumptions, and units of analysis; include effect sizes and confidence intervals (Sections 7.1, 7.2).
  - Apply corrections for multiple comparisons across metrics (e.g., Holm-Bonferroni) and report adjusted p-values (Tables 1–3 updates).
  - Add variance across runs/seeds beyond folds to strengthen conclusions about stability.- Bolded title: **Audit augmentations and add clinical risk analyses**
  - Reassess horizontal flipping for laterality-sensitive tasks; either disable it for such questions or add left-right consistent augmentation strategies with a justification (Section 5.1).
  - Include a brief risk assessment and bias analysis (e.g., modality distribution, demographic proxies) and discuss limitations for clinical deployment (Section 8 or Appendix).
  - Provide failure case analyses indicating question types or modalities where the model errs and outline mitigation strategies.- Bolded title: **Provide qualitative interpretability and retrieval-grounding evidence**
  - Add case studies with image, question, retrieved passages (k_j), their weights (softmax over Sim_j), and the final answer to demonstrate grounding (Section 7.2.1).
  - Offer attention or gating visualizations for cross-modal co-selection (Section 4.3.3) and relate them to clinical features to substantiate interpretability claims.
  - Include clinician feedback or a small user study assessing whether retrieved knowledge and spectral explanations are acceptable and useful for clinical reasoning.Score
- Overall (10): 6 — Strong empirical gains and a novel combination of spectral fusion with quantum-inspired retrieval (Table 1; Sections 4.3–4.4), but key inconsistencies and missing details (Sections 4.5.1 vs. 4.5.2; Section 4.4.3; Table 2) limit technical robustness.
- Novelty (10): 7 — Integrating frequency-domain processing with quantum fidelity-based retrieval for Med-VQA is distinctive (Sections 4.3–4.4; Table 3), though prior work on spectral methods and quantum-inspired IR exists (Section 2.2–2.3).
- Technical Quality (10): 5 — Clear formulations (Sections 4.3–4.5) and ablations (Table 3), but contradictions in feature integration, incomplete retrieval pipeline details, and unclear cross-dataset label alignment (Sections 4.5.1–4.5.2; 4.4.3; Table 2) weaken rigor.
- Clarity (10): 6 — Architecture and equations are well presented (Figure 1; Sections 4.2–4.5), yet several claims lack precise documentation (use of k_agg; statistical methods; baseline protocols) and interpretability evidence (Section 7.2.1; No direct evidence found).
- Confidence (5): 4 — The manuscript provides extensive equations and results (Sections 4–7; Tables 1–3), but unresolved inconsistencies and missing experimental details reduce certainty in conclusions.