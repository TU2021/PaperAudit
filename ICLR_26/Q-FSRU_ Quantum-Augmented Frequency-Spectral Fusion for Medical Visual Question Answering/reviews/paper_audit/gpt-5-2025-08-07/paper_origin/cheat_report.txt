Academic integrity and internal consistency risk report

Summary
The manuscript contains several high-impact internal inconsistencies, mathematical inaccuracies, and missing methodological details that materially affect correctness, reproducibility, and the credibility of the reported improvements. Below, each issue is documented with explicit anchors to the manuscript.

1) Core pipeline contradiction: whether external knowledge is used for prediction
- Description: The paper alternates between including and excluding quantum-retrieved knowledge embeddings in the classifier.
- Evidence:
  - Section 3 (Problem Definition): “Feature integration: ŷ = MLP([zfreq ∥ ki])” — explicitly states concatenation of frequency features with knowledge embeddings (Block #11).
  - Section 4.5.1 (Feature Integration Pipeline): Step 3 defines zconcat = [tfreq ∥ vfreq ∥ kagg] ∈ R^{3dmodel} (Block #23).
  - Section 4.5.2 (MLP Classifier): “The fused input consists of only the frequency-enhanced text and image features concatenated, excluding the quantum knowledge embeddings” and redefines zconcat = [tfreq ∥ vfreq] ∈ R^{2dmodel} (Block #24).
- Impact: This contradicts the claimed role of “Quantum Inspired RAG” in improving accuracy. If kagg is excluded from classification, retrieval cannot influence predictions, directly undermining results and claimed contributions.

2) Dimensional inconsistency in classifier inputs
- Description: Incompatible shapes are specified between Section 4.5.1 and 4.5.2.
- Evidence:
  - dmodel = 256 (Section 4.2.1; Block #12).
  - Section 4.5.1: zconcat = [tfreq ∥ vfreq ∥ kagg] ∈ R^{3dmodel} = 768 (Block #23).
  - Section 4.5.2: zconcat = [tfreq ∥ vfreq] ∈ R^{2dmodel} = 512; W1 ∈ R^{1024×512} (Block #24).
- Impact: The classifier’s first layer expects 512-d input, but the preceding pipeline also defines a 768-d input. This makes the implementation unclear/inconsistent and non-reproducible.

3) Retrieval input mismatch and undefined intermediate representations
- Description: The features used for quantum retrieval differ across sections, and key tensors are undefined.
- Evidence:
  - Section 4.4.3 (Knowledge Retrieval Pipeline): qmulti = 0.5(tenhanced + venhanced) (Block #21).
  - Section 4.5.1: kagg = QuantumRAG(tfreq, vfreq) (Block #23), contradicting the use of tenhanced/venhanced.
  - Section 4.3.3 (Cross-Modal Co-Selection) references tcompressed, vcompressed, then tenhanced, venhanced (Block #18), but shapes of “compressed” outputs are not specified; Section 4.3.2’s “compression” yields 4 filter-bank outputs f_m^{(k)} (Block #16), making AvgPool and gating shapes unclear.
- Impact: The retrieval stage cannot be replicated; the effect of CSC/USC on retrieval is ambiguous.

4) Frequency representation mismatch between method and figures
- Description: The manuscript describes a 1D FFT over 256-d features, but visualizations present 2D spectra with large axes.
- Evidence:
  - Section 4.3.1: 1D FFT on tfreq, vfreq ∈ R^{256} (Block #15).
  - Appendix Figure captions and images: “Medical Image Frequency Spectrum” with Y-axis 0–700 and X-axis 0–800 (Blocks #41, #46); “Text Data Linguistic Spectrum” with Sequence Length 0–120 and Semantic Dimensions 0–120 (Blocks #42, #44).
- Impact: The presented spectrograms do not correspond to the stated 1D FFT of 256-d vectors, casting doubt on the actual processing/visualization pipeline.

5) Question length inconsistency
- Description: Experimental preprocessing caps questions at 50 tokens, but the spectrum figure uses lengths up to 120.
- Evidence:
  - Section 5.1: questions truncated/padded to max length 50 (Block #28).
  - Appendix text spectrum: Sequence Length 0–120 (Blocks #42, #44).
- Impact: The visualization appears inconsistent with the described preprocessing, reducing trust in qualitative analyses.

6) Training objective inconsistency
- Description: The total loss is defined differently across sections.
- Evidence:
  - Section 3: L = LCE + α Lintra + β Lcross (Block #11).
  - Section 4.5.4: Ltotal = LCE + [0.3·(Lintra-text+Lintra-image)/2 + 0.7·Lcross] with fixed coefficients (Block #26).
- Impact: The change in loss formulation and coefficients (from α, β to fixed 0.3/0.7 and a different decomposition) is not reconciled, impacting reproducibility and the interpretation of ablation results.

7) Dataset scope inconsistency (Abstract vs Experiments)
- Description: Abstract claims evaluation on VQA-RAD only, whereas experiments report two datasets and cross-dataset transfer.
- Evidence:
  - Abstract: “We evaluated our model using the VQA-RAD dataset” (Block #2).
  - Section 5.1: “two established benchmarks” (VQA-RAD and PathVQA) with cross-dataset zero-shot transfer (Block #28).
  - Table 2 (Section 7.1.1): cross-dataset results VQA-RAD↔PathVQA (Block #33).
- Impact: Inconsistent claims about evaluation scope.

8) Cross-dataset training regime inconsistency
- Description: The zero-shot setup is described as training on VQA-RAD and evaluating on PathVQA, but results also report the reverse direction without detailing training on PathVQA.
- Evidence:
  - Section 5.1: “models trained on VQA-RAD are directly evaluated on PathVQA” (Block #28).
  - Table 2: includes “PathVQA → VQA-RAD” results (Block #33).
- Impact: The training protocol for the reverse direction is not specified, undermining the validity of those results.

9) Mathematical error in quantum fidelity range
- Description: The fidelity range is misstated.
- Evidence:
  - Section 4.4.2: “This measure satisfies … 0 < Fid(ρ1, ρ2) < 1” (Block #19).
- Correct property: Fidelity ∈ [0, 1], and equals 0 for orthogonal pure states. The manuscript incorrectly excludes 0.
- Impact: Factual mathematical error in a core similarity measure.

10) Missing knowledge base specification
- Description: The source, size, preprocessing, and embedding method for the external medical knowledge base are not described.
- Evidence:
  - Section 3: “k_i represents relevant medical knowledge retrieved from external corpora” (Block #11).
  - Section 4.4.3: retrieval pipeline defined abstractly (Block #21).
  - Section 5.1: “retrieved K=3 knowledge passages per query using direct similarity computation” (Block #28).
  - No description of the corpus (content/size), indexing, embedding model, or normalization. No direct evidence found in the manuscript specifying the knowledge source or encoding details.
- Impact: The claimed gains from quantum retrieval cannot be validated or reproduced.

11) Statistical significance reporting without test specification
- Description: p-values are reported repeatedly without the statistical test or variance source being specified.
- Evidence:
  - Section 6 and 7: “statistically significant (p-value < 0.01)” (Block #29); “p < 0.01” (Block #31); ablation p-values (Block #34).
  - No direct evidence found in the manuscript describing the statistical test (e.g., paired t-test, bootstrap), the number of runs, or whether cross-validation folds are used for hypothesis testing.
- Impact: Undermines the reliability of significance claims.

12) “Convolution” terminology vs equation
- Description: Unimodal spectrum compression is described as “parameterized convolution,” but the provided formula is a linear projection over the feature dimension.
- Evidence:
  - Section 4.3.2: “Learnable filter banks compress the frequency representations using parameterized convolution,” then defines f_m^{(k)} = Σ_j W_filter^{(k,j)}·m_freq^{(j)} + b_filter^{(k)} (Block #16).
- Impact: Method ambiguity; unclear whether 1D convolution was implemented or simple linear filters, which affects reproducibility and fair comparison.

13) Undefined augmentation for text in contrastive losses
- Description: Contrastive loss uses t_aug/v_aug but text augmentation is not described.
- Evidence:
  - Section 4.5.3: Lintra uses t_aug and v_aug (Block #25).
  - Section 5.1: augmentation described only for images (random flip, color jitter); No direct evidence found in the manuscript for text augmentation strategy.
- Impact: Incomplete training setup description.

Conclusion
The manuscript contains multiple substantive inconsistencies and missing details affecting core aspects of the method (feature fusion pipeline, use of external knowledge, retrieval inputs), mathematics (fidelity range), data preprocessing/visualization, and experimental protocols (cross-dataset training, statistical testing). These issues materially undermine the correctness, interpretability, and reproducibility of the reported findings. Addressing the above points with precise, consistent definitions, corrected mathematics, and complete experimental descriptions is necessary before the results can be considered trustworthy.