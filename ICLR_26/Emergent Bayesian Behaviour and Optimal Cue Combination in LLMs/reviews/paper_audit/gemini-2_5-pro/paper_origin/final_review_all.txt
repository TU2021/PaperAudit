1) Summary
The paper investigates whether Large Language Models (LLMs) implicitly develop Bayesian-like computational strategies for perceptual tasks, akin to human cognition. The authors introduce BayesBench, a benchmark suite of four psychophysics-inspired magnitude estimation tasks (length, location, distance, duration) across text and image modalities. They evaluate nine LLMs using a methodology that measures not only accuracy (NRMSE) and multimodal cue-combination efficiency (RRE), but also adaptive behavior through a new Bayesian Consistency Score (BCS). The BCS quantifies whether a model's behavior shifts in a Bayes-consistent manner under controlled ablations of noise and context. Key findings suggest that more capable models tend to exhibit more Bayes-consistent behavior, but high accuracy does not guarantee robust uncertainty handling. For instance, GPT-5 Mini achieves high accuracy on text tasks but fails to integrate visual cues efficiently, highlighting a dissociation between raw performance and underlying strategy.2) Strengths
*   **Novel and Rigorous Methodology:** The paper successfully translates the rigorous paradigm of human psychophysics to the study of LLMs, providing a novel and powerful conceptual framework for probing their implicit computational strategies.
    *   The four estimation tasks are well-defined, synthetic, and allow for precise, controlled manipulation of stimuli, which is a significant advantage over naturalistic benchmarks where noise is unquantifiable (Section 3.1, Figure 2).
    *   The use of systematic ablations—steering prompts, input noise, and context length—is a core strength, enabling the authors to test specific hypotheses about how models adapt to changing uncertainty (Section 3.1, Appendix A.3).
    *   The analysis is grounded in established cognitive models (Linear, Static Bayesian, Sequential Bayesian observers), providing a principled vocabulary for interpreting LLM behavior beyond simple input-output mapping (Section 3.2, Equations 1-4). This is exemplified by the clear regression-to-the-mean effects shown in Figure 1.*   **Insightful Metrics Beyond Accuracy:** The work introduces metrics that conceptually capture more nuanced aspects of model behavior than standard performance measures, leading to deeper potential insights.
    *   The Bayesian Consistency Score (BCS) is a key conceptual contribution. By measuring the *change* in fitted prior weights under ablation, it is designed to detect principled adaptation even in high-accuracy models where static behavioral signatures are weak (Section 3.5).
    *   The cue-combination efficiency metric (RRE) provides a normative baseline for multimodal integration, effectively revealing which models optimally weight information from different sources. This metric is crucial for uncovering the claimed dissociation between accuracy and efficiency (Section 3.5, Figure 8 middle panel).
    *   The composite BayesBench score aims to provide a holistic evaluation by combining accuracy, efficiency, and behavioral consistency, rewarding models that are not just accurate but also robust and adaptive (Section 3.6).*   **Significant and Nuanced Findings:** The paper's central claims, if supported, would provide compelling evidence for emergent cognitive-like behaviors in LLMs and highlight critical gaps in current models.
    *   The paper demonstrates a potential "Safety Gap" where high accuracy does not imply robust strategy. The case of GPT-5 Mini, which achieves near-perfect text accuracy but poor cue-combination efficiency, is a powerful and important claimed finding (Section 5.2, Figure 8, Appendix A.12).
    *   The discovery that Llama-4 Maverick can outperform a Bayes-optimal *linear* combiner suggests the use of more sophisticated, non-linear integration strategies, opening new avenues for research (Section 5.2, Figure 7).
    *   The general, albeit noisy, correlation observed between model accuracy and Bayesian tendencies (Bayesian Factor Evidence and BCS) supports the hypothesis that principled uncertainty handling may be an emergent property of capable models (Section 5.1, Figure 8 left and right panels).3) Weaknesses
*   **Flawed "Bayesian Factor Evidence" Metric and Severe Data Integrity Issues:** The "Bayesian Factor Evidence" is a central metric, but its reporting is undermined by ambiguity, impossible values, and corrupted data tables, casting serious doubt on the validity of the associated results.
    *   The complex factor analysis procedure is relegated to Appendix A.8. This non-standard method is not sufficiently justified in the main text, and its statistical properties are not discussed, making it difficult to assess the robustness of the resulting evidence scores.
    *   The appendix reports multiple "Bayes prob (%)" values that are mathematically impossible (i.e., greater than 100%). For example, values of 109% (Figure 14), 301% (Figure 16), and 376% (Figure 16) are reported for the Gemma 3 4B model. This indicates fundamental errors in data processing or reporting.
    *   The data tables for the maze distance task are severely corrupted (Figure 16). They contain numerous duplicate entries for the same models (e.g., GPT-5 Mini, Mistral 24B), missing data points (e.g., NRMSE for Gemma 3 4B it), and the aforementioned impossible probability values. This level of error makes the supporting data for a key task unreliable.*   **Fundamental Inconsistencies in Core Metric Definitions:** The paper's main contributions, the BCS and the composite BayesBench score, are defined inconsistently across the manuscript, making the quantitative results difficult to interpret or trust.
    *   The BayesBench score is defined as an **average** of its three components in the methods (Section 3.6, Equation 5), but it is calculated and presented as a **sum** in the results (Section 5.4, Figure 9, and accompanying table). This means the reported scores are three times larger than what the paper's own formula dictates.
    *   The Bayesian Consistency Score (BCS) is defined in Section 3.5 as a sum over five ablations where each component is +1, -1, or 0, implying a maximum possible score of 5. However, the results plot shows values up to 12 (Figure 8, right panel), and the normalization table sets the score range to [-15, 15] (Table 2), contradicting the metric's definition.*   **Potential Confounding Factors in Model Comparisons:** The study compares a diverse set of models, but several uncontrolled variables could confound the interpretation of the results as purely "emergent" behavioral strategies.
    *   The authors note that reasoning in GPT-5 Mini could not be fully disabled (Section 4, Appendix A.2.1). This is a major confound that likely explains its near-perfect performance on the text-based maze task (Section 5.1, Appendix A.12), making it difficult to compare its "perceptual" strategy directly with other models.
    *   The models under evaluation vary significantly in size, architecture, training data, and fine-tuning procedures (Table 3). While acknowledged, the paper does not systematically discuss how these factors might contribute to the observed differences in Bayesian behavior, attributing them broadly to "capability."
    *   The reliance on closed-source APIs, as acknowledged in the limitations (Section 6), introduces potential non-determinism and undocumented model updates that could affect the replicability and stability of the fine-grained model rankings presented.4) Suggestions for Improvement
*   **Thoroughly Audit and Correct the "Bayesian Factor Evidence" Analysis and Data Tables:**
    *   Integrate a concise summary of the factor analysis procedure (from Appendix A.8) into the main methods section and provide a clear rationale for why this specific approach was chosen over more standard model comparison techniques.
    *   Urgently identify the source of the impossible probability values (>100%) reported in the appendix (Figures 14, 16) and either correct them or provide a clear explanation for what this metric actually represents if it is not a probability.
    *   Completely overhaul the data tables in the appendix, particularly for the maze distance task (Figure 16). This includes removing all duplicate entries, filling in missing data, and ensuring all reported values are correct and plausible. The integrity of the paper's empirical claims depends on this.*   **Resolve Contradictions in Core Metric Definitions:**
    *   Unify the definition and application of the BayesBench score. The authors must choose whether it is an average or a sum and apply this choice consistently across the methods (Equation 5) and results (Figure 9). The text, formula, and figures must all align.
    *   Provide a clear and unambiguous definition of the Bayesian Consistency Score (BCS). If the score is summed over 15 conditions (5 ablations x 3 tasks), this should be stated explicitly in Section 3.5 to justify the score range shown in Figure 8 and Table 2. The current description is misleading.*   **Address and Discuss Confounding Variables More Directly:**
    *   Throughout the results and discussion, consistently qualify the findings related to GPT-5 Mini by highlighting the potential confound of its residual reasoning capabilities. This would provide a more balanced interpretation of its unique performance profile.
    *   Expand the discussion to include a more structured consideration of how known architectural or scale differences between the models might relate to their performance on BayesBench. This would add depth to the claim that these behaviors are "emergent."
    *   To improve robustness against API variability, consider reporting the variance across a few independent runs for each model and task, which would provide a better estimate of measurement uncertainty than the current bootstrapping approach that resamples from a single data collection pass.5) Score
*   Overall (10): 3 — The paper proposes a novel and interesting research direction but is critically undermined by severe technical flaws, including contradictory metric definitions and numerous errors in the reported data.
*   Novelty (10): 9 — The application of a psychophysics paradigm to probe LLM strategies and the conceptual introduction of the BCS metric are highly original contributions.
*   Technical Quality (10): 2 — The technical execution is extremely poor, with fundamental contradictions in core metrics (BCS, BayesBench) and the presence of impossible values and corrupted tables in the supporting data (Figures 14, 16).
*   Clarity (10): 4 — While the prose is generally well-written, the manuscript contains irreconcilable contradictions in its methodology and presents key results in a confusing and erroneous manner.
*   Confidence (5): 5 — I am highly confident in my assessment, as the identified issues are severe, numerous, and directly verifiable within the manuscript and its appendix.