# Global Summary
This paper investigates whether Large Language Models (LLMs) implicitly develop optimal Bayesian strategies for integrating noisy multimodal information, similar to humans, without explicit training. The authors introduce BayesBench, a psychophysics-inspired benchmark with four magnitude estimation tasks (length, location, distance, duration) across text and image modalities. They evaluate nine LLMs and a human baseline using controlled ablations of noise, context, and prompts. Key metrics include accuracy (NRMSE), cue-combination efficiency (RRE), and a novel Bayesian Consistency Score (BCS) that measures principled behavioral shifts. The study finds that while more capable LLMs often exhibit emergent Bayes-consistent behavior, high accuracy does not guarantee robust uncertainty handling. A key finding is the dissociation between capability and strategy: GPT-5 Mini achieves perfect text accuracy but fails to efficiently integrate visual cues. In contrast, Llama-4 Maverick attains the highest overall BayesBench score (0.85), driven by superior cue-combination efficiency and Bayesian consistency, even outperforming a normative linear Bayesian model, suggesting non-linear integration. The work highlights that accuracy-centric benchmarks may overlook brittle uncertainty handling and provides tools for probing these implicit computational strategies.

# Abstract
The paper explores the implicit computational strategies of LLMs, questioning if they, like humans, use near-optimal Bayesian methods to integrate noisy signals without explicit instruction. A new behavioral benchmark, BayesBench, is introduced, comprising four magnitude estimation tasks (length, location, distance, duration) over text and image, inspired by classic psychophysics. Nine diverse LLMs are evaluated against human judgments. Through controlled ablations of noise, context, and prompts, the study measures performance, behavior, and efficiency. A novel Bayesian Consistency Score (BCS) is proposed to detect Bayes-consistent behavioral shifts. Results show that capable models often adapt in a Bayes-consistent manner, but accuracy does not guarantee robustness. Notably, GPT-5 Mini achieves perfect text accuracy but fails to integrate visual cues efficiently, revealing a dissociation between capability and strategy. The findings suggest that accuracy-focused benchmarks may miss brittle uncertainty handling and that there is an emergent, principled handling of uncertainty in LLMs that correlates with accuracy. The benchmark and metric are released as evaluation tools.

# Introduction
- The paper frames magnitude estimation (length, duration, distance) as a fundamental computation in intelligence. Humans solve this via Bayesian integration of noisy sensory cues, weighting them by reliability and incorporating prior expectations.
- The central question is whether LLMs, trained on next-token prediction, spontaneously develop similar computational strategies for handling uncertainty.
- The study applies classical psychophysics methodology to LLMs, treating them as black-box observers and using controlled synthetic tasks as "unit tests" for multimodal robustness.
- The paper presents three main contributions:
    1. A systematic psychophysics framework for LLMs with four magnitude estimation tasks and controlled ablations.
    2. A new benchmark, BayesBench, which measures task performance, cue-combination efficiency, and Bayesian consistency via a novel Bayesian Consistency Score (BCS).
    3. The demonstration of emergent Bayes-consistent behavior in capable LLMs, alongside the discovery of a "Safety Gap" where highly accurate models fail to adopt robust strategies.

# Related Work
- The paper connects its work to human psychophysics, where phenomena like Weber-Fechner's law and regression-to-the-mean are explained by optimal Bayesian inference. Figure 1 shows that Llama-4 Maverick exhibits a regression-to-the-mean effect similar to human psychophysics data.
- It distinguishes its approach from prior work on Bayesian behavior in LLMs (e.g., in-context learning as inference) by focusing on implicit perceptual tasks rather than explicit reasoning or learned statistical rules.
- It notes that existing multimodal benchmarks like MMbench and SEED-bench typically do not feature controlled manipulation of modality-specific noise, which is a core feature of this work's synthetic datasets for studying information fusion strategies.

# Method
- **Tasks**: Four psychophysics-inspired magnitude estimation tasks are used: Marker location, Line ratio, Maze distance (all multimodal text/image), and Duration estimation (text-only, from AMI Meeting Corpus).
- **Ablations**: Controlled manipulations are performed to probe LLM behavior:
    - Steering: Providing additional textual or numerical information in the system prompt.
    - Noise: Adding constant or gradually increasing Gaussian blur to the image modality.
    - Context: Varying the length of the trial history or reversing the trial sequence.
- **Behavioural Models**: LLM responses are fitted against three main model types to infer computational strategy:
    1. Linear observer: A simple linear stimulus-response model.
    2. Static Bayesian observer: Estimation is a precision-weighted average of the stimulus and a fixed prior.
    3. Sequential Bayesian observer (Kalman filter): Estimation is updated on a trial-by-trial basis.
    - Variants with logarithmic transforms and affine output transforms are also considered.
- **Cue Combination Models**: To study multimodal integration, LLM responses are compared against three baselines:
    1. Equal weighting of text and image cues.
    2. Linear regression to find the best linear weighting.
    3. Bayes-optimal fusion, where cues are weighted by their inverse variance (reliability). Both Oracle (calibrated) and Non-Oracle (uncalibrated) versions are used.
- **Metrics**:
    - **Accuracy (NRMSE)**: Normalized Root-Mean-Squared-Error relative to a baseline predictor that always outputs the mean of the stimulus range.
    - **Efficiency (RRE)**: Relative Reduction in Error, comparing the LLM's NRMSE to a reference combiner (e.g., Bayes-optimal). RRE > 1 indicates the LLM is more efficient.
    - **Bayesian Consistency Score (BCS)**: A novel metric that sums scores (+1, -1, or 0) from five specific ablations. A score of +1 is given if the model's fitted prior weight shifts in the predicted Bayes-consistent direction (e.g., increases when noise is added). The score ranges from -15 to 15 after normalization.
- **BayesBench Score**: A composite score defined as the average of three normalized factors: Accuracy (A, from NRMSE), Efficiency (E, from RRE), and Consistency (C, from BCS).

# Experiments
- **Setup**: Each task is run in three sessions (short, medium, long stimulus ranges) with 10 trials per session, repeated 5 times. A rolling context of previous trials is provided to the LLM. Nine recent LLMs were evaluated via API, with reasoning features disabled where possible (for GPT-5 Mini, it was set to the lowest level). A human baseline was also collected. Error bars in plots represent 68% bootstrap percentile intervals from 30 rounds of bootstrapping.
- **Overall Performance**:
    - Most models performed better on text than image, except for the maze distance task. GPT-5 Mini achieved near-perfect text performance on this task.
    - Evidence for Bayesian behavior was consistently higher in the image modality.
    - The strongest models (GPT-5 Mini, Claude 3.7 Sonnet, GPT-4o) achieved low error rates, comparable to or better than humans.
    - A general trend was observed where more accurate models showed stronger evidence of Bayesian behavior.
- **Cue Combination**:
    - High accuracy did not guarantee efficient cue combination. GPT-5 Mini, despite strong NRMSE, showed poor cue combination efficiency, failing to sufficiently downweight the weaker image modality in the maze task.
    - Llama-4 Maverick's multimodal performance exceeded that of a Bayesian reliability-weighted linear combiner, suggesting it uses a more sophisticated non-linear integration strategy. A random forest model fit its behavior better than linear models.
- **Bayesian Consistency**:
    - More accurate models generally achieved higher BCS. However, some less accurate models like Gemma 3 4B and Phi 4 Multimodal still obtained decent BCS values.
- **BayesBench Summary**:
    - Llama-4 Maverick achieved the highest score (0.85), driven by strong RRE and BCS.
    - Claude 3.7 Sonnet and GPT-4o followed with scores of 0.81.
    - GPT-5 Mini scored 0.68, penalized for its low cue-combination efficiency (RRE).
    - Other scores were: Mistral 24B (0.66), Gemini 2.5 Flash Lite (0.64), Qwen2.5 VL 32B (0.60), Phi 4 Multimodal (0.51), and Gemma 3 4B it (0.43).
- **Discussion & Limitations**:
    - The results suggest that capable LLMs can develop Bayes-consistent behaviors without explicit training.
    - The dissociation between accuracy and cue-combination efficiency (e.g., GPT-5 Mini) highlights a potential weakness that accuracy-only benchmarks might miss.
    - The BCS metric provides a way to evaluate behavioral strategy even when accuracy saturates.
    - Limitations include bounded test ranges, a limited scope of ablations, and potential non-determinism from using APIs.

# Conclusion
- The paper introduces BayesBench, a psychophysics-inspired benchmark that reveals emergent, cognitive-like strategies in LLMs for magnitude estimation and multimodal cue integration.
- Key findings are that capable LLMs exhibit Bayes-consistent adaptation, but high accuracy does not guarantee efficient cue combination, indicating that Bayesian-consistent behavior may emerge naturally in sufficiently capable models.
- The work bridges human psychophysics and AI research by providing an extensible framework and diagnostic metrics.
- Future work could extend BayesBench to more naturalistic settings, investigate the mechanistic underpinnings of these behaviors, and study how they scale with model size, data, and training objectives.
- A reproducibility statement confirms the public release of BayesBench, including the data generator, prompts, model code, and evaluation scripts.

# References
This section contains a list of 40 references cited in the manuscript, covering topics from human psychophysics, Bayesian brain theory, and recent LLM and multimodal AI literature.

# Appendix
- **A.1-A.3**: Details the experimental design, API interaction protocols (temp=0.7, reasoning disabled where possible), prompt structure, and specific configurations for steering, noise, and context ablations.
- **A.4**: Lists the nine LLMs studied: Claude 3.7 Sonnet, GPT-5 Mini, GPT-4o, Llama-4 Maverick (400B total/17B active), Qwen 2.5-VL 32B, Mistral 24B, Gemini 2.5 Flash-Lite, Phi-4 Multimodal, and Gemma 3 4B.
- **A.5-A.6**: Describes the human data collection via a web platform for calibration and provides an ethics statement confirming voluntary participation and data anonymization.
- **A.7**: Provides mathematical details for the Non-oracle and Oracle Bayes cue combination models.
- **A.8**: Explains the "factor analysis" procedure for fairly comparing evidence for different behavioral models (e.g., Bayesian vs. non-Bayesian) based on AIC scores.
- **A.9-A.10**: Provides a table with a model-wise breakdown of the BCS score across the three multimodal tasks and details on how the BCS was fitted.
- **A.11-A.12**: Includes figures with detailed performance breakdowns (NRMSE, Bayes factor evidence) for all models and tasks. It specifically details GPT-5 Mini's poor cue combination on the maze task, where its near-perfect text NRMSE (0.01) and worse image NRMSE (0.2) should lead to down-weighting the image, but the model fails to do so effectively.
- **A.13**: Mentions additional model variants tested, including those with logarithmic and affine transforms, which are penalized by AIC for their extra parameters.