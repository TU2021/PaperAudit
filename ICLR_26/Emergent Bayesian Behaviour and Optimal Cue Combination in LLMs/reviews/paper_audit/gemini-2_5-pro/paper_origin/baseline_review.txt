1) Summary
The paper investigates whether Large Language Models (LLMs) implicitly develop Bayesian-like computational strategies for perceptual tasks, akin to human cognition. The authors introduce BayesBench, a benchmark suite of four psychophysics-inspired magnitude estimation tasks (length, location, distance, duration) across text and image modalities. They evaluate nine LLMs using a novel methodology that measures not only accuracy (NRMSE) and multimodal cue-combination efficiency (RRE), but also adaptive behavior through a new Bayesian Consistency Score (BCS). The BCS quantifies whether a model's behavior shifts in a Bayes-consistent manner under controlled ablations of noise and context. Key findings reveal that while more capable models tend to exhibit more Bayes-consistent behavior, high accuracy does not guarantee robust uncertainty handling. For instance, GPT-5 Mini achieves high accuracy on text tasks but fails to integrate visual cues efficiently, highlighting a dissociation between raw performance and underlying strategy.2) Strengths
*   **Novel and Rigorous Methodology:** The paper successfully translates the rigorous paradigm of human psychophysics to the study of LLMs, providing a novel and powerful framework for probing their implicit computational strategies.
    *   The four estimation tasks are well-defined, synthetic, and allow for precise, controlled manipulation of stimuli, which is a significant advantage over naturalistic benchmarks where noise is unquantifiable (Section 3.1, Figure 2).
    *   The use of systematic ablations—steering prompts, input noise, and context length—is a core strength, enabling the authors to test specific hypotheses about how models adapt to changing uncertainty (Section 3.1, Appendix A.3).
    *   The analysis is grounded in established cognitive models (Linear, Static Bayesian, Sequential Bayesian observers), providing a principled vocabulary for interpreting LLM behavior beyond simple input-output mapping (Section 3.2, Equations 1-4). This is exemplified by the clear regression-to-the-mean effects shown in Figure 1.*   **Insightful Metrics Beyond Accuracy:** The work introduces metrics that capture more nuanced aspects of model behavior than standard performance measures, leading to deeper insights.
    *   The Bayesian Consistency Score (BCS) is a key conceptual contribution. By measuring the *change* in fitted prior weights under ablation, it can detect principled adaptation even in high-accuracy models where static behavioral signatures are weak (Section 3.5). This allows for a more robust assessment of a model's strategy.
    *   The cue-combination efficiency metric (RRE) provides a normative baseline for multimodal integration, effectively revealing which models optimally weight information from different sources. This metric is crucial for uncovering the dissociation between accuracy and efficiency (Section 3.5, Figure 8 middle panel).
    *   The composite BayesBench score provides a holistic evaluation by combining accuracy, efficiency, and behavioral consistency, rewarding models that are not just accurate but also robust and adaptive (Section 3.6, Figure 9).*   **Significant and Nuanced Findings:** The results provide compelling evidence for emergent cognitive-like behaviors in LLMs and highlight critical gaps in current models.
    *   The paper demonstrates a clear "Safety Gap" where high accuracy does not imply robust strategy. The case of GPT-5 Mini, which achieves near-perfect text accuracy but poor cue-combination efficiency, is a powerful and important finding (Section 5.2, Figure 8, Appendix A.12).
    *   The discovery that Llama-4 Maverick can outperform a Bayes-optimal *linear* combiner suggests the use of more sophisticated, non-linear integration strategies, opening new avenues for research (Section 5.2, Figure 7).
    *   The general, albeit noisy, correlation observed between model accuracy and Bayesian tendencies (Bayesian Factor Evidence and BCS) supports the hypothesis that principled uncertainty handling may be an emergent property of capable models (Section 5.1, Figure 8 left and right panels).*   **High-Quality Presentation and Reproducibility:** The manuscript is exceptionally well-written, clearly structured, and supported by informative visualizations and detailed appendices.
    *   Figures are clear and effectively communicate complex results, such as the multi-faceted model comparisons in Figure 8 and the component breakdown in Figure 9.
    *   The appendices provide extensive and valuable details on the experimental design (Appendix A.1), prompts (Appendix A.2.2), ablation procedures (Appendix A.3), and analysis methods (Appendix A.8, A.10), which are essential for understanding the work's technical depth.
    *   The commitment to release the benchmark code, data generator, and evaluation scripts is commendable and will be a valuable resource for the community (Reproducibility Statement, Section 1).3) Weaknesses
*   **Ambiguity in "Bayesian Factor Evidence" Metric:** The "Bayesian Factor Evidence" is a central metric for supporting claims about Bayesian behavior (Figure 8 left panel, Appendix Figures 14-17), but its calculation and interpretation lack clarity and justification.
    *   The complex factor analysis procedure is relegated to Appendix A.8. This non-standard method, involving grouping by nuisance factors and taking maximum likelihoods within cells, is not sufficiently justified in the main text, and its statistical properties are not discussed. This makes it difficult to assess the robustness of the resulting evidence scores.
    *   The link between a high "Bayes prob" score and specific, observable behaviors is not always direct. For example, in the line ratio task (Figure 14, text modality), Llama-4 Maverick has a 96% Bayes prob with a high NRMSE of 56%, while GPT-5 Mini has a 34% Bayes prob with a much lower NRMSE of 15%. This raises questions about what the evidence score is truly capturing.
    *   The claim that "more accurate models also show stronger evidence of Bayesian behaviour" (Section 5.1) appears to overstate the evidence in the corresponding plot (Figure 8, left panel), which shows a very weak and noisy correlation with several notable outliers.*   **Potential Confounding Factors in Model Comparisons:** The study compares a diverse set of models, but several uncontrolled variables could confound the interpretation of the results as purely "emergent" behavioral strategies.
    *   The authors note that reasoning in GPT-5 Mini could not be fully disabled (Section 4, Appendix A.2.1). This is a major confound that likely explains its near-perfect performance on the text-based maze task (Section 5.1, Appendix A.12), making it difficult to compare its "perceptual" strategy directly with other models that lack this explicit reasoning step.
    *   The models under evaluation vary significantly in size, architecture, training data, and fine-tuning procedures (Table 3). While acknowledged, the paper does not systematically discuss how these factors might contribute to the observed differences in Bayesian behavior, attributing them broadly to "capability."
    *   The reliance on closed-source APIs, as acknowledged in the limitations (Section 6), introduces potential non-determinism and undocumented model updates that could affect the replicability and stability of the fine-grained model rankings presented.*   **Insufficient Justification for the BayesBench Composite Score:** The final BayesBench score is defined as a simple, unweighted average of three normalized components (Equation 5), but the rationale for this aggregation method is not provided.
    *   The equal weighting of accuracy (A), efficiency (E), and consistency (C) is an implicit value judgment that is not defended. It is unclear whether these three aspects should be considered equally important for all applications or evaluation purposes.
    *   The normalization ranges used to calculate the component scores (Table 2), such as setting NRMSE_max to 2 and BCS_min/max to +/-15, appear arbitrary. The sensitivity of the final model rankings (Figure 9) to these specific choices is not analyzed or discussed.
    *   While providing a single summary score is useful, the current formulation risks oversimplifying the rich, multi-dimensional results, potentially obscuring important trade-offs that are more clearly visible in the disaggregated plots (Figure 8).4) Suggestions for Improvement
*   **Clarify and Validate the "Bayesian Factor Evidence" Metric:**
    *   Integrate a concise summary of the factor analysis procedure (from Appendix A.8) into the main methods section (e.g., Section 3.4) and provide a clear rationale for why this specific approach was chosen over more standard model comparison techniques (e.g., AIC/BIC weights across a full model set).
    *   Strengthen the connection between the abstract "Bayes prob" score and concrete behavioral data. For example, demonstrate how the score correlates with the slope of the regression-to-the-mean effect (as in Figure 1) or the magnitude of sequential dependencies.
    *   In the results section, provide a more nuanced description of the relationship between accuracy and Bayesian evidence, explicitly acknowledging the noise and discussing potential reasons for outliers, rather than presenting it as a general trend.*   **Address and Discuss Confounding Variables More Directly:**
    *   Throughout the results and discussion, consistently qualify the findings related to GPT-5 Mini by highlighting the potential confound of its residual reasoning capabilities. This would provide a more balanced interpretation of its unique performance profile.
    *   Expand the discussion to include a more structured consideration of how known architectural or scale differences between the models might relate to their performance on BayesBench. This would add depth to the claim that these behaviors are "emergent."
    *   To improve robustness against API variability, consider reporting the variance across a few independent runs for each model and task, which would provide a better estimate of measurement uncertainty than the current bootstrapping approach that resamples from a single data collection pass.*   **Refine and Justify the BayesBench Composite Score:**
    *   Provide a clear justification for the equal weighting of the A, E, and C components in Equation 5. Alternatively, frame the composite score as one possible summary view and encourage readers to primarily consider the disaggregated components, which reveal more detail.
    *   Explain the rationale behind the chosen normalization parameters in Table 2. A sensitivity analysis showing that the overall rankings are robust to reasonable changes in these parameters would significantly strengthen the credibility of the final composite score.
    *   Consider supplementing the final bar chart (Figure 9) with a visualization that avoids aggregation, such as a radar plot showing each model's performance on the three separate axes (A, E, and C). This would better highlight the unique strengths and weaknesses of each model.5) Score
*   Overall (10): 9 — The paper presents a highly novel and rigorous methodology that yields significant insights into the implicit computational strategies of LLMs.
*   Novelty (10): 10 — The application of a psychophysics paradigm, the introduction of the BCS metric, and the focus on adaptive behavior are highly original contributions.
*   Technical Quality (10): 8 — The experimental design is excellent, but the justification for certain custom metrics (Bayesian Factor Evidence, composite score) could be stronger.
*   Clarity (10): 9 — The paper is exceptionally well-written and visualized, though moving some key methodological details from the appendix to the main text would improve it further.
*   Confidence (5): 5 — I am highly confident in my assessment, as the paper and its extensive appendices provide sufficient detail to evaluate its claims thoroughly.