Summary
- The paper investigates whether large language models (LLMs) display Bayesian-like perceptual behaviour without explicit instruction, using a psychophysics-style paradigm. It introduces BayesBench with four magnitude-estimation tasks (marker location, line ratio, maze distance, subtitle duration) and controlled ablations of steering, noise, and context (Section 3.1; Figure 2; Appendix A.3). Behavioural models include linear, static Bayesian, and sequential Bayesian (Kalman) observers (Equations 1–4; Section 3.2) and cue-combination baselines (equal, linear, Bayes-optimal; Table 1; Appendix A.7). Metrics include accuracy (NRMSE), efficiency relative to Bayesian fusion (RRE), and a Bayesian Consistency Score (BCS) capturing Bayes-consistent shifts (Section 3.5; Table 2). Across nine LLMs and a human baseline, capable models often adapt Bayes-consistently, but accuracy can dissociate from robust multimodal integration (e.g., GPT-5 Mini; Section 5.2; Appendix A.12). The paper reports cross-model patterns and a BayesBench composite score (Section 3.6; Figure 9).Strengths
- Bolded: Psychophysics-inspired, controlled evaluation framework
  - The paper translates classic magnitude-estimation paradigms to LLMs, enabling precise manipulation of uncertainty via steering, noise, and context ablations (Section 3.1; Appendix A.3; Figure 2). This matters for experimental rigor and isolating computational strategies under uncertainty.
  - Stimulus sessions with overlapping ranges and rolling context (Figure 5; Section 4) mirror psychophysics designs, supporting principled analysis of regression-to-the-mean effects (Figure 1A–B), which is a known Bayesian signature (clarity and methodological grounding).
  - Synthetic datasets allow fine-grained cue-combination analyses where modality-specific noise is quantifiable (Section 2, Multimodal studies), addressing a gap in existing benchmarks (impact on multimodal robustness research).
- Bolded: Clear behavioural modelling with explicit, interpretable equations
  - The paper specifies linear, static Bayesian, and sequential Bayesian observer models with full equations (Equations 1–4, Section 3.2), enhancing technical transparency and interpretability (technical soundness).
  - Sequential effects are probed with a Kalman filter, and examples of model fits are shown (Figures 3–4), substantiating claims about trial-by-trial adaptation (evidence-first).
  - Log-transform and affine variants are included to capture Weber-like scaling and response miscalibration (Section 3.2; Appendix A.13), reflecting attention to known psychophysical phenomena (novelty in application to LLMs).
- Bolded: Principled cue-combination baselines and efficiency metric
  - Cue-combination baselines include equal weighting, linear regression, and Bayes-optimal fusion with Oracle and Non-Oracle variants (Table 1; Appendix A.7), offering normative references grounded in linear-Gaussian assumptions (clarity and technical soundness).
  - The RRE metric compares LLM multimodal performance to Bayesian baselines (Section 3.5), quantifying efficiency beyond raw accuracy (experimental rigor).
  - Non-linear fits (random forest) better capture some models' multimodal responses than linear baselines (Figure 7), highlighting emergent non-linear strategies (impact and insight).
- Bolded: Novel Bayesian Consistency Score (BCS) for behavioural shifts
  - BCS is defined via changes in the fitted prior weight in the static Bayesian model across manipulations (Section 3.5; Table 2), enabling detection of Bayes-consistent adaptation even when accuracy saturates (novelty).
  - Ablations are carefully mapped to prior/likelihood changes (Section 3.5; Appendix A.10), making BCS interpretable and hypothesis-driven (clarity).
  - BCS provides additional separation among top models (Figure 8 right; Figure 9), revealing strategy differences beyond performance (impact).
- Bolded: Comprehensive cross-model evaluation and human calibration
  - Nine diverse LLMs are evaluated (Appendix A.4; Table 3), including both closed- and open-weight releases, increasing generality of observations (experimental breadth).
  - A human baseline is included for calibration (Appendix A.5; Figure 8 left), situating LLM behaviours relative to psychophysics expectations (contextual clarity).
  - Bootstrap uncertainty estimates are provided (Section 4), with 68% intervals (Figure 8; Figure 9), adding statistical robustness (experimental rigor).
- Bolded: Key empirical findings with diagnostic value
  - Accuracy and Bayesian tendencies correlate but can dissociate: GPT-5 Mini attains near-perfect text performance yet fails to integrate visual cues efficiently (Section 5.2; Figure 6C; Figure 8 middle; Appendix A.12), underscoring the “Safety Gap” (impact).
  - Models downweight the image modality under increased visual noise (Figure 6C), indicating Bayes-consistent adaptation (behavioural soundness).
  - Llama-4 Maverick reportedly exceeds Bayesian linear fusion (Section 5.2; Figure 7), suggesting non-linear integration strategies and pointing to design headroom (insight and future directions).
- Bolded: Reproducibility commitments and detailed appendices
  - The Reproducibility Statement promises release of generators, prompts, ablation configs, and modelling code (Conclusion; Reproducibility Statement), important for community reuse (impact).
  - Detailed factor-evidence procedure guards against variant proliferation bias (Appendix A.8), demonstrating methodological care (technical quality).
  - Prompt and API interaction details (Appendix A.2) and ethics for human data (Appendix A.6) improve transparency (clarity and integrity).Weaknesses
- Bolded: Metric reporting inconsistencies and questionable values
  - “Bayes prob (%)” exceeds 100% in multiple appendix figures/tables (e.g., Figure 16 lists 131%, 301%, 376%; Figures 14–17 also contain >100% values), conflicting with the probability definition bounded in [0,1] (Appendix A.8). This undermines technical correctness and clarity.
  - Some plots conflate or inconsistently label axes/scales (e.g., NRMSE as “%” vs the factor definition in Section 3.5 and Table 2), and Figure 9’s “NRMSE (A)” appears to plot raw NRMSE magnitudes rather than the normalised A factor from Table 2; further, Appendix Figure 16 includes duplicate/missing entries that make modality comparisons hard to interpret (Appendix A.11/A.12). This complicates interpretation and comparability (clarity issue).
  - The BayesBench breakdown in Figure 9 is approximate (“~”) and lacks exact component values, and the “Total” visually appears to sum components even though the score is defined as an average S = (A + E + C)/3 (Section 3.6, Eq. 5), reducing verifiability (Figure 9; Section 3.6).
- Bolded: BCS design relies on restrictive assumptions and coarse scoring
  - BCS uses the static Bayesian observer even when sequential models better fit responses (Section 3.2; Figures 3–4), potentially mischaracterizing behaviour under context manipulations (technical limitation).
  - BCS captures only the sign of prior-weight change and applies a hard threshold s_a=0 when w_prior>0.9 (Section 3.5), which is ad hoc and discards magnitude information, reducing sensitivity to nuanced adaptations (methodological limitation). Reported per-task BCS values are non-integers in Table 4 despite the integer definition (Section 3.5; Table 2), creating definitional ambiguity (clarity/consistency).
  - The mapping from ablations to prior/likelihood changes is assumed (Section 3.5; Appendix A.10) but not empirically validated (e.g., verifying that injected blur decreases fitted measurement precision τ_x or that longer context raises τ_p without affecting τ_x), risking construct validity (experimental rigor concern).
- Bolded: Cue-combination evaluation may mis-specify the normative baseline
  - The “Bayes-optimal” reference assumes linear-Gaussian errors (Section 3.3; Appendix A.7) without testing noise normality or linearity of error structure, so the optimality claim may not hold for LLM responses (technical soundness).
  - The Non-Oracle baseline weights modalities by empirical variance of LLM responses (Table 1), which could be biased by temperature-induced sampling noise and small sample sizes, confounding “reliability” with generation stochasticity (Appendix A.2.1; Section 4).
  - Claims that Llama-4 Maverick surpasses Bayesian fusion (Section 5.2; Figure 7) rely on in-sample fit of a random forest without cross-validation evidence; overfitting cannot be ruled out (No direct evidence found in the manuscript for CV or held-out evaluation).
- Bolded: Experimental comparability is confounded by model settings and API behaviour
  - GPT-5 Mini’s reasoning could not be disabled and was set to minimum (Section 4; Appendix A.2.1), while other models ran without reasoning, introducing a systematic confound, especially on text-heavy tasks like maze (Section 5.1; Appendix A.12).
  - Temperature was fixed at 0.7 for all models (Appendix A.2.1), potentially inflating response variance and impacting both model-fitting (σ_dec) and cue-combination weights derived from empirical variances (Section 3.3), with no temperature sensitivity analysis (technical rigor).
  - API non-determinism and silent vendor updates are acknowledged (Limitations, Section 6) but not mitigated via version pinning or multiple independent runs; discard criteria are described (“Responses that are ill-formed are discarded,” Appendix A.2.2) but no discard rates are reported. This threatens reproducibility (No direct evidence found for version auditing, multi-run stability, or discard statistics).
- Bolded: Human baseline insufficiently specified for meaningful calibration
  - The manuscript does not report participant count, demographics, or recruitment details (Appendix A.5 mentions a web platform but lacks sample size or variation characteristics), limiting interpretability of human variance and bias (clarity/rigor).
  - Only two ablations were tested in humans (“constant noise” and “longer context”; Appendix A.5), restricting the ability to compare BCS across the full experimental manipulations (experimental completeness).
  - No statistical comparison (e.g., hypothesis tests) is presented between human and model behaviours beyond plotting (Figure 8 left), leaving calibration qualitative (No direct evidence found for statistical tests).
- Bolded: Task design and dataset details are under-specified, limiting generalisation
  - Subtitle duration task: details on how ground-truth durations are derived from AMI transcripts are sparse (Section 3.1 references AMI but lacks mapping procedure), raising concerns that text-only cues may be weakly informative for duration (clarity).
  - Trial counts are modest (10 trials per session, 3 sessions, repeated 5 times; Figure 5; Section 4), which may be limited for reliable estimation of model parameters and variance, especially for sequential behaviours (experimental rigor). Notational ambiguity in the Kalman observer leaves it unclear whether y_t is generated from μ_{t|t} or μ_{t|t−1} (Section 3.2; Eqs. 3–4), which can affect fits (clarity).
  - The synthesis and bounded ranges (Conclusion, Section 7; Limitations, Section 6) constrain external validity; procedures for extending to naturalistic noise or more complex multimodal stimuli are conceptual but not instantiated (generalisation limit). Additionally, “Additional model variants” suggests affine/gain adjustments for “non-linear models” (Section 3.2), whereas Appendix A.13.2 restricts affine transforms to Bayesian models, creating ambiguity about which families receive which variants (clarity).Suggestions for Improvement
- Bolded: Correct and standardize metric reporting
  - Ensure all “Bayes prob (%)” values lie in [0,100] per the definition in Appendix A.8; re-plot Figures 14–17 and Figure 16 with validated probabilities, and correct any garbled/duplicated/missing entries (Appendix A.11/A.12; Figures 14–17).
  - Harmonize NRMSE presentation and the A/E/C factors: state whether percentages refer to raw NRMSE or the normalised A factor per Table 2; use consistent axis scaling across figures; confirm that Figure 9 “NRMSE (A)” reflects the Table 2 definition and that the “Total” follows S = (A + E + C)/3 (Section 3.5; Table 2; Section 3.6 Eq. 5; Figure 9).
  - Provide precise component values and error bars (means and 68% intervals) for NRMSE (A), Bayes-RRE (E), and BCS (C) per model in a supplementary table to complement Figure 9 (Section 3.6; Figure 9).
- Bolded: Strengthen BCS design and validation
  - Complement the sign-only BCS with an effect-size variant (e.g., normalized Δw_prior with confidence intervals) and justify or tune the 0.9 prior-dominant threshold via sensitivity analysis; clarify why Table 4 reports non-integer per-task BCS despite integer s_a, or provide the averaging/weighting procedure with per-ablation s_a counts (Section 3.5; Table 2).
  - Where sequential models fit better (Figures 3–4), compute BCS using sequential Bayesian parameters (e.g., Kalman gain changes K_t across ablations) to avoid mischaracterization by a static observer (Section 3.2).
  - Empirically validate ablation-to-parameter mappings: show that blur reduces fitted τ_x (image-only fits; Appendix A.10) and that longer context raises fitted τ_p without affecting τ_x; report parameter changes with bootstrapped CIs (Section 3.5; Appendix A.10).
- Bolded: Refine cue-combination evaluation and guard against mis-specification
  - Test linear-Gaussian assumptions by analyzing residuals for normality and heteroscedasticity; if violated, add robust baselines (e.g., generalized linear models or non-linear calibrated combiners) as references for RRE (Section 3.3; Appendix A.7).
  - Separate sampling noise from perceptual variance: repeat trials at multiple temperatures (e.g., 0.0, 0.2, 0.7) to estimate σ_dec attributable to generation stochasticity vs perceptual uncertainty before weighting modalities by empirical variance (Appendix A.2.1; Section 3.3).
  - For claims of surpassing Bayesian fusion (Figure 7; Section 5.2), report cross-validated performance (e.g., k-fold or leave-one-session-out) and compare against regularized non-linear baselines to rule out overfitting (No direct evidence found for CV).
- Bolded: Reduce experimental confounds and improve reproducibility
  - Standardize reasoning settings: when reasoning cannot be disabled (GPT-5 Mini), add matched “reasoning-on” baselines for other models or isolate maze text tasks in a separate analysis to avoid confounds (Section 4; Section 5.1; Appendix A.12).
  - Conduct temperature sensitivity experiments and multi-run repeats to characterize stochasticity; report API versioning and timestamps, and, where possible, pin model releases to avoid silent updates; additionally, report discard rates and criteria for ill-formed responses per model/task (Appendix A.2.1; Appendix A.2.2; Limitations, Section 6).
  - Add stability diagnostics (e.g., re-run subsets over days) and report variability due to API non-determinism to strengthen the reliability of reported intervals (No direct evidence found).
- Bolded: Expand and document the human baseline
  - Report participant counts, demographics, and summary statistics; include confidence intervals for human metrics and parameter estimates to substantiate calibration (Appendix A.5; Figure 8 left).
  - Include more ablations (e.g., steering and noise sequences) in the human study to enable BCS comparison across the full set of manipulations (Appendix A.3; Section 3.5).
  - Provide statistical tests comparing human vs model factor evidence and BCS (e.g., permutation tests or bootstrap comparisons) to move beyond qualitative placement (No direct evidence found for statistical tests).
- Bolded: Clarify dataset generation details and broaden task scope
  - Specify the procedure for computing subtitle ground-truth durations from AMI (e.g., segment selection, speech rates, pauses), and justify that text-only cues are informative (Section 3.1; Figure 17).
  - Increase trial counts or add longer sequences to better capture sequential effects; report power analyses and parameter recovery simulations to justify sample sizes (Figure 5; Section 4). Clarify in Section 3.2 whether y_t is generated from μ_{t|t} or μ_{t|t−1} in the Kalman observer (Eqs. 3–4).
  - Include more naturalistic noise and multimodal tasks (e.g., real images with degradations; audio-text fusion) to assess generalisation beyond synthetic identity mappings, and reconcile the description of affine/gain variants across Section 3.2 vs Appendix A.13.2 (Conclusion, Section 7; Limitations, Section 6; Section 3.2; Appendix A.13.2).Score
- Overall (10): 6 — Valuable psychophysics-inspired benchmark with explicit modelling and informative findings (Sections 3.1–3.3; Figures 2, 7–9), but several core reporting inconsistencies (e.g., probabilities >100%, Figure 16 issues; Appendix A.11/A.12; Figures 14–17) and a composite score/normalization mismatch with Eq. (5) (Section 3.6; Figure 9) temper confidence.
- Novelty (10): 8 — The BCS metric and the BayesBench framework for multimodal uncertainty probing in LLMs are distinctive (Section 3.5; Section 3.6; Figure 8 right), extending psychophysics to LLM evaluation.
- Technical Quality (10): 5 — Solid modelling and factor-evidence procedure (Equations 1–4; Appendix A.8), yet probability reporting (>100%; Figures 14–17), BCS design/definitional ambiguities (Section 3.5; Table 4), unvalidated linear-Gaussian assumptions (Section 3.3), and composite-score clarity (Section 3.6 vs Figure 9) limit rigor.
- Clarity (10): 6 — Clear task and model descriptions (Sections 3.1–3.3; Table 1; Figure 2) and informative summaries (Figures 8–9); however, inconsistent plot scales/labels and table quality (Figures 14–17; Figure 16), notation ambiguity in the Kalman observer (Section 3.2), and sparse human-study details (Appendix A.5) reduce clarity.
- Confidence (5): 4 — Based on a thorough read of the manuscript and appendices with multiple concrete anchors; remaining uncertainty stems from plotting/reporting inconsistencies and lack of some methodological validations.