Summary
The paper introduces BayesBench, a psychophysics-inspired benchmark to probe whether LLMs exhibit Bayesian behaviours in magnitude estimation and multimodal cue combination. It defines four tasks (length ratio, marker location, maze distance, subtitle duration; Sec. 3.1, Fig. 2), a suite of behavioural observer models (linear, static Bayesian, sequential Bayesian/Kalman; Sec. 3.2), cue-combination baselines (equal, linear, Bayes-optimal with oracle and non-oracle variants; Sec. 3.3, Table 1), and a composite score (Eq. 5) comprising NRMSE accuracy, relative cue-combination efficiency (RRE), and a novel Bayesian Consistency Score (BCS) that tracks Bayes-consistent shifts under ablations (Sec. 3.5–3.6, Table 2). Across nine LLMs and a small human baseline, the study finds that stronger models often show higher Bayesian evidence and Bayes-consistent adaptation, but high accuracy does not guarantee optimal cue fusion (e.g., GPT‑5 Mini; Sec. 5.2, App. A.12). Llama‑4 Maverick attains the best BayesBench score (Fig. 9).

Soundness
- The overall methodology is well-motivated: adopting psychophysics-style, black-box behavioural fits to infer strategies (Sec. 3.2) and assessing cue fusion via normative baselines (Sec. 3.3) is appropriate for probing implicit computation.
- The BCS is an interesting idea, comparing direction-of-change in fitted prior weight w_prior across controlled ablations (Sec. 3.5). The choice to focus on signs rather than magnitudes is pragmatic for heterogeneous models where absolute weights vary with accuracy.
- Factor evidence is computed via an AIC-based, best-in-cell procedure to compare Bayesian vs non-Bayesian factors while mitigating variant proliferation (App. A.8), which is thoughtful, though still sensitive to model class flexibility and AIC assumptions.
- There are notable methodological concerns:
  1) The BCS reduces complex behaviours to a sign test and applies a hard threshold s_a=0 when w_prior>0.9 (Sec. 3.5), which may discard informative magnitude and uncertainty and introduces an arbitrary cutoff; a bootstrap distribution over Δw_prior or effect sizes would be more robust.
  2) The factor comparison appears to let the Bayesian family leverage exclusive variants (e.g., SEQUENTIAL, GAIN; App. A.8) while the non-Bayesian side lacks symmetric flexibility; even with AIC penalties, this can bias “Bayesian evidence” upward.
  3) AIC for time-dependent sequential fits (Kalman; Sec. 3.2) assumes model adequacy and i.i.d. residuals; PSIS-LOO/WAIC or held-out validation would better guard against overfitting and misspecification (App. A.8).
  4) Cue-combination efficiency is evaluated without explicit cue-conflict trials; inferring weights indirectly from unimodal variability can be ambiguous compared to standard conflicting-cue paradigms (Sec. 3.3, 5.2).
  5) The API protocol discards ill-formed responses (App. A.2.2), potentially biasing results in favour of brittle models; counting such cases as errors would better reflect robustness.
  6) Temperature is set to 0.7 (App. A.2.1), introducing sampling variance; while 30× bootstrapping is used (Sec. 4), stronger control (e.g., temperature 0 and multiple seeds) would improve stability.
- Despite these issues, the central claims (accuracy–behaviour dissociation; emergent Bayes-consistency) are reasonably supported by multiple analyses (Fig. 6–9), but some inferences (e.g., “beyond human” efficiency) may overreach given baseline choices and absent conflict trials.

Presentation
- The paper is generally clear and well-structured, with useful schematics (Figs. 1–2, 5, 7–9) and explicit formulations (Eqs. 1–5; Tables 1–2).
- However, several appendix figures/tables contain evident inconsistencies that undermine interpretability: “Bayes prob (%)” exceeds 100% in multiple places (e.g., Fig. 14–16; App. A.78–A.83, A.91), contradicting the probability definition in App. A.8; some rows appear duplicated or truncated (App. Fig. 16), and axes/units mix percentages and ratios (“NRMSE (%)”) inconsistently.
- The BCS design choices (threshold 0.9, scoring range −15..15, normalization; Sec. 3.5, Table 2) would benefit from a deeper justification and an uncertainty analysis.
- Claims such as “beyond some biological systems including humans” (Sec. 5.2) should be toned down or supported with direct human comparisons on matched stimuli and analyses.

Contribution
- A valuable, reproducible framework for behavioural probing of LLMs under controlled uncertainty, bridging psychophysics and AI evaluation (Sec. 3; App. A.1–A.7).
- Introduction of BCS to detect Bayes-consistent adaptation when accuracy saturates (Sec. 3.5) is a useful diagnostic, albeit in need of statistical refinements.
- The empirical finding that accuracy does not ensure optimal cue fusion (GPT‑5 Mini; Sec. 5.2, App. A.12) is important for multimodal benchmarking.
- Public release of generators, prompts, and code (Reproducibility Statement) strengthens impact.

Strengths
- Well-motivated, psychophysics-grounded tasks with controlled ablations across noise, context, and steering (Sec. 3.1, App. A.3).
- Clear separation between capability (NRMSE, RRE) and behaviour (BCS), enabling discovery of capability–strategy dissociations (Sec. 3.5–3.6, Fig. 8–9).
- Diverse model set and inclusion of human reference (Sec. 4, Fig. 8).
- Thoughtful factor-evidence procedure and cue-combination baselines, including oracle calibration to handle biases/covariances (Sec. 3.3–3.4, App. A.7–A.8).

Weaknesses
- BCS relies on sign-only changes and an arbitrary cap on w_prior (>0.9), with no uncertainty calibration (Sec. 3.5).
- Potential bias in factor evidence by giving Bayesian family more flexible variants (SEQUENTIAL, GAIN; App. A.8), despite AIC penalties.
- Lack of explicit cue-conflict trials limits identifiability of fusion weights (Sec. 3.3, 5.2).
- Use of temperature 0.7 and discarding malformed outputs may inflate performance and reduce reproducibility (App. A.2.1–A.2.2).
- Appendix figures contain numerical inconsistencies (Bayes prob >100%) and formatting errors (App. A.78–A.83, A.91), weakening credibility.
- Some overstatements (e.g., “beyond biological systems”; Sec. 5.2) without matched human baselines on the same fusion metric.

Questions
- Can you report uncertainty for Δw_prior and BCS via bootstrap CIs and test whether sign consistency remains under resampling (Sec. 3.5)?
- How sensitive are conclusions to the 0.9 threshold in BCS and to the choice of five ablations; do alternative scoring schemes (e.g., weighted by effect size) change rankings?
- Could you add cue-conflict conditions (e.g., inconsistent text vs image) to identify fusion weights more directly?
- In App. A.8, can you provide a fairness analysis showing that allowing SEQUENTIAL/GAIN variants only for the Bayesian family does not inflate Bayes evidence?
- Please clarify the “Bayes prob (%) > 100%” plots in App. Figs. 14–16; are these odds or mislabeled values?
- How many human participants and stimuli per condition were collected (App. A.5), and how do they compare statistically to the model results?

Rating
- Overall (10): 8 — A thoughtful benchmark with novel BCS and solid experimental design, but some methodological choices (BCS definition, factor-evidence fairness) and appendix inconsistencies need fixes (Sec. 3.5, App. A.8, App. Figs. 14–16).
- Novelty (10): 8 — Psychophysics-style probing and the BCS metric for LLMs are fresh contributions (Sec. 3.1–3.6), beyond existing multimodal benchmarks (Sec. 2).
- Technical Quality (10): 7 — Strong framework and analyses, tempered by BCS thresholds, lack of conflict trials, and potential evidence bias (Sec. 3.5, 5.2, App. A.8).
- Clarity (10): 8 — Main text is clear with equations and figures (Eqs. 1–5; Figs. 1–2, 8–9), but appendix plots contain inconsistencies that must be corrected (App. Figs. 14–16).
- Confidence (5): 4 — Assessment based on detailed reading of methods and appendices with cross-checks against figures and equations; some ambiguity remains due to API non-determinism and appendix errors (Sec. 4, App. A.2.1, A.78–A.83).


Summary
This work examines whether LLMs display Bayesian behaviour in magnitude estimation and multimodal cue integration by adapting psychophysics paradigms. It defines four tasks (three multimodal, one text-only; Sec. 3.1), fits linear/static/sequential Bayesian observer models to responses (Sec. 3.2), evaluates cue-combination relative to normative baselines (Sec. 3.3), and combines accuracy (NRMSE), efficiency (RRE), and behavioural adaptation (BCS) into BayesBench (Eq. 5). Results indicate that stronger models often show higher Bayesian factor evidence and Bayes-consistent shifts, but that accuracy does not guarantee optimal fusion (GPT‑5 Mini; Sec. 5.2), while Llama‑4 Maverick shows notable multimodal gains (Fig. 9).

Soundness
- The behavioural fitting approach is standard in psychophysics and is applied carefully with multiple observer variants (Sec. 3.2).
- Cue-combination modelling via BLUE/Bayesian weighting provides a principled reference (Sec. 3.3, App. A.7).
- The composite scoring (Eq. 5, Table 2) appropriately separates capability and behavioural adaptation.
- Concerns:
  - The inference that models “behave like a Bayesian observer” could be confounded by steering prompts that explicitly mention Bayesian reasoning (App. A.3.1); although BCS also uses unbiased and noise ablations, stronger evidence should come from base conditions and context-only manipulations without such wording.
  - The factor-evidence method (App. A.8) uses AIC-transformed likelihoods and best-in-cell maxima. This lacks cross-validation and can be sensitive to model misfit, particularly for sequential residuals.
  - The reliance on unimodal variance to infer fusion weights (Sec. 3.3) without cue-conflict trials can mask strategic reweighting vs. non-linear response mappings.
  - API heterogeneity, reasoning controls that cannot be fully disabled for GPT‑5 Mini, temperature=0.7, and discarding malformed outputs (App. A.2.1–A.2.2) all threaten internal validity.

Presentation
- Main narrative is clear; figures like Fig. 1–2, 6–9 aid comprehension.
- However, appendix figures contain errors (probabilities >100%, inconsistent tables; App. A.78–A.83, A.91), and the meaning of “Bayes prob (%)” is ambiguous relative to App. A.8.
- Some claims could be tightened: “surpasses Bayesian reliability-weighted linear fusion” (Sec. 5.2) should note that baseline is linear-Gaussian; exceeding it may reflect non-linearities or calibration, not necessarily superior uncertainty estimation.

Contribution
- Provides a new, psychophysics-grounded evaluation suite and the BCS metric to probe adaptive behaviour under uncertainty (Sec. 3.5–3.6).
- Establishes an important empirical observation: capability and strategy can dissociate in multimodal fusion (Sec. 5.2).
- Offers a reproducible pipeline with generator and code to be released (Reproducibility Statement).

Strengths
- Careful task design with controlled ablations (Sec. 3.1, App. A.3).
- Clear separation between performance and behavioural adaptation (Sec. 3.5–3.6).
- Multiple models and modalities with human calibration (Sec. 4, Fig. 8).
- Useful diagnostics of model brittleness not captured by NRMSE alone (Sec. 5.2–5.4).

Weaknesses
- Possible over-interpretation of “emergent Bayesian behaviour” due to steering prompts that reference Bayesian concepts (App. A.3.1).
- Factor-evidence procedure may advantage the Bayesian family and lacks out-of-sample validation (App. A.8).
- No cue-conflict trials, limiting identifiability of modality weights (Sec. 3.3).
- Reproducibility and robustness concerns: temperature, reasoning controls, discarded outputs (App. A.2).
- Appendix numerical inconsistencies (App. Figs. 14–16).

Questions
- Can you replicate the key findings with temperature 0 and treating malformed responses as maximal-error trials?
- How do conclusions change if you remove all “be Bayesian” verbal cues and rely only on context/noise ablations for BCS?
- Could you add cue-conflict conditions and report fitted weights as a function of controlled cue disparity?
- Please clarify and correct “Bayes prob (%)” scaling; if these are odds/evidence ratios, relabel and re-express within [0,1].
- Would PSIS-LOO or k-fold validation for model comparison (App. A.8) alter factor evidence?

Rating
- Overall (10): 7 — Strong benchmark and insights, but some confounds (steering wording), selection criteria, and appendix issues reduce confidence (App. A.3.1, A.8, Figs. 14–16).
- Novelty (10): 8 — New benchmark plus BCS is original in the LLM context (Sec. 3.1–3.6).
- Technical Quality (10): 6 — Solid base methodology, but factor-evidence validation, lack of conflict trials, and analysis choices need strengthening (App. A.8, Sec. 3.3).
- Clarity (10): 7 — Clear main text, but appendix inconsistencies and mislabeled probabilities need correction (App. Figs. 14–16).
- Confidence (5): 4 — High engagement with methods and appendices; reservations stem from potential confounds and reported inconsistencies.


Summary
The authors present BayesBench, a framework for studying whether LLMs adopt Bayesian strategies in magnitude estimation and multimodal fusion. They operationalize three components—accuracy (NRMSE), efficiency relative to Bayesian fusion (RRE), and Bayesian Consistency Score tracking behavioural shifts under ablations—and report results across nine LLMs and humans (Sec. 3.5–3.6, Fig. 8–9). Findings include a correlation between accuracy and Bayesian tendencies, and a dissociation between high accuracy and poor fusion in GPT‑5 Mini (Sec. 5.2).

Soundness
- The benchmark operationalizes long-standing psychophysics insights (Sec. 2) with carefully controlled synthetic stimuli (Sec. 3.1), valid behavioural observers (Sec. 3.2), and principled fusion baselines (Sec. 3.3).
- The separation of capability (NRMSE, RRE) from strategy (BCS) is conceptually sound (Sec. 3.5–3.6), and the dissociation results are compelling (Fig. 8 middle; App. A.12).
- Nonetheless, several technical aspects could be improved:
  - BCS depends on fitting a static Bayesian model under each condition (Sec. 3.5), but fit instability can propagate into s_a signs; reporting fit uncertainties and sensitivity to model choice (static vs sequential) would strengthen claims.
  - The 0.9 cap on w_prior (Sec. 3.5) introduces a discontinuity; continuous scoring (e.g., truncated effect size) could avoid information loss.
  - AIC-based factor comparisons (App. A.8) assume comparable noise models and may favour more flexible Bayesian variants; cross-validated predictive performance would be more robust.
  - Cue-combination analysis lacks controlled disparity; in classical studies, disparity manipulations reveal weights and detect non-linear integration more cleanly (Sec. 3.3, 5.2).
  - The claim that Llama‑4 Maverick surpasses Bayesian linear fusion (Sec. 5.2) may reflect non-linear mappings or better calibration rather than principled uncertainty handling; including non-linear baselines beyond a random forest fit would help.

Presentation
- The exposition is crisp; equations and metrics are explicitly defined (Eqs. 1–5; Tables 1–2).
- Figures 6–9 are informative; however, appendix figures/tables show numerical anomalies (probabilities >100%, truncated rows; App. A.78–A.83, A.91), requiring correction.
- The limitations section candidly acknowledges API non-determinism and scope (Sec. 6, Limitations).

Contribution
- Introduces a new evaluation dimension—Bayesian-consistent adaptation—to the LLM benchmarking landscape.
- Provides a reusable pipeline and diagnostic toolkit bridging psychophysics and AI evaluation.
- Highlights practical risks: accuracy-only benchmarks can miss brittleness in uncertainty handling (Sec. 5.2, 6).

Strengths
- Carefully controlled stimuli and ablations across noise, steering, and context (Sec. 3.1; App. A.3).
- Multi-model, multimodal evaluation with human baseline (Sec. 4; Fig. 8).
- Clear operationalization of cue-combination efficiency with oracle/non-oracle models (Sec. 3.3; App. A.7).
- Insightful finding on capability–strategy dissociation (GPT‑5 Mini; Sec. 5.2, App. A.12).

Weaknesses
- BCS design choices (sign-only, cap) and lack of uncertainty quantification (Sec. 3.5).
- Potential bias in factor evidence against non-Bayesian models (App. A.8).
- No cue-conflict trials; hence fusion weights are indirectly inferred (Sec. 3.3).
- Reproducibility issues: temperature 0.7, reasoning not fully disabled for one model, discarded malformed outputs (App. A.2.1–A.2.2).
- Appendix quality issues (App. Figs. 14–16).

Questions
- Could you provide BCS sensitivity analyses (alternative thresholds, effect-size versions, bootstrap confidence intervals)?
- How are malformed outputs distributed by model and condition, and what happens if they are scored as maximal error?
- Can you include disparity manipulations to directly estimate fusion weights and test optimality?
- Would PSIS-LOO or repeated-split CV alter the factor-evidence conclusions in App. A.8?
- Can you standardize context length across models to control for varying context-window capacities?

Rating
- Overall (10): 8 — Strong, well-motivated benchmark with substantive insights; technical refinements and appendix corrections are needed (Sec. 3.5, App. A.8, Figs. 14–16).
- Novelty (10): 9 — Clear new angle on LLM evaluation via psychophysics and the BCS metric (Sec. 3.1–3.6).
- Technical Quality (10): 7 — Solid analyses with some methodological gaps (AIC reliance, no conflict trials, BCS design; Sec. 3.3, 3.5, App. A.8).
- Clarity (10): 8 — Clear main text; appendix errors should be fixed (App. A.78–A.83, A.91).
- Confidence (5): 4 — High engagement with technical details and figures; some uncertainty due to reported inconsistencies and API variability.


Summary
The paper proposes BayesBench to examine emergent Bayesian behaviour and optimal cue combination in LLMs through controlled psychophysics-style tasks. It defines tasks, observer models, cue-combination baselines, a Bayesian Consistency Score (BCS), and a composite metric (Eq. 5), and evaluates nine LLMs plus human data. Key findings include that accuracy correlates with Bayesian tendencies but does not ensure efficient cue fusion; GPT‑5 Mini performs well on text yet underweights down-weighting of noisy vision (Sec. 5.2; App. A.12), while Llama‑4 Maverick excels overall (Fig. 9).

Soundness
- The behavioural-probing paradigm is appropriate, and the modelling choices cover key hypotheses (linear vs Bayesian, sequential updates; Sec. 3.2).
- The composite evaluation balances performance and behaviour (Sec. 3.5–3.6).
- Threats to validity:
  - Discarding malformed outputs (App. A.2.2) and using temperature 0.7 (App. A.2.1) may inflate performance and reduce comparability; stronger controls or reporting of failure rates are warranted.
  - The factor-evidence framework (App. A.8) may give Bayesian models extra flexibility (SEQUENTIAL/GAIN), biasing evidence even with AIC penalties.
  - BCS is sign-based with an arbitrary ceiling, and it ignores uncertainty in the fitted w_prior; a hierarchical Bayesian fit with credible intervals or a permutation test on Δw_prior signs would be more principled.
  - Absent cue-conflict trials limit the identifiability of weights and may conflate non-linear calibrations with principled uncertainty weighting (Sec. 3.3, 5.2).
  - The claim of “outperforming Bayesian reliability-weighted fusion” (Sec. 5.2) should be interpreted carefully as outperforming a linear baseline under certain assumptions, not necessarily achieving superior uncertainty handling.

Presentation
- The paper is readable, with good figures and explicit equations (Eqs. 1–5; Figs. 1–2, 8–9).
- Several appendix artifacts undermine clarity: “Bayes prob (%)” exceeds 100% (App. Figs. 14–16), and some tables seem malformed; these should be corrected to align with App. A.8 where probabilities are bounded.

Contribution
- Introduces a new benchmark probing uncertainty handling, plus a behavioural consistency metric that complements accuracy-based evaluation.
- Provides a reproducible pipeline and code release plan.
- Offers actionable insights for the design and evaluation of multimodal LLMs.

Strengths
- Psychophysics-inspired design with controlled ablations (Sec. 3.1; App. A.3).
- Separation of capability and strategy, enabling nuanced interpretation (Sec. 3.5–3.6).
- Diverse model coverage and human comparison (Sec. 4; Fig. 8).
- Careful discussion of limitations and context-dependent behaviours (Sec. 6, Limitations).

Weaknesses
- BCS design choices and lack of uncertainty treatment (Sec. 3.5).
- Factor-evidence comparison may be biased by asymmetric flexibility (App. A.8).
- No cue-conflict; reliance on unimodal variances (Sec. 3.3).
- Reproducibility threats from API temperature and output filtering (App. A.2.1–A.2.2).
- Appendix inconsistencies (App. Figs. 14–16).

Questions
- Please report the rate of discarded responses by model and condition; how do results change if counted as errors?
- Can you replicate findings with temperature 0 and multiple seeds?
- Could you provide a BCS variant with effect sizes and bootstrap CIs, and a sensitivity study for the 0.9 threshold?
- Add cue-conflict experiments and analyze fusion weights as a function of disparity and noise.
- Clarify and fix the “Bayes prob (%)” plots to be probabilities in [0,1] or relabel as odds/evidence.

Rating
- Overall (10): 7 — Valuable benchmark and insights with some methodological and presentation issues that should be addressed (Sec. 3.5, App. A.8, App. Figs. 14–16).
- Novelty (10): 8 — New behavioural metric and psychophysics framing for LLMs (Sec. 3.1–3.6).
- Technical Quality (10): 6 — Solid base but needs stronger validation and conflict trials (Sec. 3.3, App. A.8).
- Clarity (10): 8 — Clear main text; appendix inconsistencies reduce clarity (App. A.78–A.83, A.91).
- Confidence (5): 4 — Confidence is good based on detailed cross-checking; caveats stem from inconsistent appendices and API-related variability.