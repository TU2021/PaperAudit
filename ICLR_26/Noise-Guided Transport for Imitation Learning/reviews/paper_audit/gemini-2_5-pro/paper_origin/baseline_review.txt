1) Summary
This paper introduces Noise-Guided Transport (NGT), a lightweight, off-policy imitation learning method designed for the low-data regime. NGT formulates reward learning as an optimal transport problem solved through an adversarial training scheme. The core mechanism involves training a predictor network to match the outputs of a frozen, randomly initialized prior network on expert data, while simultaneously maximizing the mismatch on agent-generated data. The authors show this objective is equivalent to maximizing the Earth Mover's Distance between the expert and agent distributions. The method leverages distributional losses to successfully tackle high-dimensional continuous control tasks like Humanoid locomotion from as few as 20 expert transitions. Experimental results demonstrate that NGT outperforms a range of baselines in sample-efficiency and performance, particularly in challenging, data-scarce scenarios.2) Strengths
*   **Novel and Theoretically Grounded Method**
    *   The core idea of learning a reward function by adversarially predicting the outputs of a frozen random network is a novel formulation for imitation learning (Section 4.1). This approach elegantly sidesteps the typical binary classification setup of GAIL-style methods.
    *   The paper provides a strong theoretical connection between the proposed adversarial objective and optimal transport, showing that minimizing the objective is equivalent to maximizing the Earth Mover's Distance (EMD) between the agent and expert distributions (Section 4.2, Equation 3).
    *   The theoretical analysis is thorough, including a concentration bound for the empirical loss objective (Appendix F, Theorem F.2) and a detailed analysis of the Lipschitz properties of the potential function and its components (Section 4.3, Theorem 4.1, Theorem 4.2).
    *   The application and analysis of distributional losses (`ℓ_HLG`) for reward learning in this context is a novel contribution that proves critical for solving complex tasks (Section 4.3.2).*   **Exceptional Empirical Performance in Low-Data Regimes**
    *   NGT demonstrates state-of-the-art performance across a suite of continuous control tasks, consistently outperforming strong baselines in the low-data setting (Figure 1, Figure 2).
    *   The method's success on the high-dimensional Humanoid-v4 task with extremely limited data (e.g., 1 demonstration, corresponding to 50 transitions) is particularly impressive, as most baselines fail or perform poorly in this setting (Figure 2, Humanoid column).
    *   NGT shows stable and graceful scaling with the amount of expert data, from as few as 20 transitions (subr50, dems01) up to several hundred (Figure 3). This highlights its robustness to data scarcity.
    *   The method's strong performance extends to more challenging DeepMind Control Suite environments, which feature greater initial state stochasticity, further validating its generalization capabilities (Appendix O, Figure 9).*   **Comprehensive and Rigorous Experimental Evaluation**
    *   The paper compares NGT against a wide and relevant set of baselines, including classic methods (BC), OT-based methods (PWIL, W-DAC/SAM), GAIL variants (DAC/SAM), and recent diffusion-based approaches (DiffAIL) (Section 5).
    *   The authors provide extensive ablation studies that convincingly justify key design choices. These studies demonstrate the necessity of the distributional loss for Humanoid (Appendix N, Figure 4), the importance of spectral normalization (Appendix N, Figure 7), the benefit of orthogonal initialization (Appendix N, Figure 6), and the impact of embedding dimensionality (Appendix N, Figure 8).
    *   The experimental setup is well-controlled, with all methods built on a shared SAC backbone to ensure fair comparison (Section 5). The evaluation protocol is sound, using multiple random seeds and a robust evaluation scheme to prevent memorization.*   **Practicality and Efficiency**
    *   NGT is presented as a lightweight method that does not require large-scale pretraining or specialized architectures (Abstract).
    *   A significant practical advantage is that NGT achieves stability using only spectral normalization, avoiding the need for a computationally expensive gradient penalty, which is required by other adversarial IL methods (Section 4.3.1, Appendix N Figure 7).
    *   The method demonstrates competitive computational speed, performing on par with or faster than several key baselines on high-dimensional tasks (Appendix P, Table 3).3) Weaknesses
*   **Reliance on a Complex Loss for Challenging Tasks**
    *   The paper's strongest results on the Humanoid task are critically dependent on the use of a distributional histogram loss (`ℓ_HLG`) (Section 4.3.2). Standard losses like MSE fail completely on this task (Appendix N, Figure 4).
    *   The `ℓ_HLG` loss introduces several new, potentially sensitive hyperparameters: the support interval `[a, b]`, the number of bins `N`, and the Gaussian spread `σ` (Section 4.3.2). This complexity seems to contradict the claim that the method is "easy to implement and tune" (Abstract).
    *   The paper notes that these hyperparameters must be adapted to the environment to achieve optimal results on non-Humanoid tasks (Appendix N, Figure 5), suggesting a non-trivial tuning effort may be required in practice.*   **Lack of Clarity on Hyperparameter Selection and Sensitivity**
    *   While a default hyperparameter table is provided (Appendix N, Table 1), the process for selecting these values, particularly for the crucial `ℓ_HLG` loss, is not detailed. It is unclear if the default values were used for all experiments or if task-specific tuning was performed for the main results in Figure 2.
    *   The paper lacks a sensitivity analysis for the `ℓ_HLG` hyperparameters (`N`, `σ`, etc.). Understanding how performance varies with these parameters is important for assessing the method's robustness and reproducibility.
    *   The main text does not explicitly state which loss function (e.g., Huber or `ℓ_HLG`) was used for the non-Humanoid results presented in Figure 2, leaving the reader to infer this from the appendix (Appendix N, Section "For the Humanoid...").*   **Limited Exploration of the Prior Network Design**
    *   The frozen prior network `f_ξ^†` is a cornerstone of the method, as it generates the target "noise" for the predictor (Section 4.1). However, the design space for this network is not explored in depth.
    *   The paper justifies the use of Orthogonal Initialization (Section 4.3.1) and ablates against Kaiming initialization (Appendix N, Figure 6), but other architectural choices (e.g., network depth/width, activation functions, or alternative random initializations) are not discussed.
    *   The quality and structure of the random mapping provided by the prior network seem fundamental to the learning signal, but the paper does not provide intuition or analysis on what constitutes a "good" prior.*   **Key Ablation Results are Relegated to the Appendix**
    *   The ablation studies in Appendix N provide critical evidence supporting the method's design, but they are not mentioned or summarized in the main paper.
    *   The finding that `ℓ_HLG` is essential for Humanoid (Appendix N, Figure 4) and that spectral normalization is critical for stability (Appendix N, Figure 7) are fundamental to understanding why NGT works.
    *   Placing these crucial results entirely in the appendix weakens the self-containedness of the main paper and may cause readers to miss the full justification for the method's components.4) Suggestions for Improvement
*   **Acknowledge and Discuss the Role of the Distributional Loss**
    *   In the main text (e.g., Section 5), explicitly state that the `ℓ_HLG` loss was essential for the Humanoid results and discuss the trade-off between its performance benefits and the introduction of new hyperparameters.
    *   Briefly discuss the role of the `ℓ_HLG` hyperparameters (`N`, `σ`, etc.) and provide intuition for their tuning, perhaps linking them to task complexity as suggested in Appendix N.
    *   To strengthen the "easy to tune" claim, it would be beneficial to show that a reasonable default setting for `ℓ_HLG` works well across a range of tasks, or provide a simple heuristic for setting its parameters.*   **Improve Transparency of Hyperparameter Settings**
    *   In the main experimental section (Section 5), clarify the hyperparameter settings used to generate the main results. For instance, add a sentence stating which loss was used for each family of environments (e.g., "We used the Huber loss for all tasks except Humanoid-v4, for which the `ℓ_HLG` loss was necessary...").
    *   Include a small sensitivity analysis for the most critical `ℓ_HLG` hyperparameter (e.g., `σ` or `N`) on one task to demonstrate its impact on performance. This could be added to Appendix N.
    *   Ensure that the caption of Table 1 in Appendix N clearly states which experiments the listed "Default" values apply to.*   **Expand on the Prior Network's Role**
    *   Add a short discussion in Section 4.1 or 4.3 about the design of the prior network `f_ξ^†`.
    *   Acknowledge that the architecture and initialization of the prior are important design choices and briefly motivate the current approach beyond the ablation results.
    *   Suggesting that the exploration of more structured or learned priors could be an avenue for future work would add valuable context.*   **Integrate Key Ablation Findings into the Main Paper**
    *   Incorporate a concise summary of the most critical ablation results into the main experimental section (Section 5).
    *   For example, a brief paragraph discussing the findings from Appendix N, Figures 4 and 7 would significantly strengthen the paper's narrative by immediately justifying the use of `ℓ_HLG` and spectral normalization.
    *   Alternatively, a small, well-chosen figure in the main text could summarize 2-3 key ablations (e.g., `ℓ_HLG` vs. MSE on Humanoid, with vs. without SN) to make the paper more self-contained.5) Score
- Overall (10): 8 — The paper presents a novel, well-grounded, and empirically powerful method for low-data imitation learning, with particularly impressive results on challenging tasks (Figure 2).
- Novelty (10): 9 — The formulation of adversarial reward learning as predicting random priors within an optimal transport framework is highly novel and insightful (Section 4.1, 4.2).
- Technical Quality (10): 9 — The theoretical analysis is sound and the experimental evaluation is extensive, rigorous, and includes strong baselines and thorough ablations (Section 5, Appendix F, N).
- Clarity (10): 7 — The paper is generally well-written, but its self-containedness is hampered by relegating critical experimental details and all ablation studies to the appendix (Appendix N).
- Confidence (5): 5 — I am highly confident in my assessment, as the paper provides detailed methodology, theory, and extensive empirical evidence.