# Global Summary
This paper introduces Noise-Guided Transport (NGT), a lightweight, off-policy imitation learning (IL) method designed for the low-data regime where expert demonstrations are scarce. The core problem NGT addresses is the failure of methods like behavioral cloning to generalize from limited data. NGT formulates IL as an optimal transport (OT) problem solved via adversarial training. Its reward learning objective is derived from pseudo-density estimation using random priors, where a predictor network learns to match the outputs of a frozen, randomly initialized prior network on expert data, while being pushed away from them on agent-generated data. The paper shows this objective is equivalent to maximizing the Earth Mover's Distance (EMD) between the agent and expert distributions. NGT is evaluated on continuous control tasks from Gymnasium and DeepMind Control Suite, including the high-dimensional Humanoid task. It is compared against baselines like DAC/SAM, W-DAC/SAM, PWIL, and a diffusion-based method, DiffAIL. Key findings show that NGT achieves strong performance, often outperforming all baselines, and can learn complex tasks like humanoid locomotion from as few as 20 state-action transitions. A crucial component for this success is the use of a distributional histogram loss for reward learning. The method does not require pretraining, specialized architectures, or gradient penalty regularization, making it computationally efficient.

# Abstract
The paper addresses imitation learning in a low-data setting with limited expert demonstrations. It introduces Noise-Guided Transport (NGT), a lightweight, off-policy method that frames imitation as an optimal transport problem solved through adversarial training. NGT does not require pretraining or specialized architectures, includes uncertainty estimation by design, and is simple to implement. The method demonstrates strong performance on challenging continuous control tasks, including high-dimensional Humanoid tasks, in ultra-low data regimes with as few as 20 transitions.

# Introduction
- The paper targets imitation learning (IL) in low-data regimes, where methods like behavioral cloning (BC) fail due to compounding errors. This is relevant for applications like healthcare where data is scarce.
- The work builds on off-policy adversarial IL methods like DAC and SAM, which are evolutions of GAIL.
- Instead of starting with a divergence, the authors derive their method, Noise-Guided Transport (NGT), from the first principle of creating a reward function that distinguishes expert from agent behavior.
- The reward learning is formulated as a pseudo-density estimation problem using random priors.
- The resulting learning objective is shown to be equivalent to an earth-mover distance (EMD), a metric from optimal transport (OT) theory.
- NGT is empirically shown to be sample-efficient, scaling well with task complexity and data scarcity. It successfully learns humanoid locomotion from as few as 20 transitions, a task where only a diffusion-based baseline makes some progress.
- The use of distributional losses in reward learning is highlighted as a key factor for NGT's robustness.
- Figure 1 shows NGT achieving a higher aggregated normalized return (~0.9) compared to the best of DAC/SAM (~0.8) and W-DAC/SAM (~0.4) across tasks.

# Related Work
- NGT is positioned within the context of adversarial IL methods. It is compared to DAC and SAM, which minimize JS-divergence.
- It is framed from an optimal transport (OT) perspective, using the dual formulation. This contrasts with PWIL (primal EMD) and ROT/MAAD (Sinkhorn algorithm).
- The use of random priors for pseudo-density estimation connects NGT to Random Network Distillation (RND) and Random Expert Distillation (RED).
- The paper notes that while RND was reported as ineffective for anti-exploration in some offline RL work, later findings showed it can work with specific asymmetric architectures. NGT demonstrates that random priors can be effective for continuous control without such feature engineering.
- The authors contrast NGT's inner task of predicting random priors with the boosting approach of AILBoost.

# Preliminaries
- The problem is set within a standard Markov Decision Process (MDP) framework (S, A, P, r, γ) in an episodic setting.
- The agent's goal is to learn a policy π that mimics an expert's behavior using a demonstration dataset E, without access to the environment's reward function r.
- The paper also considers the state-only setting where expert actions are unavailable.

# Method
- NGT's architecture consists of an actor-critic agent based on Soft Actor-Critic (SAC) and a learned reward model r_ξ.
- **Reward Learning Objective**: The reward model is trained adversarially. It uses a frozen `prior network` f_ξ^† and a trainable `predictor network` f_ξ.
- The core task is to minimize a pairing loss ℓ(f_ξ(x), f_ξ^†(x)) on expert data and maximize it on agent data.
- The adversarial loss is L(ξ) = E_expert[h_ξ(x)] - E_agent[h_ξ(x)], where h_ξ(x) = ℓ(f_ξ(x), f_ξ^†(x)) is called the potential function.
- The reward is defined as r_ξ(x) = exp(-h_ξ(x)), which is positive and bounded between 0 and 1.
- **Theoretical Grounding**: The paper shows that minimizing L(ξ) over 1-Lipschitz potential functions h_ξ is equivalent to maximizing the Earth Mover's Distance (EMD) between the agent and expert distributions: inf L(ξ) = -EMD(P_agent, P_expert). This grounds NGT in optimal transport theory.
- A concentration bound is derived, showing the empirical estimate of L(ξ) converges to its true value at an exponential rate.
- **Practical Execution**: To enforce the 1-Lipschitz property on h_ξ, the paper analyzes its composite Lipschitz constant: Λ(h_ξ) = Λ(ℓ)(Λ(f_ξ) + Λ(f_ξ^†)).
- Spectral Normalization (SN) is applied to all linear layers of the predictor and prior networks to control their Lipschitz constants. Orthogonal Initialization (OI) is used for the prior network.
- A key claim is that NGT does not require a gradient penalty (GP), unlike other SOTA off-policy adversarial IL methods, making it computationally cheaper.
- **Loss Function ℓ**: The Huber loss is used as a default. For the challenging Humanoid tasks, a distributional loss called the histogram loss (Gaussian type, ℓ_HLG) is employed.
- The paper provides a theoretical analysis of ℓ_HLG, deriving a bound on its Lipschitz constant: Λ ≤ √(1 + (C/σ)²), which guides hyperparameter tuning for stability.

# Experiments
- **Setup**: Experiments are run on the Gymnasium continuous control suite (Ant, HalfCheetah, Humanoid, etc.) and DeepMind Control Suite. The low-data regime is tested with 1, 4, and 11 expert demonstrations, subsampled at a rate of 20 (e.g., 1 demo provides 50 transitions). Each experiment uses 4 random seeds.
- **Baselines**: BC, PWIL, DAC/SAM, W-DAC/SAM, MMD-DAC/SAM, RED*, and DiffAIL. All methods are re-implemented on a shared SAC backbone for fair comparison.
- **Results**:
    - NGT consistently achieves expert-level performance and outperforms baselines across most environments and data settings (Figure 2).
    - On Humanoid-v4, NGT is highly successful, while most baselines fail. DiffAIL shows some progress but is suboptimal compared to NGT.
    - Figure 3 shows NGT's performance on Humanoid-v4 across different numbers of demonstrations (1, 4, 11) and subsampling rates (10, 20, 50). It learns effectively even with as few as 20 transitions (1 demo, subr50). It also performs well in the state-state setting.
    - The paper claims NGT does not require per-task tuning, unlike some baselines.
- **Computational Performance**: All methods were sped up by ~3x using CUDA Graphs. A humanoid run takes ~5 hours on an NVIDIA RTX 4090. NGT is shown to be computationally efficient, avoiding the expensive gradient penalty regularization needed by DAC/SAM baselines. Appendix P shows NGT's speed is competitive with the fastest baselines.

# Conclusion
- The paper presents Noise-Guided Transport (NGT), a sample-efficient IL method for low-data regimes.
- NGT's reward learning objective is based on predicting random priors and is theoretically grounded in optimal transport.
- By using distributional losses, NGT successfully learns complex humanoid gaits from as few as 20 transitions, even without expert actions.
- It outperforms all baselines and does not require gradient penalization.
- A potential future direction is applying the NGT objective to general generative modeling tasks.

# References
This section lists 66 references, citing foundational and recent work in imitation learning, reinforcement learning, optimal transport, and deep learning techniques like spectral normalization and distributional losses.

# Appendix
- **A Algorithm**: Provides the pseudocode for the NGT algorithm, detailing the interlaced training loops for the reward model and the SAC actor-critic.
- **B & C HL-Gaussian Loss**: Explains the mechanism of the histogram loss (ℓ_HLG), its extension to m-dimensional vector targets, and provides a PyTorch code snippet of the implementation.
- **D Related Works (Expansion)**: Provides further discussion on connections to contrastive learning, OT concentration bounds, Lipschitz continuity, and RLHF.
- **E WGAN**: Differentiates NGT's potential function from the WGAN critic, noting NGT's potential is non-negative and anchored by the prior network, which aids stability.
- **F Concentration of Empirical Objective**: Contains the formal theorem and proof for the concentration bound of the empirical loss L̂(ξ), showing it converges exponentially to the true loss L(ξ).
- **G & H Lipschitz Continuity Proofs**: Provides proofs for the Lipschitz constant of the potential function h_ξ (Theorem 4.1) and the histogram loss ℓ_HLG (Theorem 4.2).
- **I Network Architecture**: Details the network architectures: 2 hidden layers of 256 units for actor, critic, and reward models. ReLU for actor/critic, LeakyReLU for reward. Orthogonal initialization and spectral normalization are used.
- **J Reward Numerics**: Describes a percentile-based reward scaling scheme (dividing by Perc_0.95 - Perc_0.05) to standardize rewards and avoid manual temperature tuning.
- **K CUDA Graphs**: Mentions the use of CUDA Graphs for a ~3x speedup in all experiments.
- **L Implementation Details**: States that all methods share a common actor-critic backbone. The Adam optimizer is used. Modifications to the official DiffAIL implementation were made to ensure numerical stability.
- **M Main Environments**: A table lists the state and action dimensions for the Gymnasium environments, with Humanoid-v4 having the largest space (376 state, 17 action dims).
- **N Ablation Studies**:
    - The histogram loss (ℓ_HLG) is shown to be essential for Humanoid, while an MSE Softmax loss fails.
    - Orthogonal initialization is critical for stability compared to Kaiming initialization.
    - Removing spectral normalization leads to training collapse.
    - Reward model performance is robust to output embedding sizes of 16, 32, and 64, but degrades significantly at size 8.
- **O Extra Environments**: NGT is tested on DeepMind Control Suite tasks, showing strong generalization and stability in environments with more stochastic initial states, where baselines struggle.
- **P Speeds**: A table compares the steps per second (sps) of all methods. NGT is competitive (712 sps on Humanoid) with the fastest baselines like W-DAC/SAM (763 sps) and faster than PWIL (631 sps) and DiffAIL (659 sps).