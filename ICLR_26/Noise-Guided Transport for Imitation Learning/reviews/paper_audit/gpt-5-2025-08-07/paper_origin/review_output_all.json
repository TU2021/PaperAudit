{
  "baseline_review": "Summary\n- The paper proposes Noise-Guided Transport (NGT), an off-policy imitation learning method for low-data regimes that learns a reward model by predicting random priors and optimizing an adversarial objective formalized as the difference of potential-function expectations between expert and agent (Eq. 2, Section 4.1). The potential hÎ¾ is built from a predictor fÎ¾ and a frozen, randomly initialized prior fÎ¾â€ , with pairing loss â„“. The authors assert that minimizing the objective over 1â€‘Lipschitz potentials is equivalent (up to sign) to the Kantorovichâ€“Rubinstein dual of the earth moverâ€™s distance (Eq. 3, Section 4.2), and provide a Lipschitz composition bound (Th. 4.1, Section 4.3) and a new Lipschitz bound for the histogram Gaussian loss (Th. 4.2, Section 4.3.3). They also present an empirical concentration bound for the reward loss (Th. F.2, Appendix F). Experiments on Gymnasium MuJoCo and DMC tasks (Figures 1â€“3, 9) show strong performance, including Humanoid-v4 under ultra-low data, with ablations supporting key design choices (Appendix N). Code is released.Strengths\n- Bold re-formulation of reward learning via random prior prediction\n  - The core objective L(Î¾) uses a potential hÎ¾(x)=â„“(fÎ¾(x), fÎ¾â€ (x)) with descent on expert data and ascent on agent data (Eq. 2, Section 4.1), offering a distinct adversarial signal compared to binary discriminators. This is novel relative to typical GAN-style IL (Section 1, Section 2).\n  - The method explicitly leverages pseudo-density estimation from random priors (Section 4.1), connecting to RND/RED but turning one-class signals into a principled two-sided objective; this can reduce ambiguity and avoid the limitations of positive-only signals (Section 4.1, discussion).\n  - Impact: Provides a new, lightweight and easily implementable mechanism for reward learning in off-policy IL, with potential to broaden applicability in low-data regimes (Abstract; Section 5).- Optimal transport perspective with concrete regularity analysis\n  - The dual-OT link is stated via Eq. 3 (Section 4.2), situating NGT within W1-based transport and connecting the learned potential to Kantorovich potentials; this grounds the objective in a well-studied metric space.\n  - The Lipschitz composition theorem (Th. 4.1, Section 4.3) offers a clear, modular way to control Î›(hÎ¾) via â„“, fÎ¾, and fÎ¾â€ â€”useful for operationalizing the 1â€‘Lipschitz requirement and diagnosing stability; technical soundness improves clarity.\n  - The new bound for the histogram Gaussian loss, Î› â‰¤ âˆš(1+(C/Ïƒ)^2), (Th. 4.2, Section 4.3.3) relates stability to hyperparameters (N, Ïƒ, [a,b]), informing robust configuration (Section 4.3.2, 4.3.3). Novelty: adapted derivation with input-based Lipschitz analysis.- Strong empirical performance in low-data and high-dimensional tasks\n  - Across Ant/HalfCheetah/Pusher/Walker2d, NGT generally matches or exceeds baselines (Figure 2; Figure 1 aggregated view). This is impactful in sample-efficiency contexts (Section 5).\n  - On Humanoid-v4, NGT achieves near-expert performance with as few as 1 demonstration (50 transitions post-subsampling at 20) and shows scaling over demos/subsampling (Figure 3; Section 5). This is notable given high dimensions (Appendix M; State 376, Action 17).\n  - Robustness to observation availability: Results include state-only/state-state settings (Figure 3 rows 1â€“2, Section 5), indicating broader applicability when actions are missing.- Careful, practical design for stability and efficiency\n  - Spectral normalization on reward networks and orthogonal initialization of the frozen prior (Section 4.3.1; Appendix I, J) are motivated by Lipschitz control and feature spread; reduced need for gradient penalty (Section 4.3.1; Section 5) lowers computational costâ€”relevant for practitioners.\n  - Adoption of distributional losses (â„“HLG) specifically enabled Humanoid success, with detailed mechanism and code (Section 4.3.2; Appendix Bâ€“C). Ablations demonstrate necessity on Humanoid and generalization to other tasks (Figure 4â€“5, Appendix N).\n  - CUDA Graphs and a unified SAC backbone contributed to speed and fairness (Section 5; Appendix K, L, P). Runtime comparisons show NGT competitive with fastest baselines (Appendix P, Table 3).- Theoretical concentration guarantee for empirical reward loss\n  - The bound in Appendix F (Th. F.2; Cor. F.3) shows exponential concentration in n with dependence on diam(ğ•) and Î›(h), supporting reliability of empirical estimates of L(Î¾). While generic, it provides useful control over deviation in finite samples, aligning with the low-data emphasis (Appendix F).- Clarity and reproducibility\n  - Algorithm 1 makes the training loop explicit and shows how rÎ¾ integrates with SAC (Appendix A), improving clarity of the overall pipeline.\n  - Hyperparameter tables and architecture details (Appendix I, L, Table 1) plus public code link (Section 4.3.3; Section 5; Abstract) bolster reproducibility and impact.Weaknesses\n- OT objective/sign conventions and policy-reward linkage are under-specified\n  - Eq. 3 (Section 4.2) implies minimizing L(Î¾) over 1â€‘Lipschitz h maximizes EMD(Pagent, Pexpert), i.e., increases discrepancies; yet Section 4.2 states â€œenables the agent to provably close the gap with the expert,â€ without a formal policy-side argument showing that using rÎ¾=exp(âˆ’hÎ¾) and SAC will minimize EMD. This gap matters for technical soundness.\n  - The minimax structure typical of OT/WGAN (max over critic, min over generator) is not explicitly formulated for the policy; Algorithm 1 (Appendix A) shows reward learning and policy updates but lacks a theorem connecting policy improvement to transport objectives or monotone convergence. This limits theoretical completeness.\n  - No direct proof or proposition shows that maximizing EMD via hÎ¾, combined with rÎ¾=exp(âˆ’hÎ¾), induces gradient directions in SAC that move Pagent toward Pexpert (Section 4.1â€“4.2). Impact: leaves the core claim partially unsupported.- 1â€‘Lipschitz requirement is only heuristically enforced\n  - The OT equivalence (Eq. 3) requires hÎ¾ âˆˆ H^1; but Section 4.3.1 states â€œwe do not aim for the perfect 1â€‘Lipschitz constant â€¦ keep it â€˜close enough to 1â€™,â€ relying on spectral normalization and â€œreasonably linear-likeâ€ activations. No empirical measurement of Î›(hÎ¾) or bound aggregation over layers is reported; this weakens the rigor of the OT claim.\n  - While Th. 4.1 (Section 4.3) and Th. 4.2 (Section 4.3.3) bound â„“â€™s contribution, the effective Î›(fÎ¾) and Î›(fÎ¾â€ ) under LeakyReLU and SN are not quantified for the full network compositions; Appendix I uses LeakyReLU with leak 0.05, but no net Lipschitz computation confirms Î›(hÎ¾)â‰¤1. Technical soundness is affected.\n  - The concentration bound (Appendix F) assumes Î›(h) known and bounded (Assumption F.1), yet the paper does not verify or estimate Î›(h) under the implemented choices. This reduces the practical applicability of the bound.- Experimental fairness and tuning asymmetry\n  - Section 5 states â€œwe did not carry out per-task tuning for any method,â€ but Appendix N reports that â„“HLG hyperparameters (N, [a,b], Ïƒ) were adapted per environment, and Table 1 shows distinct Ïƒ for agent/expert sides; baselines appear with fixed GP coefficients and limited tuning (Section 5). This asymmetry may advantage NGT.\n  - DiffAIL required implementation modifications (Appendix L), including activation changes and numerical epsilons; while necessary for stability, this deviates from the original and may alter performance comparisons. Fairness is impacted.\n  - Reward normalization via percentile scaling is applied to NGT (Appendix J) and RED* but not to other adversarial baselines (Section 5), potentially affecting comparability of rewards fed to SAC and thereby learning dynamics.- Incomplete reporting and statistical analysis\n  - Results are primarily curve-based (Figures 1â€“3, 9) with shaded confidence but no final tables of meanÂ±std returns, no significance tests, and limited aggregate metrics; harder to quantitatively compare across seeds and tasks (Section 5).\n  - The â€œas few as 20 transitionsâ€ claim is supported in Humanoid analyses (Figure 3 with subsampling offsets, Section 5), but broader multi-task evidence for exactly 20 transitions is limited; generality of the ultra-low regime claim remains narrow.\n  - State-only results are shown for Humanoid (Figure 3, second row), but similar analyses for other environments are not reported; the scope of the state-only setting remains unclear (Section 5).Suggestions for Improvement\n- Clarify OT objective and formalize policy-side convergence\n  - Provide a minimax formulation explicitly showing max over hÎ¾ âˆˆ H^1 of Eagent[h]âˆ’Eexpert[h] (critic) and a corresponding policy objective that provably reduces W1(Pagent, Pexpert), with a theorem linking rÎ¾=exp(âˆ’hÎ¾) within SAC to transport alignment (Section 4.1â€“4.2; Appendix A). This will directly address the gap identified around Eq. 3.\n  - Add a proposition detailing how the gradient of the SAC objective with rÎ¾ induces updates that move the agent distribution toward expert, under regularity assumptions on hÎ¾; include proof or sketch with assumptions (Section 4.2).\n  - Include a sanity-check experiment demonstrating that as hÎ¾ improves (measured by L(Î¾)), a direct empirical W1 estimate between Pagent and Pexpert decreases over training (e.g., via a held-out estimator), supporting the intended direction (Section 5).- Strengthen enforcement and measurement of the Lipschitz condition\n  - Report empirical estimates of Î›(hÎ¾) during training using input perturbations and controlled norms, and show their relation to the 1â€‘Lipschitz target; include plots or statistics (Section 4.3.1; Appendix N). This will substantiate the use of SN/activations.\n  - Quantify Î›(fÎ¾) and Î›(fÎ¾â€ ) across layers (SN singular values and activation Lipschitz), aggregate to a network-level bound, and combine with Th. 4.1 and Th. 4.2 to provide an explicit upper bound on Î›(hÎ¾) for chosen â„“HLG hyperparameters (Section 4.3, 4.3.3; Table 1).\n  - Add an ablation varying Ïƒ and N guided by Th. 4.2 to demonstrate how exceeding certain thresholds destabilizes Î›(â„“HLG), and couple this with measured Î›(hÎ¾) to validate the theory (Section 4.3.2â€“4.3.3; Appendix N).- Improve experimental fairness and baseline tuning\n  - Specify a comparable hyperparameter search budget for all methods and report tuned settings per environment for baselines (including GP coefficient sweeps for DAC/SAM/W-DAC/MMD), or lock all methods to a single global configuration and report the trade-offs (Section 5; Appendix L). This addresses tuning asymmetry.\n  - Present an additional DiffAIL run using the authorsâ€™ original settings (if feasible) alongside the stabilized variant, and discuss the impact of modifications; alternatively, cross-validate changes on NGT to ensure equal treatment (Appendix L; Section 5).\n  - Apply reward normalization schemes consistently across baselines (e.g., percentile scaling from Appendix J) or show an ablation demonstrating that NGTâ€™s advantage is not due to normalization; include quantitative comparisons (Appendix J; Section 5).- Expand reporting and statistical analyses\n  - Add tables of final returns with meanÂ±std over seeds, and significance tests (e.g., paired tests on AUC or final performance), for all environments and demo counts (Figures 1â€“3, 9; Section 5). This strengthens claims quantitatively.\n  - Provide explicit multi-task plots/tables for the 20â€‘transition regime (subr50) across several environments (not only Humanoid), to substantiate the ultra-low data claim in the Abstract (Figure 3; Section 5).\n  - Extend state-only evaluation beyond Humanoid to at least two additional tasks and report performance curves and summary statistics, clarifying the scope of applicability (Figure 2â€“3; Section 5).Score\n- Overall (10): 7 â€” Novel formulation with solid empirical results and useful theory (Eq. 2â€“3; Th. 4.1â€“4.2; Figures 1â€“3) but gaps in OT/policy linkage and Lipschitz enforcement (Section 4.2â€“4.3; Appendix F).\n- Novelty (10): 7 â€” Combining random-prior prediction with an adversarial OT objective and adapted distributional losses is fresh (Section 4.1â€“4.3.2; Appendix Bâ€“C), though related to RND/RED and WGAN concepts (Section 2; Appendix E).\n- Technical Quality (10): 6 â€” Theoretical pieces (Th. 4.1â€“4.2; Th. F.2) are valuable, but the minimax connection and 1â€‘Lipschitz enforcement are not fully formalized; experimental tuning fairness needs tightening (Section 4.2â€“4.3; Section 5; Appendix N, L).\n- Clarity (10): 7 â€” Clear algorithm and derivations (Appendix A; Sections 4.1â€“4.3.3), comprehensive figures, and ablations, but sign conventions and policyâ€“OT linkage in Section 4.2 require clarification.\n- Confidence (5): 4 â€” High confidence in reading/interpretation based on extensive anchors (Eq. 2â€“3; Figures 1â€“3; Th. 4.1â€“4.2; Appendix Fâ€“N), moderate uncertainty due to missing formal policy-side guarantees and tuning asymmetries.",
  "final_review": "Summary\n- The paper proposes Noise-Guided Transport (NGT), an off-policy imitation learning method that learns a reward via a potential hÎ¾(x)=â„“(fÎ¾(x), fÎ¾â€ (x)) derived from predicting random priors, and optimizes an adversarial objective L(Î¾)=Eexpert[hÎ¾]âˆ’Eagent[hÎ¾] (Eq. 2, Section 4.1). The authors connect minimizing L(Î¾) over 1â€‘Lipschitz potentials to the Kantorovichâ€“Rubinstein dual of the earth moverâ€™s distance (Eq. 3, Section 4.2), present a Lipschitz composition bound for hÎ¾ (Th. 4.1, Section 4.3), and derive a Lipschitz bound for the histogram Gaussian loss â„“HLG (Th. 4.2, Section 4.3.3). They also provide a concentration bound for the empirical reward loss (Appendix F). Empirical results on Gymnasium MuJoCo and DMC tasks (Figures 1â€“3, 9) show strong performance in low-data settings, including on Humanoidâ€‘v4, with ablations supporting design choices (Appendix N). Code is released.Strengths\n- Bold re-formulation of reward learning via random prior prediction\n  - The core objective L(Î¾) uses a potential hÎ¾(x)=â„“(fÎ¾(x), fÎ¾â€ (x)) with descent on expert data and ascent on agent data (Eq. 2, Section 4.1), offering a distinct adversarial signal compared to binary discriminators. This is novel relative to typical GAN-style IL (Section 1, Section 2).\n  - The method explicitly leverages pseudo-density estimation from random priors (Section 4.1), connecting to RND/RED but turning one-class signals into a principled two-sided objective; this can reduce ambiguity and avoid the limitations of positive-only signals (Section 4.1, discussion).\n  - Impact: Provides a new, lightweight and easily implementable mechanism for reward learning in off-policy IL, with potential to broaden applicability in low-data regimes (Abstract; Section 5).\n- Optimal transport perspective with concrete regularity analysis\n  - The dual-OT link is stated via Eq. 3 (Section 4.2), situating NGT within W1-based transport and connecting the learned potential to Kantorovich potentials; this grounds the objective in a well-studied metric space.\n  - The Lipschitz composition theorem (Th. 4.1, Section 4.3) offers a clear, modular way to control Î›(hÎ¾) via â„“, fÎ¾, and fÎ¾â€ â€”useful for operationalizing the 1â€‘Lipschitz requirement and diagnosing stability; technical soundness improves clarity.\n  - The new bound for the histogram Gaussian loss, Î› â‰¤ âˆš(1+(C/Ïƒ)^2), (Th. 4.2, Section 4.3.3) relates stability to hyperparameters (N, Ïƒ, [a,b]), informing robust configuration (Section 4.3.2, 4.3.3). Novelty: adapted derivation with input-based Lipschitz analysis.\n- Strong empirical performance in low-data and high-dimensional tasks\n  - Across Ant/HalfCheetah/Pusher/Walker2d, NGT generally matches or exceeds baselines (Figure 2; Figure 1 aggregated view). This is impactful in sample-efficiency contexts (Section 5).\n  - On Humanoid-v4, NGT achieves near-expert performance with as few as 1 demonstration (50 transitions post-subsampling at 20) and shows scaling over demos/subsampling (Figure 3; Section 5). This is notable given high dimensions (Appendix M; State 376, Action 17).\n  - Robustness to observation availability: Results include state-only/state-state settings (Figure 3 rows 1â€“2, Section 5), indicating broader applicability when actions are missing.\n- Careful, practical design for stability and efficiency\n  - Spectral normalization on reward networks and orthogonal initialization of the frozen prior (Section 4.3.1; Appendix I, J) are motivated by Lipschitz control and feature spread; reduced need for gradient penalty (Section 4.3.1; Section 5) lowers computational costâ€”relevant for practitioners.\n  - Adoption of distributional losses (â„“HLG) specifically enabled Humanoid success, with detailed mechanism and code (Section 4.3.2; Appendix Bâ€“C). Ablations demonstrate necessity on Humanoid and generalization to other tasks (Figure 4â€“5, Appendix N).\n  - CUDA Graphs and a unified SAC backbone contributed to speed and fairness (Section 5; Appendix K, L, P). Runtime comparisons show NGT competitive with fastest baselines (Appendix P, Table 3).\n- Theoretical concentration guarantee for empirical reward loss\n  - The bound in Appendix F (Th. F.2; Cor. F.3) shows exponential concentration in n with dependence on diam(ğ•) and Î›(h), supporting reliability of empirical estimates of L(Î¾). While generic, it provides useful control over deviation in finite samples, aligning with the low-data emphasis (Appendix F).\n- Clarity and reproducibility\n  - Algorithm 1 makes the training loop explicit and shows how rÎ¾ integrates with SAC (Appendix A), improving clarity of the overall pipeline.\n  - Hyperparameter tables and architecture details (Appendix I, L, Table 1) plus public code link (Section 4.3.3; Section 5; Abstract) bolster reproducibility and impact.Weaknesses\n- OT objective/sign conventions and policy-reward linkage are under-specified\n  - Eq. 3 (Section 4.2) implies minimizing L(Î¾) over 1â€‘Lipschitz h maximizes EMD(Pagent, Pexpert), i.e., increases discrepancies; yet Section 4.2 states â€œenables the agent to provably close the gap with the expert,â€ without a formal policy-side argument showing that using rÎ¾=exp(âˆ’hÎ¾) and SAC will minimize EMD. This gap matters for technical soundness.\n  - The minimax structure typical of OT/WGAN (max over critic, min over generator) is not explicitly formulated for the policy; Algorithm 1 (Appendix A) shows reward learning and policy updates but lacks a theorem connecting policy improvement to transport objectives or monotone convergence. This limits theoretical completeness.\n  - No direct proof or proposition shows that maximizing EMD via hÎ¾, combined with rÎ¾=exp(âˆ’hÎ¾), induces gradient directions in SAC that move Pagent toward Pexpert (Section 4.1â€“4.2). Impact: leaves the core claim partially unsupported.\n  - In practice, â„“HLG is instantiated with different Ïƒ on expert vs agent expectations (Section 4.3.2, last paragraph; Appendix Table 1: agent-side Ïƒ=0.05, expert-side Ïƒ=0.25), which departs from the single-potential assumption in Eq. 2/Section 4.2 and further obscures the exact OT-dual alignment; this affects the rigor of the equivalence claim.\n- 1â€‘Lipschitz requirement is only heuristically enforced\n  - The OT equivalence (Eq. 3) requires hÎ¾ âˆˆ H^1; but Section 4.3.1 states â€œwe do not aim for the perfect 1â€‘Lipschitz constant â€¦ keep it â€˜close enough to 1â€™,â€ relying on spectral normalization and â€œreasonably linear-likeâ€ activations. No empirical measurement of Î›(hÎ¾) or bound aggregation over layers is reported; this weakens the rigor of the OT claim.\n  - While Th. 4.1 (Section 4.3) and Th. 4.2 (Section 4.3.3) bound â„“â€™s contribution, the effective Î›(fÎ¾) and Î›(fÎ¾â€ ) under LeakyReLU and SN are not quantified for the full network compositions; Appendix I uses LeakyReLU with leak 0.05, but no net Lipschitz computation confirms Î›(hÎ¾)â‰¤1. Technical soundness is affected.\n  - The concentration bound (Appendix F) assumes Î›(h) known and bounded (Assumption F.1), yet the paper does not verify or estimate Î›(h) under the implemented choices. This reduces the practical applicability of the bound.\n  - The main text presents Î›(hÎ¾)=Î›(â„“)(Î›(fÎ¾)+Î›(fÎ¾â€ )) as an equality (Eq. 4, Section 4.3), while Appendix G only proves an upper bound via inequality (Eq. (13)â€“(14)); this discrepancy affects claims of exact control.\n  - Section 4.3.1 asserts SN â€œwould not be needed for the prior network,â€ but ablations report collapse when removing SN from prior and predictor together (Appendix N, Figure 7) without isolating prior-only removal; evidence for the prior-specific claim is therefore incomplete.\n- Experimental fairness and tuning asymmetry\n  - Section 5 states â€œwe did not carry out per-task tuning for any method,â€ but Appendix N reports that â„“HLG hyperparameters (N, [a,b], Ïƒ) were adapted per environment, and Table 1 shows distinct Ïƒ for agent/expert sides; baselines appear with fixed GP coefficients and limited tuning (Section 5). This asymmetry may advantage NGT.\n  - DiffAIL required implementation modifications (Appendix L), including activation changes and numerical epsilons; while necessary for stability, this deviates from the original and may alter performance comparisons. Fairness is impacted.\n  - Reward normalization via percentile scaling is applied to NGT (Appendix J) and RED* but not to other adversarial baselines (Section 5), potentially affecting comparability of rewards fed to SAC and thereby learning dynamics; additionally, this rescaling contradicts the â€œbounds the reward between 0 and 1â€ claim stated in Section 4.1.\n- Incomplete reporting and statistical analysis\n  - Results are primarily curve-based (Figures 1â€“3, 9) with shaded confidence but no final tables of meanÂ±std returns, no significance tests, and limited aggregate metrics; harder to quantitatively compare across seeds and tasks (Section 5).\n  - The â€œas few as 20 transitionsâ€ claim is supported in Humanoid analyses (Figure 3 with subsampling offsets, Section 5), but broader multi-task evidence for exactly 20 transitions is limited; generality of the ultra-low regime claim remains narrow.\n  - State-only results are shown for Humanoid (Figure 3, second row), but similar analyses for other environments are not reported; the scope of the state-only setting remains unclear (Section 5).\n  - Minor clarity issues: Appendix A references â€œEq. ??â€ (Appendix A), and Section 5 points to â€œAPPENDIX Oâ€ for extra environments while the section is labeled â€œC Extra Environmentsâ€ (Appendix C; Figure 9), complicating navigation.Suggestions for Improvement\n- Clarify OT objective and formalize policy-side convergence\n  - Provide a minimax formulation explicitly showing max over hÎ¾ âˆˆ H^1 of Eagent[h]âˆ’Eexpert[h] (critic) and a corresponding policy objective that provably reduces W1(Pagent, Pexpert), with a theorem linking rÎ¾=exp(âˆ’hÎ¾) within SAC to transport alignment (Section 4.1â€“4.2; Appendix A). This will directly address the gap identified around Eq. 3.\n  - Add a proposition detailing how the gradient of the SAC objective with rÎ¾ induces updates that move the agent distribution toward expert, under regularity assumptions on hÎ¾; include proof or sketch with assumptions (Section 4.2).\n  - Include a sanity-check experiment demonstrating that as hÎ¾ improves (measured by L(Î¾)), a direct empirical W1 estimate between Pagent and Pexpert decreases over training (e.g., via a held-out estimator), supporting the intended direction (Section 5).\n  - Either enforce a single potential in implementation (e.g., same Ïƒ for both expectations in L(Î¾)) or explicitly re-derive the objective for asymmetric â„“ to clarify how the implemented design relates to the OT dual (Section 4.3.2; Appendix Table 1).\n- Strengthen enforcement and measurement of the Lipschitz condition\n  - Report empirical estimates of Î›(hÎ¾) during training using input perturbations and controlled norms, and show their relation to the 1â€‘Lipschitz target; include plots or statistics (Section 4.3.1; Appendix N). This will substantiate the use of SN/activations.\n  - Quantify Î›(fÎ¾) and Î›(fÎ¾â€ ) across layers (SN singular values and activation Lipschitz), aggregate to a network-level bound, and combine with Th. 4.1 and Th. 4.2 to provide an explicit upper bound on Î›(hÎ¾) for chosen â„“HLG hyperparameters (Section 4.3, 4.3.3; Table 1).\n  - Add an ablation varying Ïƒ and N guided by Th. 4.2 to demonstrate how exceeding certain thresholds destabilizes Î›(â„“HLG), and couple this with measured Î›(hÎ¾) to validate the theory (Section 4.3.2â€“4.3.3; Appendix N).\n  - Revise Eq. 4 wording to reflect an inequality (upper bound) consistent with Appendix G (Eq. (13)â€“(14)), and carefully state which quantities are bounded versus equal (Section 4.3).\n  - Provide an ablation isolating removal of spectral normalization from only the prior network to substantiate the claim that SN â€œwould not be needed for the priorâ€ under OI (Section 4.3.1; Appendix N, Figure 7).\n- Improve experimental fairness and baseline tuning\n  - Specify a comparable hyperparameter search budget for all methods and report tuned settings per environment for baselines (including GP coefficient sweeps for DAC/SAM/W-DAC/MMD), or lock all methods to a single global configuration and report the trade-offs (Section 5; Appendix L). This addresses tuning asymmetry.\n  - Present an additional DiffAIL run using the authorsâ€™ original settings (if feasible) alongside the stabilized variant, and discuss the impact of modifications; alternatively, cross-validate changes on NGT to ensure equal treatment (Appendix L; Section 5).\n  - Apply reward normalization schemes consistently across baselines (e.g., percentile scaling from Appendix J) or show an ablation demonstrating that NGTâ€™s advantage is not due to normalization; include quantitative comparisons and clarify that rescaling may alter the nominal [0,1] bound claimed in Section 4.1 (Appendix J; Section 5).\n- Expand reporting and statistical analyses\n  - Add tables of final returns with meanÂ±std over seeds, and significance tests (e.g., paired tests on AUC or final performance), for all environments and demo counts (Figures 1â€“3, 9; Section 5). This strengthens claims quantitatively.\n  - Provide explicit multi-task plots/tables for the 20â€‘transition regime (subr50) across several environments (not only Humanoid), to substantiate the ultra-low data claim in the Abstract (Figure 3; Section 5).\n  - Extend state-only evaluation beyond Humanoid to at least two additional tasks and report performance curves and summary statistics, clarifying the scope of applicability (Figure 2â€“3; Section 5).\n  - Fix appendix cross-references and placeholders (e.g., resolve â€œEq. ??â€ in Appendix A; ensure â€œExtra Environmentsâ€ labeling matches Section 5â€™s reference to Appendix O), improving traceability (Appendix A; Appendix C/Figure 9; Section 5).Score\n- Overall (10): 6 â€” Strong empirical results and a clean objective (Eq. 2; Figures 1â€“3) with useful regularity analysis (Th. 4.2) but material gaps in OT/policy linkage and implementation-theory alignment (Eq. 3 directionality; asymmetric Ïƒ in Section 4.3.2; Appendix Table 1).\n- Novelty (10): 7 â€” Combining random-prior prediction with an adversarial OT-style objective and adapted distributional losses is fresh (Section 4.1â€“4.3.2; Appendix Bâ€“C), though related to RND/RED and WGAN concepts (Section 2; Appendix E).\n- Technical Quality (10): 5 â€” Theoretical pieces are helpful (Th. 4.1â€“4.2; Th. F.2), but 1â€‘Lipschitz enforcement and exact OT equivalence are not fully justified in practice (Section 4.3.1; Appendix F), with an equality vs. inequality mismatch (Eq. 4 vs. Appendix G) and asymmetric Ïƒ across expectations (Section 4.3.2; Table 1).\n- Clarity (10): 6 â€” Generally clear method and ablations (Appendix A; Sections 4.1â€“4.3.3), but sign conventions/policyâ€“OT linkage need tightening (Section 4.2), and minor cross-referencing issues (Appendix A â€œEq. ??â€; Appendix C vs. â€œAppendix Oâ€) hamper navigation.\n- Confidence (5): 4 â€” High confidence based on concrete anchors across theory and experiments (Eq. 2â€“3; Th. 4.1â€“4.2; Figures 1â€“3; Appendix Fâ€“N), with some uncertainty due to missing formal policy-side guarantees and the implementation-theory mismatch.",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 7,
        "novelty": 7,
        "technical_quality": 6,
        "clarity": 7,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 6,
        "novelty": 7,
        "technical_quality": 5,
        "clarity": 6,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "Summary\n- The paper proposes Noise-Guided Transport (NGT), an off-policy imitation learning method that learns a reward via a potential hÎ¾(x)=â„“(fÎ¾(x), fÎ¾â€ (x)) derived from predicting random priors, and optimizes an adversarial objective L(Î¾)=Eexpert[hÎ¾]âˆ’Eagent[hÎ¾] (Eq. 2, Section 4.1). The authors connect minimizing L(Î¾) over 1â€‘Lipschitz potentials to the Kantorovichâ€“Rubinstein dual of the earth moverâ€™s distance (Eq. 3, Section 4.2), present a Lipschitz composition bound for hÎ¾ (Th. 4.1, Section 4.3), and derive a Lipschitz bound for the histogram Gaussian loss â„“HLG (Th. 4.2, Section 4.3.3). They also provide a concentration bound for the empirical reward loss (Appendix F). Empirical results on Gymnasium MuJoCo and DMC tasks (Figures 1â€“3, 9) show strong performance in low-data settings, including on Humanoidâ€‘v4, with ablations supporting design choices (Appendix N). Code is released.Strengths\n- Bold re-formulation of reward learning via random prior prediction\n  - The core objective L(Î¾) uses a potential hÎ¾(x)=â„“(fÎ¾(x), fÎ¾â€ (x)) with descent on expert data and ascent on agent data (Eq. 2, Section 4.1), offering a distinct adversarial signal compared to binary discriminators. This is novel relative to typical GAN-style IL (Section 1, Section 2).\n  - The method explicitly leverages pseudo-density estimation from random priors (Section 4.1), connecting to RND/RED but turning one-class signals into a principled two-sided objective; this can reduce ambiguity and avoid the limitations of positive-only signals (Section 4.1, discussion).\n  - Impact: Provides a new, lightweight and easily implementable mechanism for reward learning in off-policy IL, with potential to broaden applicability in low-data regimes (Abstract; Section 5).\n- Optimal transport perspective with concrete regularity analysis\n  - The dual-OT link is stated via Eq. 3 (Section 4.2), situating NGT within W1-based transport and connecting the learned potential to Kantorovich potentials; this grounds the objective in a well-studied metric space.\n  - The Lipschitz composition theorem (Th. 4.1, Section 4.3) offers a clear, modular way to control Î›(hÎ¾) via â„“, fÎ¾, and fÎ¾â€ â€”useful for operationalizing the 1â€‘Lipschitz requirement and diagnosing stability; technical soundness improves clarity.\n  - The new bound for the histogram Gaussian loss, Î› â‰¤ âˆš(1+(C/Ïƒ)^2), (Th. 4.2, Section 4.3.3) relates stability to hyperparameters (N, Ïƒ, [a,b]), informing robust configuration (Section 4.3.2, 4.3.3). Novelty: adapted derivation with input-based Lipschitz analysis.\n- Strong empirical performance in low-data and high-dimensional tasks\n  - Across Ant/HalfCheetah/Pusher/Walker2d, NGT generally matches or exceeds baselines (Figure 2; Figure 1 aggregated view). This is impactful in sample-efficiency contexts (Section 5).\n  - On Humanoid-v4, NGT achieves near-expert performance with as few as 1 demonstration (50 transitions post-subsampling at 20) and shows scaling over demos/subsampling (Figure 3; Section 5). This is notable given high dimensions (Appendix M; State 376, Action 17).\n  - Robustness to observation availability: Results include state-only/state-state settings (Figure 3 rows 1â€“2, Section 5), indicating broader applicability when actions are missing.\n- Careful, practical design for stability and efficiency\n  - Spectral normalization on reward networks and orthogonal initialization of the frozen prior (Section 4.3.1; Appendix I, J) are motivated by Lipschitz control and feature spread; reduced need for gradient penalty (Section 4.3.1; Section 5) lowers computational costâ€”relevant for practitioners.\n  - Adoption of distributional losses (â„“HLG) specifically enabled Humanoid success, with detailed mechanism and code (Section 4.3.2; Appendix Bâ€“C). Ablations demonstrate necessity on Humanoid and generalization to other tasks (Figure 4â€“5, Appendix N).\n  - CUDA Graphs and a unified SAC backbone contributed to speed and fairness (Section 5; Appendix K, L, P). Runtime comparisons show NGT competitive with fastest baselines (Appendix P, Table 3).\n- Theoretical concentration guarantee for empirical reward loss\n  - The bound in Appendix F (Th. F.2; Cor. F.3) shows exponential concentration in n with dependence on diam(ğ•) and Î›(h), supporting reliability of empirical estimates of L(Î¾). While generic, it provides useful control over deviation in finite samples, aligning with the low-data emphasis (Appendix F).\n- Clarity and reproducibility\n  - Algorithm 1 makes the training loop explicit and shows how rÎ¾ integrates with SAC (Appendix A), improving clarity of the overall pipeline.\n  - Hyperparameter tables and architecture details (Appendix I, L, Table 1) plus public code link (Section 4.3.3; Section 5; Abstract) bolster reproducibility and impact.Weaknesses\n- OT objective/sign conventions and policy-reward linkage are under-specified\n  - Eq. 3 (Section 4.2) implies minimizing L(Î¾) over 1â€‘Lipschitz h maximizes EMD(Pagent, Pexpert), i.e., increases discrepancies; yet Section 4.2 states â€œenables the agent to provably close the gap with the expert,â€ without a formal policy-side argument showing that using rÎ¾=exp(âˆ’hÎ¾) and SAC will minimize EMD. This gap matters for technical soundness.\n  - The minimax structure typical of OT/WGAN (max over critic, min over generator) is not explicitly formulated for the policy; Algorithm 1 (Appendix A) shows reward learning and policy updates but lacks a theorem connecting policy improvement to transport objectives or monotone convergence. This limits theoretical completeness.\n  - No direct proof or proposition shows that maximizing EMD via hÎ¾, combined with rÎ¾=exp(âˆ’hÎ¾), induces gradient directions in SAC that move Pagent toward Pexpert (Section 4.1â€“4.2). Impact: leaves the core claim partially unsupported.\n  - In practice, â„“HLG is instantiated with different Ïƒ on expert vs agent expectations (Section 4.3.2, last paragraph; Appendix Table 1: agent-side Ïƒ=0.05, expert-side Ïƒ=0.25), which departs from the single-potential assumption in Eq. 2/Section 4.2 and further obscures the exact OT-dual alignment; this affects the rigor of the equivalence claim.\n- 1â€‘Lipschitz requirement is only heuristically enforced\n  - The OT equivalence (Eq. 3) requires hÎ¾ âˆˆ H^1; but Section 4.3.1 states â€œwe do not aim for the perfect 1â€‘Lipschitz constant â€¦ keep it â€˜close enough to 1â€™,â€ relying on spectral normalization and â€œreasonably linear-likeâ€ activations. No empirical measurement of Î›(hÎ¾) or bound aggregation over layers is reported; this weakens the rigor of the OT claim.\n  - While Th. 4.1 (Section 4.3) and Th. 4.2 (Section 4.3.3) bound â„“â€™s contribution, the effective Î›(fÎ¾) and Î›(fÎ¾â€ ) under LeakyReLU and SN are not quantified for the full network compositions; Appendix I uses LeakyReLU with leak 0.05, but no net Lipschitz computation confirms Î›(hÎ¾)â‰¤1. Technical soundness is affected.\n  - The concentration bound (Appendix F) assumes Î›(h) known and bounded (Assumption F.1), yet the paper does not verify or estimate Î›(h) under the implemented choices. This reduces the practical applicability of the bound.\n  - The main text presents Î›(hÎ¾)=Î›(â„“)(Î›(fÎ¾)+Î›(fÎ¾â€ )) as an equality (Eq. 4, Section 4.3), while Appendix G only proves an upper bound via inequality (Eq. (13)â€“(14)); this discrepancy affects claims of exact control.\n  - Section 4.3.1 asserts SN â€œwould not be needed for the prior network,â€ but ablations report collapse when removing SN from prior and predictor together (Appendix N, Figure 7) without isolating prior-only removal; evidence for the prior-specific claim is therefore incomplete.\n- Experimental fairness and tuning asymmetry\n  - Section 5 states â€œwe did not carry out per-task tuning for any method,â€ but Appendix N reports that â„“HLG hyperparameters (N, [a,b], Ïƒ) were adapted per environment, and Table 1 shows distinct Ïƒ for agent/expert sides; baselines appear with fixed GP coefficients and limited tuning (Section 5). This asymmetry may advantage NGT.\n  - DiffAIL required implementation modifications (Appendix L), including activation changes and numerical epsilons; while necessary for stability, this deviates from the original and may alter performance comparisons. Fairness is impacted.\n  - Reward normalization via percentile scaling is applied to NGT (Appendix J) and RED* but not to other adversarial baselines (Section 5), potentially affecting comparability of rewards fed to SAC and thereby learning dynamics; additionally, this rescaling contradicts the â€œbounds the reward between 0 and 1â€ claim stated in Section 4.1.\n- Incomplete reporting and statistical analysis\n  - Results are primarily curve-based (Figures 1â€“3, 9) with shaded confidence but no final tables of meanÂ±std returns, no significance tests, and limited aggregate metrics; harder to quantitatively compare across seeds and tasks (Section 5).\n  - The â€œas few as 20 transitionsâ€ claim is supported in Humanoid analyses (Figure 3 with subsampling offsets, Section 5), but broader multi-task evidence for exactly 20 transitions is limited; generality of the ultra-low regime claim remains narrow.\n  - State-only results are shown for Humanoid (Figure 3, second row), but similar analyses for other environments are not reported; the scope of the state-only setting remains unclear (Section 5).\n  - Minor clarity issues: Appendix A references â€œEq. ??â€ (Appendix A), and Section 5 points to â€œAPPENDIX Oâ€ for extra environments while the section is labeled â€œC Extra Environmentsâ€ (Appendix C; Figure 9), complicating navigation.Suggestions for Improvement\n- Clarify OT objective and formalize policy-side convergence\n  - Provide a minimax formulation explicitly showing max over hÎ¾ âˆˆ H^1 of Eagent[h]âˆ’Eexpert[h] (critic) and a corresponding policy objective that provably reduces W1(Pagent, Pexpert), with a theorem linking rÎ¾=exp(âˆ’hÎ¾) within SAC to transport alignment (Section 4.1â€“4.2; Appendix A). This will directly address the gap identified around Eq. 3.\n  - Add a proposition detailing how the gradient of the SAC objective with rÎ¾ induces updates that move the agent distribution toward expert, under regularity assumptions on hÎ¾; include proof or sketch with assumptions (Section 4.2).\n  - Include a sanity-check experiment demonstrating that as hÎ¾ improves (measured by L(Î¾)), a direct empirical W1 estimate between Pagent and Pexpert decreases over training (e.g., via a held-out estimator), supporting the intended direction (Section 5).\n  - Either enforce a single potential in implementation (e.g., same Ïƒ for both expectations in L(Î¾)) or explicitly re-derive the objective for asymmetric â„“ to clarify how the implemented design relates to the OT dual (Section 4.3.2; Appendix Table 1).\n- Strengthen enforcement and measurement of the Lipschitz condition\n  - Report empirical estimates of Î›(hÎ¾) during training using input perturbations and controlled norms, and show their relation to the 1â€‘Lipschitz target; include plots or statistics (Section 4.3.1; Appendix N). This will substantiate the use of SN/activations.\n  - Quantify Î›(fÎ¾) and Î›(fÎ¾â€ ) across layers (SN singular values and activation Lipschitz), aggregate to a network-level bound, and combine with Th. 4.1 and Th. 4.2 to provide an explicit upper bound on Î›(hÎ¾) for chosen â„“HLG hyperparameters (Section 4.3, 4.3.3; Table 1).\n  - Add an ablation varying Ïƒ and N guided by Th. 4.2 to demonstrate how exceeding certain thresholds destabilizes Î›(â„“HLG), and couple this with measured Î›(hÎ¾) to validate the theory (Section 4.3.2â€“4.3.3; Appendix N).\n  - Revise Eq. 4 wording to reflect an inequality (upper bound) consistent with Appendix G (Eq. (13)â€“(14)), and carefully state which quantities are bounded versus equal (Section 4.3).\n  - Provide an ablation isolating removal of spectral normalization from only the prior network to substantiate the claim that SN â€œwould not be needed for the priorâ€ under OI (Section 4.3.1; Appendix N, Figure 7).\n- Improve experimental fairness and baseline tuning\n  - Specify a comparable hyperparameter search budget for all methods and report tuned settings per environment for baselines (including GP coefficient sweeps for DAC/SAM/W-DAC/MMD), or lock all methods to a single global configuration and report the trade-offs (Section 5; Appendix L). This addresses tuning asymmetry.\n  - Present an additional DiffAIL run using the authorsâ€™ original settings (if feasible) alongside the stabilized variant, and discuss the impact of modifications; alternatively, cross-validate changes on NGT to ensure equal treatment (Appendix L; Section 5).\n  - Apply reward normalization schemes consistently across baselines (e.g., percentile scaling from Appendix J) or show an ablation demonstrating that NGTâ€™s advantage is not due to normalization; include quantitative comparisons and clarify that rescaling may alter the nominal [0,1] bound claimed in Section 4.1 (Appendix J; Section 5).\n- Expand reporting and statistical analyses\n  - Add tables of final returns with meanÂ±std over seeds, and significance tests (e.g., paired tests on AUC or final performance), for all environments and demo counts (Figures 1â€“3, 9; Section 5). This strengthens claims quantitatively.\n  - Provide explicit multi-task plots/tables for the 20â€‘transition regime (subr50) across several environments (not only Humanoid), to substantiate the ultra-low data claim in the Abstract (Figure 3; Section 5).\n  - Extend state-only evaluation beyond Humanoid to at least two additional tasks and report performance curves and summary statistics, clarifying the scope of applicability (Figure 2â€“3; Section 5).\n  - Fix appendix cross-references and placeholders (e.g., resolve â€œEq. ??â€ in Appendix A; ensure â€œExtra Environmentsâ€ labeling matches Section 5â€™s reference to Appendix O), improving traceability (Appendix A; Appendix C/Figure 9; Section 5).Score\n- Overall (10): 6 â€” Strong empirical results and a clean objective (Eq. 2; Figures 1â€“3) with useful regularity analysis (Th. 4.2) but material gaps in OT/policy linkage and implementation-theory alignment (Eq. 3 directionality; asymmetric Ïƒ in Section 4.3.2; Appendix Table 1).\n- Novelty (10): 7 â€” Combining random-prior prediction with an adversarial OT-style objective and adapted distributional losses is fresh (Section 4.1â€“4.3.2; Appendix Bâ€“C), though related to RND/RED and WGAN concepts (Section 2; Appendix E).\n- Technical Quality (10): 5 â€” Theoretical pieces are helpful (Th. 4.1â€“4.2; Th. F.2), but 1â€‘Lipschitz enforcement and exact OT equivalence are not fully justified in practice (Section 4.3.1; Appendix F), with an equality vs. inequality mismatch (Eq. 4 vs. Appendix G) and asymmetric Ïƒ across expectations (Section 4.3.2; Table 1).\n- Clarity (10): 6 â€” Generally clear method and ablations (Appendix A; Sections 4.1â€“4.3.3), but sign conventions/policyâ€“OT linkage need tightening (Section 4.2), and minor cross-referencing issues (Appendix A â€œEq. ??â€; Appendix C vs. â€œAppendix Oâ€) hamper navigation.\n- Confidence (5): 4 â€” High confidence based on concrete anchors across theory and experiments (Eq. 2â€“3; Th. 4.1â€“4.2; Figures 1â€“3; Appendix Fâ€“N), with some uncertainty due to missing formal policy-side guarantees and the implementation-theory mismatch."
}