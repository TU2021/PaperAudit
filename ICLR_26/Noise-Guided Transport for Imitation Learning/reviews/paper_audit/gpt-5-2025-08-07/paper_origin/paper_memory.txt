# Global Summary
- Problem: Sample-efficient imitation learning (IL) in low-data regimes (few expert demonstrations), where large-scale pretraining or high-capacity models are impractical and behavioral cloning (BC) struggles due to compounding errors.
- Core approach: Noise-Guided Transport (NGT), an off-policy adversarial IL method that learns a reward by predicting random priors and optimizing a potential function h_Î¾ derived from a pairing loss between a predictor and a frozen randomly initialized prior network. The reward is r_Î¾(x) = exp(âˆ’h_Î¾(x)). Training minimizes L(Î¾) = E_{expert}[h_Î¾(x)] âˆ’ E_{agent}[h_Î¾(x)]. The method enforces Lipschitz control via spectral normalization and uses distributional pairing losses (notably histogram loss, Gaussian type, â„“_HLG) to stabilize and scale to high-dimensional tasks (e.g., Humanoid).
- Theoretical grounding: The optimization of L(Î¾) over 1-Lipschitz potentials recovers an optimal transport (OT) objective: inf_{hâˆˆH^1} E_expert[h] âˆ’ E_agent[h] = âˆ’EMD(P_agent, P_expert). Concentration bounds show the empirical loss converges to the population objective at an exponential rate: P(|Ä¤L âˆ’ L| â‰¥ Îµ) â‰¤ exp(âˆ’Îµ^2 n/(Î›^2 diam(ğ•)^2)). Lipschitz bound for â„“_HLG: Î› â‰¤ âˆš(1 + (C/Ïƒ)^2) with C := Î”sâˆš((N âˆ’ 1)/(2Ï€)).
- Evaluation: Standard Gymnasium MuJoCo v4 tasks (Ant-v4, HalfCheetah-v4, Pusher-v4, Walker2d-v4, Humanoid-v4), plus state-only/stateâ€“state settings and extra DMC tasks (walker-walk, cheetah-run, finger-spin). Extremely low data: 1, 4, 11 demos, subsampled at rate 20 (1 demo = 50 transitions). 4 parallel environments; 4 seeds; up to 10^7 environment steps.
- Baselines: BC(1) vs BC(20), PWIL, DAC/SAM (R and R+), W-DAC/SAM (WGAN critic), MMD-DAC/SAM (Ïƒ âˆˆ {0.1, 1, 10}), RED* (one-class version of NGTâ€™s left term with adaptive numerics), DiffAIL (diffusion reward). All share a SAC actor-critic backbone. Gradient penalty used for adversarial baselines (e.g., 10), except NGT which only uses spectral normalization.
- Key findings: Authors state NGT achieves expert performance broadly and outperforms baselines, including robust performance on Humanoid in ultra-low data regimes, even in state-only/stateâ€“state settings. With â„“_HLG, NGT scales to Humanoid; other pairing losses failed on Humanoid. Explicit Humanoid unnormalized returns at 3M steps (FIGURE 3): state-action: dems11 subr50 6200.68, subr20 6380.46, subr10 6406.23; dems04 6262.04, 6178.58, 6284.78; dems01 5648.71, 6041.36, 6136.33. Stateâ€“state: dems11 6012.26, 6362.61, 6508.14; dems04 5606.09, 6190.61, 6271.76; dems01 5275.16, 5800.85, 6123.18.
- Runtime: CUDA Graphs yield up to 3Ã— speedups; â‰ˆ5 hours to train Humanoid on GPU (RTX 4090). Speed at 200k steps (steps/s): NGT 712 (Humanoid), 967 (Walker2d); DAC/SAM 741, 953; DiffAIL 659, 929; PWIL 631, 749; MMD-DAC/SAM 749, 977; W-DAC/SAM 763, 978.
- Caveats explicitly noted: DiffAIL needed stability modifications (e.g., replacing Mish with LeakyReLU and Îµ padding). NGTâ€™s â„“_HLG hyperparameters (support [a,b], number of bins N, Ïƒ) require environment-specific scaling; non-HLG losses performed poorly on Humanoid. No per-task tuning beyond shared defaults is reported; only â„“_HLG used for Humanoid results. The WGAN-style EMD estimator baseline underperforms NGT despite both relating to OT. NGT does not use gradient penalty; stability relies on spectral normalization and orthogonal initialization. Code: https://github.com/lionelblonde/ngt-pytorch.

# Abstract
- Problem: IL with limited expert demos; need methods sample-efficient in demos and environment interactions.
- Proposal: Noise-Guided Transport (NGT), a lightweight off-policy IL method casting imitation as an OT problem with adversarial training. No pretraining or specialized architectures; includes uncertainty estimation via random priors; easy to implement/tune.
- Claim: Strong performance on challenging continuous control tasks, incl. high-dimensional Humanoid, in ultra-low data settings with as few as 20 transitions.
- Code: https://github.com/lionelblonde/ngt-pytorch.

# Introduction
- Motivation: BC works with large internet-scale data but fails with few demos due to compounding errors. Low-data IL crucial for domains like healthcare gait analysis where demos are scarce/diverse.
- Context: Adversarial IL (GAIL, DAC, SAM) minimizes JS divergence but may suffer from mode collapse/vanishing gradients; off-policy DAC/SAM improve sample efficiency.
- Approach: Derive reward learning from pseudo-density estimation with random priors; show objective equals an earth-mover distance (OT). Provide concentration guarantees for the practical loss.
- Evaluation plan (Section 5): Compare against diverse baselines incl. OT-based and diffusion-based AIL [60]; standard continuous control; include Humanoid locomotion despite high dimensionality. State: NGT learns humanoid locomotion; only diffusion baseline shows some progress but suboptimal and heavier compute.
- Highlight: Use of distributional losses in reward learning (e.g., histogram Gaussian) is key to robustness. Scaling with demos shown from â€œas few as 20 transitions,â€ and includes a state-only case. Figures: FIGURE 1 (aggregate normalized return vs timesteps, excluding humanoids) shows NGT surpassing DAC/SAM variants; FIGURE 3 shows scaling with data and subsampling.

# Related Work
- OT views: Primal approaches (PWIL approximates EMD; ROT and MAAD use Sinkhorn on trajectories). Dual approaches learn Kantorovich potentials (WGAN critic). NGT adopts a dual OT perspective.
- Off-policy/state-only methods: MAAD (on-policy) improves GAIL; OPOLO (off-policy) improves DAC for state-only. NGT claims no extra regularization required beyond spectral normalization.
- Boosting/ensembles: AILBoost boosts DAC. NGTâ€™s multiple random priors can be seen as an implicit ensemble via diverse prediction tasks.
- Pseudo-density estimation (PDE): RND for exploration (novelty) and RED for expert detection; RND reported ineffective for anti-exploration in continuous control unless asymmetric architectures are used; NGT leverages random priors without such feature engineering. Theoretical analysis of RND concentration properties [14] noted.
- Additional related works expanded in Appendix D (contrastive learning analogies, Lipschitz importance, RLHF links).

# Preliminaries
- Setting: Episodic MDP (S, A, P, r, Î³), finite horizon T. Policy Ï€ maximizes returns without access to environment reward r. Learns from demonstrations E = {(s_t, a_t)}. State-only variant: learn to match expert state distribution. Not specified in this section: dataset licensing or exact sizes beyond later details.

# Method
- Architecture: Actor-critic (SAC) with stochastic policy Ï€_Î¸ and twin Q-functions Q_Ï‰ trained via generalized policy improvement; a learned reward model r_Î¾. Off-policy with replay buffer.
- Input to reward: Can be X âˆˆ {SÃ—A, SÃ—S, S}.
- Reward learning objective (random priors): A frozen prior network f^â€ _Î¾: Xâ†’R^m (randomly initialized), and a trainable predictor f_Î¾. Pairing loss â„“ defines potential h_Î¾(x) := â„“(f_Î¾(x), f^â€ _Î¾(x)). NGT minimizes L(Î¾) = E_{expert}[h_Î¾(x)] âˆ’ E_{agent}[h_Î¾(x)]. Reward is r_Î¾(x) := exp(âˆ’h_Î¾(x)) (positive, bounded).
- OT equivalence: With potentials constrained to 1-Lipschitz (h_Î¾ âˆˆ H^1), inf_{hâˆˆH^1} E_expert[h] âˆ’ E_agent[h] = âˆ’EMD(P_agent, P_expert). The method learns a single potential h_Î¾ and uses âˆ’h_Î¾ for the second dual term; off-policy distribution is the buffer mixture Î². Claim: Descending L(Î¾) maximizes discrepancies (EMD) between P_agent and P_expert (via Eq. 3 as stated).
- Concentration: Empirical estimate Ä¤L(Î¾) concentrates around L(Î¾) with probability bound P(|Ä¤L âˆ’ L| â‰¥ Îµ) â‰¤ exp(âˆ’Îµ^2 n/(Î›^2 diam(ğ•)^2)). Corollary for H^1: P(|Ä¤L âˆ’ L| â‰¥ Îµ) â‰¤ exp(âˆ’Îµ^2 n/diam(ğ•)^2). Full proofs in Appendix F.
- Lipschitz control (Theorem 4.1): Î›(h_Î¾) = Î›(â„“) [Î›(f_Î¾) + Î›(f^â€ _Î¾)]. Design: spectral normalization (SN) on linear layers of predictor and prior; choose activations to keep Lipschitz â€œclose to 1.â€ Orthogonal initialization (OI) for all layers; for the frozen prior, OI makes SN unnecessary (highest singular value 1), but SN is required for predictor. No gradient penalty (GP) needed for NGT, unlike off-policy adversarial SOTAs where GP is necessary.
- Pairing loss choices:
  - 1-Lipschitz robust regression losses (e.g., L1, Huber with Î´=1) perform well on many tasks but not Humanoid.
  - Distributional histogram Gaussian loss â„“_HLG enabled scaling to Humanoid. Hyperparameters: [a, b], number of bins N, Gaussian Ïƒ. Predictor outputs mÃ—N logits (NÃ—m bins), while prior outputs m scalars (architectural asymmetry). Mechanism and code in Appendices Bâ€“C.
  - â„“_HLG Lipschitz bound (Theorem 4.2): Î› â‰¤ âˆš(1 + (C/Ïƒ)^2), with C = Î”sâˆš((Nâˆ’1)/(2Ï€)), indicating that large Ïƒ stabilizes gradients; small Ïƒ can be unstable.
- Algorithmic loop (Algorithm 1, Appendix A): Interleave reward model updates on batches from expert dataset and replay buffer with SAC updates using r_Î¾; reward numerics include adaptive percentile-based scaling/shifting (Appendix J).
- Code: https://github.com/lionelblonde/ngt-pytorch.

# Experiments
- Setup:
  - Environments: Gymnasium MuJoCo v4 continuous control (Ant-v4, HalfCheetah-v4, Pusher-v4, Walker2d-v4, Humanoid-v4). Dims (Appendix Table 2): Ant (S=111,A=8), HalfCheetah (S=17,A=6), Pusher (S=23,A=7), Walker2d (S=17,A=6), Humanoid (S=376,A=17).
  - Data regimes: 1, 4, 11 expert demonstrations; subsampled by 20 with varying offsets (so 1 demo = 50 transitions). Also state-only and stateâ€“state (SÃ—S) variants. Each run: 4 random seeds. Vectorized envs: 4 parallel executors; time-step counter increments by 4 per joint step. Experts are SAC policies trained with different seeds.
  - Training: Up to 10^7 environment steps. Normalized returns: 0 = random; 1 = expert; below random is negative. Evaluation episodes sample different environment seeds per episode.
  - Infrastructure: PyTorch + CUDA Graphs (Appendix K). Up to 3Ã— speedup; â‰ˆ5 hours to train Humanoid on GPU. Tested on NVIDIA RTX 4090; all methods fit memory except DiffAIL unless reducing replay buffer by 1M.
  - Shared backbone: SAC actor-critic; reward networks matched in size/activations/initialization for fairness; DiffAIL retains its diffusion reward model (with stability modifications in Appendix L). Optimizer: Adam.
- Baselines:
  - BC(20) with same subsampling; BC(1) without subsampling (for perspective with more data).
  - PWIL (primal EMD procedure; computes reward, no learning).
  - DAC/SAM (JS-GAN discriminator): â€œR+â€ uses âˆ’log(1âˆ’D) âˆˆ R+; â€œRâ€ uses âˆ’log(1âˆ’D)+log(D) âˆˆ R.
  - W-DAC/SAM (WGAN critic), MMD-DAC/SAM (RBF MMD with Ïƒ âˆˆ {0.1, 1.0, 10.0}).
  - RED* (uses only E_expert[h] with adaptive numerics instead of tuned temperature).
  - DiffAIL (diffusion-based reward plugged into JS-GAN objective); gradient penalty coefficient 0.1 for DiffAIL; 10 for other dual adversarial methods.
  - Note: NGT uses SN but no gradient penalty.
- Main results:
  - Figures 1 and 2: Authors state NGT achieves expert performance across tasks and outperforms baselines. DiffAIL strong on Humanoid but struggles elsewhere.
  - Humanoid scaling (Figure 3, unnormalized returns):
    - State-action, 3,000,000 steps: dems11 subr50 6200.68, subr20 6380.46, subr10 6406.23; dems04 6262.04, 6178.58, 6284.78; dems01 5648.71, 6041.36, 6136.33.
    - Stateâ€“state, 3,000,000 steps: dems11 6012.26, 6362.61, 6508.14; dems04 5606.09, 6190.61, 6271.76; dems01 5275.16, 5800.85, 6123.18.
    - Earlier steps (examples): state-action, 1,000,000 steps: dems11 4527.42/5054.18/4457.49 (subr50/20/10); dems04 4852.78/4826.02/4896.33; dems01 2775.61/4239.46/5153.07. Stateâ€“state, 1,000,000: dems11 4911.91/5207.31/4844.69; dems04 3659.49/3878.78/4880.02; dems01 2769.43/4204.21/4730.65.
  - Claims on other tasks: NGT attains high normalized returns broadly; â€œexpert performance across the board.â€ BC(1) is strong on Humanoid with 11 demos (11K pairs), illustrating data abundance helps BC.
  - Insights: â€œOptimizing an EMD is not the whole storyâ€â€”WGAN-style baseline underperforms NGT. The m-dimensional prior prediction scales gracefully with model capacity. â„“_HLG enables Ïƒ-controlled label smoothing; authors note option to use different Ïƒ on expert vs agent sides in L(Î¾).
  - Extra environments (Appendix C/Figure 9): DMC tasks (walker-walk, cheetah-run, finger-spin) show NGT strong and stable across 1/4/11 demos; baselines often high variance or fail to approach expert behavior.
- Ablations (Appendix N):
  - Loss choice: â„“_HLG vs â€œMSE Softmaxâ€ on Humanoid (1/4/11 demos). â„“_HLG succeeds; MSE variant fails (curves stay near 0.0â€“0.2 normalized).
  - â„“_HLG hyperparameters must be adapted per environment; recovers optimal results on non-Humanoid after tuning [a,b], N, Ïƒ.
  - Initialization: Orthogonal init outperforms Kaiming (Kaiming causes instability/lower returns).
  - Spectral normalization: Removing SN collapses training across tasks/demos.
  - Output embedding size m: Robust at 16/32/64; degraded at 8.
- Runtime (Appendix P; higher is better):
  - Steps/s at 200k steps: PWIL 631/749 (Humanoid/Walker2d); DiffAIL 659/929; DAC/SAM 741/953; MMD-DAC/SAM 749/977; W-DAC/SAM 763/978; NGT 712/967. RED* comparable to NGT.
- Not specified in this section: Exact per-task final normalized returns as scalars; per-baseline hyperparameters beyond those called out; exact number of total tasks in the aggregate of Figure 1.

# Conclusion
- Summary: NGT is a sample-efficient IL method for low-data regimes, learning rewards via prediction from random priors and optimizing an OT-grounded objective. With distributional pairing losses, NGT reproduces humanoid gaits with as few as 20 transitions and works without gradient penalization.
- Claims: Outperforms baselines; scales gracefully with task complexity and data scarcity; succeeds even when expert actions are unavailable (state-only/stateâ€“state). Future work: apply the objective to general generative modeling tasks.

# References
- Citations include foundational IL/IRL/GAN works (e.g., GAIL, MaxEnt IRL, WGAN), off-policy IL (DAC, SAM), OT (Villani; PWIL; Sinkhorn-based ROT/MAAD), PDE (RND, RED), divergence-based IL, distributional losses (Imani & White), Lipschitz control (SN; gradient penalty; related analyses), scaling in RL, RLHF connections, and environment/tooling references (Gymnasium, DMC, PyTorch).

# Appendix
- Algorithm 1: NGT training loop. Reward model updated by minimizing L(Î¾) = mean(h_Î¾ on expert) âˆ’ mean(h_Î¾ on agent buffer). SAC targets use r_Î¾(s,a,sâ€²) = exp(âˆ’h_Î¾(...)).
- â„“_HLG mechanism (Appendix Bâ€“C): Predictor outputs mÃ—N logits; targets map each prior dimension to N-bin probabilities via truncated Gaussian CDF differences with Îµ-stabilized normalization; cross-entropy used.
- Theoretical details:
  - Concentration (Appendix F): Theorem F.2 gives P(|Ä¤L âˆ’ L| â‰¥ Îµ) â‰¤ exp(âˆ’Îµ^2 n/(Î›^2 diam(ğ•)^2)); Corollary F.3 for Î›=1.
  - Potential Lipschitz (Appendix G): Î›(h_Î¾) = Î›(â„“)[Î›(f_Î¾)+Î›(f^â€ _Î¾)].
  - â„“_HLG Lipschitz (Appendix H): Î› â‰¤ âˆš(1 + (C/Ïƒ)^2) with C := Î”sâˆš((Nâˆ’1)/(2Ï€)); p_max â‰ˆ Î”s/(Ïƒâˆš(2Ï€)) underpinning the bound.
  - WGAN relation (Appendix E): Both optimize EMD but use different potentials; NGTâ€™s potential is anchored by random priors and positive-valued.
- Network architecture (Appendix I): Actor/critic/reward: 2 hidden layers, 256 units; actor/critic ReLU; reward LeakyReLU 0.05; orthogonal initialization; spectral normalization on reward network layers.
- Reward numerics (Appendix J): Percentile-based scaling: divide by Perc_0.95(r) âˆ’ Perc_0.05(r) and shift by Perc_0.05(r); no EMA; no temperature.
- CUDA Graphs (Appendix K): ~3Ã— speedups; require static computation graphs.
- Implementation details (Appendix L): Shared SAC backbone; DiffAIL stability changes (Mishâ†’LeakyReLU; Îµ to logs/denominators). Optimizer: Adam.
- Default hyperparameters (Appendix Table 1): e.g., steps 10^7; replay buffer 4Ã—10^6; batch 256; Î³=0.99; Ï„=0.005; policy LR 3Ã—10^âˆ’4; Q LR 1Ã—10^âˆ’3; reward LR 1Ã—10^âˆ’3; Î± autotune (init 0.2); target entropy âˆ’|A|; output embedding size 32; â„“_HLG support [âˆ’1,1], N=21, agent-side Ïƒ=0.05, expert-side Ïƒ=0.25; SN True; GP False.
- Environment dimensions (Appendix Table 2): Ant 111/8; HalfCheetah 17/6; Pusher 23/7; Walker2d 17/6; Humanoid 376/17.
- Ablations (Appendix N): Loss choice (â„“_HLG vs MSE Softmax), â„“_HLG hyperparameter scaling, initialization (orthogonal vs Kaiming), spectral normalization necessity, output embedding size sensitivity.
- Extra environments (Appendix C/Figure 9): DMC results highlight robustness under more variable initial states.
- Speeds (Appendix P/Table 3): Steps/s at 200k stepsâ€”PWIL 631/749, DiffAIL 659/929, DAC/SAM 741/953, MMD-DAC/SAM 749/977, W-DAC/SAM 763/978, NGT 712/967. RED* similar to NGT.