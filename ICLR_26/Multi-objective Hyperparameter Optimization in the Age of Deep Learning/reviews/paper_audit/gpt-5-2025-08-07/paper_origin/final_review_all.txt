Summary
- The paper introduces PriMO, a multi-objective HPO algorithm that incorporates expert priors over multiple objectives and leverages cheap approximations via an initial design. Priors are formalized per objective (Section 2, Eq. 1) and combined with random linear scalarization (Eq. 3) to enable BO; the acquisition is augmented by a decaying prior weighting γ = exp(−n_BO^2/n_d) and an ε-greedy mixture (Section 4.1, Eq. 4; Algorithms 2–3). An initial multi-fidelity design seeds BO with MOASHA-generated points at maximum fidelity (Section 4.2; Algorithm 1). Experiments on 8 DL surrogate benchmarks with 25 seeds and budgets of 20 equivalent full evaluations (Section 5.1; Appendix F–G) show strong anytime and often strong final performance, robustness under misleading priors, and favorable results in single-objective settings (Figures 1, 3–6; Appendix I).Strengths
- Bolded title: **Clear problem formulation for multi-objective priors**
  - Evidence: Formalization of per-objective priors π_{f_i}(λ) and compound prior Π_f(λ) (Section 2, Eq. 1) clarifies the intended role of expert beliefs; this supports novelty and clarity in bringing priors to the multi-objective HPO setting.
  - Evidence: Integration objective explicitly stated to minimize f while using cheap approximations \hat{f}_i(λ, z) at maximum fidelity and guidance by Π_f (Section 2, Eq. 2); improves clarity and sets precise optimization target.
  - Evidence: Table 1 positions PriMO against MO baselines with desiderata coverage (“Integrate multi-objective expert priors” ✓ and “Utilize cheap approximations” ✓), highlighting impact and practical relevance.
  - Why it matters: A precise, factorized multi-objective prior formulation sets up systematic algorithm design and empirical evaluation in HPO.- Bolded title: **Principled acquisition design combining priors and exploration**
  - Evidence: Prior-weighted acquisition with γ = exp(−n_BO^2/n_d) and ε-greedy mixing (Section 4.1; Eq. 4; Algorithm 2) is simple, scalable, and designed to reduce overdependence on priors; this supports robustness goals.
  - Evidence: Random scalarization with normalized positive weights (Eq. 3) makes the approach scalable in n and compatible with standard BO (Section 4.1); good engineering clarity.
  - Evidence: Distinct from πBO by more aggressive decay and explicit ε exploration (Section 4.1; Appendix E.1–E.3), aiming to enable recovery from bad priors; important for technical soundness and practical impact in DL HPO.- Bolded title: **Effective use of cheap approximations through an initial design**
  - Evidence: Initial design uses MOASHA to collect strong max-fidelity seeds, counting equivalent evaluations b += z/z_max, and only passing z_max points to BO (Section 4.2; Algorithm 1); technically sound and practical.
  - Evidence: Ablations show this initial design provides substantial early boost and sustains performance only when paired with ε-BO (Section 5.5; Figure 7), supporting necessity.
  - Evidence: Compute efficiency analysis in Appendix H.3 and resource usage in Appendix K demonstrate practical gains with modest budgets and cores, aligning with desideratum “Strong anytime performance” (Table 1).
  - Why it matters: Leveraging inexpensive proxies to accelerate BO’s warm-start is a practical contribution with clear empirical benefits.- Bolded title: **Comprehensive empirical study with strong anytime/final performance**
  - Evidence: 8 diverse DL surrogates from PD1 and YAHPO-LCBench, 25 seeds, 20 equivalent evaluations per run; hypervolume-based performance and relative ranks across budgets (Section 5.1–5.3; Figures 3–4; Appendix F–G); supports experimental rigor.
  - Evidence: PriMO often leads early and at the end in multi-objective settings (Figures 3–4) and performs strongly in single-objective settings (Figure 6); supports main performance claims.
  - Evidence: “Up to ~10x speedup” illustrated on a language model/transformer benchmark (Figure 1 right), exhibiting rapid hypervolume gains; indicates potential impact.
  - Why it matters: Broad benchmarking and consistent gains strengthen claims of practical utility.- Bolded title: **Robustness to misleading priors**
  - Evidence: Under all-bad priors, PriMO recovers and delivers competitive final performance compared to BO+RW, with strong anytime behavior (Section 5.4; Figure 3 rightmost; Appendix H.2–H.3, Figures 10–12); while πBO+RW struggles (Section 5.4; Figure 5; Appendix H.2, Figure 11); highlights robustness and reliability.
  - Evidence: Design rationale tying rapid decay γ and ε to recovery (Section 4.1; Appendix H.2); supports technical justification for robustness.
  - Evidence: Statistical significance analysis corroborates trends (Appendix I; Figures 13–14); adds rigor.- Bolded title: **Thorough ablation validating each component**
  - Evidence: Systematic ablations removing initial design, priors, or ε-BO demonstrate each component’s contribution (Section 5.5; Figure 7; also Figure in Conclusion Section 8); supports technical soundness of design choices.
  - Evidence: Observation that MOASHA + PriMO sampler is robust but not best emphasizes the complementarity of PriMO’s components (Section 5.5); clarifies design trade-offs.
  - Evidence: Discussion confirms necessity for ε-BO and prior integration (Section 5.5); improves clarity about what drives performance.- Bolded title: **Reproducibility and transparency**
  - Evidence: Algorithms provided as pseudocode (Algorithms 1–3); facilitates clarity and implementation.
  - Evidence: Code repository and raw results availability (Appendix J); increases reproducibility and impact.
  - Evidence: Detailed benchmark specs and reference points (Appendix F; Tables 2–8) and evaluation protocol (Appendix G) enable independent verification; strong experimental rigor.- Bolded title: **Statistical significance analysis**
  - Evidence: Linear Mixed Effects Models, GLRT checks, and Tukey HSD tests with CD diagrams (Appendix I; Figures 13–14) support claims; important for rigor and reliability.
  - Evidence: Checks for seed independence and benchmark informativeness (Appendix I.1) provide sanity; strengthens trust in reported differences.
  - Evidence: CD diagrams reflect early/final performance differences across prior regimes; aligns with main plots (Appendix I.2); clarity.Weaknesses
- Bolded title: **Limited theoretical grounding and some ambiguity in acquisition/scalarization design**
  - Evidence: γ = exp(−n_BO^2/n_d) is introduced without derivation or theoretical analysis; only heuristic justification vs πBO (Section 4.1; Appendix E.1), and reliance reduction “after approximately 10 BO samples” is asserted but not quantified (Section 5.3). This affects technical soundness and generality.
  - Evidence: PriMO chooses one prior uniformly at random per BO step (Algorithm 2, line 2), which may be suboptimal when priors vary in reliability; no justification or comparison to alternative mixing (Section 4.1). This impacts robustness design.
  - Evidence: Algorithm 3 stores both vector objective y and a scalarized y = w^T y (Algorithm 3, lines 10–11), but it is not fully specified how the GP is trained (on scalarized values vs vector with multiple surrogates); Appendix C mentions qLogNoisyEI but not the exact target used for the surrogate. This impacts clarity and reproducibility.
  - Evidence: The ε scheduling is fixed (ε = 0.25, Appendix C) with limited justification; interaction between ε and γ is only qualitatively discussed (Appendix H.2), lacking principled guidance; affects technical soundness and tuning effort.
  - Evidence: The prior definition π_{f_i}(λ) = P(f_i(λ) = min_{λ′∈Λ} f_i(λ′)) (Section 2, Eq. 1) does not clearly define a usable density over λ and conflicts with later treatment of priors as PDFs over λ in the acquisition (Section 4.1; Eq. 4) and Gaussian priors over λ (Appendix D); affects foundational correctness and clarity.
  - Evidence: Algorithm 2 uses n_d to compute γ but does not take n_d as input, and includes η in the function signature without using it (Algorithm 2, lines 1–3, 10); undermines implementability.
  - Evidence: Section 4.2 states the initial design “choose[s] one of the priors … uniformly at random during each iteration” but Algorithm 1 does not incorporate priors in MOASHA sampling (Algorithm 1, line 4); discrepancy affects method clarity.- Bolded title: **Prior construction may not reflect realistic multi-objective expert knowledge**
  - Evidence: Priors are factorized per objective and ignore inter-objective trade-offs/correlation (Section 2; Eq. 1; Limitations Section 7) although MO problems hinge on Pareto trade-offs; impacts external validity and novelty.
  - Evidence: “Good”/“bad” priors are generated by ranking 100,000 random configs on surrogates and picking best/worst, then perturbing with Gaussian noise (Appendix D), which may overestimate quality and does not match how practitioners form priors; impacts realism and generalization.
  - Evidence: Only Gaussian priors are used in experiments (Limitations Section 7; Appendix D), even though PriMO could support other distributions; limits scope and robustness claims.
  - Evidence: The paper suggests possible Pareto-front-based priors but does not implement or simulate them (Limitations Section 7); limits exploration of the core premise of MO priors.
  - Evidence: Appendix D describes perturbation with σ = 0.01 for good priors and no perturbation for bad priors, but then sets σ = 0.25 for all priors when forming Gaussian distributions (Appendix D); inconsistency affects interpretation of prior strength.- Bolded title: **Baseline coverage omits strong MO-BO acquisition methods**
  - Evidence: Baselines include BO+RW, ParEGO, MOASHA, HB+RW, NSGA-II (Section 5.2), but EHVI/qEHVI and information-theoretic MESMO/PESMO—discussed in Related Work (Appendix B.3; Daulton 2020; Belakaria 2019; Hernández-Lobato 2016)—are not compared; impacts experimental rigor.
  - Evidence: MO-TPE-based baselines (Ozaki et al., 2020) and SMS-EGO (Ponweiser et al., 2008) are referenced (Appendix B.3) but absent from experiments (Section 5.2); undermines claims of state-of-the-art.
  - Evidence: πBO baseline in NePS uses a different decay schedule (Appendix E.1) than original ICLR’22 (β/n vs e^{−n_BO/n_d}), potentially weakening πBO+RW (Appendix E.1–E.3); affects fairness of prior-guided MO baselines.
  - Evidence: No comparison to recent random hypervolume scalarization BO (Golovin & Zhang, 2020) beyond linear scalarization (Eq. 3); impacts novelty contextualization.- Bolded title: **Surrogate-only evaluation and limited real-world validation**
  - Evidence: All benchmarks are surrogates (PD1, YAHPO-LCBench; Appendix F) and experiments run only on CPUs (Appendix K), with no real training runs; may not capture noise, instability, or wall-clock characteristics; impacts external validity.
  - Evidence: The “~10x speedup” claim is shown on a transformer surrogate (Figure 1 right) without a precise target definition or real DL training confirmation; impacts impact claims.
  - Evidence: Objectives (validation error and training cost) are surrogate-provided (Appendix F), and PD1 has single epoch fidelity (Appendix F.1), potentially limiting exploration of multi-fidelity behavior in real DL; affects generality.- Bolded title: **Evaluation protocol and fairness concerns**
  - Evidence: PriMO’s initial design size is fixed at 5 (Appendix C) while BO baselines use initial design size equal to search space dimension (Appendix E.1–E.2); this asymmetry can influence early performance comparisons (Figures 3–4); affects fairness.
  - Evidence: For MF methods, hypervolume is plotted only when an equivalent full evaluation occurs (Appendix G), which may differentially favor algorithms depending on promotion strategies; requires careful justification; impacts experimental rigor.
  - Evidence: PriMO uses qLogNoisyEI (Appendix C), but ParEGO and BO+RW—depending on implementations—may use different acquisition defaults (Appendix E.2), introducing confounders; clarity needed for consistent surrogate settings; impacts rigor.
  - Evidence: Scalarization weights are sampled once per run for PriMO (Algorithm 3, line 3) and for BO+RW “at the beginning” (Appendix E.2–E.3); but consistency of weight resampling across iterations differs among baselines; could affect comparative behavior; clarity concern.- Bolded title: **Ambiguity in speedup quantification**
  - Evidence: “~10x speedup” (Figure 1 right; Section 1 and 6) is not formally defined (e.g., evaluations to reach threshold HV), nor statistically validated; impacts clarity and validity of headline claim.
  - Evidence: No table summarizing speedup thresholds across benchmarks; only a single illustrative plot (Figure 1 right); limited evidence base; impacts impact assessment.
  - Evidence: Relation of speedup to prior quality not quantitatively analyzed beyond qualitative statements (Section 5.3); affects interpretability.- Bolded title: **Limited sensitivity analysis of key hyperparameters**
  - Evidence: ε fixed at 0.25 (Appendix C) without sensitivity study; only qualitative discussion of its role (Appendix H.2); impacts robustness.
  - Evidence: γ decay schedule depends on n_d and n_BO (Section 4.1; Appendix E.1), but no systematic analysis of its effect across dimensions/benchmarks; technical soundness concern.
  - Evidence: Initial design size fixed (Appendix C) without tuning study across tasks; potential overfitting to chosen value; affects generality.Suggestions for Improvement
- Bolded title: **Strengthen theoretical and design clarity for the acquisition/scalarization**
  - Provide a derivation or empirical justification study for γ = exp(−n_BO^2/n_d), including plots showing prior influence vs n_BO across several n_d values and resulting performance (anchor to Section 4.1; add a new appendix with sensitivity curves).
  - Evaluate alternative prior mixing strategies (e.g., reliability-weighted selection, ensemble of priors, or combining all priors) instead of uniform random choice (Algorithm 2), and report ablations (extend Section 5.5 and Figure 7).
  - Explicitly state whether the GP surrogate is trained on scalarized y = w^T y or separate per-objective surrogates; provide implementation details aligning Algorithm 3 and Appendix C (clarify lines 10–11), and release configuration files to reproduce this choice (Appendix J).
  - Analyze the interaction between ε and γ, including schedules or adaptive schemes, and report their effect on recovery under bad priors (extend Appendix H.2 with quantitative experiments).
  - Revise the prior definition to a proper density over λ (update Section 2, Eq. 1) consistent with acquisition weighting (Section 4.1; Eq. 4) and the Gaussian priors used (Appendix D), and provide clarifying text.
  - Fix pseudocode by passing n_d into moprior_bo (Algorithm 2) and removing unused η, with a note in Appendix C detailing the implemented signatures.
  - Clarify whether priors are used in the initial design; if not, update Section 4.2 wording or augment Algorithm 1 to incorporate prior-informed sampling in MOASHA, and provide ablations for both variants.- Bolded title: **Improve realism and breadth of multi-objective priors**
  - Evaluate priors that encode inter-objective trade-offs (e.g., priors built from an approximate Pareto front) alongside factorized priors (Limitations Section 7), and compare performance under both (add experiments to Section 5.4).
  - Replace “best/worst from 100k random” with practitioner-inspired priors (e.g., ranges from domain guidelines or historical configs) and measure robustness to misspecification (Appendix D; add a new subsection).
  - Test non-Gaussian priors (e.g., multimodal or uniform over regions) since PriMO supports any distribution (Limitations Section 7), and report results (extend Section 5.3/5.4).
  - Add experiments where priors are constructed from partial observations of the Pareto set (e.g., top-k points from a short pilot run) to simulate real workflows (extend Appendix H).
  - Resolve σ inconsistencies by reporting the exact σ used for perturbation and the σ used to define Gaussian priors (Appendix D), and assess sensitivity to σ choices.- Bolded title: **Expand baseline coverage to stronger MO-BO methods**
  - Include EHVI/qEHVI baselines (Daulton 2020) and report comparative performance (add to Section 5.2; Figures 3–4; Appendix H), given they are standard MO-BO acquisition functions.
  - Add MESMO and PESMO (Appendix B.3) to cover information-theoretic MO-BO approaches; analyze compute and performance trade-offs (Section 5.2; Appendix K).
  - Include MO-TPE and SMS-EGO (Appendix B.3) as additional baselines to mirror related work and strengthen SOTA claims (Section 5.2; Figures 3–4).
  - Align πBO baselines to the original decay schedule (β/n) or report both schedules with a sensitivity analysis (Appendix E.1–E.3), to ensure fairness.- Bolded title: **Validate on real training runs and contextualize speedups**
  - Run at least a subset of experiments on actual DL pipelines (e.g., a small ResNet or transformer) to verify surrogate-based findings (Appendix F.1–F.2) and report wall-clock improvements and stability.
  - Define speedup precisely (e.g., evaluations to reach a fixed HV threshold) and report across multiple benchmarks rather than a single plot (Figure 1 right); add a table summarizing results with confidence intervals.
  - Explore multi-fidelity behavior with true partial training (epochs) and real cost objectives (Appendix F) to verify that initial design advantages translate to real setups.- Bolded title: **Tighten evaluation protocol and baseline fairness**
  - Harmonize initial design sizes across BO-based methods (Appendix C vs Appendix E.1–E.2) or justify asymmetry with ablations showing insensitivity; report controlled comparisons (Section 5.3).
  - Clarify and justify the choice of plotting HV only at equivalent full evaluations for MF methods (Appendix G); add alternative plots showing cumulative HVI over all promotions to ensure a fair picture of progress.
  - Standardize acquisition functions across BO baselines (e.g., qLogNoisyEI where applicable) or document differences explicitly in a table; analyze impact on results (Appendix E.2; add a methods comparison table).
  - Document whether scalarization weights are resampled per iteration for each baseline, and consider running variants with per-iteration resampling to equalize design choices (Algorithm 3; Appendix E.2–E.3).- Bolded title: **Formalize and broaden speedup reporting**
  - Define speedup as the ratio of evaluations needed to reach a common HV target, and report for each benchmark under different prior regimes (Figure 1 right; Section 1); add confidence intervals and significance tests (Appendix I).
  - Provide a supplementary table with speedup metrics and targets, linking to the corresponding HV trajectories (Appendix H), for transparency.
  - Analyze how speedup correlates with prior quality (good/mixed/bad) quantitatively, not only qualitatively (Section 5.3), and include scatter plots or regression analyses.- Bolded title: **Conduct sensitivity analyses for ε, γ, and initial design size**
  - Sweep ε over a range (e.g., 0.0–0.75) and report performance under good/mixed/bad priors (Appendix C; Section 5.4), to guide practitioner tuning.
  - Vary γ schedules (e.g., exp(−n_BO/n_d), exp(−n_BO^2/n_d), β/n) and compare robustness and recovery speed under misleading priors (Section 4.1; Appendix E.1).
  - Sweep initial design size (e.g., 3, 5, 8, dimensionality) and analyze early and final performance across benchmarks (Appendix C; Figures 3–4), documenting recommended defaults.Score
- Overall (10): 6 — Strong empirical anytime performance and practical initial design (Figures 3–4; Section 4.2), but core formulation/implementation ambiguities (Section 2, Eq. 1; Algorithms 1–2–3; Appendix C–D) and baseline gaps (Section 5.2; Appendix B.3) temper impact.
- Novelty (10): 8 — First explicit multi-objective prior integration within BO with MF initial design (Section 2; Section 4; Table 1), supported by broad experiments (Figures 3–6), though priors are factorized and Gaussian (Appendix D; Limitations Section 7).
- Technical Quality (10): 5 — Sound components and ablations (Section 5.5; Figure 7) offset by unresolved theoretical/pseudocode inconsistencies (Section 2, Eq. 1; Algorithms 2–3; Appendix C–D) and limited sensitivity/baseline coverage (Section 5.2; Appendix B.3).
- Clarity (10): 6 — Clear motivation, pseudocode, and protocol (Algorithms 1–3; Appendix G–J), but notable ambiguities in prior definition, surrogate training target, and initial-design/prior usage (Section 2; Section 4.2; Algorithm 3; Appendix C–D).
- Confidence (5): 4 — High confidence based on extensive empirical anchors and released code/results (Section 5; Figures 3–6; Appendix H–I; Appendix J), moderated by formulation and implementation ambiguities and surrogate-only validation (Appendix F–K).