# Innovation & Motivation Report

## 1. The Authors' Claimed Contribution
- Core Problem: Multi-objective deep learning hyperparameter optimization (HPO) under tight evaluation budgets, where practitioners possess expert beliefs about promising hyperparameter regions and cheap approximations (multi-fidelity proxies) are available.
- Claimed Gap: “Introduces PriMO as ‘the first HPO algorithm that can integrate multi-objective user beliefs.’” The Introduction states that “Existing prior-guided HPO research focuses on single-objective; DL often requires multi-objective optimization (e.g., cost, latency, fairness),” and that “many [methods] also ignore cheap approximations (multi-fidelity).”
- Proposed Solution: PriMO, a Bayesian optimization algorithm that:
  - Integrates per-objective expert priors into the acquisition via multiplicative reweighting with a controlled decay, and ε-greedy exploration: “Acquisition weighting: α(λ, D) · π_{f_j}(λ)^γ with γ = exp(−n_BO^2 / n_d)… ε-greedy: with probability ε, use base α(λ, D); with 1−ε, use prior-augmented α” (Method, Section 4.1).
  - Exploits cheap approximations via a multi-fidelity initial design using MOASHA to seed BO at maximum fidelity: “Only max-fidelity evaluations (z = z_max) are included in BO dataset D” (Method, Section 4.2).
  - Manages multiple objectives via random linear scalarization.

## 2. Comparative Scrutiny (The "Trial")
Here, we analyze how the paper stands against the identified similar works.

### vs. Examples of inconsistency in optimization by expected improvement (Yarotsky)
- Identified Overlap: Both are GP/EI-based optimization; the similar work shows EI (with certain kernels) can fail to converge, while PriMO uses q-Log-NoisyEI as its base acquisition.
- Manuscript's Defense: The manuscript does not cite this work. It explicitly introduces safeguards: “ε-greedy: with probability ε, use base α(λ, D); with 1−ε, use prior-augmented α” and “γ = exp(−n_BO^2 / n_d), reducing dependence on priors as BO samples accumulate” (Method, Section 4.1). It also seeds BO via MOASHA to ensure coverage: “Use MOASHA to sample configurations at varying fidelities… Only max-fidelity evaluations (z = z_max) are included in BO dataset D” (Section 4.2).
- Reviewer's Assessment: The mitigation (ε-greedy, prior decay, MF seeding) is a practical, empirically motivated distinction that addresses known brittleness of EI trajectories. It is not a theoretical guarantee against EI failure modes, but combined with diversified scalarization and seeding, it plausibly improves robustness in practice. The difference is operationally significant but methodologically incremental.

### vs. BMOBench: Black-Box Multi-Objective Optimization Benchmarking Platform
- Identified Overlap: Shared benchmarking ethos—hypervolume metrics, Pareto analysis, budgeted comparisons across solvers.
- Manuscript's Defense: The manuscript does not cite BMOBench. It follows standard black-box MOO evaluation protocols: “Metric: mean dominated hypervolume; report relative rankings across budgets… LMEM and CD diagrams” (Experiments, Section 5.1; Appendix I), and compares against established baselines (ParEGO, NSGA-II, MOASHA, etc.).
- Reviewer's Assessment: Overlap is purely evaluative. The manuscript conforms to accepted benchmarking practice; this does not undermine the claimed gap on multi-objective priors or multi-fidelity integration. It strengthens credibility of the empirical methodology but does not affect novelty.

### vs. Stochastic (Approximate) Proximal Point Methods (aProx)
- Identified Overlap: Conceptual alignment on “model-based” robustness—early reliance on approximate models decaying over time to ensure convergence/robustness.
- Manuscript's Defense: The manuscript does not cite aProx. Its distinction is domain-specific and algorithmic: prior-weighted BO acquisition with explicit decay γ and mixing ε, plus multi-fidelity MOASHA seeding and random scalarization (Method, Sections 4.1–4.3). It frames its gap as “the first consideration of expert priors for multiple objectives; naive adaptations are not robust” (Introduction/Contributions).
- Reviewer's Assessment: The resemblance is conceptual. PriMO’s contribution lies in integrating multi-objective expert priors into BO for DL HPO with a particular decay/mixing scheme and MF initial design. This is not addressed by aProx. The distinction is valid; however, it is an engineering synthesis rather than new theory.

### vs. Two-Scale Optimization of Graded Lattice Structures respecting Buckling
- Identified Overlap: Multi-scale/multi-fidelity paradigm—using cheaper analyses early, then validating/refining with high-fidelity evaluations; conservative safeguards.
- Manuscript's Defense: The manuscript does not cite this work. It differentiates by problem setting (DL HPO) and by its specific mechanism: “Use MOASHA… accumulating ‘equivalent full function evaluations’… Only max-fidelity evaluations… are included in BO dataset D,” and “γ… reducing dependence on priors” (Method, Section 4.2; Section 4.1).
- Reviewer's Assessment: Overlap is high-level strategy (multi-fidelity), not method. PriMO’s MF-as-initial-design-only and prior-decay are tailored to BO in DL HPO. The difference is substantial in application; novelty remains domain-specific and method-compositional.

### vs. Optimal design with Gumbel-Softmax for mixed categorical/continuous variables
- Identified Overlap: Probabilistic guidance over mixed-variable spaces with annealing/decay to temper early bias; stochastic exploration.
- Manuscript's Defense: The manuscript does not cite GSM-related optimization. PriMO’s mechanism uses expert priors over λ via π_{f_i}(λ) and decays their influence with γ while mixing exploration through ε-greedy; it does not relax categorical variables for gradients. It explicitly operates in surrogate-based BO with random scalarization and MOASHA seeding (Method, Sections 4.1–4.2).
- Reviewer's Assessment: Similarity is conceptual (probabilistic guidance and annealing), but the technical apparatus and domain differ. The manuscript remains distinct; the claimed gap on multi-objective expert priors in BO is not addressed by GSM approaches.

## 3. Novelty Verdict
- Innovation Type: Incremental
- Assessment:
  The manuscript’s motivation—to integrate multi-objective expert priors into BO for DL HPO and to leverage multi-fidelity proxies via an initial design—addresses a concrete and underexplored gap in practical MOO HPO. The similar works provided are largely conceptual or from different domains; none directly implement multi-objective expert priors in BO for HPO. The paper strengthens its case by:
  - Explicitly studying naive prior integration and showing it is not robust: “Under misleading priors, always sampling causes ‘drastically poor performance’… 50% prior sampling… fails to utilize good priors as effectively” (Motivation, Figure 2).
  - Providing a specific acquisition modification with decay and ε-greedy mixing to leverage yet temper priors: “α(λ, D) · π_{f_j}(λ)^γ; γ = exp(−n_BO^2 / n_d)… ε-greedy…” (Method, Section 4.1).
  - Demonstrating strong empirical performance across multiple benchmarks and prior conditions, including recovery from misleading priors.
  Overall, the paper survives comparison: the existence of the similar works does not materially weaken the claimed gap or the practical significance of the approach, though the novelty is primarily in the composition and careful engineering of known elements (prior weighting, decay schedules, ε-greedy, MF seeding) rather than new theory.
  - Strength:
    - Clear articulation of a practical gap: multi-objective expert priors and robust integration into BO.
    - Empirically grounded motivation via analysis of naive prior strategies.
    - Broad and statistically controlled evaluation across DL benchmarks, with evidence of speedups under good priors and robustness under bad priors.
  - Weakness:
    - The algorithmic novelty is compositional; components (prior weighting, decay, ε-greedy, MF seeding, random scalarization) are known ideas adapted to a new setting, without theoretical guarantees (e.g., against EI failure modes).
    - Claims of “first” integration of multi-objective priors rest on survey of HPO literature; broader optimization works with probabilistic guidance and decay exist conceptually (not cited here), so the novelty is domain-specific rather than foundational.
    - Reliance on linear scalarization and surrogate benchmarks may limit generality; the paper notes potential benefits of hypervolume-based scalarization and acknowledges priors are modeled as Gaussians.

## 4. Key Evidence Anchors
- Abstract: “Introduces PriMO as ‘the first HPO algorithm that can integrate multi-objective user beliefs.’”
- Introduction (Desiderata and gap): “Existing prior-guided HPO research focuses on single-objective; DL often requires multi-objective… PriMO… satisfies all four desiderata; existing algorithms satisfy at most half.”
- Method, Section 4.1 (Acquisition and priors):
  - “Acquisition weighting: α(λ, D) · π_{f_j}(λ)^γ with γ = exp(−n_BO^2 / n_d), reducing dependence on priors as BO samples accumulate.”
  - “ε-greedy: with probability ε, use base α(λ, D); with 1−ε, use prior-augmented α; j ∼ Uniform(1..n).”
  - “Scalarization: random linear weights w_i > 0, sum to 1; minimize Σ_i w_i \hat{f}_i(λ, z_max).”
- Method, Section 4.2 (Initial design): “Use MOASHA… Only max-fidelity evaluations (z = z_max) are included in BO dataset D.”
- Motivation (Naive prior integration): “Under misleading priors, always sampling causes ‘drastically poor performance’… 50% prior sampling… fails to utilize good priors as effectively… motivates PriMO’s design.”
- Experiments (Performance/robustness):
  - “PriMO shows strongest anytime and final performance…” (Figure 3).
  - “PriMO reduces prior dependence after ~10 BO samples due to the chosen γ setting.”
  - “Language Model XFormer hypervolume: PriMO shows ‘~10x speedup’ leveraging good priors.”
- Appendix C (Acquisition choice): “Base acquisition q-Log-NoisyEI (BoTorch).”