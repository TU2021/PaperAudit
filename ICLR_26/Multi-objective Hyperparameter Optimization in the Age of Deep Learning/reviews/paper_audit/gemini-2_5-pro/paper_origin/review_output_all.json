{
  "baseline_review": "1) Summary\nThis paper addresses the problem of multi-objective hyperparameter optimization (HPO) in deep learning, specifically focusing on incorporating expert prior knowledge, which existing algorithms do not support. The authors propose PriMO, a Bayesian optimization (BO) algorithm that integrates user beliefs about the optima of individual objectives. PriMO uses a multi-fidelity optimizer (MOASHA) for an initial design phase to find good starting points, followed by a BO phase where the acquisition function is augmented by a randomly selected, time-decaying prior. The method also includes an exploration mechanism to ensure robustness against misleading priors. Through extensive experiments on eight deep learning benchmarks, the authors demonstrate that PriMO achieves state-of-the-art anytime and final performance in both multi-objective and single-objective settings, effectively leverages good priors, and robustly recovers from bad ones.2) Strengths\n*   **Novel and Well-Motivated Problem Formulation**\n    *   The paper identifies a clear and important gap in the HPO literature: the lack of methods that can leverage expert priors for multiple objectives (Section 1). This is a practical concern for DL practitioners who often balance trade-offs like performance and computational cost.\n    *   The desiderata for modern HPO algorithms presented in Table 1 provide a clear framework for the problem and effectively position the proposed work against existing algorithmic classes. This framing highlights the novelty and necessity of the contribution.\n    *   The motivation is strengthened by a preliminary experiment in Section 3 (Figure 2), which demonstrates that naively adapting an existing multi-objective algorithm (MOASHA) to use priors is not a robust solution, thereby justifying the need for the proposed method.*   **Comprehensive and Rigorous Experimental Evaluation**\n    *   The empirical validation is exceptionally thorough. The method is tested on eight diverse DL benchmarks from established suites (LCBench and PD1), which enhances the generality of the findings (Section 5.1, Appendix F).\n    *   The authors compare PriMO against a wide and appropriate set of baselines, including standard multi-objective optimizers (e.g., ParEGO, NSGA-II), multi-fidelity methods (e.g., MOASHA), and, crucially, several strong baselines constructed by adapting state-of-the-art single-objective prior-based methods to the multi-objective setting (e.g., πBO+RW, MO-Priorband) (Section 5.2).\n    *   The evaluation is systematic, considering various prior conditions (all good, all bad, mixed, and overall), which provides a clear picture of the algorithm's ability to both leverage good information and recover from misleading priors (Section 5.3, Section 5.4, Figure 3, Figure 5).\n    *   The inclusion of a statistical significance analysis using Linear Mixed Effect Models and Critical Difference diagrams (Appendix I, Figures 13 & 14) adds a high degree of rigor and confidence to the empirical claims, which is a commendable practice.*   **Sound and Well-Designed Method with Strong Ablations**\n    *   PriMO is a thoughtful combination of techniques. The use of MOASHA for an initial design (Section 4.2) is a clever strategy to ensure strong anytime performance by providing the BO phase with high-quality seed points.\n    *   The design of the prior-augmented acquisition function (Equation 4) explicitly considers robustness. The fast-decaying influence of the prior (`γ`) and the `ε`-greedy exploration term are simple but effective mechanisms for recovering from bad priors, a key claim supported by the results (Figure 3, \"All priors bad\" panel).\n    *   The ablation study in Section 5.5 (Figure 7) systematically deconstructs PriMO and validates the contribution of each key component (the initial design, the use of priors, and the `ε`-BO strategy). This analysis convincingly shows that the complete design is superior to its parts.*   **High Clarity and Reproducibility**\n    *   The paper is exceptionally well-written and easy to follow. The method is clearly explained and accompanied by pseudocode for all components (Algorithms 1, 2, and 3).\n    *   The appendices are remarkably detailed, providing extensive background (Appendix B), implementation details (Appendix C), and specifics on the experimental setup (Appendices D, E, F, G), which greatly aids in understanding and reproducing the work.\n    *   The authors state that a code repository with scripts and data is provided (Appendix J), demonstrating a strong commitment to reproducibility.3) Weaknesses\n*   **Limited Justification for Key Algorithmic Design Choices**\n    *   The prior decay schedule `γ = exp(−n^2_{BO}/n_d)` (Section 4.1) is a central component for ensuring robustness, but its specific functional form is not well-justified. The paper contrasts it with πBO's schedule but does not provide theoretical reasoning or empirical analysis (e.g., an ablation) to support the choice of a squared term in the exponent over other possible decay functions.\n    *   The exploration parameter `ε` is fixed to 0.25 for all experiments (Appendix C). The performance of PriMO could be sensitive to this hyperparameter, but no sensitivity analysis is provided. This leaves practitioners without guidance on how to set this value for new problems.\n    *   The initial design size `n_init` is set to 5 (Appendix C). This parameter governs the trade-off between the initial multi-fidelity search and the subsequent BO phase. The paper does not explore how this choice impacts overall performance.*   **Ambiguity in the Multi-Objective Handling Strategy**\n    *   The method uses a linear scalarization with random weights `w` that are sampled once at the beginning and then fixed for the entire optimization run (Algorithm 3, line 3). This approach is known to be unable to find solutions on non-convex parts of a Pareto front. While acknowledged as a limitation (Section 7), the paper does not explain why this was chosen over alternatives like re-sampling weights at each iteration, which can alleviate this issue.\n    *   In the BO phase, a single prior `π_{f_j}` is selected uniformly at random to augment the acquisition function (Algorithm 2, line 2). The rationale for this uniform selection is not discussed. It is unclear if this is optimal, especially in scenarios where priors may have varying quality or objectives have different importance.\n    *   The Gaussian Process in the BO phase models the scalarized objective `y` (Algorithm 3, line 11), but the acquisition function is augmented using a prior defined over a single, un-scalarized objective `f_j` (Equation 4). The interaction between modeling a composite scalar function and applying a prior from one of its components is non-trivial and is not explained or analyzed.*   **Potentially Overstated Claims and Minor Presentation Issues**\n    *   Figure 1 and the abstract highlight a \"~10x speedup\" on a language model tuning task. This is a very strong claim based on a single, best-case scenario with a good prior. This result is not representative of the average performance gains across the eight benchmarks in the main experiments and could be misleading if not properly contextualized.\n    *   The primary results in the main paper are presented as relative ranks (Figure 3, Figure 5, Figure 6). While useful for summarizing performance across diverse benchmarks, ranks can obscure the actual magnitude of performance differences. Complementing these with an aggregated plot of normalized hypervolume in the main paper would provide a more complete view.\n    *   The legend for the ablation study in Figure 7 is located in the text block above the figure, and the figure itself appears duplicated (Block #37 and Block #41). This formatting makes it difficult to map the legend entries to the corresponding lines in the plot.4) Suggestions for Improvement\n*   **Provide Justification and Sensitivity Analysis for Design Choices**\n    *   To justify the choice of the decay schedule `γ`, please include a brief ablation study or a more detailed discussion comparing the chosen form to alternatives (e.g., linear decay, `1/n` decay as in πBO) on a subset of benchmarks.\n    *   Please conduct a sensitivity analysis for the exploration parameter `ε` to demonstrate its impact on performance and provide guidance for users on how to set it.\n    *   Similarly, please analyze the effect of the initial design size `n_init` to clarify the trade-off between the multi-fidelity and BO phases.*   **Clarify and Motivate the Multi-Objective Strategy**\n    *   Please add a brief discussion explaining the rationale for fixing the scalarization weights `w` for the entire run (e.g., for simplicity, stability, or computational cost). An experiment comparing this to re-sampling weights at each iteration would be highly valuable.\n    *   Please discuss the uniform random selection of priors during the BO phase. Acknowledge potential alternatives, such as adaptive selection schemes, and justify why the current simple approach is sufficient or preferable.\n    *   Please clarify the interaction between the GP model (which sees the scalarized objective `y`) and the acquisition function augmentation (which uses a prior on a single objective `f_j`). A sentence or two explaining this relationship would improve the technical clarity of the method description.*   **Refine Presentation of Results for Balance and Clarity**\n    *   Please contextualize the \"~10x speedup\" claim in the abstract and Figure 1 by explicitly stating that it represents a best-case result on a specific benchmark under favorable prior conditions.\n    *   To complement the relative rank plots, please consider adding a summary plot showing the mean normalized hypervolume regret (or a similar metric) across all benchmarks in the main paper (e.g., in Section 5.3). This would give readers a better sense of the magnitude of the performance improvements.\n    *   Please correct the formatting of Figure 7 by integrating the legend directly into the figure panel or placing it in the caption to ensure it is clear and unambiguous.5) Score\n*   Overall (10): 9 — The paper introduces a novel and well-motivated method for a practical problem, supported by exceptionally strong and rigorous experiments.\n*   Novelty (10): 9 — The work is the first to formally introduce and solve the problem of multi-objective HPO with expert priors.\n*   Technical Quality (10): 8 — The method is sound and the experimental protocol is excellent, though some key design choices lack rigorous justification or sensitivity analysis.\n*   Clarity (10): 9 — The paper is extremely well-written and structured, with extensive appendices, though minor improvements to figure presentation are possible.\n*   Confidence (5): 5 — I am highly confident in my assessment as I am very familiar with the literature on Bayesian optimization, multi-fidelity methods, and HPO.",
  "final_review": "1) Summary\nThis paper addresses the problem of multi-objective hyperparameter optimization (HPO) in deep learning, specifically focusing on incorporating expert prior knowledge, which existing algorithms do not support. The authors propose PriMO, a Bayesian optimization (BO) algorithm that integrates user beliefs about the optima of individual objectives. PriMO uses a multi-fidelity optimizer (MOASHA) for an initial design phase to find good starting points, followed by a BO phase where the acquisition function is augmented by a randomly selected, time-decaying prior. The method also includes an exploration mechanism to ensure robustness against misleading priors. Through extensive experiments on eight deep learning benchmarks, the authors demonstrate that PriMO achieves state-of-the-art anytime and final performance in both multi-objective and single-objective settings, effectively leverages good priors, and robustly recovers from bad ones.2) Strengths\n*   **Novel and Well-Motivated Problem Formulation**\n    *   The paper identifies a clear and important gap in the HPO literature: the lack of methods that can leverage expert priors for multiple objectives (Section 1). This is a practical concern for DL practitioners who often balance trade-offs like performance and computational cost.\n    *   The desiderata for modern HPO algorithms presented in Table 1 provide a clear framework for the problem and effectively position the proposed work against existing algorithmic classes. This framing highlights the novelty and necessity of the contribution.\n    *   The motivation is strengthened by a preliminary experiment in Section 3 (Figure 2), which demonstrates that naively adapting an existing multi-objective algorithm (MOASHA) to use priors is not a robust solution, thereby justifying the need for the proposed method.*   **Comprehensive and Rigorous Experimental Evaluation**\n    *   The empirical validation is exceptionally thorough. The method is tested on eight diverse DL benchmarks from established suites (LCBench and PD1), which enhances the generality of the findings (Section 5.1, Appendix F).\n    *   The authors compare PriMO against a wide and appropriate set of baselines, including standard multi-objective optimizers (e.g., ParEGO, NSGA-II), multi-fidelity methods (e.g., MOASHA), and, crucially, several strong baselines constructed by adapting state-of-the-art single-objective prior-based methods to the multi-objective setting (e.g., πBO+RW, MO-Priorband) (Section 5.2).\n    *   The evaluation is systematic, considering various prior conditions (all good, all bad, mixed, and overall), which provides a clear picture of the algorithm's ability to both leverage good information and recover from misleading priors (Section 5.3, Section 5.4, Figure 3, Figure 5).\n    *   The inclusion of a statistical significance analysis using Linear Mixed Effect Models and Critical Difference diagrams (Appendix I, Figures 13 & 14) demonstrates a commitment to rigorous validation, which is a commendable practice.*   **Sound and Well-Designed Method with Strong Ablations**\n    *   PriMO is a thoughtful combination of techniques. The use of MOASHA for an initial design (Section 4.2) is a clever strategy to ensure strong anytime performance by providing the BO phase with high-quality seed points.\n    *   The design of the prior-augmented acquisition function (Equation 4) explicitly considers robustness. The fast-decaying influence of the prior (`γ`) and the `ε`-greedy exploration term are simple but effective mechanisms for recovering from bad priors, a key claim supported by the results (Figure 3, \"All priors bad\" panel).\n    *   The ablation study in Section 5.5 (Figure 7) systematically deconstructs PriMO and provides evidence for the utility of the main components, showing the full design outperforms the tested ablations.*   **High Clarity and Reproducibility**\n    *   The paper is well-written and the high-level structure is easy to follow. The method is clearly explained and accompanied by pseudocode for all components (Algorithms 1, 2, and 3).\n    *   The appendices are remarkably detailed, providing extensive background (Appendix B), implementation details (Appendix C), and specifics on the experimental setup (Appendices D, E, F, G), which greatly aids in understanding and reproducing the work.\n    *   The authors state that a code repository with scripts and data is provided (Appendix J), demonstrating a strong commitment to reproducibility.3) Weaknesses\n*   **Limited Justification for Key Algorithmic Design Choices**\n    *   The prior decay schedule `γ = exp(−n^2_{BO}/n_d)` (Section 4.1) is a central component for ensuring robustness, but its specific functional form is not well-justified. The paper contrasts it with πBO's schedule but does not provide theoretical reasoning or empirical analysis (e.g., an ablation) to support the choice of a squared term in the exponent over other possible decay functions.\n    *   The exploration parameter `ε` is fixed to 0.25 for all experiments (Appendix C). The performance of PriMO could be sensitive to this hyperparameter, but no sensitivity analysis is provided. This leaves practitioners without guidance on how to set this value for new problems.\n    *   The initial design size `n_init` is set to 5 (Appendix C). This parameter governs the trade-off between the initial multi-fidelity search and the subsequent BO phase. The paper does not explore how this choice impacts overall performance.*   **Ambiguity in the Multi-Objective Handling Strategy**\n    *   The method uses a linear scalarization with random weights `w` that are sampled once at the beginning and then fixed for the entire optimization run (Algorithm 3, line 3). This approach effectively converts the multi-objective problem into a single-objective one for each run and is known to be unable to find solutions on non-convex parts of a Pareto front. While acknowledged as a limitation (Section 7), the paper does not explain why this was chosen over alternatives like re-sampling weights at each iteration.\n    *   In the BO phase, a single prior `π_{f_j}` is selected uniformly at random to augment the acquisition function (Algorithm 2, line 2). The rationale for this uniform selection is not discussed. It is unclear if this is optimal, especially in scenarios where priors may have varying quality or objectives have different importance.\n    *   The Gaussian Process in the BO phase models the scalarized objective `y` (Algorithm 3, line 11), but the acquisition function is augmented using a prior defined over a single, un-scalarized objective `f_j` (Equation 4). The interaction between modeling a composite scalar function and applying a prior from one of its components is non-trivial and is not explained or analyzed.*   **Inconsistent Reporting and Analysis of Results**\n    *   The textual summary of the ablation study misrepresents the results shown in the corresponding figure. Section 5.5 claims that \"PriMO without Priors ... [is one] of the worst performing ablations overall\", but Figure 7 shows that this variant is the second-best performer, significantly outperforming other ablations like \"PriMO w/o ε-BO\". This contradiction undermines the paper's argument about the necessity of the prior-based components.\n    *   The explanation for a key baseline's performance is contradictory. Appendix H.2 attributes the failure of `πBO+RW` under bad priors to its \"slow decay schedule\". However, Appendix E.1 explicitly states that the implementation used has a fast exponential decay schedule (`e^{-n_{BO}/n_d}`), not the slow schedule from the original πBO paper. This inconsistency calls into question the validity of the comparison.*   **Potentially Overstated Claims and Minor Presentation Issues**\n    *   Figure 1 and the introduction highlight a \"~10x speedup\" on a language model tuning task. This claim is based on a single, best-case scenario run for 100 evaluations, which is inconsistent with the 20-evaluation budget used for all main experiments (Section 5.1). This result is not representative and could be misleading.\n    *   The interpretation of the statistical analysis is flawed. Appendix I.2 claims that under bad priors, PriMO is \"significantly better than all baselines\" except one. However, the corresponding Critical Difference diagram (Figure 14, \"All priors bad\") shows that PriMO's performance is statistically indistinguishable from several other baselines (ParEGO, MOASHA, HB+RW).\n    *   The primary results in the main paper are presented as relative ranks (Figure 3, Figure 5, Figure 6). While useful for summarizing performance, ranks can obscure the actual magnitude of performance differences. Complementing these with an aggregated plot of normalized hypervolume in the main paper would provide a more complete view.4) Suggestions for Improvement\n*   **Provide Justification and Sensitivity Analysis for Design Choices**\n    *   To justify the choice of the decay schedule `γ`, please include a brief ablation study or a more detailed discussion comparing the chosen form to alternatives (e.g., linear decay, `1/n` decay as in πBO) on a subset of benchmarks.\n    *   Please conduct a sensitivity analysis for the exploration parameter `ε` to demonstrate its impact on performance and provide guidance for users on how to set it.\n    *   Similarly, please analyze the effect of the initial design size `n_init` to clarify the trade-off between the multi-fidelity and BO phases.*   **Clarify and Motivate the Multi-Objective Strategy**\n    *   Please add a brief discussion explaining the rationale for fixing the scalarization weights `w` for the entire run (e.g., for simplicity, stability, or computational cost). An experiment comparing this to re-sampling weights at each iteration would be highly valuable.\n    *   Please discuss the uniform random selection of priors during the BO phase. Acknowledge potential alternatives, such as adaptive selection schemes, and justify why the current simple approach is sufficient or preferable.\n    *   Please clarify the interaction between the GP model (which sees the scalarized objective `y`) and the acquisition function augmentation (which uses a prior on a single objective `f_j`). A sentence or two explaining this relationship would improve the technical clarity of the method description.*   **Correct Inconsistencies in Reporting and Analysis**\n    *   Please revise the textual summary of the ablation study in Section 5.5 to accurately reflect the results presented in Figure 7. Specifically, the strong performance of the \"PriMO w/o Priors\" variant should be correctly described.\n    *   Please correct the contradictory explanation for the performance of the `πBO+RW` baseline. The analysis in Appendix H.2 should be consistent with the implementation details provided in Appendix E.1.*   **Refine Presentation of Results for Balance and Clarity**\n    *   Please contextualize the \"~10x speedup\" claim in the introduction and Figure 1 by explicitly stating that it represents a best-case result on a specific benchmark under a non-standard evaluation budget.\n    *   Please revise the claims in Appendix I.2 to accurately reflect the statistical groupings shown in the Critical Difference diagrams in Figure 14.\n    *   To complement the relative rank plots, please consider adding a summary plot showing the mean normalized hypervolume regret (or a similar metric) across all benchmarks in the main paper (e.g., in Section 5.3).5) Score\n*   Overall (10): 6 — The paper addresses a novel and practical problem with a well-motivated method, but the empirical claims are weakened by several internal inconsistencies and misrepresentations of results.\n*   Novelty (10): 9 — The work is the first to formally introduce and solve the problem of multi-objective HPO with expert priors.\n*   Technical Quality (10): 5 — The experimental protocol is extensive, but significant issues like contradictory reporting of ablation and baseline results (Section 5.5, Appendix H.2) and flawed statistical claims (Appendix I.2) undermine the paper's technical soundness.\n*   Clarity (10): 7 — The paper is well-structured and generally easy to read, but the presence of major internal contradictions between the text and figures (e.g., Section 5.5 vs. Figure 7) creates significant confusion.\n*   Confidence (5): 5 — I am highly confident in my assessment as I am very familiar with the literature on Bayesian optimization, multi-fidelity methods, and HPO.",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 9,
        "novelty": 9,
        "technical_quality": 8,
        "clarity": 9,
        "confidence": 5
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 6,
        "novelty": 9,
        "technical_quality": 5,
        "clarity": 7,
        "confidence": 5
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "1) Summary\nThis paper addresses the problem of multi-objective hyperparameter optimization (HPO) in deep learning, specifically focusing on incorporating expert prior knowledge, which existing algorithms do not support. The authors propose PriMO, a Bayesian optimization (BO) algorithm that integrates user beliefs about the optima of individual objectives. PriMO uses a multi-fidelity optimizer (MOASHA) for an initial design phase to find good starting points, followed by a BO phase where the acquisition function is augmented by a randomly selected, time-decaying prior. The method also includes an exploration mechanism to ensure robustness against misleading priors. Through extensive experiments on eight deep learning benchmarks, the authors demonstrate that PriMO achieves state-of-the-art anytime and final performance in both multi-objective and single-objective settings, effectively leverages good priors, and robustly recovers from bad ones.2) Strengths\n*   **Novel and Well-Motivated Problem Formulation**\n    *   The paper identifies a clear and important gap in the HPO literature: the lack of methods that can leverage expert priors for multiple objectives (Section 1). This is a practical concern for DL practitioners who often balance trade-offs like performance and computational cost.\n    *   The desiderata for modern HPO algorithms presented in Table 1 provide a clear framework for the problem and effectively position the proposed work against existing algorithmic classes. This framing highlights the novelty and necessity of the contribution.\n    *   The motivation is strengthened by a preliminary experiment in Section 3 (Figure 2), which demonstrates that naively adapting an existing multi-objective algorithm (MOASHA) to use priors is not a robust solution, thereby justifying the need for the proposed method.*   **Comprehensive and Rigorous Experimental Evaluation**\n    *   The empirical validation is exceptionally thorough. The method is tested on eight diverse DL benchmarks from established suites (LCBench and PD1), which enhances the generality of the findings (Section 5.1, Appendix F).\n    *   The authors compare PriMO against a wide and appropriate set of baselines, including standard multi-objective optimizers (e.g., ParEGO, NSGA-II), multi-fidelity methods (e.g., MOASHA), and, crucially, several strong baselines constructed by adapting state-of-the-art single-objective prior-based methods to the multi-objective setting (e.g., πBO+RW, MO-Priorband) (Section 5.2).\n    *   The evaluation is systematic, considering various prior conditions (all good, all bad, mixed, and overall), which provides a clear picture of the algorithm's ability to both leverage good information and recover from misleading priors (Section 5.3, Section 5.4, Figure 3, Figure 5).\n    *   The inclusion of a statistical significance analysis using Linear Mixed Effect Models and Critical Difference diagrams (Appendix I, Figures 13 & 14) demonstrates a commitment to rigorous validation, which is a commendable practice.*   **Sound and Well-Designed Method with Strong Ablations**\n    *   PriMO is a thoughtful combination of techniques. The use of MOASHA for an initial design (Section 4.2) is a clever strategy to ensure strong anytime performance by providing the BO phase with high-quality seed points.\n    *   The design of the prior-augmented acquisition function (Equation 4) explicitly considers robustness. The fast-decaying influence of the prior (`γ`) and the `ε`-greedy exploration term are simple but effective mechanisms for recovering from bad priors, a key claim supported by the results (Figure 3, \"All priors bad\" panel).\n    *   The ablation study in Section 5.5 (Figure 7) systematically deconstructs PriMO and provides evidence for the utility of the main components, showing the full design outperforms the tested ablations.*   **High Clarity and Reproducibility**\n    *   The paper is well-written and the high-level structure is easy to follow. The method is clearly explained and accompanied by pseudocode for all components (Algorithms 1, 2, and 3).\n    *   The appendices are remarkably detailed, providing extensive background (Appendix B), implementation details (Appendix C), and specifics on the experimental setup (Appendices D, E, F, G), which greatly aids in understanding and reproducing the work.\n    *   The authors state that a code repository with scripts and data is provided (Appendix J), demonstrating a strong commitment to reproducibility.3) Weaknesses\n*   **Limited Justification for Key Algorithmic Design Choices**\n    *   The prior decay schedule `γ = exp(−n^2_{BO}/n_d)` (Section 4.1) is a central component for ensuring robustness, but its specific functional form is not well-justified. The paper contrasts it with πBO's schedule but does not provide theoretical reasoning or empirical analysis (e.g., an ablation) to support the choice of a squared term in the exponent over other possible decay functions.\n    *   The exploration parameter `ε` is fixed to 0.25 for all experiments (Appendix C). The performance of PriMO could be sensitive to this hyperparameter, but no sensitivity analysis is provided. This leaves practitioners without guidance on how to set this value for new problems.\n    *   The initial design size `n_init` is set to 5 (Appendix C). This parameter governs the trade-off between the initial multi-fidelity search and the subsequent BO phase. The paper does not explore how this choice impacts overall performance.*   **Ambiguity in the Multi-Objective Handling Strategy**\n    *   The method uses a linear scalarization with random weights `w` that are sampled once at the beginning and then fixed for the entire optimization run (Algorithm 3, line 3). This approach effectively converts the multi-objective problem into a single-objective one for each run and is known to be unable to find solutions on non-convex parts of a Pareto front. While acknowledged as a limitation (Section 7), the paper does not explain why this was chosen over alternatives like re-sampling weights at each iteration.\n    *   In the BO phase, a single prior `π_{f_j}` is selected uniformly at random to augment the acquisition function (Algorithm 2, line 2). The rationale for this uniform selection is not discussed. It is unclear if this is optimal, especially in scenarios where priors may have varying quality or objectives have different importance.\n    *   The Gaussian Process in the BO phase models the scalarized objective `y` (Algorithm 3, line 11), but the acquisition function is augmented using a prior defined over a single, un-scalarized objective `f_j` (Equation 4). The interaction between modeling a composite scalar function and applying a prior from one of its components is non-trivial and is not explained or analyzed.*   **Inconsistent Reporting and Analysis of Results**\n    *   The textual summary of the ablation study misrepresents the results shown in the corresponding figure. Section 5.5 claims that \"PriMO without Priors ... [is one] of the worst performing ablations overall\", but Figure 7 shows that this variant is the second-best performer, significantly outperforming other ablations like \"PriMO w/o ε-BO\". This contradiction undermines the paper's argument about the necessity of the prior-based components.\n    *   The explanation for a key baseline's performance is contradictory. Appendix H.2 attributes the failure of `πBO+RW` under bad priors to its \"slow decay schedule\". However, Appendix E.1 explicitly states that the implementation used has a fast exponential decay schedule (`e^{-n_{BO}/n_d}`), not the slow schedule from the original πBO paper. This inconsistency calls into question the validity of the comparison.*   **Potentially Overstated Claims and Minor Presentation Issues**\n    *   Figure 1 and the introduction highlight a \"~10x speedup\" on a language model tuning task. This claim is based on a single, best-case scenario run for 100 evaluations, which is inconsistent with the 20-evaluation budget used for all main experiments (Section 5.1). This result is not representative and could be misleading.\n    *   The interpretation of the statistical analysis is flawed. Appendix I.2 claims that under bad priors, PriMO is \"significantly better than all baselines\" except one. However, the corresponding Critical Difference diagram (Figure 14, \"All priors bad\") shows that PriMO's performance is statistically indistinguishable from several other baselines (ParEGO, MOASHA, HB+RW).\n    *   The primary results in the main paper are presented as relative ranks (Figure 3, Figure 5, Figure 6). While useful for summarizing performance, ranks can obscure the actual magnitude of performance differences. Complementing these with an aggregated plot of normalized hypervolume in the main paper would provide a more complete view.4) Suggestions for Improvement\n*   **Provide Justification and Sensitivity Analysis for Design Choices**\n    *   To justify the choice of the decay schedule `γ`, please include a brief ablation study or a more detailed discussion comparing the chosen form to alternatives (e.g., linear decay, `1/n` decay as in πBO) on a subset of benchmarks.\n    *   Please conduct a sensitivity analysis for the exploration parameter `ε` to demonstrate its impact on performance and provide guidance for users on how to set it.\n    *   Similarly, please analyze the effect of the initial design size `n_init` to clarify the trade-off between the multi-fidelity and BO phases.*   **Clarify and Motivate the Multi-Objective Strategy**\n    *   Please add a brief discussion explaining the rationale for fixing the scalarization weights `w` for the entire run (e.g., for simplicity, stability, or computational cost). An experiment comparing this to re-sampling weights at each iteration would be highly valuable.\n    *   Please discuss the uniform random selection of priors during the BO phase. Acknowledge potential alternatives, such as adaptive selection schemes, and justify why the current simple approach is sufficient or preferable.\n    *   Please clarify the interaction between the GP model (which sees the scalarized objective `y`) and the acquisition function augmentation (which uses a prior on a single objective `f_j`). A sentence or two explaining this relationship would improve the technical clarity of the method description.*   **Correct Inconsistencies in Reporting and Analysis**\n    *   Please revise the textual summary of the ablation study in Section 5.5 to accurately reflect the results presented in Figure 7. Specifically, the strong performance of the \"PriMO w/o Priors\" variant should be correctly described.\n    *   Please correct the contradictory explanation for the performance of the `πBO+RW` baseline. The analysis in Appendix H.2 should be consistent with the implementation details provided in Appendix E.1.*   **Refine Presentation of Results for Balance and Clarity**\n    *   Please contextualize the \"~10x speedup\" claim in the introduction and Figure 1 by explicitly stating that it represents a best-case result on a specific benchmark under a non-standard evaluation budget.\n    *   Please revise the claims in Appendix I.2 to accurately reflect the statistical groupings shown in the Critical Difference diagrams in Figure 14.\n    *   To complement the relative rank plots, please consider adding a summary plot showing the mean normalized hypervolume regret (or a similar metric) across all benchmarks in the main paper (e.g., in Section 5.3).5) Score\n*   Overall (10): 6 — The paper addresses a novel and practical problem with a well-motivated method, but the empirical claims are weakened by several internal inconsistencies and misrepresentations of results.\n*   Novelty (10): 9 — The work is the first to formally introduce and solve the problem of multi-objective HPO with expert priors.\n*   Technical Quality (10): 5 — The experimental protocol is extensive, but significant issues like contradictory reporting of ablation and baseline results (Section 5.5, Appendix H.2) and flawed statistical claims (Appendix I.2) undermine the paper's technical soundness.\n*   Clarity (10): 7 — The paper is well-structured and generally easy to read, but the presence of major internal contradictions between the text and figures (e.g., Section 5.5 vs. Figure 7) creates significant confusion.\n*   Confidence (5): 5 — I am highly confident in my assessment as I am very familiar with the literature on Bayesian optimization, multi-fidelity methods, and HPO."
}