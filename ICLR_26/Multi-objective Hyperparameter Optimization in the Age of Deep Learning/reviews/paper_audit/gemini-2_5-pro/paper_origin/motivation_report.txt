# Innovation & Motivation Report

## 1. The Authors' Claimed Contribution
- **Core Problem**: To perform efficient Hyperparameter Optimization (HPO) for Deep Learning (DL) models when there are multiple, often conflicting, objectives to optimize (e.g., model accuracy vs. training cost).
- **Claimed Gap**: The authors claim that while DL practitioners often have strong prior beliefs about good hyperparameter regions, "no existing Hyperparameter Optimization (HPO) algorithms can leverage this knowledge for multiple objectives." They present a desiderata table (Table 1) to argue that existing methods in categories like Evolutionary Algorithms, Multi-Objective Multi-Fidelity (MOMF), and Multi-Objective Bayesian Optimization (MO-BO) fail to integrate multi-objective expert priors.
- **Proposed Solution**: The paper introduces PriMO, a Bayesian Optimization (BO) algorithm. It first uses a multi-fidelity method (MOASHA) to generate a strong set of initial configurations. It then proceeds with a BO loop where the multi-objective problem is scalarized using random weights. The key innovation is an acquisition function augmented by a user-specified prior, `α(λ, D) * π_{f_j}(λ)^γ`, where a prior `π_{f_j}` corresponding to one of the objectives is chosen randomly at each step. The influence of the prior (`γ`) decays over time, and an ε-greedy mechanism is included for robustness against misleading priors.

## 2. Comparative Scrutiny (The "Trial")
*Here, we analyze how the paper stands against the identified similar works.*

### vs. πBO (Various Titles)
- **Identified Overlap**: The provided analysis repeatedly and correctly identifies that PriMO's core mechanism for incorporating user knowledge is a direct generalization of the technique introduced in πBO. Both algorithms multiply a standard BO acquisition function by a user-provided prior distribution raised to a decaying exponent (`α * π^γ`). PriMO applies this principle within a multi-objective framework.

- **Manuscript's Defense**: The authors explicitly acknowledge this lineage.
    1.  **Citation and Context**: In the "Related Work" section, the manuscript cites πBO, correctly positioning it as a method for *single-objective* optimization: "User priors for single-objective optimization: Methods like Priorband and πBO exist."
    2.  **Baseline Comparison**: The authors construct a direct multi-objective adaptation of πBO, named "πBO+RW" (πBO with Random Weighting scalarization), and include it as a key baseline in their experiments.
    3.  **Demonstrating Non-Triviality**: The manuscript argues that a naive extension is insufficient. The "Motivation" section (Figure 2) shows that a simple strategy of sampling from priors within MOASHA is not robust to bad priors. Furthermore, the main experimental results (Figure 3, Figure 5) demonstrate that PriMO significantly outperforms the more direct "πBO+RW" baseline, especially in the challenging "all bad priors" scenario (RQ4), where PriMO recovers while πBO+RW's performance suffers greatly.

- **Reviewer's Assessment**: The manuscript's defense is successful and compelling. The authors do not hide the intellectual debt to πBO; instead, they frame their work as the first *robust* solution to the more complex multi-objective version of the problem. By showing that naive adaptations fail and that their proposed system (PriMO) significantly outperforms a direct baseline (πBO+RW), they successfully argue that their contribution is not a trivial extension. The combination of the multi-fidelity initial design, the specific prior-handling mechanism within the BO loop, and the robustness components (decay and ε-greedy) constitutes a novel and well-engineered algorithmic contribution. The difference is significant.

## 3. Novelty Verdict
- **Innovation Type**: **Incremental**
- **Assessment**:
  The paper survives the comparison and successfully defends its contribution. The core motivation—that no method exists to robustly leverage expert priors for multi-objective HPO—appears to be valid. While the foundational idea of augmenting an acquisition function with a prior is inherited from πBO, the manuscript convincingly demonstrates that extending this to the multi-objective setting is a non-trivial engineering and algorithmic challenge. The novelty lies not in a new mathematical theory, but in the design of a complete, robust system that solves a well-defined and practical problem that its predecessors did not.

  - **Strength**: The paper clearly identifies a practical gap in a high-impact area (DL HPO). The proposed solution is a complete system, and its components are justified through a thorough ablation study. The empirical evidence is strong, demonstrating superiority over well-chosen baselines, including a direct adaptation of the most similar prior work.
  - **Weakness**: The core inventive step is an adaptation and combination of existing ideas (MOASHA, BO, πBO's prior mechanism) rather than a fundamental breakthrough. The claim to be the "first" is heavily qualified but appears accurate within the specific problem definition.

## 4. Key Evidence Anchors
- **Table 1 (Introduction)**: Explicitly lays out the claimed gap in the literature by comparing PriMO against entire classes of existing algorithms based on four desiderata.
- **Section "Motivation" / Figure 2**: Provides evidence that a naive approach to incorporating priors in a multi-objective setting is not robust, thereby motivating the need for PriMO's more sophisticated design.
- **Section 5.3 / Figures 3 & 5**: Directly compares PriMO against baselines, including the constructed "πBO+RW", showing PriMO's superior performance and robustness, which is the central pillar of its novelty claim.
- **Section 5.5 / Figure 7 (Ablation Study)**: Demonstrates that all components of the PriMO system (initial design, priors, ε-BO) are necessary for its strong performance, supporting the claim that the contribution is the complete system design, not just one part.
- **Section "Related Work"**: Acknowledges the existence of single-objective prior-based methods like πBO, showing awareness of the state of the art and correctly framing the manuscript's contribution relative to it.