Summary
The paper introduces PriMO, a multi-objective hyperparameter optimization (HPO) method that integrates expert priors and cheap approximations to deliver strong anytime performance under tight budgets. PriMO factorizes expert priors over individual objectives, seeds the search using a multi-fidelity MOASHA initial design, and then runs an ε-greedy Bayesian optimization phase with random scalarization. In each BO iteration, the acquisition function is multiplied by one randomly selected per-objective prior, whose influence is aggressively decayed via γ = exp(−n_BO^2/n_d) to mitigate harm from misleading priors. Experiments on eight deep-learning surrogate benchmarks compare PriMO with multi-objective baselines and prior-augmented variants, reporting hypervolume-based anytime and final performance, ablations that isolate the contributions of each component, and linear mixed-effects-model significance analyses. Results show strong performance and robustness to misleading priors, including in the single-objective special case.

Strengths
- Clear and practical problem formulation focused on DL HPO under tight budgets, with explicit desiderata and an algorithm designed to meet them.
- Simple, implementable approach that extends prior-based HPO from single- to multi-objective settings while remaining scalable via random scalarization.
- Effective combination of multi-fidelity initial seeding (MOASHA) for rapid early progress and an ε-greedy, fast-decaying prior-weighted BO phase to exploit helpful priors while recovering from poor ones.
- Strong empirical results across eight diverse surrogate benchmarks: improved anytime and final hypervolume, robustness to misleading priors, and benefits persisting in the single-objective special case.
- Thorough experimental protocol with many seeds, fixed budgets, hypervolume measured at full fidelity, ablations demonstrating the importance of each component, and statistical significance analysis supporting claims.
- Reproducibility and practicality: public code, detailed experimental settings, and transparency about compute.

Weaknesses
- Prior realism and external validity: priors are derived from large surrogate sweeps and Gaussian perturbations around near-optimal/poor configurations, which can overstate the benefits of “expert beliefs.” All experiments are conducted on surrogates, limiting insight into wall-clock/runtime and real-training behavior. The headline “~10× speedup” is not backed by formal time-to-target metrics.
- Missing strong MOBO baselines: comparisons exclude modern hypervolume-aware and information-theoretic methods (e.g., qEHVI/dEHVI, qNEHVI, MESMO), making it unclear how PriMO fares against state-of-the-art MOBO algorithms that directly target hypervolume improvement.
- Heuristic design without sensitivity analysis: key choices such as using a single randomly selected prior per BO iteration, the aggressive decay schedule γ = exp(−n_BO^2/n_d), and a fixed ε lack theoretical justification and are not systematically probed for sensitivity across budgets and problem dimensions.
- Limited expressiveness of the prior: the factorized per-objective prior cannot encode trade-offs, Pareto-front structure, or reference-point preferences. This can be problematic when per-objective optima are mutually exclusive, and the method provides no mechanism to align prior influence with the scalarization weights used in the acquisition.
- Theoretical underpinnings are minimal: no convergence guarantees or analysis of how the decaying prior and ε-greedy policy affect Pareto-front recovery.
- Minor clarity inconsistencies: the text suggests priors may be sampled during the initial MOASHA seeding, but the provided algorithm does not reflect this; algorithms compute y = w^T y while storing vector outcomes, with the role of the scalar not fully clarified.
