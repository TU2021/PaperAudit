### Summary

The paper introduces **PriMO** (Prior-informed Multi-objective Hyperparameter Optimization), a novel method for hyperparameter optimization (HPO) in deep learning, particularly in multi-objective settings. It addresses a gap in the HPO literature by incorporating **multi-objective expert priors**, which is a major improvement over traditional methods that typically focus on single-objective optimization. PriMO integrates **Bayesian optimization (BO)** with expert priors and employs **cheap approximations** using **MOASHA** for initial design. Through extensive experiments across multiple deep learning benchmarks, the algorithm demonstrates **state-of-the-art performance**, both in **multi-objective** and **single-objective** settings, proving its robustness and efficiency.

---

### Strengths

1. **Novel Contribution**:

   * **PriMO** introduces a **multi-objective HPO framework** that integrates expert priors, a key advancement for deep learning practitioners who often need to optimize multiple objectives simultaneously (e.g., accuracy, latency, cost).

2. **State-of-the-Art Performance**:

   * The algorithm outperforms **multiple baselines** (both single-objective and multi-objective) in **eight deep learning benchmarks**, including tasks like **image classification**, **translation**, and **language modeling**.
   * **Empirical validation** across a range of scenarios demonstrates its robustness to different **prior strengths**.

3. **Practical Relevance**:

   * The authors provide a strong **empirical analysis** of runtime efficiency, confirming that **PriMO’s computational cost is negligible** compared to the cost of deep learning model training.
   * They demonstrate **real-world applicability** with case studies, including a **GPT2-style language model**, highlighting its potential for fine-tuning complex models.

4. **Comprehensive Analysis**:

   * The paper includes an **ablation study** that confirms the importance of each component of the PriMO algorithm, particularly the **acquisition function** and **epsilon-BO** for handling prior uncertainty.

5. **Clear Algorithm Design**:

   * The **acquisition function** and **epsilon-BO** components are well-explained, and their effectiveness in balancing **prior knowledge** with **exploration** is substantiated by experiments.

6. **Extensive Benchmarks**:

   * The authors use a **wide range of benchmarks**, ensuring that the results generalize across different problem types in deep learning (classification, language modeling, etc.).

---

### Weaknesses

1. **Theoretical Analysis**:

   * The **acquisition function** in **PriMO** (Eq. 4) works empirically, but lacks a **theoretical analysis** of its properties. The paper could benefit from **more formal discussion** of its convergence to the Pareto frontier, how **prior strength** affects exploration, and under what conditions the algorithm guarantees optimality.
   * A simple **example** illustrating the benefits of PriMO in a multi-objective bilevel optimization problem could help clarify its advantages.

2. **Runtime Efficiency**:

   * While **runtime analysis** is provided, a deeper **asymptotic complexity** analysis would help readers understand the **scalability** and **computational limits** of PriMO when applied to more complex models or larger-scale optimization problems.

3. **Prior Handling**:

   * The paper claims that PriMO is robust to both **good and bad priors**, but further clarification is needed on how **prior quality** is measured and how to handle **uncertain or ambiguous priors**.
   * The handling of **highly correlated priors** and their impact on optimization could be further explored for better clarity.

4. **Practical Implementation**:

   * The paper lacks in-depth discussion on how **practitioners** can define **multi-objective priors** in real-world applications. The authors briefly mention **Gaussian priors**, but more guidance on how to specify priors for **different problem domains** (especially in **deep learning** applications) would be useful.

5. **Title Exaggeration**:

   * The original title may overstate the novelty of the work, as the algorithm follows established methods (e.g., **PriorBand**), and its main contribution lies in applying expert priors to **multi-objective optimization**.
   * The authors have agreed to adjust the title to **“PriMO: Multi-objective Hyperparameter Optimization with Expert Priors”** to reflect a more accurate scope.

6. **Lack of Diverse Case Studies**:

   * While experiments are extensive, **real-world scenarios** could be explored further, especially for **multi-objective** tasks in complex domains like robotics or healthcare.
