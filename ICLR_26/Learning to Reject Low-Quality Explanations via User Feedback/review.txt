### Summary

The paper introduces a novel framework called **Learning to Reject Low-Quality Explanations (LtX)**, where machine learning models are equipped with a rejector to handle low-quality explanations in high-stakes domains. The key innovation is **ULER (User-centric Low-quality Explanation Rejector)**, which learns a simple rejector using human-annotated data and per-feature relevance judgments. The rejector filters out low-quality explanations, improving the model's robustness. The method is evaluated on eight benchmark datasets and a newly created human-annotated dataset, showing that ULER outperforms existing rejection strategies.

---

### Strengths

1. **Novel Problem Setting**: The paper addresses an important gap in Explainable AI (XAI) by focusing on the **quality of explanations** rather than just the predictions themselves, which is critical for building trust in AI systems, particularly in high-stakes applications.

2. **User-Centric Framework**: ULER's design is based on **human feedback** and **per-feature relevance judgments**, making it a **user-centric** approach to improve explanation quality. This is a significant step forward in making AI explanations more aligned with human expectations.

3. **Extensive Empirical Evaluation**: The authors conduct comprehensive experiments across multiple datasets, including a **new human-annotated dataset** from a real-world soccer analytics task, demonstrating the effectiveness of ULER in rejecting low-quality explanations.

4. **Human-Agnostic Framework**: ULER works with various explanation techniques, such as **SHAP**, and does not rely on a specific model architecture, making it broadly applicable to different explainability methods.

5. **Dataset Contribution**: The release of a **new dataset of human-annotated explanations** is a valuable contribution to the community, enabling future research on aligning machine explanations with human judgments.

---

### Weaknesses

1. **Limited Generalization to Other Explanation Types**:

   * ULER is designed around **feature attribution methods** (e.g., SHAP), which generate **per-feature relevance scores**. This makes ULER less applicable to other forms of explanations, such as **counterfactual explanations** or **example-based explanations**, which do not naturally produce per-feature scores.
   * The paper does not discuss how ULER could be adapted for these other explanation types, limiting the generalizability of the framework.

2. **Reliability of Simulated Human Judgments**:

   * The paper uses **Llama-3.1-8B-Instruct** to simulate **human-quality judgments** for large-scale experiments. While LLMs are used as surrogates for human feedback, their judgments may not align perfectly with human intuition, especially for domain-specific tasks.
   * The lack of validation against **real human annotations** for these simulated judgments raises concerns about the **reliability** and **generalizability** of the results. This issue is not sufficiently addressed in the paper.

3. **Overstatement of "Explainer-Agnostic" Nature**:

   * The claim that ULER is **explainer-agnostic** is an overstatement. The method heavily relies on **feature attribution techniques** like SHAP, making it unsuitable for explanations that do not involve feature importance scores. The paper could benefit from a more nuanced discussion of this limitation.

4. **Human Judgment Assumptions**:

   * ULER assumes that **human experts** can reliably identify important features and judge the quality of explanations. This assumption may not always hold, especially in complex or high-dimensional tasks. The paper could clarify the types of tasks where **human judgment** is feasible and reliable.

5. **Scalability Concerns**:

   * The method requires **per-feature feedback** for each explanation, which can be **time-consuming** and **cognitively demanding** for human annotators, especially in high-dimensional settings with many features.
   * The paper does not adequately address how ULER would scale to datasets with **hundreds or thousands of features**, which could be a significant barrier to practical adoption.

6. **Benchmarking Against Baselines**:

   * Many of the **competing methods** are not specifically designed for **rejection of low-quality explanations**, making the comparison somewhat biased. The paper could be improved by including a more thorough comparison with recent **learning-to-reject** methods that are explicitly designed for this task.

7. **Simplified Metric Choices**:

   * The authors use a **5-point Likert scale** converted into binary labels for human judgment, which **removes nuance** and limits the rejector's ability to handle **borderline or ambiguous cases**. A more fine-grained approach would better capture the complexity of explanation quality.


 
