Summary
The paper introduces Learning to Reject Low-Quality Explanations (LtX), arguing that systems should abstain when they cannot provide explanations that users would deem acceptable. The authors propose ULER, a rejector that operates on explanations z rather than on inputs or predictions. ULER is trained using modest human supervision: explanation-level quality ratings and per-feature relevance judgments indicating correct and wrong attributions. To address limited labeled data, ULER performs a feedback-aware augmentation that perturbs explanation features guided by user-identified correct/wrong sets and feature-wise variance, aiming to preserve labels while enriching training diversity. A simple scoring classifier (e.g., SVM) is then used to rank explanations and reject those below a threshold. Experiments on eight tabular datasets with simulated human judgments and on a new human-annotated soccer xG dataset show that ULER consistently rejects more low-quality explanations and achieves higher AUROC than LtR-style and explanation-aware baselines (e.g., faithfulness, stability, complexity, PASTA). Robustness checks vary explainers and oracles; ablations show augmentation yields small but consistent gains, and that operating solely on explanations performs best. The work also releases a larger-scale human-labeled dataset of explanation quality. Despite strong results on simulated labels, gains on the human dataset are modest, and several methodological assumptions (e.g., label-preserving augmentation, simulation heuristics) lack formal justification.

Strengths
- Clear and precise problem formulation that elevates explanation quality to a first-class abstention criterion, with a rejector operating over explanations rather than predictions.
- Practical, sample-efficient training pipeline that leverages explanation-level ratings and per-feature feedback, coupled with a principled, feedback-aware augmentation that targets “safe” perturbations using feature-wise variance and human-identified correct/wrong sets.
- Strong empirical performance across diverse tabular tasks: ULER consistently improves AUROC and reduces the acceptance of low-quality explanations relative to LtR approaches and explanation-aware metrics.
- Robustness analyses showing performance holds across different explainers and oracles, and ablations indicating that the rejector benefits from augmentation and performs best when using explanations alone.
- A carefully designed human study with domain-vetted annotators, quality controls, and statistical testing, complemented by comprehensive experimental details and metric definitions in the appendices.
- Clear, well-organized presentation with figures and tables that directly support claims, and transparent discussion of limitations and practical considerations (e.g., threshold selection).
- Valuable community contribution through release of a larger-scale human-annotated dataset of explanation quality.

Weaknesses
- Simulated human judgments are defined via agreement with an oracle explainer using heuristic thresholds for correlation and attribution differences; this may not capture nuanced human perceptions and risks optimistic estimates on simulated benchmarks. Sensitivity analyses for these choices are limited.
- The augmentation assumes label preservation under selective perturbations but offers no theoretical guarantees or empirical audits of mislabeling rates; risks increase if noise scale or feature sets are misspecified.
- Gains on the human-annotated dataset are modest compared to simulated settings, raising questions about generalizability to subjective or high-stakes domains highlighted in the motivation.
- Threshold selection for rejection is guided by heuristics rather than a utility- or cost-sensitive framework that balances low-quality acceptances against deferrals and resource constraints.
- Main experiments rely on feature attribution explanations and primarily a single explainer configuration in the core results; generalization to other explanation modalities (e.g., counterfactuals, rules, text) and broader explainer configurations is not demonstrated in the main body.
- Limited theoretical analysis of when augmentation preserves labels or when the rejector is consistent, and no joint optimization with the predictor/explainer, which could affect the quality–accuracy trade-off.
- Practical sensitivity to augmentation hyperparameters (e.g., noise scale, number of augmentations) and classifier choices is not deeply explored; reporting on inter-rater reliability and alternative aggregation thresholds in the human study is also limited.
