{
  "baseline_review": "1) Summary\nThis paper introduces the problem of Learning to Reject Low-Quality Explanations (LtX), where a model should abstain from making a prediction if it cannot provide a satisfactory explanation to the user. This contrasts with traditional Learning to Reject (LtR), which focuses only on prediction quality. The authors propose ULER (User-centric Low-quality Explanation Rejector), a framework that learns a rejector to mimic human judgments of explanation quality. ULER is trained on a small set of human-annotated explanations, comprising overall quality ratings and per-feature relevance judgments. It employs a novel data augmentation strategy to improve sample efficiency. Experiments on eight benchmark datasets with simulated judgments and a new human-annotated dataset from a user study show that ULER outperforms standard LtR strategies and state-of-the-art explanation quality metrics in identifying and rejecting low-quality explanations.2) Strengths\n*   **Novel and Important Problem Formulation**\n    *   The paper introduces and formalizes the Learning to Reject Low-Quality Explanations (LtX) problem (Section 3, Definition 1). This is a timely and significant contribution that bridges the gap between the fields of explainable AI (XAI) and learning with a reject option.\n    *   The motivation is compelling: standard LtR methods ignore explanation quality, which is critical for user trust and effective downstream decision-making in high-stakes applications (Section 1, Figure 1).\n    *   This work aligns with recent calls for AI systems to recognize and declare their knowledge limits, not just in terms of predictive accuracy but also in their reasoning (Section 1).*   **Technically Sound and Well-Designed Method**\n    *   The proposed method, ULER, is simple, intuitive, and effective. It directly learns a rejector from user feedback to align with human preferences (Section 3.1).\n    *   The feedback-driven data augmentation strategy is a key technical contribution (Section 3.1, Equation 2). It cleverly leverages detailed per-feature feedback (`W_z`, `C_z`) to expand a small set of annotated explanations, which is crucial for making the approach practical given the high cost of human annotation.\n    *   The ablation study confirms that the augmentation provides a consistent, albeit modest, performance boost, justifying the collection of more detailed feedback in high-stakes settings (Section 4.1, Table 6).*   **Comprehensive and Rigorous Empirical Evaluation**\n    *   The experimental design is thorough, comparing ULER against a strong and diverse set of eight competitors, including standard LtR methods and several explanation-aware strategies (Section 4, \"Competitors\").\n    *   The evaluation on eight benchmark datasets with a plausible oracle-based simulation of human judgment demonstrates ULER's consistent superiority in AUROC (Table 1) and in reducing the percentage of low-quality explanations shown to the user (Figure 2).\n    *   The paper includes extensive robustness checks in the appendix, showing that ULER's performance advantage holds when using a different explainer (LIME, Appendix C.3, Figure 4, Table 4) and a different oracle model (Appendix C.4, Figure 5, Table 5).\n    *   The ablation study on the rejector's input space provides the interesting insight that explanations alone are often sufficient for judging their quality, outperforming variants that also use the input instance and/or prediction (Section 4.1, Table 2).*   **High-Quality User Study and Public Dataset**\n    *   The paper validates its approach with a large-scale user study (N=175 participants) on a real-world task (predicting expected goals in soccer), which adds significant credibility to the findings (Section 4.2).\n    *   The annotation protocol was carefully designed with input from a psychologist, pilot studies, and attention checks to ensure data quality (Section 4.2, Appendix D.3).\n    *   The public release of the resulting dataset, containing 1050 explanations with 5 annotations each, is a valuable contribution to the research community that will facilitate future work on human-aligned XAI (Section 1).3) Weaknesses\n*   **Dependence on a Simulated Oracle for Main Experiments**\n    *   The primary quantitative results across the eight benchmark datasets rely on a simulated ML oracle to define ground-truth explanation quality (Section 4.1, \"Simulating human judgments\"). Quality is defined by the correlation between explanations from a linear model and a Random Forest.\n    *   This simulation, while reasonable, is a proxy for true human judgment and may not capture its full complexity. Human reasoning can be non-linear, context-dependent, and may not align with any single ML model, even a more powerful one.\n    *   The specific choice of a linear model versus a Random Forest oracle likely influences the results, as their explanations would naturally differ due to different inductive biases. The paper's main conclusions are therefore heavily conditioned on this specific simulation setup.*   **Limited Scope and Low Absolute Performance in User Study**\n    *   The user study, while a major strength, is based on a single, highly visual task (soccer analytics) with a very low-dimensional feature space (5 features, Appendix D.2).\n    *   It is unclear how well the findings would generalize to more abstract, non-visual, or higher-dimensional tabular domains (e.g., credit scoring, medical diagnosis) where user intuition may be less reliable or the cognitive load of evaluation is much higher.\n    *   The absolute performance in the user study, while statistically significant and better than the SOTA baseline, is modest (AUROC of 0.63 ± 0.05, Section 4.2). This highlights the inherent difficulty of the task and suggests that significant challenges remain in learning subjective human preferences.*   **Ambiguity in the Data Augmentation Mechanism**\n    *   The description of the augmentation process lacks some key implementation details. In the simulation, the threshold `u% = 0.75` used to partition features into \"wrong\" (`W_z`) and \"correct\" sets based on their contribution to the L1 distance is presented without justification (Section 4.1).\n    *   For the user study, it is not explicitly detailed how the per-feature feedback (a multiple-choice selection of incorrect features) is used to construct both the `W_z` and `C_z` sets needed for the augmentation logic described in Section 3.1. It is implied that `C_z` contains features not selected by the user, but this is not stated. This lack of clarity slightly hinders the reproducibility of the user study results.*   **Potential Scalability Issues**\n    *   The proposed method relies on collecting per-feature relevance judgments from users. While feasible for the 5 features in the user study, this approach may not scale well to domains with tens or hundreds of features.\n    *   The cognitive burden on annotators to identify all incorrect feature attributions in a high-dimensional explanation could be prohibitive, potentially leading to lower-quality feedback or making the approach impractical.\n    *   The authors acknowledge this limitation and suggest using embeddings (Section 3.2), but this is not demonstrated, and it is unclear how per-feature feedback would translate to an embedding space.4) Suggestions for Improvement\n*   **Acknowledge and Discuss Simulation Limitations**\n    *   In Section 4.1, add a brief discussion acknowledging the limitations of the oracle-based simulation. This could include reflecting on how the specific choice of models (linear vs. RF) might shape the definition of \"low-quality\" explanations and suggesting that future work could explore more diverse or human-in-the-loop simulation setups.*   **Expand Discussion on Generalizability from User Study**\n    *   In Section 4.2, temper the conclusions by discussing the potential challenges of applying ULER in more complex domains. For instance, mention that the annotation burden for per-feature feedback would increase with dimensionality and that the reliability of human judgment might decrease in non-visual or more abstract tasks, potentially affecting ULER's performance.*   **Clarify Augmentation Implementation Details**\n    *   In Section 3.1, please clarify precisely how the sets `W_z` and `C_z` are constructed from the user feedback collected in the study (Section 4.2). For example, state explicitly that `C_z` is the complement of `W_z`.\n    *   In Section 4.1, briefly justify the choice of `u% = 0.75` for the simulation or state that it was selected via validation. A sensitivity analysis in the appendix would be even better.*   **Address Scalability in More Detail**\n    *   In the limitations discussion (Section 3.2), please elaborate on the scalability concerns. Beyond mentioning embeddings, you could suggest concrete strategies for future work to mitigate this issue. For example, one could explore hierarchical feedback (e.g., on feature groups) or active learning approaches where the system only asks for feedback on the most uncertain or impactful features.5) Score\n*   Overall (10): 8 — The paper introduces a novel and important problem (LtX) and presents a sound, well-evaluated method (ULER) supported by a strong user study and a new public dataset (Section 1, Section 4).\n*   Novelty (10): 9 — The formulation of learning to reject based on explanation quality is highly novel, and the feedback-driven augmentation strategy is a creative and valuable contribution (Section 3.1).\n*   Technical Quality (10): 8 — The methodology is sound and the empirical evaluation is rigorous, with extensive experiments, baselines, and robustness checks (Section 4, Appendix C).\n*   Clarity (10): 10 — The paper is exceptionally well-written, clearly motivated, and easy to follow, with a well-structured narrative and a comprehensive appendix (Section 1-6, Appendix A-D).\n*   Confidence (5): 5 — I am highly confident in my assessment, as the paper provides extensive details and evidence for its claims in both the main text and the appendix.",
  "final_review": "1) Summary\nThis paper introduces the problem of Learning to Reject Low-Quality Explanations (LtX), where a model should abstain from making a prediction if it cannot provide a satisfactory explanation to the user. This contrasts with traditional Learning to Reject (LtR), which focuses only on prediction quality. The authors propose ULER (User-centric Low-quality Explanation Rejector), a framework that learns a rejector to mimic human judgments of explanation quality. ULER is trained on a small set of human-annotated explanations, comprising overall quality ratings and per-feature relevance judgments. It employs a novel data augmentation strategy to improve sample efficiency. Experiments on eight benchmark datasets with simulated judgments and a new human-annotated dataset from a user study show that ULER outperforms standard LtR strategies and state-of-the-art explanation quality metrics in identifying and rejecting low-quality explanations.2) Strengths\n*   **Novel and Important Problem Formulation**\n    *   The paper introduces and formalizes the Learning to Reject Low-Quality Explanations (LtX) problem (Section 3, Definition 1). This is a timely and significant contribution that bridges the gap between the fields of explainable AI (XAI) and learning with a reject option.\n    *   The motivation is compelling: standard LtR methods ignore explanation quality, which is critical for user trust and effective downstream decision-making in high-stakes applications (Section 1, Figure 1).\n    *   This work aligns with recent calls for AI systems to recognize and declare their knowledge limits, not just in terms of predictive accuracy but also in their reasoning (Section 1).*   **Technically Sound and Well-Designed Method**\n    *   The proposed method, ULER, is simple, intuitive, and effective. It directly learns a rejector from user feedback to align with human preferences (Section 3.1).\n    *   The feedback-driven data augmentation strategy is a key technical contribution (Section 3.1, Equation 2). It cleverly leverages detailed per-feature feedback (`W_z`, `C_z`) to expand a small set of annotated explanations, which is crucial for making the approach practical given the high cost of human annotation.\n    *   The ablation study confirms that the augmentation provides a consistent, albeit modest, performance boost, justifying the collection of more detailed feedback in high-stakes settings (Section 4.1, Appendix C.5, Table 6).*   **Comprehensive and Rigorous Empirical Evaluation**\n    *   The experimental design is thorough, comparing ULER against a strong and diverse set of eight competitors, including standard LtR methods and several explanation-aware strategies (Section 4, \"Competitors\").\n    *   The evaluation on eight benchmark datasets with a plausible oracle-based simulation of human judgment demonstrates ULER's consistent superiority in AUROC (Table 1) and in reducing the percentage of low-quality explanations shown to the user (Figure 2).\n    *   The paper includes extensive robustness checks in the appendix, showing that ULER's performance advantage holds when using a different explainer (LIME, Appendix C.3, Figure 4, Table 4) and a different oracle model (Appendix C.4, Figure 5, Table 5).\n    *   The ablation study on the rejector's input space provides the interesting insight that explanations alone are often sufficient for judging their quality, outperforming variants that also use the input instance and/or prediction (Section 4.1, Table 2).*   **High-Quality User Study and Public Dataset**\n    *   The paper validates its approach with a large-scale user study (N=175 participants, filtered to 149) on a real-world task (predicting expected goals in soccer), which adds significant credibility to the findings (Section 4.2).\n    *   The annotation protocol was carefully designed with input from a psychologist, pilot studies, and attention checks to ensure data quality (Section 4.2, Appendix D.3).\n    *   The public release of the resulting dataset, containing human annotations for machine explanations, is a valuable contribution to the research community that will facilitate future work on human-aligned XAI (Section 1, Section 4.2).3) Weaknesses\n*   **Dependence on a Simulated Oracle for Main Experiments**\n    *   The primary quantitative results across the eight benchmark datasets rely on a simulated ML oracle to define ground-truth explanation quality (Section 4.1, \"Simulating human judgments\"). Quality is defined by the correlation between explanations from a linear model and a Random Forest.\n    *   This simulation, while reasonable, is a proxy for true human judgment and may not capture its full complexity. Human reasoning can be non-linear, context-dependent, and may not align with any single ML model, even a more powerful one.\n    *   The specific choice of a linear model versus a Random Forest oracle likely influences the results, as their explanations would naturally differ due to different inductive biases. The paper's main conclusions are therefore heavily conditioned on this specific simulation setup.*   **Limited Scope and Low Absolute Performance in User Study**\n    *   The user study, while a major strength, is based on a single, highly visual task (soccer analytics) with a very low-dimensional feature space (5 features, Appendix D.2).\n    *   It is unclear how well the findings would generalize to more abstract, non-visual, or higher-dimensional tabular domains (e.g., credit scoring, medical diagnosis) where user intuition may be less reliable or the cognitive load of evaluation is much higher.\n    *   The absolute performance in the user study, while statistically significant and better than the SOTA baseline, is modest (AUROC of 0.63 ± 0.05, Section 4.2). This highlights the inherent difficulty of the task and suggests that significant challenges remain in learning subjective human preferences.*   **Inconsistent Reporting of Quantitative Results**\n    *   The paper repeatedly and systematically reports absolute differences in performance metrics as relative percentage improvements, which is unconventional and can be misleading. For example, in Section 4.1, an absolute AUROC difference of 0.209 is described as a \"20% improvement\". This reporting style is used when discussing results from Table 1, Table 2, and Table 6 (Appendix C.5).\n    *   The description of Figure 2 claims ULER reduces low-quality explanations by \"approximately 20% vs PASTARej\" (Section 4.1). This figure appears to be the absolute difference in percentage points at a specific rejection rate, not a relative reduction, which is not clearly stated.\n    *   There is a discrepancy between the claimed size of the contributed dataset (\"1050 examples, 5 annotations each\" in Section 1 and 4.2) and the size actually used for the user study experiments after filtering (\"718 explanations remained for our experiments\" in Appendix D.4). This 32% reduction is not mentioned in the main text.*   **Ambiguity in the Data Augmentation Mechanism**\n    *   The description of the augmentation process lacks some key implementation details. In the simulation, the threshold `u% = 0.75` used to partition features into \"wrong\" (`W_z`) and \"correct\" sets based on their contribution to the L1 distance is presented without justification (Section 4.1).\n    *   For the user study, it is not explicitly detailed how the per-feature feedback (a multiple-choice selection of incorrect features) is used to construct both the `W_z` and `C_z` sets needed for the augmentation logic described in Section 3.1. It is implied that `C_z` contains features not selected by the user, but this is not stated. This lack of clarity slightly hinders the reproducibility of the user study results.*   **Potential Scalability Issues**\n    *   The proposed method relies on collecting per-feature relevance judgments from users. While feasible for the 5 features in the user study, this approach may not scale well to domains with tens or hundreds of features.\n    *   The cognitive burden on annotators to identify all incorrect feature attributions in a high-dimensional explanation could be prohibitive, potentially leading to lower-quality feedback or making the approach impractical.\n    *   The authors acknowledge this limitation and suggest using embeddings (Section 3.2), but this is not demonstrated, and it is unclear how per-feature feedback would translate to an embedding space.4) Suggestions for Improvement\n*   **Acknowledge and Discuss Simulation Limitations**\n    *   In Section 4.1, add a brief discussion acknowledging the limitations of the oracle-based simulation. This could include reflecting on how the specific choice of models (linear vs. RF) might shape the definition of \"low-quality\" explanations and suggesting that future work could explore more diverse or human-in-the-loop simulation setups.*   **Expand Discussion on Generalizability from User Study**\n    *   In Section 4.2, temper the conclusions by discussing the potential challenges of applying ULER in more complex domains. For instance, mention that the annotation burden for per-feature feedback would increase with dimensionality and that the reliability of human judgment might decrease in non-visual or more abstract tasks, potentially affecting ULER's performance.*   **Improve Clarity and Consistency in Reporting**\n    *   Revise the text to clearly distinguish between absolute differences in performance metrics (e.g., \"an improvement of 0.21 AUROC points\") and relative percentage improvements. This should be applied consistently when discussing results in Section 4 and the appendix.\n    *   Clarify the reporting for Figure 2, specifying whether the reported reduction is in absolute percentage points or a relative percentage.\n    *   In the main text (e.g., Section 1 and Section 4.2), clarify that the user study results are based on the filtered dataset of 718 examples and state whether the full, unfiltered dataset of 1050 examples will be released.*   **Clarify Augmentation Implementation Details**\n    *   In Section 3.1, please clarify precisely how the sets `W_z` and `C_z` are constructed from the user feedback collected in the study (Section 4.2). For example, state explicitly that `C_z` is the complement of `W_z`.\n    *   In Section 4.1, briefly justify the choice of `u% = 0.75` for the simulation or state that it was selected via validation. A sensitivity analysis in the appendix would be even better.*   **Address Scalability in More Detail**\n    *   In the limitations discussion (Section 3.2), please elaborate on the scalability concerns. Beyond mentioning embeddings, you could suggest concrete strategies for future work to mitigate this issue. For example, one could explore hierarchical feedback (e.g., on feature groups) or active learning approaches where the system only asks for feedback on the most uncertain or impactful features.5) Score\n*   Overall (10): 7 — The paper introduces a novel and important problem (LtX) and a sound method, but significant reporting inconsistencies affect its trustworthiness (Section 4.1, Appendix D.4).\n*   Novelty (10): 9 — The formulation of learning to reject based on explanation quality is highly novel, and the feedback-driven augmentation strategy is a creative and valuable contribution (Section 3.1).\n*   Technical Quality (10): 7 — The methodology is sound, but the evaluation is marred by systematic misreporting of quantitative results and a discrepancy in the user study dataset size (Section 4.1, Appendix D.4).\n*   Clarity (10): 8 — The paper is generally well-written, but the unconventional and potentially misleading reporting of performance metrics is a major clarity issue (Section 4.1, Figure 2).\n*   Confidence (5): 5 — I am highly confident in my assessment, as the paper provides extensive details and evidence for its claims in both the main text and the appendix.",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 8,
        "novelty": 9,
        "technical_quality": 8,
        "clarity": 10,
        "confidence": 5
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 7,
        "novelty": 9,
        "technical_quality": 7,
        "clarity": 8,
        "confidence": 5
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "1) Summary\nThis paper introduces the problem of Learning to Reject Low-Quality Explanations (LtX), where a model should abstain from making a prediction if it cannot provide a satisfactory explanation to the user. This contrasts with traditional Learning to Reject (LtR), which focuses only on prediction quality. The authors propose ULER (User-centric Low-quality Explanation Rejector), a framework that learns a rejector to mimic human judgments of explanation quality. ULER is trained on a small set of human-annotated explanations, comprising overall quality ratings and per-feature relevance judgments. It employs a novel data augmentation strategy to improve sample efficiency. Experiments on eight benchmark datasets with simulated judgments and a new human-annotated dataset from a user study show that ULER outperforms standard LtR strategies and state-of-the-art explanation quality metrics in identifying and rejecting low-quality explanations.2) Strengths\n*   **Novel and Important Problem Formulation**\n    *   The paper introduces and formalizes the Learning to Reject Low-Quality Explanations (LtX) problem (Section 3, Definition 1). This is a timely and significant contribution that bridges the gap between the fields of explainable AI (XAI) and learning with a reject option.\n    *   The motivation is compelling: standard LtR methods ignore explanation quality, which is critical for user trust and effective downstream decision-making in high-stakes applications (Section 1, Figure 1).\n    *   This work aligns with recent calls for AI systems to recognize and declare their knowledge limits, not just in terms of predictive accuracy but also in their reasoning (Section 1).*   **Technically Sound and Well-Designed Method**\n    *   The proposed method, ULER, is simple, intuitive, and effective. It directly learns a rejector from user feedback to align with human preferences (Section 3.1).\n    *   The feedback-driven data augmentation strategy is a key technical contribution (Section 3.1, Equation 2). It cleverly leverages detailed per-feature feedback (`W_z`, `C_z`) to expand a small set of annotated explanations, which is crucial for making the approach practical given the high cost of human annotation.\n    *   The ablation study confirms that the augmentation provides a consistent, albeit modest, performance boost, justifying the collection of more detailed feedback in high-stakes settings (Section 4.1, Appendix C.5, Table 6).*   **Comprehensive and Rigorous Empirical Evaluation**\n    *   The experimental design is thorough, comparing ULER against a strong and diverse set of eight competitors, including standard LtR methods and several explanation-aware strategies (Section 4, \"Competitors\").\n    *   The evaluation on eight benchmark datasets with a plausible oracle-based simulation of human judgment demonstrates ULER's consistent superiority in AUROC (Table 1) and in reducing the percentage of low-quality explanations shown to the user (Figure 2).\n    *   The paper includes extensive robustness checks in the appendix, showing that ULER's performance advantage holds when using a different explainer (LIME, Appendix C.3, Figure 4, Table 4) and a different oracle model (Appendix C.4, Figure 5, Table 5).\n    *   The ablation study on the rejector's input space provides the interesting insight that explanations alone are often sufficient for judging their quality, outperforming variants that also use the input instance and/or prediction (Section 4.1, Table 2).*   **High-Quality User Study and Public Dataset**\n    *   The paper validates its approach with a large-scale user study (N=175 participants, filtered to 149) on a real-world task (predicting expected goals in soccer), which adds significant credibility to the findings (Section 4.2).\n    *   The annotation protocol was carefully designed with input from a psychologist, pilot studies, and attention checks to ensure data quality (Section 4.2, Appendix D.3).\n    *   The public release of the resulting dataset, containing human annotations for machine explanations, is a valuable contribution to the research community that will facilitate future work on human-aligned XAI (Section 1, Section 4.2).3) Weaknesses\n*   **Dependence on a Simulated Oracle for Main Experiments**\n    *   The primary quantitative results across the eight benchmark datasets rely on a simulated ML oracle to define ground-truth explanation quality (Section 4.1, \"Simulating human judgments\"). Quality is defined by the correlation between explanations from a linear model and a Random Forest.\n    *   This simulation, while reasonable, is a proxy for true human judgment and may not capture its full complexity. Human reasoning can be non-linear, context-dependent, and may not align with any single ML model, even a more powerful one.\n    *   The specific choice of a linear model versus a Random Forest oracle likely influences the results, as their explanations would naturally differ due to different inductive biases. The paper's main conclusions are therefore heavily conditioned on this specific simulation setup.*   **Limited Scope and Low Absolute Performance in User Study**\n    *   The user study, while a major strength, is based on a single, highly visual task (soccer analytics) with a very low-dimensional feature space (5 features, Appendix D.2).\n    *   It is unclear how well the findings would generalize to more abstract, non-visual, or higher-dimensional tabular domains (e.g., credit scoring, medical diagnosis) where user intuition may be less reliable or the cognitive load of evaluation is much higher.\n    *   The absolute performance in the user study, while statistically significant and better than the SOTA baseline, is modest (AUROC of 0.63 ± 0.05, Section 4.2). This highlights the inherent difficulty of the task and suggests that significant challenges remain in learning subjective human preferences.*   **Inconsistent Reporting of Quantitative Results**\n    *   The paper repeatedly and systematically reports absolute differences in performance metrics as relative percentage improvements, which is unconventional and can be misleading. For example, in Section 4.1, an absolute AUROC difference of 0.209 is described as a \"20% improvement\". This reporting style is used when discussing results from Table 1, Table 2, and Table 6 (Appendix C.5).\n    *   The description of Figure 2 claims ULER reduces low-quality explanations by \"approximately 20% vs PASTARej\" (Section 4.1). This figure appears to be the absolute difference in percentage points at a specific rejection rate, not a relative reduction, which is not clearly stated.\n    *   There is a discrepancy between the claimed size of the contributed dataset (\"1050 examples, 5 annotations each\" in Section 1 and 4.2) and the size actually used for the user study experiments after filtering (\"718 explanations remained for our experiments\" in Appendix D.4). This 32% reduction is not mentioned in the main text.*   **Ambiguity in the Data Augmentation Mechanism**\n    *   The description of the augmentation process lacks some key implementation details. In the simulation, the threshold `u% = 0.75` used to partition features into \"wrong\" (`W_z`) and \"correct\" sets based on their contribution to the L1 distance is presented without justification (Section 4.1).\n    *   For the user study, it is not explicitly detailed how the per-feature feedback (a multiple-choice selection of incorrect features) is used to construct both the `W_z` and `C_z` sets needed for the augmentation logic described in Section 3.1. It is implied that `C_z` contains features not selected by the user, but this is not stated. This lack of clarity slightly hinders the reproducibility of the user study results.*   **Potential Scalability Issues**\n    *   The proposed method relies on collecting per-feature relevance judgments from users. While feasible for the 5 features in the user study, this approach may not scale well to domains with tens or hundreds of features.\n    *   The cognitive burden on annotators to identify all incorrect feature attributions in a high-dimensional explanation could be prohibitive, potentially leading to lower-quality feedback or making the approach impractical.\n    *   The authors acknowledge this limitation and suggest using embeddings (Section 3.2), but this is not demonstrated, and it is unclear how per-feature feedback would translate to an embedding space.4) Suggestions for Improvement\n*   **Acknowledge and Discuss Simulation Limitations**\n    *   In Section 4.1, add a brief discussion acknowledging the limitations of the oracle-based simulation. This could include reflecting on how the specific choice of models (linear vs. RF) might shape the definition of \"low-quality\" explanations and suggesting that future work could explore more diverse or human-in-the-loop simulation setups.*   **Expand Discussion on Generalizability from User Study**\n    *   In Section 4.2, temper the conclusions by discussing the potential challenges of applying ULER in more complex domains. For instance, mention that the annotation burden for per-feature feedback would increase with dimensionality and that the reliability of human judgment might decrease in non-visual or more abstract tasks, potentially affecting ULER's performance.*   **Improve Clarity and Consistency in Reporting**\n    *   Revise the text to clearly distinguish between absolute differences in performance metrics (e.g., \"an improvement of 0.21 AUROC points\") and relative percentage improvements. This should be applied consistently when discussing results in Section 4 and the appendix.\n    *   Clarify the reporting for Figure 2, specifying whether the reported reduction is in absolute percentage points or a relative percentage.\n    *   In the main text (e.g., Section 1 and Section 4.2), clarify that the user study results are based on the filtered dataset of 718 examples and state whether the full, unfiltered dataset of 1050 examples will be released.*   **Clarify Augmentation Implementation Details**\n    *   In Section 3.1, please clarify precisely how the sets `W_z` and `C_z` are constructed from the user feedback collected in the study (Section 4.2). For example, state explicitly that `C_z` is the complement of `W_z`.\n    *   In Section 4.1, briefly justify the choice of `u% = 0.75` for the simulation or state that it was selected via validation. A sensitivity analysis in the appendix would be even better.*   **Address Scalability in More Detail**\n    *   In the limitations discussion (Section 3.2), please elaborate on the scalability concerns. Beyond mentioning embeddings, you could suggest concrete strategies for future work to mitigate this issue. For example, one could explore hierarchical feedback (e.g., on feature groups) or active learning approaches where the system only asks for feedback on the most uncertain or impactful features.5) Score\n*   Overall (10): 7 — The paper introduces a novel and important problem (LtX) and a sound method, but significant reporting inconsistencies affect its trustworthiness (Section 4.1, Appendix D.4).\n*   Novelty (10): 9 — The formulation of learning to reject based on explanation quality is highly novel, and the feedback-driven augmentation strategy is a creative and valuable contribution (Section 3.1).\n*   Technical Quality (10): 7 — The methodology is sound, but the evaluation is marred by systematic misreporting of quantitative results and a discrepancy in the user study dataset size (Section 4.1, Appendix D.4).\n*   Clarity (10): 8 — The paper is generally well-written, but the unconventional and potentially misleading reporting of performance metrics is a major clarity issue (Section 4.1, Figure 2).\n*   Confidence (5): 5 — I am highly confident in my assessment, as the paper provides extensive details and evidence for its claims in both the main text and the appendix."
}