Academic integrity and internal consistency risk report

Summary of substantive issues identified

1) Probabilistic outputs assumed, but non-probabilistic predictor used without clarification
- Evidence:
  - Preliminaries state a probabilistic classifier is assumed: “We view the predictor as a probabilistic classifier that assigns a predictive distribution P(Y | X = x)… predictions via MAP inference” (Section 2, Block #7).
  - Several metrics and baselines require probabilities (e.g., PredAmb uses margins of class probabilities for classification; Δ_f in faithfulness uses “absolute difference in positive class probability” for classification) (Section 4, Competitors, PredAmb, Block #16; Appendix B.1, Block #47).
  - Experiments report using a linear SVC as the predictor “with the default scikit-learn implementations” (Section 4.1, Block #21).
- Problem: In scikit-learn, SVC by default does not produce calibrated class probabilities (probability=False). The manuscript does not state that probabilities were enabled or calibrated, nor does it explain how probability-dependent metrics were computed for the SVC predictor (e.g., PredAmb and the faithfulness metric’s Δ_f).
- Impact: This is a high-impact reproducibility and correctness issue. If probabilities were not available or calibrated, the stated probability-based computations are undefined or inconsistent with the described methods. It affects the validity of several reported results and comparisons.

2) Ambiguity and potential mathematical inconsistency in augmentation distribution
- Evidence: The augmentation is defined as z_aug ∼ N(z, ε0 s × Σ), where Σ is a diagonal matrix of per-feature standard deviations and s is a binary vector (Section 3.1, Eq. (2), Block #12).
- Problem: The covariance of a multivariate normal must be a positive semi-definite matrix. The expression “ε0 s × Σ” is ambiguous: it mixes a vector (s) and a matrix (Σ) without specifying whether × denotes element-wise multiplication, left/right multiplication, or construction of a diagonal mask (e.g., diag(s) Σ diag(s)). As written, it is not a well-defined covariance matrix.
- Impact: This prevents unambiguous implementation of the core augmentation mechanism and could lead to non-PSD covariances. It materially affects reproducibility and the correctness of ULER’s training data generation.

3) Inconsistency in test set size
- Evidence:
  - The setup states: “Both predictors are evaluated on a disjoint test set consisting of 2000 instances” (Section 4.1, Block #21).
  - Table 3 reports the churn dataset test set size as 1850, not 2000 (Appendix C.1, Table 3, Block #51).
- Problem: The stated uniform test size (2000) conflicts with the reported churn test size (1850).
- Impact: This inconsistency affects the interpretation of averages across datasets and could change rejection-rate calculations and AUROC variances. It also harms reproducibility.

4) Missing method detail: conditional variance for regression baseline with SVR
- Evidence:
  - PredAmb for regression: “the conditional variance for each input is computed and then the score is obtained applying a monotonically decreasing function” (Section 4, Competitors, Block #16).
  - The predictor for regression tasks is an SVR with default scikit-learn settings (Section 4.1, Block #21).
- Problem: scikit-learn’s SVR does not provide a predictive conditional variance. The manuscript does not specify how conditional variance was computed for PredAmb on regression tasks (e.g., via ensembles, bootstrap, probabilistic SVR, or other approximations).
- Impact: This omission affects the validity of the PredAmb baseline and any comparisons involving regression tasks.

5) Use of “Cz” (indices of features deemed correct) not operationally defined in the human study
- Evidence:
  - ULER relies on both Wz (wrong) and Cz (correct) feature sets for augmentation (Section 3.1, Block #12).
  - In the human study, annotators only “optionally select individual features they believed were misused” (Section 4.2, Block #29), and feature-level feedback is “marked … as incorrect if the majority of annotators agreed” (Section 4.2, Block #30).
- Problem: The manuscript does not explicitly define how Cz is obtained from human annotations (e.g., whether Cz is the complement of Wz over the feature set or requires explicit confirmation). This matters because augmentation uses Cz vs Wz to decide which entries to perturb.
- Impact: Ambiguity in Cz derivation compromises the exact reproducibility of the augmentation in the human-annotated setting.

6) Unsupported quantitative claim
- Evidence: “ULER rejects the highest number of low-quality explanations in around 94% of the experiments against all competitors.” (Section 4.1, Block #24).
- Problem: No table or figure provides the explicit count or percentage underpinning the “94%” claim across all experiments.
- Impact: The lack of direct evidence undermines the credibility of this quantitative assertion.
- Note: No direct evidence found in the manuscript.

7) Human study: final sample size used for reported AUROC not stated in the main text
- Evidence:
  - Main text: “We collected annotations for 1050 explanations from five annotators each” and report AUROC results (Section 4.2, Blocks #27 and #31).
  - Appendix: After filtering, “718 explanations remained for our experiments.” (Appendix D.4, Block #69).
- Problem: The main text does not state that the reported AUROC (0.63 ± 0.05) was computed on the filtered set (718 explanations), nor does it specify the final sample used.
- Impact: This omission can mislead readers about the effective sample size and variability in the reported human-study results, impacting trust and reproducibility.

Additional notes

- Threshold vs fixed rejection rate: The method section defines a threshold τ (Section 3.1, Block #12), whereas the experiments reject the lowest ρ% by score (Section 4, Block #16; Section 4, Block #18). While common in practice (τ chosen to achieve a target rate), this mapping is not explicitly stated and could be clarified to avoid confusion. Impact is low, but clarity would help.

If addressed, these issues would substantially improve the manuscript’s clarity, correctness, and reproducibility.

If no other issues are found
The above list represents high-impact, evidence-based inconsistencies and missing details observed directly from the manuscript.