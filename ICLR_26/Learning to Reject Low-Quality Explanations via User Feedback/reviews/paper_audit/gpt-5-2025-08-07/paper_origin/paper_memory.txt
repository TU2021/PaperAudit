# Global Summary
- Problem: Learning to Reject Low-Quality Explanations (LtX). The authors argue that in high-stakes applications, models should abstain when they can only produce explanations that users find unsatisfactory, even if predictions may be accurate.
- Approach: ULER (User-centric Low-quality Explanation Rejector) adds a rejector r that scores explanation quality from the user’s perspective and abstains if r(z) < τ. ULER learns r from modest human feedback: (i) binary explanation quality labels (high vs low), and (ii) per-feature relevance feedback indicating which feature importances are wrong/correct. It uses a quality-aware data augmentation: z_aug ∼ N(z, ε0 s × Σ), with s set according to yz and Wz/Cz, then trains a simple classifier (e.g., SVM) to estimate explanation quality.
- Evaluation scope: Eight tabular datasets (4 classification: compas, creditcard, adult, churn; 4 regression: news, wine, parkinson, appliances) with simulated human judgments; plus a new human-annotated soccer expected goals (xG) dataset with 1050 explanations and 5 annotators each (5250 annotations), released publicly. Explanations via KernelSHAP (100 samples), with robustness to LIME evaluated.
- Metrics: (i) Percentage of low-quality explanations in accepted vs rejected sets across rejection rates ρ from 1% to 25%; (ii) AUROC for ranking low-quality explanations below high-quality ones.
- Key findings: ULER rejects more low-quality explanations than 8 competitors (standard LtR and explanation-aware) across datasets and rejection rates. Average AUROC per dataset shows ULER is best, e.g., 0.92 ± 0.01 (churn), 0.93 ± 0.03 (wine), 0.91 ± 0.01 (news). Against baselines, ULER reduces low-quality in the accepted set by ~20% vs PASTARej, 21% vs FaithRej, 24% vs StabRej and ComplRej, 27% vs PredAmb, 31% vs RandRej, and 32% vs NovRejX/Z. ULER rejects the highest number of low-quality explanations in ~94% of experiments. In the human study, ULER achieves AUROC 0.63 ± 0.05 vs PASTARej 0.51 ± 0.09 (p < 0.01), and rejects more low-quality explanations in ~84% of experiments across ρ.
- Caveats explicitly stated: ULER relies on high-quality human annotations; focuses on tabular data; yields only marginal gains in predictive performance (its goal is filtering low-quality explanations); sample complexity may increase with many features; augmentation offers modest but consistent gains (~2%) and can be skipped if per-feature feedback is unavailable.

# Abstract
- Problem: Explanations may be low quality (hard to interpret/believe), affecting trust and downstream decisions. Proposes that classifiers should refuse handling inputs whose predictions cannot be properly explained.
- LtX framework: Equip predictors with a rejector evaluating explanation quality, focusing on attribution techniques.
- Method: ULER learns a simple rejector from human ratings and per-feature relevance judgments to mirror human judgments of explanation quality. It uses quality-aware augmentation and trains a binary rejector.
- Claims: ULER outperforms state-of-the-art and explanation-aware LtR strategies across eight benchmarks (classification and regression) and a new human-annotated dataset. New dataset publicly released.

# Introduction
- Motivation: LtR traditionally abstains based on prediction uncertainty or input novelty, ignoring explanation quality. In high-stakes domains (medical diagnosis, credit scoring), explanation quality determines whether users trust the model.
- LtX: Formalizes abstaining when only low-quality explanations can be provided, aligning with the NIST Four Principles of XAI urging systems to recognize and declare limits.
- Method overview: ULER uses expert annotations (quality judgments and per-feature relevance judgments), applies quality-aware augmentation, and learns a rejector to evaluate explanation quality.
- Contributions:
  - Introduces LtX, addressing the gap in LtR regarding explanation quality.
  - Designs ULER to learn rejection policies from modest human annotations (explanation ratings and per-feature judgments).
  - Empirical evaluation on popular datasets and a new human-annotated task, outperforming standard LtR and explanation-quality metrics.
  - Releases the first larger-scale dataset of human-annotated machine explanations: “1050 examples, 5 annotations each”, and a template for the collection campaign.

# Preliminaries
- Predictor and explainer:
  - Predictor f: For classification, uses MAP inference f(x) = argmax_c P(Y=c|x). For regression, f(x) = E[Y|X=x].
  - Explainer e: Local feature importance explanation z assigns relevance z_i ∈ R per feature x_i; returns (f(x), z).
- Learning to reject (LtR):
  - Rejector r extends target space with abstain symbol ®; two strategies: ambiguity rejection (uncertainty) and novelty rejection (input far from training data). These improve reliability but ignore explanation quality.
- Metrics of explanation quality:
  - Machine-side: faithfulness, stability, complexity; may not align with human judgment.
  - Human-side: PASTA mimics human preferences; compared in experiments.
- Gap: No integration of explanation-quality metrics into rejection. Authors propose a framework incorporating user-perceived explanation quality into rejection.

# Method
- LtX model definition:
  - m_(f,e,r)(x) returns ® if r(z) < τ; otherwise returns (f(x), z). Rejector operates on explanation z rather than x or f(x).
- Challenges: Traditional LtR abstains to avoid mispredictions; existing explanation metrics don’t measure user-perceived quality; LtX needs human-judgment labels per prediction explanation.
- ULER steps:
  - Training data D = {(z, y_z)} with y_z ∈ {0,1} (0=low-quality, 1=high-quality) plus per-feature human labels: W_z indices deemed wrong, C_z indices deemed correct.
  - Augmentation: For each (z, y_z), create K new explanations z_aug sharing label y_z, via z_aug ∼ N(z, ε0 s × Σ), where Σ is diagonal with per-feature std devs across D, ε0 controls magnitude, and s selects which features to perturb (for low-quality explanations, s_i=1 for C_z and s_i=0 for W_z; opposite for high-quality).
  - Rejector learning: Train a binary classifier r (e.g., kernel SVM) on augmented D_aug. Set threshold τ via validation to control rejection rate or match proportion of low-quality explanations.
- Benefits and limitations:
  - Designed to offload predictions tied to unsatisfactory explanations; only marginally improves predictive performance on accepted inputs (by rejecting explanations of incorrect predictions due to reasoning mismatch).
  - Can be combined with LtR strategies focused on prediction errors.
  - Requires high-quality human annotations; argued feasible in high-stakes settings.
  - Sample efficient: Outperforms SOTA with fewer than 1000 annotations; augmentation boosts performance.
  - Focus on tabular data; many features may increase sample complexity; possible adaptation to embedding spaces (as in PASTA).

# Experiments
- Research questions: Q1—Does ULER reject more low-quality explanations than competitors? Q2—What inputs are needed for ULER’s rejector? Q3—Can ULER mimic human judgments in a user study?
- Competitors (all produce scores; lowest ρ% rejected):
  - Standard LtR: RandRej (random score), NovRej_X (input novelty via kNN distance transformed by 1/(1+x)), PredAmb (prediction confidence or 1/(1+variance) for regression).
  - Explanation-aware: StabRej (stability), FaithRej (faithfulness), ComplRej (complexity with monotonic transform), PASTARej (PASTA scoring network adapted for tabular explanations), NovRej_Z (novelty in explanation space).
- Evaluation metrics:
  - Percentage of low-quality explanations in accepted vs rejected sets across rejection rates ρ from 1% to 25%.
  - AUROC to assess ranking of low- vs high-quality explanations.
- Setup:
  - Splits: 70% train / 10% val / 20% test per dataset; rejectors trained on train and tuned on val; metrics computed on test; 10 repetitions averaged.
  - Explanations: KernelSHAP with 100 samples using predictor’s training set as background; robustness via LIME also reported in Appendix.
  - ULER’s rejector: SVM; hyperparameters selected via grid search (see Appendix C.2).
  - Compute: Python; Intel i7-12700, 64 GB RAM; approximately two days total runtime.
- Q1/Q2: Benchmark datasets with simulated human judgments:
  - Datasets: compas, creditcard, adult, churn (classification); news, wine, parkinson, appliances (regression). Domains include healthcare (parkinson), economics (creditcard, adult), industry (appliances), law (compas), etc. Full dataset characteristics and proportions of low-quality explanations γ in Appendix C.1.
  - Simulation of human judgments:
    - Oracle O: Random Forest (classifier/regressor). Predictor f: linear SVC/SVR. Evaluated on disjoint test set of 2000 instances: oracle average balanced accuracy 0.76 and MSE 0.008; model average balanced accuracy 0.69 and MSE 0.020.
    - Labeling: Explanations z labeled low-quality if correlation with oracle explanation z_O < τ_z; τ_z fixed at 0.25, yielding datasets with 1%–48% low-quality explanations. Wrong relevance set W_z: indices cumulatively accounting for u% = 0.75 of L1 distance between z and z_O (for low-quality; for high-quality, features accounting for 1 − u%).
  - Results—Percentage reduction and dominance:
    - ULER reduces low-quality explanations in accepted set by ~20% vs PASTARej, 21% vs FaithRej, 24% vs StabRej and ComplRej, 27% vs PredAmb, 31% vs RandRej, 32% vs NovRejX and NovRejZ.
    - ULER rejects the highest number of low-quality explanations in ~94% of experiments across rejection rates.
    - Explanation-aware rejectors generally outperform standard LtR, supporting that prediction ambiguity/novelty is misaligned with LtX.
  - Results—AUROC (Table 1; all KernelSHAP):
    - ULER: compas 0.75 ± 0.04; creditcard 0.87 ± 0.02; adult 0.85 ± 0.04; churn 0.92 ± 0.01; news 0.91 ± 0.01; wine 0.93 ± 0.03; parkinson 0.87 ± 0.01; appliances 0.82 ± 0.01.
    - Runner-ups vary per dataset; average improvement vs runners-up: 20% (PASTARej) and 24% (FaithRej).
    - Robustness confirmed when changing explainer to LIME (Appendix C.3; ULER remains best; e.g., compas 1.00 ± 0.00; news 1.00 ± 0.00; parkinson 0.99 ± 0.00) and oracle choice (Appendix C.4/C.5; ULER AUROCs up to 0.99 ± 0.00).
  - Q2—ULER’s input variants (Table 2):
    - Variants: ULER_Z,X (explanation + instance), ULER_Z,Y (explanation + prediction), ULER_Z,X,Y (explanation + instance + prediction).
    - ULER (explanations only) consistently achieves highest AUROC across datasets. On average, ULER outperforms ULER_Z,X and ULER_Z,X,Y by ~16%. Including prediction as in ULER_Z,Y causes a small decrease (~3% typically). Large drop in news attributed to sub-optimal predictor performance.
    - Ablation of augmentation: modest but consistent average improvement ≈ 2%; ULER beats no-augmentation variant in 79% of experiments. Without augmentation, ULER still outperforms all competitors (Appendix C.5/C.6).
- Q3: User study with human annotations (Section 4.2):
  - Task: Explain an xG model predicting probability a shot results in a goal. Visualizable snapshots enable intuitive assessment.
  - Predictor: XGBoost ensemble classifier with 50 trees, max depth 3; trained on 21,337 shot events (2015–16 England, Spain, Germany, France). Test set: 1050 shots from 2015–16 Italian league. Performance: AUROC 0.81; Brier score 0.067.
  - Explanations: KernelSHAP with 100 samples; only features directly visualizable: distance to goal, angle between ball and posts, goalkeeper (GK) distance to goal line, GK distance to mid line, distance to closest defender.
  - Annotation campaign: Prolific; N = 175 participants, each 30 trials; two 5-point Likert-scale questions (prediction agreement; explanation consistency) and per-feature multiple choice feedback; ethics approval Protocol No. 2025-006ESA; compensation £3 for expected 25 minutes. Filtering removed inattentive/low-quality annotations and explanations with low agreement, leaving 149 participants; explanations with low inter-annotator agreement removed. Aggregation: explanations with average score < 3 labeled low-quality; per-feature wrong relevance via majority vote. After filters, 718 explanations remained for experiments (Appendix D.4).
  - Results: ULER AUROC 0.63 ± 0.05 vs PASTARej 0.51 ± 0.09; paired t-test p < 0.01. ULER rejects more low-quality explanations in ~84% of experiments across ρ = 1%–25%. Additional comparison with all competitors (Appendix C.6): ULER 0.63 ± 0.05; RandRej 0.55 ± 0.07; PredAmb 0.46 ± 0.06; NovRejX 0.44 ± 0.07; StabRej 0.55 ± 0.11; FaithRej 0.55 ± 0.08; ComplRej 0.45 ± 0.09; NovRejZ 0.45 ± 0.04.
  - Additional observation: Annotators flagged on average 1.8 features as incorrect in low-quality explanations vs 0.7 in high-quality ones.

# Related Work
- Learning to Reject/Defer/Triage/Human Assistance/Complement: Strategies defer decisions to human experts based on uncertainty, team performance, or jointly learned policies [5, 61–69]. None consider explanation quality in rejection. ULER has a complementary goal and could be combined with existing LtR to defer both incorrect predictions and unsatisfactory explanations.
- XAI: Focus on feature attribution (LIME, SHAP, input gradients, formal feature attributions). ULER assesses perceived quality irrespective of explainer; robustness to LIME demonstrated.
- Evaluating explanations: Machine-side metrics (faithfulness, stability, complexity) often misalign with human judgment; PASTA is closest human-side metric but not designed for rejection. Authors’ experiments show machine-side metrics and standard LtR do not reliably reflect user judgments.

# Conclusion
- Summary: Introduces LtX and ULER, learning to abstain when explanations are low-quality per user judgments. ULER trained with modest expert feedback (quality labels and per-feature relevance), augmented to improve sample efficiency.
- Empirical findings: ULER effectively identifies low-quality explanations, outperforming standard LtR and explanation-quality metrics across simulated and human-annotated tasks.
- Future work: Jointly learning rejector and classifier to optimize team performance, and leveraging ULER’s rejector as a loss term to align and debias explanations in confounded models.

# Appendix
- Acknowledgments and funding: Flemish Government “Onderzoeksprogramma Artificiële Intelligentie (AI) Vlaanderen” [LS, JD], KU Leuven Research Fund (iBOF/21/075) [JD]; EU Grant No. 101120763 – TANGO [ST, DP].
- Broader Impact: Rejecting low-quality explanations filters outputs based on human-validated reasoning and improves decision quality by ensuring explanations are trustworthy.
- Explanation quality metrics (machine-side and human-side):
  - Stability: stab(z) = E_{z'∼Z}[Sim(z, z')], with Pearson correlation across 10 runs.
  - Faithfulness: harmonic mean of sufficiency and necessity via interventional perturbations on relevant/irrelevant features; normalized to [0,1] using exp transforms; Δ_f computed as absolute change in positive class probability (classification) or prediction (regression).
  - Complexity: compl = −∑_i \overline{z}_i ln(\overline{z}_i), with \overline{z}_i = |z_i| / ∑_j |z_j|.
  - PASTA: embedding + scoring network trained on human ratings; adapted here by dropping the embedding network and fitting only the scoring network to tabular explanations.
- Dataset characteristics and predictors’ performance (Appendix C.1):
  - Classification—compas: #T=10000, d=12, #D=2000, BACC_O=0.770, BACC_f=0.690, γ=0.05.
  - creditcard: #T=10000, d=23, #D=2000, BACC_O=0.660, BACC_f=0.608, γ=0.12.
  - adult: #T=10000, d=12, #D=2000, BACC_O=0.756, BACC_f=0.757, γ=0.02.
  - churn: #T=1000, d=13, #D=1850, BACC_O=0.838, BACC_f=0.696, γ=0.15.
  - Regression—news: #T=10000, d=58, #D=2000, MSE_O=0.004, MSE_f=0.009, γ=0.48.
  - wine: #T=1000, d=11, #D=2000, MSE_O=0.014, MSE_f=0.015, γ=0.02.
  - parkinson: #T=1000, d=19, #D=2000, MSE_O=0.008, MSE_f=0.044, γ=0.46.
  - appliances: #T=10000, d=13, #D=2000, MSE_O=0.005, MSE_f=0.010, γ=0.32.
- Hyperparameter selection (Appendix C.2):
  - ULER SVM kernel ∈ {linear, polynomial, RBF}; C ∈ {0.1, 1, 10}; augmentations k ∈ {5, 10, 20}; noise ε0 ∈ {0.1, 0.5, 1}.
  - PASTA loss weights α ∈ {0.1, 1, 10}, β ∈ {0.001, 0.01, 0.1}, γ ∈ {0.01, 0.1, 1}.
  - NovRej_x and NovRej_z: k_NN ∈ {1, 5, 10}.
- Robustness to explainer (Appendix C.3):
  - Using LIME, ULER remains best across datasets and rejection rates. Average reduction of low-quality in accepted set by 18% vs PASTARej. Example AUROCs (ULER): compas 1.00 ± 0.00; news 1.00 ± 0.00; parkinson 0.99 ± 0.00; appliances 0.82 ± 0.02.
- Robustness to oracle choice (Appendix C.4/C.5):
  - Oracle as SVM RBF: average classification BACC 0.67; regression MSE 0.02. ULER remains best across rejection rates. Example AUROCs (ULER): compas 0.98 ± 0.02; creditcard 0.99 ± 0.00; churn 0.95 ± 0.01; parkinson 0.96 ± 0.02.
- Ablation (Appendix C.5/C.6):
  - ULER vs ULER-NOAUG: ULER gains ≈ 2% AUROC consistently; ULER-NOAUG still outperforms best baseline; average AUROC advantage 12% over nearest competitor when skipping augmentation.
- User study details (Appendix D):
  - Data: StatsBomb 360 event stream data with snapshots (public).
  - Features extracted: distance to goal, angle between ball and posts, GK distance to goal line, GK distance to mid line, distance to closest defender.
  - Training: XGBoost 50 trees, depth 3; AUROC 0.81; Brier 0.067.
  - Recruitment and vetting: Prolific filters for soccer expertise; countries (UK, Germany, France, Spain, Belgium, Italy, Netherlands, Portugal); compensation £3; pilot studies for clarity/consistency; attention checks.
  - Annotation process: Two Likert (1–5) questions (prediction agreement; explanation consistency); feature-level multiple choice; instructions via Google Doc; warm-up trials; 30 main trials. Filtering: excluded failures of attention checks, uniform scorers, and those not flagging any incorrect features; removed explanations with std dev > 1.25; resulting in 718 explanations used.
  - Additional user-study AUROC across competitors (Appendix Table 7): ULER 0.63 ± 0.05; RandRej 0.55 ± 0.07; PredAmb 0.46 ± 0.06; NovRejX 0.44 ± 0.07; StabRej 0.55 ± 0.11; FaithRej 0.55 ± 0.08; ComplRej 0.45 ± 0.09; NovRejZ 0.45 ± 0.04.

# References
- Cited works cover: high-stakes ML deployments (medical imaging), surveys on LtR and XAI, attribution methods (LIME, SHAP, gradients), explanation evaluation metrics (faithfulness, stability, complexity), PASTA human-side metric, LtR/defer/triage/human assistance/complement strategies, human-centered evaluation and cognitive biases, soccer analytics for xG, dataset sources (UCI, StatsBomb), and methodological components (SVM, XGBoost).
- Key numeric or procedural references:
  - LtR survey [5]; abstention strategies [6, 7, 8, 27]; OOD/novelty detection [9, 29].
  - Attribution methods [20, 21, 18, 76]; evaluation metrics [33–36, 37–39].
  - PASTA [39] used/adapted; human annotation principles and Likert scale guidance [59, 60, 97].
  - xG modeling references [52–55].