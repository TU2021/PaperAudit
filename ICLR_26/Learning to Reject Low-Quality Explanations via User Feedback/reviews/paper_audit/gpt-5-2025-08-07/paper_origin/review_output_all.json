{
  "baseline_review": "Summary\n- The paper introduces Learning to Reject Low-Quality Explanations (LtX), where a model abstains when its explanation is judged unsatisfactory by users. The proposed method, ULER (User-centric Low-quality Explanation Rejector), trains a rejector using modest amounts of expert feedback: binary explanation-quality labels and per-feature relevance judgments, augmented via a label-preserving stochastic perturbation (Equation 2 in Section 3.1) and classified with a simple SVM and thresholding (Definition 1 and Equation 1 in Section 3). Experiments compare ULER to eight baselines across eight tabular datasets (Section 4.1) and a new human-annotated soccer xG dataset (Section 4.2), showing improved rejection of low-quality explanations (Figure 2, Table 1, Table 2; Table 7 for the human study). The paper provides robustness analyses with alternative explainers (LIME; Appendix C.3) and oracle predictors (Appendix C.4), and an ablation on augmentation (Appendix C.5). A human-annotated dataset is released to support future research (Abstract; Section 4.2).Strengths\n- **Clear problem formulation and rejector definition**\n  - The LtX setting is formalized with a clear model definition, including predictor, explainer, and rejector, and a concrete abstention rule (Definition 1; Equation 1 in Section 3). This matters for clarity and reproducibility, making the contribution easy to situate within LtR/XAI literature.\n  - The paper explicitly motivates why explanation quality should drive rejection (Section 1; Figure 1), aligning with recognized guidelines (e.g., NIST Four Principles; Section 1), which increases impact in high-stakes domains.\n  - Distinguishes LtX from traditional LtR (Section 2, “Learning to reject”) and existing quality metrics (Section 2, “Metrics of Explanation Quality”), improving conceptual novelty.- **Simple, user-centered method (ULER) leveraging human feedback**\n  - ULER uses human binary explanation-quality labels and per-feature judgments to construct training data (Section 3.1, “The rejector’s training data”), grounding the rejector in user perception — important for practical human alignment.\n  - A feedback-driven augmentation strategy preserves labels while perturbing selected features (Equation 2 and the s-vector design in Section 3.1), a technically sound and pragmatic approach to address data scarcity.\n  - Method is model-agnostic for the rejector and explainer choices (Section 3.1, “Learning the rejector”; Section 3.2), enhancing applicability.- **Comprehensive empirical evaluation and competitive baselines**\n  - Eight baselines span standard LtR and explanation-aware strategies (Section 4, “Competitors”), including PASTARej (human-side metric) and faithfulness/stability/complexity (machine-side metrics), ensuring broad coverage and experimental rigor.\n  - Evaluation metrics include AUROC and percentages of low-quality explanations in accepted/rejected sets across rejection rates (Section 4, “Evaluation metrics”), appropriate for ranking-based rejectors.\n  - Multi-dataset evaluation across diverse tasks (classification/regression; healthcare/economics/law/industry) improves external validity (Section 4.1, “Datasets”; Appendix C.1).- **Consistent performance gains and robust trends**\n  - ULER shows clear improvements over baselines in separating low- vs. high-quality explanations (Table 1; Figure 2; Section 4.1, Q1), with quantified relative reductions in low-quality explanations in the accepted set and high fractions in the rejected set.\n  - Robustness to alternative explainer (LIME) maintains advantages (Appendix C.3; Figure 4; Table 4), supporting method stability.\n  - Robustness to alternative oracle predictor (Appendix C.4; Figure 5; Table 5) and an augmentation ablation (Appendix C.5; Table 6) further underlines consistency and technical soundness.- **Carefully designed human annotation study and dataset contribution**\n  - The paper details recruitment, vetting, and interface design for human ratings and per-feature feedback (Section 4.2; Appendix D.3), including ethical approval (Section 4.2), which strengthens the credibility of human-centered evaluation.\n  - Provides preprocessing steps for quality control and aggregation (Section 4.2; Appendix D.4), ensuring higher-quality labels for training and evaluation.\n  - ULER outperforms PASTARej and other competitors on human-annotated data (Section 4.2; Table 7), supporting the central claim that ULER mimics human judgments better than baseline metrics.- **Transparency on setup and reproducibility details**\n  - Clear description of setup (splits, validation, grid search, hardware; Section 4, “Setup”; Section 4, “Model selection”; Appendix C.2), aiding reproducibility.\n  - Choice of explainer (KernelSHAP, 100 samples; Section 4, “Model selection”; Appendix D.2–D.3) and predictors (Section 4.1) is specified, making replication feasible.- **Limitations and scope explicitly discussed**\n  - The paper candidly discusses ULER’s limitations (e.g., reliance on high-quality annotations, focus on tabular data, sample complexity; Section 3.2), and proposes future directions (Section 6), which benefits clarity and realistic positioning.Weaknesses\n- **Heuristic simulation of “human judgments” in benchmarks**\n  - The simulated labels hinge on a fixed correlation threshold τz=0.25 (Section 4.1, “Simulating human judgments”), chosen to “ensure datasets with varying amounts of low-quality explanations” rather than validated against actual human perception — limiting construct validity.\n  - Per-feature wrongness sets Wz are derived from oracle–model L1 differences with u%=0.75 (Section 4.1), again without empirical calibration against human assessments — risking mismatch between simulated and actual human feedback.\n  - The oracle choice (Random Forest) and predictor choice (linear SVC/SVR) impose inductive-bias differences (Section 4.1), which may conflate explanation quality with model-class disparities rather than user-centric quality.\n  - Although robustness to alternative oracle is reported (Appendix C.4), the simulation remains fundamentally machine-defined rather than human-grounded, potentially inflating ULER’s advantages on synthetic tasks relative to subjective real-world settings.- **Limited theoretical grounding and assumptions in augmentation**\n  - The augmentation assumes label preservation under selective Gaussian perturbations (Equation 2; Section 3.1: “we stipulate…”), but provides no theoretical or empirical validation that small perturbations on “correct” vs. “wrong” features reliably preserve human judgments.\n  - No analysis of calibration or consistency of the rejector’s score r or threshold τ (Section 3.1), e.g., monotonicity properties, false-abstain vs. false-accept trade-offs beyond empirical rejection-rate sweeps.\n  - The choice of Σ (feature-wise standard deviations from D) and ε0 (noise scale) is justified via grid search (Appendix C.2) but lacks principled guidance or sensitivity analysis beyond modest ablation (Appendix C.5).\n  - No sample-complexity or generalization bounds are provided for learning r from augmented labels, leaving technical guarantees unaddressed (No direct evidence found in the manuscript).- **Evaluation focuses on AUROC and fractions, not user outcomes**\n  - The main metrics (AUROC; fraction of low-quality explanations in accepted/rejected sets; Section 4, “Evaluation metrics”) do not measure downstream impacts such as trust calibration or decision quality, despite being central to the motivation (Section 1; Figure 1).\n  - The rejection-threshold τ is conceptually tied to user capacity (Section 3.1), but experiments only sweep rejection rates (Section 4, “Setup”) — missing calibration studies that reflect practical constraints.\n  - Cost-sensitive analyses (e.g., human effort vs. abstention benefit) are not reported, though abstention incurs costs in many high-stakes applications (No direct evidence found in the manuscript).\n  - No measurement of how ULER affects correctness of accepted predictions or joint human–AI team performance, even though abstention should ideally improve overall decision-making (Section 3.2 admits marginal predictive gains but no empirical quantification).- **Human study scope and reliability reporting**\n  - The human study’s domain (soccer xG) involves only five features (Appendix D.2), which limits generality to richer tabular contexts; the overall AUROC is modest (0.63 ± 0.05; Section 4.2; Table 7).\n  - Inter-annotator reliability statistics (e.g., κ/α) are not reported; filtering based on standard deviation (>1.25) is used (Appendix D.4), but agreement metrics would improve rigor.\n  - PASTARej is adapted by dropping the embedding network (Section 4, “Competitors”), which may undercut PASTA’s strengths and complicate the fairness of comparison in the human study.\n  - The annotation interface is carefully designed (Section 4.2; Figures 6–7; Appendix D.3), but potential framing effects and biases are only qualitatively discussed (pilot studies), without quantitative validation (No direct evidence found in the manuscript).- **Generality beyond tabular attribution and scalability**\n  - The method is explicitly scoped to tabular feature attributions (Section 3.2), with no experiments on image/text or other explanation paradigms (counterfactuals, rules), limiting breadth.\n  - ULER relies on per-feature feedback, which may be costly or impractical for high-dimensional settings (Section 3.2 acknowledges sample complexity risks), and offers only a qualitative mitigation (“adapt ULER to rich pre-trained embedding space”).\n  - While the human-annotated dataset will be released (Abstract; Section 4.2), code release is not stated (No direct evidence found in the manuscript), hindering full reproducibility.\n  - The augmentation’s dependence on Σ estimated from D may be brittle across distributions or domains; cross-domain generalization is not studied (No direct evidence found in the manuscript).- **Limited analysis of interaction with prediction correctness/LtR**\n  - ULER targets explanation quality, but the relationship to prediction correctness is only discussed qualitatively (Section 3.2), with no empirical breakdown of error types (correct prediction/low-quality explanation vs. incorrect prediction/high-quality explanation).\n  - No experiments combining ULER with standard LtR are provided, although suggested as future work (Section 3.2; Section 6) — missing evidence on joint benefits when both mispredictions and low-quality explanations are deferred.\n  - The paper does not quantify how ULER affects calibration or confidence of accepted predictions (No direct evidence found in the manuscript).\n  - It is unclear how often ULER rejects correct predictions due to noisy explanations vs. truly problematic explanations — false-alarm analysis is absent (No direct evidence found in the manuscript).Suggestions for Improvement\n- **Strengthen the realism of simulated “human judgments”**\n  - Validate τz=0.25 (Section 4.1) by correlating the simulated low-/high-quality labels with a subset of human ratings on the benchmark datasets; report agreement statistics to justify the threshold.\n  - Calibrate the construction of Wz (u%=0.75; Section 4.1) against human per-feature flags on a small held-out set; adjust u% or the selection mechanism to better match human judgments.\n  - Include additional oracle/predictor pairs with similar inductive biases and report how simulation-based labels vary (Appendix C.4 already explores one; extend to more and quantify impacts).\n  - Where feasible, augment benchmarks with limited human annotations (e.g., 50–100 items per dataset) to directly test simulated labels against human perception.- **Provide theoretical or empirical guarantees for augmentation and rejector calibration**\n  - Study label preservation under perturbations (Equation 2; Section 3.1) by measuring human ratings pre/post perturbation on a small subset; report empirical invariance margins.\n  - Analyze sensitivity to ε0, Σ estimation, and s-vector design via targeted experiments beyond grid search (Appendix C.2) to quantify robustness and provide practical guidance.\n  - Calibrate r and τ (Section 3.1) with calibration curves (e.g., reliability plots of r-scores vs. empirical low-quality rates) and report false-accept/false-reject trade-offs.\n  - Consider deriving simple consistency properties (e.g., monotonicity under feature-wise perturbations) or sample-efficiency bounds for r trained on augmented data, even if approximate.- **Expand evaluation to user outcomes and cost-sensitive metrics**\n  - Measure trust and decision quality: run a small human-in-the-loop study where abstentions triggered by ULER affect downstream human actions, and quantify changes vs. baselines (Section 1 motivation; Figure 1).\n  - Implement user-capacity calibration of τ (Section 3.1) and report performance under realistic constraints (e.g., fixed human bandwidth, cost per abstention).\n  - Add cost-sensitive metrics (e.g., weighted utility combining false-accept/false-reject rates and abstention cost) to complement AUROC and fraction metrics (Section 4, “Evaluation metrics”).\n  - Quantify the effect of ULER on correctness/calibration of accepted predictions and team performance (e.g., deferral accuracy), thereby connecting explanation rejection to end goals (Section 3.2).- **Increase rigor and fairness in the human study**\n  - Report inter-annotator reliability (κ or α) and agreement distributions; include these alongside the filtering criterion (Appendix D.4) to strengthen validity.\n  - Compare ULER against PASTARej variants that retain an embedding backbone compatible with tabular data (e.g., feature encoders) to ensure fair comparison (Section 4, “Competitors”).\n  - Quantitatively assess framing effects: randomize question orders and interface variants (Figures 6–7), and report whether ratings differ statistically across conditions.\n  - Explore additional domains or richer tabular settings (more features) to test generality; report how performance scales with feature count and annotator effort (Appendix D.2).- **Broaden scope and improve scalability/reproducibility**\n  - Evaluate ULER on image/text attributions (e.g., gradient-based explanations for CNNs; Section 3.2 acknowledges scope), or rule/counterfactual explanations, to demonstrate breadth.\n  - Introduce strategies for high-dimensional per-feature feedback (e.g., grouping features, active feature querying) and quantify annotation cost savings (Section 3.2).\n  - Release code and detailed scripts (splits, hyperparameters; Section 4, “Setup”; Appendix C.2) alongside the dataset to facilitate end-to-end replication.\n  - Test cross-domain generalization by training ULER on one dataset and evaluating on another with related features; analyze robustness of Σ-based augmentation across domains.- **Analyze interaction with prediction correctness and combine with LtR**\n  - Provide confusion-matrix-style breakdowns (correct/incorrect prediction vs. high/low explanation quality) and show how ULER shifts these counts; quantify false-alarm rates and benefits (Section 3.2).\n  - Implement and report experiments that combine ULER with a standard LtR method (e.g., PredAmb or NovRejX; Section 3.2, Section 6) to quantify joint advantages in deferring both mispredictions and low-quality explanations.\n  - Study effects on calibration of accepted predictions (e.g., Brier score changes; Appendix D.2 reports Brier for xG model) when ULER is applied.\n  - Report how often ULER rejects correct predictions due to explainer noise vs. truly problematic explanations, and assess robustness under different explainers (extend Appendix C.3) and SHAP sampling budgets.Score\n- Overall (10): 7 — Clear formulation (Definition 1; Equation 1), strong empirical gains across diverse baselines (Figure 2; Table 1; Table 2), and a useful human-annotated dataset, tempered by heuristic simulation and limited user-outcome evaluation (Section 4.1; Section 4.2).\n- Novelty (10): 7 — Introduces LtX and a user-centric rejector trained from human labels and per-feature feedback (Section 3.1), distinguishing from LtR and machine-side metrics (Section 2), though augmentation is conceptually simple.\n- Technical Quality (10): 6 — Method is sound and robustly evaluated (Appendix C.3–C.5; Tables 4–6), but lacks theoretical guarantees for augmentation/label preservation and outcome-focused metrics (Section 3.1; Section 4).\n- Clarity (10): 8 — Well-structured with precise definitions (Section 3), comprehensive experimental details (Section 4; Appendix C.2), and transparent limitations (Section 3.2), though some reliability and calibration analyses are missing.\n- Confidence (5): 4 — Judgment based on thorough reading of the manuscript, multiple quantitative results (Figure 2; Tables 1–7) and detailed appendices; some claims (e.g., simulation realism, user outcomes) would benefit from additional evidence.",
  "final_review": "Summary\n- The paper introduces Learning to Reject Low-Quality Explanations (LtX) and proposes ULER (User-centric Low-quality Explanation Rejector), which abstains when the explanation accompanying a prediction is deemed unsatisfactory by users. ULER trains a rejector using human feedback consisting of binary explanation-quality labels and per-feature relevance judgments, augmented via a label-preserving stochastic perturbation (Equation 2 in Section 3.1), and learned with a simple classifier and thresholding (Definition 1 and Equation 1 in Section 3). Experiments across eight tabular datasets with simulated human judgments (Section 4.1) and a human-annotated soccer xG dataset (Section 4.2) show ULER improves rejection of low-quality explanations (Figure 2; Table 1; Table 2; Appendix Table 7). Robustness analyses vary the explainer (LIME; Appendix C.3) and oracle predictors (Appendix C.4), and an augmentation ablation is included (Appendix C.5). A human-annotated dataset is released (Abstract; Section 4.2).Strengths\n- **Clear problem formulation and rejector definition**\n  - The LtX setting is formalized with a clear model definition, including predictor, explainer, and rejector, and a concrete abstention rule (Definition 1; Equation 1 in Section 3). This matters for clarity and reproducibility, making the contribution easy to situate within LtR/XAI literature.\n  - The paper explicitly motivates why explanation quality should drive rejection (Section 1; Figure 1), aligning with recognized guidelines (e.g., NIST Four Principles; Section 1), which increases impact in high-stakes domains.\n  - Distinguishes LtX from traditional LtR (Section 2, “Learning to reject”) and existing quality metrics (Section 2, “Metrics of Explanation Quality”), improving conceptual novelty.\n- **Simple, user-centered method (ULER) leveraging human feedback**\n  - ULER uses human binary explanation-quality labels and per-feature judgments to construct training data (Section 3.1, “The rejector’s training data”), grounding the rejector in user perception — important for practical human alignment.\n  - A feedback-driven augmentation strategy preserves labels while perturbing selected features (Equation 2 and the s-vector design in Section 3.1), a technically sound and pragmatic approach to address data scarcity.\n  - Method is model-agnostic for the rejector and explainer choices (Section 3.1, “Learning the rejector”; Section 3.2), enhancing applicability.\n- **Comprehensive empirical evaluation and competitive baselines**\n  - Eight baselines span standard LtR and explanation-aware strategies (Section 4, “Competitors”), including PASTARej (human-side metric) and faithfulness/stability/complexity (machine-side metrics), ensuring broad coverage and experimental rigor.\n  - Evaluation metrics include AUROC and percentages of low-quality explanations in accepted/rejected sets across rejection rates (Section 4, “Evaluation metrics”), appropriate for ranking-based rejectors.\n  - Multi-dataset evaluation across diverse tasks (classification/regression; healthcare/economics/law/industry) improves external validity (Section 4.1, “Datasets”; Appendix C.1).\n- **Consistent performance gains and robust trends**\n  - ULER shows clear improvements over baselines in separating low- vs. high-quality explanations (Table 1; Figure 2; Section 4.1, Q1), with quantified relative reductions in low-quality explanations in the accepted set and high fractions in the rejected set.\n  - Robustness to alternative explainer (LIME) maintains advantages (Appendix C.3; Figure 4; Table 4), supporting method stability.\n  - Robustness to alternative oracle predictor (Appendix C.4; Figure 5; Table 5) and an augmentation ablation (Appendix C.5; Table 6) further underlines consistency and technical soundness.\n- **Carefully designed human annotation study and dataset contribution**\n  - The paper details recruitment, vetting, and interface design for human ratings and per-feature feedback (Section 4.2; Appendix D.3), including ethical approval (Section 4.2), which strengthens the credibility of human-centered evaluation.\n  - Provides preprocessing steps for quality control and aggregation (Section 4.2; Appendix D.4), ensuring higher-quality labels for training and evaluation.\n  - ULER outperforms PASTARej and other competitors on human-annotated data (Section 4.2; Appendix C.6; Table 7), supporting the central claim that ULER mimics human judgments better than baseline metrics.\n- **Transparency on setup and reproducibility details**\n  - Clear description of setup (splits, validation, grid search, hardware; Section 4, “Setup”; Section 4, “Model selection”; Appendix C.2), aiding reproducibility.\n  - Choice of explainer (KernelSHAP, 100 samples; Section 4, “Model selection”; Appendix D.2–D.3) and predictors (Section 4.1) is specified, making replication feasible.\n- **Limitations and scope explicitly discussed**\n  - The paper candidly discusses ULER’s limitations (e.g., reliance on high-quality annotations, focus on tabular data, sample complexity; Section 3.2), and proposes future directions (Section 6), which benefits clarity and realistic positioning.Weaknesses\n- **Heuristic simulation of “human judgments” in benchmarks**\n  - The simulated labels hinge on a fixed correlation threshold τz=0.25 (Section 4.1, “Simulating human judgments”), chosen to “ensure datasets with varying amounts of low-quality explanations” rather than validated against actual human perception — limiting construct validity.\n  - Per-feature wrongness sets Wz are derived from oracle–model L1 differences with u%=0.75 (Section 4.1), again without empirical calibration against human assessments — risking mismatch between simulated and actual human feedback.\n  - The oracle choice (Random Forest) and predictor choice (linear SVC/SVR) impose inductive-bias differences (Section 4.1), which may conflate explanation quality with model-class disparities rather than user-centric quality.\n  - Although robustness to alternative oracle is reported (Appendix C.4), the simulation remains fundamentally machine-defined rather than human-grounded, potentially inflating ULER’s advantages on synthetic tasks relative to subjective real-world settings.\n- **Limited theoretical grounding and assumptions in augmentation**\n  - The augmentation assumes label preservation under selective Gaussian perturbations (Equation 2; Section 3.1: “we stipulate…”), but provides no theoretical or empirical validation that small perturbations on “correct” vs. “wrong” features reliably preserve human judgments.\n  - No analysis of calibration or consistency of the rejector’s score r or threshold τ (Section 3.1), e.g., monotonicity properties, false-abstain vs. false-accept trade-offs beyond empirical rejection-rate sweeps.\n  - The choice of Σ (feature-wise standard deviations from D) and ε0 (noise scale) is justified via grid search (Appendix C.2) but lacks principled guidance or sensitivity analysis beyond modest ablation (Appendix C.5).\n  - No sample-complexity or generalization bounds are provided for learning r from augmented labels, leaving technical guarantees unaddressed (No direct evidence found in the manuscript).\n  - Equation 2’s covariance specification “ε0 s × Σ” (Section 3.1) is ambiguous about how the binary vector s interacts with the diagonal matrix Σ, raising concerns about positive semi-definiteness and unambiguous implementation — which matters for reproducibility and technical soundness.\n  - ULER relies on both Wz and Cz (Section 3.1), but in the human study only incorrect features are explicitly annotated (Section 4.2; Appendix D.4), and the operational derivation of Cz (e.g., complement of Wz) is not specified — affecting implementability of augmentation in the human-annotated setting.\n- **Evaluation focuses on AUROC and fractions, not user outcomes**\n  - The main metrics (AUROC; fraction of low-quality explanations in accepted/rejected sets; Section 4, “Evaluation metrics”) do not measure downstream impacts such as trust calibration or decision quality, despite being central to the motivation (Section 1; Figure 1).\n  - The rejection-threshold τ is conceptually tied to user capacity (Section 3.1), but experiments only sweep rejection rates (Section 4, “Setup”) — missing calibration studies that reflect practical constraints.\n  - Cost-sensitive analyses (e.g., human effort vs. abstention benefit) are not reported, though abstention incurs costs in many high-stakes applications (No direct evidence found in the manuscript).\n  - No measurement of how ULER affects correctness of accepted predictions or joint human–AI team performance, even though abstention should ideally improve overall decision-making (Section 3.2 admits marginal predictive gains but no empirical quantification).\n- **Human study scope and reliability reporting**\n  - The human study’s domain (soccer xG) involves only five features (Appendix D.2), which limits generality to richer tabular contexts; the overall AUROC is modest (0.63 ± 0.05; Section 4.2; Table 7).\n  - Inter-annotator reliability statistics (e.g., κ/α) are not reported; filtering based on standard deviation (>1.25) is used (Appendix D.4), and the final sample size used for reported AUROC is only stated in the appendix (718 explanations; Appendix D.4), not in the main text (Section 4.2) — this impacts transparency.\n  - PASTARej is adapted by dropping the embedding network (Section 4, “Competitors”), which may undercut PASTA’s strengths and complicate the fairness of comparison in the human study.\n  - The annotation interface is carefully designed (Section 4.2; Figures 6–7; Appendix D.3), but potential framing effects and biases are only qualitatively discussed (pilot studies), without quantitative validation (No direct evidence found in the manuscript).\n- **Generality beyond tabular attribution and scalability**\n  - The method is explicitly scoped to tabular feature attributions (Section 3.2), with no experiments on image/text or other explanation paradigms (counterfactuals, rules), limiting breadth.\n  - ULER relies on per-feature feedback, which may be costly or impractical for high-dimensional settings (Section 3.2 acknowledges sample complexity risks), and offers only a qualitative mitigation (“adapt ULER to rich pre-trained embedding space”).\n  - While the human-annotated dataset will be released (Abstract; Section 4.2), code release is not stated (No direct evidence found in the manuscript), hindering full reproducibility.\n  - The augmentation’s dependence on Σ estimated from D may be brittle across distributions or domains; cross-domain generalization is not studied (No direct evidence found in the manuscript).\n- **Limited analysis of interaction with prediction correctness/LtR**\n  - ULER targets explanation quality, but the relationship to prediction correctness is only discussed qualitatively (Section 3.2), with no empirical breakdown of error types (correct prediction/low-quality explanation vs. incorrect prediction/high-quality explanation).\n  - No experiments combining ULER with standard LtR are provided, although suggested as future work (Section 3.2; Section 6) — missing evidence on joint benefits when both mispredictions and low-quality explanations are deferred.\n  - The paper does not quantify how ULER affects calibration or confidence of accepted predictions (No direct evidence found in the manuscript).\n  - It is unclear how often ULER rejects correct predictions due to noisy explanations vs. truly problematic explanations — false-alarm analysis is absent (No direct evidence found in the manuscript).\n- **Ambiguities in probability and variance computations for baselines and metrics**\n  - PredAmb for classification uses margins of class probabilities (Section 4, “Competitors”), but the predictor is a linear SVC “with default scikit-learn implementations” (Section 4.1), which does not natively provide calibrated probabilities; the manuscript does not state how probabilities were obtained — affecting reproducibility and validity of this baseline.\n  - The faithfulness metric computes Δf using “absolute difference in positive class probability” for classification (Appendix B.1; Equation after (4)), yet the probability source for the linear SVC is not specified — impacting technical correctness of machine-side metrics.\n  - PredAmb for regression relies on conditional variance for each input (Section 4, “Competitors”), but SVR does not provide predictive variance by default (Section 4.1); the method for estimating variance is not described — undermining comparability on regression tasks.Suggestions for Improvement\n- **Strengthen the realism of simulated “human judgments”**\n  - Validate τz=0.25 (Section 4.1) by correlating the simulated low-/high-quality labels with a subset of human ratings on the benchmark datasets; report agreement statistics to justify the threshold.\n  - Calibrate the construction of Wz (u%=0.75; Section 4.1) against human per-feature flags on a small held-out set; adjust u% or the selection mechanism to better match human judgments.\n  - Include additional oracle/predictor pairs with similar inductive biases and report how simulation-based labels vary (Appendix C.4 already explores one; extend to more and quantify impacts).\n  - Where feasible, augment benchmarks with limited human annotations (e.g., 50–100 items per dataset) to directly test simulated labels against human perception.\n- **Provide theoretical or empirical guarantees for augmentation and rejector calibration**\n  - Study label preservation under perturbations (Equation 2; Section 3.1) by measuring human ratings pre/post perturbation on a small subset; report empirical invariance margins.\n  - Analyze sensitivity to ε0, Σ estimation, and s-vector design via targeted experiments beyond grid search (Appendix C.2) to quantify robustness and provide practical guidance.\n  - Calibrate r and τ (Section 3.1) with calibration curves (e.g., reliability plots of r-scores vs. empirical low-quality rates) and report false-accept/false-reject trade-offs.\n  - Consider deriving simple consistency properties (e.g., monotonicity under feature-wise perturbations) or sample-efficiency bounds for r trained on augmented data, even if approximate.\n  - Clarify the covariance construction in Equation 2 (Section 3.1), e.g., by specifying whether diag(s)Σdiag(s) is used and how PSD is ensured; this improves implementability.\n  - Explicitly define how Cz is obtained in the human-annotated setting (Section 4.2; Appendix D.4), e.g., whether it is the complement of Wz over the feature set, and state this in Section 3.1 to support faithful augmentation.\n- **Expand evaluation to user outcomes and cost-sensitive metrics**\n  - Measure trust and decision quality: run a small human-in-the-loop study where abstentions triggered by ULER affect downstream human actions, and quantify changes vs. baselines (Section 1 motivation; Figure 1).\n  - Implement user-capacity calibration of τ (Section 3.1) and report performance under realistic constraints (e.g., fixed human bandwidth, cost per abstention).\n  - Add cost-sensitive metrics (e.g., weighted utility combining false-accept/false-reject rates and abstention cost) to complement AUROC and fraction metrics (Section 4, “Evaluation metrics”).\n  - Quantify the effect of ULER on correctness/calibration of accepted predictions and team performance (e.g., deferral accuracy), thereby connecting explanation rejection to end goals (Section 3.2).\n- **Increase rigor and fairness in the human study**\n  - Report inter-annotator reliability (κ or α) and agreement distributions; include these alongside the filtering criterion and explicitly state the final sample size used for AUROC in the main text (Section 4.2; Appendix D.4) to strengthen validity.\n  - Compare ULER against PASTARej variants that retain an embedding backbone compatible with tabular data (e.g., feature encoders) to ensure fair comparison (Section 4, “Competitors”).\n  - Quantitatively assess framing effects: randomize question orders and interface variants (Figures 6–7), and report whether ratings differ statistically across conditions.\n  - Explore additional domains or richer tabular settings (more features) to test generality; report how performance scales with feature count and annotator effort (Appendix D.2).\n- **Broaden scope and improve scalability/reproducibility**\n  - Evaluate ULER on image/text attributions (e.g., gradient-based explanations for CNNs; Section 3.2 acknowledges scope), or rule/counterfactual explanations, to demonstrate breadth.\n  - Introduce strategies for high-dimensional per-feature feedback (e.g., grouping features, active feature querying) and quantify annotation cost savings (Section 3.2).\n  - Release code and detailed scripts (splits, hyperparameters; Section 4, “Setup”; Appendix C.2) alongside the dataset to facilitate end-to-end replication.\n  - Test cross-domain generalization by training ULER on one dataset and evaluating on another with related features; analyze robustness of Σ-based augmentation across domains.\n- **Analyze interaction with prediction correctness and combine with LtR**\n  - Provide confusion-matrix-style breakdowns (correct/incorrect prediction vs. high/low explanation quality) and show how ULER shifts these counts; quantify false-alarm rates and benefits (Section 3.2).\n  - Implement and report experiments that combine ULER with a standard LtR method (e.g., PredAmb or NovRejX; Section 3.2, Section 6) to quantify joint advantages in deferring both mispredictions and low-quality explanations.\n  - Study effects on calibration of accepted predictions (e.g., Brier score changes; Appendix D.2 reports Brier for xG model) when ULER is applied.\n  - Report how often ULER rejects correct predictions due to explainer noise vs. truly problematic explanations, and assess robustness under different explainers (extend Appendix C.3) and SHAP sampling budgets.\n- **Clarify probability and variance computations supporting baselines and metrics**\n  - For PredAmb in classification, specify how class probabilities were obtained for the linear SVC (Section 4.1; Section 4, “Competitors”), e.g., enabling probability estimation or post-hoc calibration, and report calibration checks.\n  - For the faithfulness metric’s Δf on classification (Appendix B.1), detail the probability source used by the linear SVC and verify stability across calibration methods.\n  - For PredAmb in regression (Section 4, “Competitors”; Section 4.1), describe the method used to estimate conditional variance for the SVR (e.g., ensembles or bootstrap) and include sensitivity analyses.Score\n- Overall (10): 7 — Clear LtX formalization (Definition 1; Equation 1) and strong empirical gains with robustness checks (Figure 2; Table 1; Appendix Tables 4–6), tempered by heuristic simulation (Section 4.1) and missing details on probability/variance computations for baselines (Section 4; Section 4.1; Appendix B.1).\n- Novelty (10): 7 — LtX and a user-centric rejector trained from human labels and per-feature feedback (Section 3.1) distinct from LtR and machine-side metrics (Section 2), with augmentation conceptually simple but well-motivated.\n- Technical Quality (10): 5 — Sound empirical evaluation with robustness (Appendix C.3–C.5), but limited theoretical guarantees for augmentation (Section 3.1), ambiguity in Equation 2’s covariance (Section 3.1), and unclear probability/variance estimation for PredAmb and faithfulness Δf (Section 4; Section 4.1; Appendix B.1).\n- Clarity (10): 7 — Generally well-structured with precise definitions (Section 3), detailed experimental setup (Section 4; Appendix C.2), and transparent limitations (Section 3.2), with room to improve reporting on human-study sample size in main text (Section 4.2; Appendix D.4) and certain implementation details.\n- Confidence (5): 4 — Based on careful reading of the manuscript’s methods and extensive experimental results (Figure 2; Tables 1–7; Appendices C–D); some aspects (e.g., simulation realism and probability/variance details) would benefit from further clarification and evidence.",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 7,
        "novelty": 7,
        "technical_quality": 6,
        "clarity": 8,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 7,
        "novelty": 7,
        "technical_quality": 5,
        "clarity": 7,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "Summary\n- The paper introduces Learning to Reject Low-Quality Explanations (LtX) and proposes ULER (User-centric Low-quality Explanation Rejector), which abstains when the explanation accompanying a prediction is deemed unsatisfactory by users. ULER trains a rejector using human feedback consisting of binary explanation-quality labels and per-feature relevance judgments, augmented via a label-preserving stochastic perturbation (Equation 2 in Section 3.1), and learned with a simple classifier and thresholding (Definition 1 and Equation 1 in Section 3). Experiments across eight tabular datasets with simulated human judgments (Section 4.1) and a human-annotated soccer xG dataset (Section 4.2) show ULER improves rejection of low-quality explanations (Figure 2; Table 1; Table 2; Appendix Table 7). Robustness analyses vary the explainer (LIME; Appendix C.3) and oracle predictors (Appendix C.4), and an augmentation ablation is included (Appendix C.5). A human-annotated dataset is released (Abstract; Section 4.2).Strengths\n- **Clear problem formulation and rejector definition**\n  - The LtX setting is formalized with a clear model definition, including predictor, explainer, and rejector, and a concrete abstention rule (Definition 1; Equation 1 in Section 3). This matters for clarity and reproducibility, making the contribution easy to situate within LtR/XAI literature.\n  - The paper explicitly motivates why explanation quality should drive rejection (Section 1; Figure 1), aligning with recognized guidelines (e.g., NIST Four Principles; Section 1), which increases impact in high-stakes domains.\n  - Distinguishes LtX from traditional LtR (Section 2, “Learning to reject”) and existing quality metrics (Section 2, “Metrics of Explanation Quality”), improving conceptual novelty.\n- **Simple, user-centered method (ULER) leveraging human feedback**\n  - ULER uses human binary explanation-quality labels and per-feature judgments to construct training data (Section 3.1, “The rejector’s training data”), grounding the rejector in user perception — important for practical human alignment.\n  - A feedback-driven augmentation strategy preserves labels while perturbing selected features (Equation 2 and the s-vector design in Section 3.1), a technically sound and pragmatic approach to address data scarcity.\n  - Method is model-agnostic for the rejector and explainer choices (Section 3.1, “Learning the rejector”; Section 3.2), enhancing applicability.\n- **Comprehensive empirical evaluation and competitive baselines**\n  - Eight baselines span standard LtR and explanation-aware strategies (Section 4, “Competitors”), including PASTARej (human-side metric) and faithfulness/stability/complexity (machine-side metrics), ensuring broad coverage and experimental rigor.\n  - Evaluation metrics include AUROC and percentages of low-quality explanations in accepted/rejected sets across rejection rates (Section 4, “Evaluation metrics”), appropriate for ranking-based rejectors.\n  - Multi-dataset evaluation across diverse tasks (classification/regression; healthcare/economics/law/industry) improves external validity (Section 4.1, “Datasets”; Appendix C.1).\n- **Consistent performance gains and robust trends**\n  - ULER shows clear improvements over baselines in separating low- vs. high-quality explanations (Table 1; Figure 2; Section 4.1, Q1), with quantified relative reductions in low-quality explanations in the accepted set and high fractions in the rejected set.\n  - Robustness to alternative explainer (LIME) maintains advantages (Appendix C.3; Figure 4; Table 4), supporting method stability.\n  - Robustness to alternative oracle predictor (Appendix C.4; Figure 5; Table 5) and an augmentation ablation (Appendix C.5; Table 6) further underlines consistency and technical soundness.\n- **Carefully designed human annotation study and dataset contribution**\n  - The paper details recruitment, vetting, and interface design for human ratings and per-feature feedback (Section 4.2; Appendix D.3), including ethical approval (Section 4.2), which strengthens the credibility of human-centered evaluation.\n  - Provides preprocessing steps for quality control and aggregation (Section 4.2; Appendix D.4), ensuring higher-quality labels for training and evaluation.\n  - ULER outperforms PASTARej and other competitors on human-annotated data (Section 4.2; Appendix C.6; Table 7), supporting the central claim that ULER mimics human judgments better than baseline metrics.\n- **Transparency on setup and reproducibility details**\n  - Clear description of setup (splits, validation, grid search, hardware; Section 4, “Setup”; Section 4, “Model selection”; Appendix C.2), aiding reproducibility.\n  - Choice of explainer (KernelSHAP, 100 samples; Section 4, “Model selection”; Appendix D.2–D.3) and predictors (Section 4.1) is specified, making replication feasible.\n- **Limitations and scope explicitly discussed**\n  - The paper candidly discusses ULER’s limitations (e.g., reliance on high-quality annotations, focus on tabular data, sample complexity; Section 3.2), and proposes future directions (Section 6), which benefits clarity and realistic positioning.Weaknesses\n- **Heuristic simulation of “human judgments” in benchmarks**\n  - The simulated labels hinge on a fixed correlation threshold τz=0.25 (Section 4.1, “Simulating human judgments”), chosen to “ensure datasets with varying amounts of low-quality explanations” rather than validated against actual human perception — limiting construct validity.\n  - Per-feature wrongness sets Wz are derived from oracle–model L1 differences with u%=0.75 (Section 4.1), again without empirical calibration against human assessments — risking mismatch between simulated and actual human feedback.\n  - The oracle choice (Random Forest) and predictor choice (linear SVC/SVR) impose inductive-bias differences (Section 4.1), which may conflate explanation quality with model-class disparities rather than user-centric quality.\n  - Although robustness to alternative oracle is reported (Appendix C.4), the simulation remains fundamentally machine-defined rather than human-grounded, potentially inflating ULER’s advantages on synthetic tasks relative to subjective real-world settings.\n- **Limited theoretical grounding and assumptions in augmentation**\n  - The augmentation assumes label preservation under selective Gaussian perturbations (Equation 2; Section 3.1: “we stipulate…”), but provides no theoretical or empirical validation that small perturbations on “correct” vs. “wrong” features reliably preserve human judgments.\n  - No analysis of calibration or consistency of the rejector’s score r or threshold τ (Section 3.1), e.g., monotonicity properties, false-abstain vs. false-accept trade-offs beyond empirical rejection-rate sweeps.\n  - The choice of Σ (feature-wise standard deviations from D) and ε0 (noise scale) is justified via grid search (Appendix C.2) but lacks principled guidance or sensitivity analysis beyond modest ablation (Appendix C.5).\n  - No sample-complexity or generalization bounds are provided for learning r from augmented labels, leaving technical guarantees unaddressed (No direct evidence found in the manuscript).\n  - Equation 2’s covariance specification “ε0 s × Σ” (Section 3.1) is ambiguous about how the binary vector s interacts with the diagonal matrix Σ, raising concerns about positive semi-definiteness and unambiguous implementation — which matters for reproducibility and technical soundness.\n  - ULER relies on both Wz and Cz (Section 3.1), but in the human study only incorrect features are explicitly annotated (Section 4.2; Appendix D.4), and the operational derivation of Cz (e.g., complement of Wz) is not specified — affecting implementability of augmentation in the human-annotated setting.\n- **Evaluation focuses on AUROC and fractions, not user outcomes**\n  - The main metrics (AUROC; fraction of low-quality explanations in accepted/rejected sets; Section 4, “Evaluation metrics”) do not measure downstream impacts such as trust calibration or decision quality, despite being central to the motivation (Section 1; Figure 1).\n  - The rejection-threshold τ is conceptually tied to user capacity (Section 3.1), but experiments only sweep rejection rates (Section 4, “Setup”) — missing calibration studies that reflect practical constraints.\n  - Cost-sensitive analyses (e.g., human effort vs. abstention benefit) are not reported, though abstention incurs costs in many high-stakes applications (No direct evidence found in the manuscript).\n  - No measurement of how ULER affects correctness of accepted predictions or joint human–AI team performance, even though abstention should ideally improve overall decision-making (Section 3.2 admits marginal predictive gains but no empirical quantification).\n- **Human study scope and reliability reporting**\n  - The human study’s domain (soccer xG) involves only five features (Appendix D.2), which limits generality to richer tabular contexts; the overall AUROC is modest (0.63 ± 0.05; Section 4.2; Table 7).\n  - Inter-annotator reliability statistics (e.g., κ/α) are not reported; filtering based on standard deviation (>1.25) is used (Appendix D.4), and the final sample size used for reported AUROC is only stated in the appendix (718 explanations; Appendix D.4), not in the main text (Section 4.2) — this impacts transparency.\n  - PASTARej is adapted by dropping the embedding network (Section 4, “Competitors”), which may undercut PASTA’s strengths and complicate the fairness of comparison in the human study.\n  - The annotation interface is carefully designed (Section 4.2; Figures 6–7; Appendix D.3), but potential framing effects and biases are only qualitatively discussed (pilot studies), without quantitative validation (No direct evidence found in the manuscript).\n- **Generality beyond tabular attribution and scalability**\n  - The method is explicitly scoped to tabular feature attributions (Section 3.2), with no experiments on image/text or other explanation paradigms (counterfactuals, rules), limiting breadth.\n  - ULER relies on per-feature feedback, which may be costly or impractical for high-dimensional settings (Section 3.2 acknowledges sample complexity risks), and offers only a qualitative mitigation (“adapt ULER to rich pre-trained embedding space”).\n  - While the human-annotated dataset will be released (Abstract; Section 4.2), code release is not stated (No direct evidence found in the manuscript), hindering full reproducibility.\n  - The augmentation’s dependence on Σ estimated from D may be brittle across distributions or domains; cross-domain generalization is not studied (No direct evidence found in the manuscript).\n- **Limited analysis of interaction with prediction correctness/LtR**\n  - ULER targets explanation quality, but the relationship to prediction correctness is only discussed qualitatively (Section 3.2), with no empirical breakdown of error types (correct prediction/low-quality explanation vs. incorrect prediction/high-quality explanation).\n  - No experiments combining ULER with standard LtR are provided, although suggested as future work (Section 3.2; Section 6) — missing evidence on joint benefits when both mispredictions and low-quality explanations are deferred.\n  - The paper does not quantify how ULER affects calibration or confidence of accepted predictions (No direct evidence found in the manuscript).\n  - It is unclear how often ULER rejects correct predictions due to noisy explanations vs. truly problematic explanations — false-alarm analysis is absent (No direct evidence found in the manuscript).\n- **Ambiguities in probability and variance computations for baselines and metrics**\n  - PredAmb for classification uses margins of class probabilities (Section 4, “Competitors”), but the predictor is a linear SVC “with default scikit-learn implementations” (Section 4.1), which does not natively provide calibrated probabilities; the manuscript does not state how probabilities were obtained — affecting reproducibility and validity of this baseline.\n  - The faithfulness metric computes Δf using “absolute difference in positive class probability” for classification (Appendix B.1; Equation after (4)), yet the probability source for the linear SVC is not specified — impacting technical correctness of machine-side metrics.\n  - PredAmb for regression relies on conditional variance for each input (Section 4, “Competitors”), but SVR does not provide predictive variance by default (Section 4.1); the method for estimating variance is not described — undermining comparability on regression tasks.Suggestions for Improvement\n- **Strengthen the realism of simulated “human judgments”**\n  - Validate τz=0.25 (Section 4.1) by correlating the simulated low-/high-quality labels with a subset of human ratings on the benchmark datasets; report agreement statistics to justify the threshold.\n  - Calibrate the construction of Wz (u%=0.75; Section 4.1) against human per-feature flags on a small held-out set; adjust u% or the selection mechanism to better match human judgments.\n  - Include additional oracle/predictor pairs with similar inductive biases and report how simulation-based labels vary (Appendix C.4 already explores one; extend to more and quantify impacts).\n  - Where feasible, augment benchmarks with limited human annotations (e.g., 50–100 items per dataset) to directly test simulated labels against human perception.\n- **Provide theoretical or empirical guarantees for augmentation and rejector calibration**\n  - Study label preservation under perturbations (Equation 2; Section 3.1) by measuring human ratings pre/post perturbation on a small subset; report empirical invariance margins.\n  - Analyze sensitivity to ε0, Σ estimation, and s-vector design via targeted experiments beyond grid search (Appendix C.2) to quantify robustness and provide practical guidance.\n  - Calibrate r and τ (Section 3.1) with calibration curves (e.g., reliability plots of r-scores vs. empirical low-quality rates) and report false-accept/false-reject trade-offs.\n  - Consider deriving simple consistency properties (e.g., monotonicity under feature-wise perturbations) or sample-efficiency bounds for r trained on augmented data, even if approximate.\n  - Clarify the covariance construction in Equation 2 (Section 3.1), e.g., by specifying whether diag(s)Σdiag(s) is used and how PSD is ensured; this improves implementability.\n  - Explicitly define how Cz is obtained in the human-annotated setting (Section 4.2; Appendix D.4), e.g., whether it is the complement of Wz over the feature set, and state this in Section 3.1 to support faithful augmentation.\n- **Expand evaluation to user outcomes and cost-sensitive metrics**\n  - Measure trust and decision quality: run a small human-in-the-loop study where abstentions triggered by ULER affect downstream human actions, and quantify changes vs. baselines (Section 1 motivation; Figure 1).\n  - Implement user-capacity calibration of τ (Section 3.1) and report performance under realistic constraints (e.g., fixed human bandwidth, cost per abstention).\n  - Add cost-sensitive metrics (e.g., weighted utility combining false-accept/false-reject rates and abstention cost) to complement AUROC and fraction metrics (Section 4, “Evaluation metrics”).\n  - Quantify the effect of ULER on correctness/calibration of accepted predictions and team performance (e.g., deferral accuracy), thereby connecting explanation rejection to end goals (Section 3.2).\n- **Increase rigor and fairness in the human study**\n  - Report inter-annotator reliability (κ or α) and agreement distributions; include these alongside the filtering criterion and explicitly state the final sample size used for AUROC in the main text (Section 4.2; Appendix D.4) to strengthen validity.\n  - Compare ULER against PASTARej variants that retain an embedding backbone compatible with tabular data (e.g., feature encoders) to ensure fair comparison (Section 4, “Competitors”).\n  - Quantitatively assess framing effects: randomize question orders and interface variants (Figures 6–7), and report whether ratings differ statistically across conditions.\n  - Explore additional domains or richer tabular settings (more features) to test generality; report how performance scales with feature count and annotator effort (Appendix D.2).\n- **Broaden scope and improve scalability/reproducibility**\n  - Evaluate ULER on image/text attributions (e.g., gradient-based explanations for CNNs; Section 3.2 acknowledges scope), or rule/counterfactual explanations, to demonstrate breadth.\n  - Introduce strategies for high-dimensional per-feature feedback (e.g., grouping features, active feature querying) and quantify annotation cost savings (Section 3.2).\n  - Release code and detailed scripts (splits, hyperparameters; Section 4, “Setup”; Appendix C.2) alongside the dataset to facilitate end-to-end replication.\n  - Test cross-domain generalization by training ULER on one dataset and evaluating on another with related features; analyze robustness of Σ-based augmentation across domains.\n- **Analyze interaction with prediction correctness and combine with LtR**\n  - Provide confusion-matrix-style breakdowns (correct/incorrect prediction vs. high/low explanation quality) and show how ULER shifts these counts; quantify false-alarm rates and benefits (Section 3.2).\n  - Implement and report experiments that combine ULER with a standard LtR method (e.g., PredAmb or NovRejX; Section 3.2, Section 6) to quantify joint advantages in deferring both mispredictions and low-quality explanations.\n  - Study effects on calibration of accepted predictions (e.g., Brier score changes; Appendix D.2 reports Brier for xG model) when ULER is applied.\n  - Report how often ULER rejects correct predictions due to explainer noise vs. truly problematic explanations, and assess robustness under different explainers (extend Appendix C.3) and SHAP sampling budgets.\n- **Clarify probability and variance computations supporting baselines and metrics**\n  - For PredAmb in classification, specify how class probabilities were obtained for the linear SVC (Section 4.1; Section 4, “Competitors”), e.g., enabling probability estimation or post-hoc calibration, and report calibration checks.\n  - For the faithfulness metric’s Δf on classification (Appendix B.1), detail the probability source used by the linear SVC and verify stability across calibration methods.\n  - For PredAmb in regression (Section 4, “Competitors”; Section 4.1), describe the method used to estimate conditional variance for the SVR (e.g., ensembles or bootstrap) and include sensitivity analyses.Score\n- Overall (10): 7 — Clear LtX formalization (Definition 1; Equation 1) and strong empirical gains with robustness checks (Figure 2; Table 1; Appendix Tables 4–6), tempered by heuristic simulation (Section 4.1) and missing details on probability/variance computations for baselines (Section 4; Section 4.1; Appendix B.1).\n- Novelty (10): 7 — LtX and a user-centric rejector trained from human labels and per-feature feedback (Section 3.1) distinct from LtR and machine-side metrics (Section 2), with augmentation conceptually simple but well-motivated.\n- Technical Quality (10): 5 — Sound empirical evaluation with robustness (Appendix C.3–C.5), but limited theoretical guarantees for augmentation (Section 3.1), ambiguity in Equation 2’s covariance (Section 3.1), and unclear probability/variance estimation for PredAmb and faithfulness Δf (Section 4; Section 4.1; Appendix B.1).\n- Clarity (10): 7 — Generally well-structured with precise definitions (Section 3), detailed experimental setup (Section 4; Appendix C.2), and transparent limitations (Section 3.2), with room to improve reporting on human-study sample size in main text (Section 4.2; Appendix D.4) and certain implementation details.\n- Confidence (5): 4 — Based on careful reading of the manuscript’s methods and extensive experimental results (Figure 2; Tables 1–7; Appendices C–D); some aspects (e.g., simulation realism and probability/variance details) would benefit from further clarification and evidence."
}