Summary
- The paper introduces Learning to Reject Low-Quality Explanations (LtX), where a model abstains when its explanation is judged unsatisfactory by users. The proposed method, ULER (User-centric Low-quality Explanation Rejector), trains a rejector using modest amounts of expert feedback: binary explanation-quality labels and per-feature relevance judgments, augmented via a label-preserving stochastic perturbation (Equation 2 in Section 3.1) and classified with a simple SVM and thresholding (Definition 1 and Equation 1 in Section 3). Experiments compare ULER to eight baselines across eight tabular datasets (Section 4.1) and a new human-annotated soccer xG dataset (Section 4.2), showing improved rejection of low-quality explanations (Figure 2, Table 1, Table 2; Table 7 for the human study). The paper provides robustness analyses with alternative explainers (LIME; Appendix C.3) and oracle predictors (Appendix C.4), and an ablation on augmentation (Appendix C.5). A human-annotated dataset is released to support future research (Abstract; Section 4.2).Strengths
- **Clear problem formulation and rejector definition**
  - The LtX setting is formalized with a clear model definition, including predictor, explainer, and rejector, and a concrete abstention rule (Definition 1; Equation 1 in Section 3). This matters for clarity and reproducibility, making the contribution easy to situate within LtR/XAI literature.
  - The paper explicitly motivates why explanation quality should drive rejection (Section 1; Figure 1), aligning with recognized guidelines (e.g., NIST Four Principles; Section 1), which increases impact in high-stakes domains.
  - Distinguishes LtX from traditional LtR (Section 2, “Learning to reject”) and existing quality metrics (Section 2, “Metrics of Explanation Quality”), improving conceptual novelty.- **Simple, user-centered method (ULER) leveraging human feedback**
  - ULER uses human binary explanation-quality labels and per-feature judgments to construct training data (Section 3.1, “The rejector’s training data”), grounding the rejector in user perception — important for practical human alignment.
  - A feedback-driven augmentation strategy preserves labels while perturbing selected features (Equation 2 and the s-vector design in Section 3.1), a technically sound and pragmatic approach to address data scarcity.
  - Method is model-agnostic for the rejector and explainer choices (Section 3.1, “Learning the rejector”; Section 3.2), enhancing applicability.- **Comprehensive empirical evaluation and competitive baselines**
  - Eight baselines span standard LtR and explanation-aware strategies (Section 4, “Competitors”), including PASTARej (human-side metric) and faithfulness/stability/complexity (machine-side metrics), ensuring broad coverage and experimental rigor.
  - Evaluation metrics include AUROC and percentages of low-quality explanations in accepted/rejected sets across rejection rates (Section 4, “Evaluation metrics”), appropriate for ranking-based rejectors.
  - Multi-dataset evaluation across diverse tasks (classification/regression; healthcare/economics/law/industry) improves external validity (Section 4.1, “Datasets”; Appendix C.1).- **Consistent performance gains and robust trends**
  - ULER shows clear improvements over baselines in separating low- vs. high-quality explanations (Table 1; Figure 2; Section 4.1, Q1), with quantified relative reductions in low-quality explanations in the accepted set and high fractions in the rejected set.
  - Robustness to alternative explainer (LIME) maintains advantages (Appendix C.3; Figure 4; Table 4), supporting method stability.
  - Robustness to alternative oracle predictor (Appendix C.4; Figure 5; Table 5) and an augmentation ablation (Appendix C.5; Table 6) further underlines consistency and technical soundness.- **Carefully designed human annotation study and dataset contribution**
  - The paper details recruitment, vetting, and interface design for human ratings and per-feature feedback (Section 4.2; Appendix D.3), including ethical approval (Section 4.2), which strengthens the credibility of human-centered evaluation.
  - Provides preprocessing steps for quality control and aggregation (Section 4.2; Appendix D.4), ensuring higher-quality labels for training and evaluation.
  - ULER outperforms PASTARej and other competitors on human-annotated data (Section 4.2; Table 7), supporting the central claim that ULER mimics human judgments better than baseline metrics.- **Transparency on setup and reproducibility details**
  - Clear description of setup (splits, validation, grid search, hardware; Section 4, “Setup”; Section 4, “Model selection”; Appendix C.2), aiding reproducibility.
  - Choice of explainer (KernelSHAP, 100 samples; Section 4, “Model selection”; Appendix D.2–D.3) and predictors (Section 4.1) is specified, making replication feasible.- **Limitations and scope explicitly discussed**
  - The paper candidly discusses ULER’s limitations (e.g., reliance on high-quality annotations, focus on tabular data, sample complexity; Section 3.2), and proposes future directions (Section 6), which benefits clarity and realistic positioning.Weaknesses
- **Heuristic simulation of “human judgments” in benchmarks**
  - The simulated labels hinge on a fixed correlation threshold τz=0.25 (Section 4.1, “Simulating human judgments”), chosen to “ensure datasets with varying amounts of low-quality explanations” rather than validated against actual human perception — limiting construct validity.
  - Per-feature wrongness sets Wz are derived from oracle–model L1 differences with u%=0.75 (Section 4.1), again without empirical calibration against human assessments — risking mismatch between simulated and actual human feedback.
  - The oracle choice (Random Forest) and predictor choice (linear SVC/SVR) impose inductive-bias differences (Section 4.1), which may conflate explanation quality with model-class disparities rather than user-centric quality.
  - Although robustness to alternative oracle is reported (Appendix C.4), the simulation remains fundamentally machine-defined rather than human-grounded, potentially inflating ULER’s advantages on synthetic tasks relative to subjective real-world settings.- **Limited theoretical grounding and assumptions in augmentation**
  - The augmentation assumes label preservation under selective Gaussian perturbations (Equation 2; Section 3.1: “we stipulate…”), but provides no theoretical or empirical validation that small perturbations on “correct” vs. “wrong” features reliably preserve human judgments.
  - No analysis of calibration or consistency of the rejector’s score r or threshold τ (Section 3.1), e.g., monotonicity properties, false-abstain vs. false-accept trade-offs beyond empirical rejection-rate sweeps.
  - The choice of Σ (feature-wise standard deviations from D) and ε0 (noise scale) is justified via grid search (Appendix C.2) but lacks principled guidance or sensitivity analysis beyond modest ablation (Appendix C.5).
  - No sample-complexity or generalization bounds are provided for learning r from augmented labels, leaving technical guarantees unaddressed (No direct evidence found in the manuscript).- **Evaluation focuses on AUROC and fractions, not user outcomes**
  - The main metrics (AUROC; fraction of low-quality explanations in accepted/rejected sets; Section 4, “Evaluation metrics”) do not measure downstream impacts such as trust calibration or decision quality, despite being central to the motivation (Section 1; Figure 1).
  - The rejection-threshold τ is conceptually tied to user capacity (Section 3.1), but experiments only sweep rejection rates (Section 4, “Setup”) — missing calibration studies that reflect practical constraints.
  - Cost-sensitive analyses (e.g., human effort vs. abstention benefit) are not reported, though abstention incurs costs in many high-stakes applications (No direct evidence found in the manuscript).
  - No measurement of how ULER affects correctness of accepted predictions or joint human–AI team performance, even though abstention should ideally improve overall decision-making (Section 3.2 admits marginal predictive gains but no empirical quantification).- **Human study scope and reliability reporting**
  - The human study’s domain (soccer xG) involves only five features (Appendix D.2), which limits generality to richer tabular contexts; the overall AUROC is modest (0.63 ± 0.05; Section 4.2; Table 7).
  - Inter-annotator reliability statistics (e.g., κ/α) are not reported; filtering based on standard deviation (>1.25) is used (Appendix D.4), but agreement metrics would improve rigor.
  - PASTARej is adapted by dropping the embedding network (Section 4, “Competitors”), which may undercut PASTA’s strengths and complicate the fairness of comparison in the human study.
  - The annotation interface is carefully designed (Section 4.2; Figures 6–7; Appendix D.3), but potential framing effects and biases are only qualitatively discussed (pilot studies), without quantitative validation (No direct evidence found in the manuscript).- **Generality beyond tabular attribution and scalability**
  - The method is explicitly scoped to tabular feature attributions (Section 3.2), with no experiments on image/text or other explanation paradigms (counterfactuals, rules), limiting breadth.
  - ULER relies on per-feature feedback, which may be costly or impractical for high-dimensional settings (Section 3.2 acknowledges sample complexity risks), and offers only a qualitative mitigation (“adapt ULER to rich pre-trained embedding space”).
  - While the human-annotated dataset will be released (Abstract; Section 4.2), code release is not stated (No direct evidence found in the manuscript), hindering full reproducibility.
  - The augmentation’s dependence on Σ estimated from D may be brittle across distributions or domains; cross-domain generalization is not studied (No direct evidence found in the manuscript).- **Limited analysis of interaction with prediction correctness/LtR**
  - ULER targets explanation quality, but the relationship to prediction correctness is only discussed qualitatively (Section 3.2), with no empirical breakdown of error types (correct prediction/low-quality explanation vs. incorrect prediction/high-quality explanation).
  - No experiments combining ULER with standard LtR are provided, although suggested as future work (Section 3.2; Section 6) — missing evidence on joint benefits when both mispredictions and low-quality explanations are deferred.
  - The paper does not quantify how ULER affects calibration or confidence of accepted predictions (No direct evidence found in the manuscript).
  - It is unclear how often ULER rejects correct predictions due to noisy explanations vs. truly problematic explanations — false-alarm analysis is absent (No direct evidence found in the manuscript).Suggestions for Improvement
- **Strengthen the realism of simulated “human judgments”**
  - Validate τz=0.25 (Section 4.1) by correlating the simulated low-/high-quality labels with a subset of human ratings on the benchmark datasets; report agreement statistics to justify the threshold.
  - Calibrate the construction of Wz (u%=0.75; Section 4.1) against human per-feature flags on a small held-out set; adjust u% or the selection mechanism to better match human judgments.
  - Include additional oracle/predictor pairs with similar inductive biases and report how simulation-based labels vary (Appendix C.4 already explores one; extend to more and quantify impacts).
  - Where feasible, augment benchmarks with limited human annotations (e.g., 50–100 items per dataset) to directly test simulated labels against human perception.- **Provide theoretical or empirical guarantees for augmentation and rejector calibration**
  - Study label preservation under perturbations (Equation 2; Section 3.1) by measuring human ratings pre/post perturbation on a small subset; report empirical invariance margins.
  - Analyze sensitivity to ε0, Σ estimation, and s-vector design via targeted experiments beyond grid search (Appendix C.2) to quantify robustness and provide practical guidance.
  - Calibrate r and τ (Section 3.1) with calibration curves (e.g., reliability plots of r-scores vs. empirical low-quality rates) and report false-accept/false-reject trade-offs.
  - Consider deriving simple consistency properties (e.g., monotonicity under feature-wise perturbations) or sample-efficiency bounds for r trained on augmented data, even if approximate.- **Expand evaluation to user outcomes and cost-sensitive metrics**
  - Measure trust and decision quality: run a small human-in-the-loop study where abstentions triggered by ULER affect downstream human actions, and quantify changes vs. baselines (Section 1 motivation; Figure 1).
  - Implement user-capacity calibration of τ (Section 3.1) and report performance under realistic constraints (e.g., fixed human bandwidth, cost per abstention).
  - Add cost-sensitive metrics (e.g., weighted utility combining false-accept/false-reject rates and abstention cost) to complement AUROC and fraction metrics (Section 4, “Evaluation metrics”).
  - Quantify the effect of ULER on correctness/calibration of accepted predictions and team performance (e.g., deferral accuracy), thereby connecting explanation rejection to end goals (Section 3.2).- **Increase rigor and fairness in the human study**
  - Report inter-annotator reliability (κ or α) and agreement distributions; include these alongside the filtering criterion (Appendix D.4) to strengthen validity.
  - Compare ULER against PASTARej variants that retain an embedding backbone compatible with tabular data (e.g., feature encoders) to ensure fair comparison (Section 4, “Competitors”).
  - Quantitatively assess framing effects: randomize question orders and interface variants (Figures 6–7), and report whether ratings differ statistically across conditions.
  - Explore additional domains or richer tabular settings (more features) to test generality; report how performance scales with feature count and annotator effort (Appendix D.2).- **Broaden scope and improve scalability/reproducibility**
  - Evaluate ULER on image/text attributions (e.g., gradient-based explanations for CNNs; Section 3.2 acknowledges scope), or rule/counterfactual explanations, to demonstrate breadth.
  - Introduce strategies for high-dimensional per-feature feedback (e.g., grouping features, active feature querying) and quantify annotation cost savings (Section 3.2).
  - Release code and detailed scripts (splits, hyperparameters; Section 4, “Setup”; Appendix C.2) alongside the dataset to facilitate end-to-end replication.
  - Test cross-domain generalization by training ULER on one dataset and evaluating on another with related features; analyze robustness of Σ-based augmentation across domains.- **Analyze interaction with prediction correctness and combine with LtR**
  - Provide confusion-matrix-style breakdowns (correct/incorrect prediction vs. high/low explanation quality) and show how ULER shifts these counts; quantify false-alarm rates and benefits (Section 3.2).
  - Implement and report experiments that combine ULER with a standard LtR method (e.g., PredAmb or NovRejX; Section 3.2, Section 6) to quantify joint advantages in deferring both mispredictions and low-quality explanations.
  - Study effects on calibration of accepted predictions (e.g., Brier score changes; Appendix D.2 reports Brier for xG model) when ULER is applied.
  - Report how often ULER rejects correct predictions due to explainer noise vs. truly problematic explanations, and assess robustness under different explainers (extend Appendix C.3) and SHAP sampling budgets.Score
- Overall (10): 7 — Clear formulation (Definition 1; Equation 1), strong empirical gains across diverse baselines (Figure 2; Table 1; Table 2), and a useful human-annotated dataset, tempered by heuristic simulation and limited user-outcome evaluation (Section 4.1; Section 4.2).
- Novelty (10): 7 — Introduces LtX and a user-centric rejector trained from human labels and per-feature feedback (Section 3.1), distinguishing from LtR and machine-side metrics (Section 2), though augmentation is conceptually simple.
- Technical Quality (10): 6 — Method is sound and robustly evaluated (Appendix C.3–C.5; Tables 4–6), but lacks theoretical guarantees for augmentation/label preservation and outcome-focused metrics (Section 3.1; Section 4).
- Clarity (10): 8 — Well-structured with precise definitions (Section 3), comprehensive experimental details (Section 4; Appendix C.2), and transparent limitations (Section 3.2), though some reliability and calibration analyses are missing.
- Confidence (5): 4 — Judgment based on thorough reading of the manuscript, multiple quantitative results (Figure 2; Tables 1–7) and detailed appendices; some claims (e.g., simulation realism, user outcomes) would benefit from additional evidence.