# Global Summary
- Problem: Unsupervised representation learning in spiking neural networks (SNNs) for large-scale temporal video data is limited by shallow/local learning approaches that fail to model long-range temporal dependencies and maintain temporal feature consistency, yielding semantically unstable features.
- Core approach: PredNext is a plug-and-play temporal prediction module for unsupervised SNNs that explicitly models temporal relationships via cross-view Future Step Prediction and Clip Prediction. It augments standard self-supervised objectives without changing encoders.
- Evaluation scope: Benchmarks on video datasets UCF101, HMDB51, and MiniKinetics with SNN implementations of SimCLR, MoCo, SimSiam, BYOL, and Barlow Twins; ablations on prediction components, step length, hidden dimensions, cross-view vs same-view; studies of temporal consistency curves; video retrieval; forced consistency constraint comparison.
- Key findings:
  - PredNext consistently improves top-1/top-5 accuracy across self-supervised methods and datasets; e.g., UCF101→UCF101 top-1: SimCLR 57.04 → PredNext-SimCLR 59.47 (+2.43), BYOL 56.41 → 58.58 (+2.17), Barlow Twins 56.15 → 59.76 (+3.61), SimSiam 50.81 → 54.93 (+4.12).
  - PredNext yields performance comparable to supervised ImageNet-pretrained weights through unsupervised training solely on UCF101; e.g., SimSiam (ImageNet) unsupervised pretraining on UCF101 achieves UCF101 top-1 70.32 vs supervised ImageNet init 64.42; PredNext-SimSiam (ImageNet) reaches 72.24.
  - Larger pretraining datasets improve SNN performance (e.g., MiniKinetics pretraining yields higher fine-tuning metrics).
  - PredNext substantially lowers temporal consistency error; forced consistency constraints reduce consistency error faster but hurt downstream performance (UCF101 top-1 drops from 70.32 to 65.69 at β=0.5, and 60.35 at β=0.8).
  - Retrieval improves with PredNext (e.g., UCF101 R@1: SimCLR 34.58 → 37.09; SimSiam 27.84 → 36.27; SimSiam PredNext (ImageNet) 53.19).
- Major quantitative details:
  - Datasets: UCF101 13,320 videos/101 classes; HMDB51 6,766 videos/51 classes; miniKinetics 200 classes (~400 train, 25 val per class).
  - Training: SEW ResNet18 backbone; AdamW; batch size up to 256 (Method) and 128 (Appendix); UCF101/HMDB51 crops 128×128, T=16, stride τ=2, 200 epochs; MiniKinetics crops 112–114×112–114, T=8, τ=8, 120 epochs; 3 clips for validation.
  - PredNext loss combines original self-supervised loss with prediction loss weighted by α; step and clip predictors are 2-layer MLPs. Base hidden dimensionality stated as 128 in Method; Appendix E.2 reports 512 (noted inconsistency).
- Caveats explicitly stated:
  - Optical flow is excluded to focus on temporal feature consistency in SNNs.
  - Forced consistency constraints degrade downstream performance despite reducing consistency error.
  - Some hyperparameter details vary between Method and Appendix (e.g., optimizer weight decay, batch size, predictor hidden dimension).

# Abstract
- Motivation: Unsupervised SNNs typically use shallow architectures or localized plasticity rules, limiting long-range temporal modeling and temporal feature consistency, leading to semantically unstable representations.
- Proposal: PredNext introduces explicit cross-view temporal modeling via Future Step Prediction and Clip Prediction; integrates with diverse self-supervised objectives as a plug-and-play module.
- Benchmarks: Establish SNN self-supervised learning benchmarks on UCF101, HMDB51, and MiniKinetics, larger than conventional DVS datasets.
- Claims:
  - PredNext delivers significant performance improvements across tasks and self-supervised methods.
  - Achieves performance comparable to supervised ImageNet-pretrained weights via unsupervised training solely on UCF101.
  - Improves temporal feature consistency and network generalization more effectively than forced consistency constraints.
- Outcome: Provides a foundation for unsupervised deep SNNs on large-scale temporal video data.
- Quantitative specifics: Not specified in this section.

# Introduction
- Context: Unsupervised learning extracts structure from unlabeled data; SNNs have temporal processing via membrane potentials but intrinsic LIF dynamics are insufficient for large-scale video with complex temporal dependencies; SNNs often preserve full temporal resolution leading to instability without explicit temporal modeling.
- Hypothesis: Effective temporal modeling should increase consistency among features extracted across timesteps; high-consistency SNNs extract stable semantic features invariant to temporal fluctuations.
- Figures:
  - Figure 1(a): Inter-frame feature similarity evolves during SNN training, indicating increasing consistency as semantic capability improves (no exact numbers here).
  - Figure 1(b): Conceptual clustering—higher consistency yields tighter clusters.
  - Figure 2: PredNext framework with Step Prediction and Clip Prediction, plug-and-play with self-supervised methods.
- Dataset summary (Table 1):
  - DVS-Gesture: “1.3K × 10 s” action; Real Scene; Small.
  - CIFAR10-DVS: “10K × 1.2 s” images; Camera Shift; Small.
  - N-Caltech101: “9K × 0.3 s” images; Camera Shift; Small.
  - UCF101: “13K × 4 s” action; Real Scene; Medium.
  - HMDB51: “6.7K × 7 s” action; Real Scene; Medium.
  - miniKinetics: “80K × 10 s” action; Real Scene; Large.
- Method summary: PredNext predicts future features across contrastive views both within a clip (next timestep) and across clips (next sampled clip). Cross-view prediction intended to enhance feature discrimination by suppressing view-specific noise.
- Experimental plan: Adapt self-supervised methods to SNNs; pretrain on UCF101 and MiniKinetics; assess improvements and temporal consistency; demonstrate that larger datasets benefit SNNs; show that forced consistency constraints hurt performance.
- Quantitative details in this section: Not specified beyond dataset scale descriptors.

# Method
- Self-supervised baselines in SNNs:
  - Implement SNN variants of SimCLR, MoCo, Barlow Twins (contrastive) and SimSiam, BYOL (non-contrastive).
  - Representations: time-averaged z_i = ∑_{t=1}^n z_i^t / n.
  - Losses:
    - InfoNCE for SimCLR/MoCo: L = −log [ exp(sim(z_i, z_j)/τ) / ∑_{k=1}^N exp(sim(z_i, z_k)/τ) ] with cosine similarity, temperature τ, batch size N.
    - SimSiam/BYOL: L = 1 − ( z_j / ||z_j||_2 ) · ( h(z_i) / ||h(z_i)||_2 ); BYOL uses momentum encoder; SimSiam uses stop-gradient Siamese setup.
    - Barlow Twins: L = ∑_i (1 − C_ii)^2 + λ ∑_i ∑_{j ≠ i} C_ij^2, minimizing redundancy.
- Backbone and training setup (Method section):
  - Backbone: SEW ResNet18.
  - Optimizer: AdamW, initial LR 2e-3, weight decay 1e-4, cosine annealing; batch size b=256.
  - UCF101/HMDB51: 128×128 crops; 200 epochs; T=16 frames; stride τ=2.
  - MiniKinetics: 114×114 crops; 120 epochs; T=8 frames; stride τ=8.
  - Data augmentation per Feichtenhofer et al. (2021); validation uses 3 clips per video.
- PredNext (Section 2.2):
  - Components: feature extractor F (SNN encoder + projection), Step Predictor P_T (next-timestep), Clip Predictor P_C (next clip), both 2-layer MLPs; outputs match projection dimension.
  - Step Predictor loss: Q(p_i^t, z_j^{t+m}) = − (p_i^t / ||p_i^t||) · (z_j^{t+m} / ||z_j^{t+m}||), where m is prediction step interval; symmetric across views.
  - Clip Predictor loss: M(c_i, z_j^*) = − (c_i / |c_i|) · (z_j^* / |z_j^*|), where z^* is temporally aggregated future clip feature; symmetric across views.
  - PredNext loss: L_pred = Σ_t (½ Q(p_i^t, z_j^{t+m}) + ½ Q(p_j^t, z_i^{t+m})) + ½ M(c_i, z_j^*) + ½ M(c_j, z_i^*).
  - Total loss: L = (1 − α) · L_ssl + α · L_pred.
  - Training procedure (Algorithm 1): computes L_ssl; predictions via P_T and P_C; L_pred includes terms scaled by 0.25 in algorithmic summary; weight coefficient α balances with L_ssl; updates F, P_T, P_C.
- Base settings (Section 2.2 end):
  - P_T and P_C: 2-layer MLP with batch norm; hidden dimension 128; output dimension matches F(x) representation.
- Noted cross-view design: predictions from one view must match future features of the other view, improving generalization; ablation shows cross-view > same-view.
- Consistency across sections: Appendix E.2 later reports P_T/P_C hidden dimension 512 (inconsistency relative to 128 stated here). Optimizer and batch size also differ in Appendix (see Appendix heading).

# Related Work
- Predictive coding comparison:
  - DPC/MemDPC perform dense predictions on video sequences with additional temporal aggregator modules; Lorre et al. use CPC-like future timestep prediction.
  - PredNext focuses on cross-view prediction with streamlined architecture and no complex auxiliary structures; adds both step and clip predictions.
- Table 2 summary:
  - DPC: no additional module needed (×), step pred (✓), clip pred (×).
  - MemDPC: ×, ✓, ×.
  - CPC-like (Lorre’s): ×, ✓, ×.
  - PredNext: ✓, ✓, ✓.

# Experiments
- 3.1 Dataset and Implementation:
  - Datasets:
    - UCF101: 13,320 clips, 101 actions.
    - HMDB51: 6,766 clips, 51 actions.
    - miniKinetics: 200 categories; ~400 training and 25 validation per class.
  - Backbone: SEW ResNet18 across all experiments.
  - Preprocessing:
    - UCF101/HMDB51: 128×128, 16 frames, stride 2.
    - miniKinetics: 112×112, 8 frames, stride 8.
  - Evaluation: 3 uniformly sampled clips per video.
  - Optical flow excluded.
- 3.2 Results of Unsupervised Representation Evaluation:
  - Supervised baselines (top-1/top-5):
    - Random init: UCF101 44.07/70.84; HMDB51 18.04/45.69; MiniKinetics 40.53/68.59.
    - ImageNet init: UCF101 64.42/87.36; HMDB51 34.31/67.84; MiniKinetics 50.48/76.53.
    - ImageNet + MiniKinetics init: UCF101 70.02/91.62; HMDB51 44.97/78.37.
  - Pretrain→finetune comparisons (selected):
    - UCF101→UCF101 top-1/top-5:
      - SimCLR 57.04/83.82 → PredNext-SimCLR 59.47/85.28 (+2.43/+1.46).
      - MoCo 49.70/79.70 → PredNext-MoCo 54.98/82.87 (+5.28/+3.17).
      - BYOL 56.41/83.18 → PredNext-BYOL 58.58/83.82 (+2.17/+0.64).
      - Barlow Twins 56.15/84.25 → PredNext-BT 59.76/84.85 (+3.61/+0.60).
      - SimSiam 50.81/81.07 → PredNext-SimSiam 54.93/82.77 (+4.12/+1.70).
      - SimSiam (ImageNet init): 70.32/91.56 → PredNext-SimSiam (ImageNet) 72.24/91.81 (+1.92/+0.25).
    - UCF101→HMDB51 top-1/top-5:
      - SimCLR 30.59/64.97 → PredNext-SimCLR 31.58/66.19 (+0.99/+1.22).
      - MoCo 28.04/62.22 → PredNext-MoCo 29.60/64.31 (+1.56/+2.09).
      - BYOL 29.35/64.58 → PredNext-BYOL 31.57/64.51 (+2.22/−0.07).
      - Barlow Twins 30.33/64.12 → PredNext-BT 31.18/66.01 (+0.85/+1.89).
      - SimSiam 28.10/63.46 → PredNext-SimSiam 30.00/64.37 (+1.90/+0.91).
      - SimSiam (ImageNet): 39.65/74.35 → PredNext-SimSiam (ImageNet) 41.50/75.42 (+1.85/+1.07).
    - MiniKinetics→UCF101 top-1/top-5:
      - SimCLR 59.03/85.96 → PredNext-SimCLR 61.06/87.21 (+2.03/+1.25).
      - MoCo 45.63/76.55 → PredNext-MoCo 51.60/79.65 (+5.97/+3.10).
      - BYOL 59.27/86.23 → PredNext-BYOL 62.01/88.26 (+2.74/+2.03).
      - Barlow Twins 58.04/85.83 → PredNext-BT 62.75/88.66 (+4.71/+2.83).
      - SimSiam 43.77/74.89 → PredNext-SimSiam 50.65/79.01 (+6.88/+4.12).
      - SimSiam (ImageNet): 68.70/91.91 → PredNext-SimSiam (ImageNet) 71.66/92.07 (+2.96/+0.16).
    - MiniKinetics→HMDB51 top-1/top-5:
      - SimCLR 35.42/67.97 → PredNext-SimCLR 36.80/68.37 (+1.38/+0.40).
      - MoCo 20.72/46.86 → PredNext-MoCo 25.69/51.37 (+4.97/+4.51).
      - BYOL 36.74/68.24 → PredNext-BYOL 37.25/69.28 (+0.51/+1.04).
      - Barlow Twins 36.53/68.17 → PredNext-BT 37.65/69.35 (+1.12/+1.18).
      - SimSiam 19.08/45.75 → PredNext-SimSiam 25.03/51.04 (+5.95/+5.29).
      - SimSiam (ImageNet): 36.67/73.53 → PredNext-SimSiam (ImageNet) 38.63/74.25 (+1.96/+0.72).
    - MiniKinetics→MiniKinetics top-1/top-5:
      - SimCLR 50.61/77.16 → PredNext-SimCLR 53.61/78.59 (+3.00/+1.43).
      - MoCo 42.65/70.23 → PredNext-MoCo 46.51/73.64 (+3.86/+3.41).
      - BYOL 51.23/77.69 → PredNext-BYOL 54.37/79.61 (+3.14/+1.92).
      - Barlow Twins 51.28/77.61 → PredNext-BT 54.68/79.85 (+3.40/+2.24).
      - SimSiam 41.52/69.75 → PredNext-SimSiam 46.31/73.68 (+4.79/+3.93).
  - Narrative claims: Unsupervised methods can outperform supervised training in some settings (e.g., SimSiam on UCF101) and benefit from data scale; PredNext consistently improves temporal representation learning.
- 3.3 Consistency Curves and Manifold:
  - Metric: Feature-consistency error E_consistency = (1/N)(1/[T(T−1)]) ∑_{i,t,s≠t} (1 − cos(f_i^t, f_i^s)); lower is better.
  - Observation: Consistency errors decrease during training for all methods; PredNext avoids post-saturation decline and achieves lower errors (no exact values in curves).
  - Forced consistency constraint:
    - Loss: L_forced = L_ssl + β · E_{i,t,s}[1 − cos(f_i^t, f_i^s)].
    - Table 4 (UCF101, SimSiam ImageNet init):
      - Baseline: top-1 70.32; consistency 0.773.
      - PredNext: top-1 72.24 (+1.92); consistency 0.819 (+0.046).
      - Forced β=0.1: top-1 70.45 (+0.13); consistency 0.803 (+0.03).
      - Forced β=0.5: top-1 65.69 (−4.63); consistency 0.852 (+0.08).
      - Forced β=0.8: top-1 60.35 (−9.97); consistency 0.884 (+0.11).
- 3.4 Video Retrieval:
  - Setup: UCF101 split 1, queries from validation set, candidates from training set; uniformly sample 10 frames; nearest neighbor search; metrics Recall@K (K=1,5,10,20).
  - Table 5:
    - UCF101 recalls:
      - SimCLR: R@1 34.58, R@5 55.72, R@10 65.50, R@20 74.70.
      - SimCLRPredNext: 37.09, 56.01, 66.38, 75.20.
      - SimSiam: 27.84, 48.53, 59.79, 71.56.
      - SimSiamPredNext: 36.27, 55.70, 65.13, 74.15.
      - SimSiamPredNext (ImageNet): 53.19, 69.39, 76.53, 83.11.
    - HMDB51 recalls:
      - SimCLR: 12.22, 34.71, 49.67, 64.71.
      - SimCLRPredNext: 13.60, 35.36, 50.32, 66.86.
      - SimSiam: 11.70, 32.68, 45.95, 60.98.
      - SimSiamPredNext: 13.20, 35.16, 47.32, 64.05.
      - SimSiamPredNext (ImageNet): 15.95, 40.46, 53.53, 68.43.
- 4 Ablation Studies:
  - Prediction head components (Table 6, UCF101 pretraining):
    - SimSiam UCF101→UCF101 top-1/top-5:
      - None: 50.81/81.07.
      - Step only: 51.33/81.26.
      - Clip only: 54.40/82.37.
      - Both: 54.93/82.77 (best).
    - SimSiam UCF101→HMDB51 top-1/top-5: None 28.10/63.46 → Both 30.00/64.37.
    - SimCLR UCF101→UCF101 top-1/top-5: None 57.04/83.82 → Both 59.48/85.28.
    - SimCLR UCF101→HMDB51 top-1/top-5: None 30.59/64.97 → Both 31.57/66.34.
  - Prediction step length (Figure 6a): best at step length m=1; performance declines at longer intervals (no exact values).
  - Cross-view vs same-view (Table 7):
    - UCF101 top-1: cross-view 54.93; same-view 53.66 (−1.27); cross-view only 52.37 (−2.56); same-view only 5.03 (−49.90, collapse).
    - HMDB51 top-1: cross-view 30.00; same-view 29.67 (−0.33); cross-view only 29.41 (−0.59); same-view only 3.07 (−26.93).
  - Prediction head size (Figure 6b): hidden dims tested 64–1024; performance improves up to ~256–512 and stabilizes; selected 512 in Appendix E.2; claims minimal parameter overhead and no inference cost.
  - Time length and sampling stride (Figure 6c): performance improves with longer sequences (e.g., 16 vs 10 frames) and wider sampling intervals (e.g., stride 4 vs 1); no exact numbers beyond plotted trends.

# Conclusion
- Summary: PredNext enhances unsupervised SNNs via future feature prediction (step and clip), strengthening temporal consistency and improving performance across self-supervised baselines on large-scale video datasets.
- Evidence: Consistent top-1/top-5 improvements in fine-tuning tasks; lower temporal consistency errors; improved video retrieval recalls.
- Quantitative highlights: E.g., UCF101 top-1 gains up to +6.88 (SimSiam MiniKinetics→UCF101); forced consistency reduces consistency error but degrades accuracy (e.g., UCF101 top-1 70.32 → 60.35 at β=0.8).

# Appendix
- Ethics Statement: No ethical issues reported; adheres to academic standards; no sensitive data or privacy concerns.
- Reproducibility Statement: Detailed settings and hyperparameters provided; code submitted in supplementary materials; plan to release code and pretrained models.
- LLM Usage: LLMs used only for language refinement; not for composition, experiments, or conceptual development.
- Extended Related Work:
  - SNNs background and challenges for video; unsupervised SNNs mostly shallow/local (STDP, plasticity); emerging contrastive SNN methods; need for video-centric SNN approaches.
  - Video unsupervised learning in ANNs: DPC, VideoJigsaw, CoCLR, ρ-series, VideoMoCo, generative models; adaptation to SNNs is underexplored.
- More retrieval visualizations: PredNext retrieves semantically consistent videos despite large visual variation.
- KNN training curves (UCF101 split 1):
  - Top-1 approximate endpoints after 200 epochs: SimSiam ≈30%; BYOL ≈40%; SimCLR ≈40%; MoCo ≈30%; BarlowTwins ≈40%.
  - Top-5 approximate endpoints: SimSiam ≈60%; BYOL ≈70%; SimCLR ≈60%; MoCo ≈60%; BarlowTwins ≈70%.
  - PredNext variants exceed baselines in these curves (qualitative).
- Setting Details:
  - Implementation: PyTorch; synchronized batch norm (multi-GPU); mixed precision (AMP).
  - Pre-training (Appendix E.1):
    - Optimizer: AdamW LR 0.002; weight decay 1e-6; cosine annealing; warmup 20 epochs (UCF101) / 12 (MiniKinetics); batch size 128.
    - Augmentation: random crop (scale (0.2, 0.766), ratio (0.75, 1.3333)), flip p=0.5, color jitter (brightness/contrast/saturation 0.6, hue 0.1), random gray p=0.2.
    - UCF101: 128×128, 16 frames, stride 2, 200 epochs.
    - MiniKinetics: 112×112, 8 frames, stride 8, 120 epochs.
  - Fine-tuning:
    - AdamW LR 0.0003, no weight decay; cosine annealing; batch size 128; 100 epochs (UCF101/HMDB51), 50 epochs (MiniKinetics); 3 clips per sample for evaluation.
  - Model details (Appendix E.2):
    - SimCLR: proj dim 256, τ=0.5.
    - MoCo: proj dim 256, momentum 0.99, queue 4096, τ=0.5.
    - BYOL: proj/pred dim 2048, pred hidden 512, momentum 0.99.
    - Barlow Twins: proj dim 1024.
    - SimSiam: proj/pred dim 2048, pred hidden 512.
    - PredNext P_T/P_C hidden layer 512; outputs match base method projection dimension.
  - Note: These Appendix settings differ from Method section (e.g., batch size 128 vs 256; AdamW weight decay 1e-6 vs 1e-4; P_T/P_C hidden 512 vs 128).
- Not specified: Exact value of α (loss weighting), exact m (prediction interval) used in main experiments (except qualitative step length analysis), training hardware resources.

# References
- Key citations:
  - Unsupervised/self-supervised methods: SimCLR (Chen et al., 2020), MoCo (He et al., 2020), SimSiam (Chen & He, 2021), BYOL (Grill et al., 2020), Barlow Twins (Zbontar et al., 2021), predictive coding (Huang & Rao, 2011; Spratling, 2017), DPC/MemDPC (Han et al., 2019; 2020a), CoCLR (Han et al., 2020b), VideoMoCo (Pan et al., 2021), BEVT (Wang et al., 2022), masked feature prediction (Chen et al., 2022).
  - SNNs: SEW ResNet18 (Fang et al., 2021a,b), surrogate gradients (Neftci et al., 2019; Zenke & Vogels, 2021), STDP and plasticity (Diehl & Cook, 2015; Bi & Poo, 1998; Saunders et al., 2019), SNN contrastive (Bahariasl & Kheradpisheh, 2024; Ma et al., 2025), event-based optical flow (Hagenaars et al., 2021), SpikeCLIP (Li et al., 2023).
  - Datasets: UCF101 (Soomro et al., 2012), HMDB51 (Kuehne et al., 2011), Kinetics (Carreira & Zisserman, 2017), CIFAR10-DVS (Li et al., 2017), N-Caltech101 (Orchard et al., 2015).
  - Visualization: UMAP (McInnes et al., 2018).
- Citation style: Standard conference/journal references included; no missing bibliographic data noted beyond typical arXiv identifiers.