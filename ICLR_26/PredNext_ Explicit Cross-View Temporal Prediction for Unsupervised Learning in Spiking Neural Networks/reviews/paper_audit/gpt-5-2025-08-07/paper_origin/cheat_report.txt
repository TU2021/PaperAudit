Academic integrity and internal consistency risk report

Summary of high-impact issues observed, with explicit anchors to the manuscript. Each item identifies clear internal contradictions, numerical mismatches, or missing details that materially affect correctness or trustworthiness.

1) PredNext loss weighting inconsistency
- Evidence:
  - Section: Method, 2.2 PredNext, final loss definition: “L_pred = Σ_t (1/2 · Q(p_i^t, z_j^{t+m}) + 1/2 · Q(p_j^t, z_i^{t+m})) + 1/2 · M(c_i, z_j^*) + 1/2 · M(c_j, z_i^*)” (Block #16).
  - Section: Method, Algorithm 1, line 11: “L_pred = 0.25 · (Σ_t [Q(p_i^t, z_j^{t+m}) + Q(p_j^t, z_i^{t+m})] + M(c_i, z_j^*) + M(c_j, z_i^*))” (Block #14).
- Problem: The algorithm scales all terms by 0.25, whereas the formal definition uses 0.5 (each term weighted by 1/2). This is a direct contradiction affecting optimization strength and reported results.

2) Temporal consistency metric vs reported numbers and claims
- Evidence:
  - Section: Experiments, 3.3, definition: “E_consistency = … (1 − cos(f_i^t, f_i^s)); Lower values indicate higher temporal feature consistency.” (Blocks #25–#26).
  - Section: Experiments, Table 4: reports “consistency” values increasing from 0.773 (SimSiam ImageNet) to 0.819 (PredNext), and even higher (0.852, 0.884) for forced constraints (Block #28).
  - Section: Experiments, text: “this direct constraint indeed rapidly reduces consistency errors, even faster than PredNext.” (Block #28).
- Problem: The metric is defined as an error (lower is better), but table values increase with PredNext and further increase with forced constraints, while the text claims errors are reduced. This is a clear contradiction between metric definition, numbers reported, and narrative interpretation.
- Impact: Undermines the central claim about improving temporal consistency. Requires clarification (are they reporting similarity rather than 1 − cosine? If so, the metric definition and table labeling must align).

3) Hyperparameter and configuration mismatches across sections
- Evidence:
  - Section: Method, 2.1: “Across all experiments, AdamW (lr=2e-3, weight decay=1e-4), batch size b=256; MiniKinetics crops 114×114; UCF/HMDB: T=16, stride=2; MiniKinetics: T=8, stride=8.” (Block #13).
  - Section: Appendix, E.1: Pre-training uses AdamW lr=0.002, weight decay=1e-6, batch size=128, warmup; MiniKinetics crops 112×112; Fine-tuning uses AdamW lr=0.0003, no weight decay; evaluation specs differ (Block #57).
  - Section: Experiments, Implementation details: MiniKinetics processing utilizes 112×112 (Block #22).
- Problems:
  - Optimizer weight decay conflicts (1e-4 vs 1e-6).
  - Batch size conflicts (256 vs 128).
  - MiniKinetics crop size conflicts (114×114 vs 112×112).
  - “Across all experiments” statement contradicts per-experiment settings.
- Impact: These discrepancies materially affect reproducibility and interpretation of performance gains.

4) Prediction head dimensionality inconsistency
- Evidence:
  - Section: Method, 2.2 Base settings: “prediction heads P_T and P_C … using a 128-dimensional hidden layer” (Block #17).
  - Section: Experiments, 3.4 Impact of head size: selected 512 as optimal (Block #36).
  - Section: Appendix, E.2 Model details: PredNext heads used hidden dimension 512 (Block #58).
- Problem: Base setting states 128-dim hidden layer, but experiments and model details use 512. This contradicts the standardized configuration claim and affects reported results.

5) Table caption and column content errors
- Evidence:
  - Section: Introduction, Table 1 caption: “Summary of commonly used DVS and video datasets,” but table mixes DVS and video datasets; the second column label “#classes” contains entries like “1.3K × 10 s action” (Block #7).
  - Section: Related Work, Table 2 caption: also “Summary of commonly used DVS and video datasets,” while the content is a methods comparison (DPC, MemDPC, CPC-like, PredNext) with checkmarks (Block #18).
- Problems:
  - Table 1 column labeled “#classes” contains instance counts and durations, not numbers of classes; e.g., DVS-Gesture is known to have 11 classes, but table shows “1.3K × 10 s action.”
  - Table 2 caption is incorrect for its content (methods comparison), duplicating the dataset caption.
- Impact: Mislabeling and factual mismatches in tables reduce trustworthiness of dataset descriptions and related comparisons.

6) Abstract claim about matching supervised ImageNet-pretrained performance via unsupervised training solely on UCF101 not supported quantitatively
- Evidence:
  - Section: Abstract: “PredNext achieves performance comparable to supervised ImageNet-pretrained weights through unsupervised training solely on UCF101.” (Block #2).
  - Section: Experiments, Table 3:
    - Supervised ImageNet init on UCF101 top-1: 64.42 (Block #23).
    - PredNext (unsupervised, UCF101→UCF101) top-1 best entries are ~59.76 (PredNext-Barlow Twins) and ~59.47 (PredNext-SimCLR) (Block #23).
- Problem: A ~4.7–5.0 percentage point gap is non-trivial; “comparable” is not quantitatively demonstrated for “solely on UCF101” unless additional context is given. The only PredNext entry exceeding 64 is “PredNext-SimSiam (ImageNet) 72.24,” which uses ImageNet initialization (not “solely” UCF101).
- Impact: Overstated claim in the abstract relative to tabulated evidence.

7) KNN evaluation figure labeling inconsistency
- Evidence:
  - Section: Appendix D, text describes “Top-1/Top-5 accuracy curves” (Block #54).
  - Figure 8 shows y-axis labeled “Similarity” rather than accuracy for KNN curves (Block #55).
- Problem: Metric mismatch between description and figure labels.
- Impact: Confuses interpretation of the reported evaluation and undermines clarity of evidence.

8) Ambiguity in “next clip” prediction setup
- Evidence:
  - Section: Method, Algorithm 1: uses “Next Clip Prediction” and z_j^* but does not specify how the “subsequently sampled clip” is selected (temporal offset, overlap, same video constraints) (Block #14).
  - Section: Method, 2.2: “z_i^* and z_j^* denote temporally aggregated features of the subsequently sampled clip,” yet c_i = P_C(average of current clip) is used to predict z_j^* of the “next” clip (Blocks #15–#16).
- Problem: Missing procedural details on how the “next clip” is chosen (exact offset, non-overlap, sampling policy). The sentence stating both z_i^* and z_j^* are from the “subsequently sampled clip” conflicts with Algorithm 1, where z_i^* is computed from the current clip.
- Impact: This affects correctness of the clip prediction objective and its reproducibility.
- Evidence status: No direct evidence found in the manuscript specifying the default temporal offset m for clips or the exact sampling policy beyond the ablation in Figure 6(a).

9) Notation inconsistency for norms in loss definition
- Evidence:
  - Section: Method, 2.1 and 2.2 use “|| · ||_2” for normalization (Block #11, #15).
  - Section: Method, 2.2 uses “|c_i|” and “|z_j^*|” in M(c_i, z_j^*) (Block #16).
- Problem: Inconsistent notation for vector norms (| · | vs || · ||), which can cause confusion about the exact normalization.
- Impact: Minor, but affects mathematical clarity of the loss definition.

Recommendations to address
- Unify and correct the PredNext loss scaling across Algorithm 1 and formal definitions; state exact coefficients used in experiments.
- Align the temporal consistency metric definition, table labels, and narrative; if a different metric (similarity) is reported, redefine accordingly and adjust all references and interpretations.
- Standardize and clearly report hyperparameters actually used for pre-training and fine-tuning (optimizer, weight decay, batch size, input resolution), and ensure “across all experiments” statements match experimental reality.
- Correct the prediction head hidden dimensionality description (base settings vs actual used) and specify the configuration used for all reported tables.
- Fix table captions and columns: correct Table 1 column headings and values (e.g., number of classes vs instance counts), and correct Table 2 caption to reflect method comparison.
- Temper the abstract claim or provide quantitative evidence demonstrating “comparable” performance strictly under “unsupervised training solely on UCF101.”
- Correct KNN figure labels to match the described metric (accuracy) or adjust the text.
- Provide explicit details on next-clip sampling: define temporal offset m, non-overlap policy, and alignment across views, and reconcile the statement about z_i^* vs z_j^* being from current vs subsequent clips.
- Standardize notation for vector norms in loss functions.

If resolved, these changes would substantially improve the paper’s scientific clarity and integrity. If not, the highlighted inconsistencies materially weaken confidence in the reported methods and results.

No additional integrity concerns (e.g., duplicated figures or unverifiable claims outside the manuscript) were found.