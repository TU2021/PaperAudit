1) Summary
This paper addresses the challenge of learning temporally consistent representations in deep Spiking Neural Networks (SNNs) from unlabeled video data. The authors identify that existing unsupervised SNNs, often shallow or reliant on local plasticity, struggle to model long-range temporal dependencies, leading to unstable features. They propose PredNext, a plug-and-play module that integrates with existing self-supervised learning (SSL) frameworks. PredNext introduces an auxiliary task of predicting future representations using two components: Step Prediction (for the next timestep) and Clip Prediction (for a future video clip). A key aspect is the use of cross-view prediction, where one augmented view of the data predicts the future of another view. The authors establish SNN SSL benchmarks on large-scale video datasets (UCF101, HMDB51, MiniKinetics) and demonstrate that PredNext consistently improves performance across multiple SSL methods and downstream tasks, while also enhancing the temporal consistency of the learned features.2) Strengths
*   **Well-Motivated and Novel Method:** The paper effectively identifies a key weakness in unsupervised SNNs—poor temporal modeling leading to feature inconsistency—and proposes a principled solution. The idea of using explicit, cross-view future prediction as an auxiliary task is a novel contribution to the SNN literature.
    *   The motivation is clearly established by analyzing the evolution of feature similarity during training and visualizing the desired properties of high-consistency representations (Section 1, Figure 1).
    *   The proposed PredNext module, with its Step and Clip prediction components, directly targets this problem by forcing the model to learn features that are predictive of future states (Section 2.2, Figure 2).
    *   The paper makes a compelling case for its approach over a naive "forced consistency" constraint, showing experimentally that simply forcing features to be similar degrades performance, whereas their predictive approach improves it (Section 3.3, Table 4). This highlights the technical soundness of the predictive coding-inspired design.*   **Comprehensive Experimental Setup and Benchmarking:** The authors conduct a rigorous and extensive empirical evaluation. A significant contribution is the establishment of systematic benchmarks for self-supervised learning in SNNs on large-scale video datasets, moving beyond the typically smaller DVS datasets.
    *   The paper adapts five prominent self-supervised learning methods (SimCLR, MoCo, SimSiam, BYOL, Barlow Twins) to SNNs, creating a strong set of baselines for comparison (Section 2.1, Figure 3).
    *   Experiments are conducted on three standard video action recognition datasets (UCF101, HMDB51, MiniKinetics), which are substantially larger and more complex than datasets commonly used in SNN research (Section 3.1, Table 1).
    *   The evaluation protocol is thorough, covering pre-training on different source datasets and fine-tuning on various target datasets, demonstrating the generalizability of the learned representations (Table 3).*   **Strong and Consistent Empirical Results:** The proposed PredNext module demonstrates significant and consistent performance improvements across a wide range of settings.
    *   In fine-tuning experiments, adding PredNext improves Top-1 accuracy for all five SSL baselines on multiple dataset combinations. For instance, it improves SimCLR from 57.04% to 59.47% on UCF101 and Barlow Twins from 51.28% to 54.68% on MiniKinetics (Table 3).
    *   The method also shows clear benefits in video retrieval tasks, improving Recall@K metrics for both SimCLR and SimSiam backbones (Table 5).
    *   Analysis of feature consistency shows that PredNext not only achieves lower consistency error (i.e., higher similarity) but also avoids the performance degradation seen in later training epochs with baseline methods (Figure 4, top row). The UMAP visualizations further support the claim of learning more compact, class-separable representations (Figure 4, middle and bottom rows).*   **Thorough Ablation Studies and Analysis:** The paper includes a detailed set of ablation studies that validate the key design choices of PredNext and provide valuable insights into the method's behavior.
    *   The individual contributions of the Step Prediction and Clip Prediction components are quantified, showing that both are beneficial and their combination is most effective (Table 6).
    *   The importance of the cross-view prediction strategy is clearly demonstrated by comparing it against same-view prediction, which is shown to be less effective and prone to collapse when used alone (Table 7).
    *   The sensitivity to crucial hyperparameters like prediction step length, prediction head size, and input clip length/stride is systematically analyzed, justifying the final model configuration (Section 4, Figure 6).3) Weaknesses
*   **Ambiguous Narrative on Temporal Consistency:** The paper's core motivation revolves around improving "temporal feature consistency," but the relationship between the proposed consistency metric and downstream performance is not straightforward, leading to some confusion.
    *   The paper initially frames higher consistency as a desirable goal that correlates with model convergence (Section 1, Figure 1). The metric is defined as the average cosine distance between features from different timesteps (Section 3.3).
    *   However, the "Forced Consistency" experiment shows that directly optimizing this metric leads to the highest consistency scores (e.g., 0.884) but results in significantly worse downstream performance (60.35% top-1) compared to PredNext (0.819 consistency, 72.24% top-1) (Table 4).
    *   This creates a contradiction: the method is motivated by improving a metric that, when optimized directly, harms performance. The paper argues that PredNext finds a "better" kind of consistency, but this distinction is not formally defined, and the initial motivation becomes less clear as a result. The y-axis label "similarity" in Figure 1a and Figure 4 is also potentially confusing, as the text discusses consistency *error* (distance).*   **Lack of Direct Comparison to ANN-based Methods:** While the paper provides a strong evaluation within the SNN domain, it lacks a direct quantitative comparison to equivalent Artificial Neural Network (ANN) based video self-supervised learning methods.
    *   The related work section discusses several influential ANN-based methods like DPC and VideoMoCo (Section B.2), and a qualitative comparison is provided (Table 2), but no quantitative performance numbers from these methods are included in the main results tables.
    *   For example, Table 3 compares against supervised SNNs but not unsupervised ANNs using the same SEW ResNet18 backbone. This makes it difficult to assess the absolute performance of the proposed SNN models and understand the remaining performance gap between SNNs and ANNs on these large-scale video tasks.
    *   Without this context, the significance of the results for the broader video understanding community is harder to gauge.*   **Missing Details on Key Hyperparameters:** Several important hyperparameters are not specified or justified, which could hinder reproducibility and a full understanding of the method's sensitivity.
    *   The final loss function `L = (1 − α) · L_ssl + α · L_pred` includes a critical weighting coefficient `α` that balances the original SSL loss and the PredNext loss (Algorithm 1, Section 2.2). The value of `α` used in the experiments is never stated in the main paper or the appendix, nor is the method for its selection described.
    *   There is a minor inconsistency in the reporting of the prediction head's hidden layer dimensionality. The text in Section 4 ("Impact of Prediction Head Size") states, "We selected 512 dimensions as the optimal configuration," while the corresponding plot (Figure 6b) shows performance saturating at 256 dimensions. The appendix then states 512 was used for all experiments (Section E.2). This could be clarified.4) Suggestions for Improvement
*   **Refine the Discussion of Temporal Consistency:** To address the ambiguity, the narrative around consistency should be clarified.
    *   Explicitly differentiate between "trivial" or "forced" consistency (achieved by suppressing all temporal variation) and "semantically meaningful" consistency (where high-level features are stable while low-level dynamics can still be represented). This could be framed as PredNext learning an invariance to nuisance temporal variations while retaining discriminative information.
    *   In Section 3.3, when presenting the results from Table 4, elaborate on why forced consistency fails. For example, explain that it may lead to collapsed or overly smooth representations that lack discriminative power for the downstream task.
    *   Ensure consistent labeling in figures. For clarity, the y-axis in Figure 1a and Figure 4 (top row) should be explicitly labeled as "Cosine Similarity (1 - E_consistency)" or something similar to align with the metric definition.*   **Contextualize Performance with ANN Baselines:** To strengthen the paper's impact, provide performance context from the ANN domain.
    *   The most direct way would be to train an ANN version of the SEW ResNet18 backbone with a representative SSL method (e.g., SimSiam) on one of the key benchmarks (e.g., UCF101). Including this single data point in Table 3 would provide a valuable anchor for the SNN results.
    *   Alternatively, if new experiments are not feasible, the authors could add a row to Table 3 with results reported in prior work (e.g., Feichtenhofer et al., 2021) that use a comparable architecture, clearly noting that the results are from a different codebase. A detailed discussion in the text about the expected performance gap would also be beneficial.*   **Provide Complete Hyperparameter Specifications:** To improve reproducibility, all critical hyperparameters should be reported.
    *   Please state the value of the loss weighting coefficient `α` used for the experiments in Section 2.2 or the appendix (Section E.1). It would also be helpful to briefly mention how this value was determined (e.g., via a hyperparameter sweep on a held-out validation set).
    *   Please clarify the choice of the 512 hidden dimension for the prediction head in Section 4. For example: "While performance began to saturate at 256 dimensions, we found a small but consistent improvement at 512 dimensions with negligible impact on training time, and thus selected it for all experiments." This would resolve the minor inconsistency between the text and the plot.5) Score
- Overall (10): 8 — The paper introduces a novel and effective method for a relevant problem, supported by a very strong and thorough experimental evaluation.
- Novelty (10): 8 — The application of cross-view predictive coding to enforce temporal consistency in unsupervised SNNs for video is a novel and well-motivated contribution.
- Technical Quality (10): 9 — The technical execution is excellent, with rigorous benchmarking, extensive ablations, and strong empirical evidence (Table 3, Table 5, Figure 4).
- Clarity (10): 7 — The paper is generally well-written, but the narrative around "temporal consistency" is slightly confusing (Table 4), and some key hyperparameters are missing (Section 2.2).
- Confidence (5): 5 — I am highly confident in my assessment, as the claims are well-supported by extensive experiments and the paper aligns with my area of expertise.