1) Summary
This paper addresses the challenge of learning temporally consistent representations in deep Spiking Neural Networks (SNNs) from unlabeled video data. The authors identify that existing unsupervised SNNs, often shallow or reliant on local plasticity, struggle to model long-range temporal dependencies, leading to unstable features. They propose PredNext, a plug-and-play module that integrates with existing self-supervised learning (SSL) frameworks. PredNext introduces an auxiliary task of predicting future representations using two components: Step Prediction (for the next timestep) and Clip Prediction (for a future video clip). A key aspect is the use of cross-view prediction, where one augmented view of the data predicts the future of another view. The authors establish SNN SSL benchmarks on large-scale video datasets (UCF101, HMDB51, MiniKinetics) and demonstrate that PredNext consistently improves performance across multiple SSL methods and downstream tasks, while also enhancing the temporal consistency of the learned features.2) Strengths
*   **Well-Motivated and Novel Method:** The paper effectively identifies a key weakness in unsupervised SNNs—poor temporal modeling leading to feature inconsistency—and proposes a principled solution. The idea of using explicit, cross-view future prediction as an auxiliary task is a novel contribution to the SNN literature.
    *   The motivation is clearly established by analyzing the evolution of feature similarity during training and visualizing the desired properties of high-consistency representations (Section 1, Figure 1).
    *   The proposed PredNext module, with its Step and Clip prediction components, directly targets this problem by forcing the model to learn features that are predictive of future states (Section 2.2, Figure 2).
    *   The paper makes a compelling case for its approach over a naive "forced consistency" constraint, showing experimentally that simply forcing features to be similar degrades performance, whereas their predictive approach improves it (Section 3.3, Table 4). This highlights the technical soundness of the predictive coding-inspired design.*   **Comprehensive Experimental Setup and Benchmarking:** The authors conduct a rigorous and extensive empirical evaluation. A significant contribution is the establishment of systematic benchmarks for self-supervised learning in SNNs on large-scale video datasets, moving beyond the typically smaller DVS datasets.
    *   The paper adapts five prominent self-supervised learning methods (SimCLR, MoCo, SimSiam, BYOL, Barlow Twins) to SNNs, creating a strong set of baselines for comparison (Section 2.1, Figure 3).
    *   Experiments are conducted on three standard video action recognition datasets (UCF101, HMDB51, MiniKinetics), which are substantially larger and more complex than datasets commonly used in SNN research (Section 3.1, Table 1).
    *   The evaluation protocol is thorough, covering pre-training on different source datasets and fine-tuning on various target datasets, demonstrating the generalizability of the learned representations (Table 3).*   **Strong and Consistent Empirical Results:** The proposed PredNext module demonstrates significant and consistent performance improvements across a wide range of settings.
    *   In fine-tuning experiments, adding PredNext improves Top-1 accuracy for all five SSL baselines on multiple dataset combinations. For instance, it improves SimCLR from 57.04% to 59.47% on UCF101 and Barlow Twins from 51.28% to 54.68% on MiniKinetics (Table 3).
    *   The method also shows clear benefits in video retrieval tasks, improving Recall@K metrics for both SimCLR and SimSiam backbones (Table 5).
    *   Analysis of feature consistency shows that PredNext not only achieves higher similarity but also avoids the performance degradation seen in later training epochs with baseline methods (Figure 4, top row). The UMAP visualizations further support the claim of learning more compact, class-separable representations (Figure 4, middle and bottom rows).*   **Thorough Ablation Studies and Analysis:** The paper includes a detailed set of ablation studies that validate the key design choices of PredNext and provide valuable insights into the method's behavior.
    *   The individual contributions of the Step Prediction and Clip Prediction components are quantified, showing that both are beneficial and their combination is most effective (Table 6).
    *   The importance of the cross-view prediction strategy is clearly demonstrated by comparing it against same-view prediction, which is shown to be less effective and prone to collapse when used alone (Table 7).
    *   The sensitivity to crucial hyperparameters like prediction step length, prediction head size, and input clip length/stride is systematically analyzed, justifying the final model configuration (Section 4, Figure 6).3) Weaknesses
*   **Contradictory Narrative on Temporal Consistency:** The paper's core motivation revolves around improving "temporal feature consistency," but the metric is defined and reported in a contradictory manner, undermining a central claim.
    *   The consistency metric `E_consistency` is defined as an average cosine distance, where "Lower values indicate higher temporal feature consistency" (Section 3.3). This defines an *error* metric.
    *   However, the results in Table 4 show PredNext achieving a *higher* "consistency" score (0.819) than the baseline (0.773). If this is the error metric, it implies PredNext worsens consistency. This directly contradicts the paper's narrative.
    *   Similarly, the plots in Figure 1a and Figure 4 (top row) show PredNext achieving higher values on a y-axis labeled "similarity." This suggests the reported values are similarity, not error, which contradicts the formal definition in Section 3.3. This inconsistency makes it difficult to interpret the paper's claims about consistency.*   **Lack of Direct Comparison to ANN-based Methods:** While the paper provides a strong evaluation within the SNN domain, it lacks a direct quantitative comparison to equivalent Artificial Neural Network (ANN) based video self-supervised learning methods.
    *   The related work section discusses several influential ANN-based methods like DPC and VideoMoCo (Appendix B.2), and a qualitative comparison is provided (Table 2), but no quantitative performance numbers from these methods are included in the main results tables.
    *   For example, Table 3 compares against supervised SNNs but not unsupervised ANNs using the same SEW ResNet18 backbone. This makes it difficult to assess the absolute performance of the proposed SNN models and understand the remaining performance gap between SNNs and ANNs on these large-scale video tasks.
    *   Without this context, the significance of the results for the broader video understanding community is harder to gauge.*   **Inconsistent and Missing Hyperparameter Details:** Several critical hyperparameters are missing, reported with conflicting values across different sections, or defined ambiguously, which severely hinders reproducibility.
    *   The loss weighting coefficient `α` in the final loss function `L = (1 − α) · L_ssl + α · L_pred` is a crucial hyperparameter that is never stated in the main paper or appendix (Algorithm 1, Section 2.2).
    *   The prediction head's hidden layer dimensionality is reported with conflicting values: 128 in the method description (Section 2.2), but 512 in the ablation study and appendix (Section 4, Appendix E.2).
    *   The pre-training weight decay is stated as 1e-4 in the main text (Section 2.1) but 1e-6 in the appendix (Appendix E.1).
    *   The crop size for MiniKinetics is given as 114x114 in one section (Section 2.1) but 112x112 in others (Section 3.1, Appendix E.1).
    *   The prediction loss `L_pred` is formulated differently in the text equation (Section 2.2) and in Algorithm 1, leading to ambiguity in its calculation.*   **Minor Presentation and Formatting Issues:** The manuscript contains several small errors that suggest a lack of careful proofreading and detract from the overall quality.
    *   The caption for Table 2 is an erroneous copy of the caption for Table 1 (Section: Related Work).
    *   The text in Section 4 refers to sub-figures 6(a), 6(b), and 6(c), but the corresponding plots are presented as separate, unlabeled figures, making them difficult to follow.
    *   There is a numerical discrepancy in reported results: the HMDB51 Top-5 accuracy for PredNext-SimCLR is reported as 66.19% in Table 3 but 66.34% in Table 6.4) Suggestions for Improvement
*   **Clarify the Definition and Reporting of Temporal Consistency:** To resolve the contradiction, the authors should ensure the definition, reporting, and discussion of the consistency metric are aligned.
    *   Choose one convention (error or similarity) and use it consistently. For example, if reporting similarity, redefine the metric as `S_consistency = cos(...)` and state that higher is better.
    *   Update the values in Table 4 and the y-axis labels in Figure 1a and Figure 4 to match the chosen convention.
    *   When discussing the "Forced Consistency" experiment (Table 4), elaborate on why maximizing raw similarity harms performance (e.g., it may lead to collapsed or overly smooth representations that lack discriminative power).*   **Contextualize Performance with ANN Baselines:** To strengthen the paper's impact, provide performance context from the ANN domain.
    *   The most direct way would be to train an ANN version of the SEW ResNet18 backbone with a representative SSL method (e.g., SimSiam) on one of the key benchmarks (e.g., UCF101). Including this single data point in Table 3 would provide a valuable anchor for the SNN results.
    *   Alternatively, if new experiments are not feasible, the authors could add a row to Table 3 with results reported in prior work (e.g., Feichtenhofer et al., 2021) that use a comparable architecture, clearly noting that the results are from a different codebase.*   **Provide Complete and Consistent Hyperparameter Specifications:** To ensure reproducibility, all critical hyperparameters must be reported accurately and consistently.
    *   Please state the value of the loss weighting coefficient `α` used for the experiments and briefly describe how it was chosen (Section 2.2).
    *   Please resolve the conflicting reports for the prediction head's hidden dimension and state the single, correct value used for the experiments (Section 2.2, Section 4, Appendix E.2).
    *   Please correct the conflicting values for weight decay (Section 2.1, Appendix E.1) and MiniKinetics crop size (Section 2.1, Section 3.1) to reflect the actual experimental setup.
    *   Please ensure the mathematical formulation of `L_pred` is identical in the text equation and Algorithm 1.*   **Proofread and Correct Presentation Errors:** A thorough proofreading of the manuscript is needed to fix formatting and reporting errors.
    *   Please correct the caption for Table 2 to accurately describe its content.
    *   Please group the plots for the ablation studies into a single Figure 6 and add the corresponding (a), (b), (c) sub-labels as referenced in the text.
    *   Please identify and correct the numerical discrepancy for the result reported in Table 3 and Table 6.5) Score
- Overall (10): 7 — The paper presents a novel and well-motivated method for an important problem, but its strong empirical results are undermined by significant inconsistencies in reporting that affect clarity and reproducibility.
- Novelty (10): 8 — The application of cross-view predictive coding to enforce temporal consistency in unsupervised SNNs for video is a novel and well-motivated contribution to its specific domain.
- Technical Quality (10): 7 — The experimental evaluation is extensive, but numerous inconsistencies in reported hyperparameters and results (Table 3 vs Table 6, Section 2.1 vs Appendix E.1) raise concerns about reproducibility and rigor.
- Clarity (10): 6 — The paper is generally readable, but a fundamental contradiction in the core "consistency" metric (Section 3.3 vs Table 4) and multiple presentation errors (Table 2, Figure 6) significantly obscure key details.
- Confidence (5): 5 — I am highly confident in my assessment, as the claims are well-supported by extensive experiments and the paper aligns with my area of expertise.