Integrity and Consistency Risk Report for “ChemBOMAS”

Summary
Based on the provided manuscript, several high-impact internal inconsistencies, numerical mismatches, and missing methodological details materially affect the paper’s correctness and reproducibility. Evidence for each issue is explicitly anchored to sections, tables, figures, or appendices in the manuscript.

Major Internal Inconsistencies and Contradictions

1) UCB child selection: single vs. top‑5 nodes
- In Section 3.4 Step 3, the method states “At each layer, the top-5 nodes by UCB value are selected for further exploration” (Block #14).
- In Algorithm 1 (Appendix F), the traversal selects a single child per layer via arg max and proceeds down one path to a leaf (Block #60).
- These are contradictory core algorithmic descriptions (parallel exploration vs. a single-path greedy selection), affecting how subspaces are selected and thus the entire optimization procedure.

2) Initialization fairness claim vs. different initial values
- Section 4.3.2 asserts: “All methods were initialized with an identical, fixed set of pseudo-data to ensure a fair comparison” (Block #21).
- Table 2 shows different “Initial (%)” values across methods on the same dataset; e.g., Suzuki initial: BO w/o tree 54.04, Expert-Guided 61.96, ChemBOMAS_d-d 72.98, ChemBOMAS_k-d 72.98 (Block #22).
- If the initial pseudo-data are identical, these initial values should not differ across clustering strategies. This discrepancy undermines claims of fair initialization and comparison.

3) Claimed “up to 34-fold improvement” contradicts Table 2
- Section 4.3.2 claims “up to a 34-fold improvement in optimization efficiency compared to the standard BO baseline without a tree” (Block #22).
- Table 2 “95% Max Iter” ratios do not support this:
  - Suzuki: 16 (BO w/o tree) vs. 3 (ChemBOMAS_k-d) ≈ 5.3×
  - Arylation: 8 vs. 1 = 8×
  - Buchwald_sub-1: 26 vs. 2 = 13×
  - Buchwald_sub-2: 13 vs. 1 = 13×
- No instance approaches 34×; the statement is numerically inconsistent with the presented evidence (Block #22).

4) Percentage improvement claims for R² are inconsistent with Table 1
- Section 4.3.1 states ChemBOMAS “achieved the highest prediction accuracy … with R² scores exceeding the second-best model by 2000% (Arylation) and 140% (Buchwald)” (Block #20).
- Table 1 shows Arylation R²: ChemBOMAS 0.13; second-best appears LLaMa‑3.1‑8B at 0.09 or GLM4‑9B at 0.01. Improvements vs 0.09 ≈ 44% and vs 0.01 ≈ 1200%, not 2000% (Block #20).
- Buchwald R²: ChemBOMAS 0.20; second-best LLaMa‑3.1‑8B at 0.01 or GLM4‑9B at 0.00; improvements vs 0.01 ≈ 1900% or vs 0.00 undefined/infinite, not 140% (Block #20).
- The reported percentage improvements do not match Table 1 values.

5) Wet‑lab sample counts and iterations are inconsistent
- Section 4.3.3 claims: “identified the optimal reaction condition with a yield of 96% after evaluating only 43 samples in 2 iterations” (Block #23).
- Appendix H states “multiple acquisition functions … fourteen samples per round” (Block #74) and Figure 5 covers iteration rounds 0–4 (Blocks #71, #73). At 14 samples/round, 2 iterations would total 28 samples, not 43.
- The iteration/sample accounting for the wet-lab study is inconsistent with the stated protocol and displayed figure (Blocks #23, #74, #73).

6) “Initialized with 1% of the data as the prior” vs. varying “Initial (%)” across ablations
- Experiment setup specifies “initialized with 1% of the data as the prior” for BO (Block #18).
- Table 4 shows large differences in “Initial (%)” for the same dataset across ablations (e.g., Arylation: 78.71 for Full ChemBOMAS vs. 45.40 for w/o data module vs. 45.98 for w/o knowledge module vs. 64.64 for w/o both) (Block #30). If the prior is truly identical across ablations, the initial values should align. This suggests inconsistent initialization across experiments (Blocks #18, #30).

Unsupported or Unclear Claims Impacting Trustworthiness

7) “Poor LLM zero-shot performance confirms datasets were not part of training data”
- Section 4.3.1 states: the poor performance of GPT‑4o and GPT‑5 “confirms that these chemical datasets were not part of their training data” (Block #20).
- This inference is not logically supported by the evidence; poor zero-shot performance does not confirm absence of training exposure. No direct evidence found in the manuscript that validates this claim.

8) Multi-objective to single scalar mapping for LNP3 is missing
- Appendix J describes LNP3 as a multi‑objective problem (maximize loading and encapsulation, minimize particle size) yet Table 9 reports a single “Best Found” value (0.62) without explaining the scalarization/aggregation method (Block #82).
- No direct evidence found in the manuscript explaining the scalarization, weighting, or normalization used to produce the single objective metric in Table 9 (Block #82). This omission materially affects interpretability and reproducibility.

Methodological Details Missing or Inconsistent

9) UCB notation mismatch and initialization edge case
- Section 3.4 uses UCB_i = R̄_i + C_p√(log(N_parent)/n_i) (Block #14), while Experiment setup names κ = 20 as the exploration constant (Block #18). It is unclear whether κ = C_p, and no rationale is provided for choosing 20.
- Algorithm 1 uses ln(n_parent) (Block #60), but no handling is described for n_parent = 0 at initialization. No direct evidence found in the manuscript clarifying the initial selection policy.

10) Pseudo‑data refinement threshold τ unspecified
- Appendix D defines a similarity-based removal using a threshold τ (Eq. (2)) but provides no numerical value, selection rule, or sensitivity analysis anywhere else (Block #50). This hampers reproducibility.

11) Prompting/configuration details for GPT‑4o/GPT‑5 zero‑shot regression are absent
- Table 1 reports performance for “General-purpose LLMs with zero-shot inference” including GPT‑4o and GPT‑5 (Block #20), but the manuscript does not provide prompts, numeric inference setup, or constraints used for those evaluations.
- No direct evidence found in the manuscript detailing the prompt formats, temperature, decoding strategy, or numeric parsing for GPT‑4o/GPT‑5 zero-shot predictions, in contrast to detailed prompts provided for the authors’ own LLM (Appendix G; Blocks #55–#66).

12) Acquisition function alignment with baselines is unclear
- Section 4.2 claims “The acquisition function and other BO configurations were kept consistent with the baseline methods for a fair comparison” (Block #18).
- The method section allows UCB or EI (Section 3.5; Block #15), while Appendix C.4 states the BO baseline uses qLogEI (Block #44). No direct evidence found in the manuscript specifying the exact acquisition function used by ChemBOMAS during comparisons, creating ambiguity in fairness claims.

Additional Minor Consistencies/Notes
- Symbol inconsistency for the exploration constant (κ vs C_p) is minor but should be clarified (Blocks #14, #18).
- The statement on “up to 5-fold” acceleration in the Introduction (Block #4) conflicts with “up to 34-fold” later (Block #22); these should be reconciled.

Conclusion
The manuscript contains several substantive inconsistencies (algorithmic, numerical, and procedural) and missing details that materially affect the validity of its claims and reproducibility. Addressing the contradictions (especially UCB selection strategy, initialization fairness, fold-improvement claims, and wet-lab iteration/sample counts), and supplying missing methodological details (multi-objective scalarization, thresholds, and GPT‑4o/GPT‑5 prompting) is necessary to restore confidence in the reported results. If these issues are resolved and documented, the work’s integrity and scientific reliability would be substantially improved.