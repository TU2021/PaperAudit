1) Summary
This paper introduces ChemBOMAS, a Large Language Model (LLM)-enhanced multi-agent system designed to accelerate Bayesian Optimization (BO) for chemical discovery. The framework addresses the common BO challenges of data scarcity and vast search spaces through two synergistic strategies. A data-driven strategy fine-tunes an 8B-parameter LLM on a small fraction (1%) of labeled data to generate pseudo-data, providing a robust initialization for the optimization process. Concurrently, a knowledge-driven strategy uses a RAG-enhanced LLM to partition the search space into promising subspaces by reasoning over chemical literature and data. A UCB algorithm selects high-potential subspaces, where BO is then performed. The authors evaluate ChemBOMAS on four chemical benchmarks and a wet-lab experiment, demonstrating significant improvements in convergence speed and final performance compared to baseline methods.2) Strengths
*   **Comprehensive and Rigorous Experimental Validation**
    *   The method is evaluated on four distinct chemical benchmark datasets (Suzuki, Arylation, Buchwald_sub-1, Buchwald_sub-2), demonstrating consistent performance gains across different reaction types and data distributions (Figure 2, Section 4.3.3).
    *   The authors compare ChemBOMAS against a strong and diverse set of four baseline methods, including traditional BO and recent LLM-based approaches like GOLLuM and BO-ICL, providing a clear context for its performance improvements (Figure 2, Appendix C.4).
    *   The inclusion of a real-world wet-lab experiment on a previously unreported reaction is a standout feature. The system's ability to find a 96% yield condition in just two iterations, vastly outperforming a human expert's 15% yield, provides compelling evidence of its practical utility and ability to handle true "cold start" problems (Section 4.3.3, Appendix H, Figure 5).
    *   Thorough ablation studies are presented, which effectively isolate and quantify the contributions of the data-driven and knowledge-driven modules (Table 4), as well as the impact of pre-training and fine-tuning for the regression model (Table 3). This strengthens the claims about the synergistic nature of the framework.*   **Novel and Well-Motivated Synergistic Framework**
    *   The core contribution is the synergistic integration of two distinct LLM-powered strategies to tackle two fundamental challenges in BO. The data-driven pseudo-data generation directly addresses the "cold start" problem (Section 3.3), while the knowledge-driven search space decomposition tackles the "curse of dimensionality" (Section 3.4).
    *   The framework is designed as a closed-loop system where the two modules interact, as illustrated in the system diagram (Figure 1). Pseudo-data informs subspace selection, and experimental results from BO update the search, creating a virtuous cycle.
    *   The ablation study in Table 4 provides strong evidence for this synergy. Removing either the data-driven or knowledge-driven module leads to a significant drop in performance, demonstrating that the combination is more effective than either part in isolation. For example, on the Suzuki dataset, the full model reaches 96.15% yield in 3 iterations, while ablating the data module drops performance to 82.65% and ablating the knowledge module drops it to 88.98%.*   **Effective and Principled Application of LLMs in a Scientific Domain**
    *   The paper demonstrates a sophisticated use of different LLMs for tasks suited to their capabilities. A large, proprietary model (GPT-4o) with RAG is used for complex reasoning and knowledge extraction to partition the search space, mitigating hallucination (Section 3.4). A smaller, open-source model (LLaMA-3.1-8B) is efficiently fine-tuned for a specialized regression task, showing superior performance to zero-shot inference from larger models (Section 3.3, Table 1).
    *   The data-driven module shows that fine-tuning on a very small amount of labeled data (1%) is sufficient to create a high-quality regressor for pseudo-data generation. The analysis in Appendix I.2 (Tables 7 & 8) thoughtfully explores the trade-off between data volume, regressor accuracy, and final BO performance.
    *   The knowledge-driven module's use of RAG to ground the LLM's reasoning in scientific literature and databases is a principled approach to improving reliability and leveraging domain knowledge, a critical step for applying LLMs to scientific discovery (Section 3.4).3) Weaknesses
*   **Inclusion of Speculative and Unverifiable Experimental Results**
    *   Table 1 and the accompanying text in Section 4.3.1 include results for a model named "GPT-5," citing "OpenAI, 2025." This model is not publicly available, and its performance characteristics are unknown. Including fabricated results for a non-existent model is a serious breach of scientific reporting standards and undermines the credibility of the entire experimental evaluation.
    *   The paper asserts that the poor performance of GPT-4o "confirms that these chemical datasets were not part of their training data" (Section 4.3.1). While this is a plausible inference, it is not a confirmation. Such definitive claims about the training data of closed models are unverifiable and should be stated more cautiously as hypotheses.*   **Insufficient Methodological Detail for Reproducibility**
    *   The knowledge-driven module relies on a "hybrid Retrieval-Augmented Generation (RAG) approach" (Section 3.4), but critical implementation details are missing. The paper does not specify the exact literature sources, professional databases, or web search tools used. The process for constructing the vector database and the specific retrieval mechanism (e.g., query formulation, number of retrieved documents) are not described, making this core component of the method impossible to reproduce.
    *   The UCB-based subspace selection algorithm is described inconsistently. Section 3.4 states that "the top-5 nodes by UCB value are selected for further exploration," which suggests a parallel search. However, Algorithm 1 in Appendix F describes a greedy selection of a single best node (`arg max`), which contradicts the main text. This ambiguity makes the tree search strategy unclear.
    *   The paper introduces and evaluates a "data-driven" clustering approach (ChemBOMAS<sub>d-d</sub>) in Table 2, but this method is not described in the main methodology (Section 3). Appendix D alludes to using pseudo-points to select a tree structure (Equation 3), but the connection is not made explicit, leaving the reader to guess the implementation of a method presented as a key comparison point.*   **Ambiguity in Final Model Configuration and Component Evaluation**
    *   It is unclear which clustering strategy (knowledge-driven `ChemBOMAS_k-d` or data-driven `ChemBOMAS_d-d`) is used in the final, integrated ChemBOMAS model that is evaluated against baselines in Figure 2. The methodology focuses on the knowledge-driven approach, but the strong performance of the data-driven variant in Table 2 raises questions about the final design choice and its justification.
    *   The performance of the final model is inconsistent with its components. For the Arylation dataset, the full ChemBOMAS model achieves a best value of 81.26% (Table 4), whereas the knowledge-driven component alone (ChemBOMAS<sub>k-d</sub>) is reported to achieve a superior result of 82.23% (Table 2). This discrepancy is not explained.
    *   The pseudo-data refinement strategy described in Appendix D, which involves removing points based on similarity and performance, is an important detail for mitigating noise. However, its impact is not evaluated. An ablation study showing the effect of this refinement would strengthen the paper's claims about the robustness of the data-driven module.*   **Inconsistent and Exaggerated Quantitative Claims**
    *   The text makes quantitative claims about regression performance that are not supported by the data in Table 1. For instance, Section 4.3.1 claims the model's R² score on the Arylation dataset exceeds the second-best by "2000%," but the table shows an improvement from 0.09 to 0.13, which is approximately 44%.
    *   The claimed optimization speedup is exaggerated. Section 4.3.2 claims "up to a 34-fold improvement in optimization efficiency," but the "95% Max Iter" data in Table 2 shows a maximum speedup of 26x (from 26 to 1 on Buchwald_sub-1), with other datasets showing much smaller gains.
    *   The interpretation of the ablation study in Section 4.4 is misleading. The text claims single-module ablations are "only marginally better" than the baseline without both modules. However, for the Suzuki dataset, the data-only module ("w/o knowledge module") improves the best found value from 83.16% to 88.98% (Table 4), a significant improvement that contradicts the narrative.4) Suggestions for Improvement
*   **Ensure Academic Integrity in Experimental Reporting**
    *   Immediately remove all references to "GPT-5" and its associated results from Table 1 and the text. This is non-negotiable for publication at any reputable venue.
    *   Rephrase the claim regarding the training data of general-purpose LLMs. Instead of stating it as a confirmation, present it as a well-supported hypothesis, e.g., "The poor zero-shot performance suggests it is unlikely that these specific benchmark datasets were included in the models' pre-training corpora."*   **Provide Sufficient Detail for Full Reproducibility**
    *   In a new appendix or within Section 3.4, provide a detailed description of the RAG pipeline. This should include: (1) a list of all data sources (e.g., specific journals, Reaxys, PubChem), (2) the methodology for processing these sources into a knowledge base (e.g., text chunking strategy, embedding model used), and (3) the retrieval process (e.g., how queries are formed, the value of k for top-k retrieval).
    *   Clarify the UCB tree search algorithm. Reconcile the "top-5" description in Section 3.4 with the `arg max` in Algorithm 1. If multiple paths are explored per iteration, the algorithm and text should be updated to reflect this. If only one path is chosen, the "top-5" language should be removed.
    *   Add a clear description of the data-driven clustering method (ChemBOMAS<sub>d-d</sub>) to the methodology or an appendix. Explain precisely how the pseudo-data is used to construct or select a tree, explicitly linking to Equation 3 in Appendix D if it is part of this method.*   **Clarify Model Configuration and Strengthen Component Analysis**
    *   Explicitly state which clustering strategy is used in the final ChemBOMAS model presented in Section 4.3.3 and Figure 2. Justify this choice based on the results from Table 2 or other analysis.
    *   Investigate and explain the performance discrepancy where the full model underperforms one of its components on the Arylation dataset (Table 2 vs. Table 4).
    *   Consider adding a small ablation study, perhaps in the appendix, that evaluates the impact of the pseudo-data refinement strategy described in Appendix D. This would demonstrate its contribution to the overall performance and robustness of the framework.*   **Reconcile Quantitative Claims with Reported Data**
    *   Carefully review and correct all quantitative claims in the text to ensure they accurately reflect the data presented in the tables. For example, revise the percentage improvements for the regression model in Section 4.3.1 to match the R² values in Table 1.
    *   Correct the claim about optimization speedup in Section 4.3.2 to state the maximum observed speedup based on the data in Table 2 (e.g., "up to 26-fold").
    *   Revise the narrative for the ablation study in Section 4.4 to provide a more nuanced and accurate interpretation of the results in Table 4, acknowledging cases where single modules provide substantial (or negative) contributions compared to the baseline.5) Score
*   Overall (10): 6 — The paper presents a novel framework with strong potential and compelling wet-lab validation (Figure 5), but its credibility is severely undermined by the inclusion of fabricated results ("GPT-5" in Table 1) and a pattern of inconsistent or exaggerated claims.
*   Novelty (10): 8 — The synergistic combination of knowledge-driven search space partitioning and data-driven pseudo-data initialization for BO is a highly novel and impactful contribution to the field (Figure 1, Section 3).
*   Technical Quality (10): 4 — The core idea is sound, but the inclusion of fabricated results is a major breach of scientific integrity, and the work is further weakened by numerous inconsistencies between claims and data (Table 1, Table 2, Table 4) and a lack of key details for reproducibility.
*   Clarity (10): 6 — The paper is generally well-structured, but its clarity is significantly compromised by contradictions between the text and tables (Section 4.3.1, 4.3.2), a misleading narrative (Section 4.4), and missing methodological details (Section 3.4).
*   Confidence (5): 5 — I am highly confident in my assessment, as the topic aligns directly with my expertise in Bayesian optimization, machine learning for science, and LLM applications.