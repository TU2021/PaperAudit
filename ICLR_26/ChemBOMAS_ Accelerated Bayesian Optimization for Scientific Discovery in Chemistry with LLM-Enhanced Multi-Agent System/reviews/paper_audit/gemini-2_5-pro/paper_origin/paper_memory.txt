# Global Summary
The paper introduces ChemBOMAS, a multi-agent system that uses Large Language Models (LLMs) to accelerate Bayesian Optimization (BO) for scientific discovery in chemistry. The core problem addressed is the inefficiency of standard BO, which suffers from data scarcity ("cold start") and vast search spaces ("curse of dimensionality"). ChemBOMAS employs a synergistic dual-strategy approach: a data-driven module uses an 8B-parameter LLM, fine-tuned on only 1% of labeled data, to generate pseudo-data for robust initialization. Concurrently, a knowledge-driven module uses a RAG-enhanced LLM (GPT-4o) to partition the search space based on chemical principles, with an Upper Confidence Bound (UCB) algorithm selecting the most promising subspaces for exploration.

The framework was evaluated on four chemical benchmarks (Suzuki, Arylation, Buchwald_sub-1, Buchwald_sub-2), a materials science benchmark (LNP3), and a previously unreported wet-lab experiment. ChemBOMAS is claimed to set a new state-of-the-art, improving optimal results by 3–10% and converging up to 5 times faster than baselines like standard BO, GOLLuM, LA-MCTS, and BO-ICL. In the wet-lab experiment, it achieved a 96% yield in 2 iterations (43 samples), significantly outperforming a human expert's 15% yield. Ablation studies confirm that the synergy between the data-driven and knowledge-driven modules is essential for its performance. The authors acknowledge limitations, including dependency on the LLM's knowledge accuracy and the lack of explicit safety constraints.

# Abstract
ChemBOMAS is a Large Language Model (LLM)-enhanced multi-agent system designed to accelerate Bayesian Optimization (BO) in chemistry. It addresses the challenges of sparse experimental data and vast search spaces. The system integrates two strategies:
- A data-driven strategy uses an 8B-scale LLM regressor, fine-tuned on 1% of labeled samples, to generate pseudo-data for robust BO initialization.
- A knowledge-driven strategy uses a hybrid Retrieval-Augmented Generation (RAG) approach to guide an LLM in partitioning the search space, mitigating hallucinations. An Upper Confidence Bound (UCB) algorithm then identifies high-potential subspaces.
By performing BO within these refined subspaces with the support of generated data, ChemBOMAS improves both effectiveness and efficiency. Evaluations on multiple chemical benchmarks show it sets a new state-of-the-art, accelerating optimization by up to 5-fold compared to baselines.

# Introduction
- Traditional chemical discovery is labor-intensive. Automated labs with AI, particularly Bayesian Optimization (BO), can accelerate this process.
- BO faces two major obstacles in chemistry: the "cold start" problem due to initial data scarcity and the "curse of dimensionality" from large parameter spaces. These lead to slow convergence.
- Existing BO acceleration strategies (e.g., search space partitioning, pseudo-data generation) are often single-technique and purely data-driven, failing to leverage chemical knowledge and thus wasting resources in implausible regions.
- The paper proposes ChemBOMAS, an LLM-enhanced multi-agent system that synergistically combines a knowledge-driven search space decomposition module and a data-driven pseudo-data generation module.
- The knowledge-driven module uses an LLM to reason over chemical literature and databases to prune the search space. The data-driven module uses a fine-tuned LLM to generate pseudo-data, warm-starting the BO process.
- The paper claims ChemBOMAS improves optimal results by 3–10% and converges 2–5 times faster than baselines on four chemical benchmarks, using only 1% of the available data for fine-tuning. Its utility is also validated in a wet-lab experiment.

# Related Work
- LLMs show potential to improve BO by providing prior knowledge, enhancing surrogate models, or automating acquisition function design.
- However, directly replacing core BO components with LLMs is challenging due to issues like "hallucinations," poor uncertainty quantification, high computational cost, and lack of theoretical guarantees.
- Other methods for managing complex search spaces include tree-based decomposition (e.g., LA-MCTS) and hierarchical BO.
- This work focuses on robustly integrating LLMs as auxiliary tools to enhance BO, leveraging their strengths while mitigating weaknesses, rather than substituting core BO modules.

# Method
- **Problem Setup:** The goal is to efficiently find the optimal combination of variables in a task's search space to maximize an objective function.
- **Framework of ChemBOMAS:** A multi-agent framework with two main strategies:
    1.  **Data-driven:** A fine-tuned LLM regressor generates pseudo-data to initialize the optimization.
    2.  **Knowledge-driven:** An LLM with a hybrid RAG approach partitions the search space. A UCB algorithm selects promising subspaces from this partition.
    3.  **Optimization:** BO is performed within the selected subspaces using the generated pseudo-data.
- **Data-Driven Strategy:**
    - Step 1 (Pre-training): A LLaMA 3.1 model is pre-trained on the Pistachio dataset to predict reaction conditions from reactants and products.
    - Step 2 (Fine-tuning): The model is fine-tuned on a small (1%) labeled dataset using a regression head and LoRA (rank r=8) to predict reaction performance.
    - Step 3 (Pseudo-data Generation): The fine-tuned regressor generates pseudo-labels for all unsampled data points. This data is used to initialize the UCB algorithm for subspace selection and the BO surrogate model. A refinement strategy is used to mitigate noise (details in Appendix D).
- **Knowledge-Driven Strategy:**
    - Uses the GPT-4o API.
    - Step 1 (Partitioning): A RAG approach helps the LLM rank chemical categories by importance and then cluster candidate substances within each category based on key physicochemical properties.
    - Step 2 (Tree Construction): A hierarchical search tree is built based on the importance ranking and clustering results.
    - Step 3 (Subspace Selection): A UCB algorithm explores the tree to identify high-potential subspaces. The top-5 nodes by UCB value are selected at each layer for further exploration.
- **Bayesian Optimization in ChemBOMAS:**
    - A Gaussian Process (GP) surrogate model with a Matérn kernel is fitted on both real observations and pseudo-data.
    - An acquisition function (e.g., UCB, EI) is maximized over the selected high-potential subspaces to choose the next experiment.

# Experiments
- **Data:**
    - Pre-training: ~50,000 reactions from the Pistachio dataset.
    - Fine-tuning/BO: Suzuki, Arylation, and Buchwald datasets. The Buchwald dataset was split into two consistent subsets (Buchwald_sub-1, Buchwald_sub-2). Only 1% of labeled data was used for fine-tuning.
- **Experiment Setup:**
    - Training on 2x NVIDIA A800 GPUs. Fine-tuning used a learning rate of 1e-4 for 100 epochs.
    - BO was run for 40 iterations, sampling 0.1% of the dataset per iteration.
    - All experiments were repeated 5 times with different random seeds.
- **Performance Comparison:**
    - **Regression Models:** In Table 1, the ChemBOMAS regressor achieved superior prediction accuracy on Arylation and Buchwald datasets, with R² scores exceeding the second-best model by 2000% and 140%, respectively. Task-specific fine-tuning was shown to be essential, as zero-shot GPT-4o and GPT-5 performed poorly (negative R²).
    - **Cluster Methods:** Table 2 shows that using a subspace search tree accelerates BO by up to 34-fold compared to standard BO. The knowledge-driven clustering (ChemBOMAS_k-d) performed best, matching or surpassing an expert-guided approach.
    - **Optimization:** Figure 2 shows ChemBOMAS achieved the highest objective values across all four benchmarks: 96.15% (Suzuki), 81.26% (Arylation), 80.00% (Buchwald_sub-1), and 56.80% (Buchwald_sub-2). It also converged fastest (e.g., in 3 iterations on Suzuki). The method showed the lowest variance across runs, indicating high stability.
    - **Wet-lab Validation:** For a previously unreported reaction, ChemBOMAS found an optimal condition with 96% yield in 2 iterations (43 samples), far exceeding the 15% yield from a traditional control variable method.
- **Ablation Study:**
    - Table 3 shows that supervised fine-tuning (SFT) is the most critical component for the regression model's performance, and combining it with pre-training yields the best results.
    - Table 4 demonstrates that removing either the data-driven or knowledge-driven module significantly degrades optimization performance. For Suzuki, the full model reached 96.15%, while ablating the data module yielded 82.65% and ablating the knowledge module yielded 88.98%. This confirms the synergy between the two modules is essential.

# Discussion
- **Limitations:** The framework's performance is dependent on the accuracy and completeness of the LLM's knowledge base; errors can lead to suboptimal search space decomposition. It also lacks explicit safety and feasibility constraints, meaning it could recommend hazardous or impractical conditions, necessitating expert oversight.

# Conclusion
- ChemBOMAS is an LLM-enhanced multi-agent framework that accelerates BO for chemical reactions.
- It synergistically combines knowledge-driven search space decomposition and data-driven pseudo-data generation to address data scarcity and large search spaces.
- Benchmark evaluations and a wet-lab validation on a previously unreported industrial reaction demonstrate its potential for practical application, showing improved performance over domain expert methods.

# References
This section contains citations for the works mentioned in the manuscript, including sources for datasets, baseline algorithms, and related techniques in BO and LLMs for science.

# Appendix
- **A:** Provides an illustrative example of how the LLM constructs an optimization tree for a reaction.
- **B:** Explains that after each experimental round, the data module is retrained, the optimization tree is updated, and the BO module recommends the next conditions.
- **C:** Details the datasets. Pre-training used the Pistachio dataset (19.17M reactions). Fine-tuning and BO used Suzuki (5030 data points), Arylation (3678), Buchwald_sub-1 (629), and Buchwald_sub-2 (765). Baselines are BO, BO-ICL, GOLLuM, and LA-MCTS.
- **D:** Describes a dual-strategy for refining pseudo-data: removing points based on cosine similarity to new real data and randomly discarding points with high predicted performance to encourage exploration.
- **E:** Shows heatmaps (Figure 4) indicating that the optimization trajectories of the automated ChemBOMAS strategies are qualitatively similar to an expert-guided approach.
- **F:** Presents the complete pseudocode for the ChemBOMAS algorithm.
- **G:** Provides examples of the prompts used for the data module (pre-training and fine-tuning) and the knowledge module (clustering).
- **H:** Details the wet-lab experiment on a palladium-catalyzed cross-coupling reaction. ChemBOMAS found a 96% yield condition in the second iteration, while a human expert achieved 15%. Ten repeated initializations confirmed robust performance against the "cold-start" problem.
- **I:** Contains additional results analysis.
    - **I.1:** Varying the batch size per iteration (0.05% to 0.4%) had minimal impact on the final result but significantly increased runtime. A batch size of 0.1% was chosen as an efficient trade-off.
    - **I.2:** Increasing the fine-tuning data from 0.25% to 4% improves the regressor's R² score. However, BO performance does not always improve with more data, showing diminishing returns and suggesting 1% is a sufficient and effective amount.
- **J:** Demonstrates the framework's generalization to a materials science benchmark (LNP3 dataset). ChemBOMAS achieved competitive performance, matching the best value found (0.62) but with higher sample efficiency (28 iterations vs. 33 for GoLLuM and 38 for BO).