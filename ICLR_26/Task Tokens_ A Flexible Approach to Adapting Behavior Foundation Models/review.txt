### Summary

The paper introduces **Task Tokens**, a method designed to adapt pre-trained **Behavior Foundation Models (BFMs)** like **MaskedMimic** to specific downstream tasks in humanoid control using a **reinforcement learning (RL)** approach. The key innovation is the introduction of a **task encoder** that produces **task-specific tokens** to steer the BFM without altering the underlying model. This enables efficient adaptation with **minimal parameters** (about **200k** per task), avoiding catastrophic forgetting by freezing the pre-trained BFM. The approach is shown to outperform traditional RL methods and full fine-tuning approaches in terms of parameter efficiency, speed, and robustness, especially in out-of-distribution (OOD) scenarios. The method is also demonstrated to be effective across a variety of tasks, including **human-like control** and multi-modal task integration, providing a **parameter-efficient** and **scalable** solution for adapting BFMs to different goals.

---

### Strengths

1. **Novel and Elegant Solution**:

   * The core idea of using lightweight **Task Encoders** to adapt BFMs while keeping the underlying model frozen is innovative. This solution avoids **catastrophic forgetting** and preserves the **generalization** capabilities of the BFM.

2. **Strong Empirical Validation**:

   * The experiments show that Task Tokens are highly **parameter-efficient**, achieving up to **125¡Á fewer parameters** than full fine-tuning while maintaining competitive performance. The approach is validated across various tasks with **robustness to perturbations** like friction and gravity.

3. **Human-Like Control**:

   * The human study confirms that the adapted behaviors generated by Task Tokens are perceived as more **human-like** compared to baselines like **PPO**, **AMP**, and fine-tuned **MaskedMimic**. This indicates that Task Tokens preserve the **natural motion manifold** while enabling task-specific adaptation.

4. **Flexible and Modular**:

   * Task Tokens allow the **seamless integration** of **multi-modal inputs**, combining **prior tokens** (user-defined) and **task tokens** (learned through RL) to guide the behavior in a flexible and modular way.

5. **Scalability**:

   * The method is designed to scale across a wide variety of tasks without the need for **full retraining**, which is essential in real-world robotic applications where many different behaviors might need to be adapted.

---

### Weaknesses

1. **Overstatement of Parameter Efficiency**:

   * The **125¡Á fewer parameters** claim is somewhat overstated. Although Task Tokens are **parameter-efficient** compared to full fine-tuning, this efficiency is more significant in terms of **memory usage** and task-specific adaptation rather than **raw training speed**. There is a need for further **comparisons** in terms of **GPU-hours**, **memory usage**, and **inference latency**.

2. **Inconsistent Baseline Results**:

   * The performance variability across tasks in **Table 1** (e.g., **PPO** performing well on **Strike**, **Reach**, and **Direction**, but poorly on **Steering** and **Long Jump**) suggests that the baselines may not have been **carefully optimized** for each task. This inconsistency could impact the perceived superiority of Task Tokens.

3. **Robustness Claims**:

   * The paper's robustness claims, particularly the **"orders of magnitude"** improvement in low friction settings, might be exaggerated. The improvement may be due to the baseline methods struggling at **extreme values**, where any small increase in success rates becomes disproportionately large. This needs to be clarified.

4. **Limited Task Complexity**:

   * The tasks evaluated are **reactive motor skills** or **goal-reaching tasks**, which do not require **complex, multi-stage sequential logic**. It is unclear how well Task Tokens would generalize to more complex tasks that involve **multiple sub-goals** or **sequential reasoning**.

5. **Scalability for Multi-Task and Continual Learning**:

   * Training one encoder per task makes Task Tokens highly efficient for individual tasks, but it doesn't scale well to **multi-task** or **continual learning** setups. The paper does not provide a clear path for **compositional learning** or **task switching** in a **multi-task** environment, which is a significant limitation for more dynamic settings.

6. **Insufficient Comparison to Parameter-Efficient Fine-Tuning (PEFT)**:

   * The paper lacks a **direct comparison** to **PEFT methods** like **LoRA** or **adapters** applied inside the BFM. These techniques could offer more direct and comparable parameter-efficient alternatives to Task Tokens, and a comparison would provide more context for Task Tokens' advantages and disadvantages.
