1) Summary
This paper introduces "Task Tokens," a method for adapting pre-trained, transformer-based behavior foundation models (BFMs) to specific downstream tasks. The core idea is to train a small, task-specific "Task Encoder" using reinforcement learning to map task observations into a conditioning token. This learned token is then fed as an additional input to the original BFM, which remains frozen to preserve its rich motion priors. This hybrid approach combines reward-driven optimization with goal-based control, allowing for parameter-efficient and sample-efficient adaptation. The authors demonstrate experimentally that Task Tokens achieves high success rates on various humanoid control tasks, preserves the BFM's robustness to out-of-distribution perturbations, and maintains high-quality, human-like motion, while being compatible with other prompting modalities.2) Strengths
*   **Novel and Elegant Method for BFM Adaptation**
    *   The proposed method provides a clean and intuitive way to bridge reward-driven RL with large, pre-trained generative models for control. By learning a new input token instead of fine-tuning the model, it elegantly leverages the BFM's capabilities as a strong, structured motion prior (Section 3.1, Figure 1).
    *   The technical approach of backpropagating gradients from a task-specific RL objective through the frozen BFM to update only the Task Encoder is a sound and effective way to specialize behavior without catastrophic forgetting (Section 3.3).
    *   This approach creates a flexible framework that can seamlessly combine learned task-specific signals (Task Tokens) with user-defined behavioral priors like joint conditions or text prompts, as demonstrated in the multi-modal prompting experiments (Section 4.4, Figures 5, 6).*   **Comprehensive and Rigorous Experimental Evaluation**
    *   The paper evaluates the method on a diverse set of five challenging humanoid control tasks, including locomotion, reaching, and dynamic skills like striking and long jumping (Section 4, Tasks paragraph).
    *   The comparison includes a strong and well-chosen set of baselines, covering pure RL (PPO), full fine-tuning, the base BFM, and state-of-the-art hierarchical (PULSE) and discriminator-based (AMP) methods (Section 4, Baselines paragraph).
    *   The evaluation is multi-faceted, assessing not just task success (Table 1), but also sample efficiency (Figure 3), robustness to out-of-distribution physics changes (Figure 4), and motion quality via a human study (Table 2), providing a holistic view of the method's performance.*   **High Parameter and Sample Efficiency**
    *   The method is highly parameter-efficient, requiring the training of only a small Task Encoder (~200k parameters) for each new task, which is significantly smaller than baselines like PULSE (9.3M) or fine-tuning the entire BFM (25M) (Section 4.1). This makes the approach scalable for adapting a single BFM to many tasks.
    *   The convergence curves show that Task Tokens is substantially more sample-efficient than strong baselines. For the Strike task, it converges to a high success rate in approximately 50 million steps, whereas PULSE requires around 300 million steps to reach similar performance (Figure 3).*   **Effective Preservation of Foundation Model Properties**
    *   By keeping the BFM frozen, the method successfully retains key properties of the foundation model. The paper reports that Task Tokens maintains a high success rate under significant variations in gravity and friction, outperforming baselines in these OOD scenarios (Section 4.2, Figure 4).
    *   The human study confirms that the generated motions remain more natural and human-like than methods that alter the BFM's weights. Task Tokens is rated as significantly more human-like than fine-tuning or methods trained from scratch like PPO (Section 4.3, Table 2).3) Weaknesses
*   **Performance Gap on More Complex Tasks**
    *   While Task Tokens performs competitively across most tasks, it is outperformed in final success rate on the "Strike" task by both the PULSE baseline and the MaskedMimic Fine-Tune variant (76.61% vs. 83.18% and 83.07% respectively, Table 1).
    *   The Long Jump task also shows a large performance gap between Task Tokens (99.75%) and MaskedMimic Fine-Tune (47.36%), but the extremely high variance for the fine-tuning result (±54.78) makes this comparison difficult to interpret and suggests instability in the fine-tuning baseline for this specific task (Table 1).
    *   This suggests that for tasks requiring behaviors that deviate more significantly from the BFM's pre-trained motion distribution, simply conditioning via an input token may be insufficient compared to methods that can learn new skills (PULSE) or modify the BFM's weights (Fine-Tune).*   **Lower Perceived Human-Likeness Compared to a Key Baseline**
    *   The human study, while demonstrating the superiority of Task Tokens over fine-tuning and pure RL, reveals a notable weakness: the PULSE baseline was consistently rated as more human-like across all five tasks (Table 2).
    *   For example, in the Direction task, Task Tokens only won 14.80% of the head-to-head comparisons against PULSE, and the results were similar for Strike (24.19%) and Long Jump (39.13%) (Table 2).
    *   The paper acknowledges this finding with a single sentence (Section 4.3, last paragraph) but does not provide any analysis or hypothesis for why PULSE, despite being less sample-efficient, produces motions perceived as more natural.*   **Insufficient Detail on Task Encoder Inputs**
    *   The paper describes the Task Encoder's input $g_t^i$ as "observations that define the current task goal" (Section 3.2). It provides a clear example for the Steering task but lacks equivalent detail for the other four tasks.
    *   The specific composition of the input vector for tasks like Strike (e.g., target position, velocity?) and Long Jump is not specified in the main text.
    *   Appendix B mentions that "current positions of the head and pelvis joints" were concatenated to the input, but a systematic, per-task breakdown of the goal observation vector $g_t^i$ is missing, which hinders clarity and reproducibility.*   **Inconsistent and Unverifiable Experimental Reporting**
    *   A key claim regarding out-of-distribution (OOD) generalization is not verifiable from the provided evidence. The plots in Figure 4 show a clear best-performing method (red line), but the figure caption does not contain a color legend, making it impossible to identify which method corresponds to which line (Section 4.2).
    *   There are numerical inconsistencies in the reported results for the same experiment. For the Steering task, the main method (`Task Tokens (ours) + J.C.`) is reported with a success rate of 88.69 ± 4.04% in Table 1 and Table 3, but as 87.77 ± 7.14% in Table 4.
    *   The exact configuration of the main method, "Task Tokens (ours)", is ambiguous. Table 1 presents a composite result that uses joint conditioning for some tasks but not others, while Table 3 reports results for a "Task Tokens (ours)" variant without J.C., leading to confusion about the default setup and its performance.4) Suggestions for Improvement
*   **Analyze and Discuss Performance on Complex Tasks**
    *   Please add a discussion in Section 4.1 analyzing the performance gap on the Strike task (Table 1). A hypothesis could be that the frozen BFM's prior is not well-suited for the specific striking motion required, and methods like PULSE (which learns a skill library) or fine-tuning (which modifies the prior) have an advantage.
    *   It would be beneficial to clarify the result for Long Jump, perhaps by discussing the instability of the fine-tuning baseline that led to the high variance reported in Table 1.
    *   Consider discussing the trade-offs. Is the slightly lower performance on Strike an acceptable price for the significant gains in sample efficiency, parameter efficiency, and robustness?*   **Investigate and Elaborate on Human-Likeness Discrepancy**
    *   Please expand the discussion regarding PULSE's superior human-likeness rating (Table 2). It would be valuable to hypothesize why this might be the case. For instance, does PULSE's hierarchical structure, which selects from a library of motion clips, lead to more temporally coherent or stylistically consistent behaviors compared to the token-guided generation of your method?
    *   A qualitative analysis could strengthen this section. For example, showing side-by-side video frames of failure modes or common motion artifacts from both Task Tokens and PULSE could provide concrete evidence for the quantitative results in Table 2.*   **Provide Clear Specification of Task Encoder Inputs**
    *   To improve clarity and reproducibility, please add a table either in the main text (e.g., in Section 4) or in Appendix A that explicitly defines the components and dimensionality of the Task Encoder's input vector $g_t^i$ for each of the five experimental tasks.
    *   This table should clearly list all variables provided to the encoder for Reach, Direction, Steering, Strike, and Long Jump.*   **Improve Clarity and Consistency of Experimental Reporting**
    *   Please correct the legend for Figure 4 to clearly associate each colored line with a specific method, allowing for verification of the OOD generalization claims.
    *   Please reconcile the conflicting success rates for the Steering task reported across Tables 1, 3, and 4. Ensure that all reported numbers for the same experimental condition are consistent throughout the manuscript.
    *   Please clarify the exact configuration being reported in each table, for instance by using more descriptive and consistent row labels (e.g., "Task Tokens (ours, no J.C.)") to avoid ambiguity.5) Score
*   Overall (10): 7 — The paper presents a novel and effective method, but its contributions are undermined by significant issues in experimental reporting, including inconsistent numerical results (Tables 1, 3, 4) and an uninterpretable figure supporting a key claim (Figure 4).
*   Novelty (10): 8 — The core idea of learning a task-specific input token for a frozen BFM via RL is a simple yet powerful contribution to the field of BFM adaptation (Section 3.1).
*   Technical Quality (10): 7 — The methodology is sound, but the technical quality is diminished by inconsistent reporting of key results across different tables (Tables 1, 3, 4) and a flawed figure (Figure 4) that makes a core claim unverifiable.
*   Clarity (10): 6 — The paper is generally well-written, but suffers from a lack of detail on task inputs (Section 3.2) and critical reporting issues, such as an uninterpretable figure legend (Figure 4) and ambiguous table entries (Table 1 vs Table 3).
*   Confidence (5): 5 — I am highly confident in my assessment, as the paper provides detailed results and appendices that support a thorough evaluation.