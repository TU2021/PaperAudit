# Global Summary
This paper introduces "Task Tokens," a method for adapting transformer-based behavior foundation models (BFMs) for specific humanoid control tasks without compromising their pre-trained knowledge. The core problem is that while BFMs like MaskedMimic generate robust, human-like motions, tailoring them to new tasks requires difficult prompt engineering and can yield suboptimal results. The proposed solution is to freeze the pre-trained BFM and train a small, task-specific "Task Encoder" using reinforcement learning (PPO). This encoder maps task observations to a new "Task Token," which is fed as an additional input to the BFM, guiding its behavior to optimize a task-specific reward. This creates a hybrid system balancing user-defined prompts (priors) and learned, reward-driven control.

The method is evaluated on five simulated humanoid tasks (Reach, Direction, Steering, Strike, Long Jump) using an SMPL character in Isaac Gym. Task Tokens achieves high success rates (e.g., 94.88% on Reach, 99.75% on Long Jump) and is highly parameter-efficient, adding only ~200k parameters per task. It converges significantly faster (~50M steps) than baselines like PULSE (~300M steps). A human study with ~100 participants found its motions to be substantially more human-like than fine-tuning the BFM (e.g., >84% win rate) or pure RL (>81% win rate), though less human-like than PULSE. The method also demonstrates superior robustness to out-of-distribution physics changes (friction, gravity). Stated limitations include reliance on the quality of the base BFM and being restricted to simulated environments.

# Abstract
The paper addresses the challenge of adapting transformer-based behavior foundation models (BFMs) for specific humanoid control tasks, which often requires difficult prompt engineering. The proposed method, "Task Tokens," tailors BFMs by learning a new task-specific encoder via reinforcement learning while keeping the original BFM frozen. This encoder maps observations to tokens that are used as additional inputs to the BFM, guiding it towards task-specific goals defined by a reward function. This approach balances user-defined priors (via prompt engineering) with reward-driven optimization. The method is shown to be effective across various tasks, including out-of-distribution scenarios, and is compatible with other prompting modalities, preserving the BFM's generalization capabilities and diverse control characteristics.

# Introduction
- The paper focuses on Goal-Conditioned Behavior Foundation Models (GC-BFMs), specifically MaskedMimic, which uses a transformer architecture to generate human-like motions from tokenized goals.
- A key challenge is that crafting precise goal prompts for complex tasks is difficult. Some task aspects are better specified by goals (e.g., locomotion direction), while others are better defined by rewards (e.g., striking a target with force).
- The proposed solution, "Task Tokens," integrates goal-based control with reward-driven optimization. It allows users to provide high-level priors via goals while the system learns task-specific embeddings (Task Tokens) to optimize dense rewards.
- A Task Token encoder is trained with reinforcement learning to map task observations to supplementary goal tokens. The pre-trained BFM's weights are frozen to preserve its motion knowledge.
- The authors claim this hybrid approach achieves rapid convergence, high success rates, and better sample efficiency than traditional hierarchical RL, while also showing strong generalization to environmental changes (e.g., friction, gravity).

# Related Work
- The work is situated in the context of humanoid control, which uses physics simulation and imitation learning to generate realistic behaviors in animation and robotics.
- It discusses Behavior Foundation Models (BFMs) that generate diverse behaviors. It contrasts methods like PSM, ASE, and PULSE (which often use hierarchical controllers) with Goal-Conditioned BFMs (GC-BFMs).
- GC-BFMs, like MaskedMimic, can solve new tasks in a zero-shot manner by mapping goals to actions. However, they struggle with out-of-distribution constraints.
- Task Tokens is proposed to address this limitation by enabling task-specific optimization while preserving the natural motion priors of the GC-BFM.

# Preliminaries
- The method builds on standard Reinforcement Learning, modeled as a Markov Decision Process (MDP).
- The core foundation model used is MaskedMimic, a unified framework for humanoid control based on goal-conditioned reinforcement learning (GCRL) and imitation learning.
- MaskedMimic uses a transformer architecture and learns from demonstration data via online distillation (DAgger). It processes tokenized goals (e.g., future joint positions, text) and uses random masking during training to generalize to new objectives without extra training.

# Method
- The core idea is to leverage MaskedMimic's transformer architecture to integrate new task-specific information via additional tokens without modifying the base model.
- A dedicated "Task Encoder" is trained to produce a specialized "Task Token" for each new task. This token encapsulates the task's specific requirements.
- The Task Encoder is a feed-forward neural network that maps task goal observations ($g_t^i$) to a Task Token ($\tau_t^i \in \mathbb{R}^{512}$). The input also includes proprioceptive information to align with the BFM's pre-trained representations.
- The generated Task Token is concatenated with other input tokens (e.g., state token, user-defined prior tokens) to form a "token sentence" for the BFM.
- The Task Encoder is trained using Proximal Policy Optimization (PPO). The BFM remains frozen, and gradients from the PPO objective flow back through it to update only the Task Encoder's parameters. This preserves the BFM's knowledge while adapting its behavior.

# Experiments
- **Setup**: Experiments use the SMPL humanoid (69 DoF) in the Isaac Gym simulator.
- **Tasks**: Five tasks are evaluated: Reach, Direction, Steering, Strike, and Long Jump.
- **Baselines**: Pure RL (PPO), MaskedMimic Fine-Tune, MaskedMimic (J.C. only), PULSE, and AMP.
- **Parameter Efficiency**: Task Tokens adds only ~200k parameters per task, which is 46.5x fewer than PULSE (9.3M) and 125x fewer than MaskedMimic Fine-Tune (25M).

- **Task Adaptation (Section 4.1)**:
    - Task Tokens achieves high success rates across tasks: Reach (94.88 ± 1.99%), Direction (99.26 ± 0.79%), Steering (88.69 ± 4.04%), Long Jump (99.75 ± 0.57%), and Strike (76.61 ± 3.49%).
    - On the Strike task, PULSE (83.18%) and MaskedMimic Fine-Tune (83.07%) achieve higher success rates.
    - Task Tokens converges much faster than baselines, reaching high performance on the Strike task in ~50 million steps, whereas PULSE requires ~300 million steps.

- **OOD Generalization (Section 4.2)**:
    - On the Steering task, robustness was tested against unseen changes in ground friction and gravity.
    - Task Tokens demonstrates superior robustness, maintaining higher success rates under perturbation. It shows an "order of magnitude higher rate of success" in very low friction (e.g., 0.4x) and high gravity (e.g., 1.5x) scenarios compared to baselines.

- **Human Study (Section 4.3)**:
    - A study with ~100 participants compared the human-likeness of generated motions.
    - Task Tokens was preferred over MaskedMimic Fine-Tune in 84-99% of comparisons and over PPO in 81-99% of comparisons.
    - However, PULSE was consistently rated as more human-like than Task Tokens. For example, Task Tokens only won 14.80% of matchups against PULSE on the Direction task and 24.19% on the Strike task.

- **Multi-modal Prompting (Section 4.4)**:
    - The paper demonstrates that Task Tokens can be combined with user-defined priors (e.g., joint conditioning, text prompts).
    - In the Direction task, an orientation prior was used to prevent the agent from learning to walk backward.
    - In the Strike task, an orientation prior and a text prompt ("a person performs a kick") were used to guide the agent to face the target and kick it, avoiding an unnatural "whirlwind" motion.

# Conclusion
- The paper summarizes its contribution as Task Tokens, a hybrid control method that integrates goal-based control and reward-driven optimization for GC-BFMs.
- The method is shown to achieve rapid convergence, high success rates, and strong generalization while being parameter-efficient.
- Human studies confirm that the generated motions are more human-like than those from fine-tuning or pure RL.
- The paper acknowledges limitations, including the method's dependence on the quality of the underlying BFM and its evaluation being confined to simulated environments.

# References
This section lists the academic papers and works cited throughout the manuscript, including foundational works on reinforcement learning (PPO, MDPs), humanoid control (DeepMimic, AMP), and behavior foundation models (MaskedMimic, PULSE, Decision Transformer).

# Appendix
- **A Environments Technical Details**:
    - Provides specifics on the five tasks and their success criteria.
    - Reach: right hand within 20 cm of target.
    - Direction: speed deviation < 20% from target.
    - Steering: speed deviation < 20% and facing deviation < 45°.
    - Strike: target is knocked over.
    - Long Jump: jump distance > 1.5 meters.
    - Controller operates at 30 Hz; simulation at 120 Hz.

- **B Training and Evaluation Details**:
    - Training was done with 1024 parallel environments for 120M frames.
    - The Task Encoder is an MLP of size [512, 512, 512]. The critic is an MLP of size [1024, 1024, 1024].
    - The Task Encoder's input includes current head and pelvis positions.
    - All results are reported as mean and standard deviation over 5 seeds, using the last model checkpoint.

- **C Ablation Study**:
    - Table 3 shows that combining Task Tokens with Joint Conditioning (J.C.) improves performance on relevant tasks (e.g., Steering success rate from 83.66% to 88.69%).
    - Table 4 ablates the Task Encoder architecture (MLP size, use of current pose). Performance is sensitive to these choices; for Steering, success rates vary from 66.31% to 87.77% depending on the configuration.

- **D Human Study Technical Details**:
    - The study used Google Forms with 96 participants across 3 forms.
    - Each question presented three side-by-side videos (A, B, C), with one always being from Task Tokens. Participants chose the most human-like motion.
    - The win rate was calculated as (# Task Tokens chosen) / (# Task Tokens chosen + # Algorithm A chosen).