{
  "baseline_review": "Summary\n- The paper proposes Task Tokens, an approach for adapting goal-conditioned behavior foundation models (GC-BFMs), specifically MaskedMimic, to downstream control tasks by learning a small task-specific encoder via reinforcement learning while freezing the BFM. The encoder maps task observations to a 512-d token that is concatenated with existing BFM tokens (state and optional user priors) and fed to the transformer (Section 3.1–3.3; Figure 1; Section 3.2). Training uses PPO to optimize the encoder with gradients flowing through the frozen BFM (Section 3.3; Figure 1 caption). Experiments on five humanoid tasks in Isaac Gym show high success rates (Table 1), fast convergence on Strike (Figure 3), improved robustness to friction/gravity perturbations on Steering (Figure 4), strong parameter efficiency (~200k/task; Section 4.1), and favorable human-likeness against several baselines (Table 2). The method also supports multi-modal prompting with joint and text priors (Figures 5–6). Limitations include reliance on the underlying BFM quality and focus on simulation (Conclusion).Strengths\n- Bolded Title: **Clear, modular method design leveraging GC-BFM tokenization**\n  - Evidence: The approach concatenates a learned Task Token with Prior and State tokens, preserving the frozen GC-BFM architecture (Section 3.1–3.2; Figure 1; Figure 1 caption). Why it matters: This modularity enables plug-in adaptation without risking catastrophic forgetting, supporting technical soundness and practical impact.\n  - Evidence: Task Encoder outputs a 512-d embedding from egocentric task observations and proprioception (Section 3.2). Why it matters: A compact interface makes the approach parameter-efficient and applicable across tasks (novelty and practicality).\n  - Evidence: PPO optimizes only the encoder while gradients flow through the frozen BFM (Section 3.3; Figure 1 caption). Why it matters: Freezing the foundation model preserves learned motion priors and stability, supporting robustness.- Bolded Title: **Strong performance across diverse humanoid tasks**\n  - Evidence: Task Tokens achieve top or near-top success rates on Reach (94.88±1.99%), Direction (99.26±0.79%), Steering (88.69±4.04%), Long Jump (99.75±0.57%), and competitive results on Strike (76.61±3.49%) (Table 1). Why it matters: Demonstrates broad applicability and task coverage (experimental rigor and impact).\n  - Evidence: Convergence curves on Strike show adaptation in under ~50M steps, with PULSE requiring ~300M steps for similar performance (Figure 3). Why it matters: Indicates sample efficiency and training effectiveness (technical quality).\n  - Evidence: Ablations confirm benefits of joint conditioning when available (Table 3) and explore encoder design choices (Table 4). Why it matters: Strengthens claims with controlled analyses (experimental rigor).- Bolded Title: **Parameter efficiency and minimal footprint per task**\n  - Evidence: The encoder uses ~200k parameters per task, compared to ~9.3M for PULSE and ~25M for MaskedMimic fine-tune (Section 4.1). Why it matters: Lower memory and compute requirements facilitate deployment and scalability (impact).- Bolded Title: **Robustness to out-of-distribution perturbations**\n  - Evidence: On Steering, Task Tokens (with and without joint conditioning) outperform baselines across varying friction and gravity multipliers, with large gains at low friction (~0.4) and high gravity (~1.5) (Figure 4; Figures 27–28 provide detailed curves). Why it matters: Preserving robustness under perturbations is critical for practical control (technical soundness and impact).\n  - Evidence: The method leverages the BFM’s pretraining on diverse data to retain robustness characteristics (Section 4.2). Why it matters: Shows that freezing the BFM helps maintain generalization while adapting to new tasks (clarity and soundness).- Bolded Title: **Human-likeness validated via user study**\n  - Evidence: Task Tokens win against MaskedMimic fine-tune, AMP, and PPO in human-likeness across multiple tasks (e.g., 99.48% vs fine-tune on Direction; 90.40% on Steering) (Table 2). Why it matters: Supports the claim that freezing BFMs preserves natural motion (impact and experimental support).\n  - Evidence: Study includes ~96 participants evaluating 40 triplets across tasks (Appendix D). Why it matters: Provides nontrivial empirical validation (experimental rigor).\n  - Evidence: Motion quality benefit is a stated motivation for freezing (Section 4.3). Why it matters: Aligns design choice with outcomes (technical soundness).- Bolded Title: **Multi-modal prompting compatibility**\n  - Evidence: Combining Task Tokens with joint conditioning and text prompts (e.g., “a person performs a kick”) resolves undesirable behaviors (e.g., backward walking) and directs motion style in Direction and Strike (Figures 5–6). Why it matters: Demonstrates flexibility to integrate human priors for more desirable behaviors (novelty and impact).- Bolded Title: **Transparent limitations and scope**\n  - Evidence: The paper acknowledges reliance on BFM quality and current restriction to simulation (Conclusion). Why it matters: Increases credibility and helps future work planning (clarity).Weaknesses\n- Bolded Title: **Methodological specificity is insufficient for full reproducibility**\n  - Evidence: The PPO objective is described qualitatively (“compute the PPO objective with respect to the task-specific reward and the BFM’s action probabilities”) without explicit loss formulas, action distribution types, or clipping/entropy settings (Section 3.3). Why it matters: Missing optimization details hinder reproducibility and technical scrutiny.\n  - Evidence: Token concatenation order, positional encoding scheme, and attention masking for mixed tokens (“sentence”) are not specified beyond “concatenation” (Section 3.2). Why it matters: Transformer behavior can be sensitive to token ordering and positional encodings; lacking details limits clarity and replicability.\n  - Evidence: Reward functions are invoked (dense rewards) but not specified; Appendix A provides success criteria only, not full reward shaping/weights (Appendix A; Section 4). Why it matters: Reward design critically affects PPO training and behaviors; missing definitions reduce interpretability of results.- Bolded Title: **Evaluation fairness and scope are limited**\n  - Evidence: PULSE is trained on 128 environments vs 1024 for Task Tokens (Appendix B), potentially affecting sample throughput and learning dynamics. Why it matters: Unequal parallelization can bias convergence and efficiency comparisons.\n  - Evidence: Sample efficiency is demonstrated only on Strike (Figure 3) and not across other tasks. Why it matters: Lacking multi-task convergence curves weakens general claims about sample efficiency.\n  - Evidence: Wall-clock times and compute resources are not reported (Appendix B mentions frames and seeds but no timing). Why it matters: Practical efficiency and deployment decisions rely on time and hardware, not only environment steps.- Bolded Title: **OOD generalization evaluation is narrow**\n  - Evidence: OOD tests cover only Steering, varying friction and gravity (Section 4.2; Figure 4). Why it matters: Robustness claims are not validated across other tasks (e.g., Reach, Long Jump).\n  - Evidence: Perturbations are limited to two physics parameters; other realistic shifts (terrain roughness, sensor noise, object mass changes) are not considered (Section 4.2). Why it matters: Narrow perturbations limit generalization claims.\n  - Evidence: Distributional changes in goals or observation noise are not evaluated (No direct evidence found in the manuscript). Why it matters: Goal/observation shifts are common in real applications; missing tests reduce completeness.- Bolded Title: **Human study design leaves room for bias and lacks statistical analysis**\n  - Evidence: Every triplet included Task Tokens (“We ensured that Task Tokens was presented every time”) (Appendix D). Why it matters: Always including Task Tokens may anchor participants or affect perceived motion diversity, introducing bias.\n  - Evidence: No statistical significance tests or confidence intervals are reported for win rates (Section 4.3; Table 2; Appendix D). Why it matters: Without statistical analysis, it is hard to assess reliability and the strength of conclusions.\n  - Evidence: Participant demographics and expertise are not provided; only counts (96 participants) are listed (Appendix D). Why it matters: Demographics and expertise can influence judgments; missing data limit interpretability.- Bolded Title: **Limited quantitative motion-quality metrics beyond human study**\n  - Evidence: No kinematic or contact metrics (e.g., foot sliding distance, joint jerk, COM smoothness) are reported (No direct evidence found in the manuscript). Why it matters: Objective motion-quality measures complement subjective studies and strengthen technical claims.\n  - Evidence: No reporting of physics consistency (e.g., penetration, contact impulse distributions) (No direct evidence found in the manuscript). Why it matters: Physics realism is critical for control; quantitative checks increase rigor.\n  - Evidence: Failure mode analysis (fall rates, excessive torso bending, limb self-collisions) is not provided (No direct evidence found in the manuscript). Why it matters: Understanding failure patterns guides method design and practical use.- Bolded Title: **Real-world applicability and dependence on BFM quality not empirically assessed**\n  - Evidence: Experiments are only in simulation using SMPL in Isaac Gym (Section 4; Appendix A). Why it matters: Without sim-to-real or hardware tests, practical deployment remains unverified.\n  - Evidence: The Conclusion acknowledges reliance on underlying BFM quality (Conclusion). Why it matters: No experiments vary the BFM to assess sensitivity or portability (No direct evidence found in the manuscript).\n  - Evidence: No domain randomization or sensor/actuator noise tests are reported to bridge sim-to-real gaps (No direct evidence found in the manuscript). Why it matters: Such tests are standard precursors to real-world validation.Suggestions for Improvement\n- Bolded Title: **Provide complete methodological details for reproducibility**\n  - Include a formal PPO objective with all terms (e.g., clipped surrogate loss, entropy coefficient, advantage estimation) and specify action distributions used by the frozen BFM (Section 3.3); add exact hyperparameters in Appendix B.\n  - Document token concatenation order, positional encoding or learned time embeddings, and any attention masks across Prior/Task/State tokens; add a small ablation on token order and positional encoding choices (Section 3.2).\n  - Publish full reward functions per task (component terms, weights, normalization) alongside success criteria (Appendix A), enabling independent reproduction and sensitivity analyses.- Bolded Title: **Strengthen evaluation fairness and broaden efficiency reporting**\n  - Match parallelization across methods (e.g., re-run PULSE with 1024 environments or report per-step throughput) and report compute resources (GPUs, batch sizes) to contextualize comparisons (Appendix B).\n  - Provide convergence curves (success and return) for all tasks, not only Strike, and report per-task sample budgets (Figures 3; Section 4.1), clarifying consistent stopping criteria.\n  - Report wall-clock training times and total compute (e.g., GPU-hours) for all methods to complement environment-step metrics (Appendix B), enabling practical efficiency assessment.- Bolded Title: **Expand OOD robustness evaluation**\n  - Evaluate friction/gravity perturbations across all tasks (Reach, Direction, Long Jump, Strike) to generalize robustness claims (Section 4.2).\n  - Add perturbations such as terrain roughness, external pushes, sensor noise, and object mass/geometry changes to stress-test policies (Section 4.2), quantifying performance drop.\n  - Include distribution shifts in goals (e.g., extreme speeds or orientations) and observation noise to test policy sensitivity, reporting success and stability metrics (No direct evidence found in the manuscript).- Bolded Title: **Improve human study rigor and mitigate bias**\n  - Report statistical significance (e.g., binomial tests or bootstrap CIs) for win rates in Table 2 and aggregate across forms (Section 4.3; Appendix D).\n  - Use balanced pairings (e.g., pairwise or tournament designs) that do not always include Task Tokens, and randomize algorithm presentation to reduce anchoring (Appendix D).\n  - Provide participant demographics (age, domain expertise) and pre-registered analysis plans; report inter-rater reliability and include sanity checks (Appendix D).- Bolded Title: **Add objective motion-quality and physics-consistency metrics**\n  - Report kinematic metrics such as foot sliding (cm/s), joint jerk/acceleration statistics, and COM smoothness across tasks to complement human judgments (No direct evidence found in the manuscript).\n  - Include physics metrics (penetration depth, contact impulse distributions, energy consumption) and constraints satisfaction (e.g., torque limits) to assess realism (No direct evidence found in the manuscript).\n  - Provide failure mode statistics (fall rates, self-collisions, near-singular postures) and qualitative analysis, linking them to token configurations or rewards (No direct evidence found in the manuscript).- Bolded Title: **Assess real-world transfer and BFM dependence**\n  - Conduct sim-to-real or hardware-in-the-loop experiments (even if small-scale) or, at minimum, domain-randomized sim tests with sensor/actuator noise to probe real-world applicability (Section 4; Conclusion).\n  - Evaluate sensitivity to the choice of BFM by testing with an alternative GC-BFM referenced in the paper (e.g., Masked Trajectory Models; References: Wu et al., 2023), documenting portability and performance differences (Conclusion).\n  - Add experiments with extensive domain randomization and partial observability to measure robustness to real-world uncertainties (No direct evidence found in the manuscript).Score\n- Overall (10): 7 — Compelling modular method with strong results (Table 1), sample-efficient convergence on Strike (Figure 3), OOD robustness on Steering (Figure 4), and parameter efficiency (Section 4.1), but limited methodological detail (Section 3.3) and narrow robustness/human-study analyses (Section 4.2; Table 2; Appendix D).\n- Novelty (10): 7 — Learning a task-specific token encoder via PPO while freezing a GC-BFM (Sections 3.1–3.3; Figure 1) offers a clean integration of reward-driven adaptation into tokenized goal conditioning, with multi-modal prompting demonstrations (Figures 5–6).\n- Technical Quality (10): 6 — Solid empirical performance (Table 1; Figure 3; Figure 4) and ablations (Tables 3–4), yet missing optimization and reward details (Section 3.3; Appendix A) and limited fairness/robustness breadth (Appendix B; Section 4.2).\n- Clarity (10): 6 — High-level method and visuals are clear (Sections 3.1–3.2; Figure 1), but absent formulas, tokenization specifics, and reward definitions reduce reproducibility (Section 3.3; Appendix A).\n- Confidence (5): 4 — The assessment is grounded in provided results and appendices, but the lack of technical specifics and limited scope of some evaluations temper certainty (Section 3.3; Appendix A–B; Figure 4; Table 2).",
  "final_review": "Summary\n- The paper proposes Task Tokens, an approach for adapting goal-conditioned behavior foundation models (GC-BFMs), specifically MaskedMimic, to downstream control tasks by learning a small task-specific encoder via reinforcement learning while freezing the BFM. The encoder maps task observations to a 512-d token that is concatenated with existing BFM tokens (state and optional user priors) and fed to the transformer (Section 3.1–3.3; Figure 1; Section 3.2). Training uses PPO to optimize the encoder with gradients flowing through the frozen BFM (Section 3.3; Figure 1 caption). Experiments on five humanoid tasks in Isaac Gym show high success rates (Table 1), fast convergence on Strike (Figure 3), improved robustness to friction/gravity perturbations on Steering (Figure 4), strong parameter efficiency (~200k/task; Section 4.1), and favorable human-likeness against several baselines (Table 2). The method also supports multi-modal prompting with joint and text priors (Figures 5–6). Limitations include reliance on the underlying BFM quality and focus on simulation (Conclusion).Strengths\n- Bolded Title: **Clear, modular method design leveraging GC-BFM tokenization**\n  - Evidence: The approach concatenates a learned Task Token with Prior and State tokens, preserving the frozen GC-BFM architecture (Section 3.1–3.2; Figure 1; Figure 1 caption). Why it matters: This modularity enables plug-in adaptation without risking catastrophic forgetting, supporting technical soundness and practical impact.\n  - Evidence: Task Encoder outputs a 512-d embedding from egocentric task observations and proprioception (Section 3.2). Why it matters: A compact interface makes the approach parameter-efficient and applicable across tasks (novelty and practicality).\n  - Evidence: PPO optimizes only the encoder while gradients flow through the frozen BFM (Section 3.3; Figure 1 caption). Why it matters: Freezing the foundation model preserves learned motion priors and stability, supporting robustness.\n- Bolded Title: **Strong performance across diverse humanoid tasks**\n  - Evidence: Task Tokens achieve top or near-top success rates on Reach (94.88±1.99%), Direction (99.26±0.79%), Steering (88.69±4.04%), Long Jump (99.75±0.57%), and competitive results on Strike (76.61±3.49%) (Table 1). Why it matters: Demonstrates broad applicability and task coverage (experimental rigor and impact).\n  - Evidence: Convergence curves on Strike show adaptation in under ~50M steps, with PULSE requiring ~300M steps for similar performance (Figure 3). Why it matters: Indicates sample efficiency and training effectiveness (technical quality).\n  - Evidence: Ablations confirm benefits of joint conditioning when available (Table 3) and explore encoder design choices (Table 4). Why it matters: Strengthens claims with controlled analyses (experimental rigor).\n- Bolded Title: **Parameter efficiency and minimal footprint per task**\n  - Evidence: The encoder uses ~200k parameters per task, compared to ~9.3M for PULSE and ~25M for MaskedMimic fine-tune (Section 4.1). Why it matters: Lower memory and compute requirements facilitate deployment and scalability (impact).\n- Bolded Title: **Robustness to out-of-distribution perturbations**\n  - Evidence: On Steering, Task Tokens (with and without joint conditioning) outperform baselines across varying friction and gravity multipliers, with large gains at low friction (~0.4) and high gravity (~1.5) (Figure 4). Why it matters: Preserving robustness under perturbations is critical for practical control (technical soundness and impact).\n  - Evidence: The method leverages the BFM’s pretraining on diverse data to retain robustness characteristics (Section 4.2). Why it matters: Shows that freezing the BFM helps maintain generalization while adapting to new tasks (clarity and soundness).\n- Bolded Title: **Human-likeness validated via user study**\n  - Evidence: Task Tokens win against MaskedMimic fine-tune, AMP, and PPO in human-likeness across multiple tasks (e.g., 99.48% vs fine-tune on Direction; 90.40% on Steering) (Table 2). Why it matters: Supports the claim that freezing BFMs preserves natural motion (impact and experimental support).\n  - Evidence: Study includes ~96 participants evaluating 40 triplets across tasks (Appendix D). Why it matters: Provides nontrivial empirical validation (experimental rigor).\n  - Evidence: Motion quality benefit is a stated motivation for freezing (Section 4.3). Why it matters: Aligns design choice with outcomes (technical soundness).\n- Bolded Title: **Multi-modal prompting compatibility**\n  - Evidence: Combining Task Tokens with joint conditioning and text prompts (e.g., “a person performs a kick”) resolves undesirable behaviors (e.g., backward walking) and directs motion style in Direction and Strike (Figures 5–6). Why it matters: Demonstrates flexibility to integrate human priors for more desirable behaviors (novelty and impact).\n- Bolded Title: **Transparent limitations and scope**\n  - Evidence: The paper acknowledges reliance on BFM quality and current restriction to simulation (Conclusion). Why it matters: Increases credibility and helps future work planning (clarity).Weaknesses\n- Bolded Title: **Methodological specificity is insufficient for full reproducibility**\n  - Evidence: The PPO objective is described qualitatively (“compute the PPO objective with respect to the task-specific reward and the BFM’s action probabilities”) without explicit loss formulas, action distribution types, or clipping/entropy settings (Section 3.3). Why it matters: Missing optimization details hinder reproducibility and technical scrutiny.\n  - Evidence: Token concatenation order, positional encoding scheme, and attention masking for mixed tokens (“sentence”) are not specified beyond “concatenation” (Section 3.2). Why it matters: Transformer behavior can be sensitive to token ordering and positional encodings; lacking details limits clarity and replicability.\n  - Evidence: Reward functions are invoked (dense rewards) but not specified; Appendix A provides success criteria only, not full reward shaping/weights (Appendix A; Section 4). Why it matters: Reward design critically affects PPO training and behaviors; missing definitions reduce interpretability of results.\n  - Evidence: The reported parameter count (~200k) for the task encoder (Section 4.1) is not reconciled with the “Bigger MLP [512,512,512]” used in main results (Appendix B), which typically implies a larger parameter footprint. Why it matters: Ambiguity on actual parameterization affects claims of parameter efficiency and reproducibility.\n  - Evidence: The manuscript states that joint conditioning (J.C.) is not available for Strike and Long Jump in the main results (Section 4, text after Figure 3), yet Section 4.4 and Figure 6 qualitatively demonstrate Strike with joint and text priors. Why it matters: Clarity on which priors are used in quantitative vs qualitative settings is necessary for consistent interpretation.\n  - Evidence: A sentence in Section 3.3 appears truncated (“This design choice is fundamental—while fine-tuning the entire model might yield …”; Section 3.3), leaving the methodological justification incomplete. Why it matters: Editorial gaps reduce clarity on key design choices.\n- Bolded Title: **Evaluation fairness and scope are limited**\n  - Evidence: PULSE is trained on 128 environments vs 1024 for Task Tokens (Appendix B), potentially affecting sample throughput and learning dynamics. Why it matters: Unequal parallelization can bias convergence and efficiency comparisons.\n  - Evidence: Sample efficiency is demonstrated only on Strike (Figure 3) and not across other tasks. Why it matters: Lacking multi-task convergence curves weakens general claims about sample efficiency.\n  - Evidence: Wall-clock times and compute resources are not reported (Appendix B mentions frames and seeds but no timing). Why it matters: Practical efficiency and deployment decisions rely on time and hardware, not only environment steps.\n  - Evidence: Figure 3 shows convergence up to 350M steps and mentions PULSE reaching similar performance around 300M steps (Section 4.1; Figure 3), whereas Appendix B states training used 120M frames for all methods. Why it matters: Discrepancy between steps shown and stated budgets obscures the fairness of sample-efficiency comparisons.\n  - Evidence: Training and evaluation use different episode termination rules (Appendix B), which can change success metrics. Why it matters: Inconsistent termination criteria can confound cross-method comparisons unless uniformly applied and reported.\n- Bolded Title: **OOD generalization evaluation is narrow**\n  - Evidence: OOD tests cover only Steering, varying friction and gravity (Section 4.2; Figure 4). Why it matters: Robustness claims are not validated across other tasks (e.g., Reach, Long Jump).\n  - Evidence: Perturbations are limited to two physics parameters; other realistic shifts (terrain roughness, sensor noise, object mass changes) are not considered (Section 4.2). Why it matters: Narrow perturbations limit generalization claims.\n  - Evidence: Distributional changes in goals or observation noise are not evaluated (No direct evidence found in the manuscript). Why it matters: Goal/observation shifts are common in real applications; missing tests reduce completeness.\n  - Evidence: The text claims “order of magnitude” higher success at low friction and high gravity (Section 4.2), but Figure 4 provides only qualitative curves without numerical values supporting a ≥10× gap. Why it matters: Overstated claims reduce interpretability and require quantification.\n- Bolded Title: **Human study design leaves room for bias and lacks statistical analysis**\n  - Evidence: Every triplet included Task Tokens (“We ensured that Task Tokens was presented every time”) (Appendix D). Why it matters: Always including Task Tokens may anchor participants or affect perceived motion diversity, introducing bias.\n  - Evidence: No statistical significance tests or confidence intervals are reported for win rates (Section 4.3; Table 2; Appendix D). Why it matters: Without statistical analysis, it is hard to assess reliability and the strength of conclusions.\n  - Evidence: Participant demographics and expertise are not provided; only counts (96 participants) are listed (Appendix D). Why it matters: Demographics and expertise can influence judgments; missing data limit interpretability.\n  - Evidence: The Conclusion states “Human studies confirm that Task Tokens generate more human-like movements” (Conclusion), while the paper itself acknowledges PULSE scores higher on human-likeness (Section 4.3; Table 2). Why it matters: Inconsistent summary language can mislead readers about comparative motion quality.\n- Bolded Title: **Limited quantitative motion-quality metrics beyond human study**\n  - Evidence: No kinematic or contact metrics (e.g., foot sliding distance, joint jerk, COM smoothness) are reported (No direct evidence found in the manuscript). Why it matters: Objective motion-quality measures complement subjective studies and strengthen technical claims.\n  - Evidence: No reporting of physics consistency (e.g., penetration, contact impulse distributions) (No direct evidence found in the manuscript). Why it matters: Physics realism is critical for control; quantitative checks increase rigor.\n  - Evidence: Failure mode analysis (fall rates, excessive torso bending, limb self-collisions) is not provided (No direct evidence found in the manuscript). Why it matters: Understanding failure patterns guides method design and practical use.\n- Bolded Title: **Real-world applicability and dependence on BFM quality not empirically assessed**\n  - Evidence: Experiments are only in simulation using SMPL in Isaac Gym (Section 4; Appendix A). Why it matters: Without sim-to-real or hardware tests, practical deployment remains unverified.\n  - Evidence: The Conclusion acknowledges reliance on underlying BFM quality (Conclusion). Why it matters: No experiments vary the BFM to assess sensitivity or portability (No direct evidence found in the manuscript).\n  - Evidence: No domain randomization or sensor/actuator noise tests are reported to bridge sim-to-real gaps (No direct evidence found in the manuscript). Why it matters: Such tests are standard precursors to real-world validation.Suggestions for Improvement\n- Bolded Title: **Provide complete methodological details for reproducibility**\n  - Include a formal PPO objective with all terms (e.g., clipped surrogate loss, entropy coefficient, advantage estimation) and specify action distributions used by the frozen BFM (Section 3.3); add exact hyperparameters in Appendix B.\n  - Document token concatenation order, positional encoding or learned time embeddings, and any attention masks across Prior/Task/State tokens; add a small ablation on token order and positional encoding choices (Section 3.2).\n  - Publish full reward functions per task (component terms, weights, normalization) alongside success criteria (Appendix A), enabling independent reproduction and sensitivity analyses.\n  - Report exact parameter counts for the task encoder configuration used in main results, clarifying the “~200k” statement (Section 4.1) vs the “Bigger MLP [512,512,512]” in Appendix B; specify whether the critic is included.\n  - Clearly delineate which priors (e.g., J.C., text) are used for quantitative results versus qualitative demos, especially for Strike/Long Jump (Section 4; Section 4.4; Figure 6).\n  - Fix the truncated sentence in Section 3.3 and ensure uninterrupted, explicit justification for freezing the BFM versus full fine-tuning (Section 3.3).\n- Bolded Title: **Strengthen evaluation fairness and broaden efficiency reporting**\n  - Match parallelization across methods (e.g., re-run PULSE with 1024 environments or report per-step throughput) and report compute resources (GPUs, batch sizes) to contextualize comparisons (Appendix B).\n  - Provide convergence curves (success and return) for all tasks, not only Strike, and report per-task sample budgets (Figures 3; Section 4.1), clarifying consistent stopping criteria.\n  - Report wall-clock training times and total compute (e.g., GPU-hours) for all methods to complement environment-step metrics (Appendix B), enabling practical efficiency assessment.\n  - Reconcile the steps shown in Figure 3 (up to 350M, with PULSE around 300M; Section 4.1) with the stated 120M-frame budget (Appendix B); define “step” vs “frame” and list exact budgets per method for the plotted curves.\n  - Standardize and document episode termination criteria across training/evaluation (Appendix B), and confirm identical evaluation rules for all methods.\n- Bolded Title: **Expand OOD robustness evaluation**\n  - Evaluate friction/gravity perturbations across all tasks (Reach, Direction, Long Jump, Strike) to generalize robustness claims (Section 4.2).\n  - Add perturbations such as terrain roughness, external pushes, sensor noise, and object mass/geometry changes to stress-test policies (Section 4.2), quantifying performance drop.\n  - Include distribution shifts in goals (e.g., extreme speeds or orientations) and observation noise to test policy sensitivity, reporting success and stability metrics (No direct evidence found in the manuscript).\n  - Quantify the claimed gains (Section 4.2) at specific perturbation settings (e.g., friction ×0.4, gravity ×1.5) with numerical values and uncertainty, or soften the language to match the plotted differences (Figure 4).\n- Bolded Title: **Improve human study rigor and mitigate bias**\n  - Report statistical significance (e.g., binomial tests or bootstrap CIs) for win rates in Table 2 and aggregate across forms (Section 4.3; Appendix D).\n  - Use balanced pairings (e.g., pairwise or tournament designs) that do not always include Task Tokens, and randomize algorithm presentation to reduce anchoring (Appendix D).\n  - Provide participant demographics (age, domain expertise) and pre-registered analysis plans; report inter-rater reliability and include sanity checks (Appendix D).\n  - Qualify the overall conclusion to reflect mixed human-likeness outcomes, noting PULSE’s higher perceived motion quality (Section 4.3; Table 2), while keeping the benefits vs PPO/AMP/fine-tune.\n- Bolded Title: **Add objective motion-quality and physics-consistency metrics**\n  - Report kinematic metrics such as foot sliding (cm/s), joint jerk/acceleration statistics, and COM smoothness across tasks to complement human judgments (No direct evidence found in the manuscript).\n  - Include physics metrics (penetration depth, contact impulse distributions, energy consumption) and constraints satisfaction (e.g., torque limits) to assess realism (No direct evidence found in the manuscript).\n  - Provide failure mode statistics (fall rates, self-collisions, near-singular postures) and qualitative analysis, linking them to token configurations or rewards (No direct evidence found in the manuscript).\n- Bolded Title: **Assess real-world transfer and BFM dependence**\n  - Conduct sim-to-real or hardware-in-the-loop experiments (even if small-scale) or, at minimum, domain-randomized sim tests with sensor/actuator noise to probe real-world applicability (Section 4; Conclusion).\n  - Evaluate sensitivity to the choice of BFM by testing with an alternative GC-BFM referenced in the paper (e.g., Masked Trajectory Models; References: Wu et al., 2023), documenting portability and performance differences (Conclusion).\n  - Add experiments with extensive domain randomization and partial observability to measure robustness to real-world uncertainties (No direct evidence found in the manuscript).Score\n- Overall (10): 7 — Strong empirical results across five tasks (Table 1), fast Strike convergence (Figure 3), OOD robustness on Steering (Figure 4), and parameter-efficient adaptation (Section 4.1), tempered by missing methodological details (Section 3.3; Appendix A) and several clarity inconsistencies (Section 4.1; Figure 3; Appendix B).\n- Novelty (10): 7 — Learning a task-specific token encoder via PPO while freezing a GC-BFM (Sections 3.1–3.3; Figure 1) cleanly integrates reward-driven adaptation into tokenized goal conditioning with multimodal prompting (Figures 5–6).\n- Technical Quality (10): 6 — Solid performance and ablations (Table 1; Figure 3; Figure 4; Tables 3–4), but incomplete optimization/reward specifications (Section 3.3; Appendix A) and unresolved budget/termination inconsistencies (Figure 3; Appendix B) limit rigor.\n- Clarity (10): 5 — High-level method and visuals are clear (Sections 3.1–3.2; Figure 1), but missing formulas, tokenization specifics, parameter-count reconciliation, and a truncated sentence (Section 3.3; Section 4.1; Appendix B) reduce reproducibility and interpretability.\n- Confidence (5): 4 — Assessment is grounded in the presented figures, tables, and appendices, though absent technical specifics and some inconsistencies (Section 3.3; Section 4.1; Figure 3; Appendix A–B) constrain certainty.",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 7,
        "novelty": 7,
        "technical_quality": 6,
        "clarity": 6,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 7,
        "novelty": 7,
        "technical_quality": 6,
        "clarity": 5,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "Summary\n- The paper proposes Task Tokens, an approach for adapting goal-conditioned behavior foundation models (GC-BFMs), specifically MaskedMimic, to downstream control tasks by learning a small task-specific encoder via reinforcement learning while freezing the BFM. The encoder maps task observations to a 512-d token that is concatenated with existing BFM tokens (state and optional user priors) and fed to the transformer (Section 3.1–3.3; Figure 1; Section 3.2). Training uses PPO to optimize the encoder with gradients flowing through the frozen BFM (Section 3.3; Figure 1 caption). Experiments on five humanoid tasks in Isaac Gym show high success rates (Table 1), fast convergence on Strike (Figure 3), improved robustness to friction/gravity perturbations on Steering (Figure 4), strong parameter efficiency (~200k/task; Section 4.1), and favorable human-likeness against several baselines (Table 2). The method also supports multi-modal prompting with joint and text priors (Figures 5–6). Limitations include reliance on the underlying BFM quality and focus on simulation (Conclusion).Strengths\n- Bolded Title: **Clear, modular method design leveraging GC-BFM tokenization**\n  - Evidence: The approach concatenates a learned Task Token with Prior and State tokens, preserving the frozen GC-BFM architecture (Section 3.1–3.2; Figure 1; Figure 1 caption). Why it matters: This modularity enables plug-in adaptation without risking catastrophic forgetting, supporting technical soundness and practical impact.\n  - Evidence: Task Encoder outputs a 512-d embedding from egocentric task observations and proprioception (Section 3.2). Why it matters: A compact interface makes the approach parameter-efficient and applicable across tasks (novelty and practicality).\n  - Evidence: PPO optimizes only the encoder while gradients flow through the frozen BFM (Section 3.3; Figure 1 caption). Why it matters: Freezing the foundation model preserves learned motion priors and stability, supporting robustness.\n- Bolded Title: **Strong performance across diverse humanoid tasks**\n  - Evidence: Task Tokens achieve top or near-top success rates on Reach (94.88±1.99%), Direction (99.26±0.79%), Steering (88.69±4.04%), Long Jump (99.75±0.57%), and competitive results on Strike (76.61±3.49%) (Table 1). Why it matters: Demonstrates broad applicability and task coverage (experimental rigor and impact).\n  - Evidence: Convergence curves on Strike show adaptation in under ~50M steps, with PULSE requiring ~300M steps for similar performance (Figure 3). Why it matters: Indicates sample efficiency and training effectiveness (technical quality).\n  - Evidence: Ablations confirm benefits of joint conditioning when available (Table 3) and explore encoder design choices (Table 4). Why it matters: Strengthens claims with controlled analyses (experimental rigor).\n- Bolded Title: **Parameter efficiency and minimal footprint per task**\n  - Evidence: The encoder uses ~200k parameters per task, compared to ~9.3M for PULSE and ~25M for MaskedMimic fine-tune (Section 4.1). Why it matters: Lower memory and compute requirements facilitate deployment and scalability (impact).\n- Bolded Title: **Robustness to out-of-distribution perturbations**\n  - Evidence: On Steering, Task Tokens (with and without joint conditioning) outperform baselines across varying friction and gravity multipliers, with large gains at low friction (~0.4) and high gravity (~1.5) (Figure 4). Why it matters: Preserving robustness under perturbations is critical for practical control (technical soundness and impact).\n  - Evidence: The method leverages the BFM’s pretraining on diverse data to retain robustness characteristics (Section 4.2). Why it matters: Shows that freezing the BFM helps maintain generalization while adapting to new tasks (clarity and soundness).\n- Bolded Title: **Human-likeness validated via user study**\n  - Evidence: Task Tokens win against MaskedMimic fine-tune, AMP, and PPO in human-likeness across multiple tasks (e.g., 99.48% vs fine-tune on Direction; 90.40% on Steering) (Table 2). Why it matters: Supports the claim that freezing BFMs preserves natural motion (impact and experimental support).\n  - Evidence: Study includes ~96 participants evaluating 40 triplets across tasks (Appendix D). Why it matters: Provides nontrivial empirical validation (experimental rigor).\n  - Evidence: Motion quality benefit is a stated motivation for freezing (Section 4.3). Why it matters: Aligns design choice with outcomes (technical soundness).\n- Bolded Title: **Multi-modal prompting compatibility**\n  - Evidence: Combining Task Tokens with joint conditioning and text prompts (e.g., “a person performs a kick”) resolves undesirable behaviors (e.g., backward walking) and directs motion style in Direction and Strike (Figures 5–6). Why it matters: Demonstrates flexibility to integrate human priors for more desirable behaviors (novelty and impact).\n- Bolded Title: **Transparent limitations and scope**\n  - Evidence: The paper acknowledges reliance on BFM quality and current restriction to simulation (Conclusion). Why it matters: Increases credibility and helps future work planning (clarity).Weaknesses\n- Bolded Title: **Methodological specificity is insufficient for full reproducibility**\n  - Evidence: The PPO objective is described qualitatively (“compute the PPO objective with respect to the task-specific reward and the BFM’s action probabilities”) without explicit loss formulas, action distribution types, or clipping/entropy settings (Section 3.3). Why it matters: Missing optimization details hinder reproducibility and technical scrutiny.\n  - Evidence: Token concatenation order, positional encoding scheme, and attention masking for mixed tokens (“sentence”) are not specified beyond “concatenation” (Section 3.2). Why it matters: Transformer behavior can be sensitive to token ordering and positional encodings; lacking details limits clarity and replicability.\n  - Evidence: Reward functions are invoked (dense rewards) but not specified; Appendix A provides success criteria only, not full reward shaping/weights (Appendix A; Section 4). Why it matters: Reward design critically affects PPO training and behaviors; missing definitions reduce interpretability of results.\n  - Evidence: The reported parameter count (~200k) for the task encoder (Section 4.1) is not reconciled with the “Bigger MLP [512,512,512]” used in main results (Appendix B), which typically implies a larger parameter footprint. Why it matters: Ambiguity on actual parameterization affects claims of parameter efficiency and reproducibility.\n  - Evidence: The manuscript states that joint conditioning (J.C.) is not available for Strike and Long Jump in the main results (Section 4, text after Figure 3), yet Section 4.4 and Figure 6 qualitatively demonstrate Strike with joint and text priors. Why it matters: Clarity on which priors are used in quantitative vs qualitative settings is necessary for consistent interpretation.\n  - Evidence: A sentence in Section 3.3 appears truncated (“This design choice is fundamental—while fine-tuning the entire model might yield …”; Section 3.3), leaving the methodological justification incomplete. Why it matters: Editorial gaps reduce clarity on key design choices.\n- Bolded Title: **Evaluation fairness and scope are limited**\n  - Evidence: PULSE is trained on 128 environments vs 1024 for Task Tokens (Appendix B), potentially affecting sample throughput and learning dynamics. Why it matters: Unequal parallelization can bias convergence and efficiency comparisons.\n  - Evidence: Sample efficiency is demonstrated only on Strike (Figure 3) and not across other tasks. Why it matters: Lacking multi-task convergence curves weakens general claims about sample efficiency.\n  - Evidence: Wall-clock times and compute resources are not reported (Appendix B mentions frames and seeds but no timing). Why it matters: Practical efficiency and deployment decisions rely on time and hardware, not only environment steps.\n  - Evidence: Figure 3 shows convergence up to 350M steps and mentions PULSE reaching similar performance around 300M steps (Section 4.1; Figure 3), whereas Appendix B states training used 120M frames for all methods. Why it matters: Discrepancy between steps shown and stated budgets obscures the fairness of sample-efficiency comparisons.\n  - Evidence: Training and evaluation use different episode termination rules (Appendix B), which can change success metrics. Why it matters: Inconsistent termination criteria can confound cross-method comparisons unless uniformly applied and reported.\n- Bolded Title: **OOD generalization evaluation is narrow**\n  - Evidence: OOD tests cover only Steering, varying friction and gravity (Section 4.2; Figure 4). Why it matters: Robustness claims are not validated across other tasks (e.g., Reach, Long Jump).\n  - Evidence: Perturbations are limited to two physics parameters; other realistic shifts (terrain roughness, sensor noise, object mass changes) are not considered (Section 4.2). Why it matters: Narrow perturbations limit generalization claims.\n  - Evidence: Distributional changes in goals or observation noise are not evaluated (No direct evidence found in the manuscript). Why it matters: Goal/observation shifts are common in real applications; missing tests reduce completeness.\n  - Evidence: The text claims “order of magnitude” higher success at low friction and high gravity (Section 4.2), but Figure 4 provides only qualitative curves without numerical values supporting a ≥10× gap. Why it matters: Overstated claims reduce interpretability and require quantification.\n- Bolded Title: **Human study design leaves room for bias and lacks statistical analysis**\n  - Evidence: Every triplet included Task Tokens (“We ensured that Task Tokens was presented every time”) (Appendix D). Why it matters: Always including Task Tokens may anchor participants or affect perceived motion diversity, introducing bias.\n  - Evidence: No statistical significance tests or confidence intervals are reported for win rates (Section 4.3; Table 2; Appendix D). Why it matters: Without statistical analysis, it is hard to assess reliability and the strength of conclusions.\n  - Evidence: Participant demographics and expertise are not provided; only counts (96 participants) are listed (Appendix D). Why it matters: Demographics and expertise can influence judgments; missing data limit interpretability.\n  - Evidence: The Conclusion states “Human studies confirm that Task Tokens generate more human-like movements” (Conclusion), while the paper itself acknowledges PULSE scores higher on human-likeness (Section 4.3; Table 2). Why it matters: Inconsistent summary language can mislead readers about comparative motion quality.\n- Bolded Title: **Limited quantitative motion-quality metrics beyond human study**\n  - Evidence: No kinematic or contact metrics (e.g., foot sliding distance, joint jerk, COM smoothness) are reported (No direct evidence found in the manuscript). Why it matters: Objective motion-quality measures complement subjective studies and strengthen technical claims.\n  - Evidence: No reporting of physics consistency (e.g., penetration, contact impulse distributions) (No direct evidence found in the manuscript). Why it matters: Physics realism is critical for control; quantitative checks increase rigor.\n  - Evidence: Failure mode analysis (fall rates, excessive torso bending, limb self-collisions) is not provided (No direct evidence found in the manuscript). Why it matters: Understanding failure patterns guides method design and practical use.\n- Bolded Title: **Real-world applicability and dependence on BFM quality not empirically assessed**\n  - Evidence: Experiments are only in simulation using SMPL in Isaac Gym (Section 4; Appendix A). Why it matters: Without sim-to-real or hardware tests, practical deployment remains unverified.\n  - Evidence: The Conclusion acknowledges reliance on underlying BFM quality (Conclusion). Why it matters: No experiments vary the BFM to assess sensitivity or portability (No direct evidence found in the manuscript).\n  - Evidence: No domain randomization or sensor/actuator noise tests are reported to bridge sim-to-real gaps (No direct evidence found in the manuscript). Why it matters: Such tests are standard precursors to real-world validation.Suggestions for Improvement\n- Bolded Title: **Provide complete methodological details for reproducibility**\n  - Include a formal PPO objective with all terms (e.g., clipped surrogate loss, entropy coefficient, advantage estimation) and specify action distributions used by the frozen BFM (Section 3.3); add exact hyperparameters in Appendix B.\n  - Document token concatenation order, positional encoding or learned time embeddings, and any attention masks across Prior/Task/State tokens; add a small ablation on token order and positional encoding choices (Section 3.2).\n  - Publish full reward functions per task (component terms, weights, normalization) alongside success criteria (Appendix A), enabling independent reproduction and sensitivity analyses.\n  - Report exact parameter counts for the task encoder configuration used in main results, clarifying the “~200k” statement (Section 4.1) vs the “Bigger MLP [512,512,512]” in Appendix B; specify whether the critic is included.\n  - Clearly delineate which priors (e.g., J.C., text) are used for quantitative results versus qualitative demos, especially for Strike/Long Jump (Section 4; Section 4.4; Figure 6).\n  - Fix the truncated sentence in Section 3.3 and ensure uninterrupted, explicit justification for freezing the BFM versus full fine-tuning (Section 3.3).\n- Bolded Title: **Strengthen evaluation fairness and broaden efficiency reporting**\n  - Match parallelization across methods (e.g., re-run PULSE with 1024 environments or report per-step throughput) and report compute resources (GPUs, batch sizes) to contextualize comparisons (Appendix B).\n  - Provide convergence curves (success and return) for all tasks, not only Strike, and report per-task sample budgets (Figures 3; Section 4.1), clarifying consistent stopping criteria.\n  - Report wall-clock training times and total compute (e.g., GPU-hours) for all methods to complement environment-step metrics (Appendix B), enabling practical efficiency assessment.\n  - Reconcile the steps shown in Figure 3 (up to 350M, with PULSE around 300M; Section 4.1) with the stated 120M-frame budget (Appendix B); define “step” vs “frame” and list exact budgets per method for the plotted curves.\n  - Standardize and document episode termination criteria across training/evaluation (Appendix B), and confirm identical evaluation rules for all methods.\n- Bolded Title: **Expand OOD robustness evaluation**\n  - Evaluate friction/gravity perturbations across all tasks (Reach, Direction, Long Jump, Strike) to generalize robustness claims (Section 4.2).\n  - Add perturbations such as terrain roughness, external pushes, sensor noise, and object mass/geometry changes to stress-test policies (Section 4.2), quantifying performance drop.\n  - Include distribution shifts in goals (e.g., extreme speeds or orientations) and observation noise to test policy sensitivity, reporting success and stability metrics (No direct evidence found in the manuscript).\n  - Quantify the claimed gains (Section 4.2) at specific perturbation settings (e.g., friction ×0.4, gravity ×1.5) with numerical values and uncertainty, or soften the language to match the plotted differences (Figure 4).\n- Bolded Title: **Improve human study rigor and mitigate bias**\n  - Report statistical significance (e.g., binomial tests or bootstrap CIs) for win rates in Table 2 and aggregate across forms (Section 4.3; Appendix D).\n  - Use balanced pairings (e.g., pairwise or tournament designs) that do not always include Task Tokens, and randomize algorithm presentation to reduce anchoring (Appendix D).\n  - Provide participant demographics (age, domain expertise) and pre-registered analysis plans; report inter-rater reliability and include sanity checks (Appendix D).\n  - Qualify the overall conclusion to reflect mixed human-likeness outcomes, noting PULSE’s higher perceived motion quality (Section 4.3; Table 2), while keeping the benefits vs PPO/AMP/fine-tune.\n- Bolded Title: **Add objective motion-quality and physics-consistency metrics**\n  - Report kinematic metrics such as foot sliding (cm/s), joint jerk/acceleration statistics, and COM smoothness across tasks to complement human judgments (No direct evidence found in the manuscript).\n  - Include physics metrics (penetration depth, contact impulse distributions, energy consumption) and constraints satisfaction (e.g., torque limits) to assess realism (No direct evidence found in the manuscript).\n  - Provide failure mode statistics (fall rates, self-collisions, near-singular postures) and qualitative analysis, linking them to token configurations or rewards (No direct evidence found in the manuscript).\n- Bolded Title: **Assess real-world transfer and BFM dependence**\n  - Conduct sim-to-real or hardware-in-the-loop experiments (even if small-scale) or, at minimum, domain-randomized sim tests with sensor/actuator noise to probe real-world applicability (Section 4; Conclusion).\n  - Evaluate sensitivity to the choice of BFM by testing with an alternative GC-BFM referenced in the paper (e.g., Masked Trajectory Models; References: Wu et al., 2023), documenting portability and performance differences (Conclusion).\n  - Add experiments with extensive domain randomization and partial observability to measure robustness to real-world uncertainties (No direct evidence found in the manuscript).Score\n- Overall (10): 7 — Strong empirical results across five tasks (Table 1), fast Strike convergence (Figure 3), OOD robustness on Steering (Figure 4), and parameter-efficient adaptation (Section 4.1), tempered by missing methodological details (Section 3.3; Appendix A) and several clarity inconsistencies (Section 4.1; Figure 3; Appendix B).\n- Novelty (10): 7 — Learning a task-specific token encoder via PPO while freezing a GC-BFM (Sections 3.1–3.3; Figure 1) cleanly integrates reward-driven adaptation into tokenized goal conditioning with multimodal prompting (Figures 5–6).\n- Technical Quality (10): 6 — Solid performance and ablations (Table 1; Figure 3; Figure 4; Tables 3–4), but incomplete optimization/reward specifications (Section 3.3; Appendix A) and unresolved budget/termination inconsistencies (Figure 3; Appendix B) limit rigor.\n- Clarity (10): 5 — High-level method and visuals are clear (Sections 3.1–3.2; Figure 1), but missing formulas, tokenization specifics, parameter-count reconciliation, and a truncated sentence (Section 3.3; Section 4.1; Appendix B) reduce reproducibility and interpretability.\n- Confidence (5): 4 — Assessment is grounded in the presented figures, tables, and appendices, though absent technical specifics and some inconsistencies (Section 3.3; Section 4.1; Figure 3; Appendix A–B) constrain certainty."
}