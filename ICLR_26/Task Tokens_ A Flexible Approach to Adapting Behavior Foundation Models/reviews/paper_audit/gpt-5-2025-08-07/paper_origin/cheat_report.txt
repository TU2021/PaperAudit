Academic Integrity and Consistency Risk Report

Scope: Review focuses on internal logical consistency, numerical alignment, and missing details that materially affect validity. Citations refer to manuscript sections, figures, tables, and appendices.

1) Training budget inconsistency (steps/frames)
- Evidence:
  - Appendix B states: “All experiments were trained in parallel on 1024 environments for 4000 epochs resulting in 120M frames. PULSE was also trained on 120M frames” (Appendix B).
  - Main text and Figure 3: Strike convergence curves are plotted up to 350M steps, and the text claims “PULSE reaches the same performance around 300 million steps” (Section 4.1; Figure 3 and accompanying paragraph).
- Issue: The paper asserts that PULSE was trained on 120M frames (Appendix B) but reports/plots performance at 300–350M steps (Figure 3). It is unclear how results beyond 120M frames were obtained. This discrepancy materially affects claims of sample efficiency and comparative convergence.
- Requested clarification: Define “steps” vs “frames,” reconcile the 120M-frame budget with the 300–350M-step curves, and state the exact training budgets per method used for Figure 3.

2) Parameter-count mismatch for Task Encoder
- Evidence:
  - Main text: “Task Tokens requires only ~200k parameters for each additional task” (Section 4; also repeated in Section 4.1).
  - Appendix B: “Task Encoder … MLP of size [512, 512, 512] … output 512-dim token” (Appendix B). With three 512×512 layers, the encoder alone typically exceeds ~200k parameters (on the order of ~0.8M, excluding input/output layer specifics). A 256-width MLP could be ~200k (Appendix C.2 mentions a “Bigger MLP [512,512,512] versus [256,256]”).
- Issue: The stated ~200k parameters contradict the main-results configuration (Appendix B uses the “Bigger MLP”). If the headline results rely on the larger 512-width encoder, the ~200k claim is inaccurate or needs to be explicitly limited to the smaller architecture.
- Requested clarification: Report exact parameter counts for the encoder actually used in the main results (and whether the critic is included/excluded), and adjust comparisons to PULSE/Fine-Tune accordingly.

3) Overgeneralized human-study conclusion contradicts results vs PULSE
- Evidence:
  - Human study table: For PULSE, Task Tokens’ win rates are 14.80% (Direction), 46.44% (Steering), 36.28% (Reach), 24.19% (Strike), 39.13% (Long Jump) (Section 4.3, Table 2).
  - Main text acknowledges: “PULSE scores higher in terms of human-likeness of the motion” (Section 4.3, after Figure 6).
  - Conclusion claims: “Human studies confirm that Task Tokens generate more human-like movements” (Conclusion).
- Issue: The blanket statement in the Conclusion is inconsistent with the human study outcomes vs PULSE (Task Tokens loses in most tasks). The conclusion needs qualification (e.g., “more human-like than AMP/PPO/fine-tuning, but less than PULSE”).
- Requested correction: Qualify the conclusion to reflect mixed results, particularly Task Tokens’ inferiority to PULSE in perceived human-likeness.

4) “Order-of-magnitude” robustness claim not supported by presented plots
- Evidence:
  - OOD section: “Task Tokens exhibits an order of magnitude higher rate of success in very low friction scenarios (×0.4) and very large gravity (×1.5)” (Section 4.2).
  - Figure 4/5 (plots of Success Rate % vs Friction and Gravity): Visual inspection shows improvements but not clearly an order-of-magnitude (e.g., at friction ~0.4, differences appear within a factor of ~1.5–2, not 10×).
- Issue: The “order-of-magnitude” claim is not substantiated by numerical values or error bars demonstrating ≥10× improvement at the cited operating points.
- Requested correction: Provide exact numerical values at the referenced perturbations or soften the claim to match the plotted differences.

5) Joint Conditioning (J.C.) availability vs use in Strike
- Evidence:
  - Main text: “Long Jump and Strike pose a great challenge… thus J.C. is not available for them neither in Task Tokens nor in MaskedMimic” (Section 4, immediately after Figure 3; Block #22).
  - Multi-modal prompting: “Joint and text-based goals can be provided alongside the task-specific Task Tokens… in Strike” with captions implying joint and text priors used (Section 4.4; Figure 6 caption).
- Issue: The paper states J.C. is not available for Strike in main results but then demonstrates Strike with joint/text priors in Section 4.4. It is unclear whether J.C. was used in final evaluations for Strike or only as a qualitative/demo setting.
- Requested clarification: Explicitly distinguish (a) quantitative results settings (no J.C. for Strike/Long Jump) vs (b) qualitative multi-modal demonstrations, and state which priors were used in Figures 5–6.

6) Rewards for PPO training not specified
- Evidence:
  - Method claims optimizing “dense rewards” (Sections 3, 3.3).
  - Appendix A gives success criteria but not reward functions/shaping terms for Reach, Direction, Steering, Strike, Long Jump (Appendix A).
- Issue: Missing reward definitions materially affect reproducibility and interpretation of sample efficiency and performance.
- Requested addition: Provide full reward formulations (terms, weights, normalization, termination penalties) for all tasks, used across all methods compared.

7) Different termination rules in training vs evaluation
- Evidence:
  - “Success rate results in training and evaluation might differ since we do not use the same episode termination rules” (Appendix B).
- Issue: Using different termination criteria can confound reported success curves and comparisons across methods unless consistently applied and clearly documented.
- Requested clarification: Specify termination conditions for training and for evaluation, and confirm all methods use identical evaluation rules.

8) Broken/incomplete sentence in Training section
- Evidence:
  - “This design choice is fundamental—while fine-tuning the entire model might yield optimizing for task performance.” (Section 3.3; Block #22).
- Issue: The sentence is truncated and unclear; it likely should compare fine-tuning vs frozen-BFM training. While minor editorially, it obscures a key methodological justification.
- Requested fix: Provide the complete sentence to avoid ambiguity.

If addressed, these issues would materially strengthen the paper’s clarity and trustworthiness. If not addressed, they leave room for misinterpretation of sample efficiency, parameter efficiency, human-likeness conclusions, and robustness claims.

No additional clear evidence of plagiarism or data fabrication was found in the manuscript.