Summary
The paper proposes Task Tokens, a method for adapting a frozen Goal-Conditioned Behavior Foundation Model (GC-BFM), specifically MaskedMimic, to new tasks by learning a small neural “Task Encoder” that maps task observations to an additional conditioning token. The Task Encoder is trained via reinforcement learning (PPO) while the BFM remains frozen, allowing the approach to combine human-specified priors (e.g., joint conditioning or text goals) with reward-driven optimization. Experiments in Isaac Gym on five humanoid tasks (Reach, Direction, Steering, Strike, Long Jump) show improved success rates and sample efficiency compared to baselines, better robustness to out-of-distribution friction/gravity, and human-study evidence of more human-like motions than most baselines.

Soundness
- Methodological plausibility: The idea of training a small adapter (Task Encoder) while keeping a large pretrained controller frozen is sound and consistent with adapter/prompt-tuning paradigms. The gradient path described in Section 3.3/Figure 1—computing PPO updates with respect to action probabilities while gradients flow through the frozen GC-BFM into the Task Encoder—is feasible if the BFM is differentiable and outputs probabilistic actions.
- Experimental setup: The evaluation spans multiple tasks and includes baselines (Pure RL, AMP, PULSE, MaskedMimic Fine-Tune, MaskedMimic J.C.). Reporting success rates with mean±std over 5 seeds (Table 1; Appendix B) is standard. OOD tests on friction/gravity (Figure 4/Appendix plots) and ablations (Appendix C) strengthen the claims.
- Points needing clarification/verification: 
  - PPO details are under-specified (e.g., action distribution type, entropy regularization, clipping, advantage estimator, horizon), making reproduction difficult (Section 3.3; Appendix B lacks PPO hyperparameters).
  - Reward definitions are only implicitly described; success criteria are detailed (Appendix A) but the reward shaping for each task is not (Section 4, Appendix A). This hinders reproducibility and attribution of performance gains to the learned token vs reward design.
  - Sample efficiency claims (Figure 3) show convergence by ~50M steps vs ~300M for PULSE, but Appendix B states “all experiments were trained on 120M frames; PULSE also on 120M frames,” which appears inconsistent with the 350M timeline in Figure 3. This should be reconciled (frames vs steps vs environments).
  - Human study methodology (Appendix D) gives a winning-rate comparison, but statistical significance and inter-rater reliability are not reported.

Presentation
- The manuscript is generally clear and well-structured, with a concise motivation (Sections 1–2), method description (Sections 3.1–3.3), and broad experimental coverage (Section 4). Figures illustrating the pipeline (Figure 1) and OOD performance (Figure 4, Appendices) are helpful.
- Certain sections are truncated or overly high-level (e.g., Section 3.3 ends mid-sentence in the provided excerpt; reward descriptions are missing from main text). 
- Tables and figures are legible, but some captions are brief and could better explain evaluation details (e.g., Figure 3’s axis units and training schedules).
- The human study description (Appendix D) is clear, but summary Table 2 lacks confidence intervals or significance testing.

Contribution
- The paper’s main contribution is a simple, parameter-efficient mechanism (≈200k parameters per task; Section 4/Appendix B) to adapt GC-BFMs to new tasks without fine-tuning the foundation model, preserving natural motion quality while improving task performance and robustness.
- It advances practical control by blending reward-driven optimization with multi-modal prompting through tokens (Sections 3.1–3.2; Section 4.4), demonstrating synergy across modalities (joint conditioning, text).
- Novelty lies in learning task-specific tokens via RL that condition a frozen BFM—a pragmatic and effective variant of adapter- or prompt-based control for embodied agents. While related in spirit to adapter/prompt tuning, applying PPO-trained tokens to preserve the motion manifold in MaskedMimic is a meaningful engineering/scientific contribution.

Strengths
- Strong empirical results across diverse tasks with success-rate improvements and faster convergence (Table 1; Figure 3; Section 4.1).
- Robustness to OOD perturbations (friction/gravity), maintaining performance advantages (Figure 4; Appendix plots).
- Human-likeness supported via human study, with Task Tokens outperforming fine-tuning, AMP, PPO, and MaskedMimic J.C. in most tasks (Table 2; Section 4.3).
- Parameter efficiency and modularity (≈200k parameters per task; Section 4; Appendix B); compatibility with other prompts (Section 4.4).
- Useful ablations on architecture and priors (Appendix C.2/C.1) showing practical guidance for users.

Weaknesses
- PPO/reward specification lacks detail, impeding reproducibility and clarity of attribution (Section 3.3; Appendix B/A).
- Apparent inconsistency between training frames and convergence plot timelines (Figure 3 vs Appendix B), which raises questions about fairness and sample-efficiency claims.
- Human study analysis reports win rates but omits statistical significance, effect sizes, or inter-rater measures (Appendix D; Table 2).
- Limited discussion of failure cases where PULSE is more human-like (Section 4.3) and of how Task Tokens could be improved for motion realism.
- No real-world or hardware results; all evaluations are in simulation (Section 5 acknowledges limitation), and transfer remains untested.
- The claim of “~200k parameters” could be more rigorously substantiated given the [512,512,512] MLP architecture (Appendix B); a precise count per task and per variant would help.

Questions
- Can the authors provide full PPO hyperparameters and action distribution details (Gaussian, tanh-squashed, PD targets) and training schedules (clip range, GAE, entropy, batch sizes)?
- What are the exact reward functions for each task (Reach, Direction, Steering, Strike, Long Jump), including shaping terms and weights? Are they shared across methods?
- How do “steps,” “frames,” and “environments” map in Figure 3, and why does the plot extend to 350M if Appendix B says 120M frames? Were some baselines trained longer?
- Could the authors include statistical tests (e.g., binomial proportion tests) for Table 2 win rates and report confidence intervals?
- How sensitive is Task Tokens’ performance to the dimensionality of τ and the architecture size (beyond Appendix C.2)? Is 512 necessary?
- Are there scenarios where the Task Encoder induces non-natural motions despite the frozen BFM (failure analysis)?
- Can Task Tokens be used for sim-to-real transfer or on hardware (even preliminary results)?

Rating
- Overall (10): 8 — Strong empirical evidence of a simple, effective adapter mechanism for GC-BFMs with robustness and human-likeness benefits (Table 1, Figure 4, Table 2), though some reproducibility details need clarification (Section 3.3; Appendix B).
- Novelty (10): 8 — RL-trained task tokens to condition a frozen BFM is a neat and practical twist on adapter/prompt tuning for embodied control (Sections 3.1–3.3; Section 4.4).
- Technical Quality (10): 7 — Method is sound and evaluations are broad, but PPO/reward specification and training-time inconsistencies (Figure 3 vs Appendix B) limit rigor (Sections 3.3, 4.1; Appendix A/B).
- Clarity (10): 7 — Generally clear with helpful figures (Figure 1, Figure 4), but missing reward and PPO details and the convergence plot ambiguity reduce clarity (Sections 3.3, 4.1; Appendix B).
- Confidence (5): 4 — High-level method understanding and multiple empirical anchors; moderate uncertainty due to missing training/reward specifics and human-study statistics (Sections 3–4; Appendices).


Summary
The paper introduces Task Tokens, which learn a compact task-specific encoder that emits an embedding token to condition a frozen Goal-Conditioned Behavior Foundation Model (MaskedMimic). The encoder is trained via PPO on task rewards while the BFM remains fixed, combining user priors (joint/text tokens) with reward-driven optimization. Experiments on five humanoid tasks demonstrate high success rates, rapid convergence, OOD robustness, and improved human-likeness versus several baselines, with notable parameter efficiency.

Soundness
- The approach is conceptually sound: the frozen backbone provides priors for natural motion, while a small learnable interface steers behavior toward task rewards (Sections 3.1–3.3). This aligns with adapter/prompt training ideas and preserves the motion manifold.
- Empirical coverage is broad, including multi-task comparisons, OOD testing, ablations, and a human study (Sections 4.1–4.4; Appendices).
- Concerns:
  - The PPO training description is sparse; practical aspects (distribution types, critic design, rollout lengths, GAE parameters, clip ranges) are needed (Section 3.3; Appendix B gives critic size but not PPO hyperparameters).
  - Fairness and consistency: Figure 3 shows 0–350M steps while Appendix B states 120M frames for all experiments; this discrepancy should be resolved.
  - Reward design is not fully specified (Appendix A defines success, not rewards), which could affect replicability and interpretation of gains.

Presentation
- The manuscript is readable and logically organized. Figures illustrating the architecture and OOD results are effective (Figure 1; Figure 4).
- Some text is truncated or lacks detail (Section 3.3 ends mid-sentence in the provided excerpt). Tables are informative but could include more statistical context (Table 2 human study).
- The training/evaluation setup is partially described (Appendix B), yet missing PPO hyperparameters and reward formulas.

Contribution
- The work offers a practical, parameter-efficient mechanism to adapt GC-BFMs to task-specific objectives, preserving natural motion while improving task success and robustness. It also shows how learned tokens can blend with existing modalities (text, joint conditioning) for better control (Section 4.4).
- While related to adapters/prompts, the specific integration with MaskedMimic and RL-based token optimization in control contexts is valuable.

Strengths
- Demonstrated performance across tasks with competitive or superior success rates and faster convergence (Table 1; Figure 3; Section 4.1).
- Robustness under OOD friction/gravity perturbations (Figure 4; Appendices).
- Human-likeness benefits vs fine-tuning, AMP, PPO, and MaskedMimic J.C. (Table 2; Section 4.3).
- Minimal additional parameters (~200k) and modular integration with priors (Section 4; Section 4.4).
- Useful ablations and detailed environment specs (Appendix A/C).

Weaknesses
- Missing PPO hyperparameters and reward formulations reduce reproducibility and make it hard to attribute gains purely to Task Tokens (Section 3.3; Appendix A/B).
- Timeline inconsistency between Figure 3 and Appendix B (steps vs frames) challenges the sample-efficiency narrative.
- Human study lacks statistical tests and reliability measures (Appendix D), and PULSE is more human-like in some cases (Section 4.3), which warrants deeper analysis.
- No real-world validation; reliance on simulation (Section 5).
- Parameter-count claim (~200k) should be precisely computed and reported for various encoder configurations (Appendix B/C.2).

Questions
- Please provide full PPO hyperparameters, action distributions, rollout lengths, and training schedules for all methods.
- What are the exact reward functions for each task? Are they identical across methods? How sensitive are results to reward weights?
- Can you reconcile the 120M frames claim (Appendix B) with the 350M-step axis in Figure 3? If “steps” are per-environment, please clarify.
- Could you include confidence intervals or statistical tests for human-study win rates in Table 2?
- How does Task Tokens perform when τ dimension is varied (e.g., 128, 256, 1024)? Is 512 best?
- Are there safety/regularization mechanisms to prevent the encoder from exploiting the BFM in unintended ways?
- Could you report per-task parameter counts precisely to support the ~200k claim?

Rating
- Overall (10): 7 — Solid, practical adapter approach with strong results and robustness (Table 1, Figure 4), but reproducibility gaps and training-time ambiguity reduce impact (Section 3.3; Appendix B; Figure 3).
- Novelty (10): 7 — RL-trained task tokens for a frozen GC-BFM is a meaningful application of prompt/adapter ideas to embodied control (Sections 3.1–3.3; Section 4.4).
- Technical Quality (10): 6 — Sound concept and broad experiments, but insufficient training/reward details and figure inconsistency hamper rigor (Section 3.3; Appendix A/B; Figure 3).
- Clarity (10): 6 — Clear motivation and visuals (Figure 1), yet missing essential training specifics and statistical analyses (Sections 3.3, 4.3; Appendices).
- Confidence (5): 3 — Good grasp of method and results from provided anchors, but limited by missing hyperparameters/reward details and human-study statistics.


Summary
This paper presents Task Tokens, a reinforcement learning–trained task encoder that produces an additional conditioning token for a frozen MaskedMimic GC-BFM. The approach aims to bridge the gap between prompt-based high-level control and precise task optimization by learning task-specific embeddings from observations and rewards, while optionally combining joint/text priors. Experiments on Reach, Direction, Steering, Strike, and Long Jump show strong success rates, better sample efficiency, robustness to OOD conditions, and improved human-likeness compared to several baselines.

Soundness
- The training scheme—freeze the large pretrained controller and optimize a small encoder via PPO—is reasonable and aligns with established adapter paradigms. The gradient pathway (Figure 1; Section 3.3) is coherent if the BFM outputs differentiable action probabilities.
- The choice of baselines is appropriate and the experimental coverage is broad (Table 1; Figure 4; Appendix C).
- Gaps that weaken soundness:
  - Training specifics for PPO and reward definitions are missing (Section 3.3; Appendix A/B), making replication and attribution difficult.
  - The sample-efficiency figure (Figure 3) is not clearly reconciled with the 120M frames statement in Appendix B; it creates uncertainty about fairness/compute budgets.
  - Human-likeness comparisons (Table 2) act as pairwise win rates without statistical testing; moreover, PULSE outperforms Task Tokens in human-like motion in certain tasks (Section 4.3), calling for analysis of why.

Presentation
- The paper is well organized and readable; figures and tables are mostly clear. Figure 1 effectively communicates the architecture; Figure 4 and Appendix plots help interpret OOD robustness.
- Some important details are not presented (PPO hyperparameters, reward formulations). The convergence plot needs clearer axes and explanation (Figure 3).
- Human study setup is described (Appendix D), but the main text could benefit from statistical reporting and richer qualitative failure analyses.

Contribution
- A practical method to adapt a powerful GC-BFM to new tasks using a parameter-efficient encoder while preserving natural motion. The integration of multi-modal priors with learned token conditioning is useful for control scenarios.
- Novelty is moderate: the idea of training a task adapter/token is conceptually akin to prompt/adapter tuning, but applying it to MaskedMimic with PPO and demonstrating OOD robustness and human-likeness adds value.

Strengths
- Strong task performance and fast convergence (Table 1; Figure 3; Section 4.1).
- Robustness under friction/gravity perturbations (Figure 4; Appendix plots), consistent with leveraging pretrained priors.
- Human-likeness advantage vs multiple baselines (Table 2), supporting the motivation to freeze the BFM.
- Parameter efficiency (~200k per task; Section 4; Appendix B), making multi-task adaptation feasible.
- Ablations (Appendix C) and multi-modal demonstration (Section 4.4) clarify practical considerations.

Weaknesses
- Insufficient training and reward details (Section 3.3; Appendix A/B) limit reproducibility and causal interpretation.
- Potential inconsistency in training budgets (Figure 3 vs Appendix B); unclear whether baselines received comparable compute.
- Human study lacks statistical analysis; PULSE sometimes looks more human-like (Section 4.3), without deeper diagnosis.
- No real-world trials; the approach’s reliance on MaskedMimic limits applicability where such BFMs are unavailable or domain-shifted.
- Parameter-count statement (~200k) should be backed by exact counts per architecture choice (Appendix B/C.2).

Questions
- Please provide PPO hyperparameters (clip, lr, GAE λ/γ, entropy, batch sizes), action distributions, and rollout lengths for each task.
- What reward functions were used per task? Are weights and shaping terms shared across methods?
- How do “steps” map to “frames” and environments in Figure 3? Did some baselines train longer than 120M frames?
- Can you include statistical significance (e.g., binomial tests) for Table 2 win rates?
- What causes PULSE to be judged more human-like in some tasks (Section 4.3), and can Task Tokens incorporate style priors to match?
- How sensitive is performance to τ dimensionality and encoder size? Any results with smaller tokens?
- Could you report exact parameter counts to substantiate the ~200k claim under different encoder settings?

Rating
- Overall (10): 7 — Effective adapter method with strong empirical performance and robustness (Table 1, Figure 4), but reproducibility and training-budget clarity need improvement (Section 3.3; Appendix B; Figure 3).
- Novelty (10): 6 — Solid application of adapter/prompt ideas to GC-BFMs with RL; meaningful but incremental (Sections 3.1–3.3; 4.4).
- Technical Quality (10): 7 — Sound methodology and broad experiments; missing hyperparameters/rewards and plot ambiguity temper rigor (Section 3.3; Appendix A/B; Figure 3).
- Clarity (10): 8 — Clear architecture and experimental story (Figure 1; Section 4), though key training details are absent (Sections 3.3; Appendix B).
- Confidence (5): 4 — Good evidence across tasks and OOD; moderate uncertainty due to missing training specifics and human-study statistics.


Summary
The authors propose Task Tokens, an RL-trained task encoder that outputs a 512-d token to condition a frozen MaskedMimic GC-BFM. By concatenating this task token with state/prior tokens, the method seeks to optimize task rewards without compromising natural motion priors. Across five Isaac Gym tasks, the approach achieves high success rates, faster convergence than hierarchical baselines, robustness to friction/gravity changes, and favorable human-likeness outcomes, with ~200k trainable parameters per task.

Soundness
- The method is plausible: a differentiable frozen policy yields gradients to an upstream encoder trained via PPO (Section 3.3; Figure 1). Keeping the backbone fixed aligns with preserving learned motion priors.
- Evidence: Table 1 shows strong success rates; Figure 3 indicates faster convergence; OOD robustness (Figure 4); ablations (Appendix C); human study (Table 2).
- Concerns:
  - PPO and reward details are insufficient (Sections 3.3; Appendix A/B), obscuring reproducibility and confounding variables.
  - Training timeline discrepancies (Figure 3 vs Appendix B) could affect the fairness of sample-efficiency comparisons.
  - Human study lacks statistical analysis and admits PULSE’s advantage in human-likeness in some tasks (Section 4.3), suggesting the approach may still lag on motion style without additional priors.

Presentation
- Clear high-level narrative with helpful figures. Method diagrams (Figure 1) and OOD curves (Figure 4; Appendices) are informative.
- Missing low-level training details; captions could clarify axes/units and compute budgets (Figure 3).
- The text is accessible; Appendices add needed context (A/B/C/D), but reward formulas and full PPO configs are still missing.

Contribution
- A practical, modular, and parameter-efficient approach to task adaptation for GC-BFMs that preserves natural motion. Demonstrated multi-modal integration with joint/text priors and improved robustness.
- Conceptual novelty is moderate, but experimental validation in humanoid control is compelling and useful.

Strengths
- Competitive-to-superior performance and fast adaptation (Table 1; Figure 3; Section 4.1).
- Robustness to OOD physics perturbations (Figure 4; Appendices).
- Human-likeness advantages versus several baselines (Table 2), supporting the frozen-backbone design.
- Small number of new parameters per task and ability to combine with priors (Section 4; Section 4.4).
- Ablative insights on encoder size and proprio inputs (Appendix C.2).

Weaknesses
- Reproducibility gaps: missing PPO hyperparameters, action distributions, and reward functions (Section 3.3; Appendix A/B).
- Ambiguous training budgets in convergence plots vs Appendix claims (Figure 3, Appendix B).
- Human study lacks statistical rigor; no inter-rater reliability or significance testing (Appendix D).
- Motion style still trails PULSE on some tasks (Section 4.3), implying room for improvement in style control.
- No real-world validation; reliance on simulation and on availability of a suitable BFM (Section 5).

Questions
- Please provide complete PPO and policy distribution details, including critic training and any regularization (entropy, KL).
- What rewards were used per task (exact formula and weights)? Did you ensure fair, identical rewards for baselines?
- Can you reconcile the steps/frames discrepancy and specify compute budgets per method in Figure 3?
- Could you add statistical tests and confidence intervals to Table 2? How robust are results across different prompts?
- How does Task Tokens scale to multi-task training (one encoder for multiple tasks)? Any catastrophic interference?
- Is the ~200k parameter count exact for the [512,512,512] encoder? Please report counts for all variants.
- Are there safeguards to prevent degenerate behaviors when reward shaping is imperfect?

Rating
- Overall (10): 6 — Useful, well-validated adapter for GC-BFMs with strong empirical results (Table 1; Figure 4) but notable reproducibility and analysis gaps (Section 3.3; Appendix B; Table 2).
- Novelty (10): 5 — The idea resembles adapter/prompt tuning; the RL-conditioned token in this control context is incremental but practical (Sections 3.1–3.3; 4.4).
- Technical Quality (10): 6 — Sound high-level design and broad experiments; missing training/reward specifics and timeline ambiguities weaken rigor (Sections 3.3; Appendix A/B; Figure 3).
- Clarity (10): 7 — Clear exposition and visuals (Figure 1; Section 4), with key details omitted (PPO/rewards) (Sections 3.3; Appendix B).
- Confidence (5): 3 — Moderate confidence based on reported results; limited by absent hyperparameters/reward details and human-study statistics (Sections 3–4; Appendices).