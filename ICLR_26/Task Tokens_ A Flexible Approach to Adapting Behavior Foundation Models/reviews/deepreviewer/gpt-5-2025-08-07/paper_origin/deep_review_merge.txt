Summary
The paper introduces Task Tokens, a parameter-efficient approach for adapting a frozen Goal-Conditioned Behavior Foundation Model (GC-BFM), specifically MaskedMimic, to new tasks. A small neural Task Encoder maps task observations to an additional conditioning token that is concatenated with existing inputs (e.g., joint or text tokens) to steer the frozen policy. The Task Encoder is trained via reinforcement learning (PPO), with gradients flowing through the differentiable GC-BFM to update only the encoder, preserving the backboneâ€™s natural motion priors while optimizing task rewards. Experiments in Isaac Gym on five humanoid tasks (Reach, Direction, Steering, Strike, Long Jump) compare against Pure RL/PPO, AMP, PULSE, MaskedMimic fine-tuning, and MaskedMimic joint conditioning. Results show higher success rates, faster convergence, improved robustness to out-of-distribution physics (friction/gravity), and better human-likeness in a user study, with approximately 200k trainable parameters per task. Ablations and multi-modal integrations (combining with joint conditioning or text goals) are presented.

Strengths
- Conceptual soundness and practicality: Freezing a large pretrained controller and optimizing a small task-specific adapter aligns with adapter/prompt-tuning paradigms, leveraging strong motion priors while enabling reward-driven control.
- Strong empirical performance: Across five humanoid tasks, Task Tokens achieve competitive or superior success rates and faster convergence relative to baselines, supported by multi-seed reporting.
- Robustness: The method maintains advantages under out-of-distribution perturbations to friction and gravity, indicating robustness due to leveraging pretrained priors.
- Human-likeness: A human study indicates Task Tokens produce more human-like motions than most baselines, supporting the motivation to keep the backbone fixed.
- Parameter efficiency and modularity: The approach adds a small number of parameters (~200k) per task, making adaptation lightweight and feasible. It integrates cleanly with other priors/modalities (joint conditioning, text).
- Breadth of evaluation: Inclusion of baselines (Pure RL, AMP, PULSE, MaskedMimic variants), OOD tests, and ablations (architecture choices, priors) provides a comprehensive empirical picture.
- Clear high-level presentation: The manuscript is generally well-structured with helpful figures illustrating the pipeline and robustness results.

Weaknesses
- Reproducibility gaps in PPO details: The paper lacks essential training specifics for PPO (action distribution, entropy regularization, clipping, advantage estimator/GAE parameters, rollout horizon, training schedules, batch sizes, critic design), hindering replication and making it difficult to attribute gains specifically to Task Tokens versus optimizer settings.
- Unspecified reward functions: Success criteria are provided, but exact reward shaping formulas and weights per task are absent. Without clear rewards, it is hard to assess fairness across methods or isolate the contribution of the learned token from task-specific shaping.
- Sample-efficiency and compute inconsistencies: There is a discrepancy between the convergence plot timeline (up to ~350M steps) and the stated training budget (120M frames) in the appendix. The mapping among steps, frames, and number of parallel environments, and whether baselines received comparable compute, should be clarified to support claims of faster convergence.
- Human-study analysis lacks statistical rigor: Win-rate comparisons are reported without statistical significance tests, confidence intervals, or inter-rater reliability, limiting the strength of conclusions about human-likeness. Additionally, cases where PULSE is judged more human-like are not analyzed in depth.
- Limited discussion of failure cases and motion realism: The paper does not deeply examine scenarios where the Task Encoder may induce non-natural motions or where the approach underperforms on style realism, nor does it propose safeguards against unintended exploitation of the frozen backbone.
- No real-world validation: All results are in simulation; sim-to-real transfer and hardware deployment are untested, which limits practical impact.
- Parameter-count precision: The claim of ~200k parameters for a [512, 512, 512] MLP encoder is not substantiated with exact counts across variants, and precise per-task parameter reporting would improve transparency.
- Presentation issues: Some method sections appear truncated or overly high-level, and figure/table captions (e.g., axes units and training schedules in the convergence plot; statistical context in human-study tables) lack detail, reducing clarity.
- Scope and novelty: While effective, the approach is conceptually close to existing adapter/prompt-tuning ideas; its novelty lies more in application to masked-mimic BFMs and empirical validation than in fundamentally new theory.
