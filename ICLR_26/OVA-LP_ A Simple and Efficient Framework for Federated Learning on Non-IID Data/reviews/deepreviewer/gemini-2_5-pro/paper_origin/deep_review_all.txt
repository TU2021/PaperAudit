Here are four separate reviews of the paper "OvA-LP: A Simple and Efficient Framework for Federated Learning on Non-IID Data".

***

### **Review 1**

**Summary**
This paper introduces OvA-LP, a novel framework for federated fine-tuning (FFT) of foundation models designed to be robust against non-IID client data distributions. The core idea is to prevent client drift at its source, rather than correcting it after aggregation. The method combines three simple components: (1) linear probing on a frozen pretrained encoder, (2) one-vs-all (OvA) classification heads to decouple class updates, and (3) a two-stage training schedule to stabilize convergence. The authors demonstrate through extensive experiments that OvA-LP achieves near-IID performance in extremely heterogeneous settings, significantly outperforming recent state-of-the-art methods.

**Soundness**
The methodology is sound and well-motivated. The authors ground their design in a bias-variance decomposition of federated gradients, systematically linking each component of OvA-LP to a specific source of drift (feature skew, label skew, and variance). The experimental evaluation is comprehensive and convincing. The authors test their method on CIFAR-100 under various challenging non-IID partitions (Shard-1/2, Dirichlet), compare it against relevant baselines (FFT-MoE, PFPT), and conduct thorough ablation studies to validate each design choice. Additional analyses on encoder/task variations, partition types, and label noise robustness further strengthen the claims. The results are striking, showing massive improvements in non-IID robustness.

**Presentation**
The paper is exceptionally well-written and easy to follow. The core "source-level prevention" philosophy provides a strong, clear narrative that runs through the entire paper. The structure is logical, moving from the high-level motivation to the detailed methodology and then to empirical validation. Figures are clear and effective; in particular, the relative performance plots (e.g., Fig. 4) provide a powerful and intuitive visualization of the method's robustness compared to baselines. The use of tables in the appendix to detail efficiency metrics (Tables 3-6) is also very helpful.

**Contribution**
The paper makes a significant and impactful contribution to the field of federated learning. While the individual components (linear probing, OvA) are not new, their novel combination and application to solve the non-IID problem in FFT is highly effective. The results are dramatic enough to suggest that this "proactive prevention" approach could represent a paradigm shift away from "reactive correction" methods. OvA-LP establishes a new, strong, and surprisingly simple baseline for robust FFT, which is of high practical and academic importance.

**Strengths**
1.  **Simplicity and Elegance:** The method is minimalist and avoids the architectural complexity of many competing approaches, making it easy to understand and implement.
2.  **Exceptional Empirical Performance:** OvA-LP demonstrates outstanding robustness in extreme non-IID settings (retaining 95.9% of IID accuracy vs. 10-35% for SOTA baselines, as shown in Fig. 4) and against label noise (Table 2).
3.  **Clear Conceptual Framework:** The motivation based on the bias-variance decomposition and the "source-level" philosophy is compelling and provides clear insight into why the method works.
4.  **High Efficiency:** The pre-computation of encoder features makes per-round costs extremely low and independent of encoder size, a major practical advantage for deployment (Abstract, Appendix A.2).

**Weaknesses**
1.  **Full Participation Assumption:** The experiments are conducted with 100% client participation. As acknowledged in the limitations (Sec. 5) and shown in the appendix (Fig. 9), performance degrades with lower participation. This limits the direct applicability to common real-world FL scenarios with client subsampling.
2.  **Dependence on Encoder Quality:** The method's success hinges on the quality of the pretrained encoder's feature geometry. The paper acknowledges this (Sec. 5), but it means the method is more about extracting knowledge from a good encoder than learning robust representations itself.

**Questions**
1.  The two-stage training involves just one round of "positive-only" training. What is the sensitivity of the model to this hyperparameter? Have you experimented with more Stage 1 rounds, and how does that affect the speed and stability of convergence?
2.  The brief overshoot in relative performance in the early rounds (e.g., Fig. 3, Fig. 4) is attributed to FedAvg's weighting. While it appears benign here, could this indicate potential instability under different conditions, such as with different optimizers or more local epochs?
3.  How does performance change if the encoder is not completely frozen, but fine-tuned with a very small learning rate? Would this allow the model to adapt to feature drift while still benefiting from the pretrained geometry?

**Rating**
- Overall (10): 9 — The paper presents a simple, highly effective, and efficient method that dramatically advances the state of the art in robust federated fine-tuning.
- Novelty (10): 8 — While the components are known, their combination and the "source-level prevention" philosophy for FFT are novel and insightful.
- Technical Quality (10): 9 — The methodology is sound, and the experimental evaluation is rigorous, extensive, and provides strong support for the claims.
- Clarity (10): 10 — The paper is exceptionally clear, well-organized, and presents its core ideas and results in a compelling and easy-to-understand manner.
- Confidence (5): 5 — I am an expert in this area and am highly confident in my assessment of the paper's strengths and weaknesses.

***

### **Review 2**

**Summary**
This paper proposes OvA-LP, a method for federated fine-tuning (FFT) on non-IID data. The approach combines three main ideas: using a frozen encoder (linear probing), replacing the standard softmax classifier with one-vs-all (OvA) heads, and employing a two-stage training schedule. The authors argue this design prevents client drift at its source and show experimentally that it achieves high accuracy on CIFAR-100 under severe data heterogeneity, significantly outperforming two recent baselines, FFT-MoE and PFPT.

**Soundness**
The methodology itself is straightforward, combining existing techniques. However, the soundness of the experimental evaluation, particularly the baseline comparisons, is questionable. The paper compares OvA-LP (using a ViT-L/16 encoder and 100% client participation) against baselines using different encoders (ViT-B/16, ViT-B/32) and, crucially, different participation rates (PFPT uses 10%, as per Table 8). Comparing a method with full participation against one with 10% participation in an FL setting is not an apples-to-apples comparison, as client sampling is a major source of variance. The dramatic performance gap reported in Figure 4 could be significantly influenced by this experimental mismatch, potentially exaggerating OvA-LP's relative advantage. The bias-variance analysis in Section 3.1 is also presented without formal proof, serving more as a high-level intuition than a rigorous justification.

**Presentation**
The paper is well-written and structured. The core message is communicated clearly. However, the framing of "source-level prevention" versus "post-hoc correction" feels like an oversimplification. Many existing methods, such as FedProx and Scaffold, modify the local client objective to regularize updates and prevent them from drifting too far from the global model, which can also be interpreted as a form of "source-level" intervention. The paper's narrative, while effective, could be more nuanced in its positioning relative to prior work.

**Contribution**
The contribution is primarily empirical. The paper demonstrates that a specific combination of known techniques can be highly effective for non-IID FFT. The novelty of the components themselves is low (linear probing and OvA heads are standard). The main contribution is the finding that freezing the encoder and decoupling the classifier is a surprisingly powerful strategy for mitigating drift, a result that is valuable to the community. However, the claim of introducing a new "philosophy" seems overstated.

**Strengths**
1.  **Simplicity:** The proposed method is simple to implement, which is a significant advantage for practical adoption.
2.  **Strong Ablation Study:** The ablation in Section 4.2 and Figure 3 clearly isolates the contribution of each component (OvA heads and two-stage training), providing convincing evidence for the design choices.
3.  **Thorough Heterogeneity Analysis:** The paper evaluates robustness across a wide variety of non-IID partitions, including label, feature, and quantity skew (Section 4.4.1, Figure 5), which is a definite strength.

**Weaknesses**
1.  **Unfair Baseline Comparison:** The experimental setup for comparing OvA-LP to baselines is inconsistent. Key hyperparameters like client participation ratio (100% for OvA-LP vs. 10% for PFPT) and encoder architecture (ViT-L vs. ViT-B) differ, undermining the validity of the direct performance comparison in Section 4.3.
2.  **Limited Novelty:** The method is a combination of existing, well-known techniques. The paper's claims of being the "first" to address drift at the source (Abstract) are too strong and overlook the conceptual contributions of prior work.
3.  **Weak Theoretical Grounding:** The bias-variance framework (Sec. 3.1) is used as a narrative device but lacks mathematical rigor. The connections between the method's components and their purported effects on bias and variance are asserted rather than proven.

**Questions**
1.  To ensure a fair comparison, could the authors provide results for OvA-LP running with 10% client participation, matching the setup for PFPT? Conversely, how do the baselines perform with 100% participation?
2.  The abstract claims OvA-LP is the "first explicitly designed to suppress drift at its source". How do you distinguish this from methods like FedProx, which adds a proximal term to the local loss `||w - w^t||^2` precisely to prevent local models from drifting away from the global model during local training?
3.  The learning rate for OvA-LP is 0.01, while for baselines it is `1e-4` or `3e-4` (Table 8). Given that OvA-LP only trains a linear head, this difference might be justified, but could the authors comment on the sensitivity to this choice and whether the baselines were tuned with a similarly aggressive learning rate schedule?

**Rating**
- Overall (10): 6 — The paper presents an interesting empirical result, but the claims are weakened by questionable baseline comparisons and overstated novelty.
- Novelty (10): 5 — The method is a novel combination of existing techniques, but the conceptual novelty is limited.
- Technical Quality (10): 6 — The core method is sound, but the experimental design for baseline comparison has significant flaws that affect the main conclusions.
- Clarity (10): 8 — The paper is generally well-written, though its positioning with respect to prior work could be more precise.
- Confidence (5): 5 — I am very familiar with the literature on federated learning and non-IID data and am confident in this assessment.

***

### **Review 3**

**Summary**
This paper introduces OvA-LP, a framework for federated fine-tuning (FFT) that demonstrates remarkable robustness to non-IID data and high efficiency. The method works by freezing the large pretrained encoder, pre-computing feature representations on clients, and then only training lightweight one-vs-all (OvA) linear heads via federated averaging. A simple two-stage training schedule is used to accelerate convergence. The authors show that this minimalist approach is highly effective, maintaining performance close to the IID case under extreme data heterogeneity and significantly reducing computational and communication costs.

**Soundness**
The approach is sound and highly pragmatic. The core idea of freezing the encoder and only training the head is a well-established technique (linear probing), but its application in the federated setting to prevent drift is compelling. The efficiency gains from pre-computing features are a direct and logical consequence of this design. The experiments effectively demonstrate the method's robustness across various non-IID settings and its surprising resilience to label noise. The ablation study (Sec. 4.2) provides clear evidence for the utility of both the OvA heads and the two-stage schedule.

**Presentation**
The paper is clearly written and effectively communicates its primary advantages: robustness and efficiency. The main text focuses on the impressive robustness results, while the appendix provides detailed tables (Tables 5 and 6) that quantify the massive efficiency gains. These efficiency results are a key strength and could have been given more prominence in the main paper. The figures are clean and support the narrative well.

**Contribution**
The primary contribution of this paper is a highly practical, scalable, and effective method for FFT. While not inventing new algorithmic components, the authors have identified a combination that solves a critical problem (non-IID robustness) while also delivering orders-of-magnitude improvements in efficiency (Appendix A.2). In many real-world FL applications, per-round cost and scalability are paramount. By making training cost nearly independent of encoder size, OvA-LP presents a solution that is not just academically interesting but also immediately deployable and economically viable in practice.

**Strengths**
1.  **Exceptional Efficiency:** The pre-computation strategy is a standout feature. It drastically reduces per-round computation and communication costs, making FFT with very large models feasible even with many clients. The reported `10^2`–`10^4` reduction in time-to-convergence cost is a massive practical gain (Table 5).
2.  **Simplicity and Scalability:** The method is simple to implement and avoids complex mechanisms like expert routing or custom aggregation rules. This simplicity, combined with its low cost, makes it highly scalable.
3.  **Strong Robustness:** Despite its simplicity, the method achieves state-of-the-art robustness on extremely non-IID data (Fig. 4) and label noise (Table 2), demonstrating that complexity is not always necessary.

**Weaknesses**
1.  **Full Participation Requirement:** The reliance on 100% client participation is the most significant practical limitation. Real-world FL systems almost always involve sampling a fraction of clients in each round. The appendix (Fig. 9) shows that performance degrades with lower participation, which severely limits its out-of-the-box utility. The authors' argument that the method's efficiency makes full participation feasible is not always true, as availability can be constrained by factors other than cost (e.g., network connectivity, device status).
2.  **Static Data Assumption:** The pre-computation model seems to assume a static dataset on each client. It is unclear how the system would efficiently handle clients receiving new data over time, which is a common scenario. Would clients need to re-run the expensive feature extraction for their entire dataset?
3.  **Dependence on a Strong FM:** The method is fundamentally limited by the quality of the pretrained encoder (Sec. 5). It cannot learn new features, only a linear classifier on top of existing ones. This restricts its use to domains where powerful foundation models are readily available.

**Questions**
1.  The paper argues that OvA-LP's efficiency makes full participation practical. However, in cross-device FL, client availability is often sporadic and unpredictable. Given the performance drop with client sampling (Appendix A.3), do you see a path to making OvA-LP robust to partial participation, perhaps by combining it with server-side momentum or adaptive aggregation schemes?
2.  Could you quantify the one-time cost of the pre-computation step? For a client with a very large local dataset (e.g., millions of images), could this initial feature extraction become a practical bottleneck, even if subsequent rounds are fast?
3.  How does the two-stage training procedure interact with a continuous learning scenario where clients might join or leave the federation? The "first round" being special seems ill-suited for a dynamic, long-running system.

**Rating**
- Overall (10): 7 — A very practical and efficient method with impressive results, but its reliance on full client participation is a major drawback for many real-world applications.
- Novelty (10): 6 — The contribution is more in the clever and effective combination of existing ideas for a practical purpose rather than inventing new concepts.
- Technical Quality (10): 8 — The method is well-designed for efficiency, and the experiments are strong, though the full-participation assumption is a key simplification.
- Clarity (10): 9 — The paper is very clearly written, with a strong focus on the practical benefits of the proposed approach.
- Confidence (5): 5 — I have extensive experience in deploying machine learning systems and am confident in my assessment of the paper's practical implications.

***

### **Review 4**

**Summary**
The paper presents OvA-LP, a framework for federated fine-tuning (FFT) on non-IID data. The authors motivate their approach with a bias-variance decomposition of the federated learning process. They propose to mitigate drift at the source by: 1) freezing the encoder to preserve pretrained feature geometry and limit feature-skew bias; 2) using one-vs-all (OvA) heads to decouple class updates and eliminate label-skew bias from softmax coupling; and 3) using a two-stage training curriculum to control variance. The empirical results show that this simple combination is highly effective in heterogeneous settings.

**Soundness**
The empirical results are strong, but the connection to the theoretical motivation is somewhat tenuous. The bias-variance framework presented in Section 3.1 is descriptive and serves as a high-level justification. The claims about how each component of OvA-LP addresses a specific term in the decomposition are intuitive but not formally proven. For instance, the argument that freezing the encoder "bounds the effect of feature skew" (Sec. 3.2) is plausible but lacks a formal statement or proof. Similarly, the link between the two-stage training and variance reduction (Sec. 3.4) is explained heuristically as an "easy-first, hard-later curriculum" rather than through a formal optimization analysis. While the method works well in practice, its theoretical underpinnings could be strengthened.

**Presentation**
The paper is well-structured and clearly written. The conceptual organization around tackling feature bias, label bias, and variance (Sec. 3) is an effective narrative tool that makes the design choices easy to understand. The figures and tables are clear and support the main claims. However, the paper's central dichotomy of "source-level prevention" vs. "post-hoc correction" is a bit of a false one. Methods like Scaffold or FedProx, which modify the local objective, also aim to prevent drift from arising during local training and could be considered "source-level" interventions.

**Contribution**
The paper's main contribution is empirical: it identifies a simple, non-obvious combination of techniques that yields state-of-the-art results for a challenging problem. The conceptual contribution lies in its clear articulation of how different sources of non-IID-induced drift (feature skew vs. label skew) can be tackled by different components of a learning system. The analysis of softmax coupling as a key mechanism for variance amplification under label skew (Sec. 3.3), building on work like FedRS, is a particularly valuable insight. While the method is not derived from first principles, it provides a new and powerful perspective on designing robust FL algorithms.

**Strengths**
1.  **Clear, Component-wise Motivation:** The paper does an excellent job of explaining the role of each component (frozen encoder, OvA heads, two-stage training) and linking it to a specific problem (feature skew, label skew, variance), as laid out in Section 3.
2.  **Insightful Analysis of Softmax Coupling:** The discussion in Section 3.3 provides a clear and compelling explanation for why standard softmax cross-entropy is problematic under label skew in FL, which is an important insight for the community.
3.  **Thorough Empirical Validation:** The experiments are comprehensive, covering multiple non-IID types (Fig. 5), encoder architectures, and even label noise (Table 2), demonstrating the broad applicability of the method.

**Weaknesses**
1.  **Lack of Formal Analysis:** The theoretical claims are not rigorously supported. The bias-variance decomposition is used as a qualitative framework, and the paper would be much stronger if it could provide formal bounds or a convergence analysis that quantitatively links the method's components to improved performance.
2.  **Overstated Novelty Claims:** The claim of being the "first" to suppress drift at the source (Abstract) is debatable and overlooks the spirit of prior work (e.g., FedProx, Scaffold) that also constrains local updates to prevent divergence.
3.  **Ad-hoc Training Schedule:** The two-stage training, particularly the choice of a single "positive-only" round, feels heuristic. A more principled justification or analysis for this curriculum design would strengthen the paper's argument that it is a systematic way to handle variance.

**Questions**
1.  Could the authors formalize the connection between their method and the bias-variance framework? For example, is it possible to derive a theoretical bound on the local bias term `b_i` (Sec. 3.1) that is a function of the feature geometry metrics (Alignment/Inter from Sec. 3.2)?
2.  The paper argues that OvA decoupling removes cross-class covariance. While the gradients for different weight vectors `w_c` and `w_j` are independent given a single data point `(x, y)`, they are not independent across the client's dataset, as they are trained on the same batches of features `h(x)`. Could this shared source of stochasticity still induce problematic correlations under non-IID data sampling?
3.  How does OvA-LP relate conceptually to methods based on disentangled representation learning? Freezing the feature extractor and learning separate, independent heads seems like a strong form of disentanglement. Could you position your work within that literature?

**Rating**
- Overall (10): 7 — A paper with strong empirical results and an insightful conceptual framework, but which is held back by a lack of theoretical rigor and some overstated claims.
- Novelty (10): 7 — The combination of ideas is novel, and the framing of the problem is insightful, even if the individual components are not new.
- Technical Quality (10): 7 — The empirical work is solid, but the theoretical justification is weak and several design choices appear ad-hoc.
- Clarity (10): 9 — The paper is very well-written and presents its ideas clearly, with a logical and compelling structure.
- Confidence (5): 4 — I am confident in my assessment, though my primary expertise is more on the theoretical side of machine learning.