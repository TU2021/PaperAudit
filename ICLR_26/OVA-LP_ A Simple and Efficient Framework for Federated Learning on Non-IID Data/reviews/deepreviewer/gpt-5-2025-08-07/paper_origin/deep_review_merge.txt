Summary
The paper introduces OvA-LP, a minimalist federated fine-tuning framework designed to suppress client drift at its source under non-IID data. The method freezes the pretrained encoder to preserve representation geometry, replaces the softmax classifier with independent one-vs-all logistic heads to decouple label interactions, and trains the heads in two local stages: first on positives only to align with class centroids, then on positives plus sampled negatives to expand margins. The design is motivated by a bias–variance decomposition of drift, with arguments that pretrained feature geometry bounds feature-skew bias and that OvA reduces label-skew coupling that amplifies drift. Extensive experiments, primarily on CIFAR-100 with 100 clients under multiple non-IID partitions (Dirichlet, shard-1/2, Zipf, feature-skew), show near-IID robustness, strong performance under label noise, and substantial efficiency gains attributed to precomputing features, with per-round costs largely independent of encoder size. Appendices provide absolute accuracies and detailed system metrics.

Strengths
- Simple, modular method leveraging well-established components (frozen encoder, linear probing, OvA heads) with a clear curriculum that aligns with intuitive representation geometry and a bias–variance framing connecting design choices to drift sources.
- Strong empirical performance across diverse non-IID scenarios and label noise, with near-IID robustness reported across partitions, encoder scales, and datasets; results include ablations and multiple seeds.
- Efficiency benefits are convincingly demonstrated with concrete per-round and total cost metrics, showing practical advantages for large backbones via precomputed features.
- Clear presentation with informative figures and comprehensive appendices, including absolute accuracies, system cost breakdowns, and robustness sweeps; limitations are candidly acknowledged.

Weaknesses
- Comparative fairness concerns: baselines differ in backbones and client participation ratios (e.g., PFPT at 0.1 vs OvA-LP at 1.0), confounding convergence speed and system cost comparisons and weakening claims of superiority.
- Theoretical analysis is qualitative; strong statements (e.g., OvA “eliminates” label-skew amplification) are not supported by formal bounds or empirical measurements (such as cross-class gradient covariance under softmax vs OvA).
- Missing implementation details affecting reproducibility and variance control: specifics of Stage 2 (positive/negative ratios), negative sampling policy and class coverage under shard partitions, and sensitivity to these hyperparameters.
- Efficiency claims emphasize per-round cost independence from encoder size but omit quantification of the initial precompute time, storage overhead, and practical handling of data updates requiring recomputation; memory footprint and privacy risks of storing features are not analyzed.
- Scope is limited: evaluation is vision-only with full participation, no study of text/speech/LLM encoders or cross-silo settings, and limited exploration of partial participation and combinations with variance-reducing aggregators or personalization methods.
- Dependence on encoder quality and domain alignment is noted but not systematically quantified; positive-only Stage 1 presumes well-separated features, which may not hold for weaker or shifted encoders.
- Some presentation choices (heavy use of relative ratios R(t) in the main text) obscure absolute performance; while appendices mitigate this, prominent reporting in the main narrative would strengthen interpretability.
