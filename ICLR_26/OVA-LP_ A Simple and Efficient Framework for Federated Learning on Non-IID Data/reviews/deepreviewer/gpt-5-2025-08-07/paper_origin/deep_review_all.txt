Review 1

Summary
The paper proposes OvA-LP, a minimalist federated fine-tuning (FFT) framework intended to suppress client drift at its source for non-IID data. OvA-LP freezes the encoder, performs linear probing, replaces the softmax head with one-vs-all (OvA) binary logistic heads, and uses a two-stage local training schedule (Stage 1: positives only; Stage 2: positives + negatives). The authors motivate the design via a bias–variance decomposition (Sec. 3.1), argue pretrained feature geometry bounds feature-skew bias (Sec. 3.2; Fig. 2), claim OvA decoupling removes label-skew bias/variance amplification (Sec. 3.3), and show the two-stage schedule stabilizes variance (Sec. 3.4). Experiments on CIFAR-100 with 100 clients under several non-IID partitions report near-IID robustness (R(50) ≈ 95–97%) and large gains over baselines FFT-MoE and PFPT (Fig. 4; Table 5), as well as robustness to label noise (Table 2) and efficiency benefits due to precomputed features (Tables 3–4).

Soundness
- The core algorithmic elements are simple and plausible: frozen encoder linear probing and OvA heads are standard components; a positive-first curriculum is reasonable in clustered feature spaces (Sec. 3.4). The bias–variance framing (Sec. 3.1) is conceptually sound and aligns with known FL phenomena.
- However, several claims are stronger than what is formally established. For example, the assertion that OvA “eliminates the mechanism behind label-skew drift” (Sec. 3.3) is argued via the softmax denominator and covariance intuition but lacks quantitative bounds showing reduction in bias/variance under realistic skew. The link between feature geometry metrics (Alignment/Intra/Inter; Fig. 2) and gradient bias bounds is qualitative rather than formal.
- The two-stage schedule rationale (Stage 1 toward centroids; Stage 2 margin expansion) is persuasive but not theoretically analyzed (no margin or convergence guarantees). Negative sampling specifics (ratio, class coverage) are not fully specified in the main text, which matters for reproducibility and variance control.
- Self-verification: cross-checking Appendix shows per-round cost reductions consistent with precomputed features (Table 4) and Acc@95 convergence gaps (Table 3), supporting the efficiency claims. Baseline differences (Table 8: PFPT 0.1 participation, different backbones) introduce fairness concerns that could inflate the reported advantage; the paper acknowledges reproducing baselines “as in original papers” (Sec. 4.3, App. B.2), but direct apples-to-apples comparisons are limited.

Presentation
- The paper is clearly organized: motivation (Sec. 1), related work positioning (Sec. 2), method with sub-analyses (Sec. 3.1–3.4), and comprehensive experiments (Sec. 4.1–4.4, Appendices).
- Figures are informative: workflow schematics (Fig. 1; Blocks 12–13), geometry comparison (Fig. 2), ablation and baseline curves (Figs. 3–4, 7–8), robustness across partitions and encoders (Figs. 5–6), participation sweep (Fig. 9).
- Some plots use relative ratios R(t) without reporting all absolute accuracies in the main text; the appendix provides absolute curves (Fig. 7–8), which helps. Text occasionally overstates causal claims (e.g., “eliminates”) without caveats.

Contribution
- Unifying frozen-encoder linear probing with OvA heads and a two-stage curriculum for FFT under non-IID is a practical contribution. Prior OvA FL methods focused on scratch training and label imbalance (Sec. 2; Blocks 8, 10), while OvA-LP targets drift suppression at the source within PEFT/FFT.
- The empirical finding that such a minimalist approach achieves near-IID robustness across varied non-IID patterns (Fig. 5) and label noise (Table 2) is significant if the comparisons are fair and reproducible.
- The efficiency angle—precompute features once, making per-round cost nearly independent of encoder size (Sec. 3 Overview; App. Tables 3–4)—adds practical value.

Strengths
- Clear, simple method with strong empirical results across multiple non-IID settings (Sec. 4.2–4.4; Figs. 3–6).
- Conceptual framing via bias–variance decomposition that guides design choices (Sec. 3.1–3.4).
- Robustness to label noise beyond specialized baselines (Table 2).
- Efficiency demonstrated with concrete per-round and total cost metrics (Tables 3–6).

Weaknesses
- Fairness of baseline comparisons: different encoders and participation ratios (Table 8; PFPT at 0.1 vs OvA-LP at 1.0; FFT-MoE with different backbone), potentially confounding gains (Sec. 4.3).
- Lack of formal analysis quantifying bias/variance reduction; claims of “elimination” may be overstated (Sec. 3.3).
- Reproducibility gaps: missing details on negatives sampling policy, proportions retained in Stage 2, memory/storage overhead of precomputed features; privacy considerations of storing features are unaddressed.
- Limited evaluation beyond vision and full participation; the limitation is acknowledged (Sec. 5), but practical FL often uses partial participation and variable client availability.

Questions
- What exact ratios are used in Stage 2 for positives vs negatives per head? How are negatives sampled across classes on clients with shard-1/shard-2 label sets?
- How much memory do clients need to store precomputed features, and how is privacy risk handled (feature leakage)? Any compression applied?
- Can you provide an ablation on encoder strength (e.g., weakly pretrained vs strong) quantifying the dependence noted in Sec. 5?
- Can you run matched-baseline comparisons with the same backbone and full participation to confirm the relative gains?
- How does OvA-LP perform under partial participation beyond the brief note in App. A.3 (more rounds, different aggregation like SCAFFOLD)?

Rating
- Overall (10): 8 — Strong empirical robustness and a practical source-level design, but fairness and formal guarantees are limited (Sec. 4.3; Table 8; Sec. 3.3).
- Novelty (10): 7 — A thoughtful unification of known components for FFT with a new bias–variance lens; prior OvA works differ in setting (Sec. 2; Blocks 8–10).
- Technical Quality (10): 7 — Sound implementation and thorough experiments, yet theoretical claims are mostly qualitative (Sec. 3.1–3.4; Fig. 2; Table 2).
- Clarity (10): 8 — Well-written with informative figures and appendices, minor overstatements and missing hyperparameter details (Figs. 3–6; Tables 3–6; Sec. 3.4).
- Confidence (5): 4 — High due to detailed appendices and multiple seeds, tempered by baseline comparability concerns (Sec. 4.1–4.3; App. B.2; Table 8).


Review 2

Summary
The paper presents OvA-LP, a federated fine-tuning strategy for non-IID data that freezes the encoder, performs linear probing, replaces softmax with independent logistic heads, and trains in two stages. The method is motivated by a bias–variance decomposition of drift (Sec. 3.1), argues that pretrained feature geometry limits feature skew (Sec. 3.2), and that OvA heads decouple label interactions that amplify drift (Sec. 3.3). Experiments show near-IID performance across non-IID partitions and label noise, and substantial efficiency gains (Figs. 3–6; Tables 3–6).

Soundness
- The method is coherent and uses standard components; the two-stage curriculum is plausible in clustered feature spaces. The efficiency via precomputed features is well supported by per-round metrics (Table 4).
- However, comparative claims are undermined by non-uniform baselines: different encoders and client participation ratios for PFPT and FFT-MoE (Table 8) mean system costs and convergence rates are not directly comparable to OvA-LP. The main text highlights failures of baselines (Fig. 4), but some of this may be attributable to configuration differences rather than algorithmic weakness.
- The theoretical analysis is descriptive. The covariance argument against softmax (Sec. 3.3) is reasonable but lacks empirical measurement (e.g., estimating Cov(g_c, g_j) under skew) or formal bounds. The connection from Fig. 2 geometry to gradient bias reduction lacks a derivation.

Presentation
- Clear narrative and helpful figures (workflow, geometry, ablations). The use of R(t) is convenient, but absolute accuracies for baselines and OvA-LP should be presented prominently (Appendix does provide them: Fig. 7–8; Tables 5–6).
- The paper is upfront about limitations (Sec. 5), which is appreciated. Some terms (e.g., “eliminate”) could be toned down or supported with measurements.

Contribution
- The source-level drift suppression idea within FFT/PEFT, implemented by an OvA linear head and two-stage training, is a practical contribution. The demonstration that simple components can achieve robustness comparable to or better than complex post-hoc methods is valuable.
- If the performance gains hold under matched experimental settings, this would represent a significant shift in FFT design for heterogeneous FL.

Strengths
- Strong empirical results across non-IID partitions and label noise (Figs. 4–6; Table 2).
- Simple, efficient method with concrete system metrics (Tables 3–4).
- Clear bias–variance framing connecting design choices to drift sources (Sec. 3.1–3.4).

Weaknesses
- Baseline comparability: differing backbones and participation ratios (Table 8) reduce the strength of the claims; PFPT at 0.1 participation vs OvA-LP at 1.0 is particularly problematic for convergence speed comparisons (Sec. 4.3; App. B.2).
- Missing details for key steps: how negatives are selected, class coverage under shard partitions, and ratios in Stage 2 (Sec. 3.4).
- No evaluation on text or speech foundation models or cross-silo FL; all vision and full participation (Sec. 5).
- The claim that per-round cost is “nearly independent of encoder size” (Abstract; Sec. 3) may omit the initial precompute cost and storage overhead; memory footprint is not quantified.

Questions
- Can you provide matched-backbone experiments (e.g., ViT-L/16 across all methods) and uniform participation ratios to validate the reported robustness and efficiency?
- What is the memory cost per client for storing precomputed features, and how does it scale with dataset size? Any compression applied?
- How are negatives selected in Stage 2 for shard-1 clients with only one (or two) classes locally?
- Have you measured cross-class gradient covariances under softmax vs OvA empirically to support the Sec. 3.3 argument?
- How does OvA-LP handle class expansion or dynamic class sets at deployment (new labels)?

Rating
- Overall (10): 6 — Promising and simple approach, but baseline comparability and missing details weaken the strength of claims (Sec. 4.3; Table 8; Sec. 3.4).
- Novelty (10): 6 — The unification is useful, though components are known; positioning in FFT/PEFT is a contribution (Sec. 2; Blocks 8–10).
- Technical Quality (10): 6 — Good engineering and thorough experiments, but comparisons are confounded and theory is qualitative (Fig. 4; Tables 5–6; Sec. 3.1–3.4).
- Clarity (10): 7 — Clear structure and figures; some crucial hyperparameters and system costs (storage) missing (Sec. 3.4; Abstract; Tables 3–4).
- Confidence (5): 4 — Moderate confidence based on detailed appendices, but concerns about fairness and reproducibility remain (App. B.2; Table 8).


Review 3

Summary
OvA-LP aims to make FFT robust under non-IID conditions by preventing drift at its origin. It freezes the encoder and performs linear probing with OvA binary heads, trained via a two-stage schedule. The authors motivate this via bias–variance decomposition (Sec. 3.1), argue pretrained geometry constrains feature-skew bias (Sec. 3.2), show OvA decoupling mitigates label-skew coupling (Sec. 3.3), and propose a curriculum to manage variance (Sec. 3.4). Extensive experiments on CIFAR-100 with 100 clients report near-IID performance across non-IID partitions and label noise, plus efficiency advantages (Figs. 3–6; Tables 3–6).

Soundness
- The algorithm leverages robust pretrained features and known benefits of OvA heads in imbalanced settings. The two-stage schedule is well motivated by the geometry of representations (centroid alignment followed by margin expansion).
- Empirical evidence is broad: multiple partitions (Dirichlet, shard-1/2, Zipf, feature-skew; Fig. 5), encoder scales and datasets (Fig. 6), label noise (Table 2), and ablations (Fig. 3; Tables 3–4).
- Caveats: The theoretical analysis is heuristic; formal quantification of bias–variance reduction is not provided. The baseline comparison fairness is questionable due to different backbones and participation ratios (Table 8), although the paper is transparent about reproducing baselines per original configurations.

Presentation
- Very clear exposition; figures succinctly convey ideas and results. Appendices provide absolute accuracies and cost breakdowns (Figs. 7–8; Tables 5–6).
- Limitations are candidly discussed (Sec. 5), including dependence on encoder quality and full participation.

Contribution
- The key contribution is showing that source-level interventions (frozen encoder + OvA heads + simple curriculum) can deliver near-IID robustness in FFT without complex aggregation or personalization. This is a compelling practical insight for federated adaptation of foundation models.
- The efficiency angle (precompute features) is important for large backbones.

Strengths
- Strong empirical performance across diverse non-IID scenarios (Fig. 5; R(50) ≥ 94.9%).
- Label noise robustness surpassing prior SOTA (Table 2).
- Simple, modular design; compatible with existing FFT pipelines (Sec. 2 positioning; Sec. 6).
- Detailed system metrics and seeds for reproducibility (Sec. 4.1; Tables 3–6).

Weaknesses
- Non-matched baselines undermine some comparative claims (Table 8). A matched-backbone/full-participation study is needed.
- Lacks discussion of storage/privacy costs of precomputed features and handling data drift (new samples require re-precompute).
- Theory is qualitative; the claim of “elimination” of label-skew mechanisms could be empirically validated via gradient covariance measurements.
- Limited to vision; generalization to text/LLMs is not tested (Sec. 5).

Questions
- Could you add experiments with matched backbones and identical participation ratios to quantify gains attributable to OvA-LP rather than settings?
- What are the exact positive/negative sampling ratios in Stage 2, and how sensitive are results to these hyperparameters?
- How do you handle feature updates when clients acquire new data after precompute? Is there a scheduled recomputation?
- Any results combining OvA-LP with SCAFFOLD/FedProx to handle partial participation variance (Sec. 5)?
- Can you report calibration metrics (e.g., ECE) for OvA vs softmax heads under non-IID and label noise?

Rating
- Overall (10): 9 — Compelling empirical evidence for a simple, source-level approach to robust FFT, with some caveats on baseline fairness and theory (Fig. 5; Table 2; Table 8).
- Novelty (10): 8 — New in FFT/PEFT to unify OvA LP and a two-stage schedule for drift prevention; prior OvA work focused on scratch training (Sec. 2; Blocks 8–10).
- Technical Quality (10): 8 — Strong experiments and system analysis, though theory is qualitative and fairness concerns remain (Figs. 3–6; Tables 3–6; Sec. 3.1–3.4).
- Clarity (10): 9 — Clear writing, visualizations, and appendices; limitations stated (Sec. 5; Figs. 7–8).
- Confidence (5): 3 — Good overall but tempered by non-matched baselines; would like to see matched comparisons (Table 8; Sec. 4.3).


Review 4

Summary
The authors propose OvA-LP, a source-level method for federated fine-tuning on non-IID data. It freezes the encoder to preserve feature geometry, uses independent OvA logistic heads to decouple labels, and adopts a two-stage training schedule to control variance. They justify each component within a bias–variance decomposition (Sec. 3.1–3.4) and present experiments showing near-IID robustness across multiple non-IID partitions, label noise, encoders, and datasets, with efficiency gains from precomputing features (Figs. 3–6; Tables 3–6).

Soundness
- The method is sensible and grounded in known ML practices. The bias–variance framing provides a coherent narrative but stops short of formal guarantees. For example, Sec. 3.3 claims OvA decoupling “eliminates” cross-class covariance-driven amplification, but OvA heads are trained on the same features and share global aggregation; independence of losses does not imply zero covariance in global updates.
- Stage 1 positive-only training presumes well-separated pretrained features; this assumption is validated on ViT backbones (Fig. 2) but may fail for weaker encoders or domain shift. Sec. 5 acknowledges dependence on encoder quality.
- Efficiency claims focus on per-round costs (Tables 3–4), but initial precompute time and storage requirements are not quantified, and privacy implications of storing features are not discussed.

Presentation
- The manuscript is well organized and uses effective figures. The R(t) ratio makes trends easy to compare; appendices provide absolute values and system metrics (Figs. 7–8; Tables 5–6).
- The limitations section is thorough (Sec. 5). Some methodological details are missing in the main text (exact Stage 2 ratios, negative sampling policy).

Contribution
- A practical and modular FFT approach that aims to prevent drift at the client side rather than correct it post hoc. The demonstrated robustness under diverse non-IID and label noise settings is potentially impactful if confirmed under more rigorous comparisons.
- The notion that precomputing features yields encoder-size-agnostic per-round cost is useful for scaling FFT.

Strengths
- Clear design, strong empirical performance, and efficiency analysis (Figs. 3–6; Tables 3–6).
- Broad robustness tests: partitions, encoders, label noise (Fig. 5; Fig. 6; Table 2).
- Honest discussion of limitations (Sec. 5), including partial participation.

Weaknesses
- Comparative fairness: baselines use different backbones and participation (Table 8). This confounds the large reported gains (Fig. 4; Tables 5–6).
- Overstated theoretical claims without quantitative bounds (Sec. 3.3); lack of empirical covariance measurements.
- Storage/privacy not addressed for precomputed features; potential leakage risks in FL contexts.
- Limited generality: vision-only; no combination with aggregation/personalization methods yet (Sec. 5).

Questions
- Can you provide matched-backbone, matched-participation experiments for FFT-MoE and PFPT to validate the stepwise robustness ranking in Table 1?
- What is the storage footprint per client for features, and how does OvA-LP handle data updates and feature recomputation policies?
- Have you tested partial participation beyond App. A.3 with variance-reducing aggregators (SCAFFOLD/FedProx)?
- Can you quantify the sensitivity of OvA-LP to encoder quality (e.g., random or weakly pretrained encoders), beyond Fig. 2?
- Could you report class-wise performance under shard-1/shard-2 to confirm minority-class improvements claimed for OvA decoupling?

Rating
- Overall (10): 7 — A practical, well-executed approach with promising results, but baseline comparability and theoretical rigor limit the strength of conclusions (Fig. 4; Table 8; Sec. 3.3).
- Novelty (10): 7 — The source-level unification in FFT/PEFT is new relative to prior OvA works focused on label imbalance (Sec. 2; Blocks 8–10).
- Technical Quality (10): 6 — Solid engineering and experiments, but confounded comparisons and qualitative theory (Tables 5–6; Sec. 3.1–3.4).
- Clarity (10): 8 — Clear and thorough presentation with useful appendices, minor missing details (Figs. 3–6; Sec. 3.4).
- Confidence (5): 5 — High confidence in the identified issues and in the empirical trends based on detailed appendices and anchors (Table 8; Sec. 5; Figs. 7–8).