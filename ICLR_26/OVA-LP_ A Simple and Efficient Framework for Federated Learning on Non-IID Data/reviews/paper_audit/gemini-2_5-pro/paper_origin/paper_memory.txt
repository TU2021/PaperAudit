# Global Summary
This paper introduces OvA-LP, a framework for federated fine-tuning (FFT) designed to be robust against non-IID data distributions. The core problem it addresses is "local drift," where client updates diverge, introducing bias and variance into the global model. Unlike prior methods that correct drift post-hoc, OvA-LP aims to prevent it at the source. The method combines three components: 1) linear probing on a frozen pretrained encoder to preserve feature geometry, 2) a one-vs-all (OvA) classification head to decouple class logits and prevent label-skew-induced drift, and 3) a two-stage training schedule to control variance.

Experiments are primarily conducted on CIFAR-100 with 100 clients under various non-IID partitions (Shard-1, Shard-2, Dirichlet). The main quantitative claim is that OvA-LP retains 95.9% of its IID accuracy on average across these partitions, whereas state-of-the-art baselines FFT-MoE and PFPT retain only 34.5% and 10.1%, respectively. The framework also shows robustness to label noise and is computationally efficient, as encoder features are precomputed once. Key limitations stated by the authors are a strong reliance on the quality of the pretrained encoder and the assumption of full client participation in all experiments.

# Abstract
- **Problem:** Federated fine-tuning (FFT) of foundation models is fragile under heterogeneous client data (non-IID) due to local drift, which causes bias and variance in the global model. Existing methods correct drift post-hoc and are brittle in extreme non-IID conditions.
- **Method:** The paper proposes OvA-LP, a framework designed to suppress drift at its source. It combines linear probing on a frozen encoder, a one-vs-all (OvA) head, and a two-stage training procedure. This approach preserves pretrained feature geometry and decouples logits.
- **Key Results:** On CIFAR-100 with 100 clients (averaged over shard-1, shard-2, and Bernoulli–Dirichlet partitions), OvA-LP retains 95.9% of its IID accuracy. In contrast, baselines PFPT and FFT-MoE retain only 10.1% and 34.5%, respectively.
- **Other Claims:** OvA-LP is resilient to symmetric and asymmetric label noise. Its per-round cost is nearly independent of encoder size due to precomputation of features.

# Introduction
- **Context:** Federated fine-tuning (FFT) using parameter-efficient fine-tuning (PEFT) methods is a practical paradigm for adapting foundation models on decentralized data. However, its robustness to data heterogeneity is a major challenge.
- **Core Problem:** Local drift, where client updates diverge due to non-IID data, introduces systematic bias and amplified variance into the aggregated global model. This leads to slow convergence and poor accuracy, with state-of-the-art methods retaining less than half their IID accuracy in 50 rounds.
- **Prior Work:** Existing solutions are categorized into aggregation strategies (e.g., FedProx, Scaffold) and personalization frameworks (e.g., adapters, experts). The paper argues these are "post-hoc" corrections that treat drift as unavoidable.
- **Proposed Solution (OvA-LP):** A minimalist framework that suppresses drift at its root. It combines linear probing (LP) on a frozen encoder, one-vs-all (OvA) binary heads, and a two-stage training schedule. This is framed as a proactive "source-level" prevention of drift.
- **Main Findings:**
    - OvA-LP prevents local drift at the client level.
    - On CIFAR-100 (100 clients, various non-IID partitions), OvA-LP retains 95.9% of IID accuracy, while FFT-MoE and PFPT retain 34.5% and 10.1%.
    - OvA-LP is robust to label noise.
    - It is efficient due to precomputing encoder features.

# Related Work
- **Aggregation strategies:** Methods like FedProx, Scaffold, FLoRA, and FedSA-LoRA modify the global update rule but act after local divergence has occurred.
- **Personalization frameworks:** Client-specific modules like FedAdapter, FedPrompt, and FFT-MoE absorb drift locally but cannot prevent it from propagating to the shared model.
- **Classification heads for label imbalance:** Methods like FedRS, FedOVA, FedABC, and ATHENA-FL modify the head to handle skewed labels, but are typically designed for training from scratch and focus on label imbalance rather than general feature drift.
- **Label noise robustness:** Orthogonal methods like FedCorr and FedLTF design specific mechanisms to handle noisy labels.
- **Positioning:** OvA-LP is presented as a novel combination designed to prevent drift by freezing the encoder and using a two-stage OvA head, making it distinct from prior OvA-based methods. It is modular and potentially compatible with other families of methods.

# Method
- **Overall Philosophy:** Prevent drift at its source rather than correcting it post-hoc.
- **Workflow (Fig. 1):** Clients precompute features using a frozen encoder once. Then, they perform a two-stage local training of one-vs-all (OvA) heads.
- **Bias–Variance Framework:** The method is motivated by decomposing federated gradients into local bias (from feature/label skew), global bias, and variance. OvA-LP targets each component.
- **Linear Probing and Feature Geometry:**
    - Freezing the encoder preserves the pretrained feature geometry, which is characterized by good alignment (small distance between same-class samples) and separation (large distance between different-class centroids).
    - This geometry bounds the bias from feature skew.
    - Quantitative metrics on CIFAR-10 (ViT-L/16) show a pretrained encoder has better Alignment (1.366 vs 1.761), Intra-class distance (0.818 vs 0.917), Inter-class distance (0.840 vs 0.487), and Ratio (0.974 vs 1.885) compared to a randomly initialized one.
- **OvA Head and Decoupling:**
    - Standard softmax heads cause "coupling," where updates for one class are affected by all other classes. This introduces bias and amplifies variance under label skew.
    - OvA heads use independent binary classifiers (logistic loss) for each class, decoupling the gradients. This eliminates the mechanism of label-skew-induced drift.
- **Two-Stage Training for Variance Control:**
    - **Stage 1 (positive-only):** Clients train each OvA head only on positive examples for that class. This quickly pulls the classifier weights toward the class centroids with minimal variance.
    - **Stage 2 (positive+negative):** Clients train on a large set of negatives to expand margins, while retaining a small fraction of positives as anchors to maintain stability.
    - This curriculum implements an easy-first, hard-later strategy to control the destabilizing effects of variance.

# Experiments
- **Experimental Setup:**
    - **Dataset/Clients:** CIFAR-100 with 100 clients, trained for 50 rounds.
    - **Seeds:** 5 fixed random seeds.
    - **Partitions:** IID and three non-IID settings (Shard-1, Shard-2, Dirichlet with p=0.1, α=0.001).
    - **OvA-LP Model:** Frozen ViT-L/16 encoder, 100% client participation, 3 local epochs, LR 0.01, AdamW. Stage 1 is used for the first round, Stage 2 for all subsequent rounds.
    - **Baselines:** FFT-MoE and PFPT, reproduced using their original architectures (e.g., ViT-B/32, ViT-B/16).
    - **Evaluation:** Relative performance R(t) = AccNonIID(t)/AccIID(t) * 100.
- **Ablation Study (Fig. 3):**
    - On CIFAR-100 (averaged non-IID), performance improves stepwise:
        - LP-Softmax (frozen encoder only): 56.3% relative accuracy at round 50 (R(50)).
        - OvA-LP w/o 2 stage (adds OvA head): 95.4% R(50).
        - OvA-LP 2 stage (full model): 95.9% R(50).
- **Comparison with Baselines (Fig. 4, Table 1):**
    - Under the averaged extreme non-IID setting:
        - FFT-MoE: plateaus near 10.1% R(50).
        - PFPT: saturates at 34.5% R(50).
        - OvA-LP: converges to 95.9% R(50).
    - The paper attributes this to OvA-LP addressing bias and variance at the source, while baselines do not.
- **Additional Analyses:**
    - **Partition-wise Robustness (Fig. 5):** OvA-LP maintains high relative accuracy across five partitions: Dirichlet (94.9%), Shard-1 (96.1%), Shard-2 (96.7%), Zipf=2 (97.7%), and Feature Skew (99.7%).
    - **Encoder and Task Variations (Fig. 6):** Robustness holds across different encoders (ViT-B/16, ViT-L/16, DINOv2-L/14) and datasets (CIFAR-100, TinyImageNet). Relative performance remains high in all cases.
    - **Label Noise Robustness (Table 2):** On CIFAR-100 with label noise, OvA-LP shows a much smaller accuracy decline rate than prior SOTA (FedLTF) and other baselines. For example, with 70% symmetric noise, OvA-LP's decline is 10.35% vs. 20.91% for FedLTF. With 40% asymmetric noise, the decline is 1.53% vs. 24.80% for FedLTF.

# Conclusion
- **Summary:** OvA-LP is a minimalist FFT framework that prevents client drift at its source by combining linear probing, an OvA head, and a two-stage training schedule. It achieves near-IID accuracy in highly non-IID settings and is robust to label noise.
- **Limitations:**
    1.  The method's effectiveness depends heavily on the quality (alignment and separation) of the pretrained encoder's features.
    2.  All experiments assume full client participation, which abstracts away from the variance introduced by partial participation.
- **Future Work:** The paper suggests combining OvA-LP with existing aggregation or personalization frameworks and extending it beyond vision benchmarks.
- **Positioning:** OvA-LP is presented as a strong new baseline and a step toward source-level robustness in FFT, complementary to existing post-hoc methods.

# Appendix
- **A.1 Ablation Results:**
    - Under non-IID conditions, the full OvA-LP (2 stage) reaches 95% of its final accuracy (Acc@95) in 1 round, with a total time of 0.02s.
    - In contrast, LP-Softmax takes 27 rounds and 0.73s. OvA-LP w/o 2 stage takes 15 rounds and 0.37s.
    - Per-round client time for OvA-LP (2 stage) is 14.9ms, compared to 23.8ms for LP-Softmax.
- **A.2 Baseline Comparisons:**
    - Under non-IID, OvA-LP reaches Acc@95 in 1 round (total time 0.02s).
    - FFT-MoE takes 37 rounds (61.00s) and PFPT takes 44 rounds (1437.00s).
    - Per-round client time for OvA-LP is ~14.9ms, versus ~1556ms for FFT-MoE and ~1855ms for PFPT.
    - OvA-LP's non-IID accuracy is 86.34%, while FFT-MoE is 9.83% and PFPT is 33.17%.
- **A.3 Participation Rate Sweep:**
    - Experiments on a Dirichlet partition show that lower client participation rates slow convergence.
    - Final relative accuracies (R(50)) are: 91.4% (0.1 ratio), 96.8% (0.4 ratio), 97.5% (0.7 ratio), and 97.7% (1.0 ratio).
- **B Baseline Settings:**
    - The experimental setup of 100 clients is noted as comparable to or larger than recent FL benchmarks (Table 7).
    - Detailed hyperparameters for baselines are provided (Table 8). PFPT uses ViT-B/32, 5 local epochs, and 0.1 active client ratio. FFT-MoE uses ViT-B/16, 1 local epoch, and full participation.

# References
- The paper cites relevant work in federated learning, parameter-efficient fine-tuning, non-IID data handling, and foundation models. Key references include McMahan et al. (FedAvg), Li et al. (FedProx), Karimireddy et al. (SCAFFOLD), and recent FFT papers like Weng et al. (PFPT) and Hu et al. (FFT-MoE).