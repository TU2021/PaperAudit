{
  "baseline_review": "1) Summary\nThis paper addresses the problem of local drift in federated fine-tuning (FFT), where client models diverge on non-IID data, degrading the global model. The authors propose OvA-LP, a framework designed to prevent drift at its source rather than correcting it post-hoc. The method combines three components: linear probing (LP) on a frozen pretrained encoder to leverage its stable feature geometry, one-vs-all (OvA) classification heads to decouple class updates and mitigate label skew, and a two-stage training curriculum to control variance. Experiments on CIFAR-100 with 100 clients show that OvA-LP retains 95.9% of its IID accuracy under severe non-IID conditions, significantly outperforming state-of-the-art baselines like PFPT (10.1%) and FFT-MoE (34.5%). The method also demonstrates strong robustness to label noise and high computational efficiency.2) Strengths\n*   **Exceptional Empirical Performance and Robustness**\n    *   The proposed method achieves near-IID performance under extreme non-IID conditions, retaining 95.9% of its IID accuracy on average across challenging partitions (Abstract; Figure 4). This is a dramatic improvement over recent baselines FFT-MoE (10.1%) and PFPT (34.5%).\n    *   The method's robustness is demonstrated across a wide variety of data heterogeneity settings, including label skew (Shard-1, Shard-2, Dirichlet), quantity skew (Zipf), and feature skew (k-means clustering), consistently maintaining over 94.9% relative accuracy (Section 4.4.1; Figure 5).\n    *   OvA-LP shows remarkable robustness to label noise, outperforming specialized state-of-the-art methods like FedLTF by a large margin across various symmetric and asymmetric noise ratios (Section 4.4.3; Table 2).\n    *   The performance gains are consistent across different encoder architectures (ViT-B/16, ViT-L/16, DINOv2-L/14) and datasets (CIFAR-100, TinyImageNet), suggesting the approach is not overly sensitive to model choice (Section 4.4.2; Figure 6).*   **Simple, Well-Motivated, and Interpretable Method**\n    *   The framework is conceptually simple, combining three well-established techniques (linear probing, OvA classifiers, two-stage training) into a novel and effective pipeline (Section 3; Figure 1).\n    *   The design is clearly motivated by a bias-variance decomposition of federated gradients (Section 3.1). Each component directly targets a specific source of drift: frozen encoder for feature-skew bias (Section 3.2), OvA heads for label-skew bias and variance (Section 3.3), and the two-stage schedule for optimization variance (Section 3.4).\n    *   The connection between the method's components and their effect on drift is clearly articulated and summarized effectively in Table 1, which provides a clear conceptual map of why the proposed method works better than alternatives.*   **Thorough Ablation and Analysis**\n    *   The paper includes a strong ablation study that clearly isolates the contribution of each component. The stepwise performance gains from LP-Softmax (56.3%) to OvA-LP without the two-stage schedule (95.4%) to the full OvA-LP (95.9%) provide compelling evidence for the efficacy of both the OvA decoupling and the two-stage training (Section 4.2; Figure 3).\n    *   The analysis of feature geometry (Section 3.2; Figure 2) provides a quantitative justification for freezing the encoder, showing that pretrained features are significantly more aligned and separated than randomly initialized ones.\n    *   The appendix provides detailed absolute accuracy curves (Appendix A.1, Figure 7; Appendix A.2, Figure 8) and efficiency metrics (Tables 3-6), which complement the relative performance plots in the main text and add depth to the experimental validation.*   **High Computational and Communication Efficiency**\n    *   By precomputing encoder features once, the per-round training cost becomes nearly independent of the encoder's size, making the approach highly scalable (Abstract; Section 1).\n    *   The method is shown to be orders of magnitude faster than baselines. For example, under non-IID conditions, OvA-LP reaches 95% of its final accuracy in just one round and 0.02 seconds, while PFPT requires 44 rounds and 1437 seconds (Appendix A.2; Table 5).\n    *   Per-round client and server computation times are drastically lower (14.86ms and 1.91ms for OvA-LP vs. >1500ms for baselines), making the approach practical for resource-constrained edge devices (Appendix A.2; Table 6).3) Weaknesses\n*   **Limited Scope and Potential Unfairness of Baseline Comparisons**\n    *   The paper compares against only two recent methods, FFT-MoE and PFPT (Section 4.3). While these are relevant, the comparison does not include more classical FL algorithms like FedProx or Scaffold, which are designed to handle heterogeneity and would provide a broader context for the results. No direct evidence found in the manuscript for why these were excluded.\n    *   There is a fundamental paradigm difference between OvA-LP and the baselines that is not fully discussed. OvA-LP freezes the entire encoder and only trains a linear head, whereas methods like PFPT (prompt-tuning) and LoRA-based approaches (referenced in Section 2) adapt parts of the encoder. OvA-LP effectively sidesteps the harder problem of federated encoder adaptation, making the direct comparison of non-IID robustness potentially misleading. The experimental setup for baselines also differs (e.g., PFPT uses 10% client participation while OvA-LP uses 100%) (Table 8).*   **Over-reliance on Full Client Participation**\n    *   All main experiments are conducted with 100% client participation (Section 4.1; Section 2.5). This is an idealized setting that is rare in practical cross-device FL deployments, where client availability is sporadic and partial participation is the norm.\n    *   The paper acknowledges this limitation and shows in the appendix that performance degrades with lower participation rates, with convergence slowing significantly at a 10% ratio (Section 5; Appendix A.3, Figure 9). This suggests the method's core variance-reduction strategy is less effective in the common partial-participation setting.\n    *   The argument that high efficiency makes full participation \"operationally feasible\" (Section 5) is a valid point for some cross-silo scenarios but does not address the fundamental challenge in cross-device settings.*   **Uncertain Generalizability Beyond Vision Tasks**\n    *   All experiments are conducted on image classification datasets (CIFAR-100, TinyImageNet) (Section 4). The method's success relies heavily on the strong, well-separated feature geometry of pretrained vision models (Section 3.2).\n    *   It is unclear how well this approach would transfer to other modalities like NLP, where fine-tuning parts of the encoder is often critical for achieving high performance on downstream tasks. The assumption that a frozen encoder is sufficient may not hold.\n    *   The paper notes this as a direction for future work (Section 5), but the lack of even preliminary results or a deeper theoretical discussion on its applicability to other domains limits the perceived generality of the contribution.4) Suggestions for Improvement\n*   **Expand and Contextualize Baseline Comparisons**\n    *   To provide a more comprehensive evaluation, please include experiments comparing OvA-LP against foundational FL algorithms like FedProx and Scaffold, adapted to the same federated fine-tuning setting (i.e., training only the head).\n    *   Please add a discussion that explicitly frames the paradigm difference. Acknowledge that OvA-LP is a \"head-tuning\" method and compare it to \"encoder-tuning\" baselines. This would clarify that the contribution is showing the surprising effectiveness of a simpler approach in high-heterogeneity regimes, rather than claiming superiority on an identical task. Harmonizing experimental settings like participation ratio (Table 8) would also strengthen the comparison.*   **Investigate and Address Partial Participation Effects**\n    *   The results on partial participation are important and should be moved from the appendix (Appendix A.3, Figure 9) to the main experimental section to present a more balanced picture of the method's capabilities and limitations.\n    *   Please provide an analysis of *why* performance degrades. For instance, does the two-stage strategy become unstable, or is it simply a matter of slower convergence due to increased variance from client sampling?\n    *   Discuss how this limitation could be addressed. For example, could OvA-LP be combined with aggregation rules like FedProx or momentum-based methods on the server to mitigate the variance introduced by partial participation? Exploring this would significantly enhance the practical relevance of the work.*   **Provide Evidence or Deeper Discussion on Generalizability**\n    *   To strengthen the paper's claims of being a general framework, please consider adding a preliminary experiment on a standard federated NLP benchmark (e.g., text classification).\n    *   If new experiments are not feasible, please expand the discussion in Section 5. For example, you could analyze the feature geometry of popular pretrained language models (similar to Figure 2 for ViT) and discuss whether the core assumptions of OvA-LP are likely to hold, citing relevant literature on linear probing in LLMs.5) Score\n*   Overall (10): 8 — The paper presents a simple, elegant, and highly effective method for a critical problem, supported by exceptionally strong empirical results and thorough analysis.\n*   Novelty (10): 7 — While the components are not new, their combination and the \"source-level drift prevention\" framing for FFT are novel and insightful.\n*   Technical Quality (10): 8 — The methodology is well-motivated by the bias-variance framework, and the experiments are rigorous, though limited by the baseline scope and full-participation assumption.\n*   Clarity (10): 9 — The paper is exceptionally well-written, clearly structured, and easy to follow, with figures and tables that effectively support the core claims.\n*   Confidence (5): 5 — I am highly confident in my assessment; the paper's claims are well-supported by the provided evidence, and its limitations are transparently discussed.",
  "final_review": "1) Summary\nThis paper addresses the problem of local drift in federated fine-tuning (FFT), where client models diverge on non-IID data, degrading the global model. The authors propose OvA-LP, a framework designed to prevent drift at its source rather than correcting it post-hoc. The method combines three components: linear probing (LP) on a frozen pretrained encoder to leverage its stable feature geometry, one-vs-all (OvA) classification heads to decouple class updates and mitigate label skew, and a two-stage training curriculum to control variance. Experiments on CIFAR-100 with 100 clients show that OvA-LP retains 95.9% of its IID accuracy under severe non-IID conditions, significantly outperforming state-of-the-art baselines like FFT-MoE (10.1%) and PFPT (34.5%). The method also demonstrates strong robustness to label noise and high computational efficiency.2) Strengths\n*   **Exceptional Empirical Performance and Robustness**\n    *   The proposed method achieves near-IID performance under extreme non-IID conditions, retaining 95.9% of its IID accuracy on average across challenging partitions (Abstract; Figure 4). This is a dramatic improvement over recent baselines FFT-MoE (10.1%) and PFPT (34.5%).\n    *   The method's robustness is demonstrated across a wide variety of data heterogeneity settings, including label skew (Shard-1, Shard-2, Dirichlet), quantity skew (Zipf), and feature skew (k-means clustering), consistently maintaining over 94.9% relative accuracy (Section 4.4.1; Figure 5).\n    *   OvA-LP shows remarkable robustness to label noise, outperforming specialized state-of-the-art methods like FedLTF by a large margin across various symmetric and asymmetric noise ratios (Section 4.4.3; Table 2).\n    *   The performance gains are consistent across different encoder architectures (ViT-B/16, ViT-L/16, DINOv2-L/14) and datasets (CIFAR-100, TinyImageNet), suggesting the approach is not overly sensitive to model choice (Section 4.4.2; Figure 6).*   **Simple, Well-Motivated, and Interpretable Method**\n    *   The framework is conceptually simple, combining three well-established techniques (linear probing, OvA classifiers, two-stage training) into a novel and effective pipeline (Section 3; Figure 1).\n    *   The design is clearly motivated by a bias-variance decomposition of federated gradients (Section 3.1). Each component directly targets a specific source of drift: frozen encoder for feature-skew bias (Section 3.2), OvA heads for label-skew bias and variance (Section 3.3), and the two-stage schedule for optimization variance (Section 3.4).\n    *   The connection between the method's components and their effect on drift is clearly articulated and summarized effectively in Table 1, which provides a clear conceptual map of why the proposed method works better than alternatives.*   **Thorough Ablation and Analysis**\n    *   The paper includes a strong ablation study that clearly isolates the contribution of each component. The stepwise performance gains from LP-Softmax (56.3%) to OvA-LP without the two-stage schedule (95.4%) to the full OvA-LP (95.9%) provide compelling evidence for the efficacy of both the OvA decoupling and the two-stage training (Section 4.2; Figure 3).\n    *   The analysis of feature geometry (Section 3.2; Figure 2) provides a quantitative justification for freezing the encoder, showing that pretrained features are significantly more aligned and separated than randomly initialized ones.\n    *   The appendix provides detailed absolute accuracy curves (Appendix A.1, Figure 7; Appendix A.2, Figure 8) and efficiency metrics (Tables 3-6), which complement the relative performance plots in the main text and add depth to the experimental validation.*   **High Computational and Communication Efficiency**\n    *   By precomputing encoder features once, the per-round training cost becomes nearly independent of the encoder's size, making the approach highly scalable (Abstract; Section 1).\n    *   The method is shown to be orders of magnitude faster than baselines. For example, under non-IID conditions, OvA-LP reaches 95% of its final accuracy in just one round and 0.02 seconds, while PFPT requires 44 rounds and 1437 seconds (Appendix A.2; Table 5).\n    *   Per-round client and server computation times are drastically lower (14.86ms and 1.91ms for OvA-LP vs. >1500ms for baselines), making the approach practical for resource-constrained edge devices (Appendix A.2; Table 6).3) Weaknesses\n*   **Internal Numerical Inconsistencies**\n    *   There are several numerical discrepancies between the main text and the appendix that affect the reproducibility and trustworthiness of the results. For the PFPT baseline, the main text claims a relative performance of 34.5% (Figure 4), but the absolute accuracies in the appendix (Table 5) yield a calculated value of (33.17 / 80.27) ≈ 41.3%.\n    *   A similar inconsistency exists for the LP-Softmax ablation. The main text reports 56.3% relative performance (Figure 3), while the appendix data (Table 3) implies a value of (53.70 / 90.13) ≈ 59.6%. These discrepancies directly impact the paper's central claims about the magnitude of improvement over baselines.\n    *   The abstract incorrectly swaps the performance of the two main baselines, stating \"10.1% (PFPT) and 34.5% (FFT-MoE)\" (Abstract), which contradicts the values reported consistently throughout the rest of the paper (e.g., Figure 4, Section 4.3).*   **Limited Scope and Potential Unfairness of Baseline Comparisons**\n    *   The paper compares against only two recent methods, FFT-MoE and PFPT (Section 4.3). While these are relevant, the comparison does not include more classical FL algorithms like FedProx or Scaffold, which are designed to handle heterogeneity and would provide a broader context for the results. No direct evidence found in the manuscript for why these were excluded.\n    *   There is a fundamental paradigm difference between OvA-LP and the baselines that is not fully discussed. OvA-LP freezes the entire encoder and only trains a linear head, whereas methods like PFPT (prompt-tuning) and LoRA-based approaches (referenced in Section 2) adapt parts of the encoder. OvA-LP effectively sidesteps the harder problem of federated encoder adaptation, making the direct comparison of non-IID robustness potentially misleading. The experimental setup for baselines also differs (e.g., PFPT uses 10% client participation while OvA-LP uses 100%) (Table 8).*   **Over-reliance on Full Client Participation**\n    *   All main experiments are conducted with 100% client participation (Section 4.1; Section 2.5). This is an idealized setting that is rare in practical cross-device FL deployments, where client availability is sporadic and partial participation is the norm.\n    *   The paper acknowledges this limitation and shows in the appendix that performance degrades with lower participation rates, with convergence slowing significantly at a 10% ratio (Section 5; Appendix A.3, Figure 9). This suggests the method's core variance-reduction strategy is less effective in the common partial-participation setting.\n    *   The argument that high efficiency makes full participation \"operationally feasible\" (Section 5) is a valid point for some cross-silo scenarios but does not address the fundamental challenge in cross-device settings.*   **Uncertain Generalizability Beyond Vision Tasks**\n    *   All experiments are conducted on image classification datasets (CIFAR-100, TinyImageNet) (Section 4). The method's success relies heavily on the strong, well-separated feature geometry of pretrained vision models (Section 3.2).\n    *   It is unclear how well this approach would transfer to other modalities like NLP, where fine-tuning parts of the encoder is often critical for achieving high performance on downstream tasks. The assumption that a frozen encoder is sufficient may not hold.\n    *   The paper notes this as a direction for future work (Section 5), but the lack of even preliminary results or a deeper theoretical discussion on its applicability to other domains limits the perceived generality of the contribution.4) Suggestions for Improvement\n*   **Correct and Clarify Numerical Reporting**\n    *   Please re-calculate and report consistent relative performance values for the PFPT baseline across the main text and appendix. If there is a valid reason for the discrepancy (e.g., different averaging protocols), this must be explicitly stated and justified.\n    *   Similarly, please correct the reported relative performance for the LP-Softmax ablation to ensure it is consistent with the absolute accuracy data provided in the appendix (Table 3).\n    *   Please correct the swapped baseline results in the Abstract to align with the data presented in the main experimental sections (Figure 4, Section 4.3).*   **Expand and Contextualize Baseline Comparisons**\n    *   To provide a more comprehensive evaluation, please include experiments comparing OvA-LP against foundational FL algorithms like FedProx and Scaffold, adapted to the same federated fine-tuning setting (i.e., training only the head).\n    *   Please add a discussion that explicitly frames the paradigm difference. Acknowledge that OvA-LP is a \"head-tuning\" method and compare it to \"encoder-tuning\" baselines. This would clarify that the contribution is showing the surprising effectiveness of a simpler approach in high-heterogeneity regimes, rather than claiming superiority on an identical task. Harmonizing experimental settings like participation ratio (Table 8) would also strengthen the comparison.*   **Investigate and Address Partial Participation Effects**\n    *   The results on partial participation are important and should be moved from the appendix (Appendix A.3, Figure 9) to the main experimental section to present a more balanced picture of the method's capabilities and limitations.\n    *   Please provide an analysis of *why* performance degrades. For instance, does the two-stage strategy become unstable, or is it simply a matter of slower convergence due to increased variance from client sampling?\n    *   Discuss how this limitation could be addressed. For example, could OvA-LP be combined with aggregation rules like FedProx or momentum-based methods on the server to mitigate the variance introduced by partial participation? Exploring this would significantly enhance the practical relevance of the work.*   **Provide Evidence or Deeper Discussion on Generalizability**\n    *   To strengthen the paper's claims of being a general framework, please consider adding a preliminary experiment on a standard federated NLP benchmark (e.g., text classification).\n    *   If new experiments are not feasible, please expand the discussion in Section 5. For example, you could analyze the feature geometry of popular pretrained language models (similar to Figure 2 for ViT) and discuss whether the core assumptions of OvA-LP are likely to hold, citing relevant literature on linear probing in LLMs.5) Score\n*   Overall (10): 7 — The paper presents a simple and highly effective method for a critical problem, but significant numerical inconsistencies in the reporting of key results undermine its claims.\n*   Novelty (10): 7 — While the components are not new, their combination and the \"source-level drift prevention\" framing for FFT are novel and insightful.\n*   Technical Quality (10): 6 — The methodology is well-motivated, but the experimental reporting contains several internal contradictions and calculation errors that affect key comparative claims (Table 3, Table 5, Abstract).\n*   Clarity (10): 7 — The paper is generally well-written, but the abstract contains a factual error and numerical inconsistencies between the main text and appendix reduce clarity and trustworthiness.\n*   Confidence (5): 5 — I am highly confident in my assessment; the paper's claims and its numerical reporting flaws are well-supported by the provided evidence.",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 8,
        "novelty": 7,
        "technical_quality": 8,
        "clarity": 9,
        "confidence": 5
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 7,
        "novelty": 7,
        "technical_quality": 6,
        "clarity": 7,
        "confidence": 5
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "1) Summary\nThis paper addresses the problem of local drift in federated fine-tuning (FFT), where client models diverge on non-IID data, degrading the global model. The authors propose OvA-LP, a framework designed to prevent drift at its source rather than correcting it post-hoc. The method combines three components: linear probing (LP) on a frozen pretrained encoder to leverage its stable feature geometry, one-vs-all (OvA) classification heads to decouple class updates and mitigate label skew, and a two-stage training curriculum to control variance. Experiments on CIFAR-100 with 100 clients show that OvA-LP retains 95.9% of its IID accuracy under severe non-IID conditions, significantly outperforming state-of-the-art baselines like FFT-MoE (10.1%) and PFPT (34.5%). The method also demonstrates strong robustness to label noise and high computational efficiency.2) Strengths\n*   **Exceptional Empirical Performance and Robustness**\n    *   The proposed method achieves near-IID performance under extreme non-IID conditions, retaining 95.9% of its IID accuracy on average across challenging partitions (Abstract; Figure 4). This is a dramatic improvement over recent baselines FFT-MoE (10.1%) and PFPT (34.5%).\n    *   The method's robustness is demonstrated across a wide variety of data heterogeneity settings, including label skew (Shard-1, Shard-2, Dirichlet), quantity skew (Zipf), and feature skew (k-means clustering), consistently maintaining over 94.9% relative accuracy (Section 4.4.1; Figure 5).\n    *   OvA-LP shows remarkable robustness to label noise, outperforming specialized state-of-the-art methods like FedLTF by a large margin across various symmetric and asymmetric noise ratios (Section 4.4.3; Table 2).\n    *   The performance gains are consistent across different encoder architectures (ViT-B/16, ViT-L/16, DINOv2-L/14) and datasets (CIFAR-100, TinyImageNet), suggesting the approach is not overly sensitive to model choice (Section 4.4.2; Figure 6).*   **Simple, Well-Motivated, and Interpretable Method**\n    *   The framework is conceptually simple, combining three well-established techniques (linear probing, OvA classifiers, two-stage training) into a novel and effective pipeline (Section 3; Figure 1).\n    *   The design is clearly motivated by a bias-variance decomposition of federated gradients (Section 3.1). Each component directly targets a specific source of drift: frozen encoder for feature-skew bias (Section 3.2), OvA heads for label-skew bias and variance (Section 3.3), and the two-stage schedule for optimization variance (Section 3.4).\n    *   The connection between the method's components and their effect on drift is clearly articulated and summarized effectively in Table 1, which provides a clear conceptual map of why the proposed method works better than alternatives.*   **Thorough Ablation and Analysis**\n    *   The paper includes a strong ablation study that clearly isolates the contribution of each component. The stepwise performance gains from LP-Softmax (56.3%) to OvA-LP without the two-stage schedule (95.4%) to the full OvA-LP (95.9%) provide compelling evidence for the efficacy of both the OvA decoupling and the two-stage training (Section 4.2; Figure 3).\n    *   The analysis of feature geometry (Section 3.2; Figure 2) provides a quantitative justification for freezing the encoder, showing that pretrained features are significantly more aligned and separated than randomly initialized ones.\n    *   The appendix provides detailed absolute accuracy curves (Appendix A.1, Figure 7; Appendix A.2, Figure 8) and efficiency metrics (Tables 3-6), which complement the relative performance plots in the main text and add depth to the experimental validation.*   **High Computational and Communication Efficiency**\n    *   By precomputing encoder features once, the per-round training cost becomes nearly independent of the encoder's size, making the approach highly scalable (Abstract; Section 1).\n    *   The method is shown to be orders of magnitude faster than baselines. For example, under non-IID conditions, OvA-LP reaches 95% of its final accuracy in just one round and 0.02 seconds, while PFPT requires 44 rounds and 1437 seconds (Appendix A.2; Table 5).\n    *   Per-round client and server computation times are drastically lower (14.86ms and 1.91ms for OvA-LP vs. >1500ms for baselines), making the approach practical for resource-constrained edge devices (Appendix A.2; Table 6).3) Weaknesses\n*   **Internal Numerical Inconsistencies**\n    *   There are several numerical discrepancies between the main text and the appendix that affect the reproducibility and trustworthiness of the results. For the PFPT baseline, the main text claims a relative performance of 34.5% (Figure 4), but the absolute accuracies in the appendix (Table 5) yield a calculated value of (33.17 / 80.27) ≈ 41.3%.\n    *   A similar inconsistency exists for the LP-Softmax ablation. The main text reports 56.3% relative performance (Figure 3), while the appendix data (Table 3) implies a value of (53.70 / 90.13) ≈ 59.6%. These discrepancies directly impact the paper's central claims about the magnitude of improvement over baselines.\n    *   The abstract incorrectly swaps the performance of the two main baselines, stating \"10.1% (PFPT) and 34.5% (FFT-MoE)\" (Abstract), which contradicts the values reported consistently throughout the rest of the paper (e.g., Figure 4, Section 4.3).*   **Limited Scope and Potential Unfairness of Baseline Comparisons**\n    *   The paper compares against only two recent methods, FFT-MoE and PFPT (Section 4.3). While these are relevant, the comparison does not include more classical FL algorithms like FedProx or Scaffold, which are designed to handle heterogeneity and would provide a broader context for the results. No direct evidence found in the manuscript for why these were excluded.\n    *   There is a fundamental paradigm difference between OvA-LP and the baselines that is not fully discussed. OvA-LP freezes the entire encoder and only trains a linear head, whereas methods like PFPT (prompt-tuning) and LoRA-based approaches (referenced in Section 2) adapt parts of the encoder. OvA-LP effectively sidesteps the harder problem of federated encoder adaptation, making the direct comparison of non-IID robustness potentially misleading. The experimental setup for baselines also differs (e.g., PFPT uses 10% client participation while OvA-LP uses 100%) (Table 8).*   **Over-reliance on Full Client Participation**\n    *   All main experiments are conducted with 100% client participation (Section 4.1; Section 2.5). This is an idealized setting that is rare in practical cross-device FL deployments, where client availability is sporadic and partial participation is the norm.\n    *   The paper acknowledges this limitation and shows in the appendix that performance degrades with lower participation rates, with convergence slowing significantly at a 10% ratio (Section 5; Appendix A.3, Figure 9). This suggests the method's core variance-reduction strategy is less effective in the common partial-participation setting.\n    *   The argument that high efficiency makes full participation \"operationally feasible\" (Section 5) is a valid point for some cross-silo scenarios but does not address the fundamental challenge in cross-device settings.*   **Uncertain Generalizability Beyond Vision Tasks**\n    *   All experiments are conducted on image classification datasets (CIFAR-100, TinyImageNet) (Section 4). The method's success relies heavily on the strong, well-separated feature geometry of pretrained vision models (Section 3.2).\n    *   It is unclear how well this approach would transfer to other modalities like NLP, where fine-tuning parts of the encoder is often critical for achieving high performance on downstream tasks. The assumption that a frozen encoder is sufficient may not hold.\n    *   The paper notes this as a direction for future work (Section 5), but the lack of even preliminary results or a deeper theoretical discussion on its applicability to other domains limits the perceived generality of the contribution.4) Suggestions for Improvement\n*   **Correct and Clarify Numerical Reporting**\n    *   Please re-calculate and report consistent relative performance values for the PFPT baseline across the main text and appendix. If there is a valid reason for the discrepancy (e.g., different averaging protocols), this must be explicitly stated and justified.\n    *   Similarly, please correct the reported relative performance for the LP-Softmax ablation to ensure it is consistent with the absolute accuracy data provided in the appendix (Table 3).\n    *   Please correct the swapped baseline results in the Abstract to align with the data presented in the main experimental sections (Figure 4, Section 4.3).*   **Expand and Contextualize Baseline Comparisons**\n    *   To provide a more comprehensive evaluation, please include experiments comparing OvA-LP against foundational FL algorithms like FedProx and Scaffold, adapted to the same federated fine-tuning setting (i.e., training only the head).\n    *   Please add a discussion that explicitly frames the paradigm difference. Acknowledge that OvA-LP is a \"head-tuning\" method and compare it to \"encoder-tuning\" baselines. This would clarify that the contribution is showing the surprising effectiveness of a simpler approach in high-heterogeneity regimes, rather than claiming superiority on an identical task. Harmonizing experimental settings like participation ratio (Table 8) would also strengthen the comparison.*   **Investigate and Address Partial Participation Effects**\n    *   The results on partial participation are important and should be moved from the appendix (Appendix A.3, Figure 9) to the main experimental section to present a more balanced picture of the method's capabilities and limitations.\n    *   Please provide an analysis of *why* performance degrades. For instance, does the two-stage strategy become unstable, or is it simply a matter of slower convergence due to increased variance from client sampling?\n    *   Discuss how this limitation could be addressed. For example, could OvA-LP be combined with aggregation rules like FedProx or momentum-based methods on the server to mitigate the variance introduced by partial participation? Exploring this would significantly enhance the practical relevance of the work.*   **Provide Evidence or Deeper Discussion on Generalizability**\n    *   To strengthen the paper's claims of being a general framework, please consider adding a preliminary experiment on a standard federated NLP benchmark (e.g., text classification).\n    *   If new experiments are not feasible, please expand the discussion in Section 5. For example, you could analyze the feature geometry of popular pretrained language models (similar to Figure 2 for ViT) and discuss whether the core assumptions of OvA-LP are likely to hold, citing relevant literature on linear probing in LLMs.5) Score\n*   Overall (10): 7 — The paper presents a simple and highly effective method for a critical problem, but significant numerical inconsistencies in the reporting of key results undermine its claims.\n*   Novelty (10): 7 — While the components are not new, their combination and the \"source-level drift prevention\" framing for FFT are novel and insightful.\n*   Technical Quality (10): 6 — The methodology is well-motivated, but the experimental reporting contains several internal contradictions and calculation errors that affect key comparative claims (Table 3, Table 5, Abstract).\n*   Clarity (10): 7 — The paper is generally well-written, but the abstract contains a factual error and numerical inconsistencies between the main text and appendix reduce clarity and trustworthiness.\n*   Confidence (5): 5 — I am highly confident in my assessment; the paper's claims and its numerical reporting flaws are well-supported by the provided evidence."
}