# Global Summary
- Problem: Federated fine-tuning (FFT) of foundation models is brittle under heterogeneous (non-IID) client data due to local drift that introduces systematic bias and amplified variance in global updates.
- Core approach: OvA-LP is a minimalist, PEFT-based FFT framework that suppresses drift at its source by (1) freezing the encoder and doing linear probing, (2) replacing softmax with one-vs-all (OvA) binary heads to decouple classes, and (3) using a simple two-stage training schedule (Stage 1: positives only; Stage 2: positives + negatives). Clients precompute encoder features once; aggregation uses FedAvg.
- Evaluation scope: Main experiments on CIFAR-100 with 100 clients over 50 rounds under multiple non-IID partitions (Shard-1, Shard-2, Bernoulli–Dirichlet with p = 0.1, α = 0.001; additional Zipf quantity skew and feature-skew partitions). Encoders include ViT-B/16, ViT-L/16, DINOv2 ViT-L/14; extension to TinyImageNet. Five fixed seeds (0, 42, 777, 1337, 15254). Full client participation unless noted.
- Key findings:
  - Robustness: OvA-LP retains near-IID performance. On CIFAR-100 with 100 clients, averaged over shard-1, shard-2, and Dirichlet partitions, OvA-LP retains “95.9%” of its IID accuracy (main reported figure). In contrast, baseline retention is reported as “10.1% (PFPT)” and “34.5% (FFT-MoE)” in the Abstract, while Experiments report “FFT-MoE ≈ 10.1%” and “PFPT ≈ 34.5%”.
  - Ablation: Relative performance progresses stepwise 56.3% → 95.4% → 95.9% when moving from LP-softmax to OvA without two-stage, then adding the two-stage schedule.
  - Partition-wise robustness: R(50) ranges 94.9%–99.7% across Dirichlet (94.9%), Shard-1 (96.1%), Shard-2 (96.7%), Zipf=2 (97.7%), Feature Skew (99.7%).
  - Label noise: Under CIFAR-100 noise benchmarks (from FedLTF), OvA-LP shows much smaller accuracy decline rates than prior methods. Symmetric noise declines: 0.76% (0.30), 2.35% (0.40), 4.52% (0.50), 10.35% (0.70) from baseline accuracy 88.78%. Asymmetric noise declines: 0.63% (0.30), 1.53% (0.40) from baseline 89.28%.
  - Efficiency: OvA-LP reaches Acc@95 in 1–3 rounds and shows vastly reduced time and communication compared to baselines (e.g., per-round client time ~14–15 ms vs. seconds for FFT-MoE/PFPT).
- Caveats explicitly stated:
  - Heavy reliance on a strong pretrained encoder; OvA-LP cannot compensate for weak encoders.
  - Main experiments assume full client participation; reduced participation slows convergence and is not fully mitigated by the two-stage schedule.
  - Current study focuses on vision and does not combine with advanced aggregation or personalization frameworks yet.

# Abstract
- Motivation: FFT with PEFT is efficient but fragile under extreme non-IID conditions due to local drift that leads to bias/variance amplification; most methods correct drift post hoc and remain brittle.
- Proposal: OvA-LP—linear probing with a frozen encoder, one-vs-all head, and two-stage training—to preserve feature geometry and decouple logits, suppressing drift at its source.
- Main quantitative claim (CIFAR-100, 100 clients; averaged over shard-1, shard-2, Dirichlet p = 0.1, α = 0.001):
  - OvA-LP retains “95.9%” of IID accuracy.
  - Baselines retain only “10.1% (PFPT)” and “34.5% (FFT-MoE)” under the same conditions (note: later Experiments section reports the opposite pairing).
- Additional claims:
  - Robust under symmetric and asymmetric label noise.
  - Precomputing encoder features makes per-round cost nearly independent of encoder size.

# Introduction
- Context: Foundation models enable FFT; PEFT (adapters, LoRA, prompt tuning) reduces cost and makes FFT practical.
- Problem: Heterogeneous client distributions induce local drift, producing systematic bias and amplified variance during aggregation; SOTA FFT methods under extreme non-IID often retain well below half of IID accuracy within 50 rounds.
- Prior approaches:
  - Aggregation methods (FedProx, Scaffold; PEFT variants FLoRA, FedSA-LoRA, FRLoRA) adjust server-side updates post hoc.
  - Personalization methods (FedAdapter, FedPrompt; expert-based like FFT-MoE, PFPT) attach client-specific modules but still allow drift to propagate into shared models.
- Contribution: OvA-LP integrates linear probing on a frozen encoder, OvA binary heads, and a two-stage schedule to prevent drift at the source via a bias–variance decomposition linking feature geometry, label decoupling, and variance control.
- Key findings highlighted:
  - Non-IID robustness: retention “95.9%” vs. “34.5% (FFT-MoE)” and “10.1% (PFPT)” (as written here).
  - Innate robustness to label noise with resilience at higher noise levels, surpassing noise-robust baselines.
  - Efficiency: precomputed features reduce per-round cost dependence on encoder size.

# Related Work
- Aggregation strategies: FedProx, Scaffold; PEFT-oriented variants FLoRA, FedSA-LoRA, FRLoRA aim to reduce update variance but act after local divergence.
- Personalization: FedAdapter, FedPrompt; expert-based extensions including FFT-MoE. These improve local adaptation but do not prevent drift from entering the shared model.
- Classification heads for label imbalance: FedRS restricts softmax updates for missing classes; OvA-based methods FedOVA, FedABC, ATHENA-FL decompose into binary classifiers to address label imbalance in scratch training; they do not address feature drift in FFT.
- Label noise robustness: FedCorr and FedLTF correct noise or use robust objectives; orthogonal to tackling drift mechanisms.
- Positioning: OvA-LP is tailored to prevent drift by freezing the encoder and using a two-stage OvA head in FFT; modular and compatible with aggregation and personalization families.

# Method
- Workflow: Clients precompute encoder features with a frozen backbone; local training uses one-vs-all heads with a two-stage schedule (Stage 1 positives-only; Stage 2 positives+negatives).
- Bias–variance framework (Sec. 3.1): Defines local bias b_i = ∇L_i(w) − ∇L(w), global bias B = E_i[∇L_i(w)] − ∇L(w), local variance v_i = Var[g_i], and aggregated variance V = Var[∑_i p_i g_i]. Softmax coupling under label skew introduces cross-class covariance and variance amplification.
- Linear Probing and Feature Geometry (Sec. 3.2; CIFAR-10, ViT-L/16):
  - Metrics: Alignment (expected squared distance of positive pairs), Intra (cluster compactness), Inter (centroid separation), Ratio = Intra/Inter.
  - Reported values (Pretrained vs. Randomly initialized):
    - Alignment: “1.366 ± 0.006” vs. “1.761 ± 0.008” (smaller is better).
    - Intra: “0.818 ± 0.002” vs. “0.917 ± 0.004” (smaller is better).
    - Inter: “0.840 ± 0.004” vs. “0.487 ± 0.008” (larger is better).
    - Ratio: “0.974 ± 0.007” vs. “1.885 ± 0.038” (smaller is better).
  - Claim: Pretrained geometry (alignment, separation) bounds feature-skew-induced bias; local updates remain anchored to global geometry.
- OvA Head and Decoupling (Sec. 3.3):
  - Softmax gradient: g_c(x, y) = (1[y = c] − p_c(x)) h(x), with shared denominator causing coupling; label skew leads to pull/push imbalance, bias, and cross-class covariance Cov(g_c, g_j) ≠ 0.
  - OvA logistic gradient: g_c^OvA(x, y) = (1[y = c] − q_c(x)) h(x), q_c(x) = σ(w_c^⊤ h(x)). Independent heads remove cross-class covariance and label-skew-induced bias/variance amplification.
- Variance and Two-Stage Training (Sec. 3.4):
  - Stage 1 (positives-only): Quickly pulls classifier weights toward class centroids with minimal variance.
  - Stage 2 (positives+negatives): Introduces many negatives to expand margins while retaining a small fraction of positives as anchors; stabilizes training.
  - Claim: Curriculum reduces variance effects and stabilizes optimization.

# Experiments
- Setup (Sec. 4.1):
  - Datasets and clients: CIFAR-100 with 100 clients, 50 rounds.
  - Seeds: 0, 42, 777, 1337, 15254.
  - Partitions: IID; Non-IID—Shard-1 (1 class/client), Shard-2 (2 classes/client), Dirichlet p = 0.1, α = 0.001 (FedCorr construction).
  - Additional analyses: Alternative partitions (Zipf with s = 2.0 for quantity skew; feature-based clustering), encoder scaling, TinyImageNet, label noise robustness.
- Model/training (Sec. 4.1–4.2):
  - Encoder: Frozen ViT-L/16.
  - Participation: 100% clients per round.
  - Local epochs: 3; Batch size: 50; Learning rate: 0.01.
  - Optimizer: AdamW, weight decay 1×10^-4.
  - Aggregation: FedAvg.
  - Two-stage schedule: First round Stage 1 (positive-only), subsequent rounds Stage 2 (positive+negative).
- Baselines (Sec. 4.1–4.3):
  - Methods: FFT-MoE, PFPT; reproduced with original encoders (e.g., ViT-B/32, ViT-B/16); details in Appendix B.2.
- Evaluation philosophy (Sec. 4.1): Robustness measured via R(t) = AccNonIID(t)/AccIID(t) × 100; report R(t) curves and R(50).
- Ablation (Sec. 4.2):
  - Configurations: (i) LP-softmax, (ii) OvA-LP w/o two-stage, (iii) OvA-LP two-stage.
  - Relative performance progression: “56.3% → 95.4% → 95.9%”.
  - Observation: Brief early overshoot due to FedAvg’s size-weighting; settles quickly.
- Comparison with baselines (Sec. 4.3):
  - Under extreme Non-IID: FFT-MoE plateaus near “10.1%”; PFPT rises slowly to “34.5%”.
  - OvA-LP converges to “95.9%” and tracks IID trajectory closely.
  - Stepwise ranking: Post-hoc baselines poor; LP-softmax moderate; OvA-LP w/o two-stage 95.4%; OvA-LP two-stage 95.9%.
- Partition-wise robustness (Sec. 4.4.1):
  - Final R(50): Dirichlet “94.9%”, Shard-1 “96.1%”, Shard-2 “96.7%”, Zipf=2 “97.7%”, Feature Skew “99.7%”.
- Encoder and task variations (Sec. 4.4.2):
  - Encoders: ViT-B/16, ViT-L/16, DINOv2 ViT-L/14; datasets: CIFAR-100 and TinyImageNet under Dirichlet.
  - Reported bars: ViT-B/16 “92.9%” (CIFAR-100) / “93.9%” (Tiny); ViT-L/16 “97.4%” / “97.3%”; DINOv2 ViT-L/14 “98.1%” / “96.7%”. All track IID trajectory with brief benign overshoot.
- Label noise robustness (Sec. 4.4.3; Table 2):
  - Symmetric noise baseline accuracy: “88.78%”.
  - Symmetric decline rates at noise ratios 0.30/0.40/0.50/0.60/0.70: “0.76% / 2.35% / 4.52% / 10.35%” (four values explicitly bolded; 0.60 is “4.52%”). Competing methods’ declines are larger (e.g., FedLTF Stage 3: 3.70%, 9.24%, 14.65%, 20.91%).
  - Asymmetric noise baseline accuracy: “89.28%”.
  - Asymmetric decline rates (0.30/0.40): “0.63% / 1.53%”.
  - Claim: OvA-LP surpasses FedLTF and others in minimized decline rates.
- Summary claim (Sec. 4.4.3): OvA-LP is stable under diverse heterogeneity, across encoder scales/domains, and under label corruption.

# Conclusion
- Limitations (Sec. 5):
  - Dependence on pretrained encoder quality; OvA-LP cannot compensate for weak encoders.
  - Full participation assumption; reduced participation increases variance and slows convergence (Appendix A.3 shows two-stage alone cannot fully overcome this).
  - Scope limited to vision benchmarks; not yet integrated with aggregation/personalization frameworks.
  - Efficiency note under full participation: reaches Acc@95 within 1–3 rounds; lower computation/communication than prior methods with all 100 clients active.
- Overall conclusion (Sec. 6):
  - OvA-LP demonstrates that source-level drift suppression (frozen encoder, OvA head, two-stage) yields near-IID robustness without architectural complexity.
  - Complementary to post-hoc aggregation/personalization; offers a strong baseline and promotes source-level robustness as a paradigm.

# Appendix
- Acknowledgements: Supported by IITP grants (MSIT) No. RS-2024-00398360 and No. RS-2024-00399491.
- A.1 Ablation results (absolute accuracies and costs):
  - Final performance (Table 3):
    - LP-Softmax IID: “90.13 ± 0.04%”; Acc@95 “1 ± 0” rounds; Total Time “0.03 ± 0.00 s”; Total Comm “0.39 ± 0.00 MB”.
    - LP-Softmax Non-IID: “53.70 ± 6.79%”; Acc@95 “27 ± 9” rounds; Time “0.73 ± 0.23 s”; Comm “10.71 ± 3.37 MB”.
    - OvA-LP w/o 2-stage IID: “90.03 ± 0.08%”; Acc@95 “2 ± 0”; Time “0.05 ± 0.00 s”; Comm “0.78 ± 0.00 MB”.
    - OvA-LP w/o 2-stage Non-IID: “85.89 ± 1.11%”; Acc@95 “15 ± 3”; Time “0.37 ± 0.07 s”; Comm “5.72 ± 1.02 MB”.
    - OvA-LP 2-stage IID: “90.04 ± 0.12%”; Acc@95 “3 ± 0”; Time “0.05 ± 0.00 s”; Comm “1.18 ± 0.00 MB”.
    - OvA-LP 2-stage Non-IID: “86.34 ± 0.73%”; Acc@95 “1 ± 0”; Time “0.02 ± 0.00 s”; Comm “0.42 ± 0.10 MB”.
  - Per-round costs (Table 4):
    - LP-Softmax: Client “23.18–23.78 ms”; Server “2.64–2.97 ms”; Client Comm “401.20 KB”; Server Comm “39.18 MB”.
    - OvA-LP w/o 2-stage: Client “21.67–22.27 ms”; Server “2.69–2.99 ms”; Client Comm “401.20 KB”; Server Comm “39.18 MB”.
    - OvA-LP 2-stage: Client “14.15–14.86 ms”; Server “1.90–1.91 ms”; Client Comm “401.20 KB”; Server Comm “39.18 MB”.
- A.2 Baseline comparisons (absolute accuracies and costs):
  - Final performance (Table 5):
    - FFT-MoE IID: “96.39 ± 0.41%”; Acc@95 “21 ± 0”; Total Time “34.58 ± 0.70 s”; Total Comm “3.67 ± 0.08 GB”.
    - FFT-MoE Non-IID: “9.83 ± 7.96%”; Acc@95 “37 ± 13”; Time “61.00 ± 21.59 s”; Comm “6.48 ± 2.32 GB”.
    - PFPT IID: “80.27 ± 0.41%”; Acc@95 “13 ± 1”; Time “432.75 ± 32.45 s”; Comm “0.22 ± 0.02 GB”.
    - PFPT Non-IID: “33.17 ± 15.30%”; Acc@95 “44 ± 6”; Time “1437.00 ± 182.03 s”; Comm “0.74 ± 0.09 GB”.
    - OvA-LP IID: “90.04 ± 0.12%”; Acc@95 “3 ± 0”; Time “0.05 ± 0.00 s”; Comm “0.11 ± 0.00 GB”.
    - OvA-LP Non-IID: “86.34 ± 0.73%”; Acc@95 “1 ± 0”; Time “0.02 ± 0.00 s”; Comm “0.04 ± 0.01 GB”.
  - Per-round costs (Table 6):
    - FFT-MoE: Client “~1555–1560 ms”; Server “~94.76–95.49 ms”; Client Comm “1.7703 MB”; Server Comm “177.03 MB”.
    - PFPT: Client “~1855–1861 ms”; Server “~30941–31002 ms”; Client Comm “1.729 MB”; Server Comm “17.29 MB”.
    - OvA-LP: Client “~14.16–14.86 ms”; Server “~1.90–1.91 ms”; Client Comm “0.3918 MB”; Server Comm “39.18 MB”.
  - Claim: OvA-LP converges in 2 rounds (IID) and 1 round (Non-IID), with 10^2–10^4 reductions in time/communication versus baselines.
- A.3 Participation rate sweep:
  - Dirichlet (p = 0.1, α = 0.001) with participation ratios 0.1/0.4/0.7/1.0:
    - Final R(50): “91.4% / 96.8% / 97.5% / 97.7%”.
  - Observation: Lower participation slows convergence; two-stage does not fully overcome participation-induced variance.
- Benchmark scale survey (Table 7):
  - Prior works: 4–20 clients (FFT-MoE: 4/10; PFPT: 10; FedLTF: 20; FedProx, SCAFFOLD vary).
  - This work: 100 clients on CIFAR-100 and TinyImageNet.
- Baseline settings (Appendix B.2; Table 8):
  - PFPT: Batch 16; Encoder ViT-B/32; Optimizer Adam (β=(0.9, 0.98), ε=1e-6); LR 1e-4; Local epochs 5; Rounds 50; Active ratio 0.1 (10/100); tokens=10.
  - FFT-MoE: Batch 128; Encoder ViT-B/16; Optimizer Adam (weight decay=1e-2); LR 3e-4; Local epochs 1; Rounds 50; Active ratio 1.0; num_experts=8; rank_per_expert=2; top-k=1; auxiliary loss λ=1e-5.

# References
- Key cited lines of work:
  - Federated optimization and control variates: FedAvg [27], FedProx [6], SCAFFOLD [7].
  - PEFT and federated PEFT: Adapters [2], LoRA [3], prompt tuning [4]; FLoRA [8], FedSA-LoRA [9], FRLoRA [10].
  - Personalization and MoE: FedAdapter [19], FedPrompt [20], FFT-MoE [15].
  - Label imbalance and OvA: FedRS [21], FedOVA [22], FedABC [23], ATHENA-FL [24].
  - Label noise: FedCorr [18], FedLTF [25].
  - Representation geometry: Alignment/uniformity metrics [26].
- The paper provides a full bibliography; numeric details of cited works’ results are not specified here beyond what is used in this manuscript’s comparisons.