{
  "baseline_review": "Summary\n   - The paper proposes OvA-LP, a parameter-efficient federated fine-tuning (FFT) framework designed to prevent client drift at its source on non-IID data. The method freezes a pretrained encoder, performs linear probing with one-vs-all (OvA) binary heads, and uses a two-stage training schedule (Stage 1: positives only; Stage 2: positives + negatives) to suppress bias and variance components of drift (Sec. 3, Fig. 1). Empirically, OvA-LP achieves near-IID robustness across multiple non-IID partitions of CIFAR-100 with 100 clients, reporting relative performance R(50) ≈ 95.9% versus much lower figures for FFT-MoE and PFPT (Fig. 4, Table 1). Additional analyses indicate robustness across heterogeneity types (Fig. 5), encoder scales and TinyImageNet (Fig. 6/42), and label noise benchmarks (Table 2). Efficiency claims are supported by per-round timings and communication results (Tables 4–6).Strengths\n   - Bolded titles with sub-point examples- **Clear source-level drift framing**\n     • The paper formalizes drift via a bias–variance decomposition and separates local/global bias and variance contributors (Sec. 3.1). This framing anchors design choices and clarifies why post-hoc aggregation fixes are limited (Sec. 3.1 “Takeaway”). This matters for conceptual clarity and guiding method components.\n     • Softmax coupling analysis connects label skew to bias and variance amplification through shared denominators and cross-class covariances (Sec. 3.3), motivating OvA decoupling. This provides technical soundness for the design of independent heads.\n     • The two-stage curriculum is justified as variance control: early positive-only updates align weights with centroids; later negatives expand margins while anchors prevent drift (Sec. 3.4). This matters for stability under participation variance and optimization dynamics.- **Simple, modular method grounded in PEFT**\n     • The architecture is minimal—frozen encoder, linear OvA heads, and a two-stage schedule—without complex routing, aggregation, or adapters (Fig. 1; Sec. 3). This aids reproducibility and extensibility within PEFT-based FFT pipelines.\n     • Precomputing features with a frozen encoder preserves pretrained geometry while reducing per-round costs (Abstract; Sec. 3.2; Fig. 2), highlighting a practical PEFT strategy.\n     • The method remains compatible with FedAvg and in principle with other aggregation/personalization frameworks (Sec. 2 “Our positioning”), enhancing deployment potential.- **Comprehensive empirical robustness across heterogeneity**\n     • On CIFAR-100 with 100 clients under Shard-1, Shard-2, and Dirichlet (p = 0.1, α = 0.001), relative performance R(50) stays ≈95.9% (Abstract; Fig. 4; Table 1). This demonstrates impact under strong non-IID.\n     • Beyond standard partitions, OvA-LP maintains R(50) ≥ 94.9% under Zipf quantity skew and feature clustering skew (Fig. 5). This breadth strengthens claims about heterogeneity robustness.\n     • Ablation shows stepwise gains: LP-softmax (56.3%) → OvA (95.4%) → OvA+two-stage (95.9%) (Sec. 4.2; Fig. 3/29), directly tying improvements to proposed components. This is evidence of technical soundness.- **Strong efficiency results**\n     • Per-round client/server times drop to ~14.9 ms/~1.9 ms for OvA-LP vs 23.8 ms/~3.0 ms for LP-Softmax (Table 4), indicating practical system efficiency; this matters for scalable FL.\n     • Compared to FFT-MoE and PFPT, total time and communication to Acc@95 are reduced by 10^2–10^4× (Tables 5–6; Appendix A.2 narrative), underscoring high impact for deployment.\n     • OvA-LP reaches Acc@95 in 1–3 rounds under full participation (Appendix A.1, Table 3), highlighting fast convergence and reduced round complexity.- **Evidence that pretrained geometry limits feature bias**\n     • Quantitative geometry metrics (Alignment, Intra, Inter, Ratio) show compact, well-separated clusters for pretrained encoders vs random initialization (Fig. 2; Sec. 3.2), supporting the claim that freezing helps bound feature skew-induced bias. This is technically sound and motivates linear probing.\n     • The qualitative link between geometry (alignment/separation) and reduced local bias is explicitly stated (Sec. 3.2 “From the bias–variance perspective…”). It clarifies why encoder freezing contributes to drift suppression.- **Label noise robustness benchmarked against prior work**\n     • On CIFAR-100 with symmetric/asymmetric noise, OvA-LP reports the smallest decline rates across noise ratios compared to FedLTF and other baselines (Table 2), supporting the claim of robustness to corrupted labels (Sec. 4.4.3).\n     • The use of “decline rate from baseline accuracy” normalizes across methods (Table 2), improving comparability and indicating potential impact in noisy real-world settings.- **Encoder and task domain variations**\n     • OvA-LP tracks IID trajectories across ViT-B/16, ViT-L/16, and DINOv2-L/14 on CIFAR-100 and TinyImageNet Dirichlet partitions (Fig. 6/42; Sec. 4.4.2), suggesting architecture/task-agnostic robustness.\n     • The brief overshoot under FedAvg is documented and explained (Sec. 4.2, Sec. 4.4.2), reflecting careful experimental reporting.Weaknesses\n   - Bolded titles with sub-point examples- **Limited theoretical depth and measurement of bias–variance claims**\n     • While Sec. 3.1–3.4 articulate mechanisms, the paper lacks formal bounds or theorems linking OvA-LP components to quantified reductions in local/global bias and variance; Table 1’s “✓/△/×” is qualitative rather than measured (“Bias–variance view of methods,” Table 1). This limits technical rigor.\n     • No empirical measurements of gradient variance or cross-class covariance are provided to validate the “variance amplification via softmax coupling” and its removal (Sec. 3.3). This weakens evidence for mechanism-level claims.\n     • The link from geometry metrics (Fig. 2) to actual drift reduction under non-IID is argued conceptually (Sec. 3.2) but not quantified (e.g., no measured bias terms b_i or B; Sec. 3.1 definitions).- **Baseline comparability and fairness concerns**\n     • Encoder sizes differ across methods (OvA-LP uses ViT-L/16; baselines use ViT-B/16 or ViT-B/32), potentially affecting absolute accuracy and efficiency (Sec. 4.1–4.3; Appendix B.2, Table 8). This complicates cross-method comparisons.\n     • Participation ratios differ (PFPT at 0.1; OvA-LP at 1.0), affecting convergence and variance (Appendix B.2, Table 8; Sec. 5; Appendix A.3). This challenges fairness in speed and robustness claims.\n     • Hyperparameters (batch size, epochs, optimizer settings) vary substantially across methods (Table 8), making it hard to attribute performance differences solely to method design.- **Incomplete reproducibility details for the two-stage schedule and OvA training**\n     • Stage 2 is described qualitatively (“large set of negatives” and “small fraction of positives as anchors,” Sec. 3.4) without precise ratios, sampling strategies, or head update rules per client, limiting replicability.\n     • The handling of negatives under extreme label skew (e.g., Shard-1) is not fully specified—e.g., whether each client trains all K heads with local negatives from its sole class or uses server-provided negatives; No direct evidence found in the manuscript.\n     • The exact per-round curriculum (beyond “round 1: Stage 1; subsequent rounds: Stage 2,” Sec. 4.1) lacks algorithmic pseudocode and sensitivity analyses (e.g., anchor fraction sweeps), constraining verification.- **Novelty claim appears broad and is not substantiated beyond narrative**\n     • The Abstract claims OvA-LP is “the first explicitly designed to suppress drift at its source within the PEFT-based FFT paradigm,” but the paper does not provide a systematic survey or comparative analysis to validate this claim; No direct evidence found in the manuscript.\n     • Related work notes prior OvA-based FL methods (FedOVA, FedABC, ATHENA-FL) and PEFT FFT approaches, but does not rigorously argue non-overlap with “source-level” prevention beyond description (Sec. 2). This reduces confidence in exclusivity.\n     • Table 1’s positioning is qualitative and does not include other OvA- or LP-based baselines adapted to PEFT FFT, leaving novelty relative to potential combinations uncertain; No direct evidence found in the manuscript.- **Efficiency claim of “per-round cost nearly independent of encoder size” lacks direct measurements**\n     • The Abstract states per-round cost is nearly independent of encoder size due to precomputed features, but the paper does not report per-round times across different encoders to validate the claim (Sec. 4.4.2 reports performance, not cost).\n     • While Table 4 provides per-round costs for OvA-LP vs LP-Softmax, it does not vary encoder scale; No direct evidence found in the manuscript.\n     • Precomputation overhead and memory footprint for storing features are not quantified (Sec. 3; Appendix A.1–A.2), limiting practical assessment.- **Scope limitations acknowledged but significant for real-world FL**\n     • Experiments assume full participation (Sec. 4.1), and partial participation slows convergence with remaining variance (Appendix A.3; Sec. 5), indicating limited robustness under realistic client availability.\n     • The study focuses on vision benchmarks (CIFAR-100, TinyImageNet) and does not evaluate text/speech or structured data (Sec. 5), limiting generality across modalities.\n     • OvA-LP is not combined with aggregation/personalization strategies despite claims of modularity (Sec. 2; Sec. 5), leaving questions about synergy and compounded effects under varied FL pipelines.- **Evaluation metric choice may obscure absolute performance differences**\n     • The main non-IID robustness metric R(t)=AccNonIID/AccIID uses each method’s own IID baseline (Sec. 4.1, 4.3), which helps normalization but may hide absolute gaps driven by encoder/hyperparameter differences (Fig. 4–6).\n     • Table 1 and several figures emphasize R(50) without consistent absolute accuracy reporting in the main text (absolute curves/tables are relegated to Appendix A.1/A.2, Fig. 7/8; Tables 3/5), reducing immediate transparency.\n     • Statistical significance (e.g., confidence intervals over seeds) is limited in the main text; although five seeds are fixed (Sec. 4.1), error bars or tests are not shown for core comparisons, affecting experimental rigor.Suggestions for Improvement\n   - Bolded titles with sub-point examples- **Deepen theory and measure mechanism-level effects**\n     • Provide formal analyses or bounds quantifying how OvA decoupling and the two-stage curriculum reduce local/global bias and variance under label/feature skew (expand Sec. 3.1–3.4 with theorems or propositions).\n     • Empirically measure gradient variance and cross-class covariance with softmax vs OvA on representative partitions (extend Fig. 3/29 and Table 1 with measured statistics).\n     • Quantify drift metrics (e.g., b_i, B from Sec. 3.1) across methods to directly link geometry and schedule to reduced bias and aggregation variance.- **Strengthen baseline fairness with controlled settings**\n     • Re-run FFT-MoE and PFPT with the same encoder (e.g., ViT-L/16) and aligned hyperparameters to isolate algorithmic differences (complement Appendix B.2, Table 8).\n     • Align participation ratios across methods (e.g., full participation vs matched ratios) and report results for both, analyzing sensitivity (use Appendix A.3 template).\n     • Include hyperparameter tuning and report comparable batch sizes/epochs/optimizers, or add an ablation showing that OvA-LP’s advantage persists under matched settings (extend Tables 5–6).- **Detail the two-stage curriculum and OvA training for reproducibility**\n     • Provide pseudocode specifying which heads are trained per client in Stage 1/2, sampling ratios for positives/negatives, and anchor fractions; include default values used in Sec. 4.1 runs.\n     • Clarify negative sampling under Shard-1/Shard-2 (e.g., using local non-c positives for other heads) and whether any server-side negatives or memory banks are used; add a schematic to Fig. 1.\n     • Add sensitivity analyses on anchor fractions and negative set size, reporting stability/accuracy curves (complement Fig. 3/29 and Table 3).- **Substantiate novelty claims with systematic comparisons**\n     • Include a literature matrix comparing source-level drift prevention vs post-hoc correction across PEFT FFT and OvA/LP methods (augment Sec. 2), citing [21–24], [8–10], [16], [19–20] as applicable.\n     • Adapt prior OvA-based FL methods (FedOVA, FedABC, ATHENA-FL) to the PEFT FFT setting and compare against OvA-LP under identical conditions (add to Sec. 4.3/Appendix A.2).\n     • Calibrate the novelty statement in the Abstract/Intro to reflect empirical evidence and scope, or add rigorous justification (e.g., absence of prior PEFT FFT source-level designs).- **Validate efficiency independence with encoder scale and account for precomputation**\n     • Report per-round client/server times and communications for ViT-B/16, ViT-L/16, DINOv2-L/14 (extend Fig. 6/42 with cost tables), substantiating “nearly independent” per-round cost claims.\n     • Quantify one-time precomputation overhead and amortize it over rounds, including memory/storage footprint for cached features (extend Tables 4–6).\n     • Add scalability results with larger encoders and varying client dataset sizes to demonstrate independence across practical ranges.- **Broaden scope to realistic FL settings and modalities**\n     • Provide partial participation experiments with participation sweeps and variance-control strategies (e.g., Stage 1 for more rounds, stratified sampling), expanding Appendix A.3 into the main text.\n     • Evaluate non-vision domains (e.g., text classification with pretrained transformers) to test OvA-LP’s generality beyond vision (add to Sec. 4.4.2).\n     • Combine OvA-LP with aggregation (e.g., SCAFFOLD, FedProx) and personalization (adapters/prompts) to test modularity and synergy (extend Sec. 4.3).- **Complement ratio-based evaluation with absolute metrics and statistics**\n     • Report absolute accuracy and variance (error bars over seeds) in the main text figures/tables alongside R(t)/R(50) (move Appendix A.1/A.2 results forward).\n     • Add statistical tests (e.g., paired t-tests across seeds) for key comparisons (e.g., Fig. 4 baseline gaps), improving experimental rigor.\n     • Include additional metrics (e.g., calibration, per-class accuracy under label imbalance) to ensure that robustness is not achieved at the expense of fairness or calibration.Score\n     - Overall (10): 7 — Strong empirical robustness and efficiency (Fig. 4–6; Tables 3–6) with a clear, simple design, but theory and baseline fairness need strengthening (Sec. 3.1–3.4; Appendix B.2, Table 8).\n     - Novelty (10): 6 — Combines known components into a source-level framework with convincing results (Sec. 3; Fig. 1), yet the “first” claim is not substantiated beyond narrative (Abstract; Sec. 2; Table 1).\n     - Technical Quality (10): 6 — Method is sound and experiments are broad (Fig. 5–6; Table 2), but lacks formal analyses and measured mechanism-level statistics (Sec. 3.3; Table 1).\n     - Clarity (10): 8 — The paper is well-written with clear equations and figures (Sec. 3.1–3.3; Fig. 1–2; Fig. 3–6), though some implementation details of Stage 2 are missing (Sec. 3.4; Sec. 4.1).\n     - Confidence (5): 4 — High confidence based on detailed experiments and ablations (Sec. 4; Appendix A.1–A.3), but some claims (novelty, efficiency independence) would benefit from additional evidence (Abstract; Sec. 4.4.2; Table 8).",
  "final_review": "Summary\n   - The paper proposes OvA-LP, a parameter-efficient federated fine-tuning (FFT) framework designed to prevent client drift at its source on non-IID data. The method freezes a pretrained encoder, performs linear probing with one-vs-all (OvA) binary heads, and uses a two-stage training schedule (Stage 1: positives only; Stage 2: positives + negatives) to suppress bias and variance components of drift (Sec. 3, Fig. 1). Empirically, OvA-LP achieves near-IID robustness across multiple non-IID partitions of CIFAR-100 with 100 clients, reporting relative performance R(50) ≈ 95.9% versus much lower figures for FFT-MoE (~10.1%) and PFPT (~34.5%) (Fig. 4; Table 1; Sec. 4.3). Additional analyses indicate robustness across heterogeneity types (Fig. 5), encoder scales and TinyImageNet (Fig. 6/42), and label noise benchmarks (Table 2). Efficiency claims are supported by per-round timings and communication results (Tables 4–6).Strengths\n   - Bolded titles with sub-point examples- **Clear source-level drift framing**\n     • The paper formalizes drift via a bias–variance decomposition and separates local/global bias and variance contributors (Sec. 3.1). This framing anchors design choices and clarifies why post-hoc aggregation fixes are limited (Sec. 3.1 “Takeaway”). This matters for conceptual clarity and guiding method components.\n     • Softmax coupling analysis connects label skew to bias and variance amplification through shared denominators and cross-class covariances (Sec. 3.3), motivating OvA decoupling. This provides technical soundness for the design of independent heads.\n     • The two-stage curriculum is justified as variance control: early positive-only updates align weights with centroids; later negatives expand margins while anchors prevent drift (Sec. 3.4). This matters for stability under participation variance and optimization dynamics.- **Simple, modular method grounded in PEFT**\n     • The architecture is minimal—frozen encoder, linear OvA heads, and a two-stage schedule—without complex routing, aggregation, or adapters (Fig. 1; Sec. 3). This aids reproducibility and extensibility within PEFT-based FFT pipelines.\n     • Precomputing features with a frozen encoder preserves pretrained geometry while reducing per-round costs (Abstract; Sec. 3.2; Fig. 2), highlighting a practical PEFT strategy.\n     • The method remains compatible with FedAvg and in principle with other aggregation/personalization frameworks (Sec. 2 “Our positioning”), enhancing deployment potential.- **Comprehensive empirical robustness across heterogeneity**\n     • On CIFAR-100 with 100 clients under Shard-1, Shard-2, and Dirichlet (p = 0.1, α = 0.001), relative performance R(50) stays ≈95.9% (Abstract; Fig. 4; Table 1). This demonstrates impact under strong non-IID.\n     • Beyond standard partitions, OvA-LP maintains R(50) ≥ 94.9% under Zipf quantity skew and feature clustering skew (Fig. 5). This breadth strengthens claims about heterogeneity robustness.\n     • Ablation shows stepwise gains: LP-softmax (56.3%) → OvA (95.4%) → OvA+two-stage (95.9%) (Sec. 4.2; Fig. 3/29), directly tying improvements to proposed components. This is evidence of technical soundness.- **Strong efficiency results**\n     • Per-round client/server times drop to ~14.9 ms/~1.9 ms for OvA-LP vs 23.8 ms/~3.0 ms for LP-Softmax (Table 4), indicating practical system efficiency; this matters for scalable FL.\n     • Compared to FFT-MoE and PFPT, total time and communication to Acc@95 are reduced by 10^2–10^4× (Tables 5–6; Appendix A.2 narrative), underscoring high impact for deployment.\n     • OvA-LP reaches Acc@95 in 1–3 rounds under full participation (Appendix A.1, Table 3), highlighting fast convergence and reduced round complexity.- **Evidence that pretrained geometry limits feature bias**\n     • Quantitative geometry metrics (Alignment, Intra, Inter, Ratio) show compact, well-separated clusters for pretrained encoders vs random initialization (Fig. 2; Sec. 3.2), supporting the claim that freezing helps bound feature skew-induced bias. This is technically sound and motivates linear probing.\n     • The qualitative link between geometry (alignment/separation) and reduced local bias is explicitly stated (Sec. 3.2 “From the bias–variance perspective…”). It clarifies why encoder freezing contributes to drift suppression.- **Label noise robustness benchmarked against prior work**\n     • On CIFAR-100 with symmetric/asymmetric noise, OvA-LP reports the smallest decline rates across noise ratios compared to FedLTF and other baselines (Table 2), supporting the claim of robustness to corrupted labels (Sec. 4.4.3).\n     • The use of “decline rate from baseline accuracy” normalizes across methods (Table 2), improving comparability and indicating potential impact in noisy real-world settings.- **Encoder and task domain variations**\n     • OvA-LP tracks IID trajectories across ViT-B/16, ViT-L/16, and DINOv2-L/14 on CIFAR-100 and TinyImageNet Dirichlet partitions (Fig. 6/42; Sec. 4.4.2), suggesting architecture/task-agnostic robustness.\n     • The brief overshoot under FedAvg is documented and explained (Sec. 4.2, Sec. 4.4.2), reflecting careful experimental reporting.Weaknesses\n   - Bolded titles with sub-point examples- **Limited theoretical depth and measurement of bias–variance claims**\n     • While Sec. 3.1–3.4 articulate mechanisms, the paper lacks formal bounds or theorems linking OvA-LP components to quantified reductions in local/global bias and variance; Table 1’s “✓/△/×” is qualitative rather than measured (“Bias–variance view of methods,” Table 1). This limits technical rigor.\n     • No empirical measurements of gradient variance or cross-class covariance are provided to validate the “variance amplification via softmax coupling” and its removal (Sec. 3.3). This weakens evidence for mechanism-level claims.\n     • The link from geometry metrics (Fig. 2) to actual drift reduction under non-IID is argued conceptually (Sec. 3.2) but not quantified (e.g., no measured bias terms b_i or B; Sec. 3.1 definitions).- **Baseline comparability and fairness concerns**\n     • Encoder sizes differ across methods (OvA-LP uses ViT-L/16; baselines use ViT-B/16 or ViT-B/32), potentially affecting absolute accuracy and efficiency (Sec. 4.1–4.3; Appendix B.2, Table 8). This complicates cross-method comparisons.\n     • Participation ratios differ (PFPT at 0.1; OvA-LP at 1.0), affecting convergence and variance (Appendix B.2, Table 8; Sec. 5; Appendix A.3). This challenges fairness in speed and robustness claims.\n     • Hyperparameters (batch size, epochs, optimizer settings) vary substantially across methods (Table 8), making it hard to attribute performance differences solely to method design; additionally, the Abstract assigns 10.1%/34.5% retention to PFPT/FFT-MoE, while the main text/figures map 10.1% to FFT-MoE and 34.5% to PFPT (Abstract; Fig. 4; Table 1; Sec. 4.3), which reduces reporting clarity.- **Incomplete reproducibility details for the two-stage schedule and OvA training**\n     • Stage 2 is described qualitatively (“large set of negatives” and “small fraction of positives as anchors,” Sec. 3.4) without precise ratios, sampling strategies, or head update rules per client, limiting replicability.\n     • The handling of negatives under extreme label skew (e.g., Shard-1) is not fully specified—e.g., whether each client trains all K heads with local negatives from its sole class or uses server-provided negatives; No direct evidence found in the manuscript.\n     • The exact per-round curriculum (beyond “round 1: Stage 1; subsequent rounds: Stage 2,” Sec. 4.1) lacks algorithmic pseudocode and sensitivity analyses (e.g., anchor fraction sweeps), constraining verification.- **Novelty claim appears broad and is not substantiated beyond narrative**\n     • The Abstract claims OvA-LP is “the first explicitly designed to suppress drift at its source within the PEFT-based FFT paradigm,” but the paper does not provide a systematic survey or comparative analysis to validate this claim; No direct evidence found in the manuscript.\n     • Related work notes prior OvA-based FL methods (FedOVA, FedABC, ATHENA-FL) and PEFT FFT approaches, but does not rigorously argue non-overlap with “source-level” prevention beyond description (Sec. 2). This reduces confidence in exclusivity.\n     • Table 1’s positioning is qualitative and does not include other OvA- or LP-based baselines adapted to PEFT FFT, leaving novelty relative to potential combinations uncertain; No direct evidence found in the manuscript.- **Efficiency claim of “per-round cost nearly independent of encoder size” lacks direct measurements**\n     • The Abstract states per-round cost is nearly independent of encoder size due to precomputed features, but the paper does not report per-round times across different encoders to validate the claim (Sec. 4.4.2 reports performance, not cost).\n     • While Table 4 provides per-round costs for OvA-LP vs LP-Softmax, it does not vary encoder scale; No direct evidence found in the manuscript.\n     • Precomputation overhead and memory footprint for storing features are not quantified (Sec. 3; Appendix A.1–A.2), limiting practical assessment.- **Scope limitations acknowledged but significant for real-world FL**\n     • Experiments assume full participation (Sec. 4.1), and partial participation slows convergence with remaining variance (Appendix A.3; Sec. 5), indicating limited robustness under realistic client availability.\n     • The study focuses on vision benchmarks (CIFAR-100, TinyImageNet) and does not evaluate text/speech or structured data (Sec. 5), limiting generality across modalities.\n     • OvA-LP is not combined with aggregation/personalization strategies despite claims of modularity (Sec. 2; Sec. 5), leaving questions about synergy and compounded effects under varied FL pipelines.- **Evaluation metric choice may obscure absolute performance differences**\n     • The main non-IID robustness metric R(t)=AccNonIID/AccIID uses each method’s own IID baseline (Sec. 4.1, 4.3), which helps normalization but may hide absolute gaps driven by encoder/hyperparameter differences (Fig. 4–6).\n     • Table 1 and several figures emphasize R(50) without consistent absolute accuracy reporting in the main text (absolute curves/tables are relegated to Appendix A.1/A.2, Fig. 7/8; Tables 3/5), reducing immediate transparency.\n     • Although Acc@95 is used extensively to summarize convergence/cost (Appendix A.1/A.2, Tables 3/5), it is not explicitly defined in the evaluation protocol (Sec. 4.1 defines R(t) but does not define Acc@95), creating ambiguity that affects interpretability (e.g., LP-Softmax Non-IID reports Acc@95 “27 ± 9” rounds in Table 3 while its R(50)=56.3% in Fig. 3/29 suggests it never approaches 95% under the ratio metric).Suggestions for Improvement\n   - Bolded titles with sub-point examples- **Deepen theory and measure mechanism-level effects**\n     • Provide formal analyses or bounds quantifying how OvA decoupling and the two-stage curriculum reduce local/global bias and variance under label/feature skew (expand Sec. 3.1–3.4 with theorems or propositions).\n     • Empirically measure gradient variance and cross-class covariance with softmax vs OvA on representative partitions (extend Fig. 3/29 and Table 1 with measured statistics).\n     • Quantify drift metrics (e.g., b_i, B from Sec. 3.1) across methods to directly link geometry and schedule to reduced bias and aggregation variance.- **Strengthen baseline fairness with controlled settings**\n     • Re-run FFT-MoE and PFPT with the same encoder (e.g., ViT-L/16) and aligned hyperparameters to isolate algorithmic differences (complement Appendix B.2, Table 8).\n     • Align participation ratios across methods (e.g., full participation vs matched ratios) and report results for both, analyzing sensitivity (use Appendix A.3 template); also correct reporting mismatches so that headline statements in the Abstract align with Fig. 4/Table 1/Sec. 4.3 regarding which baseline attains ~10.1% vs ~34.5%.\n     • Include hyperparameter tuning and report comparable batch sizes/epochs/optimizers, or add an ablation showing that OvA-LP’s advantage persists under matched settings (extend Tables 5–6).- **Detail the two-stage curriculum and OvA training for reproducibility**\n     • Provide pseudocode specifying which heads are trained per client in Stage 1/2, sampling ratios for positives/negatives, and anchor fractions; include default values used in Sec. 4.1 runs.\n     • Clarify negative sampling under Shard-1/Shard-2 (e.g., using local non-c positives for other heads) and whether any server-side negatives or memory banks are used; add a schematic to Fig. 1.\n     • Add sensitivity analyses on anchor fractions and negative set size, reporting stability/accuracy curves (complement Fig. 3/29 and Table 3).- **Substantiate novelty claims with systematic comparisons**\n     • Include a literature matrix comparing source-level drift prevention vs post-hoc correction across PEFT FFT and OvA/LP methods (augment Sec. 2), citing [21–24], [8–10], [16], [19–20] as applicable.\n     • Adapt prior OvA-based FL methods (FedOVA, FedABC, ATHENA-FL) to the PEFT FFT setting and compare against OvA-LP under identical conditions (add to Sec. 4.3/Appendix A.2).\n     • Calibrate the novelty statement in the Abstract/Intro to reflect empirical evidence and scope, or add rigorous justification (e.g., absence of prior PEFT FFT source-level designs).- **Validate efficiency independence with encoder scale and account for precomputation**\n     • Report per-round client/server times and communications for ViT-B/16, ViT-L/16, DINOv2-L/14 (extend Fig. 6/42 with cost tables), substantiating “nearly independent” per-round cost claims.\n     • Quantify one-time precomputation overhead and amortize it over rounds, including memory/storage footprint for cached features (extend Tables 4–6).\n     • Add scalability results with larger encoders and varying client dataset sizes to demonstrate independence across practical ranges.- **Broaden scope to realistic FL settings and modalities**\n     • Provide partial participation experiments with participation sweeps and variance-control strategies (e.g., Stage 1 for more rounds, stratified sampling), expanding Appendix A.3 into the main text.\n     • Evaluate non-vision domains (e.g., text classification with pretrained transformers) to test OvA-LP’s generality beyond vision (add to Sec. 4.4.2).\n     • Combine OvA-LP with aggregation (e.g., SCAFFOLD, FedProx) and personalization (adapters/prompts) to test modularity and synergy (extend Sec. 4.3).- **Complement ratio-based evaluation with absolute metrics and statistics**\n     • Report absolute accuracy and variance (error bars over seeds) in the main text figures/tables alongside R(t)/R(50) (move Appendix A.1/A.2 results forward), and define Acc@95 precisely (e.g., 95% of IID accuracy, or an absolute threshold) in Sec. 4.1 to align with Tables 3/5.\n     • Add statistical tests (e.g., paired t-tests across seeds) for key comparisons (e.g., Fig. 4 baseline gaps), improving experimental rigor.\n     • Include additional metrics (e.g., calibration, per-class accuracy under label imbalance) to ensure that robustness is not achieved at the expense of fairness or calibration.Score\n     - Overall (10): 7 — Strong empirical robustness and efficiency (Fig. 4–6; Tables 3–6) with a clear, simple design, but theory and baseline fairness need strengthening and some reporting inconsistencies persist (Sec. 3.1–3.4; Abstract vs. Fig. 4/Table 1; Appendix B.2, Table 8).\n     - Novelty (10): 6 — Combines known components into a source-level framework with convincing results (Sec. 3; Fig. 1), yet the “first” claim is not substantiated beyond narrative (Abstract; Sec. 2; Table 1).\n     - Technical Quality (10): 6 — Method is sound and experiments are broad (Fig. 5–6; Table 2), but lacks formal analyses and measured mechanism-level statistics (Sec. 3.3; Table 1).\n     - Clarity (10): 7 — Generally well-written with clear figures (Sec. 3.1–3.3; Fig. 1–2; Fig. 3–6), but Acc@95 is undefined in the evaluation protocol (Sec. 4.1 vs. Appendix A.1/A.2), and the Abstract reverses the 10.1%/34.5% baseline mapping compared to the main text (Fig. 4; Table 1; Sec. 4.3).\n     - Confidence (5): 4 — High confidence based on detailed experiments and ablations (Sec. 4; Appendix A.1–A.3), though some claims (novelty, efficiency independence) and reporting clarity would benefit from additional evidence/definitions (Abstract; Sec. 4.4.2; Table 8).",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 7,
        "novelty": 6,
        "technical_quality": 6,
        "clarity": 8,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 7,
        "novelty": 6,
        "technical_quality": 6,
        "clarity": 7,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "Summary\n   - The paper proposes OvA-LP, a parameter-efficient federated fine-tuning (FFT) framework designed to prevent client drift at its source on non-IID data. The method freezes a pretrained encoder, performs linear probing with one-vs-all (OvA) binary heads, and uses a two-stage training schedule (Stage 1: positives only; Stage 2: positives + negatives) to suppress bias and variance components of drift (Sec. 3, Fig. 1). Empirically, OvA-LP achieves near-IID robustness across multiple non-IID partitions of CIFAR-100 with 100 clients, reporting relative performance R(50) ≈ 95.9% versus much lower figures for FFT-MoE (~10.1%) and PFPT (~34.5%) (Fig. 4; Table 1; Sec. 4.3). Additional analyses indicate robustness across heterogeneity types (Fig. 5), encoder scales and TinyImageNet (Fig. 6/42), and label noise benchmarks (Table 2). Efficiency claims are supported by per-round timings and communication results (Tables 4–6).Strengths\n   - Bolded titles with sub-point examples- **Clear source-level drift framing**\n     • The paper formalizes drift via a bias–variance decomposition and separates local/global bias and variance contributors (Sec. 3.1). This framing anchors design choices and clarifies why post-hoc aggregation fixes are limited (Sec. 3.1 “Takeaway”). This matters for conceptual clarity and guiding method components.\n     • Softmax coupling analysis connects label skew to bias and variance amplification through shared denominators and cross-class covariances (Sec. 3.3), motivating OvA decoupling. This provides technical soundness for the design of independent heads.\n     • The two-stage curriculum is justified as variance control: early positive-only updates align weights with centroids; later negatives expand margins while anchors prevent drift (Sec. 3.4). This matters for stability under participation variance and optimization dynamics.- **Simple, modular method grounded in PEFT**\n     • The architecture is minimal—frozen encoder, linear OvA heads, and a two-stage schedule—without complex routing, aggregation, or adapters (Fig. 1; Sec. 3). This aids reproducibility and extensibility within PEFT-based FFT pipelines.\n     • Precomputing features with a frozen encoder preserves pretrained geometry while reducing per-round costs (Abstract; Sec. 3.2; Fig. 2), highlighting a practical PEFT strategy.\n     • The method remains compatible with FedAvg and in principle with other aggregation/personalization frameworks (Sec. 2 “Our positioning”), enhancing deployment potential.- **Comprehensive empirical robustness across heterogeneity**\n     • On CIFAR-100 with 100 clients under Shard-1, Shard-2, and Dirichlet (p = 0.1, α = 0.001), relative performance R(50) stays ≈95.9% (Abstract; Fig. 4; Table 1). This demonstrates impact under strong non-IID.\n     • Beyond standard partitions, OvA-LP maintains R(50) ≥ 94.9% under Zipf quantity skew and feature clustering skew (Fig. 5). This breadth strengthens claims about heterogeneity robustness.\n     • Ablation shows stepwise gains: LP-softmax (56.3%) → OvA (95.4%) → OvA+two-stage (95.9%) (Sec. 4.2; Fig. 3/29), directly tying improvements to proposed components. This is evidence of technical soundness.- **Strong efficiency results**\n     • Per-round client/server times drop to ~14.9 ms/~1.9 ms for OvA-LP vs 23.8 ms/~3.0 ms for LP-Softmax (Table 4), indicating practical system efficiency; this matters for scalable FL.\n     • Compared to FFT-MoE and PFPT, total time and communication to Acc@95 are reduced by 10^2–10^4× (Tables 5–6; Appendix A.2 narrative), underscoring high impact for deployment.\n     • OvA-LP reaches Acc@95 in 1–3 rounds under full participation (Appendix A.1, Table 3), highlighting fast convergence and reduced round complexity.- **Evidence that pretrained geometry limits feature bias**\n     • Quantitative geometry metrics (Alignment, Intra, Inter, Ratio) show compact, well-separated clusters for pretrained encoders vs random initialization (Fig. 2; Sec. 3.2), supporting the claim that freezing helps bound feature skew-induced bias. This is technically sound and motivates linear probing.\n     • The qualitative link between geometry (alignment/separation) and reduced local bias is explicitly stated (Sec. 3.2 “From the bias–variance perspective…”). It clarifies why encoder freezing contributes to drift suppression.- **Label noise robustness benchmarked against prior work**\n     • On CIFAR-100 with symmetric/asymmetric noise, OvA-LP reports the smallest decline rates across noise ratios compared to FedLTF and other baselines (Table 2), supporting the claim of robustness to corrupted labels (Sec. 4.4.3).\n     • The use of “decline rate from baseline accuracy” normalizes across methods (Table 2), improving comparability and indicating potential impact in noisy real-world settings.- **Encoder and task domain variations**\n     • OvA-LP tracks IID trajectories across ViT-B/16, ViT-L/16, and DINOv2-L/14 on CIFAR-100 and TinyImageNet Dirichlet partitions (Fig. 6/42; Sec. 4.4.2), suggesting architecture/task-agnostic robustness.\n     • The brief overshoot under FedAvg is documented and explained (Sec. 4.2, Sec. 4.4.2), reflecting careful experimental reporting.Weaknesses\n   - Bolded titles with sub-point examples- **Limited theoretical depth and measurement of bias–variance claims**\n     • While Sec. 3.1–3.4 articulate mechanisms, the paper lacks formal bounds or theorems linking OvA-LP components to quantified reductions in local/global bias and variance; Table 1’s “✓/△/×” is qualitative rather than measured (“Bias–variance view of methods,” Table 1). This limits technical rigor.\n     • No empirical measurements of gradient variance or cross-class covariance are provided to validate the “variance amplification via softmax coupling” and its removal (Sec. 3.3). This weakens evidence for mechanism-level claims.\n     • The link from geometry metrics (Fig. 2) to actual drift reduction under non-IID is argued conceptually (Sec. 3.2) but not quantified (e.g., no measured bias terms b_i or B; Sec. 3.1 definitions).- **Baseline comparability and fairness concerns**\n     • Encoder sizes differ across methods (OvA-LP uses ViT-L/16; baselines use ViT-B/16 or ViT-B/32), potentially affecting absolute accuracy and efficiency (Sec. 4.1–4.3; Appendix B.2, Table 8). This complicates cross-method comparisons.\n     • Participation ratios differ (PFPT at 0.1; OvA-LP at 1.0), affecting convergence and variance (Appendix B.2, Table 8; Sec. 5; Appendix A.3). This challenges fairness in speed and robustness claims.\n     • Hyperparameters (batch size, epochs, optimizer settings) vary substantially across methods (Table 8), making it hard to attribute performance differences solely to method design; additionally, the Abstract assigns 10.1%/34.5% retention to PFPT/FFT-MoE, while the main text/figures map 10.1% to FFT-MoE and 34.5% to PFPT (Abstract; Fig. 4; Table 1; Sec. 4.3), which reduces reporting clarity.- **Incomplete reproducibility details for the two-stage schedule and OvA training**\n     • Stage 2 is described qualitatively (“large set of negatives” and “small fraction of positives as anchors,” Sec. 3.4) without precise ratios, sampling strategies, or head update rules per client, limiting replicability.\n     • The handling of negatives under extreme label skew (e.g., Shard-1) is not fully specified—e.g., whether each client trains all K heads with local negatives from its sole class or uses server-provided negatives; No direct evidence found in the manuscript.\n     • The exact per-round curriculum (beyond “round 1: Stage 1; subsequent rounds: Stage 2,” Sec. 4.1) lacks algorithmic pseudocode and sensitivity analyses (e.g., anchor fraction sweeps), constraining verification.- **Novelty claim appears broad and is not substantiated beyond narrative**\n     • The Abstract claims OvA-LP is “the first explicitly designed to suppress drift at its source within the PEFT-based FFT paradigm,” but the paper does not provide a systematic survey or comparative analysis to validate this claim; No direct evidence found in the manuscript.\n     • Related work notes prior OvA-based FL methods (FedOVA, FedABC, ATHENA-FL) and PEFT FFT approaches, but does not rigorously argue non-overlap with “source-level” prevention beyond description (Sec. 2). This reduces confidence in exclusivity.\n     • Table 1’s positioning is qualitative and does not include other OvA- or LP-based baselines adapted to PEFT FFT, leaving novelty relative to potential combinations uncertain; No direct evidence found in the manuscript.- **Efficiency claim of “per-round cost nearly independent of encoder size” lacks direct measurements**\n     • The Abstract states per-round cost is nearly independent of encoder size due to precomputed features, but the paper does not report per-round times across different encoders to validate the claim (Sec. 4.4.2 reports performance, not cost).\n     • While Table 4 provides per-round costs for OvA-LP vs LP-Softmax, it does not vary encoder scale; No direct evidence found in the manuscript.\n     • Precomputation overhead and memory footprint for storing features are not quantified (Sec. 3; Appendix A.1–A.2), limiting practical assessment.- **Scope limitations acknowledged but significant for real-world FL**\n     • Experiments assume full participation (Sec. 4.1), and partial participation slows convergence with remaining variance (Appendix A.3; Sec. 5), indicating limited robustness under realistic client availability.\n     • The study focuses on vision benchmarks (CIFAR-100, TinyImageNet) and does not evaluate text/speech or structured data (Sec. 5), limiting generality across modalities.\n     • OvA-LP is not combined with aggregation/personalization strategies despite claims of modularity (Sec. 2; Sec. 5), leaving questions about synergy and compounded effects under varied FL pipelines.- **Evaluation metric choice may obscure absolute performance differences**\n     • The main non-IID robustness metric R(t)=AccNonIID/AccIID uses each method’s own IID baseline (Sec. 4.1, 4.3), which helps normalization but may hide absolute gaps driven by encoder/hyperparameter differences (Fig. 4–6).\n     • Table 1 and several figures emphasize R(50) without consistent absolute accuracy reporting in the main text (absolute curves/tables are relegated to Appendix A.1/A.2, Fig. 7/8; Tables 3/5), reducing immediate transparency.\n     • Although Acc@95 is used extensively to summarize convergence/cost (Appendix A.1/A.2, Tables 3/5), it is not explicitly defined in the evaluation protocol (Sec. 4.1 defines R(t) but does not define Acc@95), creating ambiguity that affects interpretability (e.g., LP-Softmax Non-IID reports Acc@95 “27 ± 9” rounds in Table 3 while its R(50)=56.3% in Fig. 3/29 suggests it never approaches 95% under the ratio metric).Suggestions for Improvement\n   - Bolded titles with sub-point examples- **Deepen theory and measure mechanism-level effects**\n     • Provide formal analyses or bounds quantifying how OvA decoupling and the two-stage curriculum reduce local/global bias and variance under label/feature skew (expand Sec. 3.1–3.4 with theorems or propositions).\n     • Empirically measure gradient variance and cross-class covariance with softmax vs OvA on representative partitions (extend Fig. 3/29 and Table 1 with measured statistics).\n     • Quantify drift metrics (e.g., b_i, B from Sec. 3.1) across methods to directly link geometry and schedule to reduced bias and aggregation variance.- **Strengthen baseline fairness with controlled settings**\n     • Re-run FFT-MoE and PFPT with the same encoder (e.g., ViT-L/16) and aligned hyperparameters to isolate algorithmic differences (complement Appendix B.2, Table 8).\n     • Align participation ratios across methods (e.g., full participation vs matched ratios) and report results for both, analyzing sensitivity (use Appendix A.3 template); also correct reporting mismatches so that headline statements in the Abstract align with Fig. 4/Table 1/Sec. 4.3 regarding which baseline attains ~10.1% vs ~34.5%.\n     • Include hyperparameter tuning and report comparable batch sizes/epochs/optimizers, or add an ablation showing that OvA-LP’s advantage persists under matched settings (extend Tables 5–6).- **Detail the two-stage curriculum and OvA training for reproducibility**\n     • Provide pseudocode specifying which heads are trained per client in Stage 1/2, sampling ratios for positives/negatives, and anchor fractions; include default values used in Sec. 4.1 runs.\n     • Clarify negative sampling under Shard-1/Shard-2 (e.g., using local non-c positives for other heads) and whether any server-side negatives or memory banks are used; add a schematic to Fig. 1.\n     • Add sensitivity analyses on anchor fractions and negative set size, reporting stability/accuracy curves (complement Fig. 3/29 and Table 3).- **Substantiate novelty claims with systematic comparisons**\n     • Include a literature matrix comparing source-level drift prevention vs post-hoc correction across PEFT FFT and OvA/LP methods (augment Sec. 2), citing [21–24], [8–10], [16], [19–20] as applicable.\n     • Adapt prior OvA-based FL methods (FedOVA, FedABC, ATHENA-FL) to the PEFT FFT setting and compare against OvA-LP under identical conditions (add to Sec. 4.3/Appendix A.2).\n     • Calibrate the novelty statement in the Abstract/Intro to reflect empirical evidence and scope, or add rigorous justification (e.g., absence of prior PEFT FFT source-level designs).- **Validate efficiency independence with encoder scale and account for precomputation**\n     • Report per-round client/server times and communications for ViT-B/16, ViT-L/16, DINOv2-L/14 (extend Fig. 6/42 with cost tables), substantiating “nearly independent” per-round cost claims.\n     • Quantify one-time precomputation overhead and amortize it over rounds, including memory/storage footprint for cached features (extend Tables 4–6).\n     • Add scalability results with larger encoders and varying client dataset sizes to demonstrate independence across practical ranges.- **Broaden scope to realistic FL settings and modalities**\n     • Provide partial participation experiments with participation sweeps and variance-control strategies (e.g., Stage 1 for more rounds, stratified sampling), expanding Appendix A.3 into the main text.\n     • Evaluate non-vision domains (e.g., text classification with pretrained transformers) to test OvA-LP’s generality beyond vision (add to Sec. 4.4.2).\n     • Combine OvA-LP with aggregation (e.g., SCAFFOLD, FedProx) and personalization (adapters/prompts) to test modularity and synergy (extend Sec. 4.3).- **Complement ratio-based evaluation with absolute metrics and statistics**\n     • Report absolute accuracy and variance (error bars over seeds) in the main text figures/tables alongside R(t)/R(50) (move Appendix A.1/A.2 results forward), and define Acc@95 precisely (e.g., 95% of IID accuracy, or an absolute threshold) in Sec. 4.1 to align with Tables 3/5.\n     • Add statistical tests (e.g., paired t-tests across seeds) for key comparisons (e.g., Fig. 4 baseline gaps), improving experimental rigor.\n     • Include additional metrics (e.g., calibration, per-class accuracy under label imbalance) to ensure that robustness is not achieved at the expense of fairness or calibration.Score\n     - Overall (10): 7 — Strong empirical robustness and efficiency (Fig. 4–6; Tables 3–6) with a clear, simple design, but theory and baseline fairness need strengthening and some reporting inconsistencies persist (Sec. 3.1–3.4; Abstract vs. Fig. 4/Table 1; Appendix B.2, Table 8).\n     - Novelty (10): 6 — Combines known components into a source-level framework with convincing results (Sec. 3; Fig. 1), yet the “first” claim is not substantiated beyond narrative (Abstract; Sec. 2; Table 1).\n     - Technical Quality (10): 6 — Method is sound and experiments are broad (Fig. 5–6; Table 2), but lacks formal analyses and measured mechanism-level statistics (Sec. 3.3; Table 1).\n     - Clarity (10): 7 — Generally well-written with clear figures (Sec. 3.1–3.3; Fig. 1–2; Fig. 3–6), but Acc@95 is undefined in the evaluation protocol (Sec. 4.1 vs. Appendix A.1/A.2), and the Abstract reverses the 10.1%/34.5% baseline mapping compared to the main text (Fig. 4; Table 1; Sec. 4.3).\n     - Confidence (5): 4 — High confidence based on detailed experiments and ablations (Sec. 4; Appendix A.1–A.3), though some claims (novelty, efficiency independence) and reporting clarity would benefit from additional evidence/definitions (Abstract; Sec. 4.4.2; Table 8)."
}