{
  "paper": "OVA-LP_ A Simple and Efficient Framework for Federated Learning on Non-IID Data",
  "runs": [
    {
      "config": {
        "judge_model": "gemini-2.5-pro",
        "review_agent": "deepreviewer",
        "ai_model": "gpt-5-2025-08-07",
        "ai_review_file": "deep_review_all.txt",
        "temperature": 0.0,
        "max_tokens": 20000,
        "timeout_s": 180,
        "metric": "coverage_v1"
      },
      "config_key": "55f39c3e353412551a5ae68e81fc4acc489b4edd",
      "inputs": {
        "human_review": "review_origin.txt",
        "ai_review": "reviews/deepreviewer/gpt-5-2025-08-07/paper_origin/deep_review_all.txt"
      },
      "multi_review": {
        "detected": false,
        "num_reviews": 1,
        "summarize_used": false
      },
      "ai_review_consolidated": null,
      "alignment": {
        "strength_coverage_recall": 0.75,
        "weakness_coverage_recall": 0.5,
        "ai_extra_major_points_rate": 0.36,
        "symmetric_coverage_similarity": 0.35,
        "note": "Review B covers core weaknesses like unfair comparisons and reliance on full participation, but misses Review A's critical point that the main performance gain may come from simply freezing the encoder. Conversely, Review B raises several major points absent in Review A, including the lack of formal theoretical analysis and practical concerns about the storage/privacy costs of precomputed features."
      },
      "generated_at": "2025-12-27T23:47:45"
    },
    {
      "config": {
        "judge_model": "gemini-2.5-pro",
        "review_agent": "paper_audit",
        "ai_model": "gpt-5-2025-08-07",
        "ai_review_file": "baseline_review.txt",
        "temperature": 0.0,
        "max_tokens": 20000,
        "timeout_s": 180,
        "metric": "coverage_v1"
      },
      "config_key": "b3137abdd5dd456ecfb029f7bed4151251d086e2",
      "inputs": {
        "human_review": "review_origin.txt",
        "ai_review": "reviews/paper_audit/gpt-5-2025-08-07/paper_origin/baseline_review.txt"
      },
      "multi_review": {
        "detected": false,
        "num_reviews": 1,
        "summarize_used": false
      },
      "ai_review_consolidated": null,
      "alignment": {
        "strength_coverage_recall": 0.8,
        "weakness_coverage_recall": 0.5,
        "ai_extra_major_points_rate": 0.53,
        "symmetric_coverage_similarity": 0.39,
        "note": "The AI review identifies most technical strengths but misses the specific human praise for the codebase and the conceptual critique regarding the definition of 'fine-tuning' versus 'linear probing.' Additionally, the AI introduces several new critiques regarding theoretical bounds, text-based reproducibility, and specific efficiency verifications that were not present in the human review."
      },
      "generated_at": "2025-12-27T23:53:02"
    },
    {
      "config": {
        "judge_model": "gemini-2.5-pro",
        "review_agent": "paper_audit",
        "ai_model": "gpt-5-2025-08-07",
        "ai_review_file": "final_review_all.txt",
        "temperature": 0.0,
        "max_tokens": 20000,
        "timeout_s": 180,
        "metric": "coverage_v1"
      },
      "config_key": "0a71ca7d783136ba33fb6b5ec9aad08ece2f0c09",
      "inputs": {
        "human_review": "review_origin.txt",
        "ai_review": "reviews/paper_audit/gpt-5-2025-08-07/paper_origin/final_review_all.txt"
      },
      "multi_review": {
        "detected": false,
        "num_reviews": 1,
        "summarize_used": false
      },
      "ai_review_consolidated": null,
      "alignment": {
        "strength_coverage_recall": 0.75,
        "weakness_coverage_recall": 0.25,
        "ai_extra_major_points_rate": 0.6,
        "symmetric_coverage_similarity": 0.32,
        "note": "Review B misses the fundamental critique in Review A that the method is effectively 'Federated Linear Probing' and that performance gains stem primarily from freezing the encoder rather than the proposed design. While Review B flags baseline unfairness, it attributes this to minor configuration mismatches (model size) rather than the critical 'frozen vs. adaptive' distinction. Review B also contradicts Review A on reproducibility, criticizing text details where Review A praises the code."
      },
      "generated_at": "2025-12-28T00:04:33"
    },
    {
      "config": {
        "judge_model": "gemini-2.5-pro",
        "review_agent": "deepreviewer",
        "ai_model": "gpt-5-2025-08-07",
        "ai_review_file": "deep_review_merge.txt",
        "temperature": 0.0,
        "max_tokens": 20000,
        "timeout_s": 180,
        "metric": "coverage_v1"
      },
      "config_key": "752f27bc09e656c7d041a880d0c174422eea6ca4",
      "inputs": {
        "human_review": "review_origin.txt",
        "ai_review": "reviews/deepreviewer/gpt-5-2025-08-07/paper_origin/deep_review_merge.txt"
      },
      "multi_review": {
        "detected": false,
        "num_reviews": 1,
        "summarize_used": false
      },
      "ai_review_consolidated": null,
      "alignment": {
        "strength_coverage_recall": 0.67,
        "weakness_coverage_recall": 0.6,
        "ai_extra_major_points_rate": 0.21,
        "symmetric_coverage_similarity": 0.57,
        "note": "The AI review captured the most significant points regarding unfair baseline comparisons, limited scope, and superficial theory. However, it missed more nuanced human critiques about the work's incremental nature and the overclaiming of 'fine-tuning' versus 'linear probing'. The AI added valid new points about missing implementation details and how the presentation of results obscures absolute performance."
      },
      "generated_at": "2025-12-28T00:18:26"
    }
  ]
}