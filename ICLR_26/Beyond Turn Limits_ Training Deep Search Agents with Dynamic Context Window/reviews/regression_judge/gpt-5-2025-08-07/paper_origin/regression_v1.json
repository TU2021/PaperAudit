{
  "paper": "Beyond Turn Limits_ Training Deep Search Agents with Dynamic Context Window",
  "runs": [
    {
      "config": {
        "judge_model": "gpt-5-2025-08-07",
        "review_agent": "paper_audit",
        "ai_model": "gpt-5-2025-08-07",
        "baseline_file": "baseline_review.txt",
        "final_file": "final_review_all.txt",
        "temperature": 0.0,
        "max_tokens": 20000,
        "timeout_s": 600,
        "metric": "regression_v2_minimal",
        "prompt_hash": "2142c04332"
      },
      "config_key": "1a46f14f5902ebb0fc2c14773cec6644d2fa5338",
      "inputs": {
        "baseline_review": "reviews/paper_audit/gpt-5-2025-08-07/paper_origin/baseline_review.txt",
        "final_review": "reviews/paper_audit/gpt-5-2025-08-07/paper_origin/final_review_all.txt"
      },
      "regression": {
        "score_delta": {
          "baseline_score": 7.0,
          "final_score": 7.0,
          "delta": 0.0,
          "scale_hint": "1-10"
        },
        "differences": [
          {
            "diff_type": "new_critique",
            "summary": "Internal inconsistencies in context accounting and sequence/mask definitions",
            "paperaudit_types": [
              "METHOD_LOGIC_CONSISTENCY",
              "CONTEXT_MISALIGNMENT_INCOHERENCE"
            ],
            "why_impacts_score": "Undermines core mechanism consistency and reproducibility",
            "evidence": {
              "baseline_quote": "Training-testing consistency is carefully handled by decomposing a trajectory into K sequences and masking assistant responses so each is trained exactly once.",
              "final_quote": "Figure 2 reports 'Trajectory Context ≈14k' while 'Assistant Context ≈16k/20k'; Assistant Context should not exceed it."
            }
          },
          {
            "diff_type": "intensified_critique",
            "summary": "Evaluation protocol conflict between judge usage and baseline adoption",
            "paperaudit_types": [
              "EXPERIMENTAL_DESIGN_PROTOCOL",
              "CLAIM_RESULT_DISTORTION",
              "CONTEXT_MISALIGNMENT_INCOHERENCE"
            ],
            "why_impacts_score": "Compromises fairness; reported gains may be biased",
            "evidence": {
              "baseline_quote": "Main results adopt 'official results' of baselines rather than re-running under identical tools, prompts, and judges.",
              "final_quote": "‘All results are evaluated using ChatGPT-4o-Latest’ conflicts with ‘We adopt the official results reported in their papers’."
            }
          },
          {
            "diff_type": "new_critique",
            "summary": "Fictional dataset example (‘Mr. Capybara’) creates realism confusion",
            "paperaudit_types": [
              "RHETORICAL_PRESENTATION_MANIPULATION",
              "EVIDENCE_DATA_INTEGRITY"
            ],
            "why_impacts_score": "Raises doubt about dataset realism and transparency",
            "evidence": {
              "baseline_quote": "The pipeline uses Wikipedia for entity selection and authentic multi-source web pages via Google Search.",
              "final_quote": "The illustrative pipeline example uses a fictional entity ‘Mr. Capybara’ without an explicit disclaimer, creating confusion."
            }
          },
          {
            "diff_type": "intensified_critique",
            "summary": "Contradictory turn-limit claims and unreconciled runtime costs",
            "paperaudit_types": [
              "METHOD_LOGIC_CONSISTENCY",
              "RHETORICAL_PRESENTATION_MANIPULATION"
            ],
            "why_impacts_score": "Reduces trust in scalability and efficiency claims",
            "evidence": {
              "baseline_quote": "The trajectory limits (40k tokens, turns=60) and achieving ‘nearly 100 turns’ are not reconciled with runtime costs.",
              "final_quote": "‘nearly 100 turns within standard 32k’ vs. ‘exceed 100 turns within 40k contexts’ are not reconciled with runtime costs."
            }
          },
          {
            "diff_type": "dropped_strength",
            "summary": "Clarity praise reduced due to notational ambiguities",
            "paperaudit_types": [
              "CONTEXT_MISALIGNMENT_INCOHERENCE",
              "RHETORICAL_PRESENTATION_MANIPULATION"
            ],
            "why_impacts_score": "Harms readability and reproducibility",
            "evidence": {
              "baseline_quote": "Illustrative figures clearly convey pipeline, context dynamics, and training behavior, enhancing clarity.",
              "final_quote": "Clarity (10): 6 — offset by notational ambiguities and evaluation protocol inconsistency."
            }
          },
          {
            "diff_type": "score_rationale_shift",
            "summary": "Technical Quality score lowered due to new inconsistencies",
            "paperaudit_types": [
              "METHOD_LOGIC_CONSISTENCY"
            ],
            "why_impacts_score": "Directly lowers technical confidence",
            "evidence": {
              "baseline_quote": "Technical Quality (10): 6 — Sound algorithmic framing and empirical wins, tempered by missing ablations and limited quantitative evidence.",
              "final_quote": "Technical Quality (10): 5 — key inconsistencies in context accounting and sequence/mask definitions reduce technical confidence."
            }
          }
        ]
      },
      "generated_at": "2026-01-05T20:43:58"
    }
  ]
}