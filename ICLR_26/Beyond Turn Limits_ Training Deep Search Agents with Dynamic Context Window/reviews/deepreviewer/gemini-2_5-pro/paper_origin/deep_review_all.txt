### Review 1

**Summary**
The paper introduces DeepMiner, a framework to train deep search agents for long-horizon tasks. It tackles two key challenges: the lack of complex training data and the context length limitations of LLMs. The authors propose a "reverse construction" method to generate difficult QA pairs and a dynamic sliding window for context management that preserves reasoning traces while compressing tool outputs. The resulting model, DeepMiner-32B, demonstrates state-of-the-art performance on several search agent benchmarks.

**Soundness**
The methodology is sound and well-motivated. The identification of context explosion from tool responses as a key bottleneck (Figure 2) is insightful. The proposed sliding window mechanism is an elegant and effective solution that avoids the pitfalls of external summarization models. The two-stage training process (SFT followed by RL) is a standard and robust approach, and its adaptation to the sliding window mechanism (Figure 3) appears logically consistent, ensuring training-inference alignment. The experimental setup is thorough, with evaluations on multiple challenging benchmarks and relevant baselines.

**Presentation**
The paper is exceptionally well-written and easy to follow. The introduction clearly lays out the problems and the proposed solutions. The figures are highly effective, particularly Figure 1 (data construction pipeline), Figure 2 (context analysis), and the diagram in Figure 3 (sliding window mechanism), which clearly illustrate the core concepts. The results in Table 1 are presented compellingly, highlighting the significant performance gains.

**Contribution**
The paper makes two significant contributions. First, the dynamic context management strategy is a simple yet powerful technique that dramatically extends the interaction horizon of agents without requiring larger models or external tools. This is a practical and impactful contribution to the field of LLM agents. Second, the reverse construction method for generating high-difficulty training data provides a valuable recipe for creating challenging benchmarks and training sets that can elicit more sophisticated reasoning. The combination of these two elements leads to a substantial leap in open-source agent capabilities.

**Strengths**
1.  **Impressive Empirical Results:** The performance of DeepMiner-32B is a standout strength. Achieving 33.5% on BrowseComp-en, nearly 20 points higher than the previous SOTA open-source agent and outperforming a 20x larger model (Table 1), is a remarkable achievement.
2.  **Elegant Context Management:** The sliding window approach is a simple, effective, and computationally efficient solution to a critical problem in long-horizon agents. Its ability to enable ~100 turns within a 32k context (Section 4.3) is a game-changer.
3.  **High-Quality Data Generation:** The paper details a well-designed pipeline for creating complex and verifiable training data (Section 2), which is shown to be more effective than existing datasets like HotpotQA (Table 3).
4.  **Clarity and Presentation:** The paper is very well-written, with clear explanations and excellent visualizations that make complex ideas accessible.

**Weaknesses**
1.  **Hyperparameter Sensitivity:** The paper sets the window size to 5 and slide step to 3 for experiments (Section 4.1). It would be beneficial to include an analysis of how sensitive the model's performance is to these hyperparameters. Is there a risk of "forgetting" crucial early information if the window is too small or the slide step too large?
2.  **Limited Scope of Tools:** The tool suite (web_search, fetch, find) is standard but relatively simple (Section 4.1). The paper's claims about "deep cognitive behaviors" might be further strengthened by demonstrating the agent's performance with a more complex set of tools.

**Questions**
1.  The sliding window mechanism replaces old tool outputs with a placeholder token: “[Previous tool output skipped. Rerun tool if needed].” (Section 3.1). Have you observed the agent actually attempting to "rerun" a tool to retrieve this skipped information? If so, how does it handle this, and does it affect performance?
2.  The reverse construction method focuses on entities with "moderate visibility" (Section 2). How does this choice impact the diversity of the generated questions? Could this introduce a bias towards certain types of biographical or news-related queries?
3.  Could the dynamic context window approach be applied to other long-horizon tasks beyond web search, such as multi-turn dialogue, story generation, or complex code generation?

**Rating**
- Overall (10): 9 — The paper presents a highly effective and practical framework that achieves state-of-the-art results on challenging benchmarks (Table 1).
- Novelty (10): 8 — While sliding windows are not new, their application to selectively compress tool outputs while preserving reasoning traces in agent training is novel and impactful (Section 3.1).
- Technical Quality (10): 9 — The methodology is well-designed, the experiments are thorough, and the results are strong and well-analyzed (Section 4).
- Clarity (10): 10 — The paper is exceptionally clear, well-organized, and supported by excellent figures and tables (Figure 1, Figure 3).
- Confidence (5): 5 — I am highly confident in my assessment, as the paper's claims are well-supported by strong empirical evidence.

---
### Review 2

**Summary**
This paper proposes DeepMiner, a framework for training long-horizon search agents. The authors identify two main issues: inadequate task complexity and context length limits. They address these with a "reverse construction" method for generating difficult questions and a sliding window mechanism for context management. They train a Qwen3-32B model using SFT and RL, showing significant performance gains on benchmarks like BrowseComp.

**Soundness**
The overall methodological direction is reasonable, but several aspects lack rigorous justification. The novelty of the "dynamic context window" is overstated; sliding window attention is a well-known technique. The key adaptation here is applying it at the level of tool calls, which is a simple engineering choice. The claim that this avoids "optimization blind spots" compared to summarization (Section 1) is not fully substantiated, as information is still being lost, just in a different manner. The data generation pipeline (Section 2) has many filtering steps, but the criteria (e.g., "unreliable websites," "interpretation ambiguity") are subjective and rely on LLMs, which could introduce subtle biases. The RL training (Section 3.3) propagates a single trajectory-level advantage to all derived training sequences, which seems like a coarse approximation. A sequence that has lost critical context due to the window sliding might be unfairly penalized or rewarded based on the final outcome.

**Presentation**
The paper is generally well-written, but some figures are confusing or poorly labeled. The mermaid diagram in Figure 3 is abstract and less informative than the graphical depiction in Figure 18. The plots in Figure 4 are noisy and presented without error bars, making it difficult to assess the stability of the training process. The x-axis label "Training Step" is vague; are these gradient steps, epochs, or something else? The two plots in Figure 5 have identical titles ("Scaling on tool call budget and context length") but different x-axes, which is confusing. The left plot in Figure 5 should be titled "Performance vs. Number of Tool Calls" and the right one "Performance vs. Context Length".

**Contribution**
The primary contribution appears to be empirical. The authors have successfully combined existing techniques (RLVR, sliding windows, data synthesis) to build a high-performing search agent. The performance jump is impressive (Table 1), but the conceptual novelty is limited. The sliding window for tool outputs is a pragmatic heuristic, not a fundamental advance in context management. The data generation pipeline is a complex, multi-stage process that may be difficult for others to reproduce precisely.

**Strengths**
1.  **Strong Empirical Results:** The paper's main strength is the final performance of the DeepMiner-32B model, which significantly advances the state of the art for open-source search agents on several benchmarks (Table 1).
2.  **Clear Problem Formulation:** The introduction effectively frames the dual problems of task complexity and context management, motivating the work well (Section 1).
3.  **Ablation Study on Context Management:** The comparison of Vanilla, Summary, and DeepMiner strategies in Table 2 is a useful analysis that clearly demonstrates the efficiency of the proposed method.

**Weaknesses**
1.  **Limited Novelty of Core Techniques:** The core ideas—generating hard data and using a sliding window for context—are not fundamentally new. The paper's contribution lies more in the engineering and successful application of these ideas than in creating new ones.
2.  **Potential for Information Loss:** The paper claims to avoid information loss from summarization, but simply dropping entire tool outputs is a more extreme form of information loss. This could be problematic for tasks requiring synthesis of information from many early steps. The assumption that "distant tool responses" have "minimal impact" (Section 3.1) is a strong one and may not hold universally.
3.  **Reproducibility Concerns:** The data generation pipeline (Section 2) involves multiple LLM-based generation and filtering steps with subjective criteria. This makes the dataset difficult to reproduce exactly, which could hinder future research building on this work.
4.  **Evaluation Methodology:** The use of ChatGPT-4o as the sole judge for evaluation (Section 4.1) is a potential weakness. LLM-based evaluation can be noisy and biased. It would be stronger to include human evaluation or evaluation on benchmarks with more objective, verifiable answers.

**Questions**
1.  In the RL advantage propagation (Section 3.3), you assign the same advantage $\hat{A}_i$ to all sequences derived from a trajectory $\tau_i$. Have you considered a more fine-grained credit assignment? For instance, could a model be penalized for an action that was correct given its local (windowed) context, but which was part of an overall failing trajectory?
2.  The data filtering process (Section 2) discards questions that can be answered by a zero-shot LLM prompt. Given the rapid improvement of base models, how do you ensure that your dataset remains "difficult" over time? Does this filtering risk removing valid multi-hop questions that simply become easier for newer models?
3.  Could you provide a more detailed analysis of failure cases? When DeepMiner fails, is it due to information being lost from the sliding window, incorrect tool usage, or flawed reasoning based on the available information?

**Rating**
- Overall (10): 7 — The paper achieves impressive results through careful engineering, but the novelty of the core methods is limited and some claims are overstated (Section 3.1).
- Novelty (10): 5 — The application of a sliding window to tool outputs is a pragmatic choice but not a major conceptual innovation (Section 3.1).
- Technical Quality (10): 7 — The methodology is mostly sound, but the RL advantage approximation is coarse and the evaluation relies on a single LLM judge (Section 3.3, 4.1).
- Clarity (10): 7 — The paper is mostly clear, but several figures are confusingly labeled or presented without sufficient detail (Figure 4, Figure 5).
- Confidence (5): 4 — I am reasonably confident in my assessment, though a deeper dive into the source code would be needed to fully assess reproducibility.

---
### Review 3

**Summary**
The paper presents DeepMiner, a framework for building web search agents capable of long interactions. It addresses context length limits with a sliding window that drops old tool outputs and tackles the lack of hard training examples with a "reverse construction" data synthesis pipeline. The resulting agent, trained on Qwen3-32B, shows major improvements on several academic benchmarks.

**Soundness**
The approach is pragmatic and effective for the benchmarked tasks. The core idea of prioritizing the agent's reasoning ("assistant context") over raw data ("tool responses") is intuitive and well-justified by the analysis in Figure 2. The training-inference consistency for the sliding window is a crucial detail that the authors have handled correctly (Section 3.1). The two-stage SFT+RL training is a proven method. The experiments seem well-conducted and the results are compelling.

**Presentation**
The paper is well-structured and the core ideas are communicated clearly. The visualizations, especially the graphical depiction of the sliding window in Figure 18 (Appendix), are very helpful for understanding the mechanism. The main results table (Table 1) is comprehensive and easy to interpret. The case study in the appendix (Section E.1) is illustrative, though it also highlights the meandering and inefficient nature of the search process, even in a successful trajectory.

**Contribution**
The main contribution is a practical and replicable framework for substantially improving open-source search agents. The dynamic context window is a highly practical technique that other developers can immediately adopt to make their agents more robust to long conversations. While not a paradigm shift, it's a valuable engineering contribution. The data generation method, while complex, provides a template for creating more challenging training environments.

**Strengths**
1.  **Practicality and Efficiency:** The sliding window mechanism is a simple, low-overhead solution that dramatically improves the agent's operational range (Section 4.3). It doesn't require an extra summarizer model, reducing system complexity and cost.
2.  **Strong Performance:** The model achieves impressive results, significantly closing the gap with proprietary systems on benchmarks like BrowseComp (Table 1). This is a valuable contribution to the open-source community.
3.  **End-to-End Optimization:** A key advantage over summarization-based methods is that the entire agent policy can be trained end-to-end with RL, as the context management is a fixed, non-learnable part of the process (Section 1).
4.  **Detailed Appendix:** The appendix provides useful details on tools, templates, and a case study, which adds to the paper's practical value.

**Weaknesses**
1.  **Complexity of Data Pipeline:** The reverse construction pipeline for data generation (Section 2) is very elaborate. It involves multiple stages of entity selection, searching, filtering, LLM-based generation, and LLM-based filtering. This seems expensive and hard to scale, potentially limiting its adoption by teams with fewer resources.
2.  **Inefficient Search Strategy:** The case study (Appendix E.1) shows the agent taking 60+ steps, often going in circles and trying irrelevant search queries (e.g., searching for tornadoes in Warsaw or Rome). While it eventually finds the answer, this brute-force exploration seems highly inefficient and would be costly in terms of API calls in a real-world product. The high turn count may be a double-edged sword.
3.  **Robustness to Real-World Web:** The experiments are run on static benchmarks. The real web is messy, with dynamic content, ads, pop-ups, and varied HTML structures. The `fetch` tool returning clean Markdown (Section 4.1) is an idealization. The paper does not discuss how the agent would cope with the noise of the real web.

**Questions**
1.  What is the computational cost and wall-clock time required to run the full data generation pipeline described in Section 2 to produce the ~7,000 examples used for SFT and RL?
2.  The case study shows a very long and winding path to the solution. Does the reward function (Section 3.3) incorporate any notion of efficiency (e.g., penalizing for excessive steps)? If not, could this lead the RL process to favor long, meandering but eventually successful trajectories over shorter, more direct ones?
3.  The `fetch` tool uses pagination (Appendix C). How does the agent decide to navigate to the next page versus giving up on a document? Is this behavior learned, and how does the sliding window interact with information gathered across multiple pages of the same document?

**Rating**
- Overall (10): 8 — A strong paper with impressive results and a very practical context management technique, though with some concerns about the efficiency of the agent's strategy (Appendix E.1).
- Novelty (10): 6 — The contribution is more in the novel combination and effective engineering of existing ideas rather than groundbreaking new concepts (Section 3).
- Technical Quality (10): 8 — The training and evaluation are executed well, but the agent's reasoning efficiency in practice is a concern (Appendix E.1).
- Clarity (10): 9 — The paper is very clearly written and well-organized, with helpful figures and a detailed appendix.
- Confidence (5): 5 — I am highly confident in my evaluation based on the detailed description of the methods and results.

---
### Review 4

**Summary**
This paper introduces DeepMiner, a framework aimed at enhancing the long-horizon reasoning capabilities of web search agents. The authors propose a two-pronged approach: 1) a "reverse construction" method to generate complex, multi-source question-answering data for training, and 2) a dynamic sliding window for context management that discards distant tool outputs while retaining the agent's own reasoning steps. By training a 32B parameter model with this framework, they achieve state-of-the-art results on several challenging agent benchmarks, significantly outperforming prior open-source models.

**Soundness**
The methodology is generally sound and well-reasoned. The paper correctly identifies two critical bottlenecks for current agents and proposes plausible solutions for each. The empirical analysis motivating the context management strategy (Figure 2) is convincing. The strategy to maintain training-testing consistency by decomposing trajectories into multiple training sequences (Section 3.1) is technically correct and important for the method's success. The experimental design is comprehensive, including multiple benchmarks, strong baselines, and insightful analyses of context scaling and data efficiency.

**Presentation**
The paper is well-written, logically structured, and easy to comprehend. The introduction provides excellent motivation. The methods section is detailed, and the use of figures (e.g., Figure 1, Figure 18) effectively clarifies the proposed data generation and context management pipelines. The results are presented clearly in tables and graphs, allowing for easy comparison and interpretation. Minor issues exist, such as the confusingly titled plots in Figure 5, but these do not detract significantly from the overall high quality of the presentation.

**Contribution**
The paper makes a solid contribution to the field of LLM agents. The primary contribution is the demonstration that a combination of better data and more efficient context management can lead to dramatic performance improvements, closing a significant portion of the gap between open-source and proprietary systems. The dynamic context window is a valuable and practical technique. The data generation pipeline, while complex, serves as a useful case study in creating challenging training curricula for agents. The resulting DeepMiner-32B model is itself a valuable artifact for the research community.

**Strengths**
1.  **Significant Performance Improvement:** The paper reports substantial gains over existing open-source agents across four different benchmarks (Table 1), which is a strong and clear result.
2.  **Dual-Pronged Solution:** The work effectively addresses both the data quality and context length problems, recognizing that both are necessary for improving long-horizon reasoning. The data efficiency comparison with HotpotQA (Table 3) validates the data construction approach.
3.  **Efficient and Elegant Context Management:** The sliding window approach is simple to implement, computationally cheap, and shown to be highly effective (Table 2), avoiding the complexity and potential information loss of separate summarization models.
4.  **Thorough Analysis:** The paper includes several useful analyses, including the training dynamics (Figure 4), scaling laws with respect to context and tool calls (Figure 5), and a direct comparison of context management strategies (Table 2).

**Weaknesses**
1.  **Interaction Between Contributions:** The paper presents two main contributions (data generation and context management) but does not provide an ablation study to disentangle their effects. How much of the performance gain comes from the new dataset versus the sliding window? Training the SFT model with the new data but without the sliding window (and vice-versa) would have provided a clearer picture of each component's impact.
2.  **Generalizability of the Context Strategy:** The core assumption is that assistant reasoning traces are more important than old tool outputs. While this seems plausible for information-seeking tasks where the agent synthesizes information as it goes, it might be a detrimental heuristic for tasks that require comparing or referencing specific details from multiple, distant tool calls.
3.  **Ethical Considerations Discussion:** The ethical considerations section (Appendix B) acknowledges the risk of collecting personal information but states it will be mitigated by filtering and anonymization. This is a good first step, but the process of anonymization is notoriously difficult to perform perfectly, and a more detailed discussion of the potential failure modes would be welcome.

**Questions**
1.  To better understand the relative importance of your two main contributions, could you provide results for an ablation study where you train a model on your new dataset but with a standard context management strategy (e.g., truncation or summarization), and another model trained on a standard dataset (like HotpotQA) but using your dynamic context window?
2.  The placeholder token for omitted tool outputs says "Rerun tool if needed" (Section 3.1). Does the model learn to use this affordance? If an agent loses a critical piece of information from an early step, can it recognize this and successfully retrieve it later in the trajectory?
3.  How does the model perform on tasks that are less about finding a single final answer and more about synthesis, comparison, or generating a comprehensive report, where retaining details from multiple sources over a long history might be more critical?

**Rating**
- Overall (10): 8 — A strong, well-executed paper with significant results, a practical context management technique, and a valuable new dataset (Table 1, Table 2).
- Novelty (10): 7 — The paper combines existing ideas in a novel and highly effective way, with the specific application of the sliding window to agent context being a key insight (Section 3.1).
- Technical Quality (10): 8 — The technical execution is solid, though a direct ablation between the data and context contributions is missing (Section 4).
- Clarity (10): 9 — The paper is very well-written and organized, with clear explanations and effective use of visuals.
- Confidence (5): 5 — I am confident in my assessment of the paper's strengths and weaknesses based on the provided text.