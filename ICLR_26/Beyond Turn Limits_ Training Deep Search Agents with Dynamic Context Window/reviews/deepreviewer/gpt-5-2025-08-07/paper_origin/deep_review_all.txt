Summary
The paper introduces DeepMiner, a training framework for long-horizon, multi-turn web search agents that combines (a) reverse-constructed, high-difficulty QA tasks sourced from authentic multi-page web content (Sec. 2; Fig. 1) and (b) a dynamic sliding-window context strategy that compresses distant tool outputs while preserving assistant reasoning traces (Sec. 3.1; Fig. 2–3). The system is cold-started via supervised fine-tuning (Sec. 3.2) and optimized with GRPO-style reinforcement learning adapted to sequence-level training under sliding contexts (Sec. 3.3). Evaluations on BrowseComp-en/zh, XBench-DeepSearch, and GAIA show substantial gains over open-source agents, with 33.5% on BrowseComp-en (Table 1), improved scalability with turn budgets and context length (Fig. 5), and efficient context use versus summarization (Table 2). The paper includes a data-efficiency comparison versus HotpotQA (Table 3) and discusses tool configuration (Sec. 4.1; Appendix C) and evaluation settings (Sec. 4.1; Appendix D).

Soundness
The sliding-window design is logically motivated by empirical context saturation analyses (Sec. 3.1; Fig. 2 right) and implemented with consistent training-testing alignment by decomposing trajectories into sequences with masking (Sec. 3.1; Fig. 3; Mi(k) rule). The advantage propagation from trajectory-level rewards to sequence-level learners in GRPO (Sec. 3.3; Eq. 1) is reasonable for sparse terminal rewards. However, the claim that distant tool outputs minimally affect decisions (Sec. 3.1) is asserted but not quantitatively established (no ablation measuring performance when omitting tool content beyond the window). Reward design relies on an LLM judge with binary correctness (Sec. 3.3; Appendix D), which is practical but introduces potential bias; the authors mitigate by using precise templates (Appendix D). Overall, methods are internally consistent and well-integrated, though some empirical assumptions (e.g., locality of tool outputs) would benefit from deeper measurement.

Presentation
The paper is clear and well-organized: problem framing (Sec. 1), method (Sec. 2–3), experiments (Sec. 4), discussion (Sec. 5–6), and appendices with templates and case studies. Figures (Fig. 1 pipeline; Fig. 2 context dynamics; Fig. 3 sliding sequences) and tables (Table 1–3) effectively communicate claims. Minor issues: the mermaid diagram (Fig. 3) is helpful but text labels (“Omit Tool”) could be standardized to the φ placeholder in Sec. 3.1; preliminary results with “gpt-oss-120b” (Fig. 2) could include a brief model description. The BrowseComp case study (Appendix E.1) nicely illustrates long-horizon behavior.

Contribution
The work addresses a key bottleneck for multi-turn agents: context explosion due to verbose tool outputs, offering a non-summarization strategy compatible with end-to-end RL (Sec. 3.1). The reverse construction of complex, multi-source, obfuscated QA tasks (Sec. 2) contributes a harder training signal tailored to deep search. Empirically, the framework substantially narrows the performance gap with proprietary agents on BrowseComp, showing stable performance over long horizons (Table 1; Fig. 5). The combination—data construction + dynamic context + RL under sequence decomposition—is novel and practically impactful for web agents.

Strengths
- Clear identification of two bottlenecks and coherent solutions (Sec. 1; Sec. 2; Sec. 3.1).
- Dynamic sliding-window context retains assistant reasoning while controlling tool bloat, with training-testing consistency (Sec. 3.1; Fig. 3).
- Strong empirical gains over open-source baselines across benchmarks (Table 1), and efficient context use vs summarization (Table 2).
- Useful analysis of scaling with tool-call budget and context length (Fig. 5), plus data-efficiency vs HotpotQA (Table 3).
- Practical tool suite design enabling fine-grained browsing without external summarization (Sec. 4.1; Appendix C).

Weaknesses
- Reliance on an LLM judge (ChatGPT-4o-Latest) for evaluation (Sec. 4.1; Appendix D) may bias results; no human or rule-based corroboration.
- Limited quantitative evidence for the locality assumption of tool outputs (Sec. 3.1)—no ablation measuring the effect of different window sizes W and slide steps S on accuracy/trajectory coherence.
- Baseline differences in tools/hyperparameters may confound comparisons; fairness is not fully audited (Sec. 4.1–4.2).
- Advantage propagation to all sequences (Sec. 3.3) could over-credit early steps; no credit assignment ablation or per-step variance analysis.
- Reproducibility: data release details and approval process (Appendix B) are cautious but practical availability, licensing, and scripts for the sliding window are not specified.

Questions
- Can you provide an ablation on window size W and slide step S (Sec. 3.1) showing trade-offs between accuracy, turn depth, and assistant-context retention?
- How sensitive is performance to the φ placeholder’s wording (Sec. 3.1)? Any evidence of prompt overfitting or distribution shift from the placeholder?
- Have you validated LLM-judge accuracy against human or deterministic evaluators on a subset (Appendix D)? Any disagreement rates?
- Does advantage propagation to all sequences (Sec. 3.3) introduce bias toward earlier or later actions? Could sequence weighting by recency or length improve credit assignment?
- Can you include a fairness audit comparing tool suites and inference budgets across baselines (Sec. 4.1–4.2), or re-run key baselines under your tool constraints?
- Are the reverse-constructed questions (Sec. 2) released with source URLs and evidence chains? How is uniqueness ensured after obfuscation (Sec. 2; Sec. 2.2)?

Rating
- Overall (10): 8 — Compelling end-to-end framework with strong results (Table 1) and clear method (Sec. 3.1; Fig. 3), though some assumptions (Sec. 3.1) need deeper quantification.
- Novelty (10): 8 — The joint contribution of reverse-constructed, multi-source/obfuscated tasks (Sec. 2; Fig. 1) and dynamic context training-testing consistency (Sec. 3.1–3.3) is fresh for web agents.
- Technical Quality (10): 8 — Sound design and integration (Sec. 3.1–3.3; Table 2–3) with minor gaps in ablations (window hyperparameters; LLM-judge validation).
- Clarity (10): 8 — Well-structured, with effective figures and tables (Fig. 1–5; Table 1–3), small presentation polish issues remain (Sec. 3.1/Fig. 3 labels).
- Confidence (5): 4 — High confidence based on detailed methods and multiple benchmarks (Sec. 3–4; Table 1–3), tempered by evaluation via LLM judge (Sec. 4.1; Appendix D).

Summary
DeepMiner proposes training deep search agents via complex, reverse-constructed QA tasks (Sec. 2) and a dynamic sliding-window context strategy that retains assistant reasoning while compressing old tool outputs (Sec. 3.1; Fig. 2–3). The system uses SFT cold-start (Sec. 3.2) and GRPO RL with trajectory-level advantages propagated to sequence-level training under sliding contexts (Sec. 3.3). Experiments show large gains over open-source baselines on BrowseComp-en/zh, XBench-DeepSearch, and GAIA (Table 1), effective context utilization vs summarization (Table 2), and data efficiency vs HotpotQA (Table 3).

Soundness
The methodological core is coherent: sliding window based on empirical context saturation (Sec. 3.1; Fig. 2), aligned training via sequence decomposition/masking (Sec. 3.1; Fig. 3), and group-relative advantage propagation (Sec. 3.3; Eq. 1). The fetch/find tools (Sec. 4.1; Appendix C) support full-content access without summarization truncation. However, the assumption that tool outputs primarily affect local decisions (Sec. 3.1) is plausible but not rigorously quantified. The binary LLM-judge reward (Sec. 3.3; Appendix D) is straightforward but may cause brittleness and evaluation bias; inter-annotator reliability or calibrated rule checks are missing.

Presentation
The paper is readable with consistent terminology and progression from motivation to solution and results (Sec. 1–4). Visuals effectively illustrate pipeline and context dynamics (Fig. 1–3), and scaling plots (Fig. 5) aid interpretation. Some choices—e.g., preliminary analysis using “gpt-oss-120b” (Fig. 2)—could be contextualized better. Table captions (Table 2–3) are informative, though adding notes on variance or error bars would strengthen transparency.

Contribution
The contribution is two-fold: a data generation approach ensuring multi-source, obfuscated, verifiable tasks (Sec. 2; Fig. 1), and a practical, RL-compatible context strategy that avoids external summarization and supports up to ~100 turns within a 32k window (Sec. 3.1; Fig. 2–3). The empirical gains (Table 1) and efficiency (Table 2) demonstrate significant progress toward scalable, long-horizon search agents, narrowing the gap to proprietary systems on key benchmarks.

Strengths
- Addresses a central limitation (context explosion) with an elegant, trainable sliding window (Sec. 3.1; Fig. 3).
- Reverse construction yields harder tasks requiring cross-document synthesis and obfuscation for genuine reasoning (Sec. 2; Fig. 1).
- Consistent, sizable improvements over strong open-source agents across benchmarks (Table 1), with informative scaling analyses (Fig. 5).
- Tool suite enables full-content browsing and in-page search with pagination (Sec. 4.1; Appendix C).
- Clear training details and hyperparameters (Sec. 4.1), plus reinforcement learning dynamics visualization (Fig. 4).

Weaknesses
- Evaluation and reward rely on closed LLM judge (Sec. 4.1; Appendix D); human verification or deterministic checks are absent.
- No ablation on sliding-window hyperparameters (W, S) or placeholder token variations (Sec. 3.1).
- Limited fairness analysis across baselines (tool differences, turn limits, context length); comparisons compile “official results” but may not match settings (Sec. 4.2–4.3).
- Credit assignment under sequence decomposition (Sec. 3.3) is not stress-tested—e.g., weighting strategies, per-step variance across sequences.
- Data release details (Appendix B) suggest restricted access; reproducibility (code/scripts) is not fully specified.

Questions
- Can you provide ablations for W and S (Sec. 3.1) and report sensitivity to the φ placeholder content?
- How often does the LLM judge disagree with human annotators on a sampled subset (Appendix D), and does this affect reported gains in Table 1?
- Could you present a fairness audit re-running key open-source baselines under your tool suite and turn/context budgets (Sec. 4.1–4.2)?
- Is there measurable evidence that distant tool outputs have negligible long-term influence (Sec. 3.1)—e.g., mutual information or gradient-attribution analyses?
- How are obfuscated questions guaranteed to remain unique and non-ambiguous (Sec. 2), and do you release source URLs and evidence chains?

Rating
- Overall (10): 7 — Strong practical contribution with clear gains (Table 1) and an RL-compatible context solution (Sec. 3.1; Fig. 3), but evaluation and ablation gaps remain.
- Novelty (10): 7 — Combining reverse-constructed, obfuscated multi-source tasks (Sec. 2) with dynamic context RL training (Sec. 3.1–3.3) is novel for web agents.
- Technical Quality (10): 7 — Solid engineering and coherent training strategy (Sec. 3.2–3.3), yet lacks robust ablations and LLM-judge validation.
- Clarity (10): 7 — Well-presented with helpful figures/tables (Fig. 1–5; Table 1–3), minor contextual details could improve transparency (Sec. 4.1).
- Confidence (5): 3 — Moderate confidence based on detailed write-up and multiple benchmarks, tempered by reliance on LLM judges and missing ablations (Sec. 4.1; Sec. 3.1).

Summary
This paper presents DeepMiner, targeting long-horizon web agents with two main innovations: a reverse-construction data pipeline that synthesizes complex, multi-source, obfuscated QA tasks (Sec. 2; Fig. 1) and a dynamic sliding-window context mechanism that drops old tool outputs via a placeholder while retaining assistant reasoning (Sec. 3.1; Fig. 2–3). The approach is trained via SFT cold-start (Sec. 3.2) and GRPO-style RL with trajectory-level rewards and sequence-level training (Sec. 3.3). The model shows strong improvements over open-source agents across BrowseComp-en/zh, XBench-DeepSearch, and GAIA (Table 1), plus better context efficiency than summarization baselines (Table 2) and better SFT data-efficiency than HotpotQA (Table 3).

Soundness
The core premise—that tool responses consume context disproportionately and are mainly locally useful—is supported qualitatively (Sec. 3.1; Fig. 2). The training-testing consistency via sequence decomposition and masking (Sec. 3.1; Fig. 3) is a sound way to prevent distribution shift, and the adapted GRPO advantage propagation (Sec. 3.3; Eq. 1) is consistent with sparse terminal rewards. However, the locality hypothesis is not rigorously quantified; ablations on omitting tool outputs and measuring downstream decision quality are missing. The use of a binary LLM-judge reward (Sec. 3.3; Appendix D) is simple but could be brittle—false positives/negatives might affect optimization stability. Overall, the method is coherent and likely to generalize, with some empirical assumptions needing more scrutiny.

Presentation
The manuscript is well-structured and easy to follow, with clear figures for pipeline and context behavior (Fig. 1–3) and informative tables (Table 1–3). Training dynamics (Fig. 4) and scaling plots (Fig. 5) provide useful insights. Minor presentation issues: some labels (e.g., “Omit Tool” vs φ) could be standardized (Sec. 3.1; Fig. 3), and preliminary model “gpt-oss-120b” (Fig. 2) could be briefly characterized (size, tokenizer) for context.

Contribution
DeepMiner contributes a practical, RL-integrated solution to context explosion in multi-turn agents without external summarization (Sec. 3.1), alongside a challenging training dataset designed to require cross-source synthesis and obfuscation (Sec. 2). The empirical performance improvements (Table 1) and context efficiency (Table 2) demonstrate concrete benefits. The combination of methods—data complexity, context management, RL under dynamic sequences—offers a meaningful advance for open-source deep search agents.

Strengths
- Dynamic context mechanism is simple, effective, and RL-compatible (Sec. 3.1; Fig. 3), enabling ~100 turns at 32k context (Fig. 2).
- Reverse-constructed QA data drives genuine cross-document reasoning and verification (Sec. 2; Fig. 1; Table 3).
- Significant improvements over strong open-source baselines across benchmarks (Table 1).
- Transparent training details and tool definitions (Sec. 4.1; Appendix C), plus scaling analyses (Fig. 5).
- Addresses known limitations of summarization-based context compression (Sec. 1; Table 2).

Weaknesses
- Heavy reliance on LLM-judge evaluation and reward (Sec. 4.1; Sec. 3.3; Appendix D) without human or deterministic validation.
- Missing ablations on window hyperparameters and placeholder design (Sec. 3.1), and no quantitative test of the “locality” of tool outputs.
- Baseline comparability could be affected by different toolsets and budgets (Sec. 4.2); fairness audit is limited.
- Advantage propagation strategy does not explore alternative credit assignments (Sec. 3.3), and there is no step-level learning signal analysis.
- Data availability and reproducibility details (Appendix B) are cautious; it’s unclear when/what will be released to reproduce claims.

Questions
- Can you provide window-size/slide-step ablations (Sec. 3.1) and analyze sensitivity to placeholder content?
- Do you have a human-verified subset to estimate LLM-judge precision/recall (Appendix D), or can you cross-check with rule-based exact matching?
- How do results change if tool responses are summarized instead of omitted with φ (Sec. 3.1), under identical RL training?
- Can you quantify the effect of distant tool outputs via causal or attribution methods, validating the locality assumption (Sec. 3.1)?
- Are source URLs and evidence chains for reverse-constructed questions released (Sec. 2)? How is uniqueness after obfuscation guaranteed?

Rating
- Overall (10): 6 — Solid, coherent approach with clear improvements (Table 1) and practical context strategy (Sec. 3.1), but key ablations and evaluation validation are missing.
- Novelty (10): 6 — The integration of obfuscated multi-source tasks (Sec. 2) with an RL-consistent sliding window (Sec. 3.1–3.3) is a meaningful, though incremental, advance.
- Technical Quality (10): 6 — Methods are sound but could be strengthened by quantitative locality tests, hyperparameter ablations, and judge validation (Sec. 3.1; Sec. 4.1).
- Clarity (10): 7 — Clear writing and visuals (Fig. 1–5; Table 1–3), with minor labeling consistency issues (Sec. 3.1; Fig. 3).
- Confidence (5): 3 — Moderate confidence given comprehensive experiments but reliance on LLM judges and limited ablations tempers certainty (Sec. 4.1; Sec. 3.1).

Summary
The authors propose DeepMiner, a framework for training multi-turn web search agents that tackles (i) insufficient task complexity via reverse-constructed, multi-source, obfuscated, and verifiable QA pairs (Sec. 2; Fig. 1) and (ii) context management limitations via a sliding-window mechanism that compresses earlier tool responses while keeping assistant reasoning intact (Sec. 3.1; Fig. 2–3). The training recipe combines SFT cold-start (Sec. 3.2) and GRPO-style RL with trajectory-level rewards/advantages applied to sequence-level training under dynamic contexts (Sec. 3.3). Experiments show strong gains over open-source agents, notably 33.5% on BrowseComp-en (Table 1), stable accuracy across context lengths with sliding windows (Table 2), and better SFT data-efficiency than HotpotQA (Table 3).

Soundness
The approach is well-motivated by empirical context saturation (Sec. 3.1; Fig. 2) and supported by training-testing consistency through sequence decomposition and per-response masking (Sec. 3.1; Fig. 3; Mi(k)). The GRPO adaptation (Sec. 3.3; Eq. 1) fits long-horizon sparse rewards. That said, several elements would benefit from deeper analysis: quantitative validation of the locality of tool outputs (Sec. 3.1), sensitivity to window parameters, and robustness of the LLM-judge reward. The tools (search/fetch/find) and pagination (Sec. 4.1; Appendix C) are practical and likely reduce information loss vs truncation.

Presentation
The manuscript is clear and thorough, with helpful diagrams and tables (Fig. 1–5; Table 1–3). Case trajectory (Appendix E.1) concretely demonstrates deep search dynamics. Minor clarity improvements could include explicitly labeling the placeholder φ in figures and providing more detail on “gpt-oss-120b” used in Fig. 2. Overall, the narrative is cohesive and readable.

Contribution
DeepMiner advances open-source deep search agents by proposing a scalable, RL-compatible context strategy and a data construction pipeline targeting genuinely hard multi-source reasoning. Empirical results (Table 1) narrow gaps with proprietary systems, and the context efficiency comparison (Table 2) highlights practical gains without summarization dependencies. The contributions are significant for long-horizon agent training and inference.

Strengths
- Elegant sliding-window mechanism with training-testing alignment (Sec. 3.1; Fig. 3), enabling near-100 turns at 32k (Fig. 2; Table 2).
- Reverse-constructed, multi-source, obfuscated QA generation with strong filtering for verifiability (Sec. 2; Fig. 1).
- Substantial performance improvements across multiple benchmarks (Table 1), with informative scaling analyses (Fig. 5).
- Tool suite and pagination reduce information loss relative to truncation/summarization (Sec. 4.1; Appendix C).
- Data-efficiency advantage over HotpotQA in SFT (Table 3).

Weaknesses
- Evaluation and reward rely on LLM judges (Sec. 4.1; Sec. 3.3; Appendix D), lacking human or deterministic cross-checks.
- No ablation on sliding-window hyperparameters or placeholder variants (Sec. 3.1).
- The claim that distant tool outputs minimally influence later decisions (Sec. 3.1) is not rigorously quantified.
- Baseline comparability (tools, budgets, context limits) is not fully audited (Sec. 4.2–4.3).
- Reproducibility details (dataset/code release) are limited; Appendix B suggests restricted availability.

Questions
- Can you add ablations for W/S and the placeholder φ (Sec. 3.1) to assess sensitivity and robustness?
- How accurate is the LLM judge relative to human adjudication on a sample (Appendix D)? Any discrepancies affecting Table 1 rankings?
- Could you provide a fairness audit re-running baselines under your tool suite, context window, and turn limits (Sec. 4.1–4.2)?
- Can you quantify the locality of tool outputs via attribution or influence functions (Sec. 3.1)?
- Will you release the reverse-constructed dataset (Sec. 2) with source URLs, evidence chains, and obfuscation rules for replication?

Rating
- Overall (10): 9 — Strong, well-executed framework with meaningful gains (Table 1) and a practical, RL-aligned context solution (Sec. 3.1; Fig. 3), pending further validation ablations.
- Novelty (10): 9 — Combining hard multi-source/obfuscated data construction (Sec. 2) with dynamic context training-testing consistency (Sec. 3.1–3.3) is a notable, original advance for web agents.
- Technical Quality (10): 8 — Solid engineering and coherent RL integration (Sec. 3.2–3.3; Table 2–3), would be strengthened by window and judge validations.
- Clarity (10): 8 — Clear exposition with effective visuals (Fig. 1–5) and explicit settings (Sec. 4.1), minor labeling/context details could be improved.
- Confidence (5): 4 — High confidence from consistent multi-benchmark gains and detailed methods (Sec. 3–4), tempered by reliance on LLM judges (Sec. 4.1; Appendix D).