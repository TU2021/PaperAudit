Summary
The paper presents DeepMiner, a training framework for long-horizon, multi-turn web search agents that tackles two core challenges: insufficient task complexity and context explosion from verbose tool outputs. DeepMiner introduces a reverse-construction pipeline that generates complex, multi-source, obfuscated, and verifiable QA tasks sourced from real web content, intended to elicit cross-document synthesis and rigorous evidence use. It also proposes a dynamic sliding-window context mechanism that elides older tool outputs via a placeholder while preserving the assistant’s reasoning traces, coupled with sequence decomposition and masking to align training and inference. The system is cold-started with supervised fine-tuning and optimized with a GRPO-style reinforcement learning procedure adapted to sequence-level training under sliding contexts, propagating trajectory-level advantages to sequences under sparse terminal rewards judged by an LLM. A practical tool suite (search/fetch/find with pagination) enables full-content browsing without external summarization. Experiments on BrowseComp-en/zh, XBench-DeepSearch, and GAIA report substantial gains over open-source agents, improved scalability with turn budgets and context length, better context efficiency than summarization baselines, and stronger SFT data efficiency versus HotpotQA. The paper is clearly written with effective figures and case studies illustrating long-horizon behavior.

Strengths
- Coherent, well-motivated solution to two bottlenecks: task complexity via reverse-constructed, multi-source, obfuscated, verifiable QA data, and context management via a simple, RL-compatible sliding-window that preserves assistant reasoning and ensures training-testing consistency through sequence decomposition/masking.
- Strong empirical performance across multiple benchmarks, with informative scaling analyses showing robustness as turn budgets and context length increase, and clear advantages in context efficiency compared to summarization-based baselines.
- The tool suite (search/fetch/find and pagination) supports fine-grained browsing and full-content access, reducing reliance on external summarization and truncation, and aligning well with the proposed context strategy.
- Practical training pipeline (SFT cold-start followed by adapted GRPO) and clear methodological exposition (figures illustrating pipeline, context dynamics, and sliding sequences), plus case studies that concretely demonstrate deep search behaviors.
- Data-efficiency analysis indicates that the proposed task construction yields stronger supervision than existing datasets (e.g., HotpotQA), tailored to long-horizon web search.

Weaknesses
- Heavy reliance on a closed LLM judge with binary correctness for both reward shaping and evaluation, without human or deterministic validation, inter-annotator reliability checks, or calibration studies. This raises concerns about bias, brittleness, and the robustness of reported gains.
- The central locality assumption—that distant tool outputs have limited influence on subsequent decisions—remains qualitative. Missing quantitative ablations on sliding-window hyperparameters (window size, slide step), sensitivity to the placeholder design, direct comparisons between omission versus summarization of tool outputs, and causal/attribution analyses to measure long-range dependencies.
- Credit assignment under sequence decomposition propagates trajectory-level advantages to all sequences, which may misattribute credit to earlier steps. The paper lacks alternative weighting strategies, per-step variance analyses, or exploration of step-level rewards to assess learning stability and fairness of credit distribution.
- Baseline comparability and fairness are insufficiently audited. Differences in tool suites, inference budgets, turn limits, and context windows may confound comparisons; key baselines are not re-run under matched settings, limiting the strength of head-to-head claims.
- Reproducibility and release details are incomplete. Dataset availability appears restricted, and it is unclear whether source URLs, evidence chains, and obfuscation rules will be released to ensure uniqueness and verifiability. Implementation details for the sliding-window masking (scripts, templates), licensing, and practical availability are not fully specified. Minor presentation issues (e.g., placeholder labeling consistency, limited context on preliminary models used in analyses) also detract from transparency.
