# Global Summary
This paper introduces DeepMiner, a framework for training deep search agents capable of long-horizon, multi-turn reasoning. The authors identify two primary challenges: insufficient task complexity in existing training data and severe context management limitations in long interactions. To address these, DeepMiner proposes a two-pronged approach. First, it uses a "reverse construction" method to generate complex, verifiable question-answer pairs from authentic web sources, ensuring tasks require deep reasoning. Second, it implements a dynamic sliding window for context management, which selectively compresses distant tool responses while preserving all of the agent's reasoning steps. This strategy avoids external summarization models and enables interactions of nearly 100 turns within a standard 32k context length. The framework is applied to the Qwen3-32B model via supervised fine-tuning and reinforcement learning (using GRPO). The resulting model, DeepMiner-32B, achieves state-of-the-art performance on multiple search agent benchmarks, notably scoring 33.5% accuracy on BrowseComp-en, an improvement of almost 20 percentage points over the previous best open-source agent.

# Abstract
The paper proposes DeepMiner, a framework to elicit deep reasoning in multi-turn agents for long-horizon tasks. It introduces high-difficulty training tasks generated via a reverse-construction method from authentic web sources, creating complex but verifiable question-answer pairs. A dynamic context window strategy, based on a sliding-window mechanism, is designed for both training and inference to handle long-horizon contexts without relying on external summarization models. By training Qwen3-32B with reinforcement learning, the authors develop DeepMiner-32B. This model achieves 33.5% accuracy on BrowseComp-en, surpassing the previous best open-source agent by nearly 20 percentage points. It also shows consistent improvements on BrowseComp-zh, XBench-DeepSearch, and GAIA. The dynamic context management enables interactions of nearly 100 turns within a standard 32k context length.

# Introduction
The paper addresses the challenge of extending cognitive behaviors like self-verification and backtracking, seen in single-turn reasoning models, to multi-turn, long-horizon search agents. Existing open-source approaches lag behind proprietary systems due to two fundamental problems:
- **Insufficient Task Complexity**: Datasets like TriviaQA and HotpotQA rely on structured Wikipedia data and can often be solved with shallow information retrieval, failing to elicit sophisticated cognitive behaviors.
- **Context Management Limitations**: Long interactions quickly exhaust context windows. A typical 32k context supports only 10–15 turns. Current solutions using summarization lose fine-grained information, add complexity, and cannot be integrated into end-to-end reinforcement learning.

To solve this, the paper presents DeepMiner, a framework that combines:
1.  A reverse construction method to generate challenging QA pairs from authentic web sources that demand genuine reasoning.
2.  A dynamic sliding window strategy for context management that compresses distant tool responses while preserving all assistant reasoning traces, avoiding information loss from summarization.

The resulting model, DeepMiner-32B (based on Qwen3-32B), achieves 33.5% accuracy on BrowseComp-en, a nearly 20 percentage point improvement over the previous best open-source agent. The context management strategy enables up to 100 tool calls within a 32k context length.

# Method
The method consists of two main components: complex question construction and reinforcement learning with a dynamic context window.

### COMPLEX QUESTION CONSTRUCTION
A three-stage pipeline is used to generate challenging QA pairs:
- **Entity-driven Information Collection**: Entities with moderate visibility (1,000–10,000 Wikipedia page views in 6 months) are selected. Information is gathered via Google Search, followed by a three-stage filtering process to ensure entity correspondence, information complementarity, and source credibility.
- **Question Generation**: An LLM generates questions that require synthesizing information from at least four distinct, non-Wikipedia sources. These questions then undergo an "obfuscation" process, where specific details are replaced with more generic descriptions to increase reasoning demands.
- **Multi-stage Filtering**: Generated QA pairs are filtered for difficulty (unsolvable by direct search or zero-shot LLM prompting) and quality (no ambiguity, verifiable answer, and logically derivable from source documents).

### REINFORCEMENT LEARNING WITH DYNAMIC CONTEXT WINDOW
- **Dynamic Context Management Strategy**: An empirical analysis shows that models often fail when reaching context limits and that tool responses consume 5-10 times more context than assistant thoughts. The proposed solution is a sliding window mechanism. When the number of tool responses reaches a window size `W`, the oldest `S` (slide step) tool responses are replaced with a placeholder token `φ`, while all assistant reasoning traces are preserved. This allows for up to 100 turns within a 32k context.
- **Training-Testing Consistency**: To ensure consistency, each full trajectory is decomposed into multiple training sequences, each reflecting a different state of the sliding window as it would appear during inference. A masking strategy ensures each assistant response is trained only once.
- **Cold Start**: Supervised fine-tuning (SFT) is used as a cold-start phase on high-quality trajectories generated by powerful LLMs, also using the sliding window mechanism.
- **Reinforcement Learning Training**: The paper uses Group Relative Policy Optimization (GRPO). For each question, `G` complete rollouts are generated. A trajectory-level advantage score is computed and then propagated to all training sequences derived from that trajectory.
- **Reward Design**: A simple binary reward (1 for correct, 0 for incorrect) is assigned by an LLM judge based on whether the final answer matches the ground truth.

# Experiments
### EXPERIMENTAL SETUPS
- **Tool Configuration**: The agent uses three tools: `web_search` (returns top-10 results), `fetch` (retrieves webpage content with pagination), and `find` (in-page search).
- **Training Details**: The base model is Qwen3-32B. SFT is performed on ~3,000 trajectories (batch size 256, lr 1e-5). RL training uses ~4,000 questions (batch size 32, lr 2e-6), with 8 rollouts per question. The max trajectory length is 40k tokens with a 60-turn limit. The tool window size is 5 with a slide step of 3.
- **Evaluation**: Benchmarks include BrowseComp, BrowseComp-zh, XBench-DeepSearch, and GAIA. Evaluation uses a temperature of 0.6, top-p of 0.9, and a maximum of 100 interaction turns. The judge model is ChatGPT-4o-Latest.
- **Baselines**: Compared against commercial agents (OpenAI DeepResearch), general LLMs (OpenAI-o3, DeepSeek-V3.1-671B), and open-source agents (WebSailor-72B, WebExplorer-8B, DeepDive-32B).

### MAIN RESULTS
- As shown in Table 1, DeepMiner-32B-RL achieves 33.5% on BrowseComp-en, 40.1% on BrowseComp-zh, 62.0% on XBench-DS, and 58.7% on GAIA.
- The 33.5% on BrowseComp-en significantly outperforms the previous best open-source agent (WebExplorer-8B at 15.6%) and larger models like DeepSeek-V3.1-671B (30.0%).
- The SFT model (DeepMiner-32B-SFT) alone achieves 21.2% on BrowseComp-en, outperforming most prior open-source agents.
- RL provides substantial gains over SFT: +12.3 points on BrowseComp-en, +12.1 on BrowseComp-zh, +9.0 on XBench-DS, and +4.3 on GAIA.

### Context Management Efficiency Analysis
- A comparison on GPT-OSS-120B shows that DeepMiner's sliding window strategy achieves 33.3% accuracy on BrowseComp within a 32k context.
- In contrast, a vanilla approach reaches 30.3% only with a 128k context, and an external summarization approach reaches 31.6% with a 128k context.
- DeepMiner's performance is stable at 33.3% across 32k, 64k, and 128k context lengths.

### DETAILED ANALYSIS
- **Training Dynamics**: During RL training, the average trajectory length and reward steadily increase, with reward growing from 0.45 to 0.60. This corresponds to an in-domain performance improvement on BrowseComp from 22% to 33.5%.
- **Context Scaling**: DeepMiner's performance on BrowseComp scales with the tool-call budget, surpassing DeepSeek-V3.1-671B at around 60 calls and reaching 33.5% at 100 calls. With a 32k context length, it achieves nearly 33.0% accuracy.
- **Data Efficiency**: An SFT model trained on the DeepMiner dataset achieves 21.2% on BrowseComp, while a model trained on HotpotQA achieves only 15.6%, demonstrating the effectiveness of the proposed data construction method.

# Related Work
- **Reinforcement Learning with Verifiable Rewards (RLVR)**: The paper acknowledges RLVR as a standard method but notes its challenges in web agent scenarios due to sparse rewards and context limitations. DeepMiner's contribution is not a new RL algorithm but a dynamic context window that extends the applicability of existing RLVR methods like GRPO to long-horizon tasks.
- **Deep Research Agents**: The paper situates itself among efforts to build deep research agents, noting a large performance gap between proprietary systems (like OpenAI's DeepResearch) and open-source models. It claims that prior state-of-the-art open-source models achieve less than 20% accuracy on BrowseComp, and DeepMiner significantly narrows this gap.

# Conclusion
The paper presents DeepMiner, a framework for building deep research agents capable of handling long-horizon interactions. It addresses two key limitations: insufficient task complexity and context length constraints. The reverse construction of complex QA pairs provides a challenging training signal, while the dynamic context management strategy enables interactions exceeding 100 turns within a 40k context without external summarizers. The resulting DeepMiner-32B model achieves 33.5% on BrowseComp, nearly doubling the performance of previous state-of-the-art open-source agents and marking a shift towards unbounded, deep exploration.

# References
This section contains a list of references cited in the paper.

# Appendix
- **A THE USE OF LARGE LANGUAGE MODELS**: States that LLMs were used to aid and polish writing.
- **B ETHICAL CONSIDERATIONS**: Data was collected from public sources like Wikipedia, filtering out private content. The dataset will be released under strict access controls for academic use, and data will be anonymized. Model weights will be released after a review process to ensure legitimate use.
- **C ENHANCED TOOL SUITE**: Describes the three tools used: `Web Search`, `Fetch` (with pagination to avoid information loss from truncation), and `Find` (for in-page keyword search).
- **D TEMPLATES**: Provides the prompt template used for the LLM judge to evaluate final answers.
- **E.1 CASE STUDY**: Includes examples of the complex, obfuscated questions generated by the method. It also presents a detailed case study of a DeepMiner agent solving a complex question on BrowseComp, requiring 61 steps to identify the "Ahsan Manzil" as the correct answer, demonstrating its long-horizon reasoning capability.