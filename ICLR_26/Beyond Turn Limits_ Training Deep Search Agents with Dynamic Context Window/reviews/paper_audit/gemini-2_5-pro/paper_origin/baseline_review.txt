1) Summary
This paper introduces DeepMiner, a framework for training deep search agents capable of long-horizon reasoning. The authors identify two key challenges: insufficient task complexity in existing datasets and context length limitations in multi-turn interactions. To address these, DeepMiner proposes a "reverse construction" method to generate complex, verifiable question-answer pairs from web sources, ensuring training tasks require deep reasoning. The core technical contribution is a dynamic context management strategy using a sliding window on tool responses. This approach preserves the agent's reasoning history (assistant turns) while compressing older, less relevant tool outputs, enabling interactions of nearly 100 turns within a standard 32k context. The resulting model, DeepMiner-32B, trained with reinforcement learning on Qwen3-32B, achieves state-of-the-art performance among open-source agents on benchmarks like BrowseComp, XBench-DeepSearch, and GAIA.2) Strengths
*   **Novel and Pragmatic Context Management Strategy**
    *   The dynamic sliding window for tool responses is an elegant and effective solution to the critical problem of context explosion in long-horizon agents (Section 3.1). This approach directly addresses a major bottleneck for current systems.
    *   It offers clear advantages over summarization-based methods by preserving the full detail of recent tool outputs and avoiding information loss (Table 2). This is crucial for tasks requiring precise, fine-grained information.
    *   The strategy allows for end-to-end optimization within a reinforcement learning framework, unlike methods that rely on external, non-trainable summarization models (Table 2, Section 3.1). This ensures the entire reasoning process can be improved via RL.
    *   The empirical results demonstrate remarkable efficiency, enabling the model to sustain nearly 100 interaction turns within a 32k context and achieving performance that would otherwise require a 128k context (Section 4.3, Table 2, Figure 5 right).*   **Thorough and Convincing Experimental Evaluation**
    *   The paper evaluates DeepMiner on a diverse set of four challenging, deep research benchmarks (BrowseComp-en, BrowseComp-zh, XBench-DS, GAIA), demonstrating the generalizability of the approach (Section 4.1, Table 1).
    *   The main results show substantial performance gains, with DeepMiner-32B outperforming the previous best open-source agent on BrowseComp-en by nearly 20 percentage points (33.5% vs. 15.6%) and even surpassing a much larger model, DeepSeek-V3.1-671B (Table 1).
    *   The work includes a strong set of detailed analyses and ablations that validate key design choices. This includes a direct comparison of context management strategies (Table 2), an analysis of training dynamics (Figure 4), scaling properties with respect to tool calls and context length (Figure 5), and a data efficiency comparison against HotpotQA (Table 3).*   **High-Quality and Challenging Data Generation Pipeline**
    *   The paper proposes a well-motivated "reverse construction" method to create training data that genuinely requires deep, multi-step reasoning, a limitation of existing datasets (Section 2).
    *   The pipeline incorporates several thoughtful steps to ensure data quality and difficulty, such as selecting entities with moderate visibility, requiring synthesis from multiple sources, obfuscating questions, and applying rigorous multi-stage filtering (Figure 1, Section 2).
    *   The effectiveness of the generated data is empirically validated, as the SFT model trained on it already outperforms most prior open-source agents, and significantly surpasses a model trained on the widely-used HotpotQA dataset (Table 1, Table 3).*   **Clear Presentation and Problem Formulation**
    *   The introduction clearly articulates the two primary challenges for long-horizon agents—insufficient task complexity and context management limitations—providing a strong motivation for the proposed contributions (Section 1).
    *   The methodology is explained logically and is well-supported by clear diagrams that illustrate the data construction pipeline (Figure 1) and the sliding window mechanism (Figure 3).
    *   The case study provided in the appendix is extensive and gives a qualitative sense of the complex, multi-step reasoning the trained agent is capable of performing (Appendix E.1, Steps 1-61).3) Weaknesses
*   **Lack of Detail Regarding Data Generation and Filtering**
    *   The paper does not specify which Large Language Models (LLMs) were used for the question generation and obfuscation steps in the data pipeline (Section 2, Figure 1). This information is critical for reproducibility.
    *   Key statistics about the generated dataset are missing. The paper mentions training on approximately 3,000 trajectories for SFT and 4,000 questions for RL (Section 4.1), but does not state the total number of QA pairs generated before filtering or the rejection rates at each filtering stage (Section 2). This makes it difficult to assess the cost and efficiency of the data generation process.
    *   The filtering criteria are described qualitatively (e.g., "no ambiguous question," "verifiable answer") (Section 2). Providing quantitative metrics or more concrete examples of what constitutes an "ambiguous" or "unsolvable" question would strengthen this section.*   **Insufficient Clarity on the Training Procedure for Dynamic Contexts**
    *   The description of how a single trajectory is decomposed into multiple training sequences is dense and could be difficult to follow (Section 3.1). The masking formula `Mi(k)` is provided, but its interaction with the loss computation is only implied.
    *   The mermaid diagram in the text (Block #15) is not rendered properly and is confusing. While the image version of Figure 3 is much clearer, a small, step-by-step textual example walking through how a short trajectory is split into sequences with corresponding masks would significantly improve clarity.
    *   It is stated that "each assistant response is trained exactly once across all sequences" (Section 3.1), but the mechanism for propagating the trajectory-level advantage `Âi` to each sequence in the RL phase could be elaborated on further to ensure the training process is fully understood (Section 3.3).*   **Limited Discussion of the Sliding Window's Failure Modes**
    *   The sliding window mechanism assumes that distant tool responses are less relevant. However, there may be cases where information from a very early tool call becomes critical for solving the task much later. The paper does not discuss or analyze this potential failure mode.
    *   The mechanism includes a placeholder token `[Previous tool output skipped. Rerun tool if needed]` (Section 3.1). There is no analysis, either quantitative or qualitative, to show whether the model learns to effectively use this prompt to recover critical lost information by re-issuing tool calls.
    *   The case study in Appendix E.1 is very long, but it does not explicitly show the sliding window in action or how the agent copes with omitted context, which would be a valuable illustration of the method's robustness.*   **Potential for Inconsistency in Baseline Comparisons**
    *   The evaluation relies on ChatGPT-4o as an automated judge (Section 4.1). While this is a common practice, it is known to have potential biases and variability. The paper does not discuss this limitation or report any measures to ensure evaluation consistency (e.g., using specific judge prompts, checking inter-rater reliability).
    *   For baseline models, the paper states, "We adopt the official results reported in their papers" (Section 4.1). This can introduce confounding variables, as the evaluation setups (e.g., judge model version, exact prompts, tool implementations) may differ across papers, potentially affecting the fairness of the comparison.4) Suggestions for Improvement
*   **Enhance Reproducibility of the Data Generation Pipeline**
    *   Please specify the exact LLM(s) and their versions used for question generation, obfuscation, and filtering in Section 2.
    *   In Section 4.1 or an appendix, please provide statistics on the data generation process, including the initial number of QA pairs generated and the number of samples discarded at each of the filtering stages described in Section 2.
    *   Consider adding a small table in the appendix with examples of questions that were filtered out at each stage and the reason for their exclusion to make the filtering criteria more concrete.*   **Clarify the Multi-Sequence Training Mechanism**
    *   In Section 3.1, please add a concise, step-by-step example of a short trajectory (e.g., 4-5 turns) being decomposed into multiple training sequences. For each sequence, show the exact text the model sees (with placeholders) and which tokens have their loss masked.
    *   Please either fix the rendering of the mermaid diagram (Block #15) or remove it, as the image version of Figure 3 is sufficient and the text version is confusing.
    *   In Section 3.3, briefly elaborate on how the single advantage score `Âi` for a full trajectory is used to update the policy across the multiple training sequences derived from it. For instance, is the loss for each sequence simply weighted by the same advantage score?*   **Include a Discussion on the Limitations of the Sliding Window**
    *   Please add a paragraph in the analysis or conclusion discussing the potential failure modes of the sliding window approach, particularly for tasks that require synthesizing information over very long dependencies that might span beyond the window size.
    *   It would be highly beneficial to include a qualitative analysis of a few cases where the model either successfully recovers information from an omitted tool call (prompted by the placeholder) or fails because it cannot. This would provide a more balanced view of the method's capabilities.
    *   Consider annotating one or two steps in the Appendix E.1 case study to explicitly show where the context window would have slid and which tool outputs would have been omitted, demonstrating how the agent proceeds with the compressed context.*   **Acknowledge and Mitigate Potential Evaluation Inconsistencies**
    *   Please add a brief statement in Section 4.1 acknowledging the potential limitations and biases of using an LLM as a judge for evaluation.
    *   To improve transparency, please include the full prompt used for the ChatGPT-4o judge in the appendix, as is done for the judgment template (Appendix D), but specifically for the benchmark evaluations. If the prompts are benchmark-specific, please provide them. This would help standardize future comparisons.5) Score
- Overall (10): 8 — The paper presents a simple yet highly effective solution to a critical problem in agentic AI, supported by very strong empirical results and thorough analysis.
- Novelty (10): 8 — The dynamic context window is a clever and pragmatic adaptation of existing ideas to the agent domain, and the data generation pipeline is a solid contribution.
- Technical Quality (10): 8 — The methodology is sound and the experiments are rigorous, though the lack of detail in data generation and a full discussion of limitations slightly tempers the score.
- Clarity (10): 8 — The paper is well-written and generally easy to follow, but specific parts of the training methodology could be explained more clearly.
- Confidence (5): 5 — I am highly confident in my assessment, as the topic aligns directly with my area of expertise.