Summary
- The paper introduces DeepMiner, a framework for training long-horizon, multi-turn web search agents by (i) reverse-constructing complex, verifiable QA tasks from authentic multi-source web content with obfuscation and multi-stage filtering (Section 2; Figure 1) and (ii) applying a dynamic sliding-window context management strategy that compresses distant tool responses while preserving assistant reasoning traces (Section 3.1; Figures 2–3). Implemented on Qwen3-32B with supervised fine-tuning and reinforcement learning via GRPO adapted to sequence-level training under dynamic contexts (Section 3.3), DeepMiner-32B reports improvements on BrowseComp-en (33.5%), BrowseComp-zh, XBench-DeepSearch, and GAIA (Table 1; Section 4.2). Analyses include context-efficiency comparisons (Table 2; Section 4.3), scaling with tool-call budgets and context length (Figure 5; Section 4.4), training dynamics (Figure 4; Section 4.4), and SFT data-efficiency vs. HotpotQA (Table 3; Section 4.4).Strengths
- Bold, well-motivated dynamic context management
  - The sliding-window mechanism replaces early tool responses with an omission token while preserving assistant outputs (Section 3.1; Figure 3; placeholder “[Previous tool output skipped. Rerun tool if needed].”), addressing exponential context growth of tool outputs observed in preliminary experiments (Figure 2 right; Section 3.1). This matters because it targets a central bottleneck in long-horizon agents without external summarizers, improving technical soundness and practical impact.
  - Training-testing consistency is handled by decomposing a trajectory into K sequences and masking assistant responses so each is trained exactly once (Section 3.1; masking Mi(k) definition). This matters for algorithmic rigor and reduces distribution mismatch between training and inference.
  - Empirical evidence reports the approach can sustain nearly 100 turns within a 32k context length (Figure 2 right; Table 2; Introduction; Abstract), highlighting impact on long-horizon capability under realistic constraints.
- Reverse construction of complex, verifiable QA tasks
  - The pipeline uses Wikipedia for entity selection (moderate visibility: 1,000–10,000 page views) and authentic multi-source web pages via Google Search, followed by three-stage filtering (entity correspondence, information complementarity, credibility; Section 2; Figure 1). This matters for data reliability and realism, addressing limitations of synthetic-only datasets.
  - Questions explicitly require synthesis from at least four distinct sources and then undergo obfuscation to generalize specific details into categorical descriptions (Section 2: Question Generation). This increases genuine reasoning demand and novelty.
  - Multi-stage difficulty and quality filtering removes questions solvable via direct search or zero-shot LLM prompting and enforces unambiguous, derivable answers (Section 2: Multi-stage Filtering). This matters for training signal fidelity and verifiable rewards.
- Strong empirical performance across diverse benchmarks
  - DeepMiner-32B-RL improves over open-source agents on BrowseComp-en (33.5%) and BrowseComp-zh (40.1%), with gains on XBench-DeepSearch (62.0%) and GAIA (58.7%) (Table 1; Section 4.2). This suggests generalizability beyond a single benchmark.
  - The SFT-only model (DeepMiner-32B-SFT) already outperforms many open-source baselines (Table 1; Section 4.2), indicating that training trajectories and data design are effective. This matters for data-centric training efficacy.
  - Tool-call budget and context-length scaling analyses show monotonic performance gains with more turns and robust performance at 32k–40k context (Figure 5; Section 4.4). This is valuable for practitioners optimizing deployment constraints.
- Context efficiency relative to summarization-based approaches
  - The method outperforms both vanilla (no management) and external summarization approaches under tight 32k constraints, achieving 33.3% on BrowseComp with 32k context where baselines require 128k to reach similar numbers (Table 2; Section 4.3). This underscores practical efficiency and end-to-end RL compatibility (no external summarizer).
  - The strategy maintains access to original webpage content (via fetch pagination) rather than compressed summaries (Appendix C; Section 3.1), reducing information loss—important for technical soundness in complex reasoning tasks.
  - The sliding window enables assistant content growth while keeping tool responses bounded (Figure 2 right), aligning with the empirical finding that tool outputs dominate context usage (Section 3.1). This demonstrates measured design decisions tied to observed bottlenecks.
- RL integration adapted to dynamic contexts
  - GRPO is used with trajectory-level rewards and group-relative advantages, propagated consistently across sequence-level training instances derived from the same trajectory (Section 3.3; advantage equation; Figures 3 and 18-style schematic in Section 3.3 text/figure). This shows algorithmic alignment to the proposed context mechanism.
  - Binary reward via an LLM judge reduces reward engineering complexity (Section 3.3: Reward Design), which is practical for long-horizon tasks with verifiable answers.
- Clarity and reproducible details
  - The paper provides tool definitions and agent configuration (web_search, fetch with pagination, find; Section 4.1: Tool Configuration; Appendix C), training hyperparameters (Section 4.1: Training Details), and evaluation settings (temperature, top-p, turn limits; Section 4.1: Evaluation), aiding reproducibility.
  - Illustrative figures (Figures 1, 2, 3, 4, 5) convey pipeline, context dynamics, and training behavior, enhancing clarity.Weaknesses
- Limited ablations and component-level analyses
  - No ablation isolating the contribution of obfuscation versus multi-source synthesis versus filtering (Section 2; Figure 1). This matters for novelty attribution and technical soundness, as gains could be driven by any single component.
  - No sensitivity study on window size W and slide step S, nor alternatives to assistant-preserving vs. tool-compressing policies (Section 3.1; Figure 3). This limits understanding of robustness and design choices.
  - The masking scheme Mi(k) is presented but lacks comparative analysis against alternative credit assignment strategies (Section 3.1). This affects confidence in the specific training protocol.
  - GRPO advantage propagation to sequences is described, but no ablation against standard trajectory-level backprop without sequence decomposition is shown (Section 3.3). This reduces evidence for the necessity of the modification.
- Evaluation comparability and reliance on external judging
  - Main results adopt “official results” of baselines rather than re-running under identical tools, prompts, and judges (Section 4.1: Baselines). This matters for experimental rigor; cross-paper differences can confound comparisons.
  - The evaluation uses ChatGPT-4o-Latest as the judge (Section 4.1: Evaluation; Appendix D), while training rewards also rely on an LLM judge but its identity is unspecified (Section 3.3: Reward Design). This undermines clarity and can bias outcomes.
  - No statistical significance tests or variance measures accompany Table 1/2/3 or scaling curves (Sections 4.2–4.4; Figures 4–5). This impacts confidence in reported improvements.
  - The statement “All results are evaluated using ChatGPT-4o-Latest as the judge model” (Section 4.1: Evaluation) conflicts with “We adopt the official results reported in their papers” for baselines (Section 4.1: Baselines), weakening fairness and protocol consistency for Table 1.
- Dataset transparency and quantitative characterization
  - The paper does not report dataset statistics beyond counts used for SFT (~3,000 trajectories) and RL (~4,000 questions) (Section 4.1: Training Details). Missing: average sources per question (beyond “≥4”), question length, turn counts, and topic breakdown (Section 2; Appendix E).
  - No release timeline or documentation ensuring reproducibility of reverse construction (Appendix B mentions access controls and anonymization but no concrete release plan). This limits verifiability.
  - The “moderate visibility” criterion references Wikipedia views, but quantitative distributions of entities and domains are absent (Section 2: Entity-driven Information Collection). This affects generalizability claims.
  - The filtering step’s pass rates and inter-annotator agreement or automated checks are not reported (Section 2: Multi-stage Filtering). No direct evidence found in the manuscript.
  - The illustrative pipeline example uses a fictional entity “Mr. Capybara …” without an explicit disclaimer that it is not part of the actual dataset (Section 2; Figure 1), which may create confusion about data realism.
- Insufficient evidence supporting “distant tool responses have minimal long-term impact”
  - The paper claims long-term retention of tool responses may be unnecessary (Section 3.1), but provides no quantitative causal analysis, user studies, or controlled experiments. No direct evidence found in the manuscript.
  - No measurement of how often older tool outputs need revisiting and whether omission increases re-fetching overhead (Section 3.1; Appendix C). No direct evidence found in the manuscript.
  - No comparison of sliding-window vs. learned summarization trained end-to-end within RL (Section 4.3 compares external summarization only). This matters for completeness of design space exploration.
  - No error analysis showing if omissions harm specific task types (e.g., synthesis-heavy vs. retrieval-heavy) (Sections 4.2–4.4). No direct evidence found in the manuscript.
- Efficiency and cost reporting gaps
  - Table 2 focuses on accuracy, not memory footprint, latency, or tool-call costs; no throughput/compute metrics are reported (Section 4.3). This is important for real-world deployment.
  - Training details omit compute budget (GPUs, hours), making reproducibility and resource assessment difficult (Section 4.1: Training Details).
  - Sliding-window impact on wall-clock time, given potential re-fetching due to omissions, is unquantified (Section 3.1; Appendix C). No direct evidence found in the manuscript.
  - The trajectory limits (40k tokens, turns=60 during training; Section 4.1) and claims of “nearly 100 turns within standard 32k” (Abstract; Section 3.1; Figure 2 right) vs. “exceed 100 turns within 40k contexts” (Conclusion) are not reconciled with runtime costs. This reduces practical clarity.
- Reward design transparency and robustness
  - The training LLM judge model is unspecified (Section 3.3: Reward Design), leaving ambiguity about consistency with evaluation judging (Section 4.1; Appendix D). This matters for reproducibility.
  - Binary 1/0 reward may be overly sparse for complex multi-step research; no intermediate verification rewards or partial credit mechanisms are explored (Section 3.3). This affects optimization granularity.
  - No experiments on reward hacking or adversarial trajectories (Section 3.3). This impacts robustness.
  - The judge prompt (Appendix D) is provided for evaluation, but alignment of training and evaluation judges/prompts is unclear (Sections 3.3, 4.1). This reduces transparency.
- Internal inconsistencies in context accounting and training-sequence definitions
  - Figure 2 (right) reports “Trajectory Context w/ Sliding Window ≈ 14k” at 64 and 128 turns while “Assistant Context ≈ 16k/20k” at those turns (Section 3.1; Figure 2). If the “Trajectory Context w/ Sliding Window” denotes total context, Assistant Context should not exceed it; this undermines the quantitative support for the context mechanism.
  - The sequence count is defined as K = floor((T − W)/S) + 1 (Section 3.1: Training-Testing Consistency). For short trajectories (e.g., T < W), this can yield K ≤ 0, contradicting the statement that “The first sequence τ(1) contains complete initial context.”
  - The masking rule Mi(k) = 0 if i < W + (k − 2)·S + 2; 1 otherwise (Section 3.1) appears inconsistent with the claim that “The first sequence τ(1) contains complete initial context with all assistant responses trained,” since for k = 1 some assistant responses are masked by the rule.
  - Notation is underspecified: the set Rt and the index t used in the boundary b = max(1, t − W + S) (Section 3.1; Figure 3) are not unambiguously defined (tool-call index vs. time), which may hinder faithful reproduction.Suggestions for Improvement
- Strengthen ablations and component analyses
  - Compare models trained without obfuscation and/or without the ≥4-source constraint, and report deltas (Section 2; Figure 1) to attribute gains.
  - Conduct sensitivity studies over W and S and test alternative policies (e.g., compress assistant and retain tools) (Section 3.1; Figure 3).
  - Evaluate alternative masking/credit schemes against Mi(k) (Section 3.1) to validate the chosen training protocol.
  - Ablate GRPO’s sequence-level advantage propagation versus standard trajectory-level methods (Section 3.3).
- Improve evaluation comparability and statistical rigor
  - Re-run key open-source baselines in the same environment, tools, prompts, and judge to ensure apples-to-apples comparison (Section 4.1: Baselines).
  - Specify the training LLM judge and ensure consistency with the evaluation judge; report sensitivity to different judges (Section 3.3; Section 4.1; Appendix D).
  - Add confidence intervals or statistical tests for Tables 1–3 and Figures 4–5 (Sections 4.2–4.4).
  - Clearly denote which rows in Table 1 are externally reported and, where possible, re-evaluate them with the same judge/protocol used for DeepMiner (Section 4.1: Evaluation; Section 4.1: Baselines).
- Expand dataset reporting and release practices
  - Provide quantitative statistics: distribution of entity types, average sources per question, question length, average required turns, topic diversity (Section 2; Appendix E; Section 4.1).
  - Publish a clear release plan with documentation and scripts for reverse construction, subject to the ethical constraints (Appendix B), to improve reproducibility.
  - Report distributions of “moderate visibility” entities (Section 2: Entity-driven Information Collection) and discuss how this choice impacts generalization.
  - Quantify filtering pass rates and detail verification procedures; include inter-annotator agreement when applicable (Section 2: Multi-stage Filtering).
  - Add an explicit disclaimer that the “Mr. Capybara” example in Figure 1 is illustrative only and not part of the actual dataset (Section 2; Figure 1).
- Provide empirical evidence on the impact of omitting distant tool responses
  - Run controlled studies quantifying when older tool outputs are needed; measure how omissions affect success rates across task types (Section 3.1).
  - Track re-fetch frequency and latency introduced by the omission token policy, reporting costs and performance deltas (Section 3.1; Appendix C).
  - Compare sliding-window against end-to-end trained summarization within RL (not external-only) to test if learned compression can match or exceed performance (Section 4.3).
  - Add error analysis categorizing failure modes attributable to omissions vs. other factors (Sections 4.2–4.4).
- Report efficiency and compute costs more fully
  - Provide memory and latency measurements for training and inference under different context management strategies (Section 4.3).
  - Include compute budget details (hardware, tokens processed, hours) for SFT and RL phases (Section 4.1).
  - Quantify wall-clock tradeoffs of the sliding window (e.g., additional fetch/find calls due to omissions) (Section 3.1; Appendix C).
  - Clarify how “nearly 100 turns” at 32k (Figure 2; Abstract; Section 3.1) relates to training constraints (40k, 60 turns; Section 4.1) and the “exceed 100 turns within 40k contexts” claim (Conclusion), including runtime costs.
- Clarify reward design and test robustness
  - Specify the training LLM judge model and prompt, and evaluate training sensitivity to judge choice (Section 3.3; Appendix D).
  - Explore richer reward shaping (e.g., intermediate verifications and partial credit) to mitigate sparsity (Section 3.3).
  - Test for reward hacking/adversarial behaviors and report defense mechanisms (Section 3.3).
  - Align training and evaluation judges/prompts or document differences and their measured impacts (Sections 3.3, 4.1; Appendix D).
- Clarify and correct context accounting and training-sequence/mask definitions
  - Disambiguate Figure 2 (right) by precisely defining each column (e.g., total trajectory context vs. tool-only vs. assistant-only) and ensure reported values are consistent (Section 3.1; Figure 2).
  - Revise the definition of K to guarantee K ≥ 1 (e.g., K = max(1, floor((T − W)/S) + 1)) and document behavior for short trajectories (Section 3.1).
  - Align the masking rule Mi(k) with the narrative that τ(1) trains all intended assistant responses, and clearly define the indexing of i (Section 3.1).
  - Unambiguously define Rt and the index used in b = max(1, t − W + S), avoiding overload and specifying whether it counts tool calls or steps (Section 3.1; Figure 3).Score
- Overall (10): 7 — Strong gains across benchmarks (Table 1) and context-efficiency evidence (Table 2; Figures 2, 5), with some internal inconsistencies and missing ablations tempering confidence (Section 3.1; Sections 4.1–4.4).
- Novelty (10): 7 — The combination of dynamic sliding-window with train–test alignment (Section 3.1; Figures 2–3) and reverse-constructed multi-source obfuscated QA (Section 2; Figure 1) constitutes a meaningful system-level contribution.
- Technical Quality (10): 5 — Solid empirical results and a coherent training setup (Sections 3.1–3.3; Tables 1–3), but key inconsistencies in context accounting and sequence/mask definitions (Section 3.1; Figure 2) and limited ablations reduce technical confidence.
- Clarity (10): 6 — Generally clear exposition with helpful figures and settings (Sections 2–4; Figures 1–5), offset by notational ambiguities and the evaluation protocol inconsistency between judging and baseline adoption (Section 3.1; Section 4.1).
- Confidence (5): 4 — High confidence based on detailed manuscript and results (Tables 1–3; Figures 2–5), moderated by noted inconsistencies and missing analyses (Section 3.1; Sections 4.1–4.4).