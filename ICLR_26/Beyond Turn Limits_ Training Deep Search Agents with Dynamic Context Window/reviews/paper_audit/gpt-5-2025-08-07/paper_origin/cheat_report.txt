Integrity and internal-consistency review of “Beyond Turn Limits: Training Deep Search Agents with Dynamic Context Window”

Major issues identified (evidence-cited)

1) Impossible context accounting in Figure 2 (Section 3.1)
- Evidence: The table directly under “3.1 Dynamic Context Management Strategy” (Figure 2, right-side description) reports:
  - Number of Turns = 64: “Trajectory Context w/ Sliding Window ≈ 14k” while “Assistant Context ≈ 16k”.
  - Number of Turns = 128: “Trajectory Context w/ Sliding Window ≈ 14k” while “Assistant Context ≈ 20k”.
- Why this is a problem: The assistant-context tokens cannot exceed the total trajectory context under the sliding-window setting if “Trajectory Context w/ Sliding Window” denotes the total context used. As presented, Assistant Context > Trajectory Context (w/ SW) at 64 and 128 turns, which is mathematically impossible under any consistent definition of these quantities.
- Impact: This undermines the empirical support for the core claim that the sliding-window preserves assistant reasoning while controlling total context. It also casts doubt on the plotted/quoted magnitudes and interpretations in Figure 2 (and the associated narrative).
- Suggested fix: Clearly define each column (total context vs. tool-only context vs. assistant-only context). If “Trajectory Context w/ Sliding Window” refers only to the tool-response portion, relabel accordingly and add a column for total context. Otherwise, correct the numbers so total context ≥ assistant context at every turn.

2) Inconsistent training-sequence count K with stated behavior (Section 3.1)
- Evidence:
  - K is defined as K = floor((T − W)/S) + 1 (Section 3.1, “Training-Testing Consistency”).
  - The text states: “The first sequence τ(1) contains complete initial context with all assistant responses trained.”
- Why this is a problem: If T < W, the formula yields K = floor(negative) + 1, which can be 0 (e.g., T = 3, W = 5, S ≥ 1 → floor(−2/S) + 1 = −1 + 1 = 0), producing zero training sequences—contradicting the stated existence of a first sequence τ(1).
- Impact: This is a direct logical contradiction and affects the correctness/reproducibility of the proposed training pipeline, particularly for shorter trajectories.
- Suggested fix: Define K = max(1, floor((T − W)/S) + 1) or otherwise guarantee K ≥ 1, and update the description accordingly.

3) Masking rule contradicts claim that “first sequence trains all assistant responses” (Section 3.1)
- Evidence:
  - The paper states: “The first sequence τ(1) contains complete initial context with all assistant responses trained.”
  - The mask is defined as Mi(k) = 0 if i < W + (k − 2)·S + 2; 1 otherwise (Section 3.1).
- Why this is a problem: For k = 1, Mi(1) = 0 for i < W − S + 2, meaning some assistant responses are masked out (not trained) in the “first sequence,” directly contradicting the claim that all assistant responses are trained in τ(1).
- Impact: This inconsistency affects how gradients are applied and whether assistant responses are actually trained once as intended, undermining the stated “training-testing consistency.”
- Suggested fix: Provide a corrected mask formula consistent with the verbal description (e.g., ensure Mi(1) = 1 for all assistant indices intended to be trained) and include an explicit definition of i (indexing over which tokens/responses).

4) Evaluation protocol inconsistency: unified judge statement vs. mixed baselines (Section 4.1)
- Evidence:
  - Section 4.1, Evaluation: “All results are evaluated using ChatGPT-4o-Latest as the judge model…”
  - Section 4.1, Baselines: “We adopt the official results reported in their papers.”
- Why this is a problem: If “all results” are evaluated with the same judge (ChatGPT-4o-Latest), adopting baselines’ official numbers (which were likely obtained under different judges/prompts/tooling and sometimes different protocols) violates the “all results” claim and undermines comparability.
- Impact: Cross-system comparisons in Table 1 may not be on a uniform evaluation protocol; the headline improvements could in part reflect judge/protocol differences rather than true model gains.
- Suggested fix: Re-evaluate baselines under the same judge and settings, or explicitly state which rows are from external reports (with possibly different judges) and avoid the statement “All results are evaluated…” Clarify any protocol differences, including prompts and tool configurations, and caveat comparisons accordingly.

Additional inconsistencies or missing details that materially affect clarity/reproducibility

5) Turn-limit/context-length claims vary across sections (minor but clarifying would help)
- Evidence:
  - Abstract and Section 3.1: claims of “nearly 100 turns within standard 32k context length” and “even reach 100 turns within only 32k context.”
  - Conclusion: “enabling interactions that exceed 100 turns within 40k contexts.”
  - Training details (Section 4.1): RL training uses a “turn limit of 60” and “maximum trajectory length of 40k tokens.”
- Comment: These statements are not strictly contradictory (both 32k and 40k scenarios can be true in different setups), but the paper should harmonize the claims and clearly separate training constraints (60 turns, 40k) from inference demonstrations (≥100 turns) to avoid confusion.

6) Judge model for training reward is unspecified (Section 3.3)
- Evidence: Section 3.3, Reward Design: “We employ a straightforward reward design where an LLM judge evaluates whether the final answer matches the ground truth…” No judge model/version is named here.
- Impact: This omission affects reproducibility and comparability; the specific LLM judge can materially change reward signals.
- Suggested fix: Specify the exact judge model, prompts (Appendix D is provided for evaluation; confirm if the same is used for RL), and any aggregation or tie-breaking policy.

7) Ambiguity in notation for sliding boundary (Section 3.1)
- Evidence: “We identify the sliding boundary b = max(1, t − W + S)…” while earlier τ = {q, a1, t1, a2, t2, …}, and “sliding operation triggers when the number of accumulated tools |Rt| reaches W.” The variable t is overloaded (tool index vs. time) and Rt is not clearly defined in notation.
- Impact: Ambiguity may hinder correct implementation/reproduction.
- Suggested fix: Unambiguously define indices (e.g., tool-call index r ∈ {1…T}) and the set Rt, and consistently use them in b’s definition.

8) Illustrative data pipeline example uses fictional entity without explicit disclaimer (Section 2, Figure 1 text)
- Evidence: The running example uses “Mr. Capybara — a miner working at an animal mining farm…” (Blocks #6–#8, Figure 1).
- Impact: The method section repeatedly emphasizes grounding in “real-world entities and authentic web information,” but the example appears fictional. This can confuse readers about data realism.
- Suggested fix: Add a clear note that the example is illustrative only and not from the actual dataset construction.

Summary

- High-impact, evidence-backed inconsistencies were found in the core methodological section (impossible context accounting; contradictions in training sequence count and masking) and in the evaluation protocol description (uniform judge claim vs. adoption of official, heterogeneous baselines).
- These issues materially affect the correctness, interpretability, and reproducibility of the approach and its reported gains.

If the authors correct the numerical table in Figure 2, reconcile the K and masking formulas with the stated training behavior, and clarify the evaluation protocol and judge usage, many of the key integrity risks would be mitigated.