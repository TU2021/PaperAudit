{
  "baseline_review": "Summary\n- The paper proposes DeepMiner, a framework for training long-horizon, multi-turn web search agents by (1) constructing complex, verifiable QA tasks via reverse construction from authentic multi-source web content and obfuscation, and (2) introducing a dynamic sliding-window context management strategy that compresses distant tool responses while preserving assistant reasoning traces. The method targets Qwen3-32B, with supervised fine-tuning and reinforcement learning (GRPO) adapted to sequence-level training under dynamic contexts. DeepMiner-32B reports substantial gains on BrowseComp-en (33.5%), BrowseComp-zh, XBench-DeepSearch, and GAIA (Table 1), and demonstrates context efficiency enabling ~100 turns within 32k context length (Figure 2 right; Table 2; Section 3.1). Additional analyses include scaling with tool-call budgets and trajectory length/reward dynamics (Figure 4, Figure 5), and data efficiency versus HotpotQA (Table 3).Strengths\n- Bold, well-motivated dynamic context management\n  - The sliding-window mechanism replaces early tool responses with an omission token while preserving assistant outputs (Section 3.1; Figure 3; placeholder “[Previous tool output skipped. Rerun tool if needed].”), addressing exponential context growth of tool outputs observed in preliminary experiments (Figure 2 right; Section 3.1). This matters because it tackles a central bottleneck in long-horizon agents without external summarizers, improving technical soundness and practical impact.\n  - Training-testing consistency is carefully handled by decomposing a trajectory into K sequences and masking assistant responses so each is trained exactly once (Section 3.1; mask Mi(k) definition). This matters for algorithmic rigor and reduces distribution mismatch between training and inference.\n  - Empirical evidence shows the approach sustains nearly 100 turns within a 32k context length (Figure 2 right; Table 2; Introduction; Abstract), highlighting impact on long-horizon capability under realistic constraints.- Reverse construction of complex, verifiable QA tasks\n  - The pipeline uses Wikipedia for entity selection (moderate visibility: 1,000–10,000 page views) and authentic multi-source web pages via Google Search, followed by three-stage filtering (entity correspondence, information complementarity, credibility; Section 2; Figure 1; Blocks 6–8). This matters for data reliability and realism, addressing limitations of synthetic-only datasets.\n  - Questions explicitly require synthesis from ≥4 distinct sources and then undergo obfuscation to generalize specific details into categorical descriptions (Section 2: Question Generation; Block 9). This increases genuine reasoning demand and novelty.\n  - Multi-stage difficulty and quality filtering removes questions solvable via direct search or zero-shot LLM prompting and enforces unambiguous, derivable answers (Section 2: Multi-stage Filtering; Block 10). This matters for training signal fidelity and verifiable rewards.- Strong empirical performance across diverse benchmarks\n  - DeepMiner-32B-RL improves substantially over open-source agents on BrowseComp-en (33.5%) and BrowseComp-zh (40.1%), with consistent gains on XBench-DeepSearch (62.0%) and GAIA (58.7%) (Table 1; Section 4.2). This indicates broad impact and generalizability beyond a single benchmark.\n  - The SFT-only model (DeepMiner-32B-SFT) already outperforms many open-source baselines (Table 1), suggesting high-quality training trajectories and data design are effective (Section 4.2). This matters for data-centric training efficacy.\n  - Tool-call budget and context-length scaling analyses show monotonic performance gains with more turns and robust performance at 32k–40k context (Figure 5; Section 4.4). This is valuable for practitioners optimizing deployment constraints.- Context efficiency relative to summarization-based approaches\n  - The method outperforms both vanilla (no management) and external summarization approaches under tight 32k constraints, achieving 33.3% on BrowseComp with 32k context where baselines require 128k to reach similar numbers (Table 2; Section 4.3). This underscores practical efficiency and end-to-end RL compatibility (no external summarizer).\n  - The strategy maintains access to original webpage content (via fetch pagination) rather than compressed summaries (Appendix C; Section 3.1), reducing information loss—important for technical soundness in complex reasoning tasks.\n  - The sliding window enables assistant content growth while keeping tool responses bounded (Figure 2 right), aligning with the empirical finding that tool outputs dominate context usage (Section 3.1). This demonstrates measured design decisions tied to observed bottlenecks.- RL integration adapted to dynamic contexts\n  - GRPO is used with trajectory-level rewards and group-relative advantages, propagated consistently across sequence-level training instances derived from the same trajectory (Section 3.3; Equation for Âi; Figure 3 bottom; Figure 18). This shows algorithmic alignment to the proposed context mechanism.\n  - Binary reward via an LLM judge reduces reward engineering complexity (Section 3.3: Reward Design), which is practical for long-horizon tasks with verifiable answers.- Clarity and reproducible details\n  - The paper provides tool definitions and agent configuration (web_search, fetch with pagination, find; Section 4.1: Tool Configuration; Appendix C), training hyperparameters (Section 4.1: Training Details), and evaluation settings (temperature, top-p, turn limits; Section 4.1: Evaluation), aiding reproducibility.\n  - Illustrative figures (Figure 1, Figure 2, Figure 3, Figure 4, Figure 5) clearly convey pipeline, context dynamics, and training behavior, enhancing clarity.Weaknesses\n- Limited ablations and component-level analyses\n  - No ablation isolating the contribution of obfuscation versus multi-source synthesis versus filtering (Section 2; Figure 1; Block 9–10). This matters for novelty attribution and technical soundness, as gains could be driven by any single component.\n  - No sensitivity study on window size W and slide step S, nor alternatives to assistant-preserving vs tool-compressing policies (Section 3.1; Figure 3). This limits understanding of robustness and design choices.\n  - The masking scheme Mi(k) is presented but lacks comparative analysis against alternative credit assignment strategies (Section 3.1). This affects confidence in the specific training protocol.\n  - GRPO advantage propagation to sequences is described, but no ablation against standard trajectory-level backprop without sequence decomposition is shown (Section 3.3). This reduces evidence for the necessity of the modification.- Evaluation comparability and reliance on external judging\n  - Main results adopt “official results” of baselines rather than re-running under identical tools, prompts, and judges (Section 4.1: Evaluation; Section 4.1: Baselines). This matters for experimental rigor; cross-paper differences can confound comparisons.\n  - The evaluation uses ChatGPT-4o-Latest as the judge (Section 4.1; Appendix D), while training rewards also rely on an LLM judge but its identity is unspecified (Section 3.3: Reward Design—no model named). This undermines clarity and can bias outcomes.\n  - No statistical significance tests or variance measures accompany Table 1/2/3 or scaling curves (Sections 4.2–4.4; Figures 4–5). This impacts confidence in reported improvements.\n  - Some baselines are general LLMs with tools or proprietary agents with potentially different tool suites (Section 4.1: Baselines; Appendix C). Without a uniform environment, fairness is questionable.- Dataset transparency and quantitative characterization\n  - The paper does not report dataset statistics beyond counts used for SFT (~3,000 trajectories) and RL (~4,000 questions) (Section 4.1: Training Details). Missing: average sources per question (beyond “≥4”), question length, turn counts, and topic breakdown (Section 2; Appendix E).\n  - No release timeline or documentation ensuring reproducibility of reverse construction (Appendix B mentions access controls and anonymization but no concrete release plan). This limits verifiability.\n  - The “moderate visibility” criterion references Wikipedia views, but quantitative distributions of entities and domains are absent (Section 2: Entity-driven Information Collection). This affects generalizability claims.\n  - The filtering step’s pass rates and inter-annotator agreement or automated checks are not reported (Section 2: Multi-stage Filtering). No direct evidence found in the manuscript.- Insufficient evidence supporting “distant tool responses have minimal long-term impact”\n  - The paper claims long-term retention of tool responses may be unnecessary (Section 3.1: “Further investigation…”), but provides no quantitative causal analysis, user studies, or controlled experiments. No direct evidence found in the manuscript.\n  - No measurement of how often older tool outputs need revisiting and whether omission increases re-fetching overhead (Section 3.1; Appendix C). No direct evidence found in the manuscript.\n  - No comparison of sliding-window vs. learned summarization trained end-to-end within RL (Section 4.3 compares external summarization only). This matters for completeness of design space exploration.\n  - No error analysis showing if omissions harm specific task types (e.g., synthesis-heavy vs retrieval-heavy) (Sections 4.2–4.4). No direct evidence found in the manuscript.- Efficiency and cost reporting gaps\n  - Table 2 focuses on accuracy, not memory footprint, latency, or tool-call costs; no throughput/compute metrics are reported (Section 4.3). This is important for real-world deployment.\n  - Training details omit compute budget (GPUs, hours), making reproducibility and resource assessment difficult (Section 4.1: Training Details).\n  - Sliding-window impact on wall-clock time, given potential re-fetching due to omissions, is unquantified (Section 3.1; Appendix C). No direct evidence found in the manuscript.\n  - The trajectory limits (40k tokens, turns=60 during training; Section 4.1) and achieving “nearly 100 turns” (Abstract; Introduction; Figure 2 right) are not reconciled with runtime costs. This reduces practical clarity.- Reward design transparency and robustness\n  - The training LLM judge model is unspecified (Section 3.3: Reward Design), leaving ambiguity about consistency with evaluation judging (Section 4.1; Appendix D). This matters for reproducibility.\n  - Binary 1/0 reward may be overly sparse for complex multi-step research; no intermediate verification rewards or partial credit mechanisms are explored (Section 3.3). This affects optimization granularity.\n  - No experiments on reward hacking or adversarial trajectories (Section 3.3). This impacts robustness.\n  - The judge prompt (Appendix D) is provided for evaluation, but alignment of training and evaluation judges/prompts is unclear (Sections 3.3, 4.1). This reduces transparency.Suggestions for Improvement\n- Strengthen ablations and component analyses\n  - Compare models trained without obfuscation and/or without the ≥4-source constraint, and report deltas (Section 2; Figure 1; Blocks 9–10) to attribute gains.\n  - Conduct sensitivity studies over W and S and test alternative policies (e.g., compress assistant and retain tools) (Section 3.1; Figure 3).\n  - Evaluate alternative masking/credit schemes against Mi(k) (Section 3.1) to validate the chosen training protocol.\n  - Ablate GRPO’s sequence-level advantage propagation versus standard trajectory-level methods (Section 3.3).- Improve evaluation comparability and statistical rigor\n  - Re-run key open-source baselines in the same environment, tools, prompts, and judge to ensure apples-to-apples comparison (Section 4.1: Baselines).\n  - Specify the training LLM judge and ensure consistency with the evaluation judge; report sensitivity to different judges (Section 3.3; Section 4.1; Appendix D).\n  - Add confidence intervals or statistical tests for Tables 1–3 and Figures 4–5 (Sections 4.2–4.4).\n  - Benchmark on a standardized tool suite (Appendix C) across systems, or report how tool differences may influence outcomes (Section 4.1: Baselines).- Expand dataset reporting and release practices\n  - Provide quantitative statistics: distribution of entity types, average sources per question, question length, average required turns, topic diversity (Section 2; Appendix E; Section 4.1).\n  - Publish a clear release plan with documentation and scripts for reverse construction, subject to the ethical constraints (Appendix B), to improve reproducibility.\n  - Report distributions of “moderate visibility” entities (Section 2: Entity-driven Information Collection) and discuss how this choice impacts generalization.\n  - Quantify filtering pass rates and detail verification procedures; include inter-annotator agreement when applicable (Section 2: Multi-stage Filtering).- Provide empirical evidence on the impact of omitting distant tool responses\n  - Run controlled studies quantifying when older tool outputs are needed; measure how omissions affect success rates across task types (Section 3.1).\n  - Track re-fetch frequency and latency introduced by the omission token policy, reporting costs and performance deltas (Section 3.1; Appendix C).\n  - Compare sliding-window against end-to-end trained summarization within RL (not external-only) to test if learned compression can match or exceed performance (Section 4.3).\n  - Add error analysis categorizing failure modes attributable to omissions vs. other factors (Sections 4.2–4.4).- Report efficiency and compute costs more fully\n  - Provide memory and latency measurements for training and inference under different context management strategies (Section 4.3).\n  - Include compute budget details (hardware, tokens processed, hours) for SFT and RL phases (Section 4.1).\n  - Quantify wall-clock tradeoffs of the sliding window (e.g., additional fetch/find calls due to omissions) (Section 3.1; Appendix C).\n  - Clarify how “nearly 100 turns” at 32k (Figure 2; Abstract; Introduction) relates to training constraints (40k, 60 turns) and the operational costs.- Clarify reward design and test robustness\n  - Specify the training LLM judge model and prompt, and evaluate training sensitivity to judge choice (Section 3.3; Appendix D).\n  - Explore richer reward shaping (e.g., intermediate verifications and partial credit) to mitigate sparsity (Section 3.3).\n  - Test for reward hacking/adversarial behaviors and report defense mechanisms (Section 3.3).\n  - Align training and evaluation judges/prompts or document differences and their measured impacts (Sections 3.3, 4.1; Appendix D).Score\n- Overall (10): 7 — Substantial gains across multiple deep-research benchmarks (Table 1) and strong context-efficiency evidence (Figure 2; Table 2), but limited ablations and evaluation comparability reduce confidence (Sections 2, 3.1, 4.1–4.4).\n- Novelty (10): 7 — The dynamic sliding-window with assistant-preserving training-testing consistency (Section 3.1; Figure 3) and reverse-constructed multi-source obfuscated QA pipeline (Section 2; Figure 1) are meaningfully novel in combination.\n- Technical Quality (10): 6 — Sound algorithmic framing (Section 3.3; advantage equation; mask in Section 3.1) and empirical wins, tempered by missing ablations, sparse reward design details, and limited quantitative evidence for key claims (Sections 3.1–4.4).\n- Clarity (10): 7 — Clear method exposition and figures (Figures 1–5), with reproducible settings (Section 4.1), though some training/judging specifics are under-specified (Section 3.3; Appendix D).\n- Confidence (5): 4 — High confidence based on the thorough manuscript and reported results (Tables 1–3; Figures 2–5), moderated by the noted gaps in ablations and evaluation comparability (Sections 2–4).",
  "final_review": "Summary\n- The paper introduces DeepMiner, a framework for training long-horizon, multi-turn web search agents by (i) reverse-constructing complex, verifiable QA tasks from authentic multi-source web content with obfuscation and multi-stage filtering (Section 2; Figure 1) and (ii) applying a dynamic sliding-window context management strategy that compresses distant tool responses while preserving assistant reasoning traces (Section 3.1; Figures 2–3). Implemented on Qwen3-32B with supervised fine-tuning and reinforcement learning via GRPO adapted to sequence-level training under dynamic contexts (Section 3.3), DeepMiner-32B reports improvements on BrowseComp-en (33.5%), BrowseComp-zh, XBench-DeepSearch, and GAIA (Table 1; Section 4.2). Analyses include context-efficiency comparisons (Table 2; Section 4.3), scaling with tool-call budgets and context length (Figure 5; Section 4.4), training dynamics (Figure 4; Section 4.4), and SFT data-efficiency vs. HotpotQA (Table 3; Section 4.4).Strengths\n- Bold, well-motivated dynamic context management\n  - The sliding-window mechanism replaces early tool responses with an omission token while preserving assistant outputs (Section 3.1; Figure 3; placeholder “[Previous tool output skipped. Rerun tool if needed].”), addressing exponential context growth of tool outputs observed in preliminary experiments (Figure 2 right; Section 3.1). This matters because it targets a central bottleneck in long-horizon agents without external summarizers, improving technical soundness and practical impact.\n  - Training-testing consistency is handled by decomposing a trajectory into K sequences and masking assistant responses so each is trained exactly once (Section 3.1; masking Mi(k) definition). This matters for algorithmic rigor and reduces distribution mismatch between training and inference.\n  - Empirical evidence reports the approach can sustain nearly 100 turns within a 32k context length (Figure 2 right; Table 2; Introduction; Abstract), highlighting impact on long-horizon capability under realistic constraints.\n- Reverse construction of complex, verifiable QA tasks\n  - The pipeline uses Wikipedia for entity selection (moderate visibility: 1,000–10,000 page views) and authentic multi-source web pages via Google Search, followed by three-stage filtering (entity correspondence, information complementarity, credibility; Section 2; Figure 1). This matters for data reliability and realism, addressing limitations of synthetic-only datasets.\n  - Questions explicitly require synthesis from at least four distinct sources and then undergo obfuscation to generalize specific details into categorical descriptions (Section 2: Question Generation). This increases genuine reasoning demand and novelty.\n  - Multi-stage difficulty and quality filtering removes questions solvable via direct search or zero-shot LLM prompting and enforces unambiguous, derivable answers (Section 2: Multi-stage Filtering). This matters for training signal fidelity and verifiable rewards.\n- Strong empirical performance across diverse benchmarks\n  - DeepMiner-32B-RL improves over open-source agents on BrowseComp-en (33.5%) and BrowseComp-zh (40.1%), with gains on XBench-DeepSearch (62.0%) and GAIA (58.7%) (Table 1; Section 4.2). This suggests generalizability beyond a single benchmark.\n  - The SFT-only model (DeepMiner-32B-SFT) already outperforms many open-source baselines (Table 1; Section 4.2), indicating that training trajectories and data design are effective. This matters for data-centric training efficacy.\n  - Tool-call budget and context-length scaling analyses show monotonic performance gains with more turns and robust performance at 32k–40k context (Figure 5; Section 4.4). This is valuable for practitioners optimizing deployment constraints.\n- Context efficiency relative to summarization-based approaches\n  - The method outperforms both vanilla (no management) and external summarization approaches under tight 32k constraints, achieving 33.3% on BrowseComp with 32k context where baselines require 128k to reach similar numbers (Table 2; Section 4.3). This underscores practical efficiency and end-to-end RL compatibility (no external summarizer).\n  - The strategy maintains access to original webpage content (via fetch pagination) rather than compressed summaries (Appendix C; Section 3.1), reducing information loss—important for technical soundness in complex reasoning tasks.\n  - The sliding window enables assistant content growth while keeping tool responses bounded (Figure 2 right), aligning with the empirical finding that tool outputs dominate context usage (Section 3.1). This demonstrates measured design decisions tied to observed bottlenecks.\n- RL integration adapted to dynamic contexts\n  - GRPO is used with trajectory-level rewards and group-relative advantages, propagated consistently across sequence-level training instances derived from the same trajectory (Section 3.3; advantage equation; Figures 3 and 18-style schematic in Section 3.3 text/figure). This shows algorithmic alignment to the proposed context mechanism.\n  - Binary reward via an LLM judge reduces reward engineering complexity (Section 3.3: Reward Design), which is practical for long-horizon tasks with verifiable answers.\n- Clarity and reproducible details\n  - The paper provides tool definitions and agent configuration (web_search, fetch with pagination, find; Section 4.1: Tool Configuration; Appendix C), training hyperparameters (Section 4.1: Training Details), and evaluation settings (temperature, top-p, turn limits; Section 4.1: Evaluation), aiding reproducibility.\n  - Illustrative figures (Figures 1, 2, 3, 4, 5) convey pipeline, context dynamics, and training behavior, enhancing clarity.Weaknesses\n- Limited ablations and component-level analyses\n  - No ablation isolating the contribution of obfuscation versus multi-source synthesis versus filtering (Section 2; Figure 1). This matters for novelty attribution and technical soundness, as gains could be driven by any single component.\n  - No sensitivity study on window size W and slide step S, nor alternatives to assistant-preserving vs. tool-compressing policies (Section 3.1; Figure 3). This limits understanding of robustness and design choices.\n  - The masking scheme Mi(k) is presented but lacks comparative analysis against alternative credit assignment strategies (Section 3.1). This affects confidence in the specific training protocol.\n  - GRPO advantage propagation to sequences is described, but no ablation against standard trajectory-level backprop without sequence decomposition is shown (Section 3.3). This reduces evidence for the necessity of the modification.\n- Evaluation comparability and reliance on external judging\n  - Main results adopt “official results” of baselines rather than re-running under identical tools, prompts, and judges (Section 4.1: Baselines). This matters for experimental rigor; cross-paper differences can confound comparisons.\n  - The evaluation uses ChatGPT-4o-Latest as the judge (Section 4.1: Evaluation; Appendix D), while training rewards also rely on an LLM judge but its identity is unspecified (Section 3.3: Reward Design). This undermines clarity and can bias outcomes.\n  - No statistical significance tests or variance measures accompany Table 1/2/3 or scaling curves (Sections 4.2–4.4; Figures 4–5). This impacts confidence in reported improvements.\n  - The statement “All results are evaluated using ChatGPT-4o-Latest as the judge model” (Section 4.1: Evaluation) conflicts with “We adopt the official results reported in their papers” for baselines (Section 4.1: Baselines), weakening fairness and protocol consistency for Table 1.\n- Dataset transparency and quantitative characterization\n  - The paper does not report dataset statistics beyond counts used for SFT (~3,000 trajectories) and RL (~4,000 questions) (Section 4.1: Training Details). Missing: average sources per question (beyond “≥4”), question length, turn counts, and topic breakdown (Section 2; Appendix E).\n  - No release timeline or documentation ensuring reproducibility of reverse construction (Appendix B mentions access controls and anonymization but no concrete release plan). This limits verifiability.\n  - The “moderate visibility” criterion references Wikipedia views, but quantitative distributions of entities and domains are absent (Section 2: Entity-driven Information Collection). This affects generalizability claims.\n  - The filtering step’s pass rates and inter-annotator agreement or automated checks are not reported (Section 2: Multi-stage Filtering). No direct evidence found in the manuscript.\n  - The illustrative pipeline example uses a fictional entity “Mr. Capybara …” without an explicit disclaimer that it is not part of the actual dataset (Section 2; Figure 1), which may create confusion about data realism.\n- Insufficient evidence supporting “distant tool responses have minimal long-term impact”\n  - The paper claims long-term retention of tool responses may be unnecessary (Section 3.1), but provides no quantitative causal analysis, user studies, or controlled experiments. No direct evidence found in the manuscript.\n  - No measurement of how often older tool outputs need revisiting and whether omission increases re-fetching overhead (Section 3.1; Appendix C). No direct evidence found in the manuscript.\n  - No comparison of sliding-window vs. learned summarization trained end-to-end within RL (Section 4.3 compares external summarization only). This matters for completeness of design space exploration.\n  - No error analysis showing if omissions harm specific task types (e.g., synthesis-heavy vs. retrieval-heavy) (Sections 4.2–4.4). No direct evidence found in the manuscript.\n- Efficiency and cost reporting gaps\n  - Table 2 focuses on accuracy, not memory footprint, latency, or tool-call costs; no throughput/compute metrics are reported (Section 4.3). This is important for real-world deployment.\n  - Training details omit compute budget (GPUs, hours), making reproducibility and resource assessment difficult (Section 4.1: Training Details).\n  - Sliding-window impact on wall-clock time, given potential re-fetching due to omissions, is unquantified (Section 3.1; Appendix C). No direct evidence found in the manuscript.\n  - The trajectory limits (40k tokens, turns=60 during training; Section 4.1) and claims of “nearly 100 turns within standard 32k” (Abstract; Section 3.1; Figure 2 right) vs. “exceed 100 turns within 40k contexts” (Conclusion) are not reconciled with runtime costs. This reduces practical clarity.\n- Reward design transparency and robustness\n  - The training LLM judge model is unspecified (Section 3.3: Reward Design), leaving ambiguity about consistency with evaluation judging (Section 4.1; Appendix D). This matters for reproducibility.\n  - Binary 1/0 reward may be overly sparse for complex multi-step research; no intermediate verification rewards or partial credit mechanisms are explored (Section 3.3). This affects optimization granularity.\n  - No experiments on reward hacking or adversarial trajectories (Section 3.3). This impacts robustness.\n  - The judge prompt (Appendix D) is provided for evaluation, but alignment of training and evaluation judges/prompts is unclear (Sections 3.3, 4.1). This reduces transparency.\n- Internal inconsistencies in context accounting and training-sequence definitions\n  - Figure 2 (right) reports “Trajectory Context w/ Sliding Window ≈ 14k” at 64 and 128 turns while “Assistant Context ≈ 16k/20k” at those turns (Section 3.1; Figure 2). If the “Trajectory Context w/ Sliding Window” denotes total context, Assistant Context should not exceed it; this undermines the quantitative support for the context mechanism.\n  - The sequence count is defined as K = floor((T − W)/S) + 1 (Section 3.1: Training-Testing Consistency). For short trajectories (e.g., T < W), this can yield K ≤ 0, contradicting the statement that “The first sequence τ(1) contains complete initial context.”\n  - The masking rule Mi(k) = 0 if i < W + (k − 2)·S + 2; 1 otherwise (Section 3.1) appears inconsistent with the claim that “The first sequence τ(1) contains complete initial context with all assistant responses trained,” since for k = 1 some assistant responses are masked by the rule.\n  - Notation is underspecified: the set Rt and the index t used in the boundary b = max(1, t − W + S) (Section 3.1; Figure 3) are not unambiguously defined (tool-call index vs. time), which may hinder faithful reproduction.Suggestions for Improvement\n- Strengthen ablations and component analyses\n  - Compare models trained without obfuscation and/or without the ≥4-source constraint, and report deltas (Section 2; Figure 1) to attribute gains.\n  - Conduct sensitivity studies over W and S and test alternative policies (e.g., compress assistant and retain tools) (Section 3.1; Figure 3).\n  - Evaluate alternative masking/credit schemes against Mi(k) (Section 3.1) to validate the chosen training protocol.\n  - Ablate GRPO’s sequence-level advantage propagation versus standard trajectory-level methods (Section 3.3).\n- Improve evaluation comparability and statistical rigor\n  - Re-run key open-source baselines in the same environment, tools, prompts, and judge to ensure apples-to-apples comparison (Section 4.1: Baselines).\n  - Specify the training LLM judge and ensure consistency with the evaluation judge; report sensitivity to different judges (Section 3.3; Section 4.1; Appendix D).\n  - Add confidence intervals or statistical tests for Tables 1–3 and Figures 4–5 (Sections 4.2–4.4).\n  - Clearly denote which rows in Table 1 are externally reported and, where possible, re-evaluate them with the same judge/protocol used for DeepMiner (Section 4.1: Evaluation; Section 4.1: Baselines).\n- Expand dataset reporting and release practices\n  - Provide quantitative statistics: distribution of entity types, average sources per question, question length, average required turns, topic diversity (Section 2; Appendix E; Section 4.1).\n  - Publish a clear release plan with documentation and scripts for reverse construction, subject to the ethical constraints (Appendix B), to improve reproducibility.\n  - Report distributions of “moderate visibility” entities (Section 2: Entity-driven Information Collection) and discuss how this choice impacts generalization.\n  - Quantify filtering pass rates and detail verification procedures; include inter-annotator agreement when applicable (Section 2: Multi-stage Filtering).\n  - Add an explicit disclaimer that the “Mr. Capybara” example in Figure 1 is illustrative only and not part of the actual dataset (Section 2; Figure 1).\n- Provide empirical evidence on the impact of omitting distant tool responses\n  - Run controlled studies quantifying when older tool outputs are needed; measure how omissions affect success rates across task types (Section 3.1).\n  - Track re-fetch frequency and latency introduced by the omission token policy, reporting costs and performance deltas (Section 3.1; Appendix C).\n  - Compare sliding-window against end-to-end trained summarization within RL (not external-only) to test if learned compression can match or exceed performance (Section 4.3).\n  - Add error analysis categorizing failure modes attributable to omissions vs. other factors (Sections 4.2–4.4).\n- Report efficiency and compute costs more fully\n  - Provide memory and latency measurements for training and inference under different context management strategies (Section 4.3).\n  - Include compute budget details (hardware, tokens processed, hours) for SFT and RL phases (Section 4.1).\n  - Quantify wall-clock tradeoffs of the sliding window (e.g., additional fetch/find calls due to omissions) (Section 3.1; Appendix C).\n  - Clarify how “nearly 100 turns” at 32k (Figure 2; Abstract; Section 3.1) relates to training constraints (40k, 60 turns; Section 4.1) and the “exceed 100 turns within 40k contexts” claim (Conclusion), including runtime costs.\n- Clarify reward design and test robustness\n  - Specify the training LLM judge model and prompt, and evaluate training sensitivity to judge choice (Section 3.3; Appendix D).\n  - Explore richer reward shaping (e.g., intermediate verifications and partial credit) to mitigate sparsity (Section 3.3).\n  - Test for reward hacking/adversarial behaviors and report defense mechanisms (Section 3.3).\n  - Align training and evaluation judges/prompts or document differences and their measured impacts (Sections 3.3, 4.1; Appendix D).\n- Clarify and correct context accounting and training-sequence/mask definitions\n  - Disambiguate Figure 2 (right) by precisely defining each column (e.g., total trajectory context vs. tool-only vs. assistant-only) and ensure reported values are consistent (Section 3.1; Figure 2).\n  - Revise the definition of K to guarantee K ≥ 1 (e.g., K = max(1, floor((T − W)/S) + 1)) and document behavior for short trajectories (Section 3.1).\n  - Align the masking rule Mi(k) with the narrative that τ(1) trains all intended assistant responses, and clearly define the indexing of i (Section 3.1).\n  - Unambiguously define Rt and the index used in b = max(1, t − W + S), avoiding overload and specifying whether it counts tool calls or steps (Section 3.1; Figure 3).Score\n- Overall (10): 7 — Strong gains across benchmarks (Table 1) and context-efficiency evidence (Table 2; Figures 2, 5), with some internal inconsistencies and missing ablations tempering confidence (Section 3.1; Sections 4.1–4.4).\n- Novelty (10): 7 — The combination of dynamic sliding-window with train–test alignment (Section 3.1; Figures 2–3) and reverse-constructed multi-source obfuscated QA (Section 2; Figure 1) constitutes a meaningful system-level contribution.\n- Technical Quality (10): 5 — Solid empirical results and a coherent training setup (Sections 3.1–3.3; Tables 1–3), but key inconsistencies in context accounting and sequence/mask definitions (Section 3.1; Figure 2) and limited ablations reduce technical confidence.\n- Clarity (10): 6 — Generally clear exposition with helpful figures and settings (Sections 2–4; Figures 1–5), offset by notational ambiguities and the evaluation protocol inconsistency between judging and baseline adoption (Section 3.1; Section 4.1).\n- Confidence (5): 4 — High confidence based on detailed manuscript and results (Tables 1–3; Figures 2–5), moderated by noted inconsistencies and missing analyses (Section 3.1; Sections 4.1–4.4).",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 7,
        "novelty": 7,
        "technical_quality": 6,
        "clarity": 7,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 7,
        "novelty": 7,
        "technical_quality": 5,
        "clarity": 6,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "Summary\n- The paper introduces DeepMiner, a framework for training long-horizon, multi-turn web search agents by (i) reverse-constructing complex, verifiable QA tasks from authentic multi-source web content with obfuscation and multi-stage filtering (Section 2; Figure 1) and (ii) applying a dynamic sliding-window context management strategy that compresses distant tool responses while preserving assistant reasoning traces (Section 3.1; Figures 2–3). Implemented on Qwen3-32B with supervised fine-tuning and reinforcement learning via GRPO adapted to sequence-level training under dynamic contexts (Section 3.3), DeepMiner-32B reports improvements on BrowseComp-en (33.5%), BrowseComp-zh, XBench-DeepSearch, and GAIA (Table 1; Section 4.2). Analyses include context-efficiency comparisons (Table 2; Section 4.3), scaling with tool-call budgets and context length (Figure 5; Section 4.4), training dynamics (Figure 4; Section 4.4), and SFT data-efficiency vs. HotpotQA (Table 3; Section 4.4).Strengths\n- Bold, well-motivated dynamic context management\n  - The sliding-window mechanism replaces early tool responses with an omission token while preserving assistant outputs (Section 3.1; Figure 3; placeholder “[Previous tool output skipped. Rerun tool if needed].”), addressing exponential context growth of tool outputs observed in preliminary experiments (Figure 2 right; Section 3.1). This matters because it targets a central bottleneck in long-horizon agents without external summarizers, improving technical soundness and practical impact.\n  - Training-testing consistency is handled by decomposing a trajectory into K sequences and masking assistant responses so each is trained exactly once (Section 3.1; masking Mi(k) definition). This matters for algorithmic rigor and reduces distribution mismatch between training and inference.\n  - Empirical evidence reports the approach can sustain nearly 100 turns within a 32k context length (Figure 2 right; Table 2; Introduction; Abstract), highlighting impact on long-horizon capability under realistic constraints.\n- Reverse construction of complex, verifiable QA tasks\n  - The pipeline uses Wikipedia for entity selection (moderate visibility: 1,000–10,000 page views) and authentic multi-source web pages via Google Search, followed by three-stage filtering (entity correspondence, information complementarity, credibility; Section 2; Figure 1). This matters for data reliability and realism, addressing limitations of synthetic-only datasets.\n  - Questions explicitly require synthesis from at least four distinct sources and then undergo obfuscation to generalize specific details into categorical descriptions (Section 2: Question Generation). This increases genuine reasoning demand and novelty.\n  - Multi-stage difficulty and quality filtering removes questions solvable via direct search or zero-shot LLM prompting and enforces unambiguous, derivable answers (Section 2: Multi-stage Filtering). This matters for training signal fidelity and verifiable rewards.\n- Strong empirical performance across diverse benchmarks\n  - DeepMiner-32B-RL improves over open-source agents on BrowseComp-en (33.5%) and BrowseComp-zh (40.1%), with gains on XBench-DeepSearch (62.0%) and GAIA (58.7%) (Table 1; Section 4.2). This suggests generalizability beyond a single benchmark.\n  - The SFT-only model (DeepMiner-32B-SFT) already outperforms many open-source baselines (Table 1; Section 4.2), indicating that training trajectories and data design are effective. This matters for data-centric training efficacy.\n  - Tool-call budget and context-length scaling analyses show monotonic performance gains with more turns and robust performance at 32k–40k context (Figure 5; Section 4.4). This is valuable for practitioners optimizing deployment constraints.\n- Context efficiency relative to summarization-based approaches\n  - The method outperforms both vanilla (no management) and external summarization approaches under tight 32k constraints, achieving 33.3% on BrowseComp with 32k context where baselines require 128k to reach similar numbers (Table 2; Section 4.3). This underscores practical efficiency and end-to-end RL compatibility (no external summarizer).\n  - The strategy maintains access to original webpage content (via fetch pagination) rather than compressed summaries (Appendix C; Section 3.1), reducing information loss—important for technical soundness in complex reasoning tasks.\n  - The sliding window enables assistant content growth while keeping tool responses bounded (Figure 2 right), aligning with the empirical finding that tool outputs dominate context usage (Section 3.1). This demonstrates measured design decisions tied to observed bottlenecks.\n- RL integration adapted to dynamic contexts\n  - GRPO is used with trajectory-level rewards and group-relative advantages, propagated consistently across sequence-level training instances derived from the same trajectory (Section 3.3; advantage equation; Figures 3 and 18-style schematic in Section 3.3 text/figure). This shows algorithmic alignment to the proposed context mechanism.\n  - Binary reward via an LLM judge reduces reward engineering complexity (Section 3.3: Reward Design), which is practical for long-horizon tasks with verifiable answers.\n- Clarity and reproducible details\n  - The paper provides tool definitions and agent configuration (web_search, fetch with pagination, find; Section 4.1: Tool Configuration; Appendix C), training hyperparameters (Section 4.1: Training Details), and evaluation settings (temperature, top-p, turn limits; Section 4.1: Evaluation), aiding reproducibility.\n  - Illustrative figures (Figures 1, 2, 3, 4, 5) convey pipeline, context dynamics, and training behavior, enhancing clarity.Weaknesses\n- Limited ablations and component-level analyses\n  - No ablation isolating the contribution of obfuscation versus multi-source synthesis versus filtering (Section 2; Figure 1). This matters for novelty attribution and technical soundness, as gains could be driven by any single component.\n  - No sensitivity study on window size W and slide step S, nor alternatives to assistant-preserving vs. tool-compressing policies (Section 3.1; Figure 3). This limits understanding of robustness and design choices.\n  - The masking scheme Mi(k) is presented but lacks comparative analysis against alternative credit assignment strategies (Section 3.1). This affects confidence in the specific training protocol.\n  - GRPO advantage propagation to sequences is described, but no ablation against standard trajectory-level backprop without sequence decomposition is shown (Section 3.3). This reduces evidence for the necessity of the modification.\n- Evaluation comparability and reliance on external judging\n  - Main results adopt “official results” of baselines rather than re-running under identical tools, prompts, and judges (Section 4.1: Baselines). This matters for experimental rigor; cross-paper differences can confound comparisons.\n  - The evaluation uses ChatGPT-4o-Latest as the judge (Section 4.1: Evaluation; Appendix D), while training rewards also rely on an LLM judge but its identity is unspecified (Section 3.3: Reward Design). This undermines clarity and can bias outcomes.\n  - No statistical significance tests or variance measures accompany Table 1/2/3 or scaling curves (Sections 4.2–4.4; Figures 4–5). This impacts confidence in reported improvements.\n  - The statement “All results are evaluated using ChatGPT-4o-Latest as the judge model” (Section 4.1: Evaluation) conflicts with “We adopt the official results reported in their papers” for baselines (Section 4.1: Baselines), weakening fairness and protocol consistency for Table 1.\n- Dataset transparency and quantitative characterization\n  - The paper does not report dataset statistics beyond counts used for SFT (~3,000 trajectories) and RL (~4,000 questions) (Section 4.1: Training Details). Missing: average sources per question (beyond “≥4”), question length, turn counts, and topic breakdown (Section 2; Appendix E).\n  - No release timeline or documentation ensuring reproducibility of reverse construction (Appendix B mentions access controls and anonymization but no concrete release plan). This limits verifiability.\n  - The “moderate visibility” criterion references Wikipedia views, but quantitative distributions of entities and domains are absent (Section 2: Entity-driven Information Collection). This affects generalizability claims.\n  - The filtering step’s pass rates and inter-annotator agreement or automated checks are not reported (Section 2: Multi-stage Filtering). No direct evidence found in the manuscript.\n  - The illustrative pipeline example uses a fictional entity “Mr. Capybara …” without an explicit disclaimer that it is not part of the actual dataset (Section 2; Figure 1), which may create confusion about data realism.\n- Insufficient evidence supporting “distant tool responses have minimal long-term impact”\n  - The paper claims long-term retention of tool responses may be unnecessary (Section 3.1), but provides no quantitative causal analysis, user studies, or controlled experiments. No direct evidence found in the manuscript.\n  - No measurement of how often older tool outputs need revisiting and whether omission increases re-fetching overhead (Section 3.1; Appendix C). No direct evidence found in the manuscript.\n  - No comparison of sliding-window vs. learned summarization trained end-to-end within RL (Section 4.3 compares external summarization only). This matters for completeness of design space exploration.\n  - No error analysis showing if omissions harm specific task types (e.g., synthesis-heavy vs. retrieval-heavy) (Sections 4.2–4.4). No direct evidence found in the manuscript.\n- Efficiency and cost reporting gaps\n  - Table 2 focuses on accuracy, not memory footprint, latency, or tool-call costs; no throughput/compute metrics are reported (Section 4.3). This is important for real-world deployment.\n  - Training details omit compute budget (GPUs, hours), making reproducibility and resource assessment difficult (Section 4.1: Training Details).\n  - Sliding-window impact on wall-clock time, given potential re-fetching due to omissions, is unquantified (Section 3.1; Appendix C). No direct evidence found in the manuscript.\n  - The trajectory limits (40k tokens, turns=60 during training; Section 4.1) and claims of “nearly 100 turns within standard 32k” (Abstract; Section 3.1; Figure 2 right) vs. “exceed 100 turns within 40k contexts” (Conclusion) are not reconciled with runtime costs. This reduces practical clarity.\n- Reward design transparency and robustness\n  - The training LLM judge model is unspecified (Section 3.3: Reward Design), leaving ambiguity about consistency with evaluation judging (Section 4.1; Appendix D). This matters for reproducibility.\n  - Binary 1/0 reward may be overly sparse for complex multi-step research; no intermediate verification rewards or partial credit mechanisms are explored (Section 3.3). This affects optimization granularity.\n  - No experiments on reward hacking or adversarial trajectories (Section 3.3). This impacts robustness.\n  - The judge prompt (Appendix D) is provided for evaluation, but alignment of training and evaluation judges/prompts is unclear (Sections 3.3, 4.1). This reduces transparency.\n- Internal inconsistencies in context accounting and training-sequence definitions\n  - Figure 2 (right) reports “Trajectory Context w/ Sliding Window ≈ 14k” at 64 and 128 turns while “Assistant Context ≈ 16k/20k” at those turns (Section 3.1; Figure 2). If the “Trajectory Context w/ Sliding Window” denotes total context, Assistant Context should not exceed it; this undermines the quantitative support for the context mechanism.\n  - The sequence count is defined as K = floor((T − W)/S) + 1 (Section 3.1: Training-Testing Consistency). For short trajectories (e.g., T < W), this can yield K ≤ 0, contradicting the statement that “The first sequence τ(1) contains complete initial context.”\n  - The masking rule Mi(k) = 0 if i < W + (k − 2)·S + 2; 1 otherwise (Section 3.1) appears inconsistent with the claim that “The first sequence τ(1) contains complete initial context with all assistant responses trained,” since for k = 1 some assistant responses are masked by the rule.\n  - Notation is underspecified: the set Rt and the index t used in the boundary b = max(1, t − W + S) (Section 3.1; Figure 3) are not unambiguously defined (tool-call index vs. time), which may hinder faithful reproduction.Suggestions for Improvement\n- Strengthen ablations and component analyses\n  - Compare models trained without obfuscation and/or without the ≥4-source constraint, and report deltas (Section 2; Figure 1) to attribute gains.\n  - Conduct sensitivity studies over W and S and test alternative policies (e.g., compress assistant and retain tools) (Section 3.1; Figure 3).\n  - Evaluate alternative masking/credit schemes against Mi(k) (Section 3.1) to validate the chosen training protocol.\n  - Ablate GRPO’s sequence-level advantage propagation versus standard trajectory-level methods (Section 3.3).\n- Improve evaluation comparability and statistical rigor\n  - Re-run key open-source baselines in the same environment, tools, prompts, and judge to ensure apples-to-apples comparison (Section 4.1: Baselines).\n  - Specify the training LLM judge and ensure consistency with the evaluation judge; report sensitivity to different judges (Section 3.3; Section 4.1; Appendix D).\n  - Add confidence intervals or statistical tests for Tables 1–3 and Figures 4–5 (Sections 4.2–4.4).\n  - Clearly denote which rows in Table 1 are externally reported and, where possible, re-evaluate them with the same judge/protocol used for DeepMiner (Section 4.1: Evaluation; Section 4.1: Baselines).\n- Expand dataset reporting and release practices\n  - Provide quantitative statistics: distribution of entity types, average sources per question, question length, average required turns, topic diversity (Section 2; Appendix E; Section 4.1).\n  - Publish a clear release plan with documentation and scripts for reverse construction, subject to the ethical constraints (Appendix B), to improve reproducibility.\n  - Report distributions of “moderate visibility” entities (Section 2: Entity-driven Information Collection) and discuss how this choice impacts generalization.\n  - Quantify filtering pass rates and detail verification procedures; include inter-annotator agreement when applicable (Section 2: Multi-stage Filtering).\n  - Add an explicit disclaimer that the “Mr. Capybara” example in Figure 1 is illustrative only and not part of the actual dataset (Section 2; Figure 1).\n- Provide empirical evidence on the impact of omitting distant tool responses\n  - Run controlled studies quantifying when older tool outputs are needed; measure how omissions affect success rates across task types (Section 3.1).\n  - Track re-fetch frequency and latency introduced by the omission token policy, reporting costs and performance deltas (Section 3.1; Appendix C).\n  - Compare sliding-window against end-to-end trained summarization within RL (not external-only) to test if learned compression can match or exceed performance (Section 4.3).\n  - Add error analysis categorizing failure modes attributable to omissions vs. other factors (Sections 4.2–4.4).\n- Report efficiency and compute costs more fully\n  - Provide memory and latency measurements for training and inference under different context management strategies (Section 4.3).\n  - Include compute budget details (hardware, tokens processed, hours) for SFT and RL phases (Section 4.1).\n  - Quantify wall-clock tradeoffs of the sliding window (e.g., additional fetch/find calls due to omissions) (Section 3.1; Appendix C).\n  - Clarify how “nearly 100 turns” at 32k (Figure 2; Abstract; Section 3.1) relates to training constraints (40k, 60 turns; Section 4.1) and the “exceed 100 turns within 40k contexts” claim (Conclusion), including runtime costs.\n- Clarify reward design and test robustness\n  - Specify the training LLM judge model and prompt, and evaluate training sensitivity to judge choice (Section 3.3; Appendix D).\n  - Explore richer reward shaping (e.g., intermediate verifications and partial credit) to mitigate sparsity (Section 3.3).\n  - Test for reward hacking/adversarial behaviors and report defense mechanisms (Section 3.3).\n  - Align training and evaluation judges/prompts or document differences and their measured impacts (Sections 3.3, 4.1; Appendix D).\n- Clarify and correct context accounting and training-sequence/mask definitions\n  - Disambiguate Figure 2 (right) by precisely defining each column (e.g., total trajectory context vs. tool-only vs. assistant-only) and ensure reported values are consistent (Section 3.1; Figure 2).\n  - Revise the definition of K to guarantee K ≥ 1 (e.g., K = max(1, floor((T − W)/S) + 1)) and document behavior for short trajectories (Section 3.1).\n  - Align the masking rule Mi(k) with the narrative that τ(1) trains all intended assistant responses, and clearly define the indexing of i (Section 3.1).\n  - Unambiguously define Rt and the index used in b = max(1, t − W + S), avoiding overload and specifying whether it counts tool calls or steps (Section 3.1; Figure 3).Score\n- Overall (10): 7 — Strong gains across benchmarks (Table 1) and context-efficiency evidence (Table 2; Figures 2, 5), with some internal inconsistencies and missing ablations tempering confidence (Section 3.1; Sections 4.1–4.4).\n- Novelty (10): 7 — The combination of dynamic sliding-window with train–test alignment (Section 3.1; Figures 2–3) and reverse-constructed multi-source obfuscated QA (Section 2; Figure 1) constitutes a meaningful system-level contribution.\n- Technical Quality (10): 5 — Solid empirical results and a coherent training setup (Sections 3.1–3.3; Tables 1–3), but key inconsistencies in context accounting and sequence/mask definitions (Section 3.1; Figure 2) and limited ablations reduce technical confidence.\n- Clarity (10): 6 — Generally clear exposition with helpful figures and settings (Sections 2–4; Figures 1–5), offset by notational ambiguities and the evaluation protocol inconsistency between judging and baseline adoption (Section 3.1; Section 4.1).\n- Confidence (5): 4 — High confidence based on detailed manuscript and results (Tables 1–3; Figures 2–5), moderated by noted inconsistencies and missing analyses (Section 3.1; Sections 4.1–4.4)."
}