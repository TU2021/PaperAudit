Summary
- The paper proposes DeepMiner, a framework for training long-horizon, multi-turn web search agents by (1) constructing complex, verifiable QA tasks via reverse construction from authentic multi-source web content and obfuscation, and (2) introducing a dynamic sliding-window context management strategy that compresses distant tool responses while preserving assistant reasoning traces. The method targets Qwen3-32B, with supervised fine-tuning and reinforcement learning (GRPO) adapted to sequence-level training under dynamic contexts. DeepMiner-32B reports substantial gains on BrowseComp-en (33.5%), BrowseComp-zh, XBench-DeepSearch, and GAIA (Table 1), and demonstrates context efficiency enabling ~100 turns within 32k context length (Figure 2 right; Table 2; Section 3.1). Additional analyses include scaling with tool-call budgets and trajectory length/reward dynamics (Figure 4, Figure 5), and data efficiency versus HotpotQA (Table 3).Strengths
- Bold, well-motivated dynamic context management
  - The sliding-window mechanism replaces early tool responses with an omission token while preserving assistant outputs (Section 3.1; Figure 3; placeholder “[Previous tool output skipped. Rerun tool if needed].”), addressing exponential context growth of tool outputs observed in preliminary experiments (Figure 2 right; Section 3.1). This matters because it tackles a central bottleneck in long-horizon agents without external summarizers, improving technical soundness and practical impact.
  - Training-testing consistency is carefully handled by decomposing a trajectory into K sequences and masking assistant responses so each is trained exactly once (Section 3.1; mask Mi(k) definition). This matters for algorithmic rigor and reduces distribution mismatch between training and inference.
  - Empirical evidence shows the approach sustains nearly 100 turns within a 32k context length (Figure 2 right; Table 2; Introduction; Abstract), highlighting impact on long-horizon capability under realistic constraints.- Reverse construction of complex, verifiable QA tasks
  - The pipeline uses Wikipedia for entity selection (moderate visibility: 1,000–10,000 page views) and authentic multi-source web pages via Google Search, followed by three-stage filtering (entity correspondence, information complementarity, credibility; Section 2; Figure 1; Blocks 6–8). This matters for data reliability and realism, addressing limitations of synthetic-only datasets.
  - Questions explicitly require synthesis from ≥4 distinct sources and then undergo obfuscation to generalize specific details into categorical descriptions (Section 2: Question Generation; Block 9). This increases genuine reasoning demand and novelty.
  - Multi-stage difficulty and quality filtering removes questions solvable via direct search or zero-shot LLM prompting and enforces unambiguous, derivable answers (Section 2: Multi-stage Filtering; Block 10). This matters for training signal fidelity and verifiable rewards.- Strong empirical performance across diverse benchmarks
  - DeepMiner-32B-RL improves substantially over open-source agents on BrowseComp-en (33.5%) and BrowseComp-zh (40.1%), with consistent gains on XBench-DeepSearch (62.0%) and GAIA (58.7%) (Table 1; Section 4.2). This indicates broad impact and generalizability beyond a single benchmark.
  - The SFT-only model (DeepMiner-32B-SFT) already outperforms many open-source baselines (Table 1), suggesting high-quality training trajectories and data design are effective (Section 4.2). This matters for data-centric training efficacy.
  - Tool-call budget and context-length scaling analyses show monotonic performance gains with more turns and robust performance at 32k–40k context (Figure 5; Section 4.4). This is valuable for practitioners optimizing deployment constraints.- Context efficiency relative to summarization-based approaches
  - The method outperforms both vanilla (no management) and external summarization approaches under tight 32k constraints, achieving 33.3% on BrowseComp with 32k context where baselines require 128k to reach similar numbers (Table 2; Section 4.3). This underscores practical efficiency and end-to-end RL compatibility (no external summarizer).
  - The strategy maintains access to original webpage content (via fetch pagination) rather than compressed summaries (Appendix C; Section 3.1), reducing information loss—important for technical soundness in complex reasoning tasks.
  - The sliding window enables assistant content growth while keeping tool responses bounded (Figure 2 right), aligning with the empirical finding that tool outputs dominate context usage (Section 3.1). This demonstrates measured design decisions tied to observed bottlenecks.- RL integration adapted to dynamic contexts
  - GRPO is used with trajectory-level rewards and group-relative advantages, propagated consistently across sequence-level training instances derived from the same trajectory (Section 3.3; Equation for Âi; Figure 3 bottom; Figure 18). This shows algorithmic alignment to the proposed context mechanism.
  - Binary reward via an LLM judge reduces reward engineering complexity (Section 3.3: Reward Design), which is practical for long-horizon tasks with verifiable answers.- Clarity and reproducible details
  - The paper provides tool definitions and agent configuration (web_search, fetch with pagination, find; Section 4.1: Tool Configuration; Appendix C), training hyperparameters (Section 4.1: Training Details), and evaluation settings (temperature, top-p, turn limits; Section 4.1: Evaluation), aiding reproducibility.
  - Illustrative figures (Figure 1, Figure 2, Figure 3, Figure 4, Figure 5) clearly convey pipeline, context dynamics, and training behavior, enhancing clarity.Weaknesses
- Limited ablations and component-level analyses
  - No ablation isolating the contribution of obfuscation versus multi-source synthesis versus filtering (Section 2; Figure 1; Block 9–10). This matters for novelty attribution and technical soundness, as gains could be driven by any single component.
  - No sensitivity study on window size W and slide step S, nor alternatives to assistant-preserving vs tool-compressing policies (Section 3.1; Figure 3). This limits understanding of robustness and design choices.
  - The masking scheme Mi(k) is presented but lacks comparative analysis against alternative credit assignment strategies (Section 3.1). This affects confidence in the specific training protocol.
  - GRPO advantage propagation to sequences is described, but no ablation against standard trajectory-level backprop without sequence decomposition is shown (Section 3.3). This reduces evidence for the necessity of the modification.- Evaluation comparability and reliance on external judging
  - Main results adopt “official results” of baselines rather than re-running under identical tools, prompts, and judges (Section 4.1: Evaluation; Section 4.1: Baselines). This matters for experimental rigor; cross-paper differences can confound comparisons.
  - The evaluation uses ChatGPT-4o-Latest as the judge (Section 4.1; Appendix D), while training rewards also rely on an LLM judge but its identity is unspecified (Section 3.3: Reward Design—no model named). This undermines clarity and can bias outcomes.
  - No statistical significance tests or variance measures accompany Table 1/2/3 or scaling curves (Sections 4.2–4.4; Figures 4–5). This impacts confidence in reported improvements.
  - Some baselines are general LLMs with tools or proprietary agents with potentially different tool suites (Section 4.1: Baselines; Appendix C). Without a uniform environment, fairness is questionable.- Dataset transparency and quantitative characterization
  - The paper does not report dataset statistics beyond counts used for SFT (~3,000 trajectories) and RL (~4,000 questions) (Section 4.1: Training Details). Missing: average sources per question (beyond “≥4”), question length, turn counts, and topic breakdown (Section 2; Appendix E).
  - No release timeline or documentation ensuring reproducibility of reverse construction (Appendix B mentions access controls and anonymization but no concrete release plan). This limits verifiability.
  - The “moderate visibility” criterion references Wikipedia views, but quantitative distributions of entities and domains are absent (Section 2: Entity-driven Information Collection). This affects generalizability claims.
  - The filtering step’s pass rates and inter-annotator agreement or automated checks are not reported (Section 2: Multi-stage Filtering). No direct evidence found in the manuscript.- Insufficient evidence supporting “distant tool responses have minimal long-term impact”
  - The paper claims long-term retention of tool responses may be unnecessary (Section 3.1: “Further investigation…”), but provides no quantitative causal analysis, user studies, or controlled experiments. No direct evidence found in the manuscript.
  - No measurement of how often older tool outputs need revisiting and whether omission increases re-fetching overhead (Section 3.1; Appendix C). No direct evidence found in the manuscript.
  - No comparison of sliding-window vs. learned summarization trained end-to-end within RL (Section 4.3 compares external summarization only). This matters for completeness of design space exploration.
  - No error analysis showing if omissions harm specific task types (e.g., synthesis-heavy vs retrieval-heavy) (Sections 4.2–4.4). No direct evidence found in the manuscript.- Efficiency and cost reporting gaps
  - Table 2 focuses on accuracy, not memory footprint, latency, or tool-call costs; no throughput/compute metrics are reported (Section 4.3). This is important for real-world deployment.
  - Training details omit compute budget (GPUs, hours), making reproducibility and resource assessment difficult (Section 4.1: Training Details).
  - Sliding-window impact on wall-clock time, given potential re-fetching due to omissions, is unquantified (Section 3.1; Appendix C). No direct evidence found in the manuscript.
  - The trajectory limits (40k tokens, turns=60 during training; Section 4.1) and achieving “nearly 100 turns” (Abstract; Introduction; Figure 2 right) are not reconciled with runtime costs. This reduces practical clarity.- Reward design transparency and robustness
  - The training LLM judge model is unspecified (Section 3.3: Reward Design), leaving ambiguity about consistency with evaluation judging (Section 4.1; Appendix D). This matters for reproducibility.
  - Binary 1/0 reward may be overly sparse for complex multi-step research; no intermediate verification rewards or partial credit mechanisms are explored (Section 3.3). This affects optimization granularity.
  - No experiments on reward hacking or adversarial trajectories (Section 3.3). This impacts robustness.
  - The judge prompt (Appendix D) is provided for evaluation, but alignment of training and evaluation judges/prompts is unclear (Sections 3.3, 4.1). This reduces transparency.Suggestions for Improvement
- Strengthen ablations and component analyses
  - Compare models trained without obfuscation and/or without the ≥4-source constraint, and report deltas (Section 2; Figure 1; Blocks 9–10) to attribute gains.
  - Conduct sensitivity studies over W and S and test alternative policies (e.g., compress assistant and retain tools) (Section 3.1; Figure 3).
  - Evaluate alternative masking/credit schemes against Mi(k) (Section 3.1) to validate the chosen training protocol.
  - Ablate GRPO’s sequence-level advantage propagation versus standard trajectory-level methods (Section 3.3).- Improve evaluation comparability and statistical rigor
  - Re-run key open-source baselines in the same environment, tools, prompts, and judge to ensure apples-to-apples comparison (Section 4.1: Baselines).
  - Specify the training LLM judge and ensure consistency with the evaluation judge; report sensitivity to different judges (Section 3.3; Section 4.1; Appendix D).
  - Add confidence intervals or statistical tests for Tables 1–3 and Figures 4–5 (Sections 4.2–4.4).
  - Benchmark on a standardized tool suite (Appendix C) across systems, or report how tool differences may influence outcomes (Section 4.1: Baselines).- Expand dataset reporting and release practices
  - Provide quantitative statistics: distribution of entity types, average sources per question, question length, average required turns, topic diversity (Section 2; Appendix E; Section 4.1).
  - Publish a clear release plan with documentation and scripts for reverse construction, subject to the ethical constraints (Appendix B), to improve reproducibility.
  - Report distributions of “moderate visibility” entities (Section 2: Entity-driven Information Collection) and discuss how this choice impacts generalization.
  - Quantify filtering pass rates and detail verification procedures; include inter-annotator agreement when applicable (Section 2: Multi-stage Filtering).- Provide empirical evidence on the impact of omitting distant tool responses
  - Run controlled studies quantifying when older tool outputs are needed; measure how omissions affect success rates across task types (Section 3.1).
  - Track re-fetch frequency and latency introduced by the omission token policy, reporting costs and performance deltas (Section 3.1; Appendix C).
  - Compare sliding-window against end-to-end trained summarization within RL (not external-only) to test if learned compression can match or exceed performance (Section 4.3).
  - Add error analysis categorizing failure modes attributable to omissions vs. other factors (Sections 4.2–4.4).- Report efficiency and compute costs more fully
  - Provide memory and latency measurements for training and inference under different context management strategies (Section 4.3).
  - Include compute budget details (hardware, tokens processed, hours) for SFT and RL phases (Section 4.1).
  - Quantify wall-clock tradeoffs of the sliding window (e.g., additional fetch/find calls due to omissions) (Section 3.1; Appendix C).
  - Clarify how “nearly 100 turns” at 32k (Figure 2; Abstract; Introduction) relates to training constraints (40k, 60 turns) and the operational costs.- Clarify reward design and test robustness
  - Specify the training LLM judge model and prompt, and evaluate training sensitivity to judge choice (Section 3.3; Appendix D).
  - Explore richer reward shaping (e.g., intermediate verifications and partial credit) to mitigate sparsity (Section 3.3).
  - Test for reward hacking/adversarial behaviors and report defense mechanisms (Section 3.3).
  - Align training and evaluation judges/prompts or document differences and their measured impacts (Sections 3.3, 4.1; Appendix D).Score
- Overall (10): 7 — Substantial gains across multiple deep-research benchmarks (Table 1) and strong context-efficiency evidence (Figure 2; Table 2), but limited ablations and evaluation comparability reduce confidence (Sections 2, 3.1, 4.1–4.4).
- Novelty (10): 7 — The dynamic sliding-window with assistant-preserving training-testing consistency (Section 3.1; Figure 3) and reverse-constructed multi-source obfuscated QA pipeline (Section 2; Figure 1) are meaningfully novel in combination.
- Technical Quality (10): 6 — Sound algorithmic framing (Section 3.3; advantage equation; mask in Section 3.1) and empirical wins, tempered by missing ablations, sparse reward design details, and limited quantitative evidence for key claims (Sections 3.1–4.4).
- Clarity (10): 7 — Clear method exposition and figures (Figures 1–5), with reproducible settings (Section 4.1), though some training/judging specifics are under-specified (Section 3.3; Appendix D).
- Confidence (5): 4 — High confidence based on the thorough manuscript and reported results (Tables 1–3; Figures 2–5), moderated by the noted gaps in ablations and evaluation comparability (Sections 2–4).