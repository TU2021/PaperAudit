# Global Summary
The paper proposes DeepMiner, a framework to train long-horizon, multi-turn web search agents by combining high-difficulty training tasks with a dynamic context window. It addresses two core challenges: insufficient task complexity in existing datasets and context explosion in multi-turn interactions. The method constructs complex QA pairs via a reverse pipeline grounded in authentic web sources and applies a sliding-window mechanism that compresses distant tool outputs while preserving assistant reasoning traces, avoiding external summarization.

DeepMiner is implemented on Qwen3-32B via a two-stage process: supervised fine-tuning (SFT) for cold start and reinforcement learning (RL) with Group Relative Policy Optimization (GRPO) under dynamic context. Evaluations cover BrowseComp (en/zh), XBench-DeepSearch, and GAIA with up to 100 interaction turns, using ChatGPT-4o-Latest as judge. DeepMiner-32B achieves “33.5% accuracy on BrowseComp-en,” outperforming prior open-source agents by “almost 20 percentage points,” and shows consistent gains on BrowseComp-zh (40.1), XBench-DeepSearch (62.0), and GAIA (58.7). The dynamic context management reportedly enables “nearly 100 turns within standard 32k context length,” and in conclusion they also state “exceed 100 turns within 40k contexts.”

Key quantitative details:
- Training: SFT on ~3,000 trajectories (batch 256, lr 1e-5); RL on ~4,000 questions (batch 32, lr 2e-6), 8 rollouts per question; max trajectory length 40k tokens; turn limit 60; tool window size 5, slide step 3.
- Main results (DeepMiner-32B-RL): BrowseComp-en 33.5; BrowseComp-zh 40.1; XBench-DeepSearch 62.0; GAIA 58.7. SFT baseline: 21.2; 28.0; 53.0; 54.4.
- RL improvements over SFT: +12.3 pp (BrowseComp-en), +12.1 (BrowseComp-zh), +9.0 (XBench-DS), +4.3 (GAIA).
- Context efficiency (GPT-OSS-120B, BrowseComp): DeepMiner sliding window 33.3% at 32k, 64k, and 128k; vanilla 9.0/23.7/30.3; summary compression 10.0/25.3/31.6.
- Training dynamics: reward increases from 0.45 to 0.60; BrowseComp performance rises from 22% to 33.5%; trajectory length grows within a 40k limit.
- Data efficiency (SFT only): DeepMiner vs HotpotQA on BrowseComp: 21.2 vs 15.6; BrowseComp-zh: 28.0 vs 21.8; XBench: 53.0 vs 47.0; GAIA: 54.4 vs 51.4.

Caveats explicitly stated: reliance on LLM judges for rewards and evaluation; dataset release will be under strict access controls with anonymization; some operational details (e.g., compute budgets) are not specified; different sections cite both 32k and 40k context for 100+ turns.

# Abstract
- Problem: Multi-turn agents with long-horizon interactions struggle to exhibit deep reasoning. Existing approaches underperform compared to proprietary systems.
- Approach: DeepMiner introduces high-difficulty training tasks via reverse construction from authentic web sources and a dynamic context management strategy (sliding window) that preserves assistant reasoning and compresses distant tool outputs without external summarization.
- Implementation: Reinforcement learning on Qwen3-32B yields DeepMiner-32B.
- Results: “DeepMiner attains 33.5% accuracy on BrowseComp-en,” surpassing the best open-source agent by “almost 20 percentage points,” with consistent improvements on BrowseComp-zh, XBench-DeepSearch, and GAIA.
- Context capability: Dynamic context enables “sustained interactions of nearly 100 turns within standard 32k context length.”

# Introduction
- Context: RLVR has shown cognitive behaviors in single-turn tasks (self-verification, backtracking, subgoal decomposition). Extending these to multi-turn, long-horizon tasks remains challenging, with open-source efforts showing gaps vs proprietary systems.
- Two identified challenges:
  - Insufficient Task Complexity: Datasets like TriviaQA, 2WikiMultihopQA, HotpotQA rely on Wikipedia and permit shallow retrieval-based success, failing to elicit verification/backtracking/planning.
  - Context Management Limitations: Long-horizon interactions rapidly consume context due to lengthy tool outputs; “typical 32k context length” supports “approximately 10–15” turns. Summarization-based compression loses fine-grained detail, adds complexity, and cannot be integrated into end-to-end RLVR.
- Contributions:
  - Reverse-construction of complex QA across multiple authentic sources with rigorous filtering and obfuscation to demand cross-document synthesis.
  - Dynamic sliding window: selectively compresses distant tool responses, preserves assistant reasoning traces, maintains access to original webpage content, avoids information loss and optimization blind spots.
- Implementation and results: On Qwen3-32B, DeepMiner-32B achieves “33.5% accuracy on BrowseComp-en,” with consistent improvements elsewhere; enables “100 tool calls within standard 32k context length.”

# Method
- 2 COMPLEX QUESTION CONSTRUCTION:
  - Entity-driven Information Collection: Select Wikipedia entities with “1,000–10,000 page views over the last six months.” Gather pages via Google Search. Three-stage filtering: entity correspondence verification (right individual), information complementarity (unique content), credibility validation (trustworthy sources).
  - Question Generation: Use multiple sources (excluding Wikipedia) and constrain synthesis from “at least four distinct sources.” Provide demonstrations of complex patterns. Apply secondary obfuscation (replace specifics with generic descriptors) while maintaining unique solvable answers.
  - Multi-stage Filtering:
    - Difficulty filtering: use search engine queries and zero-shot LLM prompting; remove questions solvable by either, enforcing tool-assisted multi-step exploration.
    - Quality filtering: remove ambiguous expressions, ambiguous answers, and non-derivable answers; ensure verifiable signal.
  - Outputs: Challenging QA pairs requiring extended reasoning and strategic planning; examples in Appendix E.

- 3 REINFORCEMENT LEARNING WITH DYNAMIC CONTEXT WINDOW:
  - 3.1 Dynamic Context Management Strategy:
    - Empirical analysis (BrowseComp, GPT-OSS-120B):
      - Incorrect trajectories vs context length: 16k ~2; 32k ~5; 48k ~8; 64k ~10; 80k ~12; 96k ~15; 128k ~65 (majority failures at max length).
      - Context growth without vs with sliding window (trajectory vs assistant context):
        - Turns 1: trajectory ~1k; sliding ~1k; assistant ~0k
        - 2: ~4k; ~2k; ~1k
        - 4: ~10k; ~4k; ~2k
        - 8: ~18k; ~8k; ~4k
        - 16: ~32k; ~14k; ~6k
        - 32: ~32k; ~14k; ~10k
        - 64: ~32k; ~14k; ~16k
        - 128: ~32k; ~14k; ~20k
      - Observation: tool responses are 5–10× longer than assistant outputs; mostly locally influential; sliding window allows nearly 100 turns within 32k.
    - Sliding Window Mechanism:
      - Trajectory τ = {q, a1, t1, a2, t2, …, aT−1, tT−1, aT}; window size W; slide step S.
      - When |Rt| reaches W, boundary b = max(1, t − W + S); replace {t1, …, tb−1} with placeholder φ = “[Previous tool output skipped. Rerun tool if needed].”; retain {tb, …, tk}; assistant outputs remain intact.
    - Training-Testing Consistency:
      - Decompose each trajectory into K = ⌊(T − W)/S⌋ + 1 sequences reflecting inference states.
      - Masking ensures each assistant response is trained exactly once:
        - Mi(k) = {0 if i < W + (k − 2)·S + 2; 1 otherwise}.
  - 3.2 Cold Start:
    - SFT generates action trajectories via powerful LLMs, using sliding window during generation.
    - Filter incorrect or overly long trajectories; train with multi-sequence construction for consistency.
  - 3.3 Reinforcement Learning Training:
    - Algorithm: GRPO; trajectory-level reward with sequence-level training.
    - For each question, generate G complete trajectories under sliding window; advantage:
      - Âi = (Ri − mean({Rj}))/std({Rj}).
    - Propagate trajectory-level advantage to all derived sequences.
    - Reward Design: binary (1 if final answer matches ground truth; 0 otherwise), judged by an LLM.

# Experiments
- 4.1 Experimental Setups:
  - Tools:
    - web_search: top-10 results (titles, URLs, snippets).
    - fetch: webpage content in Markdown with pagination.
    - find: in-page search for keywords.
  - Training Details:
    - Base: Qwen3-32B (thinking mode).
    - SFT: ~3,000 trajectories; batch 256; lr 1e-5.
    - RL: ~4,000 questions; batch 32; lr 2e-6; 8 rollouts per question; max trajectory length 40k tokens; turn limit 60.
    - Context management: tool window size 5; sliding size 3.
    - Framework: VERL.
  - Evaluation:
    - Benchmarks: BrowseComp (Wei et al., 2025), BrowseComp-zh, XBench-DeepSearch, GAIA.
    - Config: temperature 0.6; top-p 0.9; up to 100 interaction turns.
    - Context: sliding window size 5; slide step 3.
    - Judge: ChatGPT-4o-Latest (prompts in Appendix D).
  - Baselines: Commercial agents (OpenAI DeepResearch, Metaso DeepResearch, Kimi Researcher), general LLMs with tools (OpenAI-o3, Claude-4-Sonnet, DeepSeek-R1, Kimi-K2-Instruct-1T, GLM-4.5-355B, DeepSeek-V3.1-671B), open-source agents (WebShaper-72B, ASearcher-32B, WebDancer-32B, WebSailor-72B, DeepDive-32B, WebExplorer-8B). Official results used.

- 4.2 Main Results:
  - Table 1 (accuracy, %):
    - Commercial: OpenAI DeepResearch 51.5 (BC-en), 42.9 (BC-zh), - (XBench-DS), 67.4 (GAIA); Metaso DeepResearch 12.0, 45.4, 64.0, -; Kimi Researcher -, -, -, 69.0.
    - General LLMs w/ tools: OpenAI-o3 49.7, 58.1, 66.7, 70.5; Claude-4-Sonnet 12.2, 29.1, 64.6, 68.3; DeepSeek-R1 8.9, 35.7, 55.0, -; Kimi-K2-Instruct-1T 14.1, 28.8, 50.0, 57.7; GLM-4.5-355B 26.4, 37.5, 70.0, 66.0; DeepSeek-V3.1-671B 30.0, 49.2, 71.2, 63.1.
    - Open-source agents: WebShaper-72B -, -, -, 60.0; ASearcher-32B -, -, 42.1, 52.8; WebDancer-32B 3.8, 18.0, 39.0, 51.5; WebSailor-72B 12.0, 30.1, 55.0, 55.4; DeepDive-32B 14.8, 25.6, 50.5, -; WebExplorer-8B 15.6, 32.0, 53.7, 50.0.
    - Our agents: DeepMiner-32B-SFT 21.2, 28.0, 53.0, 54.4; DeepMiner-32B-RL 33.5, 40.1, 62.0, 58.7.
  - Claims: DeepMiner-32B achieves “33.5%” on BrowseComp-en, exceeding prior open-source agents and even DeepSeek-V3.1-671B (671B) despite being smaller. RL improves over SFT by +12.3 (BC-en), +12.1 (BC-zh), +9.0 (XBench-DS), +4.3 (GAIA).

- 4.3 Context Management Efficiency Analysis:
  - Table 2 (GPT-OSS-120B, BrowseComp; accuracy %):
    - Strategy: Vanilla (no management): 32k 9.0; 64k 23.7; 128k 30.3.
    - Summary (external summarization): 32k 10.0; 64k 25.3; 128k 31.6.
    - DeepMiner (sliding window): 32k 33.3; 64k 33.3; 128k 33.3.
  - Claim: DeepMiner achieves “33.3% accuracy” using only 32k context, exceeding alternatives that require 128k.

- 4.4 Detailed Analysis:
  - Training Dynamics:
    - Trajectory length increases steadily while remaining within 40k limit.
    - Rewards grow from “0.45 to 0.60.”
    - BrowseComp performance increases from “22% to 33.5%.”
  - Context Scaling (BrowseComp):
    - Tool-call budget: performance surpasses DeepSeek-V3.1-671B (~30.0) at ~60 calls; at 100 calls reaches “33.5.”
    - Context length: performance approaches “33.0” at “32k” context; sustains close to 100 rounds.
  - Data Efficiency (SFT only; Table 3, accuracy %):
    - HotpotQA: BC 15.6; BCZH 21.8; Xbench 47.0; GAIA 51.4.
    - DeepMiner: BC 21.2; BCZH 28.0; Xbench 53.0; GAIA 54.4.

# Related Work
- Reinforcement Learning with Verifiable Rewards (RLVR): Now standard for training LLMs; GRPO cited; applying RLVR to web agents faces sparse rewards and context limitations. Some works emphasize new mechanisms or rewards; this work instead uses a dynamic context window to replace summarization-based context management frameworks (e.g., ReAct context handling), enabling efficient long-horizon RL still compatible with RLVR.
- Deep Research Agents: Proprietary systems (OpenAI DeepResearch, Google Gemini Deep Research, Claude Research) show expert-level capabilities but lack reproducibility. Open-source progress includes data synthesis and context management (e.g., summarization). The paper states a large gap persists (open-source models “less than 20% accuracy” on BrowseComp), and DeepMiner narrows this via reverse-constructed tasks and dynamic context window.

# Conclusion
- Summary: DeepMiner trains deep research agents capable of sustained long-horizon interactions with cognitive behaviors by addressing training task complexity and context explosion. It reverse-constructs verifiable QA pairs from authentic sources and manages context dynamically without external summarization.
- Key mechanism: Preserve assistant reasoning traces; compress distant tool outputs; maintain access to original page content.
- Capability: “exceed 100 turns within 40k contexts” (also earlier: “nearly 100 turns within standard 32k context length”).
- Results: DeepMiner-32B achieves “33.5%” on BrowseComp, nearly doubling prior open-source SOTA.

# References
- Benchmarks: BrowseComp (Wei et al., 2025), BrowseComp-ZH (Zhou et al., 2025), XBench-DeepSearch (Chen et al., 2025), GAIA (Mialon et al., 2023).
- Models/Algorithms: Qwen3-32B (Yang et al., 2025), GRPO (Shao et al., 2024), ReAct (Yao et al., 2023).
- Related agents/systems cited include OpenAI DeepResearch, Metaso, Kimi K2/Researcher, GLM-4.5, DeepSeek R1/V3.1 and several open-source web agents (WebShaper, ASearcher, WebDancer, WebSailor, WebExplorer, DeepDive).
- Other datasets: HotpotQA (Yang et al., 2018); multi-hop datasets referenced for comparison.

# Appendix
- A: Use of LLMs to aid and polish writing.
- B: Ethical considerations: data from public webpages (e.g., Wikipedia); filter irregular websites/social media; dataset release under strict academic access controls; anonymization by replacing names and identifiers; review process for model weight access.
- C: Enhanced Tool Suite:
  - Web Search (titles, URLs, snippets).
  - Fetch with paginated browsing (Markdown), avoiding truncation/summarization and preserving complete information.
  - Find for in-page keyword search and context navigation.
- D: Templates: Evaluation (judgement) template specifying extraction of final answer, correctness criteria, and reasoning guidelines; used ChatGPT-4o-Latest (explicit in Experiments).
- E: Question examples (3 provided with entities and answers) illustrating complex, obfuscated synthesis requirements.
- E.1 Case Study: A BrowseComp trajectory identifies “Ahsan Manzil”:
  - Location: Dhaka (capital of Bangladesh), beside Buriganga River.
  - Construction: started “1859,” completed “1872.”
  - Wall thickness: “0.78 m” (within 0.5–0.9 m range).
  - Tornado damage: “April 7, 1888.”
  - Earthquake damage: “June 12, 1897.”
  - Acquisition: “1985” by the government.
  - President at acquisition: Hussain Muhammad Ershad, born “1930.”
- Not specified in this section: precise dataset release timeline, hardware/compute budgets, and number of RL training runs beyond the rollout count.