### Summary

This paper proposes **DeepMiner**, a training framework for long-horizon, multi-turn **search agents** that aims to overcome “turn limits” under a fixed context length by combining:

1. **Reverse-constructed, web-grounded hard QA trajectories** (multi-source, verifiable questions built from authentic web sources, with filtering for difficulty/credibility), and
2. **Dynamic context management** via a **sliding window** that keeps the assistant’s reasoning trace while dropping older tool outputs (replacing them with a placeholder and allowing re-invocation), designed to maintain training–inference consistency.

They apply the framework to **Qwen3-32B** with SFT + RL (sequence/trajectory-level advantage style), and report large improvements on deep-search benchmarks (notably BrowseComp-en), claiming nearly ~100 turns within a 32k context budget.

### Strengths

* **Targets a real bottleneck for deep-search agents:** Context growth from tool outputs is a practical pain point; a windowed strategy that preserves reasoning tokens addresses a concrete failure mode (tool outputs crowd out reasoning).
* **Two-pronged approach (data + context) with credible motivation:** Reviewers generally agree that both (i) harder, verifiable web QA and (ii) context efficiency are important for long-horizon agent behavior.
* **Strong reported benchmark gains:** Reviewers (esp. iDV8 / 2tnR) consider the BrowseComp improvements substantial and practically meaningful, with broader gains across GAIA/XBench/BrowseComp-zh.
* **Rebuttal significantly strengthens reproducibility and attribution:** During rebuttal, authors added detailed prompts/pipeline description, released a dataset subset, and—crucially—ran **direct ablations** separating **dynamic window vs truncation** under the same data/training setup, which addressed a central weakness raised by multiple reviewers.
* **Empirical support that “dropping old tool outputs” may be safe in practice:** Rebuttal includes perturbation/corruption tests and failure analysis suggesting minimal dependence on very old tool outputs, reducing concern about catastrophic long-range failures.

### Weaknesses

* **Initial lack of ablations and baseline coverage (partly fixed in rebuttal):** Early reviews criticized missing comparisons (e.g., Search-R1, related RL/synthetic-data works) and missing hyperparameter/window sensitivity; many of these were answered with added citations, tables, and ablations during rebuttal.
* **Credit assignment / reward design concerns:** Some reviewers questioned using a **trajectory-level binary reward** and propagating it through windowed subsequences (risk of mis-crediting steps). Authors argue non-overlapping loss masks prevent over-crediting, but the learning signal remains coarse and might limit policy shaping.
* **“Discard vs summarize” debate:** Reviewers worried that fully dropping old tool outputs is an all-or-nothing choice. Authors defend it as relying on model-driven selective summarization + re-invocation and note summarization can reduce effective turns; still, this design may fail on tasks needing precise early evidence comparison.
* **Attribution between data quality and context mechanism remains a key interpretability axis:** The rebuttal’s controlled ablation (DeepMiner data + truncation vs + sliding window) helps, but a complete decomposition across multiple datasets/models remains limited by cost and data availability from concurrent works.
* **Compute / cost constraints visible in training dynamics:** Reviewers noted reward/performance fluctuations and asked for longer training; authors explain prohibitive real-web search cost and provide extended curves/checkpoint selection, but stability/convergence remains somewhat under-explored.
