Academic Integrity and Consistency Risk Report

Summary of high-impact, evidence-based issues identified in the manuscript. Each item is explicitly anchored to the paper’s own sections, tables, figures, or footnotes.

1) AUROC vs. Accuracy-at-1%-FPR numerical inconsistency (critical)
- Evidence: Table 2 (Section: Experiments, Block #19).
- Observation: For RoBERTa, AUROC is reported as 100.0 across all domains and generators (e.g., ChatGPT row: Wikipedia, Reddit, WikiHow, arXiv AUROC all 100.0), yet the Accuracy at 1% FPR is very low (e.g., 50.0, 50.0, 75.3, 60.9; Avg. 59.1).
- Why this is problematic: An AUROC of 100.0 implies perfect separability of classes across thresholds; at a fixed 1% FPR, one can choose a threshold that achieves near-perfect TPR, yielding near-perfect accuracy. Reporting AUROC=100.0 together with accuracy values as low as ~50% (which implies near-zero TPR at the chosen operating point) is internally contradictory. This discrepancy recurs for GPT-4 and Dolly-v2 rows where AUROC is 100.0 but accuracy values are far from perfect.
- Impact: Calls into question the correctness of evaluation metrics and/or thresholding procedure, undermining trust in comparative claims.

2) Contradiction about α (span selection weight) and its impact on performance (critical)
- Evidence:
  - Equation (5) (Section 2.2, Block #11): S(T) = mean over spans of α·L^{std} + (1−α)·R^{std}. Thus, larger α emphasizes span length L; smaller α emphasizes reliability R.
  - Text claim (Section 4.2, Block #22): “we observe that the higher the α, the higher the detection performance. This implies that taking the reliability score more into account … can improve detection performance.”
  - Figure 5 caption (Section 4.2, Block #24) and plots (Block #27): Show performance decreasing as α increases.
  - Appendix numeric tables (Figure 10, Block #45; Figure 47, Block #47): AUROC and Acc@1% FPR consistently drop as α increases from 0 to 1 for multiple domains/generators.
- Observation: The manuscript text states that higher α improves performance and that this means weighting reliability more, but both the formula and the figures show the opposite: higher α weights length (not reliability), and performance declines as α increases.
- Impact: Logical contradiction between method definition and analysis undermines the interpretability and correctness of the claimed design insight.

3) Misstatement of scope in α-impact description vs. figure scope (major)
- Evidence:
  - Section 4.2 text (Block #22): “Figure 5 depicts the relationship between α and the detection performance … across four domains and three generators…”
  - Figure 5 caption (Block #24): “across four domains using ChatGPT as a generator.”
- Observation: The text describes three generators, while the figure reports only ChatGPT. The broader conclusion is therefore not supported by the referenced figure.
- Impact: Overgeneralization reduces confidence in the stated robustness across generators.

4) Mismatch between stated minima and reported results for α-impact (major)
- Evidence:
  - Section 4.2 text (Block #22): “the lowest performance of AUROC and accuracy at 1% FPR are 98.5% and 93.4%.”
  - Appendix tables for GPT-4 and Dolly-v2 (Figure 10, Block #45; Figure 47, Block #47): show minima well below these values (e.g., Dolly-v2 AUROC down to ~83 and Acc@1% FPR down to ~76; GPT-4 Acc@1% FPR down to ~91).
- Observation: The stated minima are inconsistent with the appendix’s reported values for other generators.
- Impact: Inaccurate summary statistics may mislead readers about the detector’s robustness.

5) Boldface annotation inconsistent with stated rule (minor-to-major depending on intent)
- Evidence: Table 2 note (Block #19): “Bold indicates the best performance within each column for each combination of domains and generators.”
- Observation: In the ChatGPT row, Avg. AUROC is 100.0 for RoBERTa but is not bolded, while ExaGPT’s Avg. AUROC is 99.2. Similar omissions occur elsewhere whenever RoBERTa has 100.0 AUROC. This contradicts the stated bolding rule.
- Impact: Raises concerns about presentation consistency and potential bias in highlighting results.

6) Claims of code/data release lack verifiable pointers (major for reproducibility)
- Evidence:
  - Ethics section (Block #34): “we release our code and data to the public, including all human annotations.”
  - Footnotes linking to external tools/frameworks (Section 3.1 footnote 8, Block #15; Appendix B footnote 12, Block #42) point to the Streamlit framework repository, not to the authors’ implementation or dataset/human annotation artifacts.
- Observation: No direct link to the project code, data, or annotations is provided in the manuscript.
- Impact: Reproducibility and transparency claims cannot be verified.
- Note: No direct evidence found in the manuscript of the actual code/data/annotation release location.

7) Reliability score interpretation may be misleading (minor)
- Evidence: Section 2.1 (Block #10): R(x) is defined as the mean similarity between the target span and its k retrieved neighbors; k is fixed to 10. Section 2.1 text: “The reliability score R indicates how many similar spans exist in the datastore.”
- Observation: R is a mean similarity, not a count. With a fixed k, R does not directly quantify “how many” similar spans exist; it reflects how similar the nearest spans are. While correlated, the wording is inaccurate.
- Impact: Could mislead readers about what is being measured; however, this does not invalidate the core method.

8) Scope of “higher α means more reliability” (duplicate of Item 2 but phrased differently; ensuring clarity)
- Evidence: Equation (5) vs. Figure 2 text (Block #8) and Section 4.2 (Block #22).
- Observation: The manuscript equivocates on whether α weights reliability or length. Equation (5) unambiguously assigns α to length; the narrative asserts the opposite.
- Impact: Confusion about the optimization objective compromises interpretability claims.

Constructive recommendations
- Recompute or re-verify AUROC and Acc@1% FPR for all detectors; ensure internal consistency and correct thresholding procedure. If AUROC values are rounded, avoid rounding to 100.0 in ways that imply perfect separability; provide confidence intervals or more precise decimals.
- Correct Section 4.2 to align with Equation (5) and all figures/tables. If performance improves when weighting reliability more, state that lower α improves performance and show that in the plots.
- Align the scope claims: if a figure is for ChatGPT only, do not generalize to all generators in the associated text; cite appendix figures for other generators.
- Fix boldface formatting to strictly follow the stated rule in Table 2 or adjust the rule and explain any exceptions.
- Provide explicit, working links to the released code, data, and human annotations (or remove the claim if not yet available). Distinguish between external tool links (e.g., Streamlit, SHAP, GLTR) and your own artifacts.
- Clarify the interpretation of the reliability score R to avoid conflating similarity magnitude with the number of similar spans.

Conclusion
The manuscript contains several clear, high-impact internal inconsistencies and numerical mismatches (especially Items 1–4), which materially affect the credibility of the reported results and the clarity of the methodological analysis. Resolving these issues is essential for scientific validity and reader trust.