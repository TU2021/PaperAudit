1) Summary
This paper introduces ExaGPT, an example-based detector for machine-generated text designed for high interpretability. The method is motivated by the human process of verifying a text's origin by comparing it to known examples. ExaGPT classifies a text by segmenting it into spans and determining whether these spans share more similarity with human-written or LLM-generated texts from a reference datastore. The core contributions are the novel detection method that provides retrieved similar spans as evidence, a human evaluation demonstrating that this evidence is more effective for judging the detector's correctness than existing interpretable methods, and extensive experiments showing that ExaGPT significantly outperforms strong baselines, particularly at a low false positive rate.2) Strengths
*   **Novel and Well-Motivated Method**
    *   The core idea of grounding the detection process in example-based retrieval is novel for this task and is well-motivated by aligning with the intuitive human decision-making process (Section 1, Figure 1). This provides a strong conceptual foundation for the work.
    *   The technical implementation is sound, combining k-NN search for span scoring with a dynamic programming algorithm for optimal text segmentation (Section 2.2, Algorithm 1). This optimization, which balances span length and retrieval reliability (Equation 5), is a non-trivial and elegant solution to generating interpretable output.
    *   The method's output, a set of selected spans each paired with its nearest neighbors from the datastore (Equation 7, Figure 3), provides rich, concrete evidence that is qualitatively different from the feature-attribution or probability-based evidence of prior work.*   **Rigorous and Comprehensive Experimental Evaluation**
    *   The experiments are conducted on a large-scale, standard benchmark (M4 dataset), covering four diverse domains and three different language models, including both closed and open-source generators (Section 3.1). This demonstrates the method's effectiveness across various conditions.
    *   The choice of evaluation metrics, including both AUROC and Accuracy at 1% False Positive Rate (FPR), is well-justified and highly relevant for practical applications where penalizing human authors is a major concern (Section 3.1).
    *   The results are exceptionally strong. ExaGPT consistently outperforms all baselines on the Acc@1%FPR metric, often by very large margins (up to +40.9 points), showcasing its practical utility (Table 2).*   **Direct and Effective Evaluation of Interpretability**
    *   A major strength is the direct evaluation of interpretability via a human study. Instead of merely claiming the method is interpretable, the authors design an experiment to measure how well users can judge the correctness of the detector's predictions based on the provided evidence (Section 3.1).
    *   The results of this study provide clear quantitative support for the paper's central claim, showing that ExaGPT's evidence leads to higher human judgment accuracy (+13.6 points over the next best baseline) (Table 1).
    *   The paper further analyzes *why* the method is interpretable, correlating the reliability score of retrieved spans with the correctness of ExaGPT's predictions, which provides insight into the mechanism behind the human evaluation results (Section 4.1, Figure 4).*   **Thorough Ablation and Robustness Analysis**
    *   The authors conduct several analyses that strengthen the paper's conclusions and demonstrate the method's robustness.
    *   The analysis of the hyperparameter `α` shows that the model's performance is not overly sensitive to this choice, with performance remaining high across the full range of values (Section 4.2, Figure 5, Figure 10).
    *   The study on the impact of datastore size reveals that ExaGPT maintains strong performance even with a significantly smaller datastore (500 pairs), which is an important practical consideration for deployment (Section 4.3, Figure 6, Figure 11).3) Weaknesses
*   **Significant Computational Cost and Scalability Concerns**
    *   The proposed method relies on performing a k-NN search for every possible n-gram (up to n=20) in a target text against a large datastore of pre-computed embeddings. This is computationally intensive.
    *   The authors acknowledge the high inference cost, noting that experiments required four NVIDIA A6000 GPUs (Section 7, Section D). This cost could be a major barrier to real-world adoption, especially when compared to much lighter baseline models like LR-GLTR.
    *   While the analysis on datastore size shows robustness (Section 4.3), it does not fully mitigate the fundamental challenge of performing a massive number of nearest-neighbor searches for each document to be classified.*   **Insufficient Justification for Key Design Choices**
    *   The method for creating span embeddings—taking the mean of the *second-layer* hidden outputs from BERT-large—is a critical and non-standard design choice. The only justification is a brief note in a footnote stating it was chosen in a pilot study for balancing lexical and semantic similarity (Footnote 5, page 4).
    *   The impact of this specific layer choice on both detection performance and the quality of retrieved examples for interpretability is not quantified. Without an ablation study comparing it to more common embedding strategies (e.g., using the last layer or averaging multiple top layers), the optimality and robustness of this choice are unclear.
    *   The range of n-grams considered (1 to 20) is fixed (Section 3.1). The sensitivity of the model to this maximum n-gram length is not explored, yet it directly impacts both computational cost and the types of linguistic patterns the model can capture.*   **Limited Generalizability of the Human Evaluation**
    *   The human evaluation, while a core strength, was conducted with a small number of participants (four annotators) who are all experts in natural language processing (one MSc student, one PhD student, two researchers) (Section 3.1).
    *   The findings on interpretability may not generalize to the primary target users of such detectors, such as educators or content moderators, who typically lack this specialized background. The authors acknowledge this as a limitation (Section 7).
    *   The total number of samples evaluated per detector (96) is also relatively small (Footnote 6, page 5), which could limit the statistical power of the conclusions drawn from the human study.4) Suggestions for Improvement
*   **Address Scalability with Cost Analysis and Mitigation Strategies**
    *   To address the computational cost, the authors should provide a more detailed analysis of inference time, perhaps reporting the average time to classify a document and comparing it directly to the baselines.
    *   The paper would be strengthened by a discussion of potential cost-reduction strategies. This could include exploring the use of more efficient Approximate Nearest Neighbor (ANN) libraries, vector quantization, or model distillation to create smaller embeddings, thereby making the approach more practical.
    *   A brief discussion on the trade-off between the number of n-grams searched (e.g., by striding or only considering certain lengths) and performance/cost would also be valuable.*   **Strengthen Justification for Design Choices with Ablations**
    *   To justify the choice of using the second BERT layer for embeddings, the authors should add an ablation study in the appendix. This study should compare the performance (both Acc@1%FPR and human evaluation scores, if feasible) of using the second layer versus other common strategies like the final layer's CLS token, the mean of the final layer's token embeddings, or the mean of the top few layers.
    *   Similarly, an ablation on the maximum n-gram length (e.g., comparing N=10, N=20, N=30) would provide valuable insight into its impact on performance and computational load, further strengthening the technical contribution.*   **Frame Human Evaluation Results and Discuss Broader Implications**
    *   To address the limited scope of the human study, the authors should more explicitly frame the current results as a controlled study with expert users.
    *   They should expand the discussion in the limitations or future work sections to hypothesize how the results might differ with non-expert users. For instance, they could discuss potential challenges like cognitive overload from the evidence or the difficulty non-experts might face in assessing semantic similarity.
    *   Proposing a future study with a larger, more diverse participant pool would be a concrete way to acknowledge this limitation while outlining a path forward.5) Score
- Overall (10): 9 — The paper presents a highly novel, effective, and well-evaluated method that makes a significant contribution to interpretable LLM text detection, supported by strong results in both performance (Table 2) and a human study (Table 1).
- Novelty (10): 9 — The example-based retrieval framework combined with dynamic programming for span selection is a fresh and compelling approach in this domain (Section 2).
- Technical Quality (10): 9 — The methodology is sound and the experiments are rigorous, comprehensive, and include insightful analyses (Section 3, Section 4), though some design choices could be better justified.
- Clarity (10): 10 — The paper is exceptionally well-written and organized, with clear explanations and effective visualizations (Figure 1, Figure 2, Figure 3) that make complex ideas easy to understand.
- Confidence (5): 5 — I am highly confident in my evaluation, as I am familiar with the relevant literature and the paper provides sufficient detail to assess its contributions and limitations.