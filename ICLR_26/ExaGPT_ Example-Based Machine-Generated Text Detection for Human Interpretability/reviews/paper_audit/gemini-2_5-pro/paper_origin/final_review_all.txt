1) Summary
This paper introduces ExaGPT, an example-based detector for machine-generated text designed for high interpretability. The method is motivated by the human process of verifying a text's origin by comparing it to known examples. ExaGPT classifies a text by segmenting it into spans and determining whether these spans share more similarity with human-written or LLM-generated texts from a reference datastore. The core contributions are the novel detection method that provides retrieved similar spans as evidence, a human evaluation demonstrating that this evidence is more effective for judging the detector's correctness than existing interpretable methods, and extensive experiments showing that ExaGPT significantly outperforms strong baselines, particularly at a low false positive rate.2) Strengths
*   **Novel and Well-Motivated Method**
    *   The core idea of grounding the detection process in example-based retrieval is novel for this task and is well-motivated by aligning with the intuitive human decision-making process (Section 1, Figure 1). This provides a strong conceptual foundation for the work.
    *   The technical implementation is sound, combining k-NN search for span scoring with a dynamic programming algorithm for optimal text segmentation (Section 2.2, Algorithm 1). This optimization, which balances span length and retrieval reliability (Equation 5), is a non-trivial and elegant solution to generating interpretable output.
    *   The method's output, a set of selected spans each paired with its nearest neighbors from the datastore (Equation 7, Figure 3), provides rich, concrete evidence that is qualitatively different from the feature-attribution or probability-based evidence of prior work.*   **Comprehensive Experimental Setup**
    *   The experiments are conducted on a large-scale, standard benchmark (M4 dataset), covering four diverse domains and three different language models, including both closed and open-source generators (Section 3.1). This demonstrates the method's effectiveness across various conditions.
    *   The choice of evaluation metrics, including both AUROC and Accuracy at 1% False Positive Rate (FPR), is well-justified and highly relevant for practical applications where penalizing human authors is a major concern (Section 3.1).
    *   The results show that ExaGPT consistently outperforms non-classifier baselines (LR-GLTR, DNA-GPT) on the Acc@1%FPR metric, often by large margins, showcasing its practical utility (Table 2).*   **Direct and Effective Evaluation of Interpretability**
    *   A major strength is the direct evaluation of interpretability via a human study. Instead of merely claiming the method is interpretable, the authors design an experiment to measure how well users can judge the correctness of the detector's predictions based on the provided evidence (Section 3.1).
    *   The results of this study provide clear quantitative support for the paper's central claim, showing that ExaGPT's evidence leads to higher human judgment accuracy (+13.6 points over the worst baseline) (Table 1).
    *   The paper further analyzes *why* the method is interpretable, correlating the reliability score of retrieved spans with the correctness of ExaGPT's predictions, which provides insight into the mechanism behind the human evaluation results (Section 4.1, Figure 4).*   **Thorough Robustness Analysis**
    *   The authors conduct several analyses that strengthen the paper's conclusions and demonstrate the method's robustness.
    *   The study on the impact of datastore size reveals that ExaGPT maintains strong performance even with a significantly smaller datastore (500 pairs), which is an important practical consideration for deployment (Section 4.3, Figure 6, Figure 11).
    *   The paper provides a quantitative analysis correlating span characteristics (length, reliability score) with prediction correctness, offering insight into the method's interpretability mechanism (Section 4.1, Figure 4).3) Weaknesses
*   **Significant Computational Cost and Scalability Concerns**
    *   The proposed method relies on performing a k-NN search for every possible n-gram (up to n=20) in a target text against a large datastore of pre-computed embeddings. This is computationally intensive.
    *   The authors acknowledge the high inference cost, noting that experiments required four NVIDIA A6000 GPUs (Section 7, Appendix D). This cost could be a major barrier to real-world adoption, especially when compared to much lighter baseline models like LR-GLTR.
    *   While the analysis on datastore size shows robustness (Section 4.3), it does not fully mitigate the fundamental challenge of performing a massive number of nearest-neighbor searches for each document to be classified.*   **Insufficient Justification for Key Design Choices**
    *   The method for creating span embeddings—taking the mean of the *second-layer* hidden outputs from BERT-large—is a critical and non-standard design choice. The only justification is a brief note in a footnote stating it was chosen in a pilot study for balancing lexical and semantic similarity (Footnote 5, page 4).
    *   The impact of this specific layer choice on both detection performance and the quality of retrieved examples for interpretability is not quantified. Without an ablation study comparing it to more common embedding strategies (e.g., using the last layer or averaging multiple top layers), the optimality and robustness of this choice are unclear.
    *   The range of n-grams considered (1 to 20) is fixed (Section 3.1). The sensitivity of the model to this maximum n-gram length is not explored, yet it directly impacts both computational cost and the types of linguistic patterns the model can capture.*   **Limited Generalizability of the Human Evaluation**
    *   The human evaluation, while a core strength, was conducted with a small number of participants (four annotators) who are all experts in natural language processing (one MSc student, one PhD student, two researchers) (Section 3.1).
    *   The findings on interpretability may not generalize to the primary target users of such detectors, such as educators or content moderators, who typically lack this specialized background. The authors acknowledge this as a limitation (Section 7).
    *   The total number of samples evaluated per detector (96) is also relatively small (Footnote 6, page 5), which could limit the statistical power of the conclusions drawn from the human study.*   **Inconsistent Baseline Reporting and Performance Claims**
    *   The reported performance for the RoBERTa baseline in Table 2 contains internal contradictions. For several settings, the model is reported to achieve a perfect AUROC of 100.0, which indicates perfect class separability (Table 2, e.g., ChatGPT/Wikipedia).
    *   Despite this perfect AUROC, the same model is reported to have an accuracy of 50.0% at 1% FPR, which is equivalent to random chance on a balanced dataset. A model with perfect separability should achieve near-perfect accuracy at 1% FPR (Table 2).
    *   This inconsistency undermines the paper's headline performance claims, as the largest reported gain of "+40.9 points" (Abstract, Section 1) relies on a comparison to these questionable RoBERTa results (Table 2, Dolly-v2/Reddit).*   **Contradictory Analysis of a Key Hyperparameter**
    *   The textual analysis of the hyperparameter `α` in Section 4.2 is directly contradicted by the paper's own figures. The text claims that "the higher the α, the higher the detection performance" (Section 4.2).
    *   However, the corresponding plots clearly show the opposite trend: for all generators, performance (both AUROC and Acc@1%FPR) is highest at or near `α=0.0` and generally *decreases* as `α` increases (Figure 5, Figure 10).
    *   The paper also misinterprets the role of `α`, stating that higher performance is linked to prioritizing the reliability score, whereas Equation 5 shows that a high `α` actually prioritizes the length score. This suggests a lack of care in the analysis.4) Suggestions for Improvement
*   **Address Scalability with Cost Analysis and Mitigation Strategies**
    *   To address the computational cost, the authors should provide a more detailed analysis of inference time, perhaps reporting the average time to classify a document and comparing it directly to the baselines.
    *   The paper would be strengthened by a discussion of potential cost-reduction strategies. This could include exploring the use of more efficient Approximate Nearest Neighbor (ANN) libraries, vector quantization, or model distillation to create smaller embeddings, thereby making the approach more practical.
    *   A brief discussion on the trade-off between the number of n-grams searched (e.g., by striding or only considering certain lengths) and performance/cost would also be valuable.*   **Strengthen Justification for Design Choices with Ablations**
    *   To justify the choice of using the second BERT layer for embeddings, the authors should add an ablation study in the appendix. This study should compare the performance (both Acc@1%FPR and human evaluation scores, if feasible) of using the second layer versus other common strategies like the final layer's CLS token, the mean of the final layer's token embeddings, or the mean of the top few layers.
    *   Similarly, an ablation on the maximum n-gram length (e.g., comparing N=10, N=20, N=30) would provide valuable insight into its impact on performance and computational load, further strengthening the technical contribution.*   **Frame Human Evaluation Results and Discuss Broader Implications**
    *   To address the limited scope of the human study, the authors should more explicitly frame the current results as a controlled study with expert users.
    *   They should expand the discussion in the limitations or future work sections to hypothesize how the results might differ with non-expert users. For instance, they could discuss potential challenges like cognitive overload from the evidence or the difficulty non-experts might face in assessing semantic similarity.
    *   Proposing a future study with a larger, more diverse participant pool would be a concrete way to acknowledge this limitation while outlining a path forward.*   **Correct and Re-evaluate Baseline Performance**
    *   The authors should re-evaluate the RoBERTa baseline to resolve the contradiction between its AUROC and Acc@1%FPR scores. This may involve checking the thresholding logic used to calculate accuracy at a fixed FPR.
    *   The results in Table 2 should be updated with credible baseline scores.
    *   All performance claims throughout the paper (e.g., in the Abstract and Introduction) should be revised to reflect the corrected comparison.*   **Revise the Hyperparameter Analysis for Accuracy**
    *   The description in Section 4.2 must be rewritten to accurately describe the trends shown in Figure 5 and Figure 10.
    *   The authors should provide a correct interpretation of the results, discussing why lower `α` values (i.e., prioritizing the reliability score) lead to better performance.
    *   The interpretation of `α`'s role according to Equation 5 should be corrected in the text.5) Score
- Overall (10): 7 — The paper presents a highly novel and well-motivated method, but significant flaws in the experimental reporting and analysis (Table 2, Section 4.2) weaken its conclusions.
- Novelty (10): 9 — The example-based retrieval framework combined with dynamic programming for span selection is a fresh and compelling approach in this domain (Section 2).
- Technical Quality (10): 6 — The core method is sound, but major issues like inconsistent baseline results (Table 2) and a contradictory analysis (Section 4.2) detract from the paper's rigor.
- Clarity (10): 8 — The paper is generally well-written, but the analysis in Section 4.2 is fundamentally misleading as it contradicts the paper's own figures (Figure 5, Figure 10).
- Confidence (5): 5 — I am highly confident in my evaluation, as I am familiar with the relevant literature and the paper provides sufficient detail to assess its contributions and limitations.