Summary
The paper introduces ExaGPT, a retrieval-based detector for distinguishing human-written from LLM-generated text that is explicitly designed to provide human-interpretable evidence. The method segments an input text into n-gram spans, retrieves top-k similar spans from a labeled datastore (human vs. LLM), computes three span-level metrics (length L, reliability R via mean cosine similarity, and prediction P via proportion of LLM labels among neighbors), and uses dynamic programming to select a non-overlapping segmentation that balances L and R (Eq. 5). A final decision is made via the mean prediction score over selected spans (Eq. 6), with evidential outputs being the retrieved examples per span (Eq. 7). Experiments on the English subset of M4 (four domains × three generators) compare ExaGPT to RoBERTa+SHAP, LR-GLTR, and DNA-GPT, reporting AUROC and accuracy at fixed 1% FPR. Human evaluation measures whether participants can judge the correctness of the detector’s decision given its evidence; ExaGPT yields higher judgment accuracy (Table 1). Across domains/generators, ExaGPT often matches or exceeds baselines, with especially large gains at 1% FPR (Table 2). Analysis examines the effect of α (span length vs. reliability) and datastore size, and provides evidence that longer, high-reliability spans correlate with correct decisions (Fig. 4).

Soundness
The core algorithm is logically coherent: a k-NN retrieval over span embeddings provides label-weighted evidence; dynamic programming (Algorithm 1) optimizes segmentation by averaging a linear combination of normalized L and R across spans; and the final classification threshold is applied to the mean P. The use of a datastore of labeled spans (constructed from the training split) is internally consistent with the retrieval-centric design. However, several aspects warrant attention:
- Embedding choice and similarity: Using BERT-large with “mean second-layer hidden outputs” for span embeddings (Section 3.1, footnote 5) is unusual; most retrieval systems rely on later transformer layers or dedicated sentence/phrase encoders. The paper claims a pilot study motivated the second-layer selection, but no ablation is reported to verify this choice’s effect on both interpretability and accuracy.
- DP objective and α interpretation: Eq. (5) weights L and R by α. Section 4.2 states “the higher the α, the higher the detection performance,” yet Appendix C (Figure 10; Table 45) shows performance generally decreases as α increases (i.e., putting more weight on L and less on R). The text later concludes “taking the reliability score more into account can improve detection performance,” which is consistent with low α. This is an internal inconsistency that should be corrected, as it affects the interpretability of the design choice.
- Thresholding at fixed FPR=1%: The evaluation reports accuracy at FPR=1% (Section 3.1), but the procedure for setting ε (Eq. 6) is not detailed—e.g., per-domain/generator calibration, validation-based calibration applied to test, or global thresholding. Ambiguity here can impact fairness and reproducibility.
- Potential near-duplicate/memorization effects: Retrieving examples from an in-domain, in-generator datastore (Section 6) may lead to near-duplicate spans and inflate detection performance while limiting generalization. The paper acknowledges this limitation and proposes future work on cross-domain/generator testing (Section 6), but current results should be interpreted accordingly.
- Human evaluation metric: Measuring “accuracy of human judgments on detection correctness based on evidence” conflates interpretability with decision predictability under evidence. Without significance testing, inter-annotator agreement, or bias controls, the claim of improved interpretability (Table 1) is supportive but not definitive.

Presentation
The paper is generally clear, well organized, and includes helpful figures (Figures 1–3) and concrete algorithmic descriptions (Algorithm 1, Eqs. 1–7). The overview in Figures 2 and the UI in Figure 3 are useful for understanding the evidence presentation. Tables 1–2 summarize human evaluation and detection performance comprehensively. However:
- The α analysis narrative contradicts the plotted results (Section 4.2 vs. Figure 5 and Appendix C/Figure 10/Table 45); this should be clarified.
- AUROC values of 100.0 for RoBERTa across multiple settings (Table 2) are surprising given the modest accuracy at 1% FPR; more explanation (e.g., rounding, class-imbalance effects, calibration differences) is warranted.
- Footnote 5 motivates using a specific BERT layer but lacks quantitative backing; an ablation table would improve clarity.

Contribution
The paper’s main contribution is an interpretable detection framework aligned with a plausible human verification strategy: judge text origin by comparing similar spans to human vs. LLM exemplars. ExaGPT operationalizes this via span-level k-NN retrieval, a DP-based segmentation optimizing length and similarity, and evidence presentation with example spans. This is a notable shift from token-likelihood visualizations and perturbation-based explainers, and is novel in the LLM detection literature in its emphasis on example-based span evidence. Empirically, the method achieves strong detection results at low FPR and provides evidence that users find more actionable relative to baselines (Table 1). The contribution is practically relevant, especially in high-stakes contexts where interpretability matters.

Strengths
- Clear, human-aligned interpretability mechanism: example-based evidence per span (Sections 2.1–2.3; Figures 2–3; Eq. 7).
- Strong low-FPR performance across domains/generators, with large gains in some settings (Table 2; e.g., Dolly-v2 Reddit ACC: 90.8 vs. RoBERTa 50.0).
- Thoughtful span segmentation via DP optimizing length and reliability (Algorithm 1; Eq. 5), producing coherent evidence chunks.
- Useful analysis of reliability vs. correctness for long spans (Fig. 4), and robustness to datastore size (Section 4.3; Figure 6; Appendix C/Figure 11).
- Explicit discussion of limitations and ethics, including inference cost and human-judgment bias (Sections 7–8).

Weaknesses
- Inconsistency in α analysis: textual claim conflicts with plotted and tabulated results (Section 4.2 vs. Figure 5; Appendix C/Figure 10/Table 45).
- Embedding-layer choice is ad hoc; no ablations across layers/encoders, k, n, or similarity metrics (Section 3.1, footnote 5).
- Evaluation remains in-domain/in-generator; cross-domain/generator generalization is not tested (Section 6).
- Human evaluation is small (4 annotators, 96 samples per detector), with no statistical testing or agreement metrics (Section 3.1–3.2; Table 1).
- AUROC values of 100.0 for RoBERTa (Table 2) paired with mediocre accuracy at 1% FPR suggest potential calibration/rounding issues; more diagnostic reporting would help.
- DNA-GPT configuration assumes known prompts/temperatures (Section 3.1), which may benefit DNA-GPT but still does not reflect realistic black-box conditions, complicating fairness.

Questions
- How is the detection threshold ε (Eq. 6) set for the 1% FPR accuracy metric? Is it calibrated per domain/generator on validation and then fixed for test? Please specify.
- Can you provide ablations for: (a) embedding layer choices (e.g., last layer, pooled output), (b) alternative encoders (e.g., sentence-transformers), (c) k values, (d) maximum n-gram length N, and (e) similarity functions?
- The α analysis contradicts the plots and Appendix tables. Which direction is correct, and how should practitioners set α if they aim to prioritize reliability vs. length?
- How often do near-duplicate spans exist across train/test in M4, and how do you handle near-duplicate leakage? Could you report performance after deduplication?
- Could you report inter-annotator agreement and statistical significance (e.g., McNemar tests) for Table 1 to substantiate interpretability claims?
- AUROC=100.0 for RoBERTa appears implausible across multiple settings. Is this due to rounding or particular ROC estimation? Please include more decimal precision and calibration diagnostics.
- How does ExaGPT perform in cross-domain/generator scenarios and under paraphrase or obfuscation (e.g., RAID; Krishna et al., 2023)? Any preliminary results?

Rating
- Overall (10): 8 — Novel example-based, span-centric interpretability with strong low-FPR detection, but α analysis inconsistency and limited generalization/ablations temper confidence (Sections 2.1–2.3; Table 2; Section 4.2; Appendix C/Figure 10/Table 45).
- Novelty (10): 8 — Example-retrieval per span and DP segmentation are new in LLM detection and aligned to human reasoning (Figures 2–3; Eq. 7; Algorithm 1).
- Technical Quality (10): 7 — Method is coherent, but thresholding details, α inconsistency, missing ablations, and potential memorization concerns reduce rigor (Eq. 5–6; Section 3.1; Table 2; Section 4.2).
- Clarity (10): 8 — Generally clear with informative figures, but inconsistent α narrative and unexplained AUROC anomalies need clarification (Section 4.2; Table 2; Figure 5; Appendix C).
- Confidence (5): 4 — High familiarity with detection/interpretability; conclusions supported by cross-checks against tables/appendix, but some reporting gaps remain.

—

Summary
ExaGPT proposes an interpretable detector that classifies texts by comparing span-level similarity to human-written vs. LLM-generated spans stored in a labeled datastore. It creates n-gram spans (1–20 tokens), retrieves top-k similar spans via k-NN over BERT-large embeddings, computes length (L), reliability (R), and prediction (P) metrics per span, and applies dynamic programming to select a segmentation optimizing average α·L + (1−α)·R (Eq. 5). The final classification uses the mean of span-level P (Eq. 6), and the system presents the top-k example spans as evidence (Eq. 7). Human evaluation indicates users better judge correctness with ExaGPT evidence (Table 1), and experiments on M4 show strong AUROC and accuracy at fixed 1% FPR (Table 2). Analyses study reliability distributions (Fig. 4), α variation (Section 4.2), and datastore size effects (Section 4.3).

Soundness
The approach is methodologically sound in using example retrieval to ground interpretability and a DP objective to produce coherent spans. The scoring functions are simple and interpretable (Eqs. 1–3), and the decision rule (Eq. 6) is straightforward. However:
- The reliance on in-domain labeled datastores means the detector may capitalize on generator/domain-specific phrasing and template reuse; without cross-domain/generator evaluation, generalization remains uncertain (Section 6).
- The use of BERT-large second-layer outputs for embeddings (Section 3.1, footnote 5) is atypical; lack of ablations leaves open whether this choice is optimal or brittle.
- The α analysis is internally inconsistent; the paper claims higher α improves performance, while Appendix C (Figure 10/Table 45) shows decreasing performance with higher α. The correct interpretation likely is that more weight on reliability (i.e., lower α) improves performance.
- The human evaluation metric operationalizes interpretability via users’ ability to judge detector correctness from evidence (Table 1), but does not assess faithfulness (whether the evidence causally drives the prediction) or completeness, and lacks statistical testing.

Presentation
The exposition is clear, with a step-by-step methodological description and visual aids illustrating the system and evidence (Figures 2–3). Equations and Algorithm 1 are helpful. The dataset setup and baselines are described (Section 3.1), and appendices detail configurations. Areas for improvement:
- Clarify threshold calibration for FPR=1% (Eq. 6; Section 3.1).
- Resolve the α narrative vs. plotted results discrepancy (Section 4.2 vs. Appendix C/Figure 10/Table 45).
- Discuss the surprising AUROC=100.0 for RoBERTa (Table 2) alongside low accuracy at 1% FPR; provide more precise AUROC values and calibration analyses.

Contribution
The paper introduces a practical and novel interpretability paradigm for LLM detection: span-level example retrieval aligned with human reasoning and backed by a segmentation optimizer. The contribution is significant for high-stakes contexts (education, content moderation), where explainability is as important as raw accuracy. The empirical gains at low FPR make the method attractive for real-world deployment, provided domain/generator alignment.

Strengths
- Human-aligned interpretability via retrieved example spans per selected text span (Sections 2.1–2.3; Figure 3; Eq. 7).
- Competitive or superior detection accuracy at 1% FPR across many settings (Table 2), with substantial improvements over baselines in some cases (e.g., Dolly-v2 Reddit).
- Simple, transparent scoring and selection mechanisms (Eqs. 1–5; Algorithm 1) that are easy to reason about.
- Analysis links reliability of long spans to correctness (Fig. 4) and shows reasonable robustness to datastore size (Section 4.3; Figure 6).
- Ethical discussion and limitation acknowledgement (Sections 7–8).

Weaknesses
- Generalization untested: only in-domain/in-generator results (Section 6).
- Missing ablations for key design choices (embedding layer/encoder, k, N, similarity function).
- Inconsistent claims about α; this affects guidance on how to balance interpretability components.
- Human evaluation small-scale and lacks statistical rigor (Table 1); interpretability metric conflates users’ guessability with explanation quality.
- AUROC anomalies for RoBERTa (Table 2) not explained; raise concerns about reporting or calibration.

Questions
- Could you provide cross-domain/generator results (e.g., train datastore on Wikipedia/ChatGPT, evaluate on Reddit/GPT-4)?
- What is the procedure for setting ε to achieve FPR=1%? Is it fixed per domain/generator based on validation, then applied to test?
- How sensitive is performance/interpretablity to k (top-k neighbors), maximum span length N, and choice of encoder (e.g., sentence-transformers, SimCSE)?
- Can you add significance testing and inter-annotator agreement for Table 1? Also, did annotators see ground-truth labels while judging evidence?
- For DNA-GPT, relying on known prompts/temperatures (Section 3.1) may not reflect practical conditions; can you include a black-box setting?
- Could you report deduplication analyses to control for near-duplicate spans across train/test?

Rating
- Overall (10): 8 — Strong, novel interpretability and practical low-FPR performance, but inconsistencies in α analysis, limited generalization tests, and missing ablations reduce the final score (Sections 2–4; Table 2; Appendix C/Figure 10).
- Novelty (10): 8 — Example-based, span-centric evidence with DP segmentation is a meaningful advancement over perturbation/token-likelihood methods (Figures 2–3; Algorithm 1).
- Technical Quality (10): 7 — Sound core, but thresholding details, α inconsistency, AUROC anomalies, and lack of ablations weaken rigor (Eq. 6; Section 3.1; Table 2; Section 4.2).
- Clarity (10): 8 — Clear method and visuals; needs clarification on α and AUROC reporting (Section 4.2; Table 2).
- Confidence (5): 4 — Review grounded in cross-checking tables/appendix; some uncertainties remain due to reporting gaps.

—

Summary
This paper presents ExaGPT, an interpretable detector for LLM-generated text that bases decisions on example retrieval of similar spans from labeled human vs. LLM datastores. It scores each span by length, reliability (mean similarity to k neighbors), and prediction (fraction of LLM labels among neighbors), and uses dynamic programming to select a segmentation that maximizes the average of normalized length and reliability (Eq. 5). The classification aggregates span-level predictions (Eq. 6), and the evidence consists of the retrieved top-k spans per segment (Eq. 7). On M4 (English), ExaGPT often outperforms RoBERTa+SHAP, LR-GLTR, and DNA-GPT in AUROC and accuracy at 1% FPR (Table 2) and yields higher human judgment accuracy regarding detector correctness (Table 1). Analyses explore span reliability distributions, α sensitivity, and datastore size.

Soundness
The retrieval-plus-DP framework is conceptually aligned with human judgments about textual origin and is technically consistent. The formalization of scores (Eqs. 1–3) and the DP selection (Algorithm 1; Eq. 5) are clear. Nonetheless, some issues undermine soundness:
- The datastore is built from in-domain training data; if train and test share stylistic templates, near-duplicate spans could drive both interpretability and performance, limiting generalization (Section 6).
- The final decision averaging P over segments (Eq. 6) treats spans uniformly, ignoring variance in reliability across spans; a weighted average by R might be more principled and robust.
- The paper does not quantify the computational complexity; with up to 20-gram spans per token and k-NN over millions of spans, retrieval at inference is expensive (Section 7 acknowledges this). A complexity and latency analysis would be helpful.
- The α discussion is inconsistent with results (Section 4.2 vs. Figure 5 and Appendix C/Figure 10/Table 45), suggesting either a misstatement or plotting error.

Presentation
The paper’s structure is easy to follow, with concise notation and supportive diagrams (Figures 2–3). Experimental tables (Tables 1–2) are comprehensive across domains/generators. However:
- AUROC values for RoBERTa reported as 100.0 (Table 2) conflict with low accuracy at 1% FPR; without more decimal precision and calibration details, readers cannot reconcile these numbers.
- Footnote 5’s layer selection claim would benefit from empirical evidence (ablation or retrieval quality metrics).
- Thresholding methodology for 1% FPR is not described; clarity is needed to assess comparability.

Contribution
The main contribution is an interpretable, example-based span method with a principled selection strategy. It fills a gap between token-likelihood visualizations and perturbation-based explainers by offering concrete, semantically similar span exemplars as evidence—something users can readily assess. The empirical results at low FPR strengthen the practical value, particularly in high-stakes settings.

Strengths
- Evidence aligned with human reasoning: similar span examples per segment (Figures 2–3; Eq. 7).
- Consistent low-FPR gains vs. baselines in many settings, especially with Dolly-v2 (Table 2).
- DP segmentation produces contiguous, meaningful spans that aid interpretation (Algorithm 1).
- Robustness to datastore size is demonstrated (Section 4.3; Figure 6), supporting scalability trade-offs.
- Limitations and ethical considerations are explicitly discussed (Sections 7–8).

Weaknesses
- Limited generalization (no cross-domain/generator tests) and potential near-duplicate effects (Section 6).
- Inconsistent α interpretation vs. empirical curves (Section 4.2; Appendix C).
- No ablations for encoders, layers, k, N, or weighting schemes (e.g., R-weighted aggregation).
- Human evaluation small and without agreement/statistical analysis (Table 1).
- AUROC anomalies and thresholding methodology unclear (Table 2; Section 3.1).

Questions
- Would a reliability-weighted overall prediction (e.g., mean of P weighted by R) improve robustness compared to Eq. 6? Any experiments?
- How does ExaGPT behave when a text contains mixed-origin spans (e.g., human text edited by an LLM)? Can the evidence reflect such mixtures?
- What is the retrieval index size (number of spans) per datastore, and what are average latency and memory footprints? Any pruning strategies?
- Can you provide ablations comparing BERT-large layer choices, sentence-transformer encoders, and values of k? Does k=10 suffice?
- How is ε set for 1% FPR? Per combination or globally? Is calibration transferred from validation to test?
- Given the strong effects of α reported in Appendix C, what guidance do you recommend for practitioners choosing α?

Rating
- Overall (10): 8 — Valuable, human-aligned interpretability with strong low-FPR performance; reporting/analysis inconsistencies and missing ablations limit the score (Sections 2–4; Table 2; Appendix C/Figure 10).
- Novelty (10): 8 — Span-level example retrieval and DP segmentation are novel contributions to LLM detection interpretability (Figures 2–3; Algorithm 1).
- Technical Quality (10): 7 — Sound design, but unclear thresholding, α inconsistency, no ablations, and potential generalization/memorization concerns (Eq. 6; Section 4.2; Table 2).
- Clarity (10): 8 — Clear overall presentation; needs fixes for α narrative and AUROC reporting (Section 4.2; Table 2).
- Confidence (5): 4 — Analysis cross-checked against tables/appendix; some uncertainties remain due to reporting gaps.

—

Summary
The authors propose ExaGPT, an interpretable detector that classifies texts by comparing span-level similarity to human vs. LLM examples in a datastore. It retrieves top-k neighbors for each span using BERT-large embeddings, computes length, reliability (mean similarity), and prediction (fraction of LLM-labeled neighbors), and selects a segmentation via dynamic programming to maximize the average of normalized length and reliability (Eq. 5). The final prediction aggregates span-level P (Eq. 6), and the evidence is the list of retrieved examples per selected span (Eq. 7). ExaGPT is evaluated on M4 (English) against RoBERTa+SHAP, LR-GLTR, and DNA-GPT, with AUROC and accuracy at 1% FPR reported (Table 2). A human study measures users’ ability to judge decision correctness using detector evidence (Table 1). Analyses include reliability distributions (Fig. 4), α sensitivity (Section 4.2; Figure 5; Appendix C/Figure 10), and datastore size (Section 4.3; Figure 6).

Soundness
The methodology is coherent and sensibly linked to human decision-making about textual origin. The span-level k-NN retrieval provides concrete evidence, and DP segmentation yields interpretable chunks. Nonetheless, there are concerns:
- Evaluation design: ExaGPT uses in-domain, in-generator datastores drawn from the training split; without cross-domain/generator experiments, generalization is uncertain (Section 6). The large gains at 1% FPR could partly reflect in-domain stylistic memorization.
- Baseline comparability: DNA-GPT is configured with known prompts and temperatures (Section 3.1), which is favorable and yet may still not reflect realistic black-box detection; conversely, RoBERTa reports AUROC=100.0 while having low accuracy at 1% FPR (Table 2), suggesting calibration/thresholding differences that complicate comparisons.
- α analysis inconsistency: Section 4.2 claims higher α increases performance, but Appendix C shows performance decreasing with higher α (Figure 10; Table 45). The text later asserts reliability emphasis helps, which implies lower α. This should be rectified.
- Human evaluation: Small-scale (four annotators, 96 samples per detector) without significance tests or agreement metrics (Table 1). The metric measures correctness judgments, not faithfulness or cognitive load.

Presentation
The paper is well-written and includes strong visual aids (Figures 2–3). Algorithm and equations are clear. However:
- The α narrative contradicts figures/tables (Section 4.2; Figure 5; Appendix C/Figure 10/Table 45).
- AUROC=100.0 for RoBERTa (Table 2) is surprising; the paper should present more precise AUROC values and discuss calibration effects.
- Threshold calibration for FPR=1% is not specified, which is crucial for reproducibility and fairness.

Contribution
The contribution lies in marrying example-based retrieval with span-level interpretability and a principled segmentation strategy. This approach addresses the interpretability gap in LLM detection by providing evidence that non-expert users can understand. The empirical focus on low-FPR metrics is appropriate for high-stakes applications. The work is a substantial step toward interpretable, practical detectors.

Strengths
- Intuitive and actionable evidence: retrieved similar spans with label distributions per segment (Eq. 7; Figure 3).
- Strong performance at low FPR across many settings (Table 2), with notable gains in challenging generators (Dolly-v2).
- DP segmentation balances span length and reliability, improving readability (Algorithm 1; Eq. 5).
- Analysis connects reliability of long spans with correct detection (Fig. 4) and shows robustness to datastore size (Section 4.3; Figure 6).
- Transparency about limitations and ethics (Sections 7–8).

Weaknesses
- No cross-domain/generator robustness evaluation; current results are in-domain (Section 6).
- Inconsistency in α claims; unclear practical guidance (Section 4.2; Appendix C).
- Missing ablations for encoder choice, layer selection, k, N, and alternative aggregation (e.g., R-weighted P).
- Human study small and limited in scope; lacks statistical rigor (Table 1).
- AUROC anomalies and thresholding/calibration details missing (Table 2; Section 3.1).

Questions
- Please clarify how ε is chosen to ensure FPR=1% on test—per domain/generator calibration on validation, or a global threshold?
- Could you include cross-domain/generator tests (and paraphrase robustness), perhaps using RAID (Dugan et al., 2024)?
- What is the impact of using sentence-transformer embeddings or later BERT layers on both detection and interpretability?
- Would weighting the final P by span reliability R improve performance and evidence quality?
- Can you report statistical significance for Table 1 and inter-annotator agreement?
- How do you handle very short inputs where few long spans exist? Does DP select many short, low-R spans, and how does that affect evidence?

Rating
- Overall (10): 8 — Effective, user-aligned interpretability with strong practical performance; reporting inconsistencies and limited robustness testing reduce the score (Sections 2–4; Table 2; Appendix C).
- Novelty (10): 8 — The example-based span approach and DP segmentation are novel contributions to interpretable LLM detection (Figures 2–3; Algorithm 1).
- Technical Quality (10): 7 — Sound design but missing ablations, unclear thresholding, α inconsistency, and potential in-domain memorization (Eq. 6; Section 4.2; Table 2).
- Clarity (10): 8 — Clear overall; needs fixes for α and AUROC reporting (Section 4.2; Table 2).
- Confidence (5): 4 — Strong familiarity; cross-checked claims against tables/appendix; some uncertainties remain.