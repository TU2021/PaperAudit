Here are four distinct reviews of the paper "ExaGPT: Example-Based Machine-Generated Text Detection for Human Interpretability".

***

### **Review 1**

**Summary**
This paper introduces ExaGPT, a novel, interpretable method for detecting machine-generated text. The approach is inspired by the human process of comparing a given text to known examples. ExaGPT segments a target text into spans and, for each span, retrieves the k-nearest neighbors from a datastore of human-written and LLM-generated texts. The final classification is based on the proportion of LLM examples among the retrieved neighbors. Crucially, the method uses dynamic programming to find an optimal segmentation that balances span length and similarity to the datastore, and it presents the retrieved similar spans as evidence to the user. The authors evaluate ExaGPT on performance and interpretability, showing that it significantly outperforms strong baselines in accuracy at a low false-positive rate and that its evidence is more helpful for humans to judge the correctness of the model's predictions.

**Soundness**
The methodology is sound and well-motivated. The core idea of using a k-NN approach over text spans is powerful, and the two-phase process of Span Scoring and Span Selection is logical. The use of a dynamic programming algorithm to find an optimal segmentation (Algorithm 1) is an elegant solution to the problem of balancing the preference for long, meaningful spans with the need for spans that have reliable matches in the datastore. The experimental setup is thorough, using a standard benchmark (M4), multiple domains, and multiple generators. The choice of evaluation metrics, particularly the inclusion of accuracy at 1% FPR, is commendable as it reflects the practical need to minimize false accusations. The human evaluation, which directly measures the utility of the provided evidence, is a strong and appropriate way to validate the central claim of improved interpretability.

**Presentation**
The paper is exceptionally well-written and easy to follow. The introduction clearly motivates the problem and situates the work within the existing literature. Figures 1 and 2 provide an excellent high-level overview of the method, making the core concepts intuitive before diving into the technical details. The methodology section is detailed and logically structured. The results are presented clearly in tables and figures, and the analysis section provides valuable insights into why the model works well. The user interface example in Figure 3 effectively demonstrates the kind of interpretable output the system produces.

**Contribution**
The contribution of this paper is highly significant. It proposes a new paradigm for machine-generated text detection that prioritizes human interpretability without sacrificing, and in fact significantly improving, detection performance in critical scenarios (low FPR). While example-based methods exist in other NLP tasks, their application and novel adaptation (specifically the DP-based segmentation) to LLM detection is a major step forward. The paper not only introduces a high-performing model but also provides a concrete methodology for evaluating the interpretability of such detectors, which is a valuable contribution in its own right.

**Strengths**
- **Novelty and Intuition:** The method is novel and grounded in a very intuitive, human-aligned process of verification by comparison.
- **Exceptional Performance:** The reported results, especially the massive gains in accuracy at 1% FPR (Table 2), are impressive and demonstrate the method's effectiveness in a practical setting.
- **Strong Interpretability Evaluation:** The paper goes beyond simply claiming interpretability and provides empirical evidence through a well-designed human study (Table 1).
- **Thorough Analysis:** The ablation studies on the impact of the hyperparameter α (Figure 5) and the datastore size (Figure 6) demonstrate the robustness of the method.

**Weaknesses**
- **In-Domain Assumption:** The current evaluation is performed in an in-domain setting. The performance in cross-domain or cross-generator scenarios, which is a major challenge for all detectors, remains an open question (as acknowledged by the authors).
- **Computational Cost:** The reliance on k-NN search across a large datastore of spans is computationally expensive, which may limit its real-time applicability. The paper acknowledges this in the Limitations section.

**Questions**
1. The dynamic programming algorithm optimizes a score based on span length (L) and reliability (R). The final prediction, however, is based on the prediction score (P). Have you considered incorporating the prediction score P (or its variance/confidence) into the DP objective function? It seems that favoring spans where the k-NN are in strong agreement (e.g., all LLM or all Human) could also be a useful signal for segmentation.
2. The human evaluation results are very promising. Could you provide more details on what aspects of the evidence the participants found most useful? For example, was it the raw similarity scores, the label distribution, or the content of the retrieved spans themselves?

**Rating**
- Overall (10): 9 — The paper presents a novel, highly effective, and interpretable method with strong empirical backing.
- Novelty (10): 9 — The example-based approach with DP segmentation is a fresh and significant contribution to the field.
- Technical Quality (10): 9 — The methodology is technically sound, and the experiments are rigorous and well-executed.
- Clarity (10): 10 — The paper is exceptionally clear, well-structured, and supported by excellent figures.
- Confidence (5): 5 — I am highly confident in my assessment; the paper's claims are well-supported by the evidence provided.

***

### **Review 2**

**Summary**
The paper proposes ExaGPT, a k-nearest-neighbor-based method for detecting LLM-generated text. For a given input text, the method considers all possible n-gram spans, finds similar spans in a pre-compiled datastore of human and machine texts, and scores each span based on length, similarity to neighbors, and the class distribution of neighbors. A dynamic programming algorithm then selects a final, non-overlapping segmentation of the text that maximizes a combination of span length and similarity scores. The text is classified based on the average class prediction of these selected spans. The authors claim superior performance, particularly at a 1% false positive rate, and improved interpretability, which they support with a human study.

**Soundness**
The methodological soundness has some points that require clarification.
1.  **Objective Mismatch:** The DP algorithm for span selection (Algorithm 1) maximizes a weighted sum of standardized length (L) and reliability (R) scores. However, the final classification is based on the average prediction score (P) of the selected spans. It is not clear why the objective function for segmentation ignores the prediction score P. One might expect that segmenting the text into spans with high predictive certainty (e.g., P close to 0 or 1) would be beneficial for the final classification. This mismatch between the segmentation objective and the classification objective is not justified.
2.  **Human Evaluation Scale:** The central claim of superior interpretability rests on a human evaluation. However, this study involved only four expert annotators (students/researchers in NLP) and a total of 96 samples per detector (Table 1, footnote 6). This is a very small sample size, which raises concerns about the statistical significance and generalizability of the findings. A conclusion that ExaGPT is "+13.6 points" better is too strong for such a small-scale study.
3.  **Hyperparameter Justification:** Several key hyperparameters seem to be chosen without sufficient justification. For instance, footnote 5 mentions selecting the 2nd layer of BERT-large based on a pilot study, but no details of this study are provided. Why is a relatively old model like BERT-large used for embeddings instead of more modern sentence-transformer models designed for semantic similarity? The choice of k=10 for the k-NN search is also not ablated or justified.

**Presentation**
The paper is generally well-written, but some crucial details are either missing or buried. For example, the procedure for standardizing the L and R scores for the DP algorithm (footnote 2) is mentioned but not detailed. Algorithm 1 is presented at a high level; a more concrete pseudocode would be helpful. There also appears to be a referencing issue with the figures in the appendix. The text in Section 4.2 refers to Figure 5, which is located in block 27, but the appendix contains Figure 10 (block 45) with raw data tables and Figure 47 with plots, both related to the impact of α. This makes cross-referencing difficult.

**Contribution**
The idea of an example-based detector is interesting and a departure from standard classifier- or metric-based approaches. The performance results, if they hold up under further scrutiny, are significant. However, the contribution regarding interpretability is not as convincingly demonstrated as the authors claim, due to the limitations of the human study. The paper presents a promising direction, but the claims of superiority feel premature.

**Strengths**
- The core concept of using span-based retrieval is novel in the context of LLM detection.
- The focus on and strong performance in the low-FPR regime (Acc@1% FPR) is a major strength, as this is the most critical metric for real-world deployment.
- The analysis of the impact of datastore size and the α parameter adds to the paper's thoroughness.

**Weaknesses**
- The human study for interpretability is too small to draw strong, generalizable conclusions.
- There is an unexplained mismatch between the optimization objective in the span selection phase and the metric used for final classification.
- Justification for several important methodological choices (e.g., embedding model, specific layer, value of k) is lacking.

**Questions**
1. Can the authors provide a justification for not including the prediction score `P` in the objective function for the dynamic programming segmentation? Have you experimented with including it, and if so, what was the impact on performance and the quality of the resulting segmentations?
2. Regarding the human study, what were the exact instructions given to the annotators? How was inter-annotator agreement measured? Given the small scale, can you provide any statistical analysis (e.g., confidence intervals) for the results in Table 1?
3. Why was the 2nd layer of BERT-large chosen for embeddings? Did you compare this against other layers or other, more recent embedding models specifically tuned for semantic similarity tasks?

**Rating**
- Overall (10): 6 — A promising idea with strong reported performance, but weakened by methodological gaps and an underpowered human study.
- Novelty (10): 8 — The example-based detection framework is a novel and interesting direction.
- Technical Quality (10): 5 — The soundness of the methodology is questionable due to the objective function mismatch and lack of justification for key design choices.
- Clarity (10): 7 — Generally clear, but important details are missing or hard to find, and figure referencing is confusing.
- Confidence (5): 5 — I am confident in my assessment of the paper's technical weaknesses.

***

### **Review 3**

**Summary**
This paper presents ExaGPT, a system for detecting AI-generated text by comparing text fragments to a large database of known human and AI-written examples. The method works by first scoring all possible n-gram spans in a text for their similarity to database entries. It then uses dynamic programming to segment the text into the most "reliable" spans, defined by a combination of length and similarity. A text is flagged as AI-generated if its constituent spans are, on average, more similar to AI examples in the database. The authors demonstrate that this method is not only highly accurate—especially at preventing false positives—but also more interpretable to users, as confirmed by a human study.

**Soundness**
The methodology is sound for the controlled, in-domain setting presented in the paper. The logic flows well from span scoring to selection to final classification. The experiments are conducted on a standard benchmark (M4) and compared against relevant baselines. The choice to evaluate accuracy at a fixed 1% FPR is a very practical and important decision, as the cost of falsely accusing a human of using an AI is extremely high. The analysis of how performance scales with datastore size and varies with the hyperparameter α is also a solid piece of work that adds confidence in the results. The main caveat to the soundness is the method's heavy reliance on the datastore, which is a significant practical constraint.

**Presentation**
The paper is well-structured and clearly written. The motivation for an interpretable detector is compellingly laid out in the introduction. The figures, particularly the overviews in Figures 1 and 2, do an excellent job of illustrating the complex workflow in a digestible manner. The results in Table 2 are striking and effectively highlight the primary strength of the proposed method. The authors are also commendably transparent about the limitations of their work, dedicating a section to discussing the inference cost and potential biases in the human evaluation.

**Contribution**
The paper makes a strong contribution by proposing a method that excels on the most critical metric for practical deployment: accuracy at a very low false-positive rate. While many detectors chase high AUROC, ExaGPT's performance of up to +40.9 points in Acc@1%FPR (Block 2) is a game-changer for real-world applications where avoiding false accusations is paramount. The interpretability aspect, where users are shown concrete evidence for a decision, is another significant contribution that addresses a major shortcoming of existing black-box detectors. This work successfully pushes the state-of-the-art in a direction that is both more effective and more responsible.

**Strengths**
- **State-of-the-art at Low FPR:** The detector's outstanding performance at 1% FPR (Table 2) makes it highly suitable for high-stakes applications like academic integrity and content moderation.
- **Practical Interpretability:** The evidence provided (Figure 3) is concrete and intuitive, consisting of similar examples, which is more useful for a non-expert user than feature-attribution scores (like SHAP) or token probabilities (like GLTR).
- **Robustness:** The method shows robustness to the choice of the α hyperparameter (Figure 5) and maintains strong performance even with a significantly reduced datastore size (Figure 6), which are important practical considerations.

**Weaknesses**
- **Datastore Dependency:** The method's primary weakness is its reliance on a large, high-quality, in-domain datastore. Building and maintaining such a datastore for every possible domain and new LLM is a massive, perhaps prohibitive, undertaking. The paper's future work section acknowledges this by pointing to cross-domain/cross-generator settings, but the severity of this limitation for practical deployment cannot be overstated.
- **High Computational Cost:** As stated in the limitations (Section 7), the method requires significant computational resources (4x A6000 GPUs). This makes it inaccessible for many potential users and unsuitable for on-device or large-scale, low-latency applications.
- **Scalability with n-grams:** The method considers all n-grams up to n=20. The number of possible spans grows quadratically with text length, and the cost of embedding and searching for all of them seems very high. A more detailed analysis of the inference time as a function of input text length would be beneficial.

**Questions**
1. The analysis on datastore size (Section 4.3) is very interesting. Could you provide a more direct analysis of the trade-off between datastore size, inference cost/time, and detection accuracy? This would be very useful for practitioners looking to deploy your method under a specific budget.
2. How would ExaGPT handle a "mixed" text, where some parts are written by a human and others are generated by an LLM? The current span-based approach seems well-suited to identify such cases, but the final classification is a single score for the whole document. Does the system have a mechanism to flag specific parts of a document as potentially machine-generated, rather than making a single binary judgment on the whole text?
3. Building a datastore for a new domain or a new generator is a key challenge. Do you have any initial thoughts or preliminary results on how much out-of-domain data can be tolerated before performance degrades significantly? For example, could a general "web text" datastore be effective for detecting generations in a more specific domain like "legal documents"?

**Rating**
- Overall (10): 8 — An excellent paper with a novel method that achieves outstanding results on a key practical metric, though its real-world deployment is hindered by significant cost and data requirements.
- Novelty (10): 8 — The example-based paradigm is a fresh take on the detection problem.
- Technical Quality (10): 8 — The technical execution is solid for the defined problem scope, with good analysis.
- Clarity (10): 9 — The paper is very clearly written and presented.
- Confidence (5): 5 — I am very confident in my evaluation of the paper's strengths and practical limitations.

***

### **Review 4**

**Summary**
This paper introduces ExaGPT, an example-based method for detecting LLM-generated text designed for high performance and human interpretability. The method avoids training a classifier directly. Instead, it classifies a text by segmenting it into spans and comparing each span against a datastore of known human and LLM-generated text spans. A dynamic programming algorithm is used to create an optimal segmentation. The final output is not only a classification label but also the retrieved similar spans for each segment, which serve as evidence. The authors conduct extensive experiments showing that ExaGPT outperforms several strong, interpretable baselines in terms of detection accuracy (especially at 1% FPR) and that its evidence is more effective for human users to assess the reliability of a prediction.

**Soundness**
The paper's methodology is logically coherent and well-executed. The two-phase approach of scoring all potential spans and then selecting an optimal segmentation is a sensible way to tackle the problem. The experiments are comprehensive, covering multiple domains and text generators from the M4 benchmark. The inclusion of both AUROC and Accuracy at 1% FPR provides a balanced view of performance. The human study, while small, is a direct and appropriate way to measure the claimed improvement in interpretability. The analyses in Section 4, which investigate the properties of the model's predictions and its sensitivity to hyperparameters, add a good layer of validation to the work.

**Presentation**
The paper is very well-presented. The writing is clear, concise, and professional. The structure is logical, guiding the reader from motivation to method, results, and analysis. The figures are a particular strength; Figure 1 and Figure 2 provide a clear conceptual and technical overview, respectively, which greatly aids understanding. Figure 3 gives a concrete sense of the user-facing output. The tables are well-formatted and easy to read. My only minor criticism on presentation is that some important details are placed in footnotes (e.g., footnote 1 on embeddings, footnote 2 on normalization), which could be integrated into the main text for better flow. The appendix also seems to have some formatting issues where tables of data (e.g., Figure 10 in block 45) are presented before the corresponding plots (Figure 47), which can be slightly confusing.

**Contribution**
This work makes a solid and valuable contribution to the field of LLM-generated text detection. It successfully tackles two of the biggest challenges simultaneously: improving detection accuracy in high-stakes scenarios (low FPR) and providing meaningful, interpretable evidence for its decisions. The proposed method is novel and intuitive, and the strong empirical results position it as a new state-of-the-art for interpretable detection. The paper serves as an excellent example of how to design and evaluate interpretable systems, moving beyond simple claims of interpretability to empirical measurement via human studies.

**Strengths**
- **Clarity of Exposition:** The paper is exceptionally well-written and easy to understand, despite the technical complexity of the method.
- **Strong Empirical Results:** The method demonstrates clear superiority over baselines on the Acc@1% FPR metric (Table 2), which is a very important result.
- **Intuitive and Interpretable by Design:** The entire method is built around the idea of providing examples as evidence, which is inherently more understandable than abstract feature importance scores.
- **Comprehensive Evaluation:** The paper includes a well-designed human study, extensive experiments across domains/generators, and robustness analyses.

**Weaknesses**
- **Details of Human Study:** While the setup of the human study is described, more detail would be welcome. For example, what was the exact task prompt given to the annotators? Were they given any training or examples beforehand? This information is important for the reproducibility and interpretation of the results in Table 1.
- **Algorithm Detail:** Algorithm 1 is a high-level description. It would be more complete if it explicitly showed how the `dp` table is initialized and how the standardized scores `L^{std}` and `R^{std}` are computed and used within the loop.
- **Minor Presentation Issues:** As mentioned, some key details are in footnotes, and the appendix figure/table ordering could be improved for clarity.

**Questions**
1. Could you clarify the relationship between the figures in the main text (e.g., Figure 5) and the tables and plots in the appendix (e.g., Figure 10 and Figure 47)? It seems the appendix contains the full data and plots for all generators, while the main text shows a subset. A clearer reference like "See Appendix C, Figure 47 for full results" would be helpful.
2. In the human evaluation, participants judged the correctness of the final detection. Did you consider an alternative evaluation where participants are shown the evidence for a single span and asked to guess its origin (Human vs. LLM)? This might provide a more granular insight into how useful the evidence is at the span level.
3. The reliability score R (Eq. 2) is the mean similarity. Did you explore using other aggregations of the k similarities, such as the similarity of the single nearest neighbor (k=1) or a weighted average?

**Rating**
- Overall (10): 8 — A strong paper with a novel method, impressive results, and excellent presentation, with only minor room for improvement in methodological detail and clarity of appendices.
- Novelty (10): 8 — The approach is a creative and effective application of example-based methods to this problem.
- Technical Quality (10): 8 — The experiments are thorough and the method is sound, though some minor details could be expanded upon.
- Clarity (10): 9 — The paper is extremely well-written and easy to follow, a model for clear scientific communication.
- Confidence (5): 5 — I am confident in my assessment of the paper's quality and contributions.