{
  "baseline_review": "1) Summary\nThe paper introduces ECO, a performance-aware prompting framework to improve code runtime optimization for large language models of code (code-LLMs). The authors argue that existing methods, which rely on providing slow-fast code pairs, encourage superficial pattern imitation rather than genuine performance reasoning. ECO first distills human-interpretable Runtime Optimization Instructions (ROIs) from a reference dataset of slow-fast code pairs. At inference time, for a given input code, ECO combines two components: a rule-based symbolic advisor that uses Code Property Graphs to provide a deterministic bottleneck diagnosis, and an ROI retriever that finds relevant past optimization examples based on performance characteristics rather than code similarity. These components are composed into a performance-aware prompt that guides any code-LLM without requiring fine-tuning. Experiments show that ECO significantly improves code optimization, achieving speedups of up to 7.81× while minimizing correctness loss across various models and datasets.2) Strengths\n*   **Novel and Well-Motivated Framework**\n    *   The core contribution of shifting from raw code-pair examples to explicit, performance-aware guidance is a significant conceptual advance over existing methods (Section 1, Figure 1). This directly addresses the limitation that LLMs struggle to infer the *rationale* for an optimization from code alone.\n    *   The dual-module design, combining a deterministic, high-precision symbolic advisor with a flexible, high-coverage ROI retriever, is well-conceived (Section 3, Figure 2). This allows ECO to handle both common, rule-based inefficiencies and more nuanced, context-dependent optimizations.\n    *   The paper clearly positions its contributions against a comprehensive set of prior work, effectively highlighting its unique advantages in providing bottleneck diagnosis and using ROIs as the core optimization knowledge (Table 1).*   **Technically Sound and Well-Executed Components**\n    *   The Symbolic Advisor's use of Code Property Graphs (CPGs) via Joern is a robust and standard approach for deep static code analysis, allowing it to detect structural inefficiencies beyond simple linting (Section 3.2). The provided example for detecting recursion without memoization demonstrates its capability (Algorithm 1, Figure 6).\n    *   The ROI Retriever's mechanism is a key innovation. Instead of retrieving based on code embedding similarity (like standard RAG), it prompts an LLM to generate a performance analysis of the input code and retrieves based on the similarity of this analysis to the distilled ROIs (Section 3.3). This ensures that retrieved examples are relevant to the performance bottleneck, not just syntactically similar.\n    *   The qualitative analysis strongly supports the effectiveness of this performance-aware retrieval, showing that it identifies performance-relevant keywords (e.g., `scanf`, `vector`) while standard RAG focuses on superficial variable names (e.g., `sum`, `cnt`) (Figure 4, Right).*   **Comprehensive and Rigorous Evaluation**\n    *   The experimental setup is thorough, comparing ECO against a wide range of baselines, from generic prompting techniques (Instruction-only, CoT, ICL, RAG) to specialized code optimization methods (Supersonic, SBLLM) (Section 4.1.2, Table 2).\n    *   The use of the gem5 cycle-accurate simulator for runtime measurement provides deterministic and reliable results, which is a significant strength for benchmarking performance improvements and adds to the credibility of the reported speedups (Section 4.1.4).\n    *   The evaluation spans both an in-distribution dataset (a rebalanced version of PIE) and a more challenging out-of-distribution dataset (Codeforces), demonstrating the method's robustness and generalizability (Section 4.4, Table 4). The justification for rebalancing the PIE test set is sound and well-documented (Appendix C, Figure 14).*   **Strong Empirical Results and Insightful Analysis**\n    *   ECO consistently and substantially outperforms all baselines in terms of speedup (SP) and the percentage of optimized solutions (OPT), often with comparable or better accuracy (Table 2). The Best@5 speedup of 3.26× for ECO is a marked improvement over the 2.51× from the best baseline (RAG).\n    *   The ablation study provides clear evidence for the complementary roles of the two modules. The symbolic advisor alone improves accuracy, while the ROI retriever alone improves speedup, and their combination yields the best overall trade-off (Table 3). The result that a naive version of ECO (`w/o RR+SA`) underperforms RAG validates the importance of performance-aware retrieval.\n    *   The framework demonstrates excellent scaling properties. As model size and capability increase (from 3B to 14B and to GPT-series models), the performance gains from ECO become more pronounced, culminating in a 7.81× speedup with GPT-o4-mini (Table 4). This highlights the framework's ability to effectively leverage more powerful models.3) Weaknesses\n*   **Manual Effort in Symbolic Advisor Rule Creation**\n    *   The symbolic advisor relies on rules that are manually created by clustering distilled ROIs and translating them into graph queries and templates (Section 3.2). The paper states, \"We manually cluster similar ROIs and translate each cluster into formal rule–template pairs\".\n    *   This manual process raises concerns about scalability and maintainability. The effort required to create these rules is not quantified, making it difficult to assess the practicality of extending the advisor to new optimization categories, programming languages, or domains.\n    *   The performance of this component is tied to the quality and coverage of the manually curated rules, which may be a bottleneck for the entire system. No direct evidence found in the manuscript.*   **Dependence on High-Quality Reference Data**\n    *   The entire framework is bootstrapped from the PIE HQ dataset of 4,085 slow-fast code pairs (Section 3.1). The quality of the distilled ROIs, and therefore the performance of both the symbolic advisor and the ROI retriever, is contingent on the quality and diversity of this initial dataset.\n    *   The paper does not investigate the sensitivity of ECO to the size or quality of this reference dataset. It is unclear how well the framework would perform if trained on a smaller, noisier, or more domain-specific set of code pairs.\n    *   This dependency might limit the applicability of ECO in scenarios where such high-quality, paired optimization data is not readily available. No direct evidence found in the manuscript.*   **Lack of Analysis on Inference Overhead**\n    *   The ECO framework introduces several steps at inference time that are more computationally intensive than baseline methods. This includes building a CPG and running graph queries for the symbolic advisor, and performing an additional LLM call and embedding search for the ROI retriever (Figure 2, Section 3.3).\n    *   The paper focuses exclusively on the runtime performance of the *generated code* but does not report on the latency or computational cost of the *prompt generation process itself*.\n    *   This omission makes it difficult to evaluate the practical trade-offs of using ECO, especially in time-sensitive or resource-constrained applications where the overhead of generating the prompt might be a significant factor. No direct evidence found in the manuscript.4) Suggestions for Improvement\n*   **Quantify and Discuss the Manual Rule Creation Process**\n    *   The authors should provide more transparency regarding the manual effort involved in creating the symbolic advisor's rules (Section 3.2).\n    *   This could be addressed by adding a paragraph or an appendix section that details the number of rules created, the number of ROI clusters they correspond to, and an estimate of the human effort required. Discussing the potential for semi-automating this rule-creation process would also strengthen the paper.\n    *   Providing the full set of rules in the supplementary material, as mentioned in the paper (\"the complete code is provided in our public repository,\" Section 3.2), will be crucial for reproducibility and for others to build upon this work.*   **Analyze Sensitivity to Reference Data Size and Quality**\n    *   To address the concern about data dependency, the authors could conduct an ablation study on the size of the ROI database.\n    *   This could involve re-running the experiments with ROI databases built from smaller subsets (e.g., 25%, 50%) of the PIE HQ dataset (used in Section 3.1).\n    *   Such an analysis would provide valuable insights into the data efficiency of the framework and its robustness to variations in the availability of reference data.*   **Report and Analyze Inference-Time Overhead**\n    *   The authors should include an analysis of the inference-time latency and/or computational cost of the ECO framework.\n    *   A table or figure could be added to the experiments section (e.g., Section 4) comparing the average time taken to generate a prompt using ECO versus the time for baselines like instruction-only and RAG.\n    *   This would provide a more complete picture of the method's practicality and help users understand the full cost-benefit trade-off.5) Score\n*   Overall (10): 9 — The paper presents a novel, well-motivated, and technically sound framework for code optimization with very strong empirical results and analysis (Table 2, Table 4).\n*   Novelty (10): 9 — The core idea of performance-aware prompting, particularly the performance-based retrieval mechanism, is a significant and original contribution over existing RAG methods (Section 3.3, Figure 4).\n*   Technical Quality (10): 9 — The methodology is robust, and the experimental design is rigorous, employing deterministic simulation, multiple datasets, strong baselines, and insightful ablations (Section 4, Table 3).\n*   Clarity (10): 10 — The paper is exceptionally well-written and organized, with clear figures (Figure 1, Figure 2) and a comprehensive appendix that greatly aids understanding and reproducibility (Appendices A, B, C).\n*   Confidence (5): 5 — I am highly confident in my assessment, as the paper's claims are well-supported by extensive evidence and detailed explanations.",
  "final_review": "1) Summary\nThe paper introduces ECO, a performance-aware prompting framework to improve code runtime optimization for large language models of code (code-LLMs). The authors argue that existing methods, which rely on providing slow-fast code pairs, encourage superficial pattern imitation rather than genuine performance reasoning. ECO first distills human-interpretable Runtime Optimization Instructions (ROIs) from a reference dataset of slow-fast code pairs. At inference time, for a given input code, ECO combines two components: a rule-based symbolic advisor that uses Code Property Graphs to provide a deterministic bottleneck diagnosis, and an ROI retriever that finds relevant past optimization examples based on performance characteristics rather than code similarity. These components are composed into a performance-aware prompt that guides any code-LLM without requiring fine-tuning. Experiments show that ECO significantly improves code optimization, achieving speedups of up to 7.81× while minimizing correctness loss across various models and datasets.2) Strengths\n*   **Novel and Well-Motivated Framework**\n    *   The core motivation of shifting from raw code-pair examples to explicit, performance-aware guidance is a well-motivated approach to address the limitation that LLMs struggle to infer the *rationale* for an optimization from code alone (Section 1, Figure 1).\n    *   The dual-module design, combining a deterministic, high-precision symbolic advisor with a flexible, high-coverage ROI retriever, is well-conceived (Section 3, Figure 2). This allows ECO to handle both common, rule-based inefficiencies and more nuanced, context-dependent optimizations.\n    *   The paper clearly positions its contributions against a comprehensive set of prior work in code optimization, effectively highlighting its unique advantages in providing bottleneck diagnosis and using ROIs as the core optimization knowledge (Table 1).*   **Technically Sound and Well-Executed Components**\n    *   The Symbolic Advisor's use of Code Property Graphs (CPGs) via Joern is a robust and standard approach for deep static code analysis, allowing it to detect structural inefficiencies beyond simple linting (Section 3.2). The provided example for detecting recursion without memoization demonstrates its capability (Algorithm 1, Figure 6).\n    *   The ROI Retriever's mechanism is a key innovation. Instead of retrieving based on code embedding similarity (like standard RAG), it prompts an LLM to generate a performance analysis of the input code and retrieves based on the similarity of this analysis to the distilled ROIs (Section 3.3). This ensures that retrieved examples are relevant to the performance bottleneck, not just syntactically similar.\n    *   The qualitative analysis strongly supports the effectiveness of this performance-aware retrieval, showing that it identifies performance-relevant keywords (e.g., `scanf`, `vector`) while standard RAG focuses on superficial variable names (e.g., `sum`, `cnt`) (Figure 4, Right).*   **Comprehensive and Rigorous Evaluation**\n    *   The experimental setup is thorough, comparing ECO against a wide range of baselines, from generic prompting techniques (Instruction-only, CoT, ICL, RAG) to specialized code optimization methods (Supersonic, SBLLM) (Section 4.1.2, Table 2).\n    *   The use of the gem5 cycle-accurate simulator for runtime measurement provides deterministic and reliable results, which is a significant strength for benchmarking performance improvements and adds to the credibility of the reported speedups (Section 4.1.4).\n    *   The evaluation spans both an in-distribution dataset (a rebalanced version of PIE) and a more challenging out-of-distribution dataset (Codeforces), demonstrating the method's robustness and generalizability (Section 4.4, Table 4). The justification for rebalancing the PIE test set is sound and well-documented (Appendix C, Figure 14).*   **Strong Empirical Results and Insightful Analysis**\n    *   ECO consistently and substantially outperforms all baselines in terms of speedup (SP) and the percentage of optimized solutions (OPT), often with comparable or better accuracy (Table 2). The Best@5 speedup of 3.26× for ECO is a marked improvement over the 2.51× from the best baseline (RAG).\n    *   The ablation study provides clear evidence for the complementary roles of the two modules. The symbolic advisor alone improves accuracy, while the ROI retriever alone improves speedup, and their combination yields the best overall trade-off (Table 3).\n    *   The framework demonstrates excellent scaling properties. As model size and capability increase (from 3B to 14B and to GPT-series models), the performance gains from ECO become more pronounced, culminating in a 7.81× speedup with GPT-o4-mini (Table 4). This highlights the framework's ability to effectively leverage more powerful models.3) Weaknesses\n*   **Manual Effort in Symbolic Advisor Rule Creation**\n    *   The symbolic advisor relies on rules that are manually created by clustering distilled ROIs and translating them into graph queries and templates (Section 3.2). The paper states, \"We manually cluster similar ROIs and translate each cluster into formal rule–template pairs\".\n    *   This manual process raises concerns about scalability and maintainability. The effort required to create these rules is not quantified, making it difficult to assess the practicality of extending the advisor to new optimization categories or programming languages.\n    *   The performance of this component is tied to the quality and coverage of the manually curated rules, which may be a bottleneck for the entire system. No direct evidence found in the manuscript.*   **Dependence on High-Quality Reference Data**\n    *   The entire framework is bootstrapped from the PIE HQ dataset of 4,085 slow-fast code pairs (Section 3.1). The quality of the distilled ROIs, and therefore the performance of both the symbolic advisor and the ROI retriever, is contingent on the quality and diversity of this initial dataset.\n    *   The paper does not investigate the sensitivity of ECO to the size or quality of this reference dataset. It is unclear how well the framework would perform if trained on a smaller, noisier, or more domain-specific set of code pairs.\n    *   This dependency might limit the applicability of ECO in scenarios where such high-quality, paired optimization data is not readily available. No direct evidence found in the manuscript.*   **Lack of Analysis on Inference Overhead**\n    *   The ECO framework introduces several steps at inference time that are more computationally intensive than baseline methods. This includes building a CPG and running graph queries for the symbolic advisor, and performing an additional LLM call and embedding search for the ROI retriever (Figure 2, Section 3.3).\n    *   The paper focuses exclusively on the runtime performance of the *generated code* but does not report on the latency or computational cost of the *prompt generation process itself*.\n    *   This omission makes it difficult to evaluate the practical trade-offs of using ECO, especially in time-sensitive applications where the overhead of generating the prompt might be a significant factor. No direct evidence found in the manuscript.*   **Misleading and Contradictory Figures**\n    *   The central motivating figure (Figure 1) contains a significant inconsistency. The \"Generated Code\" for the baseline method is identical to the code generated by ECO. However, the baseline's output is labeled a failure with a factually incorrect comment `// ✗ O(n²) computation` (the code is O(n)), while ECO's identical output is labeled a success. This misrepresents the comparison and undermines the paper's core motivation.\n    *   The example provided to illustrate the Symbolic Advisor in Appendix A.2.2 (Figure 7) is internally contradictory. The \"Input Code\" contains the line `v.push_back(i*i);`, which is a dynamic operation, yet the \"Bottleneck Diagnosis\" claims \"The following vectors do not use dynamic operations\". This calls into question the correctness of the examples used to validate the method.*   **Inconsistencies in Data and Results Reporting**\n    *   The description of the reference dataset is confusing. Appendix C states that the \"PIE-HQ\" dataset is a \"pruned\" subset of the PIE training data, but Table 5 lists both \"PIE-Train\" and \"PIE-HQ\" with an identical sample count of 4,085. This is a contradiction that makes the data's origin unclear.\n    *   There is a numerical discrepancy in the main results. Table 2 reports the Best@1 accuracy for ECO as 36.27%, whereas the corresponding bars in Figure 3 (\"Correct but Not Optimized\" + \"Optimized\" + \"Faster than Human\") sum to 34.2% (10.7% + 10.8% + 12.7%). This suggests a lack of care in reporting.*   **Narrow Framing of Novelty**\n    *   The paper frames its novelty primarily against other methods in the specific domain of code optimization (Table 1).\n    *   The Related Work section does not discuss or cite prior work on broader prompt engineering concepts, such as neural-symbolic methods or architectures where an external module generates a guiding prompt for a frozen LLM.\n    *   This omission makes it difficult to assess the conceptual novelty of the overall framework architecture beyond its specific, albeit effective, application to code optimization.4) Suggestions for Improvement\n*   **Quantify and Discuss the Manual Rule Creation Process**\n    *   The authors should provide more transparency regarding the manual effort involved in creating the symbolic advisor's rules (Section 3.2).\n    *   This could be addressed by adding a paragraph or an appendix section that details the number of rules created, the number of ROI clusters they correspond to, and an estimate of the human effort required. Discussing the potential for semi-automating this rule-creation process would also strengthen the paper.\n    *   Providing the full set of rules in the supplementary material, as mentioned in the paper (\"the complete code is provided in our public repository,\" Section 3.2), will be crucial for reproducibility.*   **Analyze Sensitivity to Reference Data Size and Quality**\n    *   To address the concern about data dependency, the authors could conduct an ablation study on the size of the ROI database.\n    *   This could involve re-running the experiments with ROI databases built from smaller subsets (e.g., 25%, 50%) of the PIE HQ dataset (used in Section 3.1).\n    *   Such an analysis would provide valuable insights into the data efficiency of the framework and its robustness to variations in the availability of reference data.*   **Report and Analyze Inference-Time Overhead**\n    *   The authors should include an analysis of the inference-time latency and/or computational cost of the ECO framework.\n    *   A table or figure could be added to the experiments section (e.g., Section 4) comparing the average time taken to generate a prompt using ECO versus the time for baselines like instruction-only and RAG.\n    *   This would provide a more complete picture of the method's practicality and help users understand the full cost-benefit trade-off.*   **Correct Misleading and Contradictory Figures**\n    *   Revise Figure 1 to accurately and fairly represent the performance and complexity of both the baseline and ECO methods. The example should not be self-contradictory or rely on factually incorrect annotations to make its point.\n    *   Correct the example in Appendix A.2.2 (Figure 7) to ensure the provided \"Input Code\" is consistent with the \"Bottleneck Diagnosis\". A correct example would strengthen the claims about the symbolic advisor's capabilities.*   **Improve Reporting Rigor and Data Clarity**\n    *   Clarify the relationship between the \"PIE-Train\" and \"PIE-HQ\" datasets in Appendix C and Table 5. If they are identical, the description of \"pruning\" should be revised or explained.\n    *   Reconcile the numerical values for Best@1 accuracy between Table 2 and Figure 3 and correct any inconsistencies to ensure all reported results are consistent with each other.*   **Broaden Discussion of Related Work**\n    *   Expand the Related Work section to include a discussion of relevant high-level prompt engineering frameworks.\n    *   This would help to more precisely situate the paper's contributions, clarifying which aspects of the architecture are novel applications of existing concepts versus which are entirely new contributions to the field.5) Score\n*   Overall (10): 7 — The paper presents a promising framework with strong empirical results, but significant flaws in its motivating examples, reporting, and novelty framing need to be addressed (Figure 1, Figure 7, Table 5).\n*   Novelty (10): 7 — The performance-aware retrieval mechanism is a good contribution, but the overall architectural pattern is not new and the paper fails to position itself within the broader prompt engineering literature (Section 2).\n*   Technical Quality (10): 7 — The experimental design is mostly strong, but is undermined by contradictory examples, inconsistent data descriptions, and numerical discrepancies that suggest a lack of rigor (Figure 7, Table 5, Table 2 vs Figure 3).\n*   Clarity (10): 7 — While generally well-written, the paper suffers from major clarity issues due to a misleading core figure and contradictory technical examples that confuse the reader (Figure 1, Figure 7).\n*   Confidence (5): 5 — I am highly confident in my assessment, as the weaknesses are based on verifiable inconsistencies found directly within the manuscript.",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 9,
        "novelty": 9,
        "technical_quality": 9,
        "clarity": 10,
        "confidence": 5
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 7,
        "novelty": 7,
        "technical_quality": 7,
        "clarity": 7,
        "confidence": 5
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "1) Summary\nThe paper introduces ECO, a performance-aware prompting framework to improve code runtime optimization for large language models of code (code-LLMs). The authors argue that existing methods, which rely on providing slow-fast code pairs, encourage superficial pattern imitation rather than genuine performance reasoning. ECO first distills human-interpretable Runtime Optimization Instructions (ROIs) from a reference dataset of slow-fast code pairs. At inference time, for a given input code, ECO combines two components: a rule-based symbolic advisor that uses Code Property Graphs to provide a deterministic bottleneck diagnosis, and an ROI retriever that finds relevant past optimization examples based on performance characteristics rather than code similarity. These components are composed into a performance-aware prompt that guides any code-LLM without requiring fine-tuning. Experiments show that ECO significantly improves code optimization, achieving speedups of up to 7.81× while minimizing correctness loss across various models and datasets.2) Strengths\n*   **Novel and Well-Motivated Framework**\n    *   The core motivation of shifting from raw code-pair examples to explicit, performance-aware guidance is a well-motivated approach to address the limitation that LLMs struggle to infer the *rationale* for an optimization from code alone (Section 1, Figure 1).\n    *   The dual-module design, combining a deterministic, high-precision symbolic advisor with a flexible, high-coverage ROI retriever, is well-conceived (Section 3, Figure 2). This allows ECO to handle both common, rule-based inefficiencies and more nuanced, context-dependent optimizations.\n    *   The paper clearly positions its contributions against a comprehensive set of prior work in code optimization, effectively highlighting its unique advantages in providing bottleneck diagnosis and using ROIs as the core optimization knowledge (Table 1).*   **Technically Sound and Well-Executed Components**\n    *   The Symbolic Advisor's use of Code Property Graphs (CPGs) via Joern is a robust and standard approach for deep static code analysis, allowing it to detect structural inefficiencies beyond simple linting (Section 3.2). The provided example for detecting recursion without memoization demonstrates its capability (Algorithm 1, Figure 6).\n    *   The ROI Retriever's mechanism is a key innovation. Instead of retrieving based on code embedding similarity (like standard RAG), it prompts an LLM to generate a performance analysis of the input code and retrieves based on the similarity of this analysis to the distilled ROIs (Section 3.3). This ensures that retrieved examples are relevant to the performance bottleneck, not just syntactically similar.\n    *   The qualitative analysis strongly supports the effectiveness of this performance-aware retrieval, showing that it identifies performance-relevant keywords (e.g., `scanf`, `vector`) while standard RAG focuses on superficial variable names (e.g., `sum`, `cnt`) (Figure 4, Right).*   **Comprehensive and Rigorous Evaluation**\n    *   The experimental setup is thorough, comparing ECO against a wide range of baselines, from generic prompting techniques (Instruction-only, CoT, ICL, RAG) to specialized code optimization methods (Supersonic, SBLLM) (Section 4.1.2, Table 2).\n    *   The use of the gem5 cycle-accurate simulator for runtime measurement provides deterministic and reliable results, which is a significant strength for benchmarking performance improvements and adds to the credibility of the reported speedups (Section 4.1.4).\n    *   The evaluation spans both an in-distribution dataset (a rebalanced version of PIE) and a more challenging out-of-distribution dataset (Codeforces), demonstrating the method's robustness and generalizability (Section 4.4, Table 4). The justification for rebalancing the PIE test set is sound and well-documented (Appendix C, Figure 14).*   **Strong Empirical Results and Insightful Analysis**\n    *   ECO consistently and substantially outperforms all baselines in terms of speedup (SP) and the percentage of optimized solutions (OPT), often with comparable or better accuracy (Table 2). The Best@5 speedup of 3.26× for ECO is a marked improvement over the 2.51× from the best baseline (RAG).\n    *   The ablation study provides clear evidence for the complementary roles of the two modules. The symbolic advisor alone improves accuracy, while the ROI retriever alone improves speedup, and their combination yields the best overall trade-off (Table 3).\n    *   The framework demonstrates excellent scaling properties. As model size and capability increase (from 3B to 14B and to GPT-series models), the performance gains from ECO become more pronounced, culminating in a 7.81× speedup with GPT-o4-mini (Table 4). This highlights the framework's ability to effectively leverage more powerful models.3) Weaknesses\n*   **Manual Effort in Symbolic Advisor Rule Creation**\n    *   The symbolic advisor relies on rules that are manually created by clustering distilled ROIs and translating them into graph queries and templates (Section 3.2). The paper states, \"We manually cluster similar ROIs and translate each cluster into formal rule–template pairs\".\n    *   This manual process raises concerns about scalability and maintainability. The effort required to create these rules is not quantified, making it difficult to assess the practicality of extending the advisor to new optimization categories or programming languages.\n    *   The performance of this component is tied to the quality and coverage of the manually curated rules, which may be a bottleneck for the entire system. No direct evidence found in the manuscript.*   **Dependence on High-Quality Reference Data**\n    *   The entire framework is bootstrapped from the PIE HQ dataset of 4,085 slow-fast code pairs (Section 3.1). The quality of the distilled ROIs, and therefore the performance of both the symbolic advisor and the ROI retriever, is contingent on the quality and diversity of this initial dataset.\n    *   The paper does not investigate the sensitivity of ECO to the size or quality of this reference dataset. It is unclear how well the framework would perform if trained on a smaller, noisier, or more domain-specific set of code pairs.\n    *   This dependency might limit the applicability of ECO in scenarios where such high-quality, paired optimization data is not readily available. No direct evidence found in the manuscript.*   **Lack of Analysis on Inference Overhead**\n    *   The ECO framework introduces several steps at inference time that are more computationally intensive than baseline methods. This includes building a CPG and running graph queries for the symbolic advisor, and performing an additional LLM call and embedding search for the ROI retriever (Figure 2, Section 3.3).\n    *   The paper focuses exclusively on the runtime performance of the *generated code* but does not report on the latency or computational cost of the *prompt generation process itself*.\n    *   This omission makes it difficult to evaluate the practical trade-offs of using ECO, especially in time-sensitive applications where the overhead of generating the prompt might be a significant factor. No direct evidence found in the manuscript.*   **Misleading and Contradictory Figures**\n    *   The central motivating figure (Figure 1) contains a significant inconsistency. The \"Generated Code\" for the baseline method is identical to the code generated by ECO. However, the baseline's output is labeled a failure with a factually incorrect comment `// ✗ O(n²) computation` (the code is O(n)), while ECO's identical output is labeled a success. This misrepresents the comparison and undermines the paper's core motivation.\n    *   The example provided to illustrate the Symbolic Advisor in Appendix A.2.2 (Figure 7) is internally contradictory. The \"Input Code\" contains the line `v.push_back(i*i);`, which is a dynamic operation, yet the \"Bottleneck Diagnosis\" claims \"The following vectors do not use dynamic operations\". This calls into question the correctness of the examples used to validate the method.*   **Inconsistencies in Data and Results Reporting**\n    *   The description of the reference dataset is confusing. Appendix C states that the \"PIE-HQ\" dataset is a \"pruned\" subset of the PIE training data, but Table 5 lists both \"PIE-Train\" and \"PIE-HQ\" with an identical sample count of 4,085. This is a contradiction that makes the data's origin unclear.\n    *   There is a numerical discrepancy in the main results. Table 2 reports the Best@1 accuracy for ECO as 36.27%, whereas the corresponding bars in Figure 3 (\"Correct but Not Optimized\" + \"Optimized\" + \"Faster than Human\") sum to 34.2% (10.7% + 10.8% + 12.7%). This suggests a lack of care in reporting.*   **Narrow Framing of Novelty**\n    *   The paper frames its novelty primarily against other methods in the specific domain of code optimization (Table 1).\n    *   The Related Work section does not discuss or cite prior work on broader prompt engineering concepts, such as neural-symbolic methods or architectures where an external module generates a guiding prompt for a frozen LLM.\n    *   This omission makes it difficult to assess the conceptual novelty of the overall framework architecture beyond its specific, albeit effective, application to code optimization.4) Suggestions for Improvement\n*   **Quantify and Discuss the Manual Rule Creation Process**\n    *   The authors should provide more transparency regarding the manual effort involved in creating the symbolic advisor's rules (Section 3.2).\n    *   This could be addressed by adding a paragraph or an appendix section that details the number of rules created, the number of ROI clusters they correspond to, and an estimate of the human effort required. Discussing the potential for semi-automating this rule-creation process would also strengthen the paper.\n    *   Providing the full set of rules in the supplementary material, as mentioned in the paper (\"the complete code is provided in our public repository,\" Section 3.2), will be crucial for reproducibility.*   **Analyze Sensitivity to Reference Data Size and Quality**\n    *   To address the concern about data dependency, the authors could conduct an ablation study on the size of the ROI database.\n    *   This could involve re-running the experiments with ROI databases built from smaller subsets (e.g., 25%, 50%) of the PIE HQ dataset (used in Section 3.1).\n    *   Such an analysis would provide valuable insights into the data efficiency of the framework and its robustness to variations in the availability of reference data.*   **Report and Analyze Inference-Time Overhead**\n    *   The authors should include an analysis of the inference-time latency and/or computational cost of the ECO framework.\n    *   A table or figure could be added to the experiments section (e.g., Section 4) comparing the average time taken to generate a prompt using ECO versus the time for baselines like instruction-only and RAG.\n    *   This would provide a more complete picture of the method's practicality and help users understand the full cost-benefit trade-off.*   **Correct Misleading and Contradictory Figures**\n    *   Revise Figure 1 to accurately and fairly represent the performance and complexity of both the baseline and ECO methods. The example should not be self-contradictory or rely on factually incorrect annotations to make its point.\n    *   Correct the example in Appendix A.2.2 (Figure 7) to ensure the provided \"Input Code\" is consistent with the \"Bottleneck Diagnosis\". A correct example would strengthen the claims about the symbolic advisor's capabilities.*   **Improve Reporting Rigor and Data Clarity**\n    *   Clarify the relationship between the \"PIE-Train\" and \"PIE-HQ\" datasets in Appendix C and Table 5. If they are identical, the description of \"pruning\" should be revised or explained.\n    *   Reconcile the numerical values for Best@1 accuracy between Table 2 and Figure 3 and correct any inconsistencies to ensure all reported results are consistent with each other.*   **Broaden Discussion of Related Work**\n    *   Expand the Related Work section to include a discussion of relevant high-level prompt engineering frameworks.\n    *   This would help to more precisely situate the paper's contributions, clarifying which aspects of the architecture are novel applications of existing concepts versus which are entirely new contributions to the field.5) Score\n*   Overall (10): 7 — The paper presents a promising framework with strong empirical results, but significant flaws in its motivating examples, reporting, and novelty framing need to be addressed (Figure 1, Figure 7, Table 5).\n*   Novelty (10): 7 — The performance-aware retrieval mechanism is a good contribution, but the overall architectural pattern is not new and the paper fails to position itself within the broader prompt engineering literature (Section 2).\n*   Technical Quality (10): 7 — The experimental design is mostly strong, but is undermined by contradictory examples, inconsistent data descriptions, and numerical discrepancies that suggest a lack of rigor (Figure 7, Table 5, Table 2 vs Figure 3).\n*   Clarity (10): 7 — While generally well-written, the paper suffers from major clarity issues due to a misleading core figure and contradictory technical examples that confuse the reader (Figure 1, Figure 7).\n*   Confidence (5): 5 — I am highly confident in my assessment, as the weaknesses are based on verifiable inconsistencies found directly within the manuscript."
}