1) Summary
The paper introduces ECO, a performance-aware prompting framework to improve code runtime optimization for large language models of code (code-LLMs). The authors argue that existing methods, which rely on providing slow-fast code pairs, encourage superficial pattern imitation rather than genuine performance reasoning. ECO first distills human-interpretable Runtime Optimization Instructions (ROIs) from a reference dataset of slow-fast code pairs. At inference time, for a given input code, ECO combines two components: a rule-based symbolic advisor that uses Code Property Graphs to provide a deterministic bottleneck diagnosis, and an ROI retriever that finds relevant past optimization examples based on performance characteristics rather than code similarity. These components are composed into a performance-aware prompt that guides any code-LLM without requiring fine-tuning. Experiments show that ECO significantly improves code optimization, achieving speedups of up to 7.81× while minimizing correctness loss across various models and datasets.2) Strengths
*   **Novel and Well-Motivated Framework**
    *   The core contribution of shifting from raw code-pair examples to explicit, performance-aware guidance is a significant conceptual advance over existing methods (Section 1, Figure 1). This directly addresses the limitation that LLMs struggle to infer the *rationale* for an optimization from code alone.
    *   The dual-module design, combining a deterministic, high-precision symbolic advisor with a flexible, high-coverage ROI retriever, is well-conceived (Section 3, Figure 2). This allows ECO to handle both common, rule-based inefficiencies and more nuanced, context-dependent optimizations.
    *   The paper clearly positions its contributions against a comprehensive set of prior work, effectively highlighting its unique advantages in providing bottleneck diagnosis and using ROIs as the core optimization knowledge (Table 1).*   **Technically Sound and Well-Executed Components**
    *   The Symbolic Advisor's use of Code Property Graphs (CPGs) via Joern is a robust and standard approach for deep static code analysis, allowing it to detect structural inefficiencies beyond simple linting (Section 3.2). The provided example for detecting recursion without memoization demonstrates its capability (Algorithm 1, Figure 6).
    *   The ROI Retriever's mechanism is a key innovation. Instead of retrieving based on code embedding similarity (like standard RAG), it prompts an LLM to generate a performance analysis of the input code and retrieves based on the similarity of this analysis to the distilled ROIs (Section 3.3). This ensures that retrieved examples are relevant to the performance bottleneck, not just syntactically similar.
    *   The qualitative analysis strongly supports the effectiveness of this performance-aware retrieval, showing that it identifies performance-relevant keywords (e.g., `scanf`, `vector`) while standard RAG focuses on superficial variable names (e.g., `sum`, `cnt`) (Figure 4, Right).*   **Comprehensive and Rigorous Evaluation**
    *   The experimental setup is thorough, comparing ECO against a wide range of baselines, from generic prompting techniques (Instruction-only, CoT, ICL, RAG) to specialized code optimization methods (Supersonic, SBLLM) (Section 4.1.2, Table 2).
    *   The use of the gem5 cycle-accurate simulator for runtime measurement provides deterministic and reliable results, which is a significant strength for benchmarking performance improvements and adds to the credibility of the reported speedups (Section 4.1.4).
    *   The evaluation spans both an in-distribution dataset (a rebalanced version of PIE) and a more challenging out-of-distribution dataset (Codeforces), demonstrating the method's robustness and generalizability (Section 4.4, Table 4). The justification for rebalancing the PIE test set is sound and well-documented (Appendix C, Figure 14).*   **Strong Empirical Results and Insightful Analysis**
    *   ECO consistently and substantially outperforms all baselines in terms of speedup (SP) and the percentage of optimized solutions (OPT), often with comparable or better accuracy (Table 2). The Best@5 speedup of 3.26× for ECO is a marked improvement over the 2.51× from the best baseline (RAG).
    *   The ablation study provides clear evidence for the complementary roles of the two modules. The symbolic advisor alone improves accuracy, while the ROI retriever alone improves speedup, and their combination yields the best overall trade-off (Table 3). The result that a naive version of ECO (`w/o RR+SA`) underperforms RAG validates the importance of performance-aware retrieval.
    *   The framework demonstrates excellent scaling properties. As model size and capability increase (from 3B to 14B and to GPT-series models), the performance gains from ECO become more pronounced, culminating in a 7.81× speedup with GPT-o4-mini (Table 4). This highlights the framework's ability to effectively leverage more powerful models.3) Weaknesses
*   **Manual Effort in Symbolic Advisor Rule Creation**
    *   The symbolic advisor relies on rules that are manually created by clustering distilled ROIs and translating them into graph queries and templates (Section 3.2). The paper states, "We manually cluster similar ROIs and translate each cluster into formal rule–template pairs".
    *   This manual process raises concerns about scalability and maintainability. The effort required to create these rules is not quantified, making it difficult to assess the practicality of extending the advisor to new optimization categories, programming languages, or domains.
    *   The performance of this component is tied to the quality and coverage of the manually curated rules, which may be a bottleneck for the entire system. No direct evidence found in the manuscript.*   **Dependence on High-Quality Reference Data**
    *   The entire framework is bootstrapped from the PIE HQ dataset of 4,085 slow-fast code pairs (Section 3.1). The quality of the distilled ROIs, and therefore the performance of both the symbolic advisor and the ROI retriever, is contingent on the quality and diversity of this initial dataset.
    *   The paper does not investigate the sensitivity of ECO to the size or quality of this reference dataset. It is unclear how well the framework would perform if trained on a smaller, noisier, or more domain-specific set of code pairs.
    *   This dependency might limit the applicability of ECO in scenarios where such high-quality, paired optimization data is not readily available. No direct evidence found in the manuscript.*   **Lack of Analysis on Inference Overhead**
    *   The ECO framework introduces several steps at inference time that are more computationally intensive than baseline methods. This includes building a CPG and running graph queries for the symbolic advisor, and performing an additional LLM call and embedding search for the ROI retriever (Figure 2, Section 3.3).
    *   The paper focuses exclusively on the runtime performance of the *generated code* but does not report on the latency or computational cost of the *prompt generation process itself*.
    *   This omission makes it difficult to evaluate the practical trade-offs of using ECO, especially in time-sensitive or resource-constrained applications where the overhead of generating the prompt might be a significant factor. No direct evidence found in the manuscript.4) Suggestions for Improvement
*   **Quantify and Discuss the Manual Rule Creation Process**
    *   The authors should provide more transparency regarding the manual effort involved in creating the symbolic advisor's rules (Section 3.2).
    *   This could be addressed by adding a paragraph or an appendix section that details the number of rules created, the number of ROI clusters they correspond to, and an estimate of the human effort required. Discussing the potential for semi-automating this rule-creation process would also strengthen the paper.
    *   Providing the full set of rules in the supplementary material, as mentioned in the paper ("the complete code is provided in our public repository," Section 3.2), will be crucial for reproducibility and for others to build upon this work.*   **Analyze Sensitivity to Reference Data Size and Quality**
    *   To address the concern about data dependency, the authors could conduct an ablation study on the size of the ROI database.
    *   This could involve re-running the experiments with ROI databases built from smaller subsets (e.g., 25%, 50%) of the PIE HQ dataset (used in Section 3.1).
    *   Such an analysis would provide valuable insights into the data efficiency of the framework and its robustness to variations in the availability of reference data.*   **Report and Analyze Inference-Time Overhead**
    *   The authors should include an analysis of the inference-time latency and/or computational cost of the ECO framework.
    *   A table or figure could be added to the experiments section (e.g., Section 4) comparing the average time taken to generate a prompt using ECO versus the time for baselines like instruction-only and RAG.
    *   This would provide a more complete picture of the method's practicality and help users understand the full cost-benefit trade-off.5) Score
*   Overall (10): 9 — The paper presents a novel, well-motivated, and technically sound framework for code optimization with very strong empirical results and analysis (Table 2, Table 4).
*   Novelty (10): 9 — The core idea of performance-aware prompting, particularly the performance-based retrieval mechanism, is a significant and original contribution over existing RAG methods (Section 3.3, Figure 4).
*   Technical Quality (10): 9 — The methodology is robust, and the experimental design is rigorous, employing deterministic simulation, multiple datasets, strong baselines, and insightful ablations (Section 4, Table 3).
*   Clarity (10): 10 — The paper is exceptionally well-written and organized, with clear figures (Figure 1, Figure 2) and a comprehensive appendix that greatly aids understanding and reproducibility (Appendices A, B, C).
*   Confidence (5): 5 — I am highly confident in my assessment, as the paper's claims are well-supported by extensive evidence and detailed explanations.