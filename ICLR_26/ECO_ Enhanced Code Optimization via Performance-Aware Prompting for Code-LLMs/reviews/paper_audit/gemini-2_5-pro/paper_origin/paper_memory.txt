# Global Summary
This paper introduces ECO, a performance-aware prompting framework to improve code runtime optimization using large language models (LLMs). The core problem is that existing LLM-based methods, which rely on slow-fast code pairs, encourage superficial pattern imitation rather than genuine performance reasoning. ECO addresses this by first distilling "Runtime Optimization Instructions" (ROIs) from reference code pairs. At inference time, for a given input code, ECO generates a performance-aware prompt by combining outputs from two modules: (1) a symbolic advisor that uses Code Property Graphs (CPGs) to deterministically diagnose structural bottlenecks, and (2) an ROI retriever that finds relevant past optimization examples based on performance characteristics, not just code similarity. This composite prompt provides actionable guidance to any code-LLM without requiring fine-tuning. ECO is evaluated on the PIE and Codeforces C++ benchmarks across various open and closed-source LLMs. The framework demonstrates significant performance gains, achieving up to a 7.81× speedup with GPT-o4-mini, a nearly four-fold improvement over standard prompting which yielded a 1.99x speedup.

# Abstract
The paper addresses the challenge of code runtime optimization, which requires reasoning about performance trade-offs. Current methods using slow-fast code pairs as examples are limited because they obscure the causal reasons for performance gains, leading to pattern imitation. The proposed solution is ECO, a performance-aware prompting framework. ECO first distills Runtime Optimization Instructions (ROIs) from reference slow-fast code pairs, which describe the root causes of inefficiency and optimization rationales. For a new input code, ECO uses a symbolic advisor for bottleneck diagnosis and an ROI retriever to find related ROIs. These are combined into a performance-aware prompt for a code-LLM. ECO's prompts are model-agnostic, require no fine-tuning, and can be prepended to any prompt. Empirical studies show ECO significantly improves code efficiency, achieving speedups up to 7.81× while minimizing correctness loss.

# Introduction
- Code runtime optimization is a key problem in software engineering, but generating efficient code is challenging for code-LLMs which excel at correctness but not performance reasoning.
- Traditional compiler techniques (e.g., loop unrolling) are effective for low-level issues but miss program-level optimizations like algorithmic restructuring.
- Recent LLM-based methods use slow-fast code pairs via in-context learning (ICL), retrieval-augmented generation (RAG), or fine-tuning.
- A fundamental limitation of these methods is that they present raw transformation examples, forcing the LLM to infer the optimization rationale, which is often beyond its capability. This can lead to retrieving misaligned examples or memorizing superficial edits.
- The paper proposes ECO, a framework that provides explicit optimization insights. ECO first distills Runtime Optimization Instructions (ROIs) from reference pairs into a knowledge base.
- At inference time, ECO uses two modules:
    1.  A symbolic advisor uses Code Property Graph (CPG) queries to deterministically find structural inefficiencies and generate a bottleneck diagnosis.
    2.  An ROI retriever finds relevant ROIs from the knowledge base to offer generalizable guidance.
- These components are combined into a performance-aware prompt that localizes bottlenecks and prescribes transformations, which any code-LLM can use without fine-tuning.
- On GPT-4o-mini, standard prompting achieves a 1.99× speedup, while ECO achieves 7.81×.

# Related Work
- Traditional compilers like LLVM and GCC are limited to low-level, rule-based optimizations and cannot handle program-level, context-dependent bottlenecks.
- LLM-based methods include fine-tuning on slow-fast pairs (PIE) or diff patches (Supersonic), which is costly and provides limited gains as it doesn't expose optimization rationale.
- SBLLM uses RAG with iterative revision, which increases inference cost and risks semantic drift.
- Existing LLM approaches use surface-level transformations (code pairs, diffs) without providing bottleneck diagnoses or direct optimization guidance.
- ECO is distinct because it introduces ROIs and provides bottleneck diagnoses, offering richer and more targeted information.
- Table 1 contrasts ECO with other methods, highlighting that ECO is the only one that is train-free, non-iterative, and provides bottleneck diagnosis using ROIs.

# Method
- ECO is a performance-aware prompting framework that generates actionable hints for code-LLMs. It provides two forms of guidance: a bottleneck diagnosis and related ROIs.
- The framework has an offline ROI distillation step and two online modules for prompt generation: a symbolic advisor and an ROI retriever.

### 3.1 ROI DISTILLATION
- A database of Runtime Optimization Instructions (ROIs) is constructed from the PIE HQ dataset, which contains 4,085 slow-fast C++ code pairs.
- For each pair, a reasoning-oriented LLM is prompted to analyze the code and extract a compact, natural-language ROI (O_i) that explains the root cause of inefficiency and the fix.
- The final ROI database consists of triplets D = {(slow_i, fast_i, O_i)}, converting raw code pairs into structured, reusable optimization knowledge.

### 3.2 SYMBOLIC ADVISOR
- This module uses rule-based queries on a Code Property Graph (CPG) of the input code to find deterministic inefficiency patterns. It uses the Joern tool to build CPGs.
- Rules and templates are manually created by clustering the distilled ROIs. This allows the advisor to provide precise, program-level bottleneck diagnoses.
- The rule set covers four categories: (i) Inefficient Algorithms, (ii) Data Structure Usage, (iii) Library Usage, and (iv) Loop Structures.
- Algorithm 1 provides an example rule to detect recursion without memoization by analyzing self-calls and read/write operations within a function's CPG.
- Algorithm 2 shows the pipeline: build CPG, apply rules to find matches, and instantiate templates to generate a set of bottleneck diagnoses.

### 3.3 ROI RETRIEVER
- This module complements the symbolic advisor by providing guidance from past optimizations.
- Unlike traditional RAG that uses code similarity, the ROI retriever first prompts the inference model to generate a performance-aware description (E_C) of the input code C.
- This description is embedded (v_C = Φ(E_C)) and compared against the embeddings of all ROIs (v_i = Φ(O_i)) in the database.
- The top-k most similar entries are retrieved, providing the LLM with relevant slow-fast code pairs and their corresponding interpretable ROI descriptions.

# Experiments
### 4.1 EXPERIMENTAL SETTINGS
- **Models**:
    - Reasoning model for ROI distillation: DeepSeek-r1:32b.
    - Embedding model for retrieval: Qodo-Embed-1-1:5b.
    - Inference models: Qwen2.5-Coder family (3b, 7b, 14b), GPT-4o-mini, and GPT-o4-mini. Temperature is 0.7.
- **Baselines**:
    - Generic: Instruction-only, Chain-of-Thought (CoT), In-Context Learning (ICL), RAG.
    - Optimization-specific: Supersonic, SBLLM.
- **Dataset**:
    - Reference/Training: PIE HQ dataset (4,085 slow-fast C++ pairs).
    - Evaluation (In-distribution): A balanced subset of the PIE test set with 255 slow codes.
    - Evaluation (Out-of-distribution): A curated Codeforces C++ dataset with 300 slow codes. Each evaluation sample has 10 test cases.
- **Evaluation Metrics**:
    - Percent optimized (OPT): Percentage of correct solutions at least 10% faster.
    - Speedup rate (SP): T(original)/T(new). SP is 1.0 if incorrect or slower.
    - Accuracy (ACC): Percentage of functionally correct optimized codes.
    - All evaluations use the best@k protocol. Runtimes are measured with the gem5 cycle-accurate simulator. C++ code is compiled with GCC 9.3.0 and -O3.

### 4.2 COMPARISON WITH BASELINES
- On the PIE dataset with Qwen2.5-Coder-7B (Table 2), ECO achieves the highest performance.
- Best@5 results for ECO: 74.24% ACC, 3.26× SP, 48.04% OPT.
- This outperforms RAG (64.51% ACC, 2.51× SP, 30.31% OPT) and ICL (70.75% ACC, 1.82× SP, 23.10% OPT).
- Supersonic and SBLLM perform poorly due to very low accuracy. Supersonic produces malformed patches over 80% of the time, and SBLLM's iterative process leads to semantic degradation.

### 4.3 ABLATION STUDIES: ROLE OF SUBMODULES IN ECO
- An ablation study (Table 3) shows both the symbolic advisor (SA) and ROI retriever (RR) are crucial.
- ECO w/o RR (SA only): Achieves the highest accuracy (83.45% Best@5 ACC) due to its deterministic nature but lower speedup (3.08x).
- ECO w/o SA (RR only): Achieves comparable speedup (3.10x) but lower accuracy (70.39%).
- Full ECO: Achieves the best speedup (3.26x) with good accuracy (74.24%), showing the modules are complementary.
- ECO w/o RR+SA (retrieving code-similar pairs with their ROIs): Performs worse than RAG, suggesting that ungrounded ROIs can be detrimental.

### 4.4 GENERALIZABILITY OF ECO
- **Model Scaling (Table 4)**: ECO's effectiveness increases with model size. With Qwen2.5-Coder-14b, ECO achieves 3.67× SP, compared to 1.48× for instruction-only.
- **Closed-Source Models**: ECO is model-agnostic. On GPT-o4-mini, instruction-only yields 1.99× SP, while ECO achieves 7.81× SP. On GPT-4o-mini, ECO improves SP from 1.53x to 3.97x.
- **Out-of-Distribution (Table 4)**: On the Codeforces benchmark, ECO provides robust improvements. For GPT-4o-mini, SP improves from 1.01× to 2.01×. For GPT-o4-mini, SP improves from 1.41x to 4.55x.

### 4.5 CASE STUDY
- **Output Distribution (Fig 3)**: ECO produces a much higher proportion of "Optimized" outputs compared to baselines. Baselines often produce "correct but not optimized" code.
- **Bottleneck Analysis (Fig 4, Left)**: The symbolic advisor finds that the most common bottlenecks are "Library usage" (56.4% on PIE, 48.6% on Codeforces), primarily I/O, and "Algorithm" (29.8% on PIE, 32.4% on Codeforces).
- **Retrieval Quality (Fig 4, Right)**: The ROI retriever retrieves code with performance-relevant keyword overlaps (e.g., `scanf`, `cin`, `vector`). In contrast, RAG retrieves code with superficial overlaps like variable names (e.g., `cnt`, `ans`, `sum`).

# Conclusion
The paper proposes ECO, a performance-aware prompting framework for code optimization that distills ROIs from reference pairs. By combining a symbolic advisor for bottleneck diagnosis and an ROI retriever for contextual guidance, ECO generates prompts that significantly improve the runtime efficiency of code generated by various LLMs. The approach is model-agnostic, requires no fine-tuning, and is shown to be practical and effective.

# Appendix
- **A.1 ROI Distillation**: Uses DeepSeek-r1:32b (Q4_K_M quantized) on 4,085 PIE HQ pairs. A prompt template is used to extract a JSON object for each optimization, containing a description, runtime improvement rating, and category.
- **A.2 Symbolic Advisor**: Uses Joern for CPG analysis. Rules are implemented in Scala and manually derived from clustered ROIs. Four categories of rules are detailed with examples: Inefficient Algorithms (e.g., recursion without memoization), Suboptimal Data Structure Usage (e.g., `std::vector` instead of static array), Inefficient Library Usage (e.g., `cin`/`cout` vs. `scanf`/`printf`), and Inefficient Loop Usage (e.g., loop-invariant code).
- **A.3 ROI Retriever**: Uses the inference model to generate a performance analysis of the input code. This analysis is embedded using Qodo/Qodo-Embed-1-1.5B and compared to ROI embeddings for retrieval. An example shows it retrieves based on performance themes (I/O, string handling) rather than functional similarity.
- **B Implementation Details**: Inference models (Qwen2.5-Coder family) run via ollama with Q4_K_M quantization and temperature 0.7. Prompt templates for ECO, generic baselines, and retrieval baselines are provided.
- **B.3 Detailed Settings**: Supersonic uses the official trained CodeBERT model. SBLLM follows the official implementation with 4 iterations. Fine-tuning experiments follow the PIE implementation, training Qwen2.5-Coder-7B on the PIE HQ dataset.
- **B.3.4 Runtime Measurements**: Uses the gem5 system simulator with an Intel Skylake configuration for deterministic, cycle-accurate measurements.
- **C Dataset Details**: The PIE HQ dataset (4,085 pairs) is used for reference. The original PIE test set (978 pairs) was imbalanced, so a balanced version of 255 samples was created. The Codeforces test set (300 samples) was built for OOD evaluation.
- **D Additional Experiments**:
    - A fine-tuned Qwen2.5-Coder-7B model achieves 3.73× SP@5, slightly outperforming ECO on the same model (3.26×), but ECO with GPT-o4-mini is far superior (7.81×).
    - The symbolic advisor (ECO w/o RR) resolves 78.14% of detected slow I/O bottlenecks while maintaining 48.59% accuracy, whereas Supersonic resolves 80.33% but has only 7.06% accuracy.

# References
(No summary required)