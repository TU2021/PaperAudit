{
  "baseline_review": "Summary\n- The paper presents ECO, a performance-aware prompting framework for code runtime optimization. ECO distills runtime optimization instructions (ROIs) from slow–fast code pairs (Section 3.1; Appendix A.1) and combines (i) a symbolic advisor that issues deterministic bottleneck diagnoses via Code Property Graph (CPG) rules (Section 3.2; Algorithms 1–2; Fig. 2) with (ii) an ROI retriever that selects performance-relevant examples and instructions by matching input-specific performance descriptions (Section 3.3; Fig. 2). ECO is model-agnostic, requires no fine-tuning (Section 1; Section 3), and demonstrates substantial speedups on PIE and Codeforces with multiple models, including up to 7.81× on GPT‑o4‑mini (Table 4) while maintaining correctness. Ablations show both modules contribute and complement each other (Table 3). Runtime is measured with gem5 (Section 4.1.4; Appendix B.3.4).Strengths\n- Bold Problem Framing and Motivation\n  - The paper clearly defines the gap between correctness and efficiency in code-LLMs and motivates performance-aware guidance beyond pair imitation (Section 1; Fig. 1). This matters for impact and clarity because it positions ECO as addressing a real limitation in current LLM-based optimization approaches.\n  - It contrasts compiler IR-level optimizations with program-level bottlenecks that require algorithmic and structural reasoning (Section 1), highlighting novelty in scope and target.\n  - The critique of prior pair-based methods encouraging pattern imitation (Section 1; Section 2) frames ECO’s intent-aligned prompting rationale, aiding conceptual soundness.- Novel Performance-Aware Prompting Design\n  - ECO distills ROIs that articulate “what changed” and “why it improves efficiency” (Section 3.1; Appendix A.1; Fig. 5), improving interpretability and actionable guidance—key for novelty and clarity.\n  - The symbolic advisor uses CPG queries and templates to produce bottleneck diagnoses (Section 3.2; Algorithms 1–2; Fig. 2; Figs. 6–9), demonstrating technical soundness through formal rule–template instantiation.\n  - The ROI retriever matches performance characteristics rather than surface code similarity (Section 3.3; Fig. 10; Fig. 4 right), addressing prior RAG limitations and expanding contextual breadth—novelty and practical impact.- Strong Empirical Results Across Models and Datasets\n  - ECO significantly improves speedup and percent optimized compared to baselines on PIE, both Best@1 and Best@5 (Table 2), showing experimental rigor and impact.\n  - Results generalize across model sizes and to closed-source systems (Table 4), with up to 7.81× speedup on GPT‑o4‑mini, indicating model-agnostic practicality and high impact.\n  - OOD evaluation on Codeforces shows ECO’s gains over instruction-only across models (Table 4; Section 4.4), strengthening claims about generalizability and robustness.- Complementary Module Effects Verified via Ablation\n  - Ablations demonstrate each module’s independent utility and complementary trade-offs (Table 3), supporting technical soundness: w/o RR has highest correctness (Best@5 ACC 83.45%), w/o SA maintains notable speedup (Best@5 SP 3.10×), and combined ECO yields the best overall (Best@5 SP 3.26×).\n  - The text explains why ungrounded ROIs (w/o RR+SA) underperform (Section 4.3), offering interpretation that aligns with observed metrics—clarity and rigor.\n  - Case study analysis links retriever stochasticity to optimization variability (Section 4.5; Fig. 3), expanding empirical understanding.- Rigorous Measurement Protocol and Metrics\n  - The paper uses gem5 for deterministic, cycle-accurate runtime (Section 4.1.4; Appendix B.3.4), increasing experimental reliability and reproducibility.\n  - Defined metrics—Accuracy, Speedup, Percent Optimized with best@k evaluation (Section 4.1.4)—improve clarity and comparability.\n  - GCC ‑O3 compilation baseline is reported to isolate improvements beyond compiler optimization (Section 4.1.4), enhancing technical rigor.- Detailed Implementation and Reproducibility\n  - Prompts and configurations for ECO and baselines are described (Appendix B.1–B.3; Figs. 11–13), aiding reproducibility.\n  - Module and dataset details, including Joern usage and ROI DB construction, are provided (Section 3.1–3.3; Appendix A–C), supporting transparency.- Insightful Analyses and Visualizations\n  - The symbolic advisor’s detections distribution and keyword overlap analysis contrast ECO’s retriever against RAG (Fig. 4), clarifying the performance-aware nature of retrieval—experimental insight and clarity.\n  - Concrete directive examples spanning algorithms, data structures, libraries, and loops (Figs. 6–9) illustrate how ECO translates rules into actionable prompts—clarity and practical relevance.Weaknesses\n- Limited Quantitative Validation of ROI Quality and Coverage\n  - No quantitative assessment of ROI correctness, consistency, or coverage is reported (Section 3.1; Appendix A.1). This matters for technical quality because the ROI DB underpins both retriever and rule design; without metrics, its reliability is uncertain.\n  - The manual clustering of ROIs into rule–template pairs lacks detail on procedure, inter-annotator agreement, or cluster statistics (Section 3.2: “We manually cluster similar ROIs”), which impacts reproducibility and transparency.\n  - The number of total rules/templates, category-wise coverage, and the proportion of failures/unmatched bottlenecks are not reported (“No direct evidence found in the manuscript”), limiting claims about generality.- Symbolic Advisor Evaluation Is Narrow and Missing Precision/Recall\n  - Aside from an I/O bottleneck resolution check (Table 7), there is no quantitative evaluation of rule precision/recall across categories (Section 3.2; Appendix A.2; “No direct evidence found”), reducing confidence in rule reliability.\n  - Potential false positives/negatives from CPG queries and templates are not characterized (“No direct evidence found”), which matters for technical soundness and correctness stability.\n  - Scalability and maintainability of the rule set (e.g., adding new categories, handling edge cases) are not analyzed (Section 3.2; “No direct evidence found”), impacting practicality.- Evaluation Comparability and Dataset Modifications\n  - The PIE test set is rebalanced and de-duplicated (Section 4.1.3; Appendix C.1), but changes may hinder direct comparability to prior reported numbers (Appendix D.1), affecting fairness of comparisons.\n  - The Codeforces OOD benchmark is custom curated with filters (time limit ≤ 2s, ≥10 test cases) and sampling (Appendix C.2), which may bias task difficulty or bottleneck types without a documented distribution comparison (“No direct evidence found”).\n  - The best@k protocol is used (Section 4.1.4), but the number of candidates per method and any differences in decoding or candidate generation are not fully specified (“No direct evidence found”), risking unfairness across baselines.- ROI Retriever Stability and Self-Referential Design\n  - The retriever prompts the inference model itself to produce the performance description E_C (Section 3.3), potentially introducing self-bias or leakage; the effects are not studied (“No direct evidence found”), impacting technical soundness.\n  - Ablations show w/o RR yields higher correctness (Best@5 ACC 83.45% vs ECO 74.24%, Table 3), indicating the retriever can harm correctness—important for reliability.\n  - The case study acknowledges stochastic selection variability and mismatches (Section 4.5; Fig. 3), but mechanisms to control variance or calibrate retrieval quality are not provided (“No direct evidence found”), affecting robustness.- Missing End-to-End Overhead Analysis of ECO Pipeline\n  - No runtime/latency breakdown is given for Joern CPG construction and rule querying (Section 3.2; “No direct evidence found”), which matters for deployability in time-sensitive settings.\n  - ROI retrieval overhead—embedding computation plus LLM pass for E_C—is not reported (Section 3.3; “No direct evidence found”), limiting practical cost assessment.\n  - While gem5 overhead is discussed (Appendix B.3.4), the combined end-to-end optimization time per sample (including prompting, retrieval, analysis) is not measured (“No direct evidence found”), limiting operational insight.- Reporting/Clarity Gaps\n  - Some figures/tables are hard to parse (e.g., Figure 3’s stacked percentages and formatting), which affects clarity of error analysis.\n  - Key hyperparameters (e.g., retriever top‑k, similarity thresholds, the exact number of rule–template pairs) are not specified (“No direct evidence found”), affecting reproducibility.\n  - Claims of “state-of-the-art results” (Section 1; Section 4.4) are not accompanied by a broad set of fine-tuning baselines in the main text (fine-tune results only in Appendix D.2), limiting clarity of comparative positioning.Suggestions for Improvement\n- Strengthen ROI Quality and Coverage Quantification\n  - Report ROI correctness and coverage metrics by sampling ROIs for human validation (e.g., agreement rates) and automatic checks against fast code edits (Section 3.1; Appendix A.1).\n  - Document the clustering procedure used to derive rule–template pairs: criteria, statistics per cluster/category, and inter-rater agreement for manual steps (Section 3.2).\n  - Provide counts of rules/templates, category-wise coverage, and the proportion of inputs where no rule/ROI applies; include failure analyses (“No direct evidence found in the manuscript” currently).- Broaden Symbolic Advisor Validation with Precision/Recall Studies\n  - Construct a labeled subset across categories (Algorithm, Data Structure, Library, Loop) and report rule precision/recall/F1; extend beyond I/O resolution (Section 3.2; Appendix A.2; Table 7).\n  - Analyze false positive/negative cases produced by Joern queries and templates, with qualitative examples and rule refinements (“No direct evidence found” currently).\n  - Evaluate scalability by measuring the effort to add new rules/categories and the impact on detection coverage over an expanded dataset (“No direct evidence found” currently).- Improve Evaluation Comparability and Dataset Transparency\n  - Provide side-by-side results on the original PIE test set (even if smaller runs) and the curated subset to assess sensitivity to rebalancing (Section 4.1.3; Appendix C.1; Appendix D.1).\n  - Characterize the Codeforces dataset’s bottleneck distribution versus PIE (e.g., Fig. 4‑style plots) and justify filtering choices with statistics to reduce bias (Appendix C.2).\n  - Specify best@k settings, candidate counts, decoding parameters per method, and ensure uniformity; include sensitivity analysis to k and temperature (Section 4.1.4; “No direct evidence found”).- Stabilize ROI Retriever and Reduce Self-Bias\n  - Evaluate the effect of using a separate analysis model for E_C versus the inference model; report correctness/speedup and variance comparisons (Section 3.3).\n  - Add calibration or post-retrieval filtering (e.g., consistency checks with symbolic diagnoses) to reduce mismatches; measure impact on ACC and SP (Table 3; Section 4.5).\n  - Introduce confidence scores for retrieved ROIs and a fallback to symbolic-only prompts when confidence is low; quantify variance reduction (“No direct evidence found” currently).- Report End-to-End Overheads and Practical Costs\n  - Provide timing breakdowns for CPG construction, rule queries, E_C generation, embedding, retrieval, and prompting (Section 3.2–3.3).\n  - Measure and report ROI retriever overhead under different embedding models and top‑k; include throughput metrics (“No direct evidence found” currently).\n  - Present end-to-end optimization latency per code sample (including gem5 if used) to contextualize deployment scenarios (Appendix B.3.4; “No direct evidence found” currently).- Enhance Reporting and Reproducibility Details\n  - Clarify figure/table formatting (e.g., Figure 3), and include a consolidated table of key hyperparameters: retriever top‑k, similarity threshold, total rules/templates (“No direct evidence found” currently).\n  - Move fine-tuning comparisons from Appendix D.2 into the main text with consistent settings to support “state-of-the-art” claims (Section 4.4).\n  - Provide a complete prompt concatenation example for combined symbolic + ROI prompts (Appendix B.1–B.2), and release counts of ROI DB entries per category for transparency.Score\n- Overall (10): 7 — Strong, well-motivated framework with notable empirical gains (Table 2; Table 4; Fig. 2), but limited quantitative validation of rule/ROI quality and pipeline overheads (Section 3.1–3.3; Table 7; “No direct evidence found”).\n- Novelty (10): 8 — Distillation of ROIs plus dual guidance via symbolic advisor and performance-aware retrieval (Section 3.1–3.3; Fig. 2; Fig. 10) is a clear departure from pair-only RAG/ICL.\n- Technical Quality (10): 6 — Solid algorithms and ablations (Algorithms 1–2; Table 3), yet missing precision/recall for rules, ROI quality metrics, and end-to-end cost analysis (Table 7; “No direct evidence found”).\n- Clarity (10): 7 — Generally clear with detailed appendices (Appendix A–C; Figs. 6–9), but some reporting gaps (e.g., retriever top‑k, figure formatting in Fig. 3; “No direct evidence found”).\n- Confidence (5): 4 — High confidence based on extensive anchors (Sections 3–4; Tables 2–4; Figs. 1–4, 6–10), but some key quantitative validations are missing, limiting certainty on generality and overheads.",
  "final_review": "Summary\n- The paper presents ECO, a performance-aware prompting framework for code runtime optimization. ECO distills runtime optimization instructions (ROIs) from slow–fast code pairs (Section 3.1; Appendix A.1) and integrates (i) a symbolic advisor that deterministically diagnoses bottlenecks via Code Property Graph (CPG) rules (Section 3.2; Algorithms 1–2; Fig. 2) with (ii) an ROI retriever that selects performance-relevant examples and instructions by matching input-specific performance descriptions (Section 3.3; Fig. 2). ECO is model-agnostic, requires no fine-tuning (Section 1; Section 3), and reports substantial speedups on PIE and Codeforces with multiple models, including up to 7.81× on GPT‑o4‑mini (Table 4), while maintaining correctness. Ablations show both modules contribute and complement each other (Table 3). Runtime is measured with gem5 (Section 4.1.4; Appendix B.3.4).Strengths\n- Bold Problem Framing and Motivation\n  - The paper clearly defines the gap between correctness and efficiency in code-LLMs and motivates performance-aware guidance beyond pair imitation (Section 1; Fig. 1). This matters for impact and clarity because it positions ECO as addressing a real limitation in current LLM-based optimization approaches.\n  - It contrasts compiler IR-level optimizations with program-level bottlenecks that require algorithmic and structural reasoning (Section 1), highlighting novelty in scope and target.\n  - The critique of prior pair-based methods encouraging pattern imitation (Section 1; Section 2) frames ECO’s intent-aligned prompting rationale, aiding conceptual soundness.\n- Novel Performance-Aware Prompting Design\n  - ECO distills ROIs that articulate “what changed” and “why it improves efficiency” (Section 3.1; Appendix A.1; Fig. 5), improving interpretability and actionable guidance—key for novelty and clarity.\n  - The symbolic advisor uses CPG queries and templates to produce bottleneck diagnoses (Section 3.2; Algorithms 1–2; Fig. 2; Figs. 6–9), demonstrating technical soundness through formal rule–template instantiation.\n  - The ROI retriever matches performance characteristics rather than surface code similarity (Section 3.3; Fig. 10; Fig. 4 right), addressing prior RAG limitations and expanding contextual breadth—novelty and practical impact.\n- Strong Empirical Results Across Models and Datasets\n  - ECO significantly improves speedup and percent optimized compared to baselines on PIE, both Best@1 and Best@5 (Table 2), showing experimental rigor and impact.\n  - Results generalize across model sizes and to closed-source systems (Table 4), with up to 7.81× speedup on GPT‑o4‑mini, indicating model-agnostic practicality and high impact.\n  - OOD evaluation on Codeforces shows ECO’s gains over instruction-only across models (Table 4; Section 4.4), strengthening claims about generalizability and robustness.\n- Complementary Module Effects Verified via Ablation\n  - Ablations demonstrate each module’s independent utility and complementary trade-offs (Table 3), supporting technical soundness: w/o RR has highest correctness (Best@5 ACC 83.45%), w/o SA maintains notable speedup (Best@5 SP 3.10×), and combined ECO yields the best overall (Best@5 SP 3.26×).\n  - The text explains why ungrounded ROIs (w/o RR+SA) underperform (Section 4.3), offering interpretation that aligns with observed metrics—clarity and rigor.\n  - Case study analysis links retriever stochasticity to optimization variability (Section 4.5; Fig. 3), expanding empirical understanding.\n- Rigorous Measurement Protocol and Metrics\n  - The paper uses gem5 for deterministic, cycle-accurate runtime (Section 4.1.4; Appendix B.3.4), increasing experimental reliability and reproducibility.\n  - Defined metrics—Accuracy, Speedup, Percent Optimized with best@k evaluation (Section 4.1.4)—improve clarity and comparability.\n  - GCC ‑O3 compilation baseline is consistently applied (Section 4.1.4), aiding measurement stability beyond compiler optimizations.\n- Detailed Implementation and Reproducibility\n  - Prompts and configurations for ECO and baselines are described (Appendix B.1–B.3; Figs. 11–13), aiding reproducibility.\n  - Module and dataset details, including Joern usage and ROI DB construction, are provided (Section 3.1–3.3; Appendix A–C), supporting transparency.\n- Insightful Analyses and Visualizations\n  - The symbolic advisor’s detections distribution and keyword overlap analysis contrast ECO’s retriever against RAG (Fig. 4), clarifying the performance-aware nature of retrieval—experimental insight and clarity.\n  - Concrete directive examples spanning algorithms, data structures, libraries, and loops (Figs. 6–9) illustrate how ECO translates rules into actionable prompts—clarity and practical relevance.Weaknesses\n- Limited Quantitative Validation of ROI Quality and Coverage\n  - No quantitative assessment of ROI correctness, consistency, or coverage is reported (Section 3.1; Appendix A.1). This matters for technical quality because the ROI DB underpins both retriever and rule design; without metrics, its reliability is uncertain.\n  - The manual clustering of ROIs into rule–template pairs lacks detail on procedure, inter-annotator agreement, or cluster statistics (Section 3.2: “We manually cluster similar ROIs”), which impacts reproducibility and transparency.\n  - The number of total rules/templates, category-wise coverage, and the proportion of failures/unmatched bottlenecks are not reported (“No direct evidence found in the manuscript”), limiting claims about generality.\n- Symbolic Advisor Evaluation Is Narrow and Missing Precision/Recall\n  - Aside from an I/O bottleneck resolution check (Table 7), there is no quantitative evaluation of rule precision/recall across categories (Section 3.2; Appendix A.2; “No direct evidence found”), reducing confidence in rule reliability.\n  - Potential false positives/negatives from CPG queries and templates are not characterized (“No direct evidence found”), which matters for technical soundness and correctness stability.\n  - Scalability and maintainability of the rule set (e.g., adding new categories, handling edge cases) are not analyzed (Section 3.2; “No direct evidence found”), impacting practicality.\n- Evaluation Comparability and Dataset Modifications\n  - The PIE test set is rebalanced and de-duplicated (Section 4.1.3; Appendix C.1), but changes may hinder direct comparability to prior reported numbers (Appendix D.1), affecting fairness of comparisons.\n  - The Codeforces OOD benchmark is custom curated with filters (time limit ≤ 2s, ≥10 test cases) and sampling (Appendix C.2), which may bias task difficulty or bottleneck types without a documented distribution comparison (“No direct evidence found”).\n  - The best@k protocol is used (Section 4.1.4), but the number of candidates per method and any differences in decoding or candidate generation are not fully specified (“No direct evidence found”), risking unfairness across baselines.\n- ROI Retriever Stability and Self-Referential Design\n  - The retriever prompts the inference model itself to produce the performance description E_C (Section 3.3), potentially introducing self-bias or leakage; the effects are not studied (“No direct evidence found”), impacting technical soundness.\n  - Ablations show w/o RR yields higher correctness (Best@5 ACC 83.45% vs ECO 74.24%, Table 3), indicating the retriever can harm correctness—important for reliability.\n  - The case study acknowledges stochastic selection variability and mismatches (Section 4.5; Fig. 3), but mechanisms to control variance or calibrate retrieval quality are not provided (“No direct evidence found”), affecting robustness.\n- Missing End-to-End Overhead Analysis of ECO Pipeline\n  - No runtime/latency breakdown is given for Joern CPG construction and rule querying (Section 3.2; “No direct evidence found”), which matters for deployability in time-sensitive settings.\n  - ROI retrieval overhead—embedding computation plus LLM pass for E_C—is not reported (Section 3.3; “No direct evidence found”), limiting practical cost assessment.\n  - While gem5 overhead is discussed (Appendix B.3.4), the combined end-to-end optimization time per sample (including prompting, retrieval, analysis) is not measured (“No direct evidence found”), limiting operational insight.\n- Reporting/Clarity Gaps\n  - Some figures/tables are hard to parse (e.g., Figure 3’s stacked percentages and formatting), which affects clarity of error analysis (Fig. 3).\n  - Key hyperparameters (e.g., retriever top‑k, similarity thresholds, the exact number of rule–template pairs) are not specified (“No direct evidence found”), affecting reproducibility.\n  - Claims of “state-of-the-art results” (Section 1; Section 4.4) are not accompanied by a broad set of fine-tuning baselines in the main text (fine-tune results only in Appendix D.2), limiting clarity of comparative positioning.\n  - Contradictory attribution of headline speedup: Introduction attributes the 7.81× gain to GPT‑4o‑mini, but Table 4 shows 7.81× for GPT‑o4‑mini and 3.97× for GPT‑4o‑mini (Section 1, end of paragraph with Fig. 1; Table 4), undermining internal consistency.\n  - Figure 1’s annotation marks “O(n²) computation” for a single-loop implementation that is O(n), weakening the illustrative clarity (Fig. 1, “Generated Code” in the top-right panel).\n  - Embedding model specification mismatch: Section 4.1.1 names “Qodo‑Embed‑1‑1:5b”, while Appendix A.3 specifies “Qodo/Qodo‑Embed‑1‑1.5B” (Section 4.1.1; Appendix A.3), reducing reproducibility.\n  - Decoding parameter inconsistency for closed-source models: Section 4.1.1 states temperature 0.7, but Appendix B reports “official API with default decoding parameters” (Section 4.1.1; Appendix B), affecting comparability.\n  - A directive suggests variable-length arrays “int v[n];” (Appendix A.2.2; Fig. 7) under a C++17, -O3 compile setting (Section 4.1.4), which may conflict with standard C++17 (no VLA); the paper does not clarify GNU extensions, risking compilation issues.\n  - ROI category labeling inconsistency: the JSON example labels “cout→printf” as “Algorithm” (Appendix A.2, Fig. 5), while the main text defines “Library Usage” and related categories (Section 3.2), reducing coherence of the knowledge base.\n  - Ambiguous phrasing about compiler optimizations: “performance gains exclude compiler-level optimizations” while compiling with -O3 (Section 4.1.4), which should be clarified as gains measured beyond a fixed -O3 baseline.\n  - The narrative that ROI retriever overlaps with performance-relevant keywords could be more precise, as the top overlap includes “ans” (Fig. 4, right table), slightly misaligning with the claim.\n  - The statement that Supersonic outputs “over 80% malformed patches” lacks a direct quantitative report in tables/figures (Section 4.2; “No direct evidence found”), reducing transparency of baseline criticism.Suggestions for Improvement\n- Strengthen ROI Quality and Coverage Quantification\n  - Report ROI correctness and coverage metrics by sampling ROIs for human validation (e.g., agreement rates) and automatic checks against fast code edits (Section 3.1; Appendix A.1).\n  - Document the clustering procedure used to derive rule–template pairs: criteria, statistics per cluster/category, and inter-rater agreement for manual steps (Section 3.2).\n  - Provide counts of rules/templates, category-wise coverage, and the proportion of inputs where no rule/ROI applies; include failure analyses (“No direct evidence found in the manuscript” currently).\n- Broaden Symbolic Advisor Validation with Precision/Recall Studies\n  - Construct a labeled subset across categories (Algorithm, Data Structure, Library, Loop) and report rule precision/recall/F1; extend beyond I/O resolution (Section 3.2; Appendix A.2; Table 7).\n  - Analyze false positive/negative cases produced by Joern queries and templates, with qualitative examples and rule refinements (“No direct evidence found” currently).\n  - Evaluate scalability by measuring the effort to add new rules/categories and the impact on detection coverage over an expanded dataset (“No direct evidence found” currently).\n- Improve Evaluation Comparability and Dataset Transparency\n  - Provide side-by-side results on the original PIE test set (even if smaller runs) and the curated subset to assess sensitivity to rebalancing (Section 4.1.3; Appendix C.1; Appendix D.1).\n  - Characterize the Codeforces dataset’s bottleneck distribution versus PIE (e.g., Fig. 4‑style plots) and justify filtering choices with statistics to reduce bias (Appendix C.2).\n  - Specify best@k settings, candidate counts, decoding parameters per method, and ensure uniformity; include sensitivity analysis to k and temperature (Section 4.1.4; “No direct evidence found”).\n- Stabilize ROI Retriever and Reduce Self-Bias\n  - Evaluate the effect of using a separate analysis model for E_C versus the inference model; report correctness/speedup and variance comparisons (Section 3.3).\n  - Add calibration or post-retrieval filtering (e.g., consistency checks with symbolic diagnoses) to reduce mismatches; measure impact on ACC and SP (Table 3; Section 4.5).\n  - Introduce confidence scores for retrieved ROIs and a fallback to symbolic-only prompts when confidence is low; quantify variance reduction (“No direct evidence found” currently).\n- Report End-to-End Overheads and Practical Costs\n  - Provide timing breakdowns for CPG construction, rule queries, E_C generation, embedding, retrieval, and prompting (Section 3.2–3.3).\n  - Measure and report ROI retriever overhead under different embedding models and top‑k; include throughput metrics (“No direct evidence found” currently).\n  - Present end-to-end optimization latency per code sample (including gem5 if used) to contextualize deployment scenarios (Appendix B.3.4; “No direct evidence found” currently).\n- Enhance Reporting and Reproducibility Details\n  - Clarify figure/table formatting (e.g., Figure 3), and include a consolidated table of key hyperparameters: retriever top‑k, similarity threshold, total rules/templates (“No direct evidence found” currently).\n  - Move fine-tuning comparisons from Appendix D.2 into the main text with consistent settings to support “state-of-the-art” claims (Section 4.4).\n  - Provide a complete prompt concatenation example for combined symbolic + ROI prompts (Appendix B.1–B.2), and release counts of ROI DB entries per category for transparency.\n  - Correct the speedup attribution in the Introduction to align with Table 4 (Section 1; Table 4) and ensure consistency across the text and figures.\n  - Fix Figure 1’s complexity annotation to match the single-loop O(n) computation shown (Fig. 1), improving the illustrative accuracy.\n  - Harmonize the embedding model specification across Sections/Appendix (Section 4.1.1 vs Appendix A.3), and document model size/version used for retrieval.\n  - Align decoding parameter descriptions for closed-source models (Section 4.1.1 vs Appendix B): report actual temperatures or confirm defaults; ensure comparable best@k generation settings.\n  - Clarify whether GNU extensions are enabled; if not, revise directives to C++17-compliant forms (e.g., std::array or fixed-size buffers) to avoid VLAs (“int v[n];”) under C++17 (Appendix A.2.2; Section 4.1.4).\n  - Ensure ROI category labels are consistent with defined categories (Section 3.2) and correct mislabels in examples (Appendix A.2, Fig. 5).\n  - Rephrase “exclude compiler-level optimizations” to “measured beyond a fixed -O3 baseline” (Section 4.1.4) to avoid confusion.\n  - Calibrate the keyword-overlap narrative (Fig. 4, right) to reflect that performance-relevant terms dominate but are not exclusive; align text with the presented rankings.\n  - Quantify and report malformed patch rates for Supersonic (Section 4.2), or soften the claim if unsupported; include counts or percentages in a table for transparency.Score\n- Overall (10): 7 — Strong, well-motivated framework with notable empirical gains (Table 2; Table 4; Fig. 2), but limited quantitative validation of rule/ROI quality and pipeline overheads (Section 3.1–3.3; Table 7) and several reporting inconsistencies (Section 1; Fig. 1; Section 4.1.1 vs Appendix B; Appendix A.2.2).\n- Novelty (10): 8 — Distillation of ROIs plus dual guidance via symbolic advisor and performance-aware retrieval (Section 3.1–3.3; Fig. 2; Fig. 10) is a clear departure from pair-only RAG/ICL.\n- Technical Quality (10): 6 — Solid algorithms and ablations (Algorithms 1–2; Table 3), yet missing precision/recall for rules, ROI quality metrics, and end-to-end cost analysis (Table 7; “No direct evidence found”).\n- Clarity (10): 6 — Generally clear with detailed appendices (Appendix A–C; Figs. 6–9), but internal inconsistencies and reporting gaps (e.g., speedup attribution in Section 1 vs Table 4; Fig. 1 annotation; embedding/decoding parameter mismatches; Appendix A.2.2 vs Section 4.1.4) reduce readability and reproducibility.\n- Confidence (5): 4 — High confidence based on extensive anchors (Sections 3–4; Tables 2–4; Figs. 1–4, 6–10), but some key quantitative validations and consistency checks are missing, limiting certainty on generality and overheads.",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 7,
        "novelty": 8,
        "technical_quality": 6,
        "clarity": 7,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 7,
        "novelty": 8,
        "technical_quality": 6,
        "clarity": 6,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "Summary\n- The paper presents ECO, a performance-aware prompting framework for code runtime optimization. ECO distills runtime optimization instructions (ROIs) from slow–fast code pairs (Section 3.1; Appendix A.1) and integrates (i) a symbolic advisor that deterministically diagnoses bottlenecks via Code Property Graph (CPG) rules (Section 3.2; Algorithms 1–2; Fig. 2) with (ii) an ROI retriever that selects performance-relevant examples and instructions by matching input-specific performance descriptions (Section 3.3; Fig. 2). ECO is model-agnostic, requires no fine-tuning (Section 1; Section 3), and reports substantial speedups on PIE and Codeforces with multiple models, including up to 7.81× on GPT‑o4‑mini (Table 4), while maintaining correctness. Ablations show both modules contribute and complement each other (Table 3). Runtime is measured with gem5 (Section 4.1.4; Appendix B.3.4).Strengths\n- Bold Problem Framing and Motivation\n  - The paper clearly defines the gap between correctness and efficiency in code-LLMs and motivates performance-aware guidance beyond pair imitation (Section 1; Fig. 1). This matters for impact and clarity because it positions ECO as addressing a real limitation in current LLM-based optimization approaches.\n  - It contrasts compiler IR-level optimizations with program-level bottlenecks that require algorithmic and structural reasoning (Section 1), highlighting novelty in scope and target.\n  - The critique of prior pair-based methods encouraging pattern imitation (Section 1; Section 2) frames ECO’s intent-aligned prompting rationale, aiding conceptual soundness.\n- Novel Performance-Aware Prompting Design\n  - ECO distills ROIs that articulate “what changed” and “why it improves efficiency” (Section 3.1; Appendix A.1; Fig. 5), improving interpretability and actionable guidance—key for novelty and clarity.\n  - The symbolic advisor uses CPG queries and templates to produce bottleneck diagnoses (Section 3.2; Algorithms 1–2; Fig. 2; Figs. 6–9), demonstrating technical soundness through formal rule–template instantiation.\n  - The ROI retriever matches performance characteristics rather than surface code similarity (Section 3.3; Fig. 10; Fig. 4 right), addressing prior RAG limitations and expanding contextual breadth—novelty and practical impact.\n- Strong Empirical Results Across Models and Datasets\n  - ECO significantly improves speedup and percent optimized compared to baselines on PIE, both Best@1 and Best@5 (Table 2), showing experimental rigor and impact.\n  - Results generalize across model sizes and to closed-source systems (Table 4), with up to 7.81× speedup on GPT‑o4‑mini, indicating model-agnostic practicality and high impact.\n  - OOD evaluation on Codeforces shows ECO’s gains over instruction-only across models (Table 4; Section 4.4), strengthening claims about generalizability and robustness.\n- Complementary Module Effects Verified via Ablation\n  - Ablations demonstrate each module’s independent utility and complementary trade-offs (Table 3), supporting technical soundness: w/o RR has highest correctness (Best@5 ACC 83.45%), w/o SA maintains notable speedup (Best@5 SP 3.10×), and combined ECO yields the best overall (Best@5 SP 3.26×).\n  - The text explains why ungrounded ROIs (w/o RR+SA) underperform (Section 4.3), offering interpretation that aligns with observed metrics—clarity and rigor.\n  - Case study analysis links retriever stochasticity to optimization variability (Section 4.5; Fig. 3), expanding empirical understanding.\n- Rigorous Measurement Protocol and Metrics\n  - The paper uses gem5 for deterministic, cycle-accurate runtime (Section 4.1.4; Appendix B.3.4), increasing experimental reliability and reproducibility.\n  - Defined metrics—Accuracy, Speedup, Percent Optimized with best@k evaluation (Section 4.1.4)—improve clarity and comparability.\n  - GCC ‑O3 compilation baseline is consistently applied (Section 4.1.4), aiding measurement stability beyond compiler optimizations.\n- Detailed Implementation and Reproducibility\n  - Prompts and configurations for ECO and baselines are described (Appendix B.1–B.3; Figs. 11–13), aiding reproducibility.\n  - Module and dataset details, including Joern usage and ROI DB construction, are provided (Section 3.1–3.3; Appendix A–C), supporting transparency.\n- Insightful Analyses and Visualizations\n  - The symbolic advisor’s detections distribution and keyword overlap analysis contrast ECO’s retriever against RAG (Fig. 4), clarifying the performance-aware nature of retrieval—experimental insight and clarity.\n  - Concrete directive examples spanning algorithms, data structures, libraries, and loops (Figs. 6–9) illustrate how ECO translates rules into actionable prompts—clarity and practical relevance.Weaknesses\n- Limited Quantitative Validation of ROI Quality and Coverage\n  - No quantitative assessment of ROI correctness, consistency, or coverage is reported (Section 3.1; Appendix A.1). This matters for technical quality because the ROI DB underpins both retriever and rule design; without metrics, its reliability is uncertain.\n  - The manual clustering of ROIs into rule–template pairs lacks detail on procedure, inter-annotator agreement, or cluster statistics (Section 3.2: “We manually cluster similar ROIs”), which impacts reproducibility and transparency.\n  - The number of total rules/templates, category-wise coverage, and the proportion of failures/unmatched bottlenecks are not reported (“No direct evidence found in the manuscript”), limiting claims about generality.\n- Symbolic Advisor Evaluation Is Narrow and Missing Precision/Recall\n  - Aside from an I/O bottleneck resolution check (Table 7), there is no quantitative evaluation of rule precision/recall across categories (Section 3.2; Appendix A.2; “No direct evidence found”), reducing confidence in rule reliability.\n  - Potential false positives/negatives from CPG queries and templates are not characterized (“No direct evidence found”), which matters for technical soundness and correctness stability.\n  - Scalability and maintainability of the rule set (e.g., adding new categories, handling edge cases) are not analyzed (Section 3.2; “No direct evidence found”), impacting practicality.\n- Evaluation Comparability and Dataset Modifications\n  - The PIE test set is rebalanced and de-duplicated (Section 4.1.3; Appendix C.1), but changes may hinder direct comparability to prior reported numbers (Appendix D.1), affecting fairness of comparisons.\n  - The Codeforces OOD benchmark is custom curated with filters (time limit ≤ 2s, ≥10 test cases) and sampling (Appendix C.2), which may bias task difficulty or bottleneck types without a documented distribution comparison (“No direct evidence found”).\n  - The best@k protocol is used (Section 4.1.4), but the number of candidates per method and any differences in decoding or candidate generation are not fully specified (“No direct evidence found”), risking unfairness across baselines.\n- ROI Retriever Stability and Self-Referential Design\n  - The retriever prompts the inference model itself to produce the performance description E_C (Section 3.3), potentially introducing self-bias or leakage; the effects are not studied (“No direct evidence found”), impacting technical soundness.\n  - Ablations show w/o RR yields higher correctness (Best@5 ACC 83.45% vs ECO 74.24%, Table 3), indicating the retriever can harm correctness—important for reliability.\n  - The case study acknowledges stochastic selection variability and mismatches (Section 4.5; Fig. 3), but mechanisms to control variance or calibrate retrieval quality are not provided (“No direct evidence found”), affecting robustness.\n- Missing End-to-End Overhead Analysis of ECO Pipeline\n  - No runtime/latency breakdown is given for Joern CPG construction and rule querying (Section 3.2; “No direct evidence found”), which matters for deployability in time-sensitive settings.\n  - ROI retrieval overhead—embedding computation plus LLM pass for E_C—is not reported (Section 3.3; “No direct evidence found”), limiting practical cost assessment.\n  - While gem5 overhead is discussed (Appendix B.3.4), the combined end-to-end optimization time per sample (including prompting, retrieval, analysis) is not measured (“No direct evidence found”), limiting operational insight.\n- Reporting/Clarity Gaps\n  - Some figures/tables are hard to parse (e.g., Figure 3’s stacked percentages and formatting), which affects clarity of error analysis (Fig. 3).\n  - Key hyperparameters (e.g., retriever top‑k, similarity thresholds, the exact number of rule–template pairs) are not specified (“No direct evidence found”), affecting reproducibility.\n  - Claims of “state-of-the-art results” (Section 1; Section 4.4) are not accompanied by a broad set of fine-tuning baselines in the main text (fine-tune results only in Appendix D.2), limiting clarity of comparative positioning.\n  - Contradictory attribution of headline speedup: Introduction attributes the 7.81× gain to GPT‑4o‑mini, but Table 4 shows 7.81× for GPT‑o4‑mini and 3.97× for GPT‑4o‑mini (Section 1, end of paragraph with Fig. 1; Table 4), undermining internal consistency.\n  - Figure 1’s annotation marks “O(n²) computation” for a single-loop implementation that is O(n), weakening the illustrative clarity (Fig. 1, “Generated Code” in the top-right panel).\n  - Embedding model specification mismatch: Section 4.1.1 names “Qodo‑Embed‑1‑1:5b”, while Appendix A.3 specifies “Qodo/Qodo‑Embed‑1‑1.5B” (Section 4.1.1; Appendix A.3), reducing reproducibility.\n  - Decoding parameter inconsistency for closed-source models: Section 4.1.1 states temperature 0.7, but Appendix B reports “official API with default decoding parameters” (Section 4.1.1; Appendix B), affecting comparability.\n  - A directive suggests variable-length arrays “int v[n];” (Appendix A.2.2; Fig. 7) under a C++17, -O3 compile setting (Section 4.1.4), which may conflict with standard C++17 (no VLA); the paper does not clarify GNU extensions, risking compilation issues.\n  - ROI category labeling inconsistency: the JSON example labels “cout→printf” as “Algorithm” (Appendix A.2, Fig. 5), while the main text defines “Library Usage” and related categories (Section 3.2), reducing coherence of the knowledge base.\n  - Ambiguous phrasing about compiler optimizations: “performance gains exclude compiler-level optimizations” while compiling with -O3 (Section 4.1.4), which should be clarified as gains measured beyond a fixed -O3 baseline.\n  - The narrative that ROI retriever overlaps with performance-relevant keywords could be more precise, as the top overlap includes “ans” (Fig. 4, right table), slightly misaligning with the claim.\n  - The statement that Supersonic outputs “over 80% malformed patches” lacks a direct quantitative report in tables/figures (Section 4.2; “No direct evidence found”), reducing transparency of baseline criticism.Suggestions for Improvement\n- Strengthen ROI Quality and Coverage Quantification\n  - Report ROI correctness and coverage metrics by sampling ROIs for human validation (e.g., agreement rates) and automatic checks against fast code edits (Section 3.1; Appendix A.1).\n  - Document the clustering procedure used to derive rule–template pairs: criteria, statistics per cluster/category, and inter-rater agreement for manual steps (Section 3.2).\n  - Provide counts of rules/templates, category-wise coverage, and the proportion of inputs where no rule/ROI applies; include failure analyses (“No direct evidence found in the manuscript” currently).\n- Broaden Symbolic Advisor Validation with Precision/Recall Studies\n  - Construct a labeled subset across categories (Algorithm, Data Structure, Library, Loop) and report rule precision/recall/F1; extend beyond I/O resolution (Section 3.2; Appendix A.2; Table 7).\n  - Analyze false positive/negative cases produced by Joern queries and templates, with qualitative examples and rule refinements (“No direct evidence found” currently).\n  - Evaluate scalability by measuring the effort to add new rules/categories and the impact on detection coverage over an expanded dataset (“No direct evidence found” currently).\n- Improve Evaluation Comparability and Dataset Transparency\n  - Provide side-by-side results on the original PIE test set (even if smaller runs) and the curated subset to assess sensitivity to rebalancing (Section 4.1.3; Appendix C.1; Appendix D.1).\n  - Characterize the Codeforces dataset’s bottleneck distribution versus PIE (e.g., Fig. 4‑style plots) and justify filtering choices with statistics to reduce bias (Appendix C.2).\n  - Specify best@k settings, candidate counts, decoding parameters per method, and ensure uniformity; include sensitivity analysis to k and temperature (Section 4.1.4; “No direct evidence found”).\n- Stabilize ROI Retriever and Reduce Self-Bias\n  - Evaluate the effect of using a separate analysis model for E_C versus the inference model; report correctness/speedup and variance comparisons (Section 3.3).\n  - Add calibration or post-retrieval filtering (e.g., consistency checks with symbolic diagnoses) to reduce mismatches; measure impact on ACC and SP (Table 3; Section 4.5).\n  - Introduce confidence scores for retrieved ROIs and a fallback to symbolic-only prompts when confidence is low; quantify variance reduction (“No direct evidence found” currently).\n- Report End-to-End Overheads and Practical Costs\n  - Provide timing breakdowns for CPG construction, rule queries, E_C generation, embedding, retrieval, and prompting (Section 3.2–3.3).\n  - Measure and report ROI retriever overhead under different embedding models and top‑k; include throughput metrics (“No direct evidence found” currently).\n  - Present end-to-end optimization latency per code sample (including gem5 if used) to contextualize deployment scenarios (Appendix B.3.4; “No direct evidence found” currently).\n- Enhance Reporting and Reproducibility Details\n  - Clarify figure/table formatting (e.g., Figure 3), and include a consolidated table of key hyperparameters: retriever top‑k, similarity threshold, total rules/templates (“No direct evidence found” currently).\n  - Move fine-tuning comparisons from Appendix D.2 into the main text with consistent settings to support “state-of-the-art” claims (Section 4.4).\n  - Provide a complete prompt concatenation example for combined symbolic + ROI prompts (Appendix B.1–B.2), and release counts of ROI DB entries per category for transparency.\n  - Correct the speedup attribution in the Introduction to align with Table 4 (Section 1; Table 4) and ensure consistency across the text and figures.\n  - Fix Figure 1’s complexity annotation to match the single-loop O(n) computation shown (Fig. 1), improving the illustrative accuracy.\n  - Harmonize the embedding model specification across Sections/Appendix (Section 4.1.1 vs Appendix A.3), and document model size/version used for retrieval.\n  - Align decoding parameter descriptions for closed-source models (Section 4.1.1 vs Appendix B): report actual temperatures or confirm defaults; ensure comparable best@k generation settings.\n  - Clarify whether GNU extensions are enabled; if not, revise directives to C++17-compliant forms (e.g., std::array or fixed-size buffers) to avoid VLAs (“int v[n];”) under C++17 (Appendix A.2.2; Section 4.1.4).\n  - Ensure ROI category labels are consistent with defined categories (Section 3.2) and correct mislabels in examples (Appendix A.2, Fig. 5).\n  - Rephrase “exclude compiler-level optimizations” to “measured beyond a fixed -O3 baseline” (Section 4.1.4) to avoid confusion.\n  - Calibrate the keyword-overlap narrative (Fig. 4, right) to reflect that performance-relevant terms dominate but are not exclusive; align text with the presented rankings.\n  - Quantify and report malformed patch rates for Supersonic (Section 4.2), or soften the claim if unsupported; include counts or percentages in a table for transparency.Score\n- Overall (10): 7 — Strong, well-motivated framework with notable empirical gains (Table 2; Table 4; Fig. 2), but limited quantitative validation of rule/ROI quality and pipeline overheads (Section 3.1–3.3; Table 7) and several reporting inconsistencies (Section 1; Fig. 1; Section 4.1.1 vs Appendix B; Appendix A.2.2).\n- Novelty (10): 8 — Distillation of ROIs plus dual guidance via symbolic advisor and performance-aware retrieval (Section 3.1–3.3; Fig. 2; Fig. 10) is a clear departure from pair-only RAG/ICL.\n- Technical Quality (10): 6 — Solid algorithms and ablations (Algorithms 1–2; Table 3), yet missing precision/recall for rules, ROI quality metrics, and end-to-end cost analysis (Table 7; “No direct evidence found”).\n- Clarity (10): 6 — Generally clear with detailed appendices (Appendix A–C; Figs. 6–9), but internal inconsistencies and reporting gaps (e.g., speedup attribution in Section 1 vs Table 4; Fig. 1 annotation; embedding/decoding parameter mismatches; Appendix A.2.2 vs Section 4.1.4) reduce readability and reproducibility.\n- Confidence (5): 4 — High confidence based on extensive anchors (Sections 3–4; Tables 2–4; Figs. 1–4, 6–10), but some key quantitative validations and consistency checks are missing, limiting certainty on generality and overheads."
}