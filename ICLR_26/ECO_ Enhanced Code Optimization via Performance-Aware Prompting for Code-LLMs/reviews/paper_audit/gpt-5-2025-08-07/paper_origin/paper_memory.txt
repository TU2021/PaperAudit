# Global Summary
Problem: Code runtime optimization (rewriting slow code to faster code) is hard for code-LLMs because it requires performance reasoning (algorithmic/data-structure/library/loop trade-offs), and prior methods that use slow–fast pairs often lead to pattern imitation without understanding causality.

Approach: ECO (Enhanced Code Optimization) is a model-agnostic, train-free prompting framework that distills runtime optimization instructions (ROIs) from slow–fast pairs and composes performance-aware prompts by combining (i) a deterministic bottleneck diagnosis from a symbolic advisor over Code Property Graphs (CPGs) and (ii) retrieved ROIs aligned to the input code’s performance characteristics.

Evaluation: Tested on PIE (in-distribution) and Codeforces (out-of-distribution) with open-source Qwen2.5-Coder models (3B/7B/14B) and closed-source GPT-4o-mini and GPT-o4-mini. Metrics include Accuracy (ACC), Speedup rate (SP=T(o)/T(n)), and Percent optimized (OPT: correct and >10% faster). Runtime measured via gem5; C++ compiled with GCC 9.3.0, C++17, -O3.

Key findings:
- On Qwen2.5-Coder-7B (Best@5), ECO achieves ACC 74.24%, SP 3.26×, OPT 48.04%, outperforming instruction-only (ACC 68.12%, SP 1.44×, OPT 15.92%) and RAG (ACC 64.51%, SP 2.51×, OPT 30.31%).
- Across models (Best@5): Qwen2.5-Coder-14b with ECO: ACC 79.84%, SP 3.67×, OPT 53.69%. GPT-4o-mini with ECO: ACC 94.51%, SP 3.97×, OPT 60.78%. GPT-o4-mini with ECO: ACC 97.25%, SP 7.81×, OPT 84.71%.
- OOD (Codeforces, Best@5): GPT-4o-mini speedup improves from 1.01× (instruction-only) to 2.01× with ECO; GPT-o4-mini from 1.41× to 4.55×; Qwen2.5-Coder-14b from 1.09× to 2.30×.
- Ablations (Qwen2.5-Coder-7B, Best@5): w/o ROI retriever (SA only) ACC 83.45%, SP 3.08×, OPT 42.75%; w/o symbolic advisor (RR only) ACC 70.39%, SP 3.10×, OPT 42.12%; w/o both ACC 73.80%, SP 2.26×, OPT 30.75%.

Caveats explicitly stated: On small models (e.g., Qwen2.5-Coder-3B), ECO may reduce accuracy due to limited capacity to implement guidance. Retrieval can occasionally return less relevant ROIs, introducing variability; Best@k mitigates this. gem5-based measurements are computationally heavy, so datasets were curated (balanced subsets with de-duplicated test cases). Fine-tuning can yield higher speedups on the same open-source model but is impractical for closed-source models and requires substantial training resources.

# Abstract
- Problem: LLM-based code runtime optimization needs performance reasoning; slow–fast pair guidance can cause superficial imitation.
- Proposal: ECO—performance-aware prompting. Distill runtime optimization instructions (ROIs) from slow–fast pairs. At inference, combine (i) symbolic advisor (CPG-based bottleneck diagnosis) and (ii) ROI retriever (retrieve related ROIs) into prompts.
- Properties: Model-agnostic, no fine-tuning; prompts prepend to any code-LLM.
- Results: ECO significantly improves efficiency, achieving speedups up to 7.81× while minimizing correctness loss.

# Introduction
- Challenge: Correctness ≠ efficiency; optimizing requires reasoning about algorithmic restructuring, data structure choice, and library/loop performance beyond compiler IR-level optimizations.
- Prior LLM methods: CoT; ICL/RAG using slow–fast pairs (random or similarity-based retrieval); fine-tuning on slow–fast mappings. Limitations: pair-based guidance lacks interpretable rationale; models imitate surface patterns; retrieval/fine-tuning misaligns with actual bottlenecks.
- ECO: Distills ROIs from slow–fast pairs, stores as knowledge base; at inference combines symbolic advisor (CPG-based deterministic bottleneck detection) and ROI retriever (performance-aware ROI selection) to produce targeted prompts.
- Evaluation overview: PIE (in-distribution) and Codeforces (OOD); across model scales; model-agnostic, including closed-source. Reported highlight: GPT-4o-mini standard prompting speedup 1.99× vs ECO 7.81× (nearly 4× gain). Figure 1 contrasts ECO prompts vs conventional code-pair retrieval.

# Related Work
- Landscape (Table 1): Instruction-only, ICL, RAG, fine-tune (PIE), compiler, Supersonic, SBLLM vs ECO. ECO uniquely offers bottleneck diagnosis and ROI-based optimization knowledge; train-free and non-iterative at program scope.
- Summaries:
  - Compiler (LLVM/GCC): rule-based IR optimizations; limited for program-level bottlenecks.
  - PIE: fine-tunes on slow–fast pairs; encodes before–after patterns without rationale.
  - Supersonic: trains to predict diff patches; retraining required; risks invalid outputs.
  - SBLLM: RAG + iterative revision; increased inference cost; semantic drift risk.
- Contributions claimed:
  - Performance-aware prompting via distilled ROIs.
  - Complementary modules: rule-based symbolic advisor + context-aware ROI retriever.
  - Model-agnostic plug-in prompting; no fine-tuning required.

# Method
- Framework: Generate performance-aware prompts combining bottleneck diagnosis and related ROIs; prepend to any code-LLM input.
- ROI Distillation (Section 3.1):
  - Data: PIE HQ with 4,085 C++ slow–fast pairs.
  - Process: Reasoning LLM (DeepSeek-r1:32b) analyzes pairs to extract compact ROI O_i describing root inefficiency and rationale. Build ROI DB D = {(slow_i, fast_i, O_i)}. Prompt and LLM config details in Appendix A.1.
- Symbolic Advisor (Section 3.2):
  - Built on Joern CPGs; manually clustered ROIs mapped to rule–template pairs P = {(r, t)} for categories: (i) Inefficient Algorithms, (ii) Data Structure Usage, (iii) Library Usage, (iv) Loop Structures.
  - Example Rule (Algorithm 1): Detect recursion without memoization via self-call detection, read/write overlap, absence of memo table declaration.
  - Pipeline (Algorithm 2): Build CPG, apply each rule, instantiate templates into bottleneck diagnoses B.
- ROI Retriever (Section 3.3):
  - For each DB entry, compute embedding v_i = Φ(O_i) with an embedding model; at inference, prompt the inference model to produce a performance-focused explanation E_C of input code; embed v_C = Φ(E_C); retrieve top-k most similar entries (slow_i, fast_i, O_i). Supplies both examples and explicit ROI descriptions.

# Experiments
- 4.1 Experimental Settings:
  - Models:
    - Reasoning model for ROI distillation: DeepSeek-r1:32b.
    - Embedding model for retrieval: Qodo-Embed-1-1:5b.
    - Inference models: Qwen2.5-Coder family (3B/7B/14B), GPT-4o-mini, GPT-o4-mini; temperature 0.7.
  - Baselines:
    - Instruction-only; CoT (step-by-step reasoning).
    - ICL (random slow–fast pairs); RAG (code similarity retrieval).
    - Supersonic (CodeBERT encoder generating diff patches).
    - SBLLM (search + RAG, iterative refinement).
  - Datasets:
    - PIE HQ (4,085 slow–fast pairs) for reference/distillation/retrieval.
    - PIE balanced test subset: 255 slow codes, each with 10 test cases.
    - Codeforces dataset: 300 slow codes (OOD), each with 10 test cases.
  - Metrics and measurement:
    - Best@k selection protocol.
    - OPT: correct and >10% faster (T(o) − T(n) > 0.1 × T(o)).
    - SP = T(o)/T(n) (incorrect/slower assigned SP = 1.0).
    - ACC: passes all test cases.
    - Build: GCC 9.3.0, C++17, -O3; runtime via gem5 (cycle-accurate).
- Table 2 (PIE, Qwen2.5-Coder-7B, averages with ± over 10 trials):
  - Instruction-only: Best@1 ACC 33.61 (±2.18), SP 1.17× (±0.06), OPT 5.92 (±1.41). Best@5 ACC 68.12 (±1.54), SP 1.44× (±0.03), OPT 15.92 (±1.20).
  - CoT: Best@1 ACC 34.67 (±1.44), SP 1.16× (±0.03), OPT 6.04 (±0.44). Best@5 ACC 63.61 (±1.21), SP 1.39× (±0.02), OPT 15.18 (±0.79).
  - ICL: Best@1 ACC 35.33 (±2.00), SP 1.27× (±0.05), OPT 8.12 (±1.22). Best@5 ACC 70.75 (±1.24), SP 1.82× (±0.04), OPT 23.10 (±0.94).
  - RAG: Best@1 ACC 29.69 (±3.01), SP 1.52× (±0.15), OPT 11.41 (±1.43). Best@5 ACC 64.51 (±1.78), SP 2.51× (±0.12), OPT 30.31 (±1.12).
  - Supersonic: Best@1 ACC 7.06 (±3.63), SP 1.00× (±0.01), OPT 0.04 (±0.12). Best@5 ACC 14.75 (±0.61), SP 1.01× (±0.01), OPT 0.20 (±0.20).
  - SBLLM: Best@1 ACC 21.73 (±2.21), SP 1.06× (±0.01), OPT 2.35 (±0.53). Best@5 ACC 55.80 (±3.15), SP 1.22× (±0.04), OPT 7.61 (±0.95).
  - ECO: Best@1 ACC 36.27 (±2.88), SP 2.15× (±0.11), OPT 23.84 (±1.13). Best@5 ACC 74.24 (±1.46), SP 3.26× (±0.09), OPT 48.04 (±1.17).
- 4.2 Comparison with Baselines:
  - ECO highest SP and OPT while maintaining correctness; achieves gains without additional training or iterative search.
  - Supersonic/SBLLM underperform due to low accuracy; Supersonic produced >80% malformed patches (qualitative note in text).
- 4.3 Ablation Studies (Table 3; Qwen2.5-Coder-7B; ± over 10 trials):
  - ECO: Best@1 ACC 36.27 (±2.88), SP 2.15× (±0.11), OPT 23.84 (±1.13); Best@5 ACC 74.24 (±1.46), SP 3.26× (±0.09), OPT 48.04 (±1.17).
  - w/o RR (SA only): Best@1 ACC 48.59 (±1.47), SP 1.97× (±0.14), OPT 21.61 (±1.10); Best@5 ACC 83.45 (±1.29), SP 3.08× (±0.10), OPT 42.75 (±1.19).
  - w/o SA (RR only): Best@1 ACC 32.98 (±2.14), SP 1.87× (±0.15), OPT 18.39 (±2.06); Best@5 ACC 70.39 (±2.00), SP 3.10× (±0.12), OPT 42.12 (±1.11).
  - w/o RR+SA: Best@1 ACC 36.20 (±2.04), SP 1.38× (±0.13), OPT 9.41 (±1.58); Best@5 ACC 73.80 (±1.79), SP 2.26× (±0.16), OPT 30.75 (±2.49).
- 4.4 Generalizability (Table 4; Best@5):
  - Qwen2.5-Coder:3b: Instruction ACC 55.22, SP 1.30×, OPT 11.18; ECO ACC 37.80, SP 1.85×, OPT 16.67. Codeforces: Instruction ACC 18.87, SP 1.01×, OPT 0.40; ECO ACC 16.17, SP 1.11×, OPT 2.57.
  - Qwen2.5-Coder:7b: Instruction ACC 68.12, SP 1.44×, OPT 15.92; ECO ACC 74.24, SP 3.26×, OPT 48.04. Codeforces: Instruction ACC 29.20, SP 1.01×, OPT 0.47; ECO ACC 35.20, SP 1.78×, OPT 13.83.
  - Qwen2.5-Coder:14b: Instruction ACC 67.76, SP 1.48×, OPT 17.92; ECO ACC 79.84, SP 3.67×, OPT 53.69. Codeforces: Instruction ACC 39.50, SP 1.09×, OPT 2.13; ECO ACC 45.40, SP 2.30×, OPT 23.83.
  - GPT-4o-mini: Instruction ACC 83.53, SP 1.53×, OPT 19.61; ECO ACC 94.51, SP 3.97×, OPT 60.78. Codeforces: Instruction ACC 49.23, SP 1.01×, OPT 0.33; ECO ACC 59.93, SP 2.01×, OPT 18.70.
  - GPT-o4-mini: Instruction ACC 95.29, SP 1.99×, OPT 36.08; ECO ACC 97.25, SP 7.81×, OPT 84.71. Codeforces: Instruction ACC 65.63, SP 1.41×, OPT 7.33; ECO ACC 73.67, SP 4.55×, OPT 42.07.
- 4.5 Case Study:
  - Error/optimization distribution analysis (Figure 3): ECO yields higher optimized proportions and lower “correct but not optimized” proportions than baselines; variability due to retriever stochasticity (Best@k improves stability). Symbolic advisor bottleneck distribution (Figure 4, left): library usage (I/O) is most frequent; algorithmic inefficiencies also large; data structure and loop usage less frequent. Keyword overlap analysis (Figure 4, right): ROI retriever retrieves performance-relevant terms (scanf, cin, cout, vector, sort), whereas RAG overlaps on superficial tokens (sum, cnt, ans, mod).
- Additional analyses (Appendix D):
  - Fine-tune vs ECO (Table 6; Qwen2.5-Coder-7B; ± over 10 trials): Fine-tune Best@1 ACC 46.63 (±2.28), SP 2.23× (±0.22), OPT 25.28 (±2.29); Best@5 ACC 79.65 (±0.83), SP 3.73× (±0.18), OPT 52.12 (±1.00). ECO Best@5 SP 3.26×. Fine-tuning requires heavy resources (8×48GB A6000 GPUs).
  - Symbolic advisor directive quality (Table 7): Proportion of resolved slow I/O after optimization—Instruction ACC 33.61%, resolved 22.40%; RAG ACC 29.06%, resolved 48.09%; Supersonic ACC 7.06%, resolved 80.33%; ECO (symbolic advisor variant) ACC 48.59%, resolved 78.14%.

# Conclusion
- ECO supplies explicit, performance-aware prompts by combining deterministic bottleneck diagnoses with retrieved ROIs distilled from slow–fast pairs.
- Model-agnostic, no fine-tuning; consistently improves runtime efficiency across open- and closed-source LLMs and across in-distribution and OOD benchmarks.
- Results support the claim that explicit performance-aware guidance enables code-LLMs to generate optimized code more effectively.

# Appendix
- Reproducibility: Prompts in Appendix A; inference model configs, baselines, measurement tools in Appendix B; dataset curation in Section 4.1.3; public repository released.
- A.1 ROI Distillation: PIE HQ (4,085 pairs); DeepSeek-r1:32b via ollama, quantization Q4_K_M; prompt extracts description, runtime improvement (1–10), category; ROI DB D = {(slow_i, fast_i, O_i)}.
- A.2 Symbolic Advisor: Joern CPG; Scala rules/templates across four categories with examples (memoization directive, vector→static array, cin/cout→scanf/printf, loop-invariant hoisting).
- A.3 ROI Retriever: prompts inference model to produce performance analysis; embeddings via HuggingFace Qodo/Qodo-Embed-1-1.5B; retrieves top-k triplets; example retrieval replaces std::string and cin/cout with fixed-size buffer and getchar/printf.
- B Implementation Details:
  - Inference via ollama, Q4_K_M quantization; temperature 0.7; max input 4,096 tokens; max output 8,192; closed-source via official APIs.
  - Prompt formats: symbolic advisor template (where/how bullets), retrieval template with 2-shot slow–fast pairs plus ROI; baselines (instruction-only, CoT, ICL, RAG, SBLLM) templates.
  - Supersonic: CodeBERT encoder–decoder, beam search num_beams=10; outputs diff patches; evaluated in-distribution.
  - SBLLM: Iterative RAG; representative samples=3, max iterations=4; inference model Qwen2.5-Coder-7B.
  - Fine-tuning (PIE): Qwen2.5-Coder-7B; batch size 32 (micro-batch 2), LR 1e−5, cutoff length 2000, early stopping; Transformers with FSDP; hardware 8×48GB NVIDIA RTX A6000 GPUs.
  - Runtime measurements: gem5 (Intel Skylake Verbatim config); deterministic, reduced variance vs Hyperfine.
- C Dataset Details:
  - PIE-Train/HQ: 4,085 pairs; 1,474 problems; max 4 samples/problem; avg 2.77.
  - PIE-Test (original): 978 pairs, 41 problems, severe imbalance; curated PIE-Test: 255 samples, 41 problems, max 10/problem, avg 6.22; de-duplicated test cases (n-gram clustering), 10 cases per problem.
  - Codeforces-Test: 300 samples, 300 problems, 1 sample/problem; C++ only; ≥10 test cases; time limit ≤2s; 30 problems selected; 10 cases per problem.
- D Additional Experiments:
  - Detailed analysis: CoT can introduce runtime overhead; Supersonic/SBLLM lower correctness; ECO with Qwen2.5-Coder-7B outperforms instruction-only on GPT-4o-mini and GPT-o4-mini.
  - Fine-tuning comparison: Fine-tune Best@5 SP 3.73× vs ECO 3.26× on Qwen2.5-Coder-7B; closed-source unattainable by fine-tuning.
  - Directive quality: Resolved slow I/O bottlenecks higher for Supersonic and ECO (S) but Supersonic has very low ACC.

# References
- Cited works include gem5 (Binkert et al., 2011), classic compiler optimizations (Wegman & Zadeck, 1991; GCC documentation 2025; LLVM 2004), CodeNet dataset (Puri et al., 2021), LLM prompting (Brown et al., 2020; Wei et al., 2022; Poesia et al., 2021), evaluation of code LLMs (Chen et al., 2021; Xu et al., 2022), optimization-oriented methods PIE (Shypula et al., 2024), Supersonic (Chen et al., 2024), SBLLM (Gao et al., 2025), and tool references (Pc-lint plus; Hyperfine).