Summary
The paper introduces ECO, a train-free, model-agnostic prompting framework that guides code large language models toward runtime-efficient rewrites. Instead of directly feeding slow–fast code pairs, ECO distills human-interpretable runtime optimization instructions (ROIs) from a corpus of slow–fast pairs (PIE HQ, 4,085 pairs) and builds a knowledge base. At inference, ECO composes two complementary signals into a structured prompt: (i) a deterministic bottleneck diagnosis from a symbolic advisor built on Joern code property graphs (CPGs) with rules spanning algorithms, data structures, libraries, and loops, and (ii) retrieved ROIs selected via performance-oriented embeddings, using a model-produced performance analysis of the input code to guide retrieval. The system aims to encourage intent-aligned performance reasoning rather than imitation of surface patterns.

ECO is evaluated on PIE and an out-of-distribution set of Codeforces problems, across both open- and closed-source LLMs. The evaluation employs consistent compilation (-O3, C++17), deterministic runtime measurement via gem5, and multi-metric reporting (accuracy, speedup, percent optimized) with best@k decoding and a policy that assigns unit speedup to incorrect outputs. Ablations isolate the roles of the symbolic advisor and the ROI retriever, showing complementary contributions and trade-offs between accuracy and speedup. Results show sizable speedups while largely preserving correctness, with gains scaling with model capacity and transferring to closed-source models. Additional analyses (e.g., TF-IDF token overlap) suggest retrieval focuses on performance-relevant content. The paper provides extensive implementation details and artifacts.

Strengths
- Clear, principled architecture: ECO integrates deterministic, explainable CPG-based diagnoses with retrieval of distilled, interpretable ROIs, producing actionable, performance-aware prompts rather than opaque example pairs.
- Strong empirical results: Consistent and often large runtime speedups with good correctness across datasets and model families, including closed-source systems, and improved performance in an out-of-distribution Codeforces setting.
- Careful evaluation protocol: Deterministic cycle-accurate timing (gem5), standardized compilation (-O3), multi-metric reporting (accuracy, speedup, percent optimized), and best@k decoding with conservative handling of incorrect outputs mitigate common pitfalls in performance evaluation.
- Informative ablations and analyses: Experiments disentangle the contributions of the symbolic advisor and the ROI retriever, indicating both are beneficial. Additional analyses suggest retrieval emphasizes performance-relevant signals rather than superficial token overlap.
- Practicality and generality: No finetuning is required; the method works across diverse LLMs and scales with model capability. The prompting approach is simple to apply and compatible with closed-source models.
- Interpretability and determinism: Symbolic rules localize suspected bottlenecks and provide human-comprehensible diagnoses, making the guidance more transparent and auditable than black-box prompts.
- Reproducibility and transparency: The paper includes explicit algorithms, prompt and rule templates, comprehensive appendices, and details on dataset curation and implementation.

Weaknesses
- Missing intrinsic diagnostics: The paper does not quantitatively assess the fidelity of distilled ROIs (e.g., human ratings, error rates) or the precision/recall and coverage of the symbolic rules. Without rule-level activation and error statistics, it is hard to judge diagnostic reliability and robustness beyond anecdotal examples.
- Retrieval-model coupling: The retriever relies on the inference model to generate the performance analysis used for embedding and retrieval, introducing circularity that conflates retrieval quality with the model’s own reasoning ability. This reduces modularity, complicates attribution of gains, and may hinder transfer to weaker models.
- Baseline fairness concerns: Competing methods (e.g., Supersonic, SBLLM) appear under-engineered in the presented setup (malformed patches, iterative drift). Modest post-processing or additional iterations/repair loops could plausibly improve their performance, potentially narrowing reported gaps.
- Questionable or non-portable optimization advice: Some distilled guidance and examples risk non-standard or unsafe C++ practices (e.g., variable-length arrays under C++17) and omit widely accepted alternatives (e.g., sync_with_stdio(false), cin.tie(nullptr); std::array or reserve/resize). This raises external validity and portability concerns for the guidance quality.
- Limited cost and scalability analysis: The paper lacks a breakdown of end-to-end overheads (CPG construction, rule queries, retrieval, and longer prompts) and memory/latency footprints. Deployment feasibility at scale remains unclear.
- Statistical rigor and sensitivity: There are no significance tests or confidence intervals for key comparisons, and limited sensitivity analyses (e.g., temperature, number of retrieved ROIs, embedding model choice, k in retrieval). Best@k settings and sampling choices may affect fairness across methods.
- Dataset comparability: The curated PIE test set alters the evaluation distribution, complicating direct comparisons with prior work on the original PIE benchmark. A side-by-side or stratified analysis would improve external comparability.
- Prompt-length and truncation risks: The system may face prompt-length constraints when including multiple ROIs and detailed diagnoses; potential truncation effects and their impact on performance are not analyzed.
- Minor presentation issues: Some figures/tables appear cramped or partially misrendered, and minor editorial issues persist. While not critical, they slightly detract from clarity.
