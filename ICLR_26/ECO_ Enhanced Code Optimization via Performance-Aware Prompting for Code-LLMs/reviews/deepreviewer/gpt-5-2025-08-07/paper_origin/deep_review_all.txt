Review 1

Summary
The paper proposes ECO, a train-free, model-agnostic prompting framework that improves code-LLMs’ ability to optimize runtime. ECO distills “runtime optimization instructions” (ROIs) from slow–fast code pairs (PIE HQ, 4,085 pairs), builds a knowledge base, and at inference composes two signals into a performance-aware prompt: (i) a deterministic bottleneck diagnosis from a symbolic advisor using CPG/Joern rules (covering algorithms, data structures, libraries, loops) and (ii) relevant, retrieved ROIs matched via performance-oriented embeddings. ECO is evaluated on PIE and an OOD Codeforces set, across open- and closed-source LLMs, showing speedups up to 7.81× while preserving correctness (Tables 2, 4).

Soundness
- Methodologically, the combination of rule-based static analysis (Algorithms 1–2, §3.2) with retrieval from distilled, human-interpretable ROIs (§3.1, §3.3) is coherent and addresses pattern-imitation pitfalls of raw pair prompting. The advisor’s rules are plausibly grounded in clustered ROI themes (§3.2; Appendix A.2) and implemented atop CPGs (Joern), a mature static-analysis substrate.
- The experimental protocol includes accuracy, speedup, and “percent optimized” with best@k selection and SP=1 for incorrect outputs (§4.1.4). Compiling all codes at -O3 and timing via gem5 (cycle-accurate) provides a deterministic baseline (§4.1.4; Appendix B.3.4), which strengthens internal validity.
- Ablations in Table 3 isolate the roles of the symbolic advisor (SA) and ROI retriever (RR), showing both contribute and the full system improves SP and OPT.
- Potential concerns: (1) ROI distillation relies on a reasoning LLM (§3.1; Appendix A.1) without quantitative validation of ROI fidelity, potential hallucinations, or noise; (2) the retriever uses the inference model itself to generate performance analyses (§3.3), creating coupling that could bias retrieval or entangle prompt quality with model choice; (3) rule coverage/precision of the symbolic advisor are not measured (false positive/negative rates), limiting claims on diagnostic reliability; (4) baseline choices for Supersonic/SBLLM may be under-optimized (e.g., malformed patches, iterative drift) and deserve more robust error handling (Appendix B.3.1–B.3.2).
- Overall, the core claims are supported by consistent, sizable gains, but stronger measurements of ROI quality and rule coverage would improve rigor.

Presentation
- The paper is generally clear and well organized: problem framing (§1), method overview (Fig. 2), illustrative example (Fig. 1), and explicit algorithms (Algs. 1–2). The related work table is concise (Table 1), and the experimental sections are readable with comprehensive implementation details (Appendix B, C).
- A few figures/tables appear cramped or partially mangled in the text blocks (e.g., Fig. 3/4 formatting in §4.5/§4.4), but the narrative compensates. Minor editorial issues (e.g., “Related Works” heading) do not impede comprehension.

Contribution
- Conceptual: reframing optimization guidance from raw slow–fast pairs to distilled ROIs plus deterministic bottleneck diagnoses. This bridges static analysis and LLM prompting, encouraging intent-aligned performance reasoning rather than surface-pattern imitation (§1, §3).
- Empirical: consistent state-of-the-art speedups in train-free prompting across models and datasets (Table 2, Table 4). Notably large gains on GPT-o4-mini (7.81×, Table 4) demonstrate model-agnostic applicability.
- Tooling: release of rules/templates and prompts (Appendix A–B) and a curated, more balanced PIE test set (Appendix C).

Strengths
- Clear, principled architecture combining static analysis with performance-aware retrieval (Fig. 2).
- Strong empirical gains with minimal accuracy loss (Table 2) and OOD benefits (Table 4).
- Careful experimental setup: cycle-accurate timing, best@k, and thorough ablations (Table 3).
- Practicality: no finetuning; works with closed-source LLMs; prompts are simple to prepend (§1, §4.4).

Weaknesses
- Lack of quantitative evaluation for the ROI distillation quality and for the precision/recall of symbolic rules; only anecdotal examples are provided (Appendix A.2–A.2.4).
- The retriever’s reliance on the inference model for performance analysis (§3.3, §4.1.1) introduces circularity and may confound attribution of gains to retrieval vs. to the model’s own reasoning capability.
- Baseline comparisons risk fairness concerns: Supersonic’s malformed patches (Appendix B.3.1) and SBLLM’s iterative drift (Appendix B.3.2) could potentially be mitigated with stricter parsing/repair, alternative decoding, or more iterations; results might understate their ceilings.
- Some suggested edits in examples may be non-idiomatic or non-portable in C++ (e.g., replacing std::vector with int v[n] in Appendix A.2.2; VLAs are not standard C++17), and I/O recommendations ignore ios_base::sync_with_stdio(false); cin.tie(nullptr) as a fast alternative (Appendix A.2.3). This raises external-validity concerns for guidance quality.

Questions
- How many rule–template pairs are implemented in the symbolic advisor, and what are their measured precision/recall on the test sets (e.g., manual audit over a stratified sample)?
- What is the top-k used in ROI retrieval, and how sensitive are results to k and embedding model choice (§3.3, §4.1.1)?
- Can the authors report an intrinsic evaluation of ROI quality (e.g., human ratings of correctness/utility; inter-annotator agreement)?
- How robust is the approach to prompt-length constraints when scaling to large inputs and multiple ROIs (Appendix B.1/B.2)? Any truncation effects?
- Can the guidance templates be extended to recommend sync_with_stdio(false)/cin.tie(nullptr) for I/O and standard-compliant fixed-size buffers to avoid VLAs?

Rating
- Overall (10): 8 — Strong, well-motivated hybrid prompting with solid gains across datasets and models (Tables 2–4; Fig. 1–2), with remaining concerns on ROI/rule validation (§3.1, §3.2).
- Novelty (10): 8 — Distilling ROIs plus deterministic diagnoses is a notable step beyond code-pair prompting and RAG (Table 1; §1–§3).
- Technical Quality (10): 7 — Sound methodology and careful benchmarking (Algs. 1–2; §4.1.4), but lacking quantitative diagnostics for rule coverage and ROI fidelity (Appendix A.1–A.2).
- Clarity (10): 8 — Clear exposition with useful figures and ablations (Fig. 1–2; Table 3), minor formatting issues in later figures (Fig. 3–4).
- Confidence (5): 4 — Based on close reading of methods/appendices, tables/figures, and prior familiarity with CPGs/RAG/PIE; some claims hinge on unreported diagnostics.


Review 2

Summary
ECO is a prompting framework for runtime code optimization that avoids direct use of slow–fast pairs and instead provides LLMs with (i) a bottleneck diagnosis from symbolic CPG rules and (ii) retrieved runtime optimization instructions (ROIs) distilled from reference pairs. The approach is train-free and aims at intent-aligned performance guidance. Experiments on PIE and Codeforces show substantial speedups while maintaining accuracy (Table 2, Table 4).

Soundness
- The claimed limitation of pair-based guidance (pattern imitation) is plausible and well argued (§1, Fig. 1). The proposed remedy—explicit diagnoses plus ROI exemplars—is methodologically consistent (§3).
- However, several aspects need stronger evidence: (a) ROI correctness: instructions are generated by an LLM (§3.1; Appendix A.1) without an explicit quality filter; (b) Symbolic rule coverage and error rates: the paper lacks metrics on how often rules trigger correctly/incorrectly (§3.2); (c) Retrieval reliance on the inference model to form E_C (§3.3) conflates retrieval quality with model self-analysis.
- The evaluation design is otherwise careful: cycle-accurate gem5 runtime, -O3 baseline, best@k, and multiple metrics (§4.1.4). Ablations (Table 3) are informative: both SA and RR contribute, though SA-only yields the highest accuracy (Best@1: 48.59%).

Presentation
- The method is described clearly with an instructive top-level example (Fig. 1) and overview (Fig. 2). Algorithms are specified concisely (Algs. 1–2). Appendices provide prompts and implementation specifics (Appendix A–B).
- Some examples are potentially misleading: the “vector→VLA” replacement (Appendix A.2.2) is not standard C++17 and may not compile under strict flags; this should be clarified since all builds target C++17 (§4.1.4).

Contribution
- Conceptual advance: replacing opaque pair-based guidance with interpretable, performance-aware signals; mixed static–retrieval prompting for model-agnostic optimization (§1, Table 1).
- Empirical advance: large speedups, particularly with stronger LLMs (e.g., GPT-o4-mini 7.81×, Table 4), and OOD improvements on Codeforces (§4.4–4.5).

Strengths
- Clear, modular design that maps directly to actionable prompts (SA + RR, §3; Fig. 2).
- Strong performance with minimal fine-tuning demands (none required), including on closed-source LLMs (Table 4).
- Good analysis of failure modes (Fig. 3, Fig. 4 right) and role of submodules (Table 3).

Weaknesses
- Missing intrinsic evaluation of diagnostic quality: no precision/recall or human audit for rule matches or for distilled ROI correctness (Appendix A.2).
- The retriever’s dependence on the inference model’s produced analysis (§3.3) adds confounds and reduces modularity; an external, fixed analyzer might yield more stable retrieval.
- Baseline fairness concerns remain: Supersonic’s malformed patch outputs (Appendix B.3.1) could be handled with minimal post-processing/repair; SBLLM’s iterative process could be given more iterations or a repair loop, potentially improving their reported results.
- Some recommended optimizations are debatable or unsafe for portability (e.g., VLAs; I/O: not considering sync_with_stdio(false), cin.tie(nullptr), Appendix A.2.2–A.2.3).

Questions
- What is the end-to-end overhead (seconds per file) of Joern CPG construction and query execution vs. LLM inference? A scalability breakdown would be helpful.
- How many ROIs are stored and how are they clustered into rule templates? Please quantify coverage per category and rule triggers on the test set (§3.2).
- Could the authors report a user study (or expert audit) assessing the comprehensibility and usefulness of ECO prompts compared to plain RAG examples?
- How sensitive are results to quantization and temperature (Appendix B.3.0) and to the number of retrieved exemplars?
- Can the approach recommend standard-compliant alternatives for the vector case, e.g., std::array or preallocation with reserve/resize, rather than VLAs?

Rating
- Overall (10): 7 — A compelling and practical idea with strong results (Tables 2–4), tempered by missing diagnostics on ROI/rule quality and a few questionable optimization examples (Appendix A.2.2–A.2.3).
- Novelty (10): 8 — Shifts from code-pair imitation to performance-aware prompting with explicit diagnoses (Table 1; §1–§3).
- Technical Quality (10): 7 — Solid experiments and ablations (Table 3) but lacks intrinsic measurements of component quality (ROI fidelity; rule precision) (§3.1–§3.2).
- Clarity (10): 8 — Generally clear with effective figures (Fig. 1–2); minor example pitfalls should be corrected.
- Confidence (5): 4 — Based on detailed reading of methods/appendices and prior knowledge of CPG/LLM retrieval; uncertainties persist around ROI/rule evaluation.

Review 3

Summary
This work introduces ECO, a train-free prompting framework that equips code-LLMs with performance-aware guidance. ECO distills interpretable runtime optimization instructions (ROIs) from slow–fast pairs, diagnoses bottlenecks in input code with symbolic CPG rules, retrieves related ROIs using performance-focused embeddings, and concatenates both into a structured prompt. Experiments across PIE and Codeforces show notable speedups and good accuracy (Tables 2, 4), with ablations demonstrating the complementary roles of the symbolic advisor and ROI retriever (Table 3).

Soundness
- The experimental methodology is rigorous: consistent compilation flags (-O3, C++17), deterministic measurement via gem5 (§4.1.4; Appendix B.3.4), and multi-metric evaluation (ACC/SP/OPT). Best@k is defined with SP=1 for incorrect outputs, preventing inflated speedups (§4.1.4).
- The claim that ECO retrieves performance-relevant exemplars is backed by TF-IDF overlap analysis of performance tokens vs. superficial tokens (Fig. 4 right; §4.5).
- The OOD evaluation on Codeforces indicates generalization (Table 4), though the speedups there are smaller, as expected for a harder regime.
- Threats to validity: (i) no statistical significance tests across configurations; (ii) best@k sampling temperature is fixed (0.7) without sensitivity analysis (Appendix B), which may affect fairness across baselines; (iii) dataset curation alters the PIE test distribution (Appendix C), potentially complicating comparison with prior work; (iv) the retriever uses the same inference model to produce the performance analysis (§3.3), making ablations partially entangled with model capability.

Presentation
- The paper is well structured with a running example (Fig. 1), a clean framework diagram (Fig. 2), and explicit algorithms (Algs. 1–2). Implementation details are extensive (Appendix B–C).
- Some figure/table formatting looks broken in places (e.g., Fig. 3/4 rendering), but the textual explanation suffices.

Contribution
- Introduces performance-aware prompting that fuses deterministic static analysis with ROI-based retrieval to guide LLMs toward intent-aligned code optimizations (§1–§3).
- Demonstrates strong, train-free gains, scaling with model capacity and extending to closed-source LLMs (Table 4), which enhances practical relevance.

Strengths
- Measurable, significant speedups with good correctness (Table 2), including a 7.81× boost on GPT-o4-mini (Table 4).
- Deterministic, explainable diagnostics (SA) paired with flexible, context-rich guidance (RR) (Fig. 2; §3.2–§3.3).
- Robustness to OOD tasks and detailed prompt/implementation artifacts for reproducibility (Appendix A–C).

Weaknesses
- No hypothesis testing or confidence intervals for key comparisons (Tables 2–4); only mean±std over 10 trials is reported.
- The curated PIE test set introduces a new evaluation distribution (Appendix C) without a direct mapping to the original PIE leaderboard; a side-by-side on the original test set (or a stratified slice) would help external comparisons.
- Limited reporting on inference-time cost overheads (Joern build + queries + retrieval + longer prompts).
- Rule coverage/precision, and ROI correctness, are not quantitatively assessed.

Questions
- What is the end-to-end latency per sample (median and variance) for SA+RR+LLM vs. baselines? A cost–quality trade-off would be informative.
- How sensitive are results to the number of retrieved ROIs and to prompt length constraints? Any degradation due to context truncation (Appendix B.1/B.2)?
- Could the authors report results on the unmodified PIE test set for comparability, even if only on a subset?
- Do the authors have measurements of SA rule activation frequency per category and false positives (e.g., flagging I/O where it is not the bottleneck, Fig. 4 left)?
- Did the authors try a retriever that embeds code+CPG-derived features directly rather than LLM-produced text E_C (§3.3)?

Rating
- Overall (10): 8 — Strong empirical gains under a careful evaluation protocol (Tables 2–4) and a persuasive hybrid design (Fig. 2); some missing cost/coverage analyses.
- Novelty (10): 7 — Builds on known components (CPGs, retrieval) but the performance-aware prompting formulation and ROI distillation are meaningfully new (Table 1; §3.1–§3.3).
- Technical Quality (10): 8 — Deterministic timing, thorough ablations (Table 3), and cross-model tests (Table 4); would benefit from significance tests and diagnostic metrics.
- Clarity (10): 8 — Clear framing and method details (Algs. 1–2; Figs. 1–2); minor figure formatting issues.
- Confidence (5): 4 — Based on a detailed read and cross-check with appendices; some internal diagnostics are missing but overall evidence is convincing.

Review 4

Summary
ECO offers a performance-aware prompting framework for code runtime optimization that marries a symbolic advisor (CPG-based rules emitting location-specific diagnoses) with an ROI retriever (performance-centered retrieval of distilled instructions from slow–fast examples). The combined prompt steers code-LLMs to rewrite code efficiently without finetuning. Experiments on PIE and Codeforces demonstrate substantial speedups and solid correctness, scaling with model capacity and working on closed-source systems (Tables 2, 4).

Soundness
- The symbolic advisor is grounded in program analysis: rules are instantiated over Joern CPGs, covering four inefficiency categories, with a worked example for recursion without memoization (Algorithm 1; §3.2; Appendix A.2.1–A.2.4). The pipeline (Algorithm 2) is well specified.
- ROI distillation and retrieval are plausible but would benefit from quality controls. The distillation step leverages a reasoning LLM with a structured template (Appendix A.1), and retrieval embeds the model-produced performance analysis (E_C) aligning the search with optimization intent (§3.3, §3.1).
- The empirical design is careful—gem5 for deterministic timing (§4.1.4), diverse baselines (§4.1.2), and informative ablations (Table 3).
- Concerns: (i) absence of quantitative rule precision/recall; (ii) examples that could advocate non-standard or risky edits (e.g., VLAs; Appendix A.2.2) may mislead downstream LLMs; (iii) retriever’s dependence on the inference model for E_C may reduce modularity and complicate transfer to weaker models (§4.4 observations on 3B).

Presentation
- Method presentation is concise and readable with illustrative figures (Fig. 1–2) and explicit algorithms. Appendices provide prompt templates and concrete examples. Some figures in §4.5 render oddly, but descriptions suffice.
- The paper takes care to position ECO relative to compiler optimizations and existing LLM-based methods (Table 1; §2).

Contribution
- A practical, train-free method to inject performance-aware, interpretable guidance into code-LLM optimization that works across model families, including closed-source. This bridges static analysis (precise localization) with retrieval (contextual breadth) for optimization intent.

Strengths
- Strong gains in both in-distribution and OOD settings (Tables 2, 4), with particularly notable improvements on higher-capacity models (§4.4).
- Modular design with clear ablation support (Table 3).
- Reproducibility focus: prompts, implementation details, curated datasets (Appendix A–C).

Weaknesses
- No empirical assessment of the correctness of ROIs or the precision/recall of symbolic detections; hard to gauge robustness outside examples.
- Some guidance may conflict with best C++ practices or standards (e.g., VLAs, I/O alternatives), risking portability or correctness trade-offs (Appendix A.2.2–A.2.3).
- Baseline engineering (Supersonic/SBLLM) may not be fully competitive without patch repair or more iterations; this could understate their performance (Appendix B.3.1–B.3.2).
- Limited analysis of computational overhead (CPG building time, retrieval cost), which matters for deployment at scale.

Questions
- Please report rule-level statistics: how often does each rule fire on PIE/Codeforces, and what fraction leads to true improvements vs. false positives?
- Have you evaluated safer, standard-conforming guidance for array replacements (e.g., std::array, reserve/resize) and fast iostream settings, and how do those affect SP/ACC?
- Can an external analyzer (not the inference LLM) create E_C to decouple retrieval from model capability, and how does that affect results, especially for 3B (Table 4)?
- What are the end-to-end costs (CPG build/query + retrieval + inference) and memory footprints across models?
- How do results change with different embedding models (Φ) or by concatenating code and ROI text for embedding (§3.3)?

Rating
- Overall (10): 8 — A well-motivated, effective hybrid prompting approach with strong empirical gains (Tables 2–4; Fig. 1–2), needing better diagnostics of component quality.
- Novelty (10): 8 — Interpretable, performance-aware prompting via SA+ROI is a meaningful advance beyond pair-based guidance (Table 1; §3).
- Technical Quality (10): 7 — Solid experiments and ablations (Table 3) but lacks rule/ROI quality metrics and deployment cost analysis (§3.1–§3.2; Appendix A).
- Clarity (10): 8 — Clear method exposition with concrete examples; minor figure formatting issues notwithstanding.
- Confidence (5): 4 — High, due to comprehensive appendices and consistent results; some open questions about rule accuracy and ROI fidelity remain.