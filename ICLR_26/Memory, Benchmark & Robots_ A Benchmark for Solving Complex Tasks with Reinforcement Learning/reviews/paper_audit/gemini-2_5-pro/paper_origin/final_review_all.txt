1) Summary
This paper addresses the lack of a standardized benchmark for evaluating memory in Reinforcement Learning (RL), particularly in robotic manipulation. The authors make three primary contributions under the umbrella of the MIKASA benchmark suite. First, they propose a classification framework for memory-intensive tasks, categorizing them into Object, Spatial, Sequential, and Memory Capacity types. Second, they introduce MIKASA-Base, a unified collection of existing memory-RL environments under a common API. Third, they develop MIKASA-Robo, a novel suite of 32 memory-intensive tabletop robotic manipulation tasks designed according to their proposed taxonomy. Extensive experiments with online, offline, and Visual-Language-Action (VLA) models demonstrate that current state-of-the-art agents struggle with these tasks, highlighting the benchmark's utility in driving future research on memory-enhanced agents.2) Strengths
*   **Principled and Comprehensive Benchmark Design**
    *   The paper introduces a clear and intuitive taxonomy for memory-intensive tasks (Object, Spatial, Sequential, Memory Capacity), grounded in concepts from cognitive science (Section 4). This provides a structured framework for systematically assessing different facets of an agent's memory, which is a significant conceptual contribution beyond just providing new environments.
    *   The MIKASA-Robo suite is extensive and well-designed, featuring 32 tasks across 12 categories that directly map to the proposed taxonomy (Table 1). This allows for a fine-grained analysis of an agent's strengths and weaknesses across different memory types.
    *   The tasks within MIKASA-Robo incorporate varying levels of difficulty (e.g., `RememberColor` with 3, 5, or 9 options), enabling the evaluation of how memory mechanisms scale with complexity (Section 6.2, Figure 5).
    *   The work also contributes MIKASA-Base, which unifies several existing open-source memory benchmarks under a single API (Section 5, Table 7). This is a valuable service to the community that reduces fragmentation and enhances reproducibility.*   **Thorough Experimental Evaluation of POMDP Baselines**
    *   The paper provides a broad evaluation across multiple learning paradigms, including online RL (PPO, SAC, TD-MPC2), offline RL (BC, CQL, DP, DT, RATE), and large VLA models (Octo, OpenVLA) (Sections 6.2, 6.3, 6.4). This comprehensive set of baselines establishes the relevance and challenge of the benchmark for various sub-fields.
    *   The experimental results convincingly show that even memory-augmented agents like PPO-LSTM, RATE, and DT struggle or fail on many of the tasks, especially those with higher complexity or sparse rewards (Figure 6, Figure 10, Table 6). This demonstrates that MIKASA-Robo poses a significant challenge to current methods and is well-suited to drive progress.
    *   The experiments correctly identify that standard robotics algorithms like SAC and TD-MPC2, which lack explicit memory, perform poorly on these tasks, highlighting the benchmark's utility in pushing for memory-aware agent design in robotics (Section 6.2, Figure 5).*   **High Potential for Community Impact and Reproducibility**
    *   The benchmark is made highly accessible through a simple `pip` installation (`pip install mikasa-robo-suite`), which lowers the barrier to entry for researchers (Abstract, Section 8).
    *   The authors release offline datasets for all 32 MIKASA-Robo tasks, which is a major contribution that facilitates research in offline RL and imitation learning without requiring users to run expensive simulations for data collection (Section 1, Appendix B).
    *   The paper and its extensive appendix provide detailed descriptions of all tasks, implementation details, and code snippets for getting started (Appendix A, C, H, I). This focus on documentation and ease of use significantly enhances the potential for adoption and reproducible research.3) Weaknesses
*   **Clarity and Structure of the Main Paper**
    *   Key results that validate the benchmark's design are relegated to the appendix. For instance, the full set of results intended to show task solvability in the MDP setting is in Appendix F (Figures 7, 8), while the main paper only shows a small sample (Figure 4).
    *   MIKASA-Base, presented as one of the core contributions, is described very briefly in Section 5. All substantive details about the included tasks are in Appendix I and Table 7. This makes the contribution of MIKASA-Base feel underdeveloped in the main paper.
    *   The number of contributions is inconsistently reported as three in the Abstract but four in the Introduction and Conclusion (Abstract vs. Section 1), creating minor confusion.
    *   The appendix contains numerous incorrect figure references, where the text refers to a figure number that does not match the corresponding image or caption (e.g., Appendix H.1 refers to Figure 11 for the ShellGame task; Appendix H.5 refers to Figure 16 for the Intercept task). This indicates a lack of careful proofreading.*   **Discrepancies in Experimental Validation and Reporting**
    *   A core claim used to validate the benchmark—that all tasks are solvable with 100% success in the fully observable MDP setting—is contradicted by the paper's own figures. The text explicitly states a "100% success rate" is achieved (Appendix F, Figure 7 caption), but the plots in Figure 7 show several tasks plateauing below this level (e.g., `InterceptSlow`, `ShellGamePush`, `RotateStrictPos`). This inconsistency undermines the fundamental claim of task solvability and the rigor of the benchmark's validation.
    *   The reference list contains multiple instances of improper citation practices. Several references point to future-dated publications with invalid identifiers (e.g., [21], [48] are cited as 2025 preprints), while others have mismatched years and preprint IDs (e.g., [15]). This prevents verification of cited work and raises concerns about the manuscript's scholarly diligence.*   **Limited Scope of Baseline Memory Architectures**
    *   The online RL experiments use LSTM as the sole memory-enabled baseline (Section 6.2). While LSTM is a standard choice, the evaluation would be more comprehensive if it included comparisons against other modern memory architectures, such as Transformers or State-Space Models (which are mentioned in Appendix E).
    *   The offline RL evaluation shows that Transformer-based models like DT and RATE perform poorly on many tasks (Figure 6, Table 6). It is unclear whether this is a failure of the memory architecture itself or a more general difficulty of applying offline RL to these visually complex, sparse-reward tasks.
    *   The VLA models evaluated (Octo, OpenVLA) were not explicitly designed for long-term memory, as the authors acknowledge (Section 6.4). The evaluation stops short of exploring or discussing how a VLA augmented with an explicit memory module might perform.*   **Insufficient Discussion on Task Design Nuances**
    *   The paper defines the "correlation horizon" (ξ) as a key property of memory-intensive tasks (Section 3.2) but does not quantify or estimate this value for the new MIKASA-Robo tasks. Providing such a metric would help researchers better understand the temporal memory demands of each task.
    *   The paper does not discuss potential confounding factors in task design, such as the interplay between perception and memory. It is unclear how performance would be affected by more subtle perceptual challenges, which could make it difficult to isolate failures in the memory module from failures in the perception module. (No direct evidence found in the manuscript).
    *   The comparison to the concurrent work MemoryBench [21] is very brief (Section 6). A more detailed comparison in the Related Work section (Section 2) would better contextualize MIKASA-Robo's unique contributions.4) Suggestions for Improvement
*   **Improve Main Paper Structure and Clarity**
    *   To make the benchmark validation more prominent, consider adding a summary table or a more comprehensive figure in the main paper that aggregates the success rates achieved in the MDP setting across all 32 MIKASA-Robo tasks.
    *   I suggest expanding Section 5 to better highlight the contribution of MIKASA-Base, perhaps by including a summary table in the main text (a condensed version of Table 7).
    *   Please ensure the number of contributions is stated consistently throughout the manuscript, for example by clarifying in the introduction that the datasets are a sub-contribution of MIKASA-Robo.
    *   Please perform a thorough proofread of the appendix to correct all figure references and ensure they match the corresponding images and captions.*   **Improve Reporting Accuracy and Adhere to Scholarly Standards**
    *   Please correct the discrepancy in the MDP validation results. Either re-run the experiments to achieve 100% success or, more practically, revise the text and captions in Appendix F and Figure 7 to accurately reflect the plotted results. A discussion on why certain tasks are not fully solvable even with full state information would be necessary.
    *   Thoroughly review and correct the entire reference list. All placeholder, future-dated, or incorrect citations must be replaced with accurate and verifiable references.*   **Broaden the Evaluation of Memory Architectures**
    *   To provide a more robust baseline, I recommend including at least one additional, more recent memory-based agent (e.g., one based on Transformers or SSMs) in the online RL experiments in Section 6.2.
    *   Please add a discussion to Section 6.3 about the potential confounding factors in the offline RL evaluation, acknowledging that poor performance could stem from challenges in the offline learning algorithm itself, not just the memory module.
    *   In Section 6.4, consider adding a brief discussion on how the MIKASA-Robo benchmark could be used to test future VLA models that are explicitly designed with memory modules.*   **Enhance Discussion of Task Design Nuances**
    *   I suggest adding a column to Table 1 or a discussion in Appendix H that provides an estimate of the correlation horizon (ξ) for each MIKASA-Robo task.
    *   Please add a paragraph in the limitations (Section 7) or task design section (Section 6.1) discussing potential confounding factors like perceptual difficulty.
    *   I recommend expanding the comparison to MemoryBench [21] in the Related Work section (Section 2) to help readers better appreciate the distinct scope and contributions of MIKASA-Robo.5) Score
*   Overall (10): 6 — While the proposed benchmark is valuable, the paper suffers from significant flaws, including a core validation claim contradicted by its own data (Figure 7, Appendix F) and improper citation practices ([21], [48]), which undermine its technical quality and credibility.
*   Novelty (10): 8 — The proposed taxonomy and the MIKASA-Robo suite of 32 novel robotic tasks remain a significant and novel contribution, despite issues in the paper's execution (Section 4.2, Table 1).
*   Technical Quality (10): 5 — The technical quality is severely impacted by the contradiction between claimed and plotted validation results (Appendix F, Figure 7) and the use of numerous invalid, future-dated citations ([21], [48], [15]).
*   Clarity (10): 6 — The paper's clarity is hampered by key results being in the appendix, inconsistent reporting of contributions (Abstract vs. Section 1), and numerous incorrect figure references in the appendix (Appendix H).
*   Confidence (5): 5 — I am highly confident in my assessment, as the paper falls squarely within my area of expertise in reinforcement learning and robotics benchmarks.