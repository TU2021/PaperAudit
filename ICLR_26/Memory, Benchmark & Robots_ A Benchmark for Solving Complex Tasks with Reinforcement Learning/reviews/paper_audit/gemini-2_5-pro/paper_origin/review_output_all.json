{
  "baseline_review": "1) Summary\nThis paper addresses the lack of a standardized benchmark for evaluating memory in Reinforcement Learning (RL), particularly in robotic manipulation. The authors make three primary contributions under the umbrella of the MIKASA benchmark suite. First, they propose a classification framework for memory-intensive tasks, categorizing them into Object, Spatial, Sequential, and Memory Capacity types. Second, they introduce MIKASA-Base, a unified collection of existing memory-RL environments under a common API. Third, they develop MIKASA-Robo, a novel suite of 32 memory-intensive tabletop robotic manipulation tasks designed according to their proposed taxonomy. Extensive experiments with online, offline, and Visual-Language-Action (VLA) models demonstrate that current state-of-the-art agents struggle with these tasks, highlighting the benchmark's utility in driving future research on memory-enhanced agents.2) Strengths\n*   **Principled and Comprehensive Benchmark Design**\n    *   The paper introduces a clear and intuitive taxonomy for memory-intensive tasks (Object, Spatial, Sequential, Memory Capacity), grounded in concepts from cognitive science (Section 4). This provides a structured framework for systematically assessing different facets of an agent's memory, which is a significant conceptual contribution beyond just providing new environments.\n    *   The MIKASA-Robo suite is extensive and well-designed, featuring 32 tasks across 12 categories that directly map to the proposed taxonomy (Table 1). This allows for a fine-grained analysis of an agent's strengths and weaknesses across different memory types.\n    *   The tasks within MIKASA-Robo incorporate varying levels of difficulty (e.g., `RememberColor` with 3, 5, or 9 options), enabling the evaluation of how memory mechanisms scale with complexity (Section 6.2, Figure 5).\n    *   The work also contributes MIKASA-Base, which unifies several existing open-source memory benchmarks under a single API (Section 5, Table 7). This is a valuable service to the community that reduces fragmentation and enhances reproducibility.*   **Thorough and Rigorous Experimental Evaluation**\n    *   A crucial strength is the validation of the MIKASA-Robo tasks. The authors demonstrate that a standard PPO-MLP agent can achieve a 100% success rate when provided with full state information (MDP mode), confirming that the tasks are inherently solvable and that failures in the partially observable setting are attributable to memory and perception challenges (Section 6.2, Figure 4, Appendix F).\n    *   The paper provides a broad evaluation across multiple learning paradigms, including online RL (PPO, SAC, TD-MPC2), offline RL (BC, CQL, DP, DT, RATE), and large VLA models (Octo, OpenVLA) (Sections 6.2, 6.3, 6.4). This comprehensive set of baselines establishes the relevance and challenge of the benchmark for various sub-fields.\n    *   The experimental results convincingly show that even memory-augmented agents like PPO-LSTM, RATE, and DT struggle or fail on many of the tasks, especially those with higher complexity or sparse rewards (Figure 6, Figure 10, Table 6). This demonstrates that MIKASA-Robo poses a significant challenge to current methods and is well-suited to drive progress.*   **High Potential for Community Impact and Reproducibility**\n    *   The benchmark is made highly accessible through a simple `pip` installation (`pip install mikasa-robo-suite`), which lowers the barrier to entry for researchers (Abstract, Section 8).\n    *   The authors release offline datasets for all 32 MIKASA-Robo tasks, which is a major contribution that facilitates research in offline RL and imitation learning without requiring users to run expensive simulations for data collection (Section 1, Appendix B).\n    *   The paper and its extensive appendix provide detailed descriptions of all tasks, implementation details, and code snippets for getting started (Appendix A, C, H, I). This focus on documentation and ease of use significantly enhances the potential for adoption and reproducible research.3) Weaknesses\n*   **Clarity and Structure of the Main Paper**\n    *   Key results that validate the benchmark's design are relegated to the appendix. For instance, the full set of results showing that all 32 tasks are solvable in the MDP setting is in Appendix F (Figures 7, 8), while the main paper only shows a small sample (Figure 4). Presenting a more comprehensive summary of this validation in the main text would strengthen the paper's core claims.\n    *   MIKASA-Base, presented as one of the core contributions, is described very briefly in Section 5. All substantive details about the included tasks are in Appendix I and Table 7. This makes the contribution of MIKASA-Base feel underdeveloped in the main paper compared to the extensive treatment of MIKASA-Robo.\n    *   The relationship between the two main components, MIKASA-Base and MIKASA-Robo, could be established more clearly in the introduction. The narrative focuses heavily on robotics, which might lead readers to overlook that MIKASA-Base is a separate, non-robotic contribution aimed at unifying existing benchmarks.*   **Limited Scope of Baseline Memory Architectures**\n    *   The online RL experiments use LSTM as the sole memory-enabled baseline (Section 6.2). While LSTM is a standard choice, the evaluation would be more comprehensive if it included comparisons against other modern memory architectures, such as Transformers or State-Space Models (which are mentioned in Appendix E), to better situate the benchmark's difficulty.\n    *   The offline RL evaluation shows that Transformer-based models like DT and RATE perform poorly on many tasks (Figure 6, Table 6). It is unclear whether this is a failure of the memory architecture itself or a more general difficulty of applying offline RL to these visually complex, sparse-reward tasks. The analysis does not disentangle these factors.\n    *   The VLA models evaluated (Octo, OpenVLA) were not explicitly designed for long-term memory, as the authors acknowledge (Section 6.4). While these experiments effectively highlight a limitation in current VLAs, the evaluation stops short of exploring or discussing how a VLA augmented with an explicit memory module might perform.*   **Insufficient Discussion on Task Design Nuances**\n    *   The paper defines the \"correlation horizon\" (ξ) as a key property of memory-intensive tasks (Section 3.2) but does not quantify or estimate this value for the new MIKASA-Robo tasks. Providing such a metric would help researchers better understand the temporal memory demands of each task.\n    *   The paper does not discuss potential confounding factors in task design, such as the interplay between perception and memory. For example, in tasks like `RememberColor`, the colors are visually distinct. It is unclear how performance would be affected by more subtle perceptual challenges, which could make it difficult to isolate failures in the memory module from failures in the perception module. (No direct evidence found in the manuscript).\n    *   The comparison to the concurrent work MemoryBench [21] is very brief (Section 6). A more detailed comparison in the Related Work section (Section 2), perhaps in a table, would better contextualize MIKASA-Robo's unique contributions in terms of task diversity and memory types covered.4) Suggestions for Improvement\n*   **Improve Main Paper Structure and Clarity**\n    *   To make the benchmark validation more prominent, consider adding a summary table or a more comprehensive figure in the main paper that aggregates the 100% success rates achieved in the MDP setting across all 32 MIKASA-Robo tasks, rather than relying on the appendix.\n    *   I suggest expanding Section 5 to better highlight the contribution of MIKASA-Base. This could be achieved by including a summary table in the main text (a condensed version of Table 7) that lists the included environments and the memory types they test.\n    *   Please clarify the dual nature of the MIKASA contribution in the introduction. Explicitly stating that the project offers two distinct benchmarks—one that unifies existing environments (MIKASA-Base) and another that introduces novel robotic tasks (MIKASA-Robo)—would improve the clarity of the paper's structure and contributions.*   **Broaden the Evaluation of Memory Architectures**\n    *   To provide a more robust baseline, I recommend including at least one additional, more recent memory-based agent (e.g., one based on Transformers or SSMs) in the online RL experiments in Section 6.2. This would offer a more complete picture of where the difficulty frontier lies for current memory architectures.\n    *   Please add a discussion to Section 6.3 about the potential confounding factors in the offline RL evaluation. Acknowledging that poor performance could stem from challenges in the offline learning algorithm itself, not just the memory module, would add nuance. Suggesting future work to compare online vs. offline versions of the same memory architecture would be a valuable addition.\n    *   In Section 6.4, consider adding a brief discussion on how the MIKASA-Robo benchmark could be used to test future VLA models that are explicitly designed with memory modules. This would frame the current results as a valuable baseline and point toward promising research directions.*   **Enhance Discussion of Task Design Nuances**\n    *   I suggest adding a column to Table 1 or a discussion in Appendix H that provides an estimate of the correlation horizon (ξ) for each MIKASA-Robo task. This would provide a useful quantitative measure of the memory demand for other researchers.\n    *   Please add a paragraph in the limitations (Section 7) or task design section (Section 6.1) discussing potential confounding factors like perceptual difficulty. Acknowledging that visual similarity could affect performance and suggesting this as an axis for future task variations would strengthen the paper.\n    *   I recommend expanding the comparison to MemoryBench [21] in the Related Work section (Section 2). A more detailed comparison would help readers better appreciate the distinct scope and contributions of MIKASA-Robo.5) Score\n*   Overall (10): 9 — The paper introduces a high-quality, comprehensive, and much-needed benchmark suite with rigorous evaluation that clearly demonstrates its value to the community (Sections 4, 5, 6).\n*   Novelty (10): 9 — The proposed taxonomy of memory tasks and the extensive MIKASA-Robo suite of 32 novel robotic tasks are highly novel and impactful (Section 4.2, Table 1).\n*   Technical Quality (10): 9 — The benchmark design is principled, and the experimental validation is thorough, including crucial sanity checks (MDP solvability in Figure 4) and a wide range of baselines (Sections 6.2-6.4).\n*   Clarity (10): 7 — The paper is generally well-written, but its structure could be improved by moving some key validation results and details about MIKASA-Base from the appendix to the main text (Section 5, Appendix F).\n*   Confidence (5): 5 — I am highly confident in my assessment, as the paper falls squarely within my area of expertise in reinforcement learning and robotics benchmarks.",
  "final_review": "1) Summary\nThis paper addresses the lack of a standardized benchmark for evaluating memory in Reinforcement Learning (RL), particularly in robotic manipulation. The authors make three primary contributions under the umbrella of the MIKASA benchmark suite. First, they propose a classification framework for memory-intensive tasks, categorizing them into Object, Spatial, Sequential, and Memory Capacity types. Second, they introduce MIKASA-Base, a unified collection of existing memory-RL environments under a common API. Third, they develop MIKASA-Robo, a novel suite of 32 memory-intensive tabletop robotic manipulation tasks designed according to their proposed taxonomy. Extensive experiments with online, offline, and Visual-Language-Action (VLA) models demonstrate that current state-of-the-art agents struggle with these tasks, highlighting the benchmark's utility in driving future research on memory-enhanced agents.2) Strengths\n*   **Principled and Comprehensive Benchmark Design**\n    *   The paper introduces a clear and intuitive taxonomy for memory-intensive tasks (Object, Spatial, Sequential, Memory Capacity), grounded in concepts from cognitive science (Section 4). This provides a structured framework for systematically assessing different facets of an agent's memory, which is a significant conceptual contribution beyond just providing new environments.\n    *   The MIKASA-Robo suite is extensive and well-designed, featuring 32 tasks across 12 categories that directly map to the proposed taxonomy (Table 1). This allows for a fine-grained analysis of an agent's strengths and weaknesses across different memory types.\n    *   The tasks within MIKASA-Robo incorporate varying levels of difficulty (e.g., `RememberColor` with 3, 5, or 9 options), enabling the evaluation of how memory mechanisms scale with complexity (Section 6.2, Figure 5).\n    *   The work also contributes MIKASA-Base, which unifies several existing open-source memory benchmarks under a single API (Section 5, Table 7). This is a valuable service to the community that reduces fragmentation and enhances reproducibility.*   **Thorough Experimental Evaluation of POMDP Baselines**\n    *   The paper provides a broad evaluation across multiple learning paradigms, including online RL (PPO, SAC, TD-MPC2), offline RL (BC, CQL, DP, DT, RATE), and large VLA models (Octo, OpenVLA) (Sections 6.2, 6.3, 6.4). This comprehensive set of baselines establishes the relevance and challenge of the benchmark for various sub-fields.\n    *   The experimental results convincingly show that even memory-augmented agents like PPO-LSTM, RATE, and DT struggle or fail on many of the tasks, especially those with higher complexity or sparse rewards (Figure 6, Figure 10, Table 6). This demonstrates that MIKASA-Robo poses a significant challenge to current methods and is well-suited to drive progress.\n    *   The experiments correctly identify that standard robotics algorithms like SAC and TD-MPC2, which lack explicit memory, perform poorly on these tasks, highlighting the benchmark's utility in pushing for memory-aware agent design in robotics (Section 6.2, Figure 5).*   **High Potential for Community Impact and Reproducibility**\n    *   The benchmark is made highly accessible through a simple `pip` installation (`pip install mikasa-robo-suite`), which lowers the barrier to entry for researchers (Abstract, Section 8).\n    *   The authors release offline datasets for all 32 MIKASA-Robo tasks, which is a major contribution that facilitates research in offline RL and imitation learning without requiring users to run expensive simulations for data collection (Section 1, Appendix B).\n    *   The paper and its extensive appendix provide detailed descriptions of all tasks, implementation details, and code snippets for getting started (Appendix A, C, H, I). This focus on documentation and ease of use significantly enhances the potential for adoption and reproducible research.3) Weaknesses\n*   **Clarity and Structure of the Main Paper**\n    *   Key results that validate the benchmark's design are relegated to the appendix. For instance, the full set of results intended to show task solvability in the MDP setting is in Appendix F (Figures 7, 8), while the main paper only shows a small sample (Figure 4).\n    *   MIKASA-Base, presented as one of the core contributions, is described very briefly in Section 5. All substantive details about the included tasks are in Appendix I and Table 7. This makes the contribution of MIKASA-Base feel underdeveloped in the main paper.\n    *   The number of contributions is inconsistently reported as three in the Abstract but four in the Introduction and Conclusion (Abstract vs. Section 1), creating minor confusion.\n    *   The appendix contains numerous incorrect figure references, where the text refers to a figure number that does not match the corresponding image or caption (e.g., Appendix H.1 refers to Figure 11 for the ShellGame task; Appendix H.5 refers to Figure 16 for the Intercept task). This indicates a lack of careful proofreading.*   **Discrepancies in Experimental Validation and Reporting**\n    *   A core claim used to validate the benchmark—that all tasks are solvable with 100% success in the fully observable MDP setting—is contradicted by the paper's own figures. The text explicitly states a \"100% success rate\" is achieved (Appendix F, Figure 7 caption), but the plots in Figure 7 show several tasks plateauing below this level (e.g., `InterceptSlow`, `ShellGamePush`, `RotateStrictPos`). This inconsistency undermines the fundamental claim of task solvability and the rigor of the benchmark's validation.\n    *   The reference list contains multiple instances of improper citation practices. Several references point to future-dated publications with invalid identifiers (e.g., [21], [48] are cited as 2025 preprints), while others have mismatched years and preprint IDs (e.g., [15]). This prevents verification of cited work and raises concerns about the manuscript's scholarly diligence.*   **Limited Scope of Baseline Memory Architectures**\n    *   The online RL experiments use LSTM as the sole memory-enabled baseline (Section 6.2). While LSTM is a standard choice, the evaluation would be more comprehensive if it included comparisons against other modern memory architectures, such as Transformers or State-Space Models (which are mentioned in Appendix E).\n    *   The offline RL evaluation shows that Transformer-based models like DT and RATE perform poorly on many tasks (Figure 6, Table 6). It is unclear whether this is a failure of the memory architecture itself or a more general difficulty of applying offline RL to these visually complex, sparse-reward tasks.\n    *   The VLA models evaluated (Octo, OpenVLA) were not explicitly designed for long-term memory, as the authors acknowledge (Section 6.4). The evaluation stops short of exploring or discussing how a VLA augmented with an explicit memory module might perform.*   **Insufficient Discussion on Task Design Nuances**\n    *   The paper defines the \"correlation horizon\" (ξ) as a key property of memory-intensive tasks (Section 3.2) but does not quantify or estimate this value for the new MIKASA-Robo tasks. Providing such a metric would help researchers better understand the temporal memory demands of each task.\n    *   The paper does not discuss potential confounding factors in task design, such as the interplay between perception and memory. It is unclear how performance would be affected by more subtle perceptual challenges, which could make it difficult to isolate failures in the memory module from failures in the perception module. (No direct evidence found in the manuscript).\n    *   The comparison to the concurrent work MemoryBench [21] is very brief (Section 6). A more detailed comparison in the Related Work section (Section 2) would better contextualize MIKASA-Robo's unique contributions.4) Suggestions for Improvement\n*   **Improve Main Paper Structure and Clarity**\n    *   To make the benchmark validation more prominent, consider adding a summary table or a more comprehensive figure in the main paper that aggregates the success rates achieved in the MDP setting across all 32 MIKASA-Robo tasks.\n    *   I suggest expanding Section 5 to better highlight the contribution of MIKASA-Base, perhaps by including a summary table in the main text (a condensed version of Table 7).\n    *   Please ensure the number of contributions is stated consistently throughout the manuscript, for example by clarifying in the introduction that the datasets are a sub-contribution of MIKASA-Robo.\n    *   Please perform a thorough proofread of the appendix to correct all figure references and ensure they match the corresponding images and captions.*   **Improve Reporting Accuracy and Adhere to Scholarly Standards**\n    *   Please correct the discrepancy in the MDP validation results. Either re-run the experiments to achieve 100% success or, more practically, revise the text and captions in Appendix F and Figure 7 to accurately reflect the plotted results. A discussion on why certain tasks are not fully solvable even with full state information would be necessary.\n    *   Thoroughly review and correct the entire reference list. All placeholder, future-dated, or incorrect citations must be replaced with accurate and verifiable references.*   **Broaden the Evaluation of Memory Architectures**\n    *   To provide a more robust baseline, I recommend including at least one additional, more recent memory-based agent (e.g., one based on Transformers or SSMs) in the online RL experiments in Section 6.2.\n    *   Please add a discussion to Section 6.3 about the potential confounding factors in the offline RL evaluation, acknowledging that poor performance could stem from challenges in the offline learning algorithm itself, not just the memory module.\n    *   In Section 6.4, consider adding a brief discussion on how the MIKASA-Robo benchmark could be used to test future VLA models that are explicitly designed with memory modules.*   **Enhance Discussion of Task Design Nuances**\n    *   I suggest adding a column to Table 1 or a discussion in Appendix H that provides an estimate of the correlation horizon (ξ) for each MIKASA-Robo task.\n    *   Please add a paragraph in the limitations (Section 7) or task design section (Section 6.1) discussing potential confounding factors like perceptual difficulty.\n    *   I recommend expanding the comparison to MemoryBench [21] in the Related Work section (Section 2) to help readers better appreciate the distinct scope and contributions of MIKASA-Robo.5) Score\n*   Overall (10): 6 — While the proposed benchmark is valuable, the paper suffers from significant flaws, including a core validation claim contradicted by its own data (Figure 7, Appendix F) and improper citation practices ([21], [48]), which undermine its technical quality and credibility.\n*   Novelty (10): 8 — The proposed taxonomy and the MIKASA-Robo suite of 32 novel robotic tasks remain a significant and novel contribution, despite issues in the paper's execution (Section 4.2, Table 1).\n*   Technical Quality (10): 5 — The technical quality is severely impacted by the contradiction between claimed and plotted validation results (Appendix F, Figure 7) and the use of numerous invalid, future-dated citations ([21], [48], [15]).\n*   Clarity (10): 6 — The paper's clarity is hampered by key results being in the appendix, inconsistent reporting of contributions (Abstract vs. Section 1), and numerous incorrect figure references in the appendix (Appendix H).\n*   Confidence (5): 5 — I am highly confident in my assessment, as the paper falls squarely within my area of expertise in reinforcement learning and robotics benchmarks.",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 9,
        "novelty": 9,
        "technical_quality": 9,
        "clarity": 7,
        "confidence": 5
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 6,
        "novelty": 8,
        "technical_quality": 5,
        "clarity": 6,
        "confidence": 5
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "1) Summary\nThis paper addresses the lack of a standardized benchmark for evaluating memory in Reinforcement Learning (RL), particularly in robotic manipulation. The authors make three primary contributions under the umbrella of the MIKASA benchmark suite. First, they propose a classification framework for memory-intensive tasks, categorizing them into Object, Spatial, Sequential, and Memory Capacity types. Second, they introduce MIKASA-Base, a unified collection of existing memory-RL environments under a common API. Third, they develop MIKASA-Robo, a novel suite of 32 memory-intensive tabletop robotic manipulation tasks designed according to their proposed taxonomy. Extensive experiments with online, offline, and Visual-Language-Action (VLA) models demonstrate that current state-of-the-art agents struggle with these tasks, highlighting the benchmark's utility in driving future research on memory-enhanced agents.2) Strengths\n*   **Principled and Comprehensive Benchmark Design**\n    *   The paper introduces a clear and intuitive taxonomy for memory-intensive tasks (Object, Spatial, Sequential, Memory Capacity), grounded in concepts from cognitive science (Section 4). This provides a structured framework for systematically assessing different facets of an agent's memory, which is a significant conceptual contribution beyond just providing new environments.\n    *   The MIKASA-Robo suite is extensive and well-designed, featuring 32 tasks across 12 categories that directly map to the proposed taxonomy (Table 1). This allows for a fine-grained analysis of an agent's strengths and weaknesses across different memory types.\n    *   The tasks within MIKASA-Robo incorporate varying levels of difficulty (e.g., `RememberColor` with 3, 5, or 9 options), enabling the evaluation of how memory mechanisms scale with complexity (Section 6.2, Figure 5).\n    *   The work also contributes MIKASA-Base, which unifies several existing open-source memory benchmarks under a single API (Section 5, Table 7). This is a valuable service to the community that reduces fragmentation and enhances reproducibility.*   **Thorough Experimental Evaluation of POMDP Baselines**\n    *   The paper provides a broad evaluation across multiple learning paradigms, including online RL (PPO, SAC, TD-MPC2), offline RL (BC, CQL, DP, DT, RATE), and large VLA models (Octo, OpenVLA) (Sections 6.2, 6.3, 6.4). This comprehensive set of baselines establishes the relevance and challenge of the benchmark for various sub-fields.\n    *   The experimental results convincingly show that even memory-augmented agents like PPO-LSTM, RATE, and DT struggle or fail on many of the tasks, especially those with higher complexity or sparse rewards (Figure 6, Figure 10, Table 6). This demonstrates that MIKASA-Robo poses a significant challenge to current methods and is well-suited to drive progress.\n    *   The experiments correctly identify that standard robotics algorithms like SAC and TD-MPC2, which lack explicit memory, perform poorly on these tasks, highlighting the benchmark's utility in pushing for memory-aware agent design in robotics (Section 6.2, Figure 5).*   **High Potential for Community Impact and Reproducibility**\n    *   The benchmark is made highly accessible through a simple `pip` installation (`pip install mikasa-robo-suite`), which lowers the barrier to entry for researchers (Abstract, Section 8).\n    *   The authors release offline datasets for all 32 MIKASA-Robo tasks, which is a major contribution that facilitates research in offline RL and imitation learning without requiring users to run expensive simulations for data collection (Section 1, Appendix B).\n    *   The paper and its extensive appendix provide detailed descriptions of all tasks, implementation details, and code snippets for getting started (Appendix A, C, H, I). This focus on documentation and ease of use significantly enhances the potential for adoption and reproducible research.3) Weaknesses\n*   **Clarity and Structure of the Main Paper**\n    *   Key results that validate the benchmark's design are relegated to the appendix. For instance, the full set of results intended to show task solvability in the MDP setting is in Appendix F (Figures 7, 8), while the main paper only shows a small sample (Figure 4).\n    *   MIKASA-Base, presented as one of the core contributions, is described very briefly in Section 5. All substantive details about the included tasks are in Appendix I and Table 7. This makes the contribution of MIKASA-Base feel underdeveloped in the main paper.\n    *   The number of contributions is inconsistently reported as three in the Abstract but four in the Introduction and Conclusion (Abstract vs. Section 1), creating minor confusion.\n    *   The appendix contains numerous incorrect figure references, where the text refers to a figure number that does not match the corresponding image or caption (e.g., Appendix H.1 refers to Figure 11 for the ShellGame task; Appendix H.5 refers to Figure 16 for the Intercept task). This indicates a lack of careful proofreading.*   **Discrepancies in Experimental Validation and Reporting**\n    *   A core claim used to validate the benchmark—that all tasks are solvable with 100% success in the fully observable MDP setting—is contradicted by the paper's own figures. The text explicitly states a \"100% success rate\" is achieved (Appendix F, Figure 7 caption), but the plots in Figure 7 show several tasks plateauing below this level (e.g., `InterceptSlow`, `ShellGamePush`, `RotateStrictPos`). This inconsistency undermines the fundamental claim of task solvability and the rigor of the benchmark's validation.\n    *   The reference list contains multiple instances of improper citation practices. Several references point to future-dated publications with invalid identifiers (e.g., [21], [48] are cited as 2025 preprints), while others have mismatched years and preprint IDs (e.g., [15]). This prevents verification of cited work and raises concerns about the manuscript's scholarly diligence.*   **Limited Scope of Baseline Memory Architectures**\n    *   The online RL experiments use LSTM as the sole memory-enabled baseline (Section 6.2). While LSTM is a standard choice, the evaluation would be more comprehensive if it included comparisons against other modern memory architectures, such as Transformers or State-Space Models (which are mentioned in Appendix E).\n    *   The offline RL evaluation shows that Transformer-based models like DT and RATE perform poorly on many tasks (Figure 6, Table 6). It is unclear whether this is a failure of the memory architecture itself or a more general difficulty of applying offline RL to these visually complex, sparse-reward tasks.\n    *   The VLA models evaluated (Octo, OpenVLA) were not explicitly designed for long-term memory, as the authors acknowledge (Section 6.4). The evaluation stops short of exploring or discussing how a VLA augmented with an explicit memory module might perform.*   **Insufficient Discussion on Task Design Nuances**\n    *   The paper defines the \"correlation horizon\" (ξ) as a key property of memory-intensive tasks (Section 3.2) but does not quantify or estimate this value for the new MIKASA-Robo tasks. Providing such a metric would help researchers better understand the temporal memory demands of each task.\n    *   The paper does not discuss potential confounding factors in task design, such as the interplay between perception and memory. It is unclear how performance would be affected by more subtle perceptual challenges, which could make it difficult to isolate failures in the memory module from failures in the perception module. (No direct evidence found in the manuscript).\n    *   The comparison to the concurrent work MemoryBench [21] is very brief (Section 6). A more detailed comparison in the Related Work section (Section 2) would better contextualize MIKASA-Robo's unique contributions.4) Suggestions for Improvement\n*   **Improve Main Paper Structure and Clarity**\n    *   To make the benchmark validation more prominent, consider adding a summary table or a more comprehensive figure in the main paper that aggregates the success rates achieved in the MDP setting across all 32 MIKASA-Robo tasks.\n    *   I suggest expanding Section 5 to better highlight the contribution of MIKASA-Base, perhaps by including a summary table in the main text (a condensed version of Table 7).\n    *   Please ensure the number of contributions is stated consistently throughout the manuscript, for example by clarifying in the introduction that the datasets are a sub-contribution of MIKASA-Robo.\n    *   Please perform a thorough proofread of the appendix to correct all figure references and ensure they match the corresponding images and captions.*   **Improve Reporting Accuracy and Adhere to Scholarly Standards**\n    *   Please correct the discrepancy in the MDP validation results. Either re-run the experiments to achieve 100% success or, more practically, revise the text and captions in Appendix F and Figure 7 to accurately reflect the plotted results. A discussion on why certain tasks are not fully solvable even with full state information would be necessary.\n    *   Thoroughly review and correct the entire reference list. All placeholder, future-dated, or incorrect citations must be replaced with accurate and verifiable references.*   **Broaden the Evaluation of Memory Architectures**\n    *   To provide a more robust baseline, I recommend including at least one additional, more recent memory-based agent (e.g., one based on Transformers or SSMs) in the online RL experiments in Section 6.2.\n    *   Please add a discussion to Section 6.3 about the potential confounding factors in the offline RL evaluation, acknowledging that poor performance could stem from challenges in the offline learning algorithm itself, not just the memory module.\n    *   In Section 6.4, consider adding a brief discussion on how the MIKASA-Robo benchmark could be used to test future VLA models that are explicitly designed with memory modules.*   **Enhance Discussion of Task Design Nuances**\n    *   I suggest adding a column to Table 1 or a discussion in Appendix H that provides an estimate of the correlation horizon (ξ) for each MIKASA-Robo task.\n    *   Please add a paragraph in the limitations (Section 7) or task design section (Section 6.1) discussing potential confounding factors like perceptual difficulty.\n    *   I recommend expanding the comparison to MemoryBench [21] in the Related Work section (Section 2) to help readers better appreciate the distinct scope and contributions of MIKASA-Robo.5) Score\n*   Overall (10): 6 — While the proposed benchmark is valuable, the paper suffers from significant flaws, including a core validation claim contradicted by its own data (Figure 7, Appendix F) and improper citation practices ([21], [48]), which undermine its technical quality and credibility.\n*   Novelty (10): 8 — The proposed taxonomy and the MIKASA-Robo suite of 32 novel robotic tasks remain a significant and novel contribution, despite issues in the paper's execution (Section 4.2, Table 1).\n*   Technical Quality (10): 5 — The technical quality is severely impacted by the contradiction between claimed and plotted validation results (Appendix F, Figure 7) and the use of numerous invalid, future-dated citations ([21], [48], [15]).\n*   Clarity (10): 6 — The paper's clarity is hampered by key results being in the appendix, inconsistent reporting of contributions (Abstract vs. Section 1), and numerous incorrect figure references in the appendix (Appendix H).\n*   Confidence (5): 5 — I am highly confident in my assessment, as the paper falls squarely within my area of expertise in reinforcement learning and robotics benchmarks."
}