# Global Summary
This paper addresses the lack of a standardized benchmark for evaluating memory in reinforcement learning (RL), particularly in the domain of robotic manipulation. The authors introduce MIKASA (Memory-Intensive Skills Assessment Suite for Agents), a comprehensive benchmark suite with three main components. First, they propose a classification framework for memory-intensive tasks, categorizing them into Object, Spatial, Sequential, and Memory Capacity types. Second, they present MIKASA-Base, a unified collection of existing open-source memory tasks under a common API. Third, they develop MIKASA-Robo, a novel benchmark of 32 carefully designed memory-intensive tabletop robotic manipulation tasks. The paper evaluates a range of online RL (PPO, SAC, TD-MPC2), offline RL (DT, RATE, BC, CQL, DP), and Visual-Language-Action (VLA) models (Octo, OpenVLA) on the MIKASA-Robo tasks. The key finding is that most current models, including those with memory mechanisms like LSTMs and Transformers, struggle significantly with the benchmark's challenges, especially with sparse rewards or increased task complexity. For instance, offline RL baselines achieve 0% success on all tasks requiring high memory capacity or sequential memory. This highlights a critical gap in the memory capabilities of current agents and establishes MIKASA as a challenging benchmark to drive future research.

# Abstract
The paper argues that while memory is crucial for RL agents in complex tasks, the field lacks a universal benchmark to assess memory capabilities, especially in tabletop robotic manipulation. To address this, the authors introduce MIKASA (Memory-Intensive Skills Assessment Suite for Agents). The work makes three key contributions: (1) a comprehensive classification framework for memory-intensive RL tasks; (2) MIKASA-Base, a unified benchmark collecting existing memory tasks for systematic evaluation; and (3) MIKASA-Robo, a novel benchmark of 32 memory-intensive tabletop robotic manipulation tasks. The goal is to provide a unified framework to advance memory RL research.

# Introduction
- The paper highlights that many real-world problems are partially observable and require memory to handle long-term dependencies and delayed rewards.
- Unlike standard RL benchmarks like Atari or MuJoCo, memory-enhanced RL lacks a universal standard, leading to fragmented evaluation across custom environments (illustrated in Figure 1 and Table 2).
- This problem is particularly acute in robotics, where many studies create artificial partial observability (e.g., by masking states) rather than using tasks that are intrinsically memory-dependent.
- The paper makes four contributions:
    1. A classification of memory tasks into four categories.
    2. MIKASA-Base: A Gymnasium-based framework unifying existing memory-RL environments.
    3. MIKASA-Robo: A new suite of 32 robotic manipulation tasks across 12 categories (summarized in Table 1) designed to test specific memory skills.
    4. Datasets for all 32 MIKASA-Robo tasks to support offline RL research.

# Related Work
- This section reviews existing RL benchmarks designed to assess memory.
- Mentioned benchmarks include DMLab-30, PsychLab, MiniGrid, MiniWorld, MiniHack, BabyAI, POPGym, BSuite, Memory Gym, and Memory Maze.
- Table 2 illustrates the core problem: a fragmented evaluation landscape where different research papers use disparate and often non-overlapping sets of environments to test their memory-enhanced agents. Many agents are also tested on MDPs like Atari with frame stacking.

# Preliminaries
- **Partially Observable Markov Decision Process (POMDP):** Defined as a tuple (S, A, T, R, Ω, O, γ). In a POMDP, the agent's policy depends on the history of observations, π(·|o_{1:t}), not the true state.
- **Memory-intensive environments:** Defined as POMDPs where a correlation horizon ξ > 1 exists, representing the minimum timesteps between a critical event and when its information must be recalled.
- **Robotic Tabletop Manipulation:** The paper notes that most existing simulators for this domain treat tasks as fully observable MDPs, which limits the development of memory-enhanced agents for real-world scenarios.

# Method
- **4. Classification of memory-intensive tasks:**
    - To address fragmented evaluation, the paper proposes a taxonomy of memory tasks inspired by developmental psychology and cognitive science.
    - The four proposed categories are:
        1.  **Object Memory:** Maintaining information about objects, especially when occluded (object permanence).
        2.  **Spatial Memory:** Remembering environmental layouts and object locations.
        3.  **Sequential Memory:** Processing and utilizing temporally ordered information.
        4.  **Memory Capacity:** Managing multiple pieces of information simultaneously (memory span).
- **5. MIKASA-Base:**
    - A unified benchmark that consolidates existing open-source memory-intensive environments under a common Gym-like API to improve reproducibility and comparability.
    - Tasks are organized into two tiers: diagnostic vector-based environments and complex image-based tasks.
    - The selected tasks cover the four memory categories from the proposed taxonomy.
- **6. MIKASA-Robo:**
    - A new benchmark designed to evaluate memory skills in robotic manipulation, addressing a gap in existing frameworks (Table 3).
    - **6.1 MIKASA-Robo Manifestation:**
        - Contains 32 tasks across 12 categories, designed based on the four memory types. Examples include ShellGame, RememberColor, and RotateLenientPos (Figure 2).
        - Supports multiple observation modes: `state`, `RGB`, `joints`, `oracle`, and `prompt`. The standard configuration for memory testing is `RGB+joints`.
        - Implements both dense and sparse reward functions.

# Experiments
- **6.2 Online RL baselines:**
    - Baselines: PPO-MLP (memory-less), PPO-LSTM (memory), SAC, and TD-MPC2.
    - A PPO-MLP agent trained in `state` mode (full observability) achieves a 100% success rate on all tasks, confirming they are solvable (Figure 4).
    - In `RGB+joints` mode with dense rewards, PPO-LSTM outperforms PPO-MLP on the simple RememberColor3 task, but both fail as complexity increases to 5 or 9 colors (Figure 5). SAC and TD-MPC2 also perform poorly on complex tasks.
    - With sparse rewards, both PPO-MLP and PPO-LSTM fail even on the simplest tasks (Figure 10 in Appendix).
- **6.3 Offline RL baselines:**
    - Baselines: Decision Transformer (DT), RATE, Behavioral Cloning (BC-MLP), Conservative Q-Learning (CQL-MLP), and Diffusion Policy (DP).
    - Trained on datasets of 1000 successful trajectories per task, using RGB views and sparse rewards.
    - Results (Figure 6, Table 6) show that none of the models solve the majority of the 32 tasks.
    - All evaluated models achieve 0% success on tasks requiring high Memory Capacity (e.g., BunchOfColors) or Sequential Memory (e.g., ChainOfColors). For example, RATE achieves 0.92±0.01 success on ShellGameTouch-v0 but 0.00±0.00 on all ChainOfColors tasks.
- **6.4 VLA baselines:**
    - Baselines: Octo and OpenVLA, trained on 250 expert trajectories per task.
    - Octo (context size 10) achieves 0.46±0.05 success on ShellGameTouch but drops to 0.11±0.03 on RememberColor9.
    - OpenVLA with action chunk size K=8 achieves 0.47±0.05 on ShellGameTouch and 0.59±0.04 on RememberColor3, but performance drops on harder tasks. With K=4, it fails across the board (e.g., 0.12±0.05 on ShellGameTouch).
    - The authors conclude that current VLA models lack effective long-term memory and that action chunking provides only a brittle "shortcut."

# Conclusion
- The paper introduces MIKASA, a benchmark suite for memory in RL, comprising a task taxonomy, MIKASA-Base (a collection of existing tasks), MIKASA-Robo (32 new robotic tasks), and offline datasets.
- Experiments with online, offline, and VLA models demonstrate that current methods struggle with many of the benchmark's tasks, indicating a need for improved memory architectures.
- **Limitations:** The VLA model evaluations involved limited fine-tuning due to computational constraints. The benchmark could be extended with tasks requiring longer temporal dependencies or meta-RL.

# References
This section contains 102 references to related work in reinforcement learning, robotics, memory models, and benchmarks.

# Appendix
- **A, C:** Provide implementation details and code snippets for getting started with MIKASA-Robo and MIKASA-Base.
- **B:** Details the MIKASA-Robo offline RL datasets. Each of the 32 tasks has a dataset of 1000 successful trajectories collected by a PPO-MLP agent in `state` mode. Data fields include RGB images, joint states, actions, and rewards.
- **D:** Describes the setup for VLA baselines, including the dataset size (250 episodes), tasks used, and language instructions (Table 5).
- **E:** Briefly reviews memory mechanisms in RL, such as RNNs, SSMs, Transformers, and GNNs.
- **F:** Presents additional performance plots for classic baselines. Figures 7 and 8 show PPO-MLP in `state` mode achieving 100% success rate across all tasks. Figure 9 shows detailed comparisons for dense rewards, and Figure 10 shows near-zero performance for all agents with sparse rewards.
- **G:** States that experiments were run on a single NVIDIA A100 GPU and evaluation was performed over 100 episodes.
- **H:** Provides detailed descriptions for each of the 32 MIKASA-Robo tasks, organized into 12 categories (e.g., ShellGame, RememberColor, Intercept, ChainOfColors), specifying their phases, modes, success criteria, and reward structures.
- **I:** Provides descriptions for the environments included in the MIKASA-Base benchmark, with Table 7 classifying them according to the paper's memory taxonomy. Environments are drawn from sources like POPGym, BSuite, MemoryGym, and others.