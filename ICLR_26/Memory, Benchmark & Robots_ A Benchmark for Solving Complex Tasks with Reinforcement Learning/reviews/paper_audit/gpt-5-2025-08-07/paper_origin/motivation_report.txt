# Innovation & Motivation Report

## 1. The Authors' Claimed Contribution
- Core Problem: The absence of a standardized, universal benchmark to evaluate agents’ memory under partial observability and long temporal dependencies in RL, with a particular gap in tabletop robotic manipulation.
- Claimed Gap: The authors state that “Memory benchmarks exist (POPGym, DMLab-30, Memory-Gym) but focus on narrow aspects; unlike Atari or MuJoCo, memory evaluation is fragmented.” They further argue that in robotics, “Many tasks are evaluated as MDPs or made partially observable via masking/noise, not capturing real complexities (occlusions, multi-step procedures relying on memory).”
- Proposed Solution: MIKASA, comprising (i) a four-category taxonomy of memory tasks (object, spatial, sequential, capacity), (ii) MIKASA-Base, “a unified Gym-like API, reducing fragmentation,” that aggregates diverse memory environments, and (iii) MIKASA-Robo, “32 memory-intensive tabletop robotic tasks,” plus (iv) accompanying offline datasets for systematic evaluation of online RL, offline RL, and VLA models.

## 2. Comparative Scrutiny (The "Trial")
Here, we analyze how the paper stands against the identified similar works.

### vs. Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey
- Identified Overlap: Both seek to “mitigate research fragmentation” via taxonomies and standardized evaluation; the manuscript directly evaluates monolithic VLA models (Octo, OpenVLA) and emphasizes memory—the survey highlights memory as a key future direction.
- Manuscript's Defense:
  - Citation: The survey itself is not explicitly cited in the provided summary, though the manuscript cites and evaluates Octo and OpenVLA.
  - Differentiation (quoted): The manuscript’s focus is benchmarking memory in robotics with a unifying API: “MIKASA-Base: Motivation: Standardize access to diverse memory environments via a unified Gym-like API, reducing fragmentation.” It also elevates memory as an operational requirement in POMDPs: “Memory-intensive environment: POMDP with correlation horizon ξ > 1 (minimum timesteps between critical event and recall time) per [9].”
- Reviewer's Assessment: The survey is conceptual and taxonomy-oriented for VLA; the manuscript operationalizes a memory-centric benchmark and provides empirical evidence with VLA baselines. The difference is substantial and complementary: the manuscript delivers the standardized, memory-specific evaluation infrastructure the survey calls for.

### vs. An Optimistic Perspective on Offline Reinforcement Learning (DQN Replay Dataset for Atari)
- Identified Overlap: Both define offline RL benchmarking with standardized datasets and multi-algorithm evaluations; the manuscript extends this paradigm to partially observable, memory-intensive robotic manipulation with sparse rewards.
- Manuscript's Defense:
  - Citation: No explicit citation to Agarwal et al. is present in the provided summary.
  - Differentiation (quoted): The manuscript’s offline setting is robotics/memory-specific with sparse success labels: “Datasets: 1000 successful trajectories per task (Appendix B), collected by a PPO-MLP agent trained to SR=100% in state mode; RGB observations; sparse rewards (success flag).” The authors’ stated motivation to unify memory evaluation (quoted above) distinguishes their scope from Atari game replay.
- Reviewer's Assessment: The overlap is in benchmarking methodology, not content. Porting offline RL benchmarks to memory-heavy robotic POMDPs is a meaningful shift in domain characteristics (partial observability, correlation horizons, sparse rewards). The manuscript’s contribution stands as a domain-specialized benchmark rather than an incremental duplication.

### vs. Robotic Manipulation Datasets for Offline Compositional Reinforcement Learning (CompoSuite)
- Identified Overlap: Both provide large-scale, structured robotic manipulation datasets/benchmarks to test learning under systematic task variations (compositionality vs memory).
- Manuscript's Defense:
  - Citation: Not explicitly cited in the provided summary.
  - Differentiation (quoted): The manuscript’s structuring axis is memory, not composition: “4.2 Taxonomy: Object Memory; Spatial Memory; Sequential Memory; Memory Capacity.” And “32 tasks across 12 categories designed from the taxonomy; tasks isolate memory skills (object permanence, sequential recall, capacity).”
- Reviewer's Assessment: The benchmarks probe different forms of structure (temporal memory vs compositionality). While both standardize robotic offline evaluation, the manuscript’s taxonomy and task design target distinct capabilities with clear, memory-specific correlation horizons. The distinction is significant.

### vs. Benchmarking Offline Reinforcement Learning on Real-Robot Hardware
- Identified Overlap: Both standardize offline RL evaluation for robotic manipulation via expert-generated datasets and reproducible protocols, emphasizing community coordination.
- Manuscript's Defense:
  - Citation: Not explicitly cited in the provided summary.
  - Differentiation (quoted): The manuscript centers memory and partial observability, with a broader suite of tasks in simulation: “6 MIKASA-Robo: 32 tasks across 12 categories designed from the taxonomy; tasks isolate memory skills…” and provides an explicit unification motive: “MIKASA-Base: Motivation: Standardize access to diverse memory environments via a unified Gym-like API, reducing fragmentation.” It also positions itself among robotics frameworks: “Robotics framework comparison (Table 3): MIKASA-Robo supports memory tasks… many robotics frameworks do not support memory-intensive manipulation tasks (e.g., ManiSkill3, RoboCasa, Gymnasium-Robotics, etc.).”
- Reviewer's Assessment: The manuscript trades hardware execution for breadth and a targeted memory taxonomy in simulation, plus VLA and offline RL coverage under sparse rewards. Given its focus, this is a defensible, complementary niche rather than duplication. The novelty lies in memory specialization and breadth, not hardware grounding.

### vs. A Workflow for Offline Model-Free Robotic Reinforcement Learning
- Identified Overlap: Both target image-based, sparse-reward robotic manipulation with conservative offline RL; the manuscript evaluates CQL, DT, RATE, BC, and diffusion-based methods under standardized datasets.
- Manuscript's Defense:
  - Citation: The workflow paper is not explicitly cited in the provided summary; however, related algorithms (e.g., CQL) are evaluated.
  - Differentiation (quoted): The manuscript builds the benchmark substrate and shows systematic failures tied to memory demands: “Offline RL baselines largely fail on capacity and sequential memory tasks (Table 6: 0.00 ± 0.00 on BunchOfColors3/5/7, SeqOfColors3/5/7, ChainOfColors3/5/7 across all methods).”
- Reviewer's Assessment: The workflow offers procedural guidance; the manuscript offers a memory-centric testbed and empirical stress tests. The overlap is methodological, but the manuscript’s contribution is infrastructural and diagnostic for memory—complementary rather than redundant.

### vs. Benchmarking Simulated Robotic Manipulation through a Real World Dataset
- Identified Overlap: Both distribute standardized datasets, protocols, and metrics to benchmark manipulation and expose deficiencies across systems.
- Manuscript's Defense:
  - Citation: Not explicitly cited in the provided summary.
  - Differentiation (quoted): The manuscript’s focus is on memory under partial observability with unified access: “MIKASA-Base: Motivation: Standardize access to diverse memory environments via a unified Gym-like API, reducing fragmentation.” It also formalizes memory rigorously: “Memory-intensive environment: POMDP with correlation horizon ξ > 1…”
- Reviewer's Assessment: Similar benchmark-building philosophy but different target capability: memory versus simulator fidelity to real data. The manuscript’s novelty is the memory taxonomy plus extensive robotic memory tasks and datasets; this remains distinct.

## 3. Novelty Verdict
- Innovation Type: Application-Oriented
- Assessment:
  The manuscript’s central innovation is a standardized, memory-centric benchmark suite and taxonomy for RL—particularly robotics—plus carefully curated offline datasets and unified interfaces. It does not introduce new learning algorithms or theory; instead, it delivers infrastructure and evidence that existing online/offline/VLA methods fail on memory-demanding tasks. Against the provided similar works, the manuscript largely survives scrutiny: overlaps are methodological (benchmarking, offline datasets), while the scope—memory-focused robotic POMDPs with a principled taxonomy and multi-modality evaluation (state vs RGB+joints, sparse vs dense rewards, VLA baselines)—is a defensible and under-served niche.
  - Strength:
    - Clear articulation of the gap and motivation: “memory evaluation is fragmented” and robotics tasks often fail to reflect real memory challenges.
    - Concrete unification with a Gym-like API and a four-part taxonomy; broad task coverage (32 tasks) explicitly isolating memory skills.
    - Strong empirical grounding showing pervasive failures of online RL, offline RL, and VLA as memory demands increase, under standardized protocols (mean ± SEM over 100 episodes).
    - Publicly available, pip-installable suite and datasets that can catalyze community progress.
  - Weakness:
    - Several relevant benchmarking efforts (offline RL in robotics; general RL benchmarking) are not explicitly cited in the provided summary, weakening the comparative positioning beyond memory-specific prior art.
    - Offline datasets consist of “1000 successful trajectories per task… sparse rewards,” which narrows distributional coverage; this limits conclusions about offline RL beyond the specific dataset design and may overstate algorithmic shortcomings versus data limitations.
    - VLA conclusions are caveated by “limited fine-tuning… due to compute,” reducing the strength of claims about VLA memory deficits.

## 4. Key Evidence Anchors
- Introduction: “Memory benchmarks exist (POPGym, DMLab-30, Memory-Gym) but focus on narrow aspects; unlike Atari or MuJoCo, memory evaluation is fragmented.”
- Introduction: “Many tasks are evaluated as MDPs or made partially observable via masking/noise, not capturing real complexities (occlusions, multi-step procedures relying on memory).”
- Preliminaries: “Memory-intensive environment: POMDP with correlation horizon ξ > 1 (minimum timesteps between critical event and recall time) per [9].”
- Section 5 (MIKASA-Base): “Motivation: Standardize access to diverse memory environments via a unified Gym-like API, reducing fragmentation.”
- Section 6 (MIKASA-Robo): “32 tasks across 12 categories designed from the taxonomy; tasks isolate memory skills (object permanence, sequential recall, capacity).”
- Related Work / Framework comparison: “Robotics framework comparison (Table 3): MIKASA-Robo supports memory tasks… many robotics frameworks do not support memory-intensive manipulation tasks (e.g., ManiSkill3, RoboCasa, Gymnasium-Robotics, etc.).”
- Offline RL setup/results: “Datasets: 1000 successful trajectories per task (Appendix B)… sparse rewards (success flag).” and “Offline RL baselines largely fail on capacity and sequential memory tasks (Table 6: 0.00 ± 0.00 on BunchOfColors3/5/7, SeqOfColors3/5/7, ChainOfColors3/5/7 across all methods).”
- VLA baselines: Reported drops on harder memory tasks (e.g., OpenVLA K=8: “RememberColor9 0.06 ± 0.02”), supporting the claim that current VLA models lack robust memory.