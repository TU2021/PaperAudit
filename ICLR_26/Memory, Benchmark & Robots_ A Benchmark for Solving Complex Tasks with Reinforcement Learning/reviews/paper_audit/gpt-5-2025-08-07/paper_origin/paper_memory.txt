# Global Summary
- Problem: The RL community lacks a standardized, universal benchmark to evaluate agents’ memory across diverse settings, especially in tabletop robotic manipulation where partial observability and temporal dependencies are common.
- Core approach: The authors introduce MIKASA, comprising (1) a four-category taxonomy of memory tasks (object, spatial, sequential, capacity), (2) MIKASA-Base, a unified Gymnasium-like benchmark aggregating open-source memory environments, and (3) MIKASA-Robo, a suite of 32 fine-grained, memory-intensive robotic manipulation tasks, plus (4) accompanying offline datasets.
- Evaluation scope: Online RL (PPO-MLP, PPO-LSTM; SAC; TD-MPC2) in RGB+joints and state modes; Offline RL (RATE, Decision Transformer, BC-MLP, CQL-MLP, Diffusion Policy) using 1000 successful trajectories per task with sparse rewards; VLA models (Octo and OpenVLA) fine-tuned on 250 expert trajectories per task; success rates averaged over 100 evaluation episodes with mean ± SEM.
- Key findings:
  - In “state” mode (MDP, oracle-level information), PPO-MLP achieves “100% success rate” across tasks, validating task solvability without memory constraints.
  - In RGB+joints mode with dense rewards, PPO-LSTM outperforms PPO-MLP on simpler tasks (e.g., RememberColor3) but both degrade to near-zero success as difficulty increases (e.g., 5 or 9 colors); under sparse rewards, both fail even on 3 colors (Appendix F).
  - Offline RL baselines largely fail on capacity and sequential memory tasks (Table 6: 0.00 ± 0.00 on BunchOfColors3/5/7, SeqOfColors3/5/7, ChainOfColors3/5/7 across all methods). Some tasks show moderate success (e.g., RATE “0.92 ± 0.01” on ShellGameTouch-v0; DT “0.56 ± 0.01” and DP “0.68 ± 0.02” on InterceptMedium-v0).
  - VLA models show limited performance: Octo-small reaches “0.46 ± 0.05” on ShellGameTouch; OpenVLA with K=8 reaches “0.59 ± 0.04” on RememberColor3 but drops on harder variants.
- Caveats explicitly stated: Limited fine-tuning of large VLA models due to compute; the benchmark could be extended to longer temporal dependencies or meta-RL. MIKASA is publicly available (pip install mikasa-robo-suite).

# Abstract
- Motivation: Memory is crucial for RL agents facing partial observability, temporal/spatial dependencies, and delayed rewards; robotics lacks a standard memory benchmark.
- Contributions: 
  1) A classification framework for memory-intensive RL tasks.
  2) MIKASA-Base, a unified benchmark for systematic evaluation of memory-enhanced agents.
  3) MIKASA-Robo, 32 memory-intensive tabletop robotic tasks.
- Goal: Provide a unified framework to advance memory RL for real-world use. Availability: https://tinyurl.com/membenchrobots.

# Introduction
- Context: Partial observability in RL requires sequential decision-making, long-term retention, and handling delayed/sparse rewards. Memory benchmarks exist (POPGym, DMLab-30, Memory-Gym) but focus on narrow aspects; unlike Atari or MuJoCo, memory evaluation is fragmented.
- Problem in robotics: Many tasks are evaluated as MDPs or made partially observable via masking/noise, not capturing real complexities (occlusions, multi-step procedures relying on memory).
- MIKASA-Robo: 32 robotic manipulation tasks across 12 categories (Table 1) with varied difficulty and modes. Each task specifies episode timeout T, “Oracle Info” to be memorized, and (if applicable) prompts.
  - Examples and T values:
    - ShellGame (Touch/Push/Pick): T=90; Oracle Info: cup_with_ball_number; Memory: Object.
    - Intercept (Slow/Medium/Fast): T=90; Oracle Info: initial_velocity; Memory: Spatial.
    - InterceptGrab (Slow/Medium/Fast): T=90; Oracle Info: initial_velocity; Memory: Spatial.
    - RotateLenient/RotateStrict (Pos, PosNeg): T=90; Oracle Info: y_angle_diff; Prompt: target_angle; Memory: Spatial.
    - TakeItBack-v0: T=180; Oracle Info: xyz_initial; Memory: Spatial.
    - RememberColor (3/5/9): T=60; Oracle Info: true_color_indices; Memory: Object.
    - RememberShape (3/5/9): T=60; Oracle Info: true_shape_indices; Memory: Object.
    - RememberShape-AndColor (3×2 / 3×3 / 5×3): T=60; Oracle Info: true_shapes_info; true_colors_info; Memory: Object.
    - BunchOfColors (3/5/7): T=120; Oracle Info: true_color_indices; Memory: Capacity.
    - SeqOfColors (3/5/7): T=120; Oracle Info: true_color_indices; Memory: Capacity.
    - ChainOfColors (3/5/7): T=120; Oracle Info: true_color_indices; Memory: Sequential.
- Contributions overview:
  1) Memory task classification (Section 4).
  2) MIKASA-Base Gymnasium-based framework (Section 5).
  3) MIKASA-Robo (Section 6) evaluated with Online RL (6.2) and VLA (6.4).
  4) Offline datasets for all 32 tasks (Appendix B) evaluated with Offline RL baselines (6.3).
- Installation note: pip install mikasa-robo-suite.

# Related Work
- Survey of memory RL benchmarks/environments: DMLab-30, PsychLab, MiniGrid/MiniWorld, MiniHack (NetHack), BabyAI, POPGym, BSuite, Memory Gym, Memory Maze.
- Table 2 compares which methods (e.g., DRQN, DTQN, HCAM, GTrXL, AMAGO, RATE, R2I, Neural Map, S5, etc.) were evaluated on which environments (including Atari with/without FrameStack).
- Purpose: Show fragmentation and limited overlap in memory evaluation across studies.

# Preliminaries
- POMDP definition: tuple (S, A, T, R, Ω, O, γ); policy depends on observation history a_t ~ π(·|o_{1:t}); objective maximizes expected discounted return.
- Memory-intensive environment: POMDP with correlation horizon ξ > 1 (minimum timesteps between critical event and recall time) per [9].
- Robotic tabletop manipulation: Real scenarios involve spatio-temporal dependencies; many simulators assume MDPs, limiting memory research relevance.

# Method
- 4.1 From cognitive science to RL: Concepts include object permanence, categorical perception, working memory/memory span, causal reasoning, transitive inference; adapted to RL task design.
- 4.2 Taxonomy:
  - Object Memory: tracking occluded objects and properties.
  - Spatial Memory: locations, layouts, navigation.
  - Sequential Memory: serial recall, ordered actions.
  - Memory Capacity: retaining multiple items simultaneously.
- 5 MIKASA-Base:
  - Motivation: Standardize access to diverse memory environments via a unified Gym-like API, reducing fragmentation.
  - Design: Two tiers—diagnostic vector-based tasks and complex image-based tasks—support incremental validation of memory mechanisms.
  - Task selection follows the four memory categories; tasks unified under a single API (overview in Appendix I; classification in Table 7).
  - Robotics framework comparison (Table 3): MIKASA-Robo supports memory tasks spanning Manipulation/Atomic/Low-level actions; many robotics frameworks do not support memory-intensive manipulation tasks (e.g., ManiSkill3, RoboCasa, Gymnasium-Robotics, etc.).
- 6 MIKASA-Robo:
  - 32 tasks across 12 categories designed from the taxonomy; tasks isolate memory skills (object permanence, sequential recall, capacity).
  - Modes: state, RGB, joints, oracle, prompt; standard memory testing: RGB+joints; state reserved for MDP-based tasks.
  - Rewards: dense (progress-based) and sparse (success-only).

# Experiments
- Setup and platform: Built on ManiSkill3 for efficient parallel GPU-based training.
- Online RL baselines (6.2):
  - Algorithms: PPO (MLP and LSTM), SAC, TD-MPC2.
  - Validation in state mode (oracle-level information): PPO-MLP achieves 100% SR across tasks (Figures 4, 7, 8), with success curves reaching 1.0 across ShellGameTouch, RotateLenientPos, RememberColor9, TakeItBack, Intercept variants, etc. Training steps shown up to 1e7 in figures.
  - RGB+joints with dense rewards (Figure 5, Figure 9, Figure 75):
    - RememberColor: PPO-LSTM > PPO-MLP on 3-color but both degrade to near-zero on 5/9 colors.
    - Memory demands increase cause sharp performance drops; SAC and TD-MPC2 show higher sample efficiency in simpler cases but lack explicit memory and perform poorly on complex tasks.
  - Sparse rewards (Appendix F; Figure 10): PPO-MLP and PPO-LSTM fail across most environments; even three-color variants remain unsolved.
- Offline RL baselines (6.3):
  - Datasets: 1000 successful trajectories per task (Appendix B), collected by a PPO-MLP agent trained to SR=100% in state mode; RGB observations; sparse rewards (success flag).
  - Algorithms: RATE, Decision Transformer (DT), Behavior Cloning (BC-MLP), Conservative Q-Learning (CQL-MLP), Diffusion Policy (DP).
  - Results: Most tasks not solved; capacity/sequential tasks universally at “0.00 ± 0.00” (BunchOfColors3/5/7, SeqOfColors3/5/7, ChainOfColors3/5/7).
    - Selected numbers (Table 6, mean ± SEM across three runs, 100 episodes/run):
      - ShellGameTouch-v0: RATE 0.92 ± 0.01; DT 0.53 ± 0.07; BC 0.28 ± 0.01; CQL 0.16 ± 0.04; DP 0.18 ± 0.02.
      - ShellGamePush-v0: RATE 0.78 ± 0.06; DT 0.62 ± 0.14.
      - InterceptMedium-v0: DT 0.56 ± 0.01; DP 0.68 ± 0.02; RATE 0.32 ± 0.02.
      - TakeItBack-v0: RATE 0.42 ± 0.24; BC 0.33 ± 0.10.
      - RotateLenientPosNeg-v0: RATE 0.29 ± 0.03.
      - Many other tasks show low success (≤ ~0.3) across methods.
  - Visualization (Figure 6/27): Radar plot shows low performance especially in Sequential Memory and Memory Capacity categories.
- VLA baselines (6.4):
  - Models: Octo (transformer with diffusion heads; pretrained context length 10; action chunk size K=4; heads fine-tuned) and OpenVLA (Prismatic-7B with LoRA; action chunking; L1 loss; K=4 and K=8).
  - Training data: 250 expert trajectories per task (two RGB camera views at 128 × 128; end-effector control), evaluated over 100 episodes; training for 50,000 iterations (Appendix D).
  - Tasks: ShellGameTouch (T=90), InterceptMedium (T=90), RememberColor3/5/9 (T=60) with language instruction prompts.
  - Results (Table 4, mean ± SEM):
    - Octo-small: ShellGameTouch 0.46 ± 0.05; InterceptMedium 0.39 ± 0.04; RememberColor3 0.45 ± 0.06; RememberColor5 0.17 ± 0.03; RememberColor9 0.11 ± 0.03.
    - OpenVLA (K=4): ShellGameTouch 0.12 ± 0.05; InterceptMedium 0.06 ± 0.02; RememberColor3 0.21 ± 0.00; RememberColor5 0.09 ± 0.02; RememberColor9 0.08 ± 0.02.
    - OpenVLA (K=8): ShellGameTouch 0.47 ± 0.05; InterceptMedium 0.14 ± 0.03; RememberColor3 0.59 ± 0.04; RememberColor5 0.16 ± 0.03; RememberColor9 0.06 ± 0.02.
  - Observations: Larger chunk size can improve performance on simpler tasks, but models still drop sharply on harder tasks; absence of explicit memory leads to brittle performance.
- Reproducibility and compute (Appendix G): All baselines trained on a single NVIDIA A100 GPU; evaluation per task over 100 independent episodes; metrics reported as mean SR ± SEM; training runs noted (three runs for offline results).
- Additional validation:
  - State-mode success curves (Figures 49–71): Across diverse tasks (Intercept variants; ShellGame modes; Rotate variants; RememberShape/Color combinations; TakeItBack), PPO-MLP reaches SR near 1.0 with training steps up to ~1e7–1e8, reinforcing inherent solvability without memory constraints.
  - Capacity/sequential tasks in state mode (Figure 74): SeqOfColors3/5/7, BunchOfColors3/5/7, ChainOfColors3/5/7 approach SR close to 1.0 over long training horizons.

# Conclusion
- Limitations (Section 7): Octo and OpenVLA results may be under-optimized due to limited fine-tuning; future work could explore longer temporal dependencies and meta-RL.
- Summary (Section 8): MIKASA offers a taxonomy, MIKASA-Base, MIKASA-Robo (32 tasks), and offline datasets. Empirical studies show current online, offline, and VLA methods struggle on many memory-intensive tasks, motivating better memory architectures. Public release and installation: pip install mikasa-robo-suite.

# References
- Literature spans memory RL methods (DRQN, DTQN, HCAM, GTrXL, RATE, etc.), benchmarks (POPGym, DMLab-30, Memory Maze, BSuite, MiniGrid, PsychLab), RL/control (PPO, SAC, TD-MPC2), robotic benchmarks (ManiSkill3, RLBench, RoboCasa, BEHAVIOR-1K, iGibson 2.0), VLA models (Octo, OpenVLA), and cognitive memory foundations (Piaget, Baddeley).

# Appendix
- Implementation (Appendix A, C): MIKASA-Robo and MIKASA-Base follow Gymnasium API; wrappers for debugging (e.g., RenderStepInfoWrapper, DebugRewardWrapper; task-specific wrappers like RememberColorInfoWrapper).
- Datasets for Offline RL (Appendix B):
  - Collected via PPO-MLP trained to SR=100% in state mode with sparse rewards.
  - Per-task dataset: 1000 successful trajectories; stored modalities/shapes per timestep:
    - “rgb”: (T, 128, 128, 6) two RGB images (top and wrist).
    - “joints”: (T, 25) TCP position/rotation; joints positions/velocities.
    - “action”: (T, 8).
    - “reward”: (T,).
    - “success”: (T,).
    - “done”: (T,).
- VLA setup (Appendix D):
  - Tasks and instructions table (T: RememberColor3/5/9 T=60; ShellGameTouch T=90; InterceptMedium T=90).
  - Training for 50,000 iterations; evaluation per task; 128×128 RGB pairs plus end-effector control; language instruction templates provided.
- Memory mechanisms overview (Appendix E): RNNs, SSMs, Transformers, GNNs; examples of memory-based agents summarized earlier.
- Classic baselines performance (Appendix F):
  - Dense rewards in RGB+joints: PPO-LSTM reaches 100% on simpler tasks (e.g., RememberColor3) but fails as complexity rises; PPO-MLP ~25% on RememberColor3.
  - Sparse rewards: PPO-MLP and PPO-LSTM fail across environments (Figure 79/10).
  - Reporting: mean ± SEM across three runs; each agent evaluated on 16 seeds for robustness.
- Reproducibility and compute (Appendix G): Single NVIDIA A100; 100 evaluation episodes per task; seeds 1–100; mean ± SEM reported.
- Detailed task descriptions (Appendix H):
  - ShellGame-v0: three modes (Touch/Push/Pick), occlusion at step 5; success criteria per mode; sparse/dense rewards.
  - RememberColor/Shape/ShapeAndColor: phases (observe 0–4, delay 5–9, selection ≥10); complexity levels 3/5/9 or combination grids (3×2/3×3/5×3); success requires correct touch ≥0.1 seconds; sparse/dense rewards.
  - Intercept/InterceptGrab: velocity ranges per mode (Slow 0.25–0.5 m/s; Medium 0.5–0.75 m/s; Fast 0.75–1.0 m/s); success criteria (reach target or grasp and stabilize); sparse/dense rewards.
  - RotateLenient/RotateStrict: Pos and PosNeg angle ranges; success thresholds (±0.1 rad); in Strict, position deviation ≤5 cm.
  - TakeItBack-v0: move cube to goal (red), then return to initial (magenta signal); two-phase success; sparse/dense rewards.
  - SeqOfColors/BunchOfColors/ChainOfColors: N=3/5/7; observation timing (each shown for 5 steps; delay spans 5 steps); success requires touching all demonstrated cubes (unordered in Seq/Bunch; strict order in Chain); ≥0.1 s contact; sparse/dense reward composition.
- MIKASA-Base tasks classification (Appendix I; Table 7): Lists environments mapped to memory types and observation/action spaces (e.g., Memory Cards—Capacity; MiniGrid-Memory—Object; Memory Maze—Spatial; POPGym tasks across Object/Sequential/Capacity/Spatial).