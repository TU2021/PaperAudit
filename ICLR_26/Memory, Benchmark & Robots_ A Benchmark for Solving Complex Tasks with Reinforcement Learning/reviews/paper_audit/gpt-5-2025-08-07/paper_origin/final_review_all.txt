Summary
- The paper introduces MIKASA, a framework for benchmarking memory-intensive RL with two suites: MIKASA-Base (a Gymnasium-style unification of existing memory environments) and MIKASA-Robo (32 tabletop manipulation tasks requiring memory). It formalizes a four-class taxonomy (object, spatial, sequential, capacity) (Section 4.2), provides multiple observation modes and dense/sparse rewards (Section 6.1; Table 1), and releases offline datasets (Appendix B). Experiments show that tasks are solvable in fully observed “state” mode (Figures 4, 7, 8), while online RL, offline RL, and VLA baselines struggle under partial observability and sparse rewards (Figures 5, 9, 10; Table 6; Table 4). The suite includes tooling and reproducibility support (Appendix A, C, G, H).Strengths
- Bold and useful taxonomy linking cognitive memory constructs to RL
  • The four-type taxonomy (Object, Spatial, Sequential, Capacity) is clearly articulated (Section 4.2) and motivated by cognitive science concepts (Section 4.1; Figure 3), improving conceptual clarity for designing and evaluating memory tasks (clarity, impact).
  • The taxonomy is used to categorize both new robotic tasks (Table 1; Section 6.1) and consolidated base environments (Table 7; Section 5), enabling systematic coverage across diverse memory skills (novelty in unification, clarity).
  • Figure 1 highlights the motivation—fragmentation and incomparable evaluations—addressed by taxonomy-based separation (impact).- Comprehensive, well-scoped robotic benchmark with 32 tasks and multi-difficulty variants
  • MIKASA-Robo covers 12 categories with difficulty modes that target distinct memory skills (Table 1; Appendices H.1–H.12), supporting granular assessment (experimental rigor, impact).
  • Multiple observation modes (state, RGB, joints, oracle, prompt) and reward regimes (dense, sparse) are implemented (Section 6.1), enabling controlled manipulation of partial observability and feedback (technical soundness).
  • Detailed per-task descriptions, success criteria, and reward designs (Appendix H with Figures 11–22) improve reproducibility and facilitate independent validation (clarity, practical utility).- Unified access to diverse memory environments via MIKASA-Base
  • The consolidation of open-source memory tasks under a single Gymnasium-like API (Section 5; Appendix C, Code 3) directly addresses evaluation fragmentation (impact, clarity).
  • The curated classification of MIKASA-Base environments into the four memory types (Table 7) offers a clear bridge between conceptual taxonomy and practical evaluation (novelty in standardization).
  • Tiered design principles (diagnostic vector tasks vs complex image tasks) (Section 5) allow isolating memory from perception challenges incrementally (technical soundness).- Strong empirical validation of benchmark difficulty and memory dependence
  • Tasks are solvable with PPO-MLP in fully observed “state” mode (Figures 4, 7, 8), indicating inherent task solvability; success rates often approach 1.0, though some plots end below 1.0 (e.g., InterceptSlow, ShellGamePush, InterceptFast in Appendix F; Figures 49, 54, 69), which still supports the solvability claim (experimental rigor).
  • Under RGB+joints and dense rewards, PPO-LSTM outperforms PPO-MLP on simpler settings (e.g., RememberColor3-v0) but both degrade with increasing complexity (Figures 5, 9), evidencing growing memory demands (technical soundness).
  • Sparse reward settings are demonstrably hard across many tasks (Figure 10; Appendix F), aligning with real-world low-feedback conditions (impact).- Breadth of baselines: online RL, offline RL, and VLA models
  • Online RL: PPO-MLP vs PPO-LSTM, SAC, TD-MPC2 (Section 6.2; Figures 5, 9), showing that common robotics algorithms without explicit memory struggle under partial observability (experimental rigor).
  • Offline RL: RATE, DT, BC, CQL, DP evaluated across all 32 tasks with sparse rewards (Section 6.3; Table 6; Figure 6/27), revealing strong challenges especially for capacity/sequential tasks (impact).
  • VLA: Octo and OpenVLA on selected tasks (Section 6.4; Table 4), demonstrating degradation with complexity and limitations of chunking vs true memory (novelty in cross-paradigm evaluation).- Clear documentation, tooling, and reproducibility support
  • Wrappers and debugging utilities (Appendix A; Code 2), explicit environment info overlays (Appendix A), and pip install note for MIKASA-Robo (^1; Introduction) enhance usability (clarity, impact).
  • Offline dataset specification (Appendix B) with trajectory structure and availability encourages standardized evaluation (reproducibility).
  • Compute setup, seeds, and evaluation protocols (Appendix G) further support reproducible experimentation (experimental rigor).- Articulated gap in robotics memory evaluation and principled benchmark design
  • Comparative analysis with existing robotics frameworks (Table 3; Section 6) clarifies that MIKASA-Robo addresses atomic, low-level, memory-centric manipulation, unlike many existing suites (novelty, impact).
  • The design rationale for isolating memory mechanisms rather than conflating with high-level abstractions (Section 6; Table 3) is sound and useful to the community (technical soundness).Weaknesses
- Limited empirical validation of the taxonomy beyond use as organizing principle
  • The taxonomy (Section 4.2) is described but not quantitatively validated for separability or coverage; the paper does not provide measures indicating that tasks exclusively stress one type of memory (No direct evidence found in the manuscript) (novelty/technical soundness).
  • Potential overlap across categories (e.g., ShellGame—object permanence with spatial localization) is not analyzed for cross-category leakage (Table 1; Appendices H.1–H.12), risking ambiguity in conclusions (clarity).
  • While cognitive science motivates the taxonomy (Section 4.1; Figure 3), there is no alignment with cognitive diagnostics or metrics that could strengthen validation (No direct evidence found in the manuscript) (technical soundness).- Baseline coverage for online RL is narrow relative to the stated landscape
  • Online baselines use LSTM as the sole explicit memory module (Section 6.2), omitting widely cited transformer/SSM-based agents (e.g., GTrXL, HCAM, S5) listed in Table 2, limiting generality of conclusions (experimental rigor/novelty).
  • Off-policy baselines (SAC, TD-MPC2) are included without recurrent or memory-augmented variants (Section 6.2), weakening the claim about “inappropriateness of algorithms common in robotics” under memory demands (technical soundness).
  • Hyperparameter sensitivity or capacity sweeps for LSTM/MLP are not reported (Figures 5, 9), making it hard to rule out configuration-induced failures (experimental rigor).- Memory vs perception confounds not fully disentangled or quantified
  • The correlation horizon ξ is defined (Section 3.2) but not reported per task (Table 1; Appendices H.1–H.12), limiting quantification of memory demands (technical soundness).
  • Performance drops with more colors/shapes (Figures 5, 9) could reflect increasing visual discrimination difficulty rather than memory per se; no ablation with identical perception complexity but reduced memory requirements (No direct evidence found in the manuscript) (novelty/clarity).
  • Although tiering (Section 5) is proposed, there are no cross-tier ablations connecting vector diagnostics to image tasks for the same underlying memory construct (No direct evidence found in the manuscript) (experimental rigor).- Offline dataset design choices may bias or limit conclusions
  • Datasets contain only successful trajectories (Appendix B), which may reduce diversity and limit the utility of offline RL algorithms that benefit from varied returns (technical quality).
  • Trajectories are collected in fully observed “state” mode but used to train on RGB observations (Appendix B; Section 6.3), introducing a modality mismatch that may confound learning (experimental rigor).
  • Dataset size (1000 per task) and train/validation/test splits or cross-task generalization protocols are not detailed (Appendix B), making it difficult to interpret negative results (clarity).
  • The claim that datasets were collected with “SR = 100% … with sparse rewards” (Appendix B) is not supported by direct evidence of 100% SR under sparse rewards in figures/tables; Appendix F discusses 100% under dense rewards in state mode (Appendix B; Appendix F; No direct evidence found in the manuscript) (reproducibility).- Metrics and reporting inconsistencies hinder comparability
  • Mixture of Return and Success Rate across figures (Figures 5 vs. 9/10) can complicate cross-task and cross-method comparisons (clarity).
  • No explicit random or heuristic baselines shown in figures to contextualize performance statements (Section 6.4 mentions “exceeds random” but Table 4 does not present random baselines) (clarity/experimental rigor).
  • Training budgets and evaluation horizons differ across tasks (Figures 7–9, 10), and normalization across tasks/methods is not described (Appendix G covers evaluation seeds but not budget normalization), affecting fairness (experimental rigor).- No physical-robot validation or sim-to-real analysis despite real-world motivation
  • All experiments are in simulation (ManiSkill3) (Section 6; [87]) with no hardware results (No direct evidence found in the manuscript), limiting claims of real-world readiness (impact/technical soundness).
  • Reward shaping and success definitions may not directly translate to physical constraints (Appendix H), and there are no domain randomization or robustness tests to support sim-to-real transfer (No direct evidence found in the manuscript) (experimental rigor).
  • Although the benchmark targets practical memory needs (Introduction; Section 6), there is no demonstration of robustness to sensory noise, latency, or calibration drift typical in hardware (No direct evidence found in the manuscript) (impact).- Cross-referencing and internal consistency issues
  • Figure numbering and cross-references appear inconsistent: “Figure 2” is used to illustrate task executions from MIKASA-Robo (Preliminaries; Figure 2 caption) while the matrix of prior evaluations (Preliminaries; image) is not clearly numbered, hindering traceability (clarity).
  • Table 2 contains multiple citation-number mismatches relative to the References list (e.g., GTrXL labeled “[26]” while [72] corresponds to Stabilizing Transformers for RL; AMAGO labeled “[72]” while AMAGO is [26]; RATE labeled “[77]” though RATE is [10]; Neural Map labeled “[62]” though Neural Map is [71]; R2D2 labeled “[71]” though R2D2 is [45]) (Table 2; References), affecting scholarly integrity and verifiability (impact/clarity).
  • Episode timeouts for RememberColor differ between definitions and code (Table 1 lists T=60; Appendix D Table 5 lists T=60; Appendix A Code 1 uses episode_timeout=90 for RememberColor9-v0), without reconciliation (Table 1; Appendix D; Appendix A), which can impede reproducibility (experimental rigor).Suggestions for Improvement
- Strengthen empirical validation of the taxonomy
  • Provide quantitative metrics of task-category separability (e.g., controlled task variants that isolate a single type) and report per-task memory attribution scores; add an ablation showing cross-category leakage (Section 4.2; Table 1; Appendices H.1–H.12).
  • Analyze ambiguous tasks (e.g., ShellGame) for dual-memory demands via controlled variants (object vs spatial emphasis) and report results (Table 1; H.1).
  • Include alignment with cognitive diagnostics (e.g., memory span or object permanence analogs), specifying measurement protocols and reporting outcomes (Section 4.1; Figure 3).- Expand baseline coverage and configuration analysis for online RL
  • Add transformer/SSM-based memory baselines representative of Table 2 (e.g., GTrXL, HCAM, S5) on a subset spanning all four categories to test generality (Section 6.2; Table 2).
  • Evaluate recurrent/off-policy baselines (e.g., SAC+RNN, TD-MPC2 with memory) to fairly test claims about robotics algorithms under memory demands (Section 6.2).
  • Report sensitivity analyses (hidden sizes, sequence lengths, learning rates) and training-time budgets to ensure failures are not configuration-driven (Figures 5, 9; Appendix G).- Disentangle perception from memory requirements with targeted ablations and quantification
  • Annotate each task with estimated correlation horizon ξ (Section 3.2) and provide controlled experiments showing performance variation with ξ while keeping perception constant (Table 1; Appendices H.1–H.12).
  • Create perception-controlled variants (e.g., identical colors/shapes but varying occlusion/delay) and compare to memory-controlled variants (varying sequence length/capacity) to isolate which dimension drives failures (Figures 5, 9).
  • Link vector diagnostics from MIKASA-Base (Table 7) to matched image tasks (MIKASA-Robo) via shared structure, and report cross-tier correlations (Section 5).- Improve offline dataset design and reporting to support stronger conclusions
  • Include mixed-return datasets (successful and unsuccessful trajectories) and report how success rate composition affects offline RL performance (Appendix B; Section 6.3).
  • Reduce modality mismatch by also collecting RGB trajectories under RGB policy execution; optionally provide paired state/RGB datasets for controlled comparisons (Appendix B).
  • Specify train/validation/test splits, random seeds, and cross-task generalization protocols; consider scaling dataset sizes and reporting sample-efficiency curves (Appendix B; Table 6).
  • Substantiate the “SR = 100% … with sparse rewards” dataset-collection claim by adding plots/tables demonstrating 100% SR under sparse rewards in state mode across tasks, or revise the claim to match available evidence (Appendix B; Appendix F).- Standardize metrics and baselines for clearer comparisons
  • Use a consistent primary metric (e.g., Success Rate) across all figures and include secondary metrics in appendices; add random and simple heuristic baselines for context (Figures 5, 9, 10; Table 4).
  • Normalize training budgets (steps, wall-clock, environment interactions) across methods and tasks; report a standardized evaluation horizon and confidence intervals throughout (Appendix G; Figures 7–10).
  • Provide per-task difficulty indices (e.g., by ξ, number of distractors, occlusion duration) to allow nuanced comparisons across methods and tasks (Table 1; Appendices H).- Add at least preliminary real-world validation or sim-to-real robustness experiments
  • Demonstrate a subset of tasks on a physical tabletop setup (e.g., RememberColor3, ShellGameTouch) and report transfer gaps and failure modes (Section 6; Appendices H).
  • Incorporate domain randomization and noise (camera, actuation, latency) and show robustness analyses to support real-world claims (Appendix H; Section 6.1).
  • Compare reward/shaping alternatives that better reflect hardware constraints (e.g., contact uncertainty) and report impacts on learning (Appendix H; Section 6.1).- Ensure internal consistency and correct citations
  • Fix figure numbering/cross-references so each figure is uniquely identified and referenced consistently (Preliminaries; Figure 2 caption; Preliminaries image).
  • Correct method/environment citation numbers in Table 2 to match the References list (e.g., GTrXL [72], AMAGO [26], RATE [10], Neural Map [71], R2D2 [45]) and add any missing entries (Table 2; References).
  • Reconcile episode timeouts (e.g., RememberColor T=60 in Table 1 and Appendix D vs episode_timeout=90 in Appendix A Code 1); clarify default vs evaluation settings and update code snippets/documentation accordingly (Table 1; Appendix D; Appendix A).Score
- Overall (10): 7 — Strong benchmark and taxonomy with broad task coverage (Section 4.2; Table 1; Table 7) and multi-paradigm baselines showing difficulty (Figures 5, 9, 10; Table 6), tempered by narrow online baselines and internal consistency/citation issues (Table 2; Appendix A/B).
- Novelty (10): 7 — Novel unification across memory tasks and robotics with a four-type taxonomy (Section 5; Section 6; Table 3; Section 4.2), though the taxonomy lacks quantitative validation (Section 4.2; Appendices H.1–H.12).
- Technical Quality (10): 5 — Solid engineering and reproducibility (Appendix A–C, G) and state-mode solvability evidence (Figures 4, 7, 8) but memory–perception confounds (Section 3.2; Figures 5, 9), narrow baselines (Section 6.2), and citation/consistency issues (Table 2; Appendix A/B) lower confidence.
- Clarity (10): 7 — Clear task/API documentation and detailed appendices (Appendix H; Appendix C; Code snippets; Table 1; Table 7; Figures 11–22), with comparability and cross-referencing inconsistencies (Figures 5 vs 9/10; Preliminaries Figure 2; Table 2 citation numbers).
- Confidence (5): 4 — High confidence from extensive experiments and detailed appendices (Sections 6.2–6.4; Appendix F/G), with uncertainty stemming from missing taxonomy validation and internal citation/consistency issues (Section 4.2; Table 2; Appendix A/B).