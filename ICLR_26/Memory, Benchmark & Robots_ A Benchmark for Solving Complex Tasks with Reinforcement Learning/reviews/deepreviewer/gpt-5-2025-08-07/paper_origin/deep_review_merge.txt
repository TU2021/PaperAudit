Summary
The paper presents MIKASA, a unified benchmark suite for evaluating memory in reinforcement learning (RL) and robotic manipulation. It introduces a four-category taxonomy of memory tasks (Object, Spatial, Sequential, Capacity), consolidates diverse existing memory benchmarks under a common Gym-like API in MIKASA-Base, and creates MIKASA-Robo, a set of 32 tabletop manipulation tasks designed to explicitly require memory through mechanisms such as occlusion, delayed selection, ordered recall, and multi-item retention. The suite provides multiple observation modes (e.g., full state, RGB, joints, prompts) and reward settings (dense and sparse), along with curated image-based offline datasets for all 32 robotic tasks. The authors validate the benchmark with a broad set of baselines across online RL (PPO-MLP/LSTM, SAC, TD-MPC2), offline RL (DT, RATE, BC, CQL, Diffusion Policy), and vision-language-action (VLA) models (Octo, OpenVLA). They establish that tasks are solvable under full observability (state mode) and demonstrate that performance degrades markedly under partial observability (RGB+joints), especially with sparse rewards, underscoring the benchmark’s challenge and the need for stronger memory mechanisms. The paper emphasizes reproducibility through detailed reporting, multiple seeds, confidence intervals, and publicly released code and datasets.

Strengths
- Clear motivation and framing of the gap in standardized evaluation of memory for RL and robotics, with a coherent taxonomy that spans object, spatial, sequential, and capacity dimensions.
- Thoughtful benchmark design that separates diagnostic vector-based tasks (MIKASA-Base) from higher-fidelity, vision-based manipulation tasks (MIKASA-Robo), enabling attribution of failures to memory vs perception when possible.
- Strong calibration: tasks are shown to be solvable under full observability (state mode), providing a sanity check and baseline for memory demand under partial observability.
- Comprehensive and practically relevant robotic task suite with low-level actions, multiple difficulty tiers, and both dense and sparse reward variants, targeting memory requirements through occlusion and temporal recall.
- Broad experimental coverage across online RL, offline RL, and VLA models, with transparent reporting (success rates, returns, mean ± SEM, multiple seeds) and consistent configurations.
- Public release of RGB, sparse-reward datasets for all robotic tasks, unified API, wrappers, and reproducible scripts that lower adoption barriers and enable standardized evaluation.
- Clear documentation and thorough presentation of tasks, baselines, and implementation details, enhancing usability and reproducibility.
- Results convincingly highlight current method limitations under memory-demanding conditions, motivating future research on memory architectures for manipulation and VLA agents.

Weaknesses
- Limited diversity of evaluated memory mechanisms: online RL primarily uses LSTM, and advanced approaches (e.g., transformer variants like GTrXL, hierarchical memory, state-space models such as S4/Mamba, retrieval-augmented methods) are cited but not included experimentally, weakening the generality of conclusions about “current methods.”
- Memory versus perception entanglement remains in RGB-based settings; the paper does not provide controlled ablations (e.g., oracle perception with partial observability) to quantify the contribution of visual recognition and control errors versus temporal memory deficits, making it harder to attribute failures solely to memory.
- Absence of formal, per-task memory metrics (e.g., correlation horizon, occlusion durations, required recall lengths, distractor counts) reduces interpretability and hinders principled task selection and comparison; while correlation horizon is discussed conceptually, it is not reported for individual tasks.
- Offline datasets are generated using oracle policies in full-state mode, which may introduce distribution shift and biases for vision-only learners trained on these trajectories; the nature and impact of this shift are not characterized quantitatively.
- No evaluation of robustness or generalization (e.g., domain randomization, camera changes, sensor noise) and no real-robot validation, limiting claims about applicability to physical settings and sim-to-real transfer.
- The taxonomy’s theoretical grounding and category boundaries (e.g., overlap between capacity and sequential tasks) could be tightened and linked more explicitly to measurable task properties and experimental outcomes.
- Minor presentation issues: some multi-panel figures are cramped and difficult to read, and the connection between taxonomy categories and per-category performance summaries could be emphasized more consistently.
