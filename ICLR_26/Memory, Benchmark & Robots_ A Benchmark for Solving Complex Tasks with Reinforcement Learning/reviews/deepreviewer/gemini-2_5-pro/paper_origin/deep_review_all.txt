Here are four distinct reviews of the paper.

***

### **Review 1**

#### Summary
This paper introduces MIKASA, a comprehensive benchmark suite designed to evaluate memory capabilities in reinforcement learning agents. The authors make four main contributions: (1) a novel classification framework for memory-intensive tasks categorized into Object, Spatial, Sequential, and Memory Capacity; (2) MIKASA-Base, a unified collection of existing memory-RL environments under a common API; (3) MIKASA-Robo, a new suite of 32 memory-intensive tabletop robotic manipulation tasks; and (4) datasets for all MIKASA-Robo tasks to facilitate offline RL research. The authors conduct extensive experiments with online, offline, and Visual-Language-Action (VLA) models, demonstrating that current state-of-the-art agents struggle with the proposed challenges, thereby highlighting the need for improved memory mechanisms.

#### Soundness
The methodology presented in this paper is sound and well-executed. The proposed taxonomy of memory tasks is logically derived from concepts in cognitive science and provides a structured foundation for the benchmark (Section 4.2). The design of MIKASA-Robo is particularly strong; the authors validate that all tasks are solvable given full state information (Figure 4, Appendix F), confirming that failures in partially observable settings are attributable to memory and perception challenges, not inherent task unsolvability. The selection of baselines, including memory-less (MLP) and memory-based (LSTM) online agents, as well as modern offline RL and VLA models, provides a comprehensive first-pass evaluation of the benchmark's difficulty. The experimental results convincingly support the central claim that existing methods are brittle when faced with increasing memory demands (Figure 5, Table 4).

#### Presentation
The paper is exceptionally well-written and organized. The motivation is clearly articulated in the introduction and visually summarized in Figure 1, which effectively contrasts the fragmented state of memory evaluation with the authors' unified approach. The structure flows logically from the high-level taxonomy to the concrete benchmark implementations (MIKASA-Base and MIKASA-Robo). Tables are informative and well-designed, particularly Table 1, which provides a concise overview of the 32 new robotic tasks, and Table 2, which effectively illustrates the fragmentation problem in prior work. The detailed task descriptions and implementation examples in the appendix (Appendix H, Code 1-3) are a valuable resource for future users of the benchmark.

#### Contribution
The contribution of this work is highly significant and timely. The field of memory-RL has long suffered from a lack of standardized evaluation, forcing researchers to use disparate and often incomparable sets of custom tasks. MIKASA addresses this critical gap by providing a multi-faceted solution. The taxonomy offers a valuable conceptual framework, MIKASA-Base provides a practical tool for standardized comparison on existing tasks, and MIKASA-Robo pushes the research frontier into the challenging and relevant domain of robotic manipulation. By open-sourcing the entire suite and accompanying datasets, the authors have provided an invaluable resource that is likely to become a standard for evaluating memory in RL agents.

#### Strengths
1.  **Comprehensive Scope:** The work provides a complete ecosystem for memory evaluation, including a conceptual taxonomy, a unified library of existing tasks, a large suite of new robotic tasks, and offline datasets.
2.  **Principled Task Design:** The MIKASA-Robo tasks are carefully designed to isolate and test specific memory skills, with tunable difficulty and clear success criteria. The validation of task solvability in the MDP setting (Figure 4) is a key methodological strength.
3.  **Extensive and Insightful Baselines:** The evaluation spans online, offline, and VLA paradigms, providing a broad and compelling snapshot of the limitations of current methods and clearly motivating the need for future research.
4.  **High Practical Value:** The benchmark is well-documented, built on an efficient modern simulator (ManiSkill3), and is easy to install and use (`pip install mikasa-robo-suite`), which will significantly lower the barrier to entry for researchers and encourage widespread adoption.

#### Weaknesses
1.  **Limited VLA Fine-tuning:** The authors acknowledge that the VLA models (Octo, OpenVLA) underwent limited fine-tuning (Section 7). While the results are still insightful, a more thorough fine-tuning process might reveal stronger latent memory capabilities in these large models.
2.  **Predictable Online RL Results:** The finding that PPO-LSTM outperforms PPO-MLP on memory tasks (Figure 5, Figure 9) is expected. While this serves as a good sanity check for the benchmark, including more advanced memory architectures (e.g., Transformer-based or SSM-based agents) in the initial baseline suite could have provided even deeper insights.

#### Questions
1.  The complete failure of online agents on sparse-reward tasks (Figure 10) is striking. Do you believe this is primarily an exploration problem, a credit assignment problem, or a fundamental limitation of the memory architectures themselves? Have you considered any curriculum learning or reward shaping strategies to make these tasks more tractable?
2.  Given the benchmark's breadth, do you have plans to host a public leaderboard or organize a competition around MIKASA to further galvanize research and track community progress?

#### Rating
- Overall (10): 9 — The paper provides a comprehensive, well-designed, and much-needed benchmark that is poised to have a significant impact on memory-RL research (Section 1, Section 5, Section 6).
- Novelty (10): 9 — The combination of a novel taxonomy, a large-scale robotics benchmark for memory, and the unification of existing environments is highly novel (Section 4, Section 6).
- Technical Quality (10): 9 — The experimental methodology is rigorous, including sanity checks (Figure 4) and extensive evaluations across multiple learning paradigms (Section 6.2, 6.3, 6.4).
- Clarity (10): 10 — The paper is exceptionally clear, well-organized, and supported by informative figures and tables (Figure 1, Table 1).
- Confidence (5): 5 — I am highly confident in my assessment, as I am an expert in reinforcement learning and robotics benchmarks.

***

### **Review 2**

#### Summary
This paper proposes MIKASA, a benchmark suite for evaluating memory in reinforcement learning. It consists of three parts: a four-category classification of memory tasks (Object, Spatial, Sequential, Capacity), a unified library of existing memory environments called MIKASA-Base, and a new benchmark of 32 robotic manipulation tasks called MIKASA-Robo. The authors evaluate a range of baseline algorithms from online RL, offline RL, and vision-language-action (VLA) domains, concluding that current methods struggle significantly on the more complex tasks within the benchmark.

#### Soundness
The overall methodological approach is reasonable, but there are several points that weaken the soundness of the conclusions.

First, the choice of baselines for online RL is somewhat dated. PPO-LSTM (Section 6.2) is a classic baseline but does not represent the state of the art in memory-based RL. Many recent works have shown the effectiveness of Transformer or State-Space Model (SSM) architectures for POMDPs. Concluding that "current methods struggle" based primarily on an LSTM's failure seems like an overstatement.

Second, the experiments on VLA models are presented as evidence of their memory limitations, but the authors themselves admit to "limited fine-tuning due to computational constraints" (Section 7). This is a significant caveat. The poor performance (Table 4) could be a result of insufficient adaptation to the new domain rather than a fundamental lack of memory capacity. The claims about VLA models should be toned down accordingly.

Third, the analysis of the offline RL results (Figure 6, Table 6) is shallow. Transformer-based models like RATE and DT failing completely on all `SeqOfColors`, `BunchOfColors`, and `ChainOfColors` tasks is a very strong result that warrants deeper investigation. Is this due to the quality of the provided offline dataset (e.g., lack of diversity), or a fundamental architectural flaw? The paper does not explore these potential causes.

On the positive side, the verification that all tasks are solvable in an MDP setting (Figure 4) is a good piece of experimental design, providing a solid upper bound on performance.

#### Presentation
The paper is generally well-written, but some areas could be improved. The radar plot in Figure 6 is visually cluttered and difficult to interpret, especially with overlapping lines and a logarithmic scale that obscures small but potentially meaningful performance differences. A set of bar charts might have been clearer. The distinction between the "Memory Capacity" and "Sequential Memory" categories in the taxonomy (Section 4.2) could be defined more sharply; for instance, `ChainOfColors` seems to test both capacity (remembering N colors) and sequentiality (in a specific order). The relationship between these categories could be clarified.

#### Contribution
The paper makes a clear contribution by releasing a large new set of robotics tasks and datasets. This is valuable. However, the novelty should be considered in the context of concurrent work. The paper mentions MemoryBench [21] as concurrent work but could do more to differentiate itself. While MIKASA-Robo is larger, a more detailed comparison of the task design philosophies would be beneficial. The MIKASA-Base component, while useful for engineering, is primarily an aggregation of existing work and thus a less significant scientific contribution than MIKASA-Robo.

#### Strengths
1.  **Systematic Approach:** The effort to create a taxonomy and structure the benchmark around it is commendable and provides a clear conceptual framework.
2.  **Scale of MIKASA-Robo:** The creation of 32 distinct, memory-focused robotic tasks is a substantial undertaking and a valuable asset for the community.
3.  **Open-Sourcing and Datasets:** Providing open-source code and offline datasets greatly enhances the reproducibility and utility of the work.
4.  **MDP Solvability Check:** Demonstrating that tasks are solvable with full state information (Figure 4) is a crucial sanity check that strengthens the benchmark's design.

#### Weaknesses
1.  **Outdated Baselines:** The primary memory-based online RL baseline (LSTM) is not state-of-the-art, which limits the strength of the conclusions about the capabilities of "current methods."
2.  **Weak VLA Evaluation:** The claims about VLA models are based on under-tuned experiments, making the conclusions unreliable.
3.  **Superficial Analysis of Failures:** The paper reports that many models fail but does not provide a deep analysis of *why* they fail, which would be more insightful for guiding future research.
4.  **Clarity of Taxonomy:** The definitions and boundaries between some memory categories (e.g., Capacity vs. Sequential) could be more precise.

#### Questions
1.  Could you justify the choice of PPO-LSTM as the primary memory-based online RL baseline over more recent and powerful architectures like GTrXL or other Transformer-based agents? How might your conclusions change if a stronger baseline were used?
2.  Regarding the VLA experiments, can you elaborate on what "limited fine-tuning" entailed? Given that these models are often sensitive to fine-tuning protocols, how confident are you that their poor performance is due to a lack of memory versus a domain shift or optimization issue?
3.  The offline RL models, including sequence models, score exactly 0.00 on all nine `*OfColors` tasks (Table 6). This seems systematic. Did you investigate whether this is an artifact of the dataset collection policy or some other factor?

#### Rating
- Overall (10): 6 — The paper introduces a valuable set of robotic tasks, but the experimental evaluation has notable weaknesses that temper the strength of its conclusions.
- Novelty (10): 7 — The MIKASA-Robo benchmark is novel and substantial, though the taxonomy and MIKASA-Base are more incremental contributions.
- Technical Quality (10): 5 — The experimental design has flaws, particularly in the choice of baselines and the execution of the VLA experiments, which weakens the paper's claims.
- Clarity (10): 7 — The paper is mostly clear, but key figures (Figure 6) and definitions (Section 4.2) could be improved.
- Confidence (5): 5 — I am very confident in my evaluation, based on my expertise in RL methodology and benchmarking.

***

### **Review 3**

#### Summary
This paper introduces MIKASA, a benchmark suite for memory-based reinforcement learning, with a major focus on a new robotics component, MIKASA-Robo. The authors propose a taxonomy to classify memory tasks, unify existing benchmarks into MIKASA-Base, and develop 32 novel tabletop manipulation tasks in MIKASA-Robo. These tasks are designed to probe different facets of an agent's memory, such as recalling object properties, spatial locations, and action sequences. The work evaluates several RL and VLA baselines, showing that these memory-intensive manipulation tasks present a significant challenge, especially under sparse rewards.

#### Soundness
From a robotics perspective, the work is methodologically sound. The choice of the ManiSkill3 simulator (Section 6) is excellent, as it allows for efficient, GPU-parallelized simulation, which is critical for large-scale RL experiments. The task designs in MIKASA-Robo are clever and effectively isolate specific memory challenges in a controlled manipulation setting (e.g., `ShellGame` for object permanence, `TakeItBack` for spatial memory, `ChainOfColors` for sequential memory). The inclusion of both dense and sparse reward settings is crucial; the dense rewards facilitate initial algorithm development, while the sparse rewards represent a more realistic and challenging scenario for robotics. The experiments showing that online agents completely fail with sparse rewards (Figure 10) are a stark and important result, highlighting the immense difficulty of long-horizon exploration and credit assignment in memory-intensive robotic tasks.

#### Presentation
The paper is well-presented. The motivation for a memory-focused robotics benchmark is well-established by contrasting with existing frameworks in Table 3. The specific MIKASA-Robo tasks are clearly explained in the main text (Section 6.1) and meticulously detailed in Appendix H, with helpful illustrations (e.g., Figure 11, 12). The inclusion of code snippets for getting started (Code 1, 2) is a thoughtful touch that will greatly aid adoption by the robotics community. The visual results in Figure 2 and others provide a good intuition for the tasks.

#### Contribution
This work makes a significant and much-needed contribution to the robotic learning community. As the authors correctly argue, most existing manipulation benchmarks operate under the assumption of full observability (or use simple frame-stacking), largely ignoring the critical role of memory. MIKASA-Robo is, to my knowledge, the most extensive and systematically designed benchmark for probing memory in a tabletop manipulation context. It provides a challenging new testbed that moves beyond simple pick-and-place tasks to scenarios requiring genuine recall and temporal reasoning. The release of offline datasets is also a major plus, as it enables research in imitation learning and offline RL, which are highly relevant paradigms for robotics.

#### Strengths
1.  **Fills a Critical Gap:** Addresses the lack of memory-intensive benchmarks in the popular domain of tabletop robotic manipulation (Table 3).
2.  **High-Quality Simulation:** Built on ManiSkill3, a modern and highly efficient simulator, making large-scale experimentation feasible.
3.  **Well-Designed, Isolated Tasks:** The 32 tasks are thoughtfully constructed to test specific memory types, allowing for fine-grained analysis of agent capabilities.
4.  **Realistic Challenge:** The experiments with sparse rewards demonstrate that the benchmark poses a difficult and realistic challenge, pushing the boundaries of current algorithms.
5.  **Focus on Reproducibility:** The release of the installable package, detailed documentation, and offline datasets is a great service to the community.

#### Weaknesses
1.  **Limited Discussion on Sim-to-Real:** The paper does not discuss the potential for transferring policies trained on MIKASA-Robo to real-world robots. While the tasks are inspired by real-world challenges, they are also quite abstract (e.g., remembering sequences of colored cubes). A discussion on the sim-to-real viability of these specific memory skills would strengthen the paper's impact for robotics practitioners.
2.  **Potential for Intractability:** The fact that standard online RL agents achieve zero success with sparse rewards (Figure 10) might indicate that the tasks are too difficult to solve with current exploration methods. This could limit the benchmark's utility for researchers who do not have the resources to develop highly specialized solutions.
3.  **Abstract Nature of Some Tasks:** While tasks like `ShellGame` and `TakeItBack` have clear real-world analogues, the connection for tasks like `ChainOfColors` is less direct. Explicitly linking these more abstract sequential/capacity tasks to real-world robot skills (e.g., following a multi-step assembly instruction) would improve the motivation.

#### Questions
1.  What are your thoughts on the sim-to-real transfer of the memory capabilities tested in MIKASA-Robo? For example, would an agent that masters the `Intercept` task in simulation be expected to have a head start on catching a real rolling object, or are the required skills too simulation-specific?
2.  Given the difficulty of the sparse reward setting, have you considered providing datasets with sub-optimal or mixed-quality trajectories? This could open up new research avenues in offline RL, such as learning from imperfect data, which is a common scenario in robotics.
3.  Could you elaborate on the choice of the end-effector action space? How critical is this choice to the difficulty of the tasks, and did you experiment with other action representations (e.g., joint-space control)?

#### Rating
- Overall (10): 8 — This is a strong contribution that introduces a valuable and challenging benchmark for the robotic learning community.
- Novelty (10): 8 — The focus on diverse memory skills in a large-scale tabletop manipulation benchmark is highly novel for the robotics field.
- Technical Quality (10): 8 — The benchmark is well-implemented on a modern simulator, and the experiments are well-conducted, though the sparse-reward tasks may be intractably difficult.
- Clarity (10): 9 — The paper and especially the appendix provide clear and detailed descriptions of the robotic tasks and setup.
- Confidence (5): 5 — I am highly confident in this review, as my expertise lies in robot learning and simulation benchmarks.

***

### **Review 4**

#### Summary
This paper introduces MIKASA, a comprehensive framework for benchmarking memory in reinforcement learning. The work is built upon a proposed taxonomy that classifies memory tasks into four types: object, spatial, sequential, and capacity. This framework is instantiated in two ways: MIKASA-Base, which unifies existing memory-RL environments, and MIKASA-Robo, a novel suite of 32 robotic manipulation tasks designed to probe these memory types. Through experiments with various baselines, the paper demonstrates that its benchmark, particularly MIKASA-Robo, poses a significant challenge to current RL agents.

#### Soundness
The conceptual foundation of this work is very strong. The proposed taxonomy (Section 4) is well-motivated by principles from cognitive science and provides a much-needed structured language for discussing and evaluating memory in RL. This is a significant step up from the ad-hoc collections of tasks used in prior work (Table 2). The decision to create both a collection of existing tasks (MIKASA-Base) and a new, domain-specific benchmark (MIKASA-Robo) is sound, addressing both the need for standardization and the need for new challenges. The experimental results, while based on somewhat standard baselines, are sufficient to validate the primary claim: that the benchmark effectively stratifies tasks by difficulty and reveals clear limitations in existing agents as memory demands increase (e.g., the performance drop from RememberColor3 to RememberColor9 in Figure 5).

#### Presentation
The paper is exceptionally well-structured and clearly written. It successfully tells a compelling story, starting with the problem of fragmentation (Figure 1), proposing a conceptual solution (the taxonomy in Section 4), and then delivering concrete implementations (Sections 5 and 6). Figure 3, which positions MIKASA as a bridge between overly simplistic agent memory tasks and overly complex human memory tasks, is a nice conceptual illustration. The overall organization makes the paper's multi-part contribution easy to follow and appreciate. The writing is professional and precise throughout.

#### Contribution
The contribution of this paper is significant and holistic. It is more than just another benchmark; it is a well-argued proposal for *how* the community should approach the evaluation of memory in RL. The taxonomy itself is a valuable intellectual contribution that can guide the design of future tasks and the analysis of agent capabilities. MIKASA-Base addresses the practical problem of reproducibility and fragmentation. MIKASA-Robo pushes the field into a complex, embodied domain where memory is undeniably crucial. By tackling the problem at conceptual, practical, and forward-looking levels, this work provides a landmark contribution that has the potential to shape the research agenda in this area for years to come.

#### Strengths
1.  **Strong Conceptual Framework:** The taxonomy of memory tasks is a key strength, providing a principled foundation for the entire work and a useful tool for the community.
2.  **Holistic and Strategic Contribution:** The paper wisely combines the consolidation of past work with the creation of novel future challenges, maximizing its potential impact.
3.  **Clear Motivation and Narrative:** The paper does an excellent job of motivating the need for such a benchmark and presents its contributions within a clear and compelling narrative.
4.  **Ambitious and Well-Executed:** The scale of the project—designing a taxonomy, collecting and unifying old tasks, and creating 32 new ones with datasets—is impressive and the execution is of high quality.

#### Weaknesses
1.  **Limitations of the Taxonomy:** The paper presents the four-category taxonomy as comprehensive but does not discuss its potential limitations. Are there aspects of memory, such as causal reasoning or transitive inference (which are mentioned in Section 4.1), that do not fit neatly into this framework? A brief discussion of what the taxonomy does *not* cover would make the contribution more complete.
2.  **Weak Link Between MIKASA-Base and MIKASA-Robo:** While both benchmarks are part of the MIKASA suite, the paper does not draw explicit connections between the tasks in each. For example, is `POPGym Autoencode` from MIKASA-Base the abstract equivalent of `ChainOfColors` from MIKASA-Robo? Highlighting these parallels would strengthen the argument that the taxonomy is a unifying concept across different domains.
3.  **Title is a bit generic:** The title "Memory, Benchmark & Robots" is a bit bland and does not fully capture the structured, taxonomy-driven approach that is a key strength of the paper.

#### Questions
1.  Could you elaborate on the scope and potential limitations of your proposed taxonomy? For instance, how would you classify tasks that require meta-learning (i.e., "remembering" how to learn quickly) or social reasoning (remembering the behavior of other agents)?
2.  You have categorized existing tasks from benchmarks like POPGym and MemoryGym into your taxonomy (Table 7). Did this process reveal any gaps in the types of memory being tested by existing benchmarks, which in turn motivated the design of specific tasks in MIKASA-Robo?
3.  The offline RL results (Figure 6) are visualized on a radar plot organized by memory type. This is a great use of the taxonomy. Did you observe any patterns, e.g., are certain offline algorithms biased towards performing better on one type of memory (like Spatial) versus another (like Sequential)?

#### Rating
- Overall (10): 9 — A landmark paper that provides a comprehensive and conceptually grounded framework for evaluating memory in RL, likely to become a standard reference.
- Novelty (10): 9 — The taxonomy-driven approach to benchmarking is highly novel, and the MIKASA-Robo suite is a significant and original contribution.
- Technical Quality (10): 9 — The benchmark design and implementation are of high quality, and the conceptual framework is robust and well-argued.
- Clarity (10): 10 — The paper is exceptionally well-written and structured, presenting a complex, multi-part contribution in a very accessible manner.
- Confidence (5): 5 — I am very confident in my assessment, having a broad background in reinforcement learning and machine learning evaluation.