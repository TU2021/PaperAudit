Summary
The paper introduces Text-to-Talk (TtT), a unified audio–text framework that generates text autoregressively (AR) and audio tokens non-autoregressively (NAR) using absorbing discrete diffusion within a single Transformer. The central motivation is a modality asymmetry: text exhibits strong target–target dependencies that favor causal decoding, while audio tokens are largely source–target dependent and benefit from any-order conditioning. The authors formalize a partial-order (poset) factorization and order-marginalization over audio spans, mapping absorbing diffusion to any-order autoregression and deriving a unified AR+AO training objective that upper-bounds the negative log of an order-marginalized joint score. A modality-aware attention mask enforces causal dependencies for text and bidirectional conditioning within audio spans, while maintaining causal structure across spans. Three training strategies—BANOM, PPM, and SST—are proposed to reduce train–test discrepancies and support variable-length generation via an explicit end-of-audio token. The inference procedure combines AR text decoding with masked diffusion for audio blocks. Experiments at 1.5B and 3B scales report improvements over AR-only and NAR-only baselines on Audio-QA and ASR, with ablations indicating that each strategy contributes materially.

Strengths
- Clear conceptual motivation for hybrid AR–NAR modeling grounded in modality-specific dependency structures (text target–target vs audio source–target).
- Solid theoretical framing: mapping absorbing diffusion to any-order conditioning, poset-based factorization of the joint, and an upper-bound training objective via Jensen’s inequality connecting the practical AO loss to an order-marginalized conditional.
- Elegant, unified architecture: a single Transformer with modality-aware attention masks that enforce causal text decoding and bidirectional intra-audio-span modeling, remaining implementable within standard decoder-only frameworks.
- Practical training strategies (BANOM, PPM, SST) that effectively mitigate train–test mismatch and enable variable-length audio generation; ablation studies indicate substantial contribution, particularly SST for reliable termination.
- Coherent inference design that supports parallel audio synthesis within blocks and early termination, aligning with the proposed dependency structure.
- Empirical gains over AR-only and NAR-only baselines across Audio-QA and ASR tasks and model sizes, accompanied by informative figures and a generally clear, well-organized presentation.

Weaknesses
- Probabilistic interpretation of the joint “scoring function” p̃θ is unclear; there is no explicit proof of normalization, making the “upper bound on NLL of the desired joint distribution” potentially overstated unless reframed as a bound on a joint score.
- Baseline fairness is insufficiently documented: the diffusion/NAR baseline is unusually weak, and parity details (training data balance, hyperparameters, attention masks, diffusion steps, block sizes, optimization schedules) are sparse, raising questions about the strength of comparative claims.
- The evaluation pipeline relies on ASR transcription followed by an LLM-as-a-Judge for Audio-QA, introducing compounding biases (ASR errors, judge variability). No objective speech quality metrics (e.g., MOS, intelligibility/prosody) or latency/throughput measurements are reported, despite claims of streaming and low first-token latency.
- Real-time viability is unsubstantiated: the use of 200 diffusion steps per audio block and large audio spans suggests nontrivial latency, yet there are no reported real-time factors or latency benchmarks.
- Potential train/test leakage: training includes datasets that also appear in evaluation (AISHELL-1/2, WenetSpeech), with no detailed safeguards (split protocols, speaker disjointness, filtering) described to prevent contamination.
- Training data formatting indicates fixed interleaving patterns between text and audio tokens, which may contradict the stated goal of content-aware variable-length generation; while SST aims to mitigate this, broader evidence of flexible generation is limited.
- Minor presentation inconsistencies (e.g., Algorithm 1 labeled “Autoregressive Audio Generation” despite NAR masked diffusion, inconsistent dataset naming, alternating “scoring” vs “distribution” terminology) detract slightly from clarity.
- Missing robustness studies (e.g., sensitivity to diffusion steps and block sizes) and comprehensive reporting on streaming performance (first-token latency, real-time factor) leave practical scalability and deployment implications unclear.
