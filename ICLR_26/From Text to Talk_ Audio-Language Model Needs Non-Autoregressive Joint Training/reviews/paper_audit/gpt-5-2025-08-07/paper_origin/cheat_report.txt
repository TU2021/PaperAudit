Academic Integrity and Consistency Risk Report

Scope and standard: This review flags substantive, evidence-backed inconsistencies or omissions that affect the paperâ€™s correctness, trustworthiness, or reproducibility. Each point is anchored to specific locations in the manuscript. No speculation on intent is made.

1) Evaluation dataset inconsistencies (naming and splits)
- Evidence:
  - Table 1 (Block #36) evaluates Audioâ€‘QA on AE (AlpacaEval), LQ (LLaMAQuestions), TQA (TriviaQA), WQ (WebQuestions), and ASR on Fleursâ€‘zh, AISHELLâ€‘2, AISHELLâ€‘1, WenetSpeechâ€‘test_meeting (WS_met.), WenetSpeechâ€‘test_net (WS_net), Fleursâ€‘en.
  - Section 4.2 (Blocks #38â€“39) discusses performance on AlpacaEval, LLaMAQuestions, TriviaQA, WebQuestions.
  - Appendix A.5.2 Table 4 (Block #64) lists Audioâ€‘QA datasets as AlpacaEval, TriviaQA, WebQuestions, and ReasoningQA (Chinese) â€” LLaMAQuestions is not listed. For ASR, it lists â€œWenetSpeechâ€ without specifying the â€œtest_meetingâ€/â€œtest_netâ€ splits reported in Table 1.
- Why this matters: The mismatch between the main results (Table 1) and the stated evaluation datasets (Appendix) obscures exactly which benchmarks were used and which splits were evaluated, hindering verification and replication.
- Required clarification: Precisely list the Audioâ€‘QA datasets used in Table 1 (including whether LLaMAQuestions or ReasoningQA was used) and specify the exact WenetSpeech splits for the ASR results.

2) Contradictory description of audio tokenizer/decoder
- Evidence:
  - Section 3.5 (Block #26): â€œwe adopt CosyVoice (Du et al., 2024) as both the audio encoder and decoder.â€
  - Section 4.1.5 (Block #35): â€œwe directly follow the audio tokenizer and decoder design introduced in GLMâ€‘4â€‘Voice (Zeng et al., 2024).â€
- Why this matters: The encoder/decoder choice materially affects implementation and comparability. The paper does not establish that GLMâ€‘4â€‘Voiceâ€™s design and CosyVoice are identical; withinâ€‘paper evidence connecting the two is missing.
- Required clarification: Specify whether the implementation uses CosyVoice modules, GLMâ€‘4â€‘Voice modules, or a combination; detail versions and any modifications.

3) Scaling claim contradicted by reported baseline results
- Evidence:
  - Section 4.2 (Blocks #38â€“39): â€œwe observe a clear scaling trend shared with the baselines, where larger backbones consistently yield stronger performance.â€
  - Table 1 (Block #36): For the AR baseline, the 3B model underperforms the 1.5B model on all four Audioâ€‘QA metrics (e.g., AE: 14.42 < 17.99; LQ: 10.00 < 16.78; TQA: 0.60 < 1.61; WQ: 0.70 < 2.32).
- Why this matters: The stated conclusion (â€œshared with the baselinesâ€) is not supported by the AR baseline numbers and overstates the generality of the scaling behavior.
- Required correction: Revise the scaling claim to match the data, and discuss why the AR baseline degrades with scale on these metrics.

4) Ambiguity in the NAR generation algorithm vs. text description
- Evidence:
  - Algorithm title (Block #60): â€œBlockâ€‘wise Masked Diffusion for Autoregressive Audio Generationâ€ â€” labeled â€œAutoregressive,â€ though the paper consistently treats audio as nonâ€‘autoregressive diffusion (Abstract Block #2; Section 3.2 Block #16; Figure 3(b) Block #24).
  - Section 3.5 (Blocks #26â€“28): Inference alternates between AR text and NAR audio; after predicting âŸ¨EOAâŸ©, control returns to AR.
  - Algorithm 1 (Block #60) lines 30â€“32: On encountering âŸ¨EOAâŸ© within a block, the procedure returns s immediately. The function preamble (â€œEnsure: Generated token sequence sâ€¦â€) does not clearly state whether this routine is scoped to a single audio span or the whole sequence, making the early return ambiguous relative to the â€œreturn to ARâ€ behavior described in Section 3.5.
- Why this matters: Mislabeling and unclear procedure boundaries make it hard to reproduce the alternating ARâ†”NAR decoding.
- Required clarification: Fix the algorithm title (NAR vs. AR), explicitly scope Algorithm 1 to â€œaudio span generation,â€ and describe how control returns to AR decoding in the overall loop.

5) Mathematical/notation inconsistencies and undefined terms
- Evidence:
  - Eq. 9 (Block #16): Conditions include â€œA_m^{âˆª M}â€ (or visually â€œA_m^{\cup M}â€), which is never defined. The notation â€œA_m^Î»â€ appears in text but the precise construction/distribution over masked sets is not formalized.
  - Eqs. 14, 15, 19 (Blocks #18â€“19): Use |T_m|, despite earlier defining text spans as ğ’¯_m with cardinality |ğ’¯_m| (Blocks #7, #14â€“15). The switch between T_m and ğ’¯_m is not explained.
- Why this matters: Ambiguous conditioning terms and inconsistent notation in core objectives impede correct reimplementation and theoretical verification.
- Required correction: Define A_m^{âˆª M} (or remove if a typo), formalize the distribution over masked sets in Eq. 9, and unify notation for text spans throughout.

6) Training data accounting is incomplete/ambiguous
- Evidence:
  - Section 4.1.1 (Block #31): â€œwe randomly sample one million instances from the ASR dataset, the TTS dataset, and the audio chat dataset respectively.â€
  - Table 2 (Block #63): Many rows have blank â€œSamplesâ€ counts (e.g., AISHELLâ€‘1/2/3, CommonVoice, LibriSpeech, MLSâ€‘Eng, PeopleSpeech, VoxPopuli, VoiceAssistantâ€‘400K, Fireflyâ€‘Trainâ€‘1.1M, etc.). The reported â€œTotal = 6,321,321â€ equals the sum of only those rows with nonâ€‘blank counts (e.g., Emilia_zh 500k + Emilia_en 500k + GigaSpeech 600k + WenetSpeech 400k + OpenHermesâ€‘2.5 1M + MathInstruct 262,039 + Nonspeech7k 59,282 + Chineseâ€‘FineWebâ€‘Edu 1.5M + FineWebâ€‘Edu 1.5M). Datasets with blank counts appear not to contribute to the total, despite being listed as used.
- Why this matters: Without explicit counts for all listed datasets and a reconciliation to the stated sampling strategy, the actual composition of the 6.3M examples is unclear, undermining reproducibility and fairness assessments.
- Required clarification: Provide sample counts for all listed datasets (or explicitly state which were excluded), and reconcile with the â€œrandomly sample one million per categoryâ€ statement.

7) Evaluation dataset coverage mismatch
- Evidence:
  - Appendix A.5.2 (Block #64) claims Audioâ€‘QA covers both English and Chinese (lists ReasoningQA as Chinese), but the main results table (Block #36) does not report any Chinese Audioâ€‘QA column.
- Why this matters: The described evaluation scope (crossâ€‘lingual Audioâ€‘QA) is not reflected in the reported results, leaving the Chinese Audioâ€‘QA evaluation unsupported by data in the main text.
- Required clarification: Either report the Chinese Audioâ€‘QA results or adjust the stated evaluation scope.

8) Missing reference details for the judge model
- Evidence:
  - Section 4.1.2 (Block #32) uses â€œQwen3â€‘235Bâ€‘A30Bâ€ as LLMâ€‘asâ€‘aâ€‘Judge.
  - References (Blocks #45â€“49) do not include an entry for â€œQwen3â€‘235Bâ€‘A30B.â€ No configuration or version details are provided beyond the name. No direct evidence found in the manuscript regarding its citation.
- Why this matters: The judging model materially affects Audioâ€‘QA scoring; lack of citation/details reduces transparency and repeatability.
- Required clarification: Add a reference and basic configuration details (version, prompt, temperature, grading rubric).

If addressed, the manuscript would be notably more reproducible and internally consistent.

No other clear integrityâ€‘related issues were identified beyond the above inconsistencies and missing details based on the provided manuscript.