1) Summary
This paper identifies a fundamental mismatch in how current audio-language models handle text and audio generation, arguing that text relies on target-target (autoregressive, AR) dependencies while audio relies on source-target (non-autoregressive, NAR) dependencies. The authors propose Text-to-Talk (TtT), a unified Transformer framework that integrates AR text generation with NAR audio synthesis using absorbing discrete diffusion. The method is supported by a theoretical analysis showing the joint training objective provides an upper bound on the negative log-likelihood of the desired distribution. To address train-test discrepancies, three novel training strategies are introduced. Experiments on Audio-QA and ASR tasks show that TtT outperforms purely AR and NAR baselines, with ablation studies validating the contribution of each component.2) Strengths
*   **Well-Motivated and Intuitive Core Thesis**: The paper is built on the clear and compelling observation that text and audio generation have fundamentally different dependency structures. This distinction provides a strong rationale for the proposed hybrid architecture.
    *   The introduction clearly articulates the problem, contrasting the target-target dependencies of text with the source-target dependencies of audio (Section 1, Paragraph 2).
    *   Figure 1 provides a helpful visual illustration of this core motivation, highlighting the different dependency patterns that a uniform autoregressive model fails to capture.
    *   This motivation directly leads to the design of a hybrid model that treats each modality according to its inherent properties, which is a principled approach to the problem (Section 3).*   **Rigorous Theoretical Foundation**: The proposed hybrid training objective is not merely a heuristic combination but is grounded in a solid theoretical framework.
    *   The authors formalize the joint distribution using a partial-order factorization, which respects the causal nature of text while allowing flexible ordering within audio spans (Section 3.3).
    *   A key theoretical result shows that the practical training objective, $\mathcal{L}_{\text{Unified}}(x)$, is a tight upper bound on the negative log-likelihood of the desired joint distribution, $\tilde{p}_\theta(x)$ (Equation 20).
    *   The full derivation of this upper bound is provided in Appendix A.1.1, adding to the technical soundness and reproducibility of the theoretical claims.*   **Comprehensive and Rigorous Experimental Validation**: The empirical evaluation is extensive and systematically validates the paper's claims through multiple lenses.
    *   The method is compared against strong baselines (purely AR and purely NAR) at two different model scales (1.5B and 3B), demonstrating the robustness of the approach (Table 1, Main Results).
    *   The proposed TtT model shows significant improvements over baselines on both conversational understanding (Audio-QA) and cross-modal alignment (ASR) tasks (Section 4.2, Table 1). For instance, TtT-3B improves upon the AR baseline by +24.68 on LLaMAQuestions and by 42.41 WER points on AISHELL-2.
    *   Detailed ablation studies are performed for each of the three proposed training strategies (BANOM, PPM, SST), quantifying their individual contributions and demonstrating their necessity (Table 1, Ablation Study).
    *   The paper also investigates the model's performance when initialized from a multimodally aligned pretrained model, showing that the benefits of the hybrid approach hold in both from-scratch and fine-tuning scenarios (Table 1, Training Strategy Comparison).*   **Thoughtful Mitigation of Practical Challenges**: The authors identify and propose concrete solutions for the train-test discrepancies inherent in a hybrid AR-NAR framework, demonstrating a deep understanding of the practical implementation details.
    *   Three specific training strategies are introduced: Batchwise AR & NAR Objective Mixing (BANOM), Prefix Preservation Masking (PPM), and Stochastic Span Truncation (SST) (Section 3.3).
    *   Each strategy is designed to address a specific issue: BANOM for contextual distribution shift, PPM for cross-span contextual inconsistency, and SST for positional bias in termination prediction.
    *   The ablation study confirms the effectiveness of these strategies, with performance degrading significantly when any of them are removed (Table 1, Section 4.3).3) Weaknesses
*   **Ambiguity in Non-Autoregressive Baseline Implementation**: The paper's comparison against the purely NAR baseline may be weakened by a lack of clarity in its implementation.
    *   The description of the "Qwen2.5-Base (NAR)" baseline is minimal (Section 4.2). It is unclear if the NAR objective (discrete diffusion) was applied to both text and audio tokens.
    *   The text suggests this is the case, stating the NAR model "ignores the inherent sequential structure of interleaved text–audio sequences" (Section 4.2, Paragraph 3), which would make it an exceptionally weak baseline for text generation.
    *   This lack of detail makes it difficult to assess whether the performance gap between TtT and the NAR baseline is due to the hybrid nature of TtT or simply a poorly suited objective for the text portion of the NAR baseline.*   **Absence of Quantitative Latency and Speed Evaluation**: A primary motivation for using NAR models is their potential for faster inference, yet the paper provides no quantitative evidence to support this benefit.
    *   The paper claims the model enables "low first-token latency and continuous streaming audio generation" (Section 3.5) and "efficient parallel audio generation" (Section 1, Contributions).
    *   However, no empirical results such as Real-Time Factor (RTF), wall-clock inference time, or latency measurements are reported to compare TtT against the AR baseline. This is a significant omission for a work proposing an NAR-based method.
    *   Without these metrics, a key practical advantage of the proposed approach remains unsubstantiated.*   **Limited Evaluation of Generative Audio Quality**: While the model is trained on a variety of tasks including TTS (Table 2), the evaluation focuses primarily on tasks that measure correctness (Audio-QA via ASR+LLM) and alignment (ASR), rather than the perceptual quality of the generated audio.
    *   The Audio-QA evaluation pipeline relies on an ASR model to transcribe the generated speech (Section 4.1.2), which measures semantic correctness but not acoustic properties like naturalness, prosody, or speaker similarity.
    *   No standard TTS evaluation metrics, either objective (e.g., Mel-Cepstral Distortion) or subjective (e.g., Mean Opinion Score), are reported.
    *   This makes it difficult to assess how the NAR diffusion component impacts the fine-grained quality of the synthesized speech, which is a critical aspect of a text-to-talk system.*   **Minor Lack of Clarity in Attention Mechanism Description**: The description of the modality-aware attention mechanism could be more precise.
    *   Section 3.4 describes the attention patterns for the prompt, text spans, and audio spans. However, the description is high-level and slightly ambiguous compared to the visualization in Figure 2(b).
    *   For example, it states text tokens maintain "strict causal attention," which could be interpreted as only attending to previous tokens within the same span, whereas the figure shows they attend to all prior spans as well.
    *   A more formal, step-by-step definition of the attention mask would improve clarity and reproducibility.4) Suggestions for Improvement
*   **Clarify and Justify the NAR Baseline**: In Section 4.2, provide a precise definition of the training objective used for the "Qwen2.5-Base (NAR)" baseline. Specify whether the diffusion loss was applied to text tokens, audio tokens, or both. Justify this choice and discuss its implications for the comparison.*   **Provide Quantitative Inference Speed Analysis**: To substantiate the claims of efficiency, add a new section or table in the experiments comparing the inference speed of TtT with the purely AR baseline. This analysis should include standard metrics like Real-Time Factor (RTF) for audio synthesis and end-to-end latency for generating responses of varying lengths.*   **Incorporate Direct Evaluation of Audio Quality**: To provide a more complete assessment of the model's capabilities, add an evaluation of the generated audio quality. This could be achieved by:
    *   Evaluating the model on a standard TTS benchmark.
    *   Reporting objective metrics (e.g., F0 RMSE, MCD) and/or subjective metrics (e.g., a Mean Opinion Score study) to compare the audio quality of TtT against the AR baseline.*   **Refine the Description of the Attention Mechanism**: Revise the text in Section 3.4 to provide a more formal and unambiguous description of the modality-aware attention mask. This could be done by explicitly defining the set of positions that a token at a given position `i` in a specific span type can attend to, ensuring the description perfectly matches Figure 2(b).5) Score
- Overall (10): 8 — The paper presents a novel, well-motivated, and theoretically-grounded hybrid architecture with strong empirical results and thorough ablations (Table 1, Section 3.3).
- Novelty (10): 9 — The core idea of a hybrid AR-NAR model for audio-text, justified by differing dependency structures and supported by a formal theoretical framework, is highly novel (Section 1, Section 3.3).
- Technical Quality (10): 9 — The theoretical analysis providing an upper bound on the NLL is a significant strength, and the experimental design is rigorous and comprehensive (Equation 20, Table 1).
- Clarity (10): 8 — The paper is generally well-written and easy to follow, though minor clarifications regarding the NAR baseline and attention mechanism would be beneficial (Section 4.2, Section 3.4).
- Confidence (5): 5 — I am highly confident in my assessment, as I am familiar with the literature on both autoregressive and diffusion-based generative models for multimodal sequences.