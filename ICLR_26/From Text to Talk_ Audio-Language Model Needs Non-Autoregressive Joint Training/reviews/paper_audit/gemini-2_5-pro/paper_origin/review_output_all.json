{
  "baseline_review": "1) Summary\nThis paper identifies a fundamental mismatch in how current audio-language models handle text and audio generation, arguing that text relies on target-target (autoregressive, AR) dependencies while audio relies on source-target (non-autoregressive, NAR) dependencies. The authors propose Text-to-Talk (TtT), a unified Transformer framework that integrates AR text generation with NAR audio synthesis using absorbing discrete diffusion. The method is supported by a theoretical analysis showing the joint training objective provides an upper bound on the negative log-likelihood of the desired distribution. To address train-test discrepancies, three novel training strategies are introduced. Experiments on Audio-QA and ASR tasks show that TtT outperforms purely AR and NAR baselines, with ablation studies validating the contribution of each component.2) Strengths\n*   **Well-Motivated and Intuitive Core Thesis**: The paper is built on the clear and compelling observation that text and audio generation have fundamentally different dependency structures. This distinction provides a strong rationale for the proposed hybrid architecture.\n    *   The introduction clearly articulates the problem, contrasting the target-target dependencies of text with the source-target dependencies of audio (Section 1, Paragraph 2).\n    *   Figure 1 provides a helpful visual illustration of this core motivation, highlighting the different dependency patterns that a uniform autoregressive model fails to capture.\n    *   This motivation directly leads to the design of a hybrid model that treats each modality according to its inherent properties, which is a principled approach to the problem (Section 3).*   **Rigorous Theoretical Foundation**: The proposed hybrid training objective is not merely a heuristic combination but is grounded in a solid theoretical framework.\n    *   The authors formalize the joint distribution using a partial-order factorization, which respects the causal nature of text while allowing flexible ordering within audio spans (Section 3.3).\n    *   A key theoretical result shows that the practical training objective, $\\mathcal{L}_{\\text{Unified}}(x)$, is a tight upper bound on the negative log-likelihood of the desired joint distribution, $\\tilde{p}_\\theta(x)$ (Equation 20).\n    *   The full derivation of this upper bound is provided in Appendix A.1.1, adding to the technical soundness and reproducibility of the theoretical claims.*   **Comprehensive and Rigorous Experimental Validation**: The empirical evaluation is extensive and systematically validates the paper's claims through multiple lenses.\n    *   The method is compared against strong baselines (purely AR and purely NAR) at two different model scales (1.5B and 3B), demonstrating the robustness of the approach (Table 1, Main Results).\n    *   The proposed TtT model shows significant improvements over baselines on both conversational understanding (Audio-QA) and cross-modal alignment (ASR) tasks (Section 4.2, Table 1). For instance, TtT-3B improves upon the AR baseline by +24.68 on LLaMAQuestions and by 42.41 WER points on AISHELL-2.\n    *   Detailed ablation studies are performed for each of the three proposed training strategies (BANOM, PPM, SST), quantifying their individual contributions and demonstrating their necessity (Table 1, Ablation Study).\n    *   The paper also investigates the model's performance when initialized from a multimodally aligned pretrained model, showing that the benefits of the hybrid approach hold in both from-scratch and fine-tuning scenarios (Table 1, Training Strategy Comparison).*   **Thoughtful Mitigation of Practical Challenges**: The authors identify and propose concrete solutions for the train-test discrepancies inherent in a hybrid AR-NAR framework, demonstrating a deep understanding of the practical implementation details.\n    *   Three specific training strategies are introduced: Batchwise AR & NAR Objective Mixing (BANOM), Prefix Preservation Masking (PPM), and Stochastic Span Truncation (SST) (Section 3.3).\n    *   Each strategy is designed to address a specific issue: BANOM for contextual distribution shift, PPM for cross-span contextual inconsistency, and SST for positional bias in termination prediction.\n    *   The ablation study confirms the effectiveness of these strategies, with performance degrading significantly when any of them are removed (Table 1, Section 4.3).3) Weaknesses\n*   **Ambiguity in Non-Autoregressive Baseline Implementation**: The paper's comparison against the purely NAR baseline may be weakened by a lack of clarity in its implementation.\n    *   The description of the \"Qwen2.5-Base (NAR)\" baseline is minimal (Section 4.2). It is unclear if the NAR objective (discrete diffusion) was applied to both text and audio tokens.\n    *   The text suggests this is the case, stating the NAR model \"ignores the inherent sequential structure of interleaved text–audio sequences\" (Section 4.2, Paragraph 3), which would make it an exceptionally weak baseline for text generation.\n    *   This lack of detail makes it difficult to assess whether the performance gap between TtT and the NAR baseline is due to the hybrid nature of TtT or simply a poorly suited objective for the text portion of the NAR baseline.*   **Absence of Quantitative Latency and Speed Evaluation**: A primary motivation for using NAR models is their potential for faster inference, yet the paper provides no quantitative evidence to support this benefit.\n    *   The paper claims the model enables \"low first-token latency and continuous streaming audio generation\" (Section 3.5) and \"efficient parallel audio generation\" (Section 1, Contributions).\n    *   However, no empirical results such as Real-Time Factor (RTF), wall-clock inference time, or latency measurements are reported to compare TtT against the AR baseline. This is a significant omission for a work proposing an NAR-based method.\n    *   Without these metrics, a key practical advantage of the proposed approach remains unsubstantiated.*   **Limited Evaluation of Generative Audio Quality**: While the model is trained on a variety of tasks including TTS (Table 2), the evaluation focuses primarily on tasks that measure correctness (Audio-QA via ASR+LLM) and alignment (ASR), rather than the perceptual quality of the generated audio.\n    *   The Audio-QA evaluation pipeline relies on an ASR model to transcribe the generated speech (Section 4.1.2), which measures semantic correctness but not acoustic properties like naturalness, prosody, or speaker similarity.\n    *   No standard TTS evaluation metrics, either objective (e.g., Mel-Cepstral Distortion) or subjective (e.g., Mean Opinion Score), are reported.\n    *   This makes it difficult to assess how the NAR diffusion component impacts the fine-grained quality of the synthesized speech, which is a critical aspect of a text-to-talk system.*   **Minor Lack of Clarity in Attention Mechanism Description**: The description of the modality-aware attention mechanism could be more precise.\n    *   Section 3.4 describes the attention patterns for the prompt, text spans, and audio spans. However, the description is high-level and slightly ambiguous compared to the visualization in Figure 2(b).\n    *   For example, it states text tokens maintain \"strict causal attention,\" which could be interpreted as only attending to previous tokens within the same span, whereas the figure shows they attend to all prior spans as well.\n    *   A more formal, step-by-step definition of the attention mask would improve clarity and reproducibility.4) Suggestions for Improvement\n*   **Clarify and Justify the NAR Baseline**: In Section 4.2, provide a precise definition of the training objective used for the \"Qwen2.5-Base (NAR)\" baseline. Specify whether the diffusion loss was applied to text tokens, audio tokens, or both. Justify this choice and discuss its implications for the comparison.*   **Provide Quantitative Inference Speed Analysis**: To substantiate the claims of efficiency, add a new section or table in the experiments comparing the inference speed of TtT with the purely AR baseline. This analysis should include standard metrics like Real-Time Factor (RTF) for audio synthesis and end-to-end latency for generating responses of varying lengths.*   **Incorporate Direct Evaluation of Audio Quality**: To provide a more complete assessment of the model's capabilities, add an evaluation of the generated audio quality. This could be achieved by:\n    *   Evaluating the model on a standard TTS benchmark.\n    *   Reporting objective metrics (e.g., F0 RMSE, MCD) and/or subjective metrics (e.g., a Mean Opinion Score study) to compare the audio quality of TtT against the AR baseline.*   **Refine the Description of the Attention Mechanism**: Revise the text in Section 3.4 to provide a more formal and unambiguous description of the modality-aware attention mask. This could be done by explicitly defining the set of positions that a token at a given position `i` in a specific span type can attend to, ensuring the description perfectly matches Figure 2(b).5) Score\n- Overall (10): 8 — The paper presents a novel, well-motivated, and theoretically-grounded hybrid architecture with strong empirical results and thorough ablations (Table 1, Section 3.3).\n- Novelty (10): 9 — The core idea of a hybrid AR-NAR model for audio-text, justified by differing dependency structures and supported by a formal theoretical framework, is highly novel (Section 1, Section 3.3).\n- Technical Quality (10): 9 — The theoretical analysis providing an upper bound on the NLL is a significant strength, and the experimental design is rigorous and comprehensive (Equation 20, Table 1).\n- Clarity (10): 8 — The paper is generally well-written and easy to follow, though minor clarifications regarding the NAR baseline and attention mechanism would be beneficial (Section 4.2, Section 3.4).\n- Confidence (5): 5 — I am highly confident in my assessment, as I am familiar with the literature on both autoregressive and diffusion-based generative models for multimodal sequences.",
  "final_review": "1) Summary\nThis paper identifies a fundamental mismatch in how current audio-language models handle text and audio generation, arguing that text relies on target-target (autoregressive, AR) dependencies while audio relies on source-target (non-autoregressive, NAR) dependencies. The authors propose Text-to-Talk (TtT), a unified Transformer framework that integrates AR text generation with NAR audio synthesis using absorbing discrete diffusion. The method is supported by a theoretical analysis showing the joint training objective provides an upper bound on the negative log-likelihood of the desired distribution. To address train-test discrepancies, three novel training strategies are introduced. Experiments on Audio-QA and ASR tasks show that TtT outperforms purely AR and NAR baselines, with ablation studies validating the contribution of each component.2) Strengths\n*   **Well-Motivated and Intuitive Core Thesis**: The paper is built on the clear and compelling observation that text and audio generation have fundamentally different dependency structures. This distinction provides a strong rationale for the proposed hybrid architecture.\n    *   The introduction clearly articulates the problem, contrasting the target-target dependencies of text with the source-target dependencies of audio (Section 1, Paragraph 2).\n    *   Figure 1 provides a helpful visual illustration of this core motivation, highlighting the different dependency patterns that a uniform autoregressive model fails to capture.\n    *   This motivation directly leads to the design of a hybrid model that treats each modality according to its inherent properties, which is a principled approach to the problem (Section 3).*   **Rigorous Theoretical Foundation**: The proposed hybrid training objective is not merely a heuristic combination but is grounded in a solid theoretical framework.\n    *   The authors formalize the joint distribution using a partial-order factorization, which respects the causal nature of text while allowing flexible ordering within audio spans (Section 3.3).\n    *   A key theoretical result shows that the practical training objective, $\\mathcal{L}_{\\text{Unified}}(x)$, is a tight upper bound on the negative log-likelihood of the desired joint distribution, $\\tilde{p}_\\theta(x)$ (Equation 20).\n    *   The full derivation of this upper bound is provided in Appendix A.1, adding to the technical soundness and reproducibility of the theoretical claims.*   **Comprehensive Experimental Design**: The empirical evaluation is extensive in scope, comparing the proposed method against multiple baselines and ablations across different model scales and tasks.\n    *   The method is compared against strong baselines (purely AR and purely NAR) at two different model scales (1.5B and 3B), demonstrating the robustness of the approach (Table 1, Main Results).\n    *   The proposed TtT model is evaluated on both conversational understanding (Audio-QA) and cross-modal alignment (ASR) tasks (Section 4.2, Table 1).\n    *   Detailed ablation studies are performed for each of the three proposed training strategies (BANOM, PPM, SST), quantifying their individual contributions (Table 1, Ablation Study).\n    *   The paper also investigates the model's performance when initialized from a multimodally aligned pretrained model, showing the benefits of the hybrid approach in both from-scratch and fine-tuning scenarios (Table 1, Training Strategy Comparison).*   **Thoughtful Mitigation of Practical Challenges**: The authors identify and propose concrete solutions for the train-test discrepancies inherent in a hybrid AR-NAR framework, demonstrating an understanding of the practical implementation details.\n    *   Three specific training strategies are introduced: Batchwise AR & NAR Objective Mixing (BANOM), Prefix Preservation Masking (PPM), and Stochastic Span Truncation (SST) (Section 3.3).\n    *   Each strategy is designed to address a specific issue: BANOM for contextual distribution shift, PPM for cross-span contextual inconsistency, and SST for positional bias in termination prediction.\n    *   The ablation study confirms the effectiveness of these strategies, with performance degrading when they are removed (Table 1, Section 4.3).3) Weaknesses\n*   **Unverifiable and Potentially Erroneous Citations**: The manuscript's credibility is seriously undermined by numerous citations that appear to be unverifiable or contain errors. This practice misrepresents the state of the art and prevents proper verification of the paper's claims about prior work.\n    *   Several references are cited with a future publication year of \"2025\" (e.g., Xu et al., 2025; Long et al., 2025; Ding et al., 2025; Nie et al., 2025), and their associated arXiv identifiers in the reference list appear to be invalid or correspond to future dates.\n    *   Other recent citations also appear to have invalid identifiers (e.g., Défossez et al., 2024; Zeng et al., 2024).\n    *   This issue is pervasive throughout the introduction and related work sections (Section 1, Appendix A.2), making it difficult to assess the paper's positioning and claimed novelty against the actual literature.*   **Inconsistencies Between Textual Claims and Reported Results**: The manuscript contains several key claims in the main text that are directly contradicted by the numerical data presented in its own tables, which raises concerns about the care taken in reporting results.\n    *   The paper claims that \"Pretrain+TtT consistently matches or surpasses Pretrain+AR\" (Section 4.4). However, Table 1 shows `Pretrain+AR` outperforming `Pretrain+TtT` on AlpacaEval (29.45 vs 26.73) and WenetSpeech-test_meeting (26.75 vs 27.59 WER).\n    *   The paper also claims \"TtT consistently achieves the best performance on all four Audio-QA datasets\" (Section 4.2). However, for the 1.5B model size, Table 1 shows the AR baseline outperforming TtT-1.5B on AlpacaEval (17.99 vs 15.68).*   **Absence of Quantitative Latency and Speed Evaluation**: A primary motivation for using NAR models is their potential for faster inference, yet the paper provides no quantitative evidence to support this benefit.\n    *   The paper claims the model enables \"low first-token latency and continuous streaming audio generation\" (Section 3.5) and \"efficient parallel audio generation\" (Section 1, Contributions).\n    *   However, no empirical results such as Real-Time Factor (RTF), wall-clock inference time, or latency measurements are reported to compare TtT against the AR baseline. This is a significant omission for a work proposing an NAR-based method.\n    *   Without these metrics, a key practical advantage of the proposed approach remains unsubstantiated.*   **Limited Evaluation of Generative Audio Quality**: While the model is trained on a variety of tasks including TTS (Table 2), the evaluation focuses primarily on tasks that measure correctness (Audio-QA via ASR+LLM) and alignment (ASR), rather than the perceptual quality of the generated audio.\n    *   The Audio-QA evaluation pipeline relies on an ASR model to transcribe the generated speech (Section 4.1.2), which measures semantic correctness but not acoustic properties like naturalness, prosody, or speaker similarity.\n    *   No standard TTS evaluation metrics, either objective (e.g., Mel-Cepstral Distortion) or subjective (e.g., Mean Opinion Score), are reported.\n    *   This makes it difficult to assess how the NAR diffusion component impacts the fine-grained quality of the synthesized speech, which is a critical aspect of a text-to-talk system.*   **Unaddressed Methodological and Experimental Ambiguities**: Several aspects of the method and experimental setup lack clarity or contain unresolved issues, hindering reproducibility and a full assessment of the work.\n    *   The description of the \"Qwen2.5-Base (NAR)\" baseline is minimal (Section 4.2). It is unclear if the NAR objective was applied to both text and audio, which would make it an unsuitable baseline for text generation and weaken the main comparison.\n    *   The paper introduces parallel masking for training efficiency (Section 3.4) but acknowledges this creates a \"cross-span contextual inconsistency\" (Section 3.3). The proposed solution, Prefix Preservation Masking (PPM), is only applied to 30% of samples (Section 4.1.4), meaning the model is trained with this flawed context the majority of the time. The implications of this partial mitigation are not discussed.\n    *   The evaluation setup is inconsistently described. The main results (Table 1, Section 4.2) report on the \"LLaMAQuestions\" dataset, but the appendix list of evaluation datasets (Table 4) omits it and lists \"ReasoningQA\" instead.4) Suggestions for Improvement\n*   **Review and Correct All Citations**: Thoroughly review the entire reference list. Replace all unverifiable, future-dated, or erroneous citations with correct and verifiable references. Ensure that the literature review accurately reflects the current state of the field based on published or publicly available work.*   **Ensure Consistency Between Text and Tables**: Revise the textual claims in Sections 4.2 and 4.4 to accurately reflect the numerical results presented in Table 1. All performance claims should be stated precisely, acknowledging cases where baselines or alternative configurations perform better.*   **Provide Quantitative Inference Speed Analysis**: To substantiate the claims of efficiency, add a new section or table in the experiments comparing the inference speed of TtT with the purely AR baseline. This analysis should include standard metrics like Real-Time Factor (RTF) for audio synthesis and end-to-end latency for generating responses of varying lengths.*   **Incorporate Direct Evaluation of Audio Quality**: To provide a more complete assessment of the model's capabilities, add an evaluation of the generated audio quality. This could be achieved by evaluating the model on a standard TTS benchmark and reporting objective metrics (e.g., F0 RMSE, MCD) and/or subjective metrics (e.g., a Mean Opinion Score study) to compare the audio quality of TtT against the AR baseline.*   **Clarify Methodological and Experimental Details**:\n    *   In Section 4.2, provide a precise definition of the training objective for the NAR baseline and justify its design choice.\n    *   Provide a justification for applying PPM to only 30% of samples. Include an analysis or discussion of the impact of the remaining 70% of samples being trained with inconsistent cross-span context.\n    *   Resolve the discrepancy in dataset naming between Table 1 and Table 4 to clarify which dataset was used for evaluation.5) Score\n- Overall (10): 4 — While the paper presents a novel hybrid architecture, its credibility is severely undermined by unverifiable citations, major inconsistencies between claims and results (Table 1), and significant experimental omissions (latency, audio quality).\n- Novelty (10): 8 — The hybrid AR-NAR model motivated by differing modality dependencies is a novel and principled architectural contribution (Section 1, Section 3.3).\n- Technical Quality (10): 4 — The theoretical derivation is a strength (Appendix A.1), but the paper suffers from incorrect reporting of results (Table 1 vs Section 4.4), questionable citation practices, and unaddressed methodological issues (e.g., partial PPM).\n- Clarity (10): 5 — The paper is generally well-structured, but clarity is hampered by major inconsistencies between textual claims and reported data (Table 1 vs Section 4.2), as well as ambiguities in the experimental setup (Table 4, Section 4.2).\n- Confidence (5): 5 — I am highly confident in my assessment, having thoroughly cross-referenced the paper's claims against its own tables, text, and reference list.",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 8,
        "novelty": 9,
        "technical_quality": 9,
        "clarity": 8,
        "confidence": 5
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 4,
        "novelty": 8,
        "technical_quality": 4,
        "clarity": 5,
        "confidence": 5
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "1) Summary\nThis paper identifies a fundamental mismatch in how current audio-language models handle text and audio generation, arguing that text relies on target-target (autoregressive, AR) dependencies while audio relies on source-target (non-autoregressive, NAR) dependencies. The authors propose Text-to-Talk (TtT), a unified Transformer framework that integrates AR text generation with NAR audio synthesis using absorbing discrete diffusion. The method is supported by a theoretical analysis showing the joint training objective provides an upper bound on the negative log-likelihood of the desired distribution. To address train-test discrepancies, three novel training strategies are introduced. Experiments on Audio-QA and ASR tasks show that TtT outperforms purely AR and NAR baselines, with ablation studies validating the contribution of each component.2) Strengths\n*   **Well-Motivated and Intuitive Core Thesis**: The paper is built on the clear and compelling observation that text and audio generation have fundamentally different dependency structures. This distinction provides a strong rationale for the proposed hybrid architecture.\n    *   The introduction clearly articulates the problem, contrasting the target-target dependencies of text with the source-target dependencies of audio (Section 1, Paragraph 2).\n    *   Figure 1 provides a helpful visual illustration of this core motivation, highlighting the different dependency patterns that a uniform autoregressive model fails to capture.\n    *   This motivation directly leads to the design of a hybrid model that treats each modality according to its inherent properties, which is a principled approach to the problem (Section 3).*   **Rigorous Theoretical Foundation**: The proposed hybrid training objective is not merely a heuristic combination but is grounded in a solid theoretical framework.\n    *   The authors formalize the joint distribution using a partial-order factorization, which respects the causal nature of text while allowing flexible ordering within audio spans (Section 3.3).\n    *   A key theoretical result shows that the practical training objective, $\\mathcal{L}_{\\text{Unified}}(x)$, is a tight upper bound on the negative log-likelihood of the desired joint distribution, $\\tilde{p}_\\theta(x)$ (Equation 20).\n    *   The full derivation of this upper bound is provided in Appendix A.1, adding to the technical soundness and reproducibility of the theoretical claims.*   **Comprehensive Experimental Design**: The empirical evaluation is extensive in scope, comparing the proposed method against multiple baselines and ablations across different model scales and tasks.\n    *   The method is compared against strong baselines (purely AR and purely NAR) at two different model scales (1.5B and 3B), demonstrating the robustness of the approach (Table 1, Main Results).\n    *   The proposed TtT model is evaluated on both conversational understanding (Audio-QA) and cross-modal alignment (ASR) tasks (Section 4.2, Table 1).\n    *   Detailed ablation studies are performed for each of the three proposed training strategies (BANOM, PPM, SST), quantifying their individual contributions (Table 1, Ablation Study).\n    *   The paper also investigates the model's performance when initialized from a multimodally aligned pretrained model, showing the benefits of the hybrid approach in both from-scratch and fine-tuning scenarios (Table 1, Training Strategy Comparison).*   **Thoughtful Mitigation of Practical Challenges**: The authors identify and propose concrete solutions for the train-test discrepancies inherent in a hybrid AR-NAR framework, demonstrating an understanding of the practical implementation details.\n    *   Three specific training strategies are introduced: Batchwise AR & NAR Objective Mixing (BANOM), Prefix Preservation Masking (PPM), and Stochastic Span Truncation (SST) (Section 3.3).\n    *   Each strategy is designed to address a specific issue: BANOM for contextual distribution shift, PPM for cross-span contextual inconsistency, and SST for positional bias in termination prediction.\n    *   The ablation study confirms the effectiveness of these strategies, with performance degrading when they are removed (Table 1, Section 4.3).3) Weaknesses\n*   **Unverifiable and Potentially Erroneous Citations**: The manuscript's credibility is seriously undermined by numerous citations that appear to be unverifiable or contain errors. This practice misrepresents the state of the art and prevents proper verification of the paper's claims about prior work.\n    *   Several references are cited with a future publication year of \"2025\" (e.g., Xu et al., 2025; Long et al., 2025; Ding et al., 2025; Nie et al., 2025), and their associated arXiv identifiers in the reference list appear to be invalid or correspond to future dates.\n    *   Other recent citations also appear to have invalid identifiers (e.g., Défossez et al., 2024; Zeng et al., 2024).\n    *   This issue is pervasive throughout the introduction and related work sections (Section 1, Appendix A.2), making it difficult to assess the paper's positioning and claimed novelty against the actual literature.*   **Inconsistencies Between Textual Claims and Reported Results**: The manuscript contains several key claims in the main text that are directly contradicted by the numerical data presented in its own tables, which raises concerns about the care taken in reporting results.\n    *   The paper claims that \"Pretrain+TtT consistently matches or surpasses Pretrain+AR\" (Section 4.4). However, Table 1 shows `Pretrain+AR` outperforming `Pretrain+TtT` on AlpacaEval (29.45 vs 26.73) and WenetSpeech-test_meeting (26.75 vs 27.59 WER).\n    *   The paper also claims \"TtT consistently achieves the best performance on all four Audio-QA datasets\" (Section 4.2). However, for the 1.5B model size, Table 1 shows the AR baseline outperforming TtT-1.5B on AlpacaEval (17.99 vs 15.68).*   **Absence of Quantitative Latency and Speed Evaluation**: A primary motivation for using NAR models is their potential for faster inference, yet the paper provides no quantitative evidence to support this benefit.\n    *   The paper claims the model enables \"low first-token latency and continuous streaming audio generation\" (Section 3.5) and \"efficient parallel audio generation\" (Section 1, Contributions).\n    *   However, no empirical results such as Real-Time Factor (RTF), wall-clock inference time, or latency measurements are reported to compare TtT against the AR baseline. This is a significant omission for a work proposing an NAR-based method.\n    *   Without these metrics, a key practical advantage of the proposed approach remains unsubstantiated.*   **Limited Evaluation of Generative Audio Quality**: While the model is trained on a variety of tasks including TTS (Table 2), the evaluation focuses primarily on tasks that measure correctness (Audio-QA via ASR+LLM) and alignment (ASR), rather than the perceptual quality of the generated audio.\n    *   The Audio-QA evaluation pipeline relies on an ASR model to transcribe the generated speech (Section 4.1.2), which measures semantic correctness but not acoustic properties like naturalness, prosody, or speaker similarity.\n    *   No standard TTS evaluation metrics, either objective (e.g., Mel-Cepstral Distortion) or subjective (e.g., Mean Opinion Score), are reported.\n    *   This makes it difficult to assess how the NAR diffusion component impacts the fine-grained quality of the synthesized speech, which is a critical aspect of a text-to-talk system.*   **Unaddressed Methodological and Experimental Ambiguities**: Several aspects of the method and experimental setup lack clarity or contain unresolved issues, hindering reproducibility and a full assessment of the work.\n    *   The description of the \"Qwen2.5-Base (NAR)\" baseline is minimal (Section 4.2). It is unclear if the NAR objective was applied to both text and audio, which would make it an unsuitable baseline for text generation and weaken the main comparison.\n    *   The paper introduces parallel masking for training efficiency (Section 3.4) but acknowledges this creates a \"cross-span contextual inconsistency\" (Section 3.3). The proposed solution, Prefix Preservation Masking (PPM), is only applied to 30% of samples (Section 4.1.4), meaning the model is trained with this flawed context the majority of the time. The implications of this partial mitigation are not discussed.\n    *   The evaluation setup is inconsistently described. The main results (Table 1, Section 4.2) report on the \"LLaMAQuestions\" dataset, but the appendix list of evaluation datasets (Table 4) omits it and lists \"ReasoningQA\" instead.4) Suggestions for Improvement\n*   **Review and Correct All Citations**: Thoroughly review the entire reference list. Replace all unverifiable, future-dated, or erroneous citations with correct and verifiable references. Ensure that the literature review accurately reflects the current state of the field based on published or publicly available work.*   **Ensure Consistency Between Text and Tables**: Revise the textual claims in Sections 4.2 and 4.4 to accurately reflect the numerical results presented in Table 1. All performance claims should be stated precisely, acknowledging cases where baselines or alternative configurations perform better.*   **Provide Quantitative Inference Speed Analysis**: To substantiate the claims of efficiency, add a new section or table in the experiments comparing the inference speed of TtT with the purely AR baseline. This analysis should include standard metrics like Real-Time Factor (RTF) for audio synthesis and end-to-end latency for generating responses of varying lengths.*   **Incorporate Direct Evaluation of Audio Quality**: To provide a more complete assessment of the model's capabilities, add an evaluation of the generated audio quality. This could be achieved by evaluating the model on a standard TTS benchmark and reporting objective metrics (e.g., F0 RMSE, MCD) and/or subjective metrics (e.g., a Mean Opinion Score study) to compare the audio quality of TtT against the AR baseline.*   **Clarify Methodological and Experimental Details**:\n    *   In Section 4.2, provide a precise definition of the training objective for the NAR baseline and justify its design choice.\n    *   Provide a justification for applying PPM to only 30% of samples. Include an analysis or discussion of the impact of the remaining 70% of samples being trained with inconsistent cross-span context.\n    *   Resolve the discrepancy in dataset naming between Table 1 and Table 4 to clarify which dataset was used for evaluation.5) Score\n- Overall (10): 4 — While the paper presents a novel hybrid architecture, its credibility is severely undermined by unverifiable citations, major inconsistencies between claims and results (Table 1), and significant experimental omissions (latency, audio quality).\n- Novelty (10): 8 — The hybrid AR-NAR model motivated by differing modality dependencies is a novel and principled architectural contribution (Section 1, Section 3.3).\n- Technical Quality (10): 4 — The theoretical derivation is a strength (Appendix A.1), but the paper suffers from incorrect reporting of results (Table 1 vs Section 4.4), questionable citation practices, and unaddressed methodological issues (e.g., partial PPM).\n- Clarity (10): 5 — The paper is generally well-structured, but clarity is hampered by major inconsistencies between textual claims and reported data (Table 1 vs Section 4.2), as well as ambiguities in the experimental setup (Table 4, Section 4.2).\n- Confidence (5): 5 — I am highly confident in my assessment, having thoroughly cross-referenced the paper's claims against its own tables, text, and reference list."
}