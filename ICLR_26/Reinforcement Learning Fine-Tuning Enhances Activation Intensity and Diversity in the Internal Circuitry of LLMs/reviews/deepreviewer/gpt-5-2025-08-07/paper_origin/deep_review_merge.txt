Summary
The paper investigates how reinforcement learning (RL) post-training alters the internal information flow of large language models (LLMs) relative to supervised fine-tuning (SFT). It represents Transformer residual pathways as a directed acyclic graph and applies Edge Attribution Patching (EAP), a gradient-based linearization, to estimate edge importance under a self-entropy objective computed on each model’s own truncated generations. The authors define three aggregate metrics—Activation Intensity, Information Complexity (entropy-based), and Distribution Kurtosis—to summarize changes in edge activations and their diversity. Experiments on four 7B model families (DeepSeek-Math, Mistral, Distilled-Qwen, Qwen2.5) across three math benchmarks (GSM8K, MATH, College Math) show that online RL methods (PPO/GRPO) consistently increase activation strength and entropy while reducing kurtosis, suggesting more redundant and flexible information flow. In contrast, DPO exhibits weaker or inconsistent changes in this setup. The paper emphasizes a mechanistic hypothesis linking online interaction during RL to broader pathway engagement and internal diversity, and provides visualizations and a consolidated results table to support these trends.

Strengths
- Conceptually coherent and technically sound framework: modeling residual streams as a graph and using EAP as a scalable, tractable approximation to ablation-style analyses.
- Clear, principled metricization: Activation Intensity, Information Complexity, and Distribution Kurtosis capture salient properties of edge-level activation strength and diversity.
- Broad empirical coverage: cross-family, multi-dataset evaluation at 7B scale, with consistent trends for PPO/GRPO across GSM8K, MATH, and College Math.
- Insightful synthesis: a unified mechanistic perspective that plausibly links online RL to increased redundancy and flexibility in internal information flow, providing interpretability-based context for observed behavioral improvements.
- Effective presentation of core ideas: concise equations, informative figures illustrating strengthened pathways and diversity changes, and an organized narrative connecting method to findings.
- Practical pipeline: systematic filtering/truncation to maintain tractability and open-source code to support reproduction.

Weaknesses
- Attribution target confound: EAP is computed on a self-entropy objective over each model’s own outputs, so edge importance reflects both task-related circuitry and differences in generation style or logit sharpness. Without teacher-forcing or shared token targets, comparability of attributions between base and RL models is limited.
- Selection bias in evaluation: filtering to questions both models answer correctly and truncating sequences improves comparability and tractability but narrows analysis to easier items and shorter contexts, potentially underrepresenting where RL’s advantages and corresponding circuitry changes are most pronounced.
- Lack of statistical rigor: results are reported without confidence intervals, hypothesis tests, or effect sizes; exceptions and early-truncation anomalies are discussed qualitatively rather than quantified. Missing entries due to GPU memory overflow further complicate robustness claims.
- Incomplete experimental specification: key hyperparameters for filtering/truncation (α, β, γ, δ) and entropy histogram binning (B, ε) are not fully disclosed in the main text, and dataset/sample counts per configuration are unspecified, hindering reproducibility and sensitivity analysis.
- Cross-model normalization concerns: metrics aggregate across modules without per-layer/head normalization, making magnitudes sensitive to architectural and scaling differences (e.g., LayerNorm behavior). Trends are most defensible within model pairs; cross-family magnitude comparisons are less meaningful.
- No validation of EAP approximation: the study does not calibrate EAP against ground-truth ablations (e.g., ACDC) on a subset, leaving uncertainty about attribution fidelity and rank stability of important edges.
- Causal interpretation not fully established: alternative explanations for observed metric shifts (formatting changes, verbosity, or logit temperature) are not ruled out, limiting the strength of claims that RL directly increases redundancy/flexibility of task-relevant circuitry.
- Scope limited to math tasks: it remains unclear whether the reported internal changes generalize to other domains such as coding or dialogue.
- Overbroad DPO characterization: conclusions about DPO’s inability to activate broader pathways are based on a static DPO setup; online/iterative DPO variants exist and are not evaluated, so general claims should be tempered.
