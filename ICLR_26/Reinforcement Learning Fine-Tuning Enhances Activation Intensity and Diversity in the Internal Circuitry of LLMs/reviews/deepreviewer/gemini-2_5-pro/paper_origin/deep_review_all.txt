Here are 4 complete reviews of the paper.

***

### **Review 1**

#### Summary

This paper investigates the internal mechanisms through which reinforcement learning (RL) fine-tuning enhances the capabilities of large language models (LLMs). Using an Edge Attribution Patching (EAP) framework, the authors analyze the internal information flow of four pairs of LLMs before and after RL fine-tuning on mathematical reasoning tasks. The study reveals two consistent effects of online RL (PPO, GRPO): an increase in the overall intensity of activations and a greater diversity in activation patterns. In contrast, models fine-tuned with Direct Preference Optimization (DPO) do not exhibit these changes, suggesting a mechanistic difference between online RL and offline preference-based methods.

#### Soundness

The methodology is sound and well-suited for the research question. The choice of the EAP framework provides a computationally tractable yet powerful way to probe the internal circuitry of LLMs (Section 3.2). The experimental design is robust, involving multiple model families (DeepSeek-Math, Mistral, Distilled-Qwen, Qwen2.5) and datasets (MATH, GSM8K, College Math), which strengthens the generality of the findings. The three proposed metrics—Activation Intensity, Information Complexity, and Distribution Kurtosis—are well-defined and effectively capture different aspects of the internal activation distributions (Section 4.2). The analysis correctly identifies and explains the anomalous behavior of the DPO-tuned model, grounding the explanation in the theoretical differences between online and offline training paradigms (Section 4.3).

#### Presentation

The paper is exceptionally well-written and clearly organized. The introduction effectively situates the work within the existing literature and clearly articulates the research gap (Section 1). The preliminaries and methods sections (Sections 2 and 3) provide a clear and detailed explanation of the underlying concepts, from the Transformer architecture to the EAP framework. The results are presented systematically in Table 1 and are well-supported by visualizations in Figures 2 and 3, which help build intuition for the quantitative findings. The distinction between online RL and DPO is a central theme that is clearly argued and consistently revisited throughout the paper.

#### Contribution

This paper makes a significant and novel contribution to the field of LLM interpretability and alignment. While many studies have shown *that* RL fine-tuning improves LLMs, this work offers a compelling explanation for *how* it achieves this at a mechanistic level. The core findings—that online RL increases activation intensity and diversity—provide a new, unified perspective on the effects of post-training. Furthermore, the paper's empirical demonstration of the mechanistic differences between online RL (PPO/GRPO) and DPO is a timely and important contribution, offering valuable insights for practitioners choosing between different alignment techniques.

#### Strengths

1.  **Novel Mechanistic Insight:** The paper provides a clear and empirically grounded explanation for the benefits of RL fine-tuning, moving beyond simple performance metrics to analyze internal information flow.
2.  **Robust and Generalizable Findings:** The consistency of the results across three different model families and RL algorithms (PPO and GRPO) strongly supports the paper's main conclusions.
3.  **Insightful Comparison of RL Methods:** The contrast between online RL and DPO is a major strength, highlighting that not all "RL-based" methods have the same internal effects and providing a plausible explanation for this divergence (Section 4.3).
4.  **Clear Methodology and Presentation:** The use of EAP is well-justified, and the metrics are intuitive and effectively capture the phenomena of interest. The paper is very well-written and easy to follow.

#### Weaknesses

1.  **Limited Task Domain:** The analysis is confined to mathematical reasoning tasks. While this is a complex domain, it would be beneficial to see if these findings generalize to other areas where RLHF is commonly applied, such as creative writing or safety alignment.
2.  **Interpretability of Figure 2:** The 3D plot in Figure 2 is visually interesting but difficult to interpret quantitatively. A 2D heatmap or summary statistics might have conveyed the "strengthening" of connections more clearly.

#### Questions

1.  The paper argues that increased intensity and diversity may explain RL's advantage in generalization. Have the authors considered designing an experiment to test this causal link more directly? For example, could one artificially induce these properties in an SFT model and see if its generalization improves?
2.  The analysis focuses on the magnitude of edge attributions. Did you investigate any structural patterns in *which* types of edges (e.g., FFN vs. attention, early vs. late layers) are most affected by RL fine-tuning?
3.  Given the findings about DPO, do you hypothesize that other offline RL methods would show similarly weak internal changes compared to online methods like PPO and GRPO?

#### Rating

- Overall (10): 9 — The paper provides a novel, robust, and significant mechanistic explanation for the effects of RL fine-tuning on LLMs.
- Novelty (10): 9 — It is one of the first studies to systematically connect RL post-training methods to specific, measurable changes in the internal circuitry of LLMs.
- Technical Quality (10): 9 — The methodology is sound, the experiments are comprehensive across multiple models, and the analysis is rigorous.
- Clarity (10): 10 — The paper is exceptionally well-written, with clear explanations, logical flow, and effective communication of its core ideas.
- Confidence (5): 5 — I am highly confident in my evaluation, as I am familiar with the literature on both LLM interpretability and RLHF.

***

### **Review 2**

#### Summary

This paper uses the Edge Attribution Patching (EAP) method to compare the internal activation patterns of LLMs before and after reinforcement learning (RL) fine-tuning. The authors analyze four pairs of models and find that online RL algorithms (PPO, GRPO) tend to increase the average magnitude (Activation Intensity) and entropy (Information Complexity) of edge attributions, while decreasing their kurtosis. They observe that Direct Preference Optimization (DPO) does not consistently produce these effects and attribute this to its offline training nature.

#### Soundness

The methodological soundness of this paper has several points that require further clarification.

1.  **EAP as a Proxy:** The EAP method is a first-order Taylor approximation of the effect of edge ablation (Section 3.2, Eq. 9). While computationally efficient, its accuracy as a proxy for true information flow is not validated in this work. How well does this linearization hold for large parameter changes and complex tasks? Without this validation, the "edge weights" being analyzed might not accurately reflect functional importance.
2.  **Sample Selection Bias:** The methodology for selecting questions is a significant concern. The authors filter for questions that are "correctly answered by both models" (Section 3.3). This introduces a survivorship bias, as the analysis is performed only on the subset of problems where the base SFT model already succeeds. The observed changes might not be representative of how the RL model handles problems that the SFT model fails on, which is arguably the most interesting area for understanding improvement.
3.  **Metric Interpretation:** The interpretation of the proposed metrics is not entirely straightforward. For instance, higher "Activation Intensity" is interpreted as "more internal pathways are engaged and their signals become stronger" (Abstract). However, it could also indicate a less efficient, noisier network. Similarly, lower "Distribution Kurtosis" is linked to uniformity, but the implications for model performance are not directly established. The link between these metrics and the claimed benefit of "generalization" is correlational at best.

#### Presentation

The presentation is generally clear, but some aspects could be improved. Table 1 is extremely dense and difficult to parse, making it hard to verify the claims of consistency without significant effort. The use of four different values for the hyperparameter α adds to the clutter without a clear payoff in insight. The figures are also problematic. Figure 2 is a 3D surface plot that is nearly impossible to interpret meaningfully. Figure 3 combines a scatter plot with an embedded image of a table (Figure 3a) and a bar chart that is also described in a separate, poorly formatted table in the text (Block 29), which is confusing and unprofessional.

#### Contribution

The paper's contribution is in its attempt to mechanistically explain the effects of RL fine-tuning. The distinction it draws between online RL and DPO is interesting and potentially valuable. However, the contribution is weakened by the methodological concerns mentioned above. If the measurements are biased or the metrics are not robustly linked to performance, the conclusions, while plausible, rest on a shaky foundation. The work is more of an exploratory analysis than a conclusive demonstration of a mechanism.

#### Strengths

1.  **Ambitious Research Question:** The paper tackles a fundamental and important question: why does RL fine-tuning work?
2.  **Multi-Model Study:** The use of four different model pairs is a commendable effort to ensure the findings are not specific to a single architecture or training run.
3.  **Highlighting DPO's Uniqueness:** The finding that DPO behaves differently from PPO/GRPO is an important empirical result that aligns with other recent work and warrants further investigation.

#### Weaknesses

1.  **Potential for Sample Selection Bias:** Filtering for commonly solved problems may obscure the most important mechanisms of improvement (Section 3.3).
2.  **Unvalidated Proxy Method:** The reliance on the EAP approximation without validating its accuracy for this context is a key weakness.
3.  **Causality Not Established:** The paper repeatedly suggests that the observed internal changes "may explain" or "may underlie" improved generalization (Abstract, Section 6), but no evidence is provided to establish a causal link. The observed changes could be an epiphenomenon of RL training rather than the cause of its success.
4.  **Poor Data Visualization:** The figures, particularly Figure 2 and the formatting of Figure 3, detract from the paper's quality and make the results difficult to assess.

#### Questions

1.  Can you provide a justification for filtering questions based on whether both models answer them correctly? How might this filtering strategy affect your conclusions about how RL improves upon the SFT model's weaknesses?
2.  How sensitive are the EAP scores, and consequently your three metrics, to the choice of loss function? The self-entropy loss (Eq. 13) is based on the model's own generation; would using a standard cross-entropy loss against a ground-truth solution change the results?
3.  The paper claims RL reshapes information flow to be "more redundant." How do you reconcile this with the idea of a more efficient network? Could this "redundancy" also be interpreted as a lack of specialization?

#### Rating

- Overall (10): 5 — The paper addresses an interesting question, but significant methodological concerns about sample bias and the validity of the core measurement technique undermine the confidence in its conclusions.
- Novelty (10): 7 — The approach of using EAP to compare SFT and RL models is novel, and the DPO vs. online RL finding is important.
- Technical Quality (10): 4 — The experimental design suffers from potential sample selection bias, and the core attribution method is used without validation in this context.
- Clarity (10): 6 — While the text is mostly readable, the key results are presented in a cluttered table and confusing figures, hindering comprehension.
- Confidence (5): 5 — I am very confident in my assessment, having expertise in both LLM interpretability and experimental methodology.

***

### **Review 3**

#### Summary

This paper presents an empirical study on the internal changes in Large Language Models (LLMs) after undergoing reinforcement learning (RL) fine-tuning. By applying Edge Attribution Patching (EAP) to analyze the information pathways in several model families, the authors find that online RL methods like PPO and GRPO consistently lead to increased activation intensity and diversity. This suggests that RL encourages a more robust and flexible use of the model's internal circuitry. Notably, the study finds that DPO, an offline preference-tuning method, does not induce these same changes, highlighting a key difference in how these alignment techniques operate.

#### Soundness

The methodology appears appropriate for this type of exploratory, mechanistic analysis. The use of EAP provides a scalable way to get fine-grained attribution scores across the entire model. The experimental setup, which includes four distinct model pairs, lends credibility to the claim that the observed effects are a general property of online RL fine-tuning. The metrics chosen (Activation Intensity, Information Complexity, Kurtosis) are reasonable quantifications of the high-level properties the authors wish to investigate—namely, the strength and diversity of pathway usage. The logic connecting the offline nature of DPO to its lack of consistent internal changes is sound and provides a compelling explanation for the observed discrepancy.

#### Presentation

The paper is well-structured and clearly written. The introduction and related work sections do an excellent job of positioning the paper's contribution. The core ideas are communicated effectively, and the central argument is easy to follow. While Table 1 is dense, it comprehensively presents the main results. The visualizations in Figure 3, particularly the scatter plot (3a), provide a strong and intuitive summary of the increased inter-sample diversity after RL fine-tuning.

#### Contribution

The primary contribution of this work is bridging the gap between the external, performance-based evaluation of RL fine-tuning and the internal, mechanistic understanding of LLMs. It provides the first compelling, multi-model evidence of how online RL systematically reshapes the internal information flow. This is a significant step forward for the field of interpretability and alignment. The finding that DPO operates differently at this mechanistic level is highly impactful, as it suggests that the choice of alignment algorithm has profound consequences for the model's internal representations, not just its surface-level behavior. This insight could guide the development of more effective and predictable post-training methodologies.

#### Strengths

1.  **High-Impact Research Question:** The paper addresses a critical question for the LLM community: what does RL fine-tuning actually *do* inside the model?
2.  **Strong Empirical Evidence:** The consistency of the findings for PPO and GRPO across three different base models makes a strong case for a general principle at play.
3.  **Important Algorithmic Distinction:** The contrast drawn between online RL and DPO is a major contribution, providing a mechanistic basis for differences in their behavior that have been anecdotally observed elsewhere.
4.  **Connects Two Fields:** The work successfully integrates techniques from mechanistic interpretability (EAP) with the practical field of LLM alignment, creating a valuable synthesis.

#### Weaknesses

1.  **Generalizability Beyond Math:** The study's exclusive focus on mathematical reasoning tasks limits the generalizability of its conclusions. Math problems are highly structured; it is unclear if the same internal changes (e.g., increased diversity and intensity) would be beneficial or even observed for more open-ended tasks like dialogue or creative writing. The mechanisms of improvement might be task-dependent.
2.  **Correlation vs. Causation:** The paper suggests the observed internal changes are responsible for RL's superior generalization. However, the study only establishes a correlation. It does not rule out the possibility that these changes are simply a byproduct of online training, while the actual performance gains come from other factors.

#### Questions

1.  How do you expect these findings to translate to non-reasoning tasks? For instance, in safety alignment, would we expect to see a similar increase in activation diversity, or would we expect RL to *suppress* certain pathways, leading to less diversity?
2.  Could the metrics you've proposed (Activation Intensity, Information Complexity) be used as a monitoring tool during RL fine-tuning? For example, could a plateau in these metrics indicate that the model is no longer learning new internal strategies?
3.  The paper focuses on changes in existing pathways. Did you investigate whether RL fine-tuning enables entirely new pathways that were dormant (zero or near-zero attribution) in the SFT model?

#### Rating

- Overall (10): 8 — A strong paper with a significant contribution to our understanding of RL fine-tuning, though its claims about generalization would be strengthened by exploring other task domains.
- Novelty (10): 9 — The paper introduces a novel and insightful way to analyze the effects of RL fine-tuning, and the DPO vs. online RL comparison is a new and important finding.
- Technical Quality (10): 8 — The technical approach is solid, with a strong multi-model experimental design, though the scope is limited to a single task domain.
- Clarity (10): 9 — The paper is very well-written and communicates its complex ideas effectively.
- Confidence (5): 5 — I am highly confident in my review, as my research interests align with the topics of LLM alignment and interpretability.

***

### **Review 4**

#### Summary

This paper investigates how reinforcement learning (RL) fine-tuning alters the internal workings of large language models (LLMs). Using a graph-based view of the Transformer and an attribution method called EAP, the authors measure changes in "edge weights" between an SFT model and its RL-tuned version. They report that for models trained with online RL (PPO, GRPO), the internal activations become stronger and more diverse. They contrast this with a model trained with DPO, which does not show these clear trends, and argue this is due to DPO's offline nature.

#### Soundness

The overall logical flow of the paper is coherent. The authors motivate a problem, propose a method of analysis, conduct experiments, and draw conclusions that seem to follow from their results. The use of multiple models is a good practice that strengthens the claims. However, the soundness is difficult to fully assess due to presentation issues that obscure the details of the results. The core assumption that EAP scores are a faithful representation of information flow is stated but not deeply interrogated, which is a common practice but still a limitation. The sample filtering process described in Section 3.3 seems reasonable in principle, but its parameters (α, β, γ, δ) are not provided, making it difficult to judge their potential impact.

#### Presentation

The presentation of this paper has significant room for improvement, particularly in its tables and figures.

*   **Table 1:** This table is the centerpiece of the empirical results, but it is nearly unreadable. It is a huge block of numbers with three different metrics, four model pairs, three datasets, and four hyperparameter settings. It is impossible to quickly grasp the key trends. This data should have been visualized, perhaps with summary plots showing the average change for each metric and model type.
*   **Figure 1:** This schematic is helpful for understanding the EAP method.
*   **Figure 2:** This 3D plot is ineffective. It is impossible to discern any clear pattern from the surface, and the axes are not well-explained. The caption claims it shows "connections strengthen," but this is not visually evident from the plot itself. A simple heatmap of the relative change would have been far more effective.
*   **Figure 3:** This figure is poorly constructed. In the main text (Block 29), part (a) is described with a small, low-resolution image of a table, and part (b) is a text-based table. The actual image for Figure 3 (Blocks 33 & 34) shows a scatter plot and a bar chart. This is confusing. The scatter plot (3a) is good and conveys its point well, but the embedded text box is an image, which is bad practice. The bar chart (3b) is also clear, but the disconnect with the text description is jarring.
*   **Mathematical Notation:** The paper introduces a lot of mathematical notation in Sections 2 and 3. While precise, it could be supplemented with more intuitive explanations to improve accessibility for a broader audience.

#### Contribution

The paper's intended contribution—to provide a mechanistic explanation for RL's effectiveness—is valuable. The finding that DPO behaves differently from online RL methods is also an important point. However, the paper's impact is currently hindered by its poor presentation. A reader who is not willing to spend a great deal of time deciphering Table 1 and the confusing figures may not be convinced of the paper's conclusions. The core ideas are promising, but they are not communicated as effectively as they could be.

#### Strengths

1.  **Important Topic:** The paper addresses a timely and important question about how LLM post-training works.
2.  **Clear Introduction:** The motivation and research gap are well-established in the introduction.
3.  **Multi-Model Analysis:** The study's use of four different model pairs is a definite strength.

#### Weaknesses

1.  **Ineffective Data Visualization:** The primary weakness is the poor design of the main results table and figures (Table 1, Figure 2, Figure 3), which obscures the evidence and makes the paper hard to follow.
2.  **Clarity of Metrics:** While mathematically defined, the intuitive meaning of "Information Complexity" and "Distribution Kurtosis" in the context of LLM pathways could be explained more clearly. For example, what does a "peaked" distribution of edge weights (high kurtosis) actually imply about the model's strategy?
3.  **Missing Hyperparameters:** Key hyperparameters for the sample filtering process (β, γ, δ in Section 3.3) are not specified, which harms reproducibility.

#### Questions

1.  Could you please consider redesigning Table 1 to be more interpretable? For example, by showing the percentage change from SFT to RL instead of absolute values, and perhaps averaging across the α values, which seem to show consistent trends.
2.  Can you replace Figure 2 with a visualization that more clearly shows the relative change in edge activation strength? A 2D heatmap is a standard and effective way to represent such matrix data.
3.  Could you clarify the relationship between the text description of Figure 3 in Block 29 and the actual plots shown in Blocks 33 and 34? Please ensure figures are self-contained and professionally formatted in the final version.

#### Rating

- Overall (10): 6 — The paper has a promising core idea and contribution, but it is significantly held back by poor presentation of its key results.
- Novelty (10): 8 — The research question and the comparative analysis of RL methods from a mechanistic viewpoint are highly novel.
- Technical Quality (10): 7 — The experimental setup seems mostly solid, but the lack of clarity in the results presentation makes a full evaluation difficult.
- Clarity (10): 4 — The paper is difficult to understand due to an overwhelming main table and poorly designed, confusing figures.
- Confidence (5): 4 — I am confident in my assessment of the presentation quality, but less confident in the technical details due to the clarity issues.