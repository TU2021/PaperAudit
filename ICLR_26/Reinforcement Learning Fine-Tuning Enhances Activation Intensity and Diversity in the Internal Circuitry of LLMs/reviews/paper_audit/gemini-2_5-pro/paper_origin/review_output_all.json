{
  "baseline_review": "1) Summary\nThis paper investigates the internal mechanisms through which reinforcement learning (RL) fine-tuning enhances the capabilities of large language models (LLMs). The authors employ an Edge Attribution Patching (EAP) framework to analyze the internal information flow of four pairs of LLMs before and after RL-based post-training on mathematical reasoning tasks. The core contributions are the identification of two consistent effects of online RL (PPO, GRPO): an increase in the overall activation intensity of internal pathways and a greater diversity in activation patterns, measured by information complexity and distribution kurtosis. These changes suggest RL reshapes the model's internal circuitry to be more flexible. Notably, the study finds that Direct Preference Optimization (DPO) does not consistently produce these effects, highlighting a mechanistic difference between online RL and offline preference-based methods.2) Strengths\n*   **Comprehensive and Diverse Experimental Setup:** The study's conclusions are supported by experiments across a varied set of models and training paradigms, which strengthens the generality of the findings.\n    *   The analysis includes four distinct model families (DeepSeek-Math, Mistral, Distilled-Qwen, Qwen2.5), each with different architectural characteristics (Appendix A, Table 2). This diversity helps to demonstrate that the observed effects are not specific to a single architecture.\n    *   The paper examines models trained with three different RL algorithms (GRPO, PPO, and DPO), allowing for a nuanced comparison between online RL methods and offline preference optimization (Section 4.1).\n    *   The evaluation is conducted on three different mathematical reasoning datasets (MATH, College Math, GSM8K), ensuring the results are not an artifact of a single data distribution (Table 1).*   **Well-Motivated and Computationally Sound Methodology:** The paper adopts a principled and efficient method for analyzing the internal workings of LLMs, clearly justifying its choices.\n    *   The use of the Edge Attribution Patching (EAP) framework is well-motivated by its computational efficiency compared to alternative ablation-based methods like ACDC, making the analysis of large models tractable (Section 3.2, Figure 1c).\n    *   The three proposed metrics—Activation Intensity, Information Complexity, and Distribution Kurtosis—are clearly defined (Equations 14-16) and directly map to the central hypotheses regarding the strength and diversity of internal activations.\n    *   The sample selection and truncation procedure described in Section 3.3 is a thoughtful step to ensure fair comparisons between models by controlling for factors like answer correctness and sequence length (Equations 10-12).*   **Novel and Insightful Findings on RL's Internal Effects:** The paper provides a clear and unified perspective on how online RL alters LLM internals, supported by consistent empirical evidence.\n    *   The primary findings—that online RL increases activation intensity and diversity—are consistently observed across the DeepSeek-Math, Mistral, and Distilled-Qwen model pairs (Table 1, Section 4.3). This consistency suggests a fundamental mechanism at play.\n    *   The identification of DPO as a negative case is a significant strength. By showing that Qwen2.5+DPO does not exhibit the same trends, the paper provides stronger evidence that the observed effects are specific to online RL algorithms like PPO and GRPO (Section 4.3, Table 1).\n    *   The interpretation that these changes lead to more \"redundant and more flexible\" information flow provides a plausible mechanistic explanation for the improved generalization capabilities often observed in RL-tuned models (Section 1, Conclusion).*   **High Potential for Reproducibility:** The authors have made a commendable effort to ensure their work can be reproduced and built upon by the community.\n    *   The paper provides links to all open-source models and datasets used in the experiments (Section 4.1, Appendix A).\n    *   A dedicated appendix provides specific implementation details, including system specifications and software versions (Appendix B, Table 3).\n    *   The authors state that their code is open-sourced and provide an anonymous link, which is a best practice for top-tier venues (Abstract, Reproducibility Statement).3) Weaknesses\n*   **Limited Scope of Analysis and Potential for Task-Specific Bias:** The study's conclusions are drawn exclusively from experiments on mathematical reasoning tasks, which may not generalize to other domains where RL fine-tuning is applied.\n    *   All three datasets used for analysis (MATH, GSM8K, College Math) belong to the mathematical problem-solving domain (Section 4.1). Mathematical reasoning is a highly structured task that may uniquely benefit from or induce the observed increases in activation intensity and diversity.\n    *   It is unclear whether these internal changes would be observed for models fine-tuned on other common tasks like creative writing, dialogue generation, or summarization. The paper's claims about a \"unified view of how RL fine-tuning systematically alters the internal circuitry\" (Abstract) may be overstated without broader task coverage.\n    *   No direct evidence found in the manuscript to suggest the analysis was performed on non-mathematical tasks.*   **Asserted Causality and Ambiguous Interpretation of Metrics:** The paper establishes a correlation between online RL fine-tuning and changes in internal metrics but does not provide causal evidence linking these changes to improved model performance.\n    *   The paper suggests that increased activation intensity and diversity \"may explain its advantage in generalization\" (Abstract) and \"may underlie its superior generalization ability\" (Conclusion), but this link is interpretive rather than experimentally verified.\n    *   The interpretation of the metrics is not critically examined. For instance, higher \"Information Complexity\" (entropy) could alternatively be interpreted as noisier or less efficient information processing, rather than strictly \"more flexible\" flow. The paper does not explore this alternative.\n    *   The analysis does not show a correlation between the magnitude of the metric changes (Table 1) and the magnitude of performance gains on downstream tasks (Appendix C, Table 4). For example, Distilled-Qwen shows massive performance gains but relatively modest changes in some metrics compared to DeepSeek-Math.*   **Poor Clarity of Key Visualizations and Tables:** Several key figures and tables that present the main results are difficult to interpret, hindering the effective communication of the findings.\n    *   Figure 2, a 3D surface plot showing relative change in activation, is visually cluttered and lacks clear axes or reference points, making it difficult to extract quantitative insights. A 2D heatmap or aggregated statistics would likely be more effective.\n    *   Figure 3 is presented in a highly unconventional and unprofessional format. Figure 3(a) contains blurry, low-resolution text, and Figure 3(b) is formatted as a raw text table rather than a proper plot. The much clearer bar plot version of this data is relegated to the conclusion (Block 33), but should be the primary figure in the results section.\n    *   Table 1 is extremely dense, presenting results for four model pairs, three datasets, three metrics, and four hyperparameter settings in a single block. This makes it challenging to parse and identify trends, despite the helpful arrows. The presence of missing values due to memory overflow also raises questions about the analysis's robustness for longer sequences.*   **Potential Confounding Factors in Model Comparisons:** The experimental design does not fully control for all variables between the base SFT models and their RL-tuned counterparts, which could confound the interpretation of the results.\n    *   The model pairs are sourced from different providers and may have been trained with different datasets, hyperparameters, and engineering details beyond just the application of a specific RL algorithm. For example, the \"Distilled-Qwen\" pair involves a base model distilled from one large model and an RL model fine-tuned on \"curated math and code datasets\" (Section 4.1), introducing uncontrolled variables.\n    *   The paper states that `AceReason-Nemotron-7B` starts from the same checkpoint as `DeepSeek-R1-Distill-Qwen-7B` (Section 4.1), but does not provide details on the RL training data or other hyperparameters, making it difficult to isolate the effect of the GRPO algorithm itself.\n    *   Without a perfectly controlled setup (e.g., using the exact same reward model, data, and hyperparameters for PPO, GRPO, and DPO on a single base model), it is difficult to attribute the observed differences solely to the choice of RL algorithm.4) Suggestions for Improvement\n*   **Broaden the Scope of Analysis:** To validate the generality of the claims, the analysis should be extended to include LLMs fine-tuned on a more diverse set of tasks.\n    *   Apply the same EAP-based analysis to model pairs fine-tuned on non-mathematical domains, such as general instruction-following (e.g., using the Alpaca dataset) or code generation (e.g., CodeLlama models).\n    *   If the same trends hold across different task domains, it would significantly strengthen the paper's central claim of providing a \"unified view.\" If not, a discussion of the task-dependent nature of these internal changes would be a valuable contribution.\n    *   This would directly address the concern that the findings might be an artifact of the mathematical reasoning domain.*   **Strengthen the Link Between Internal Changes and Performance:** The paper should include analysis that more directly connects the observed internal phenomena to the external behavior of the models.\n    *   Provide a correlational analysis between the magnitude of change in the proposed metrics (Activation Intensity, etc.) and the performance improvements reported in Appendix C, Table 4. A strong positive correlation would provide more compelling evidence for the functional importance of these internal changes.\n    *   Discuss the alternative interpretations of the metrics. For example, add a paragraph considering whether high entropy could ever be detrimental and under what circumstances.\n    *   As a more exploratory direction, consider a simple intervention. For example, does the difference in activation patterns between correct and incorrect answers from the same model align with the observed trends (e.g., are correct answers associated with higher intensity/diversity)?*   **Improve Clarity of Visualizations and Tables:** The key results should be re-visualized to be clearer, more professional, and easier to interpret.\n    *   Replace the 3D plot in Figure 2 with a 2D heatmap, perhaps aggregated by layer or module type (e.g., attention vs. FFN), to provide a more structured and quantitative view of where activations are strengthening.\n    *   Re-create Figure 3 using a standard plotting library. The scatter plot from Figure 3(a) should be rendered in high resolution with clear labels. The data from Figure 3(b) should be presented as a proper bar chart, similar to the one shown in the conclusion (Block 33), and placed within the main results section.\n    *   To improve the readability of Table 1, consider splitting it into multiple smaller tables (e.g., one per metric) or using color-coding to highlight the increase/decrease patterns more effectively. Also, add a brief discussion on the implications of the missing data points caused by memory overflow.*   **Acknowledge and Discuss Potential Confounding Factors:** The manuscript should be more transparent about the limitations of its experimental setup and discuss potential confounding variables.\n    *   Add a paragraph in the experimental setup (Section 4.1) or a dedicated limitations section acknowledging that the model pairs are not from perfectly controlled experiments.\n    *   Explicitly list the known differences between the training of the SFT and RL models beyond the core algorithm (e.g., different data mixes, reward models, or training durations) and discuss how these factors might have influenced the results.\n    *   This would add nuance to the claims and demonstrate a more critical evaluation of the experimental design, thereby increasing the credibility of the findings that can be confidently attributed to the RL algorithms themselves.5) Score\n*   Overall (10): 7 — The paper tackles an important problem with a solid methodology and presents interesting, consistent findings, but the analysis is limited to a single domain and the presentation of results needs significant improvement.\n*   Novelty (10): 7 — Applying EAP to systematically compare SFT vs. online RL vs. DPO models and identifying the distinct internal signatures of activation intensity and diversity is a novel contribution.\n*   Technical Quality (10): 7 — The experimental framework is technically sound, but the lack of controlled comparisons for model pairs introduces potential confounds, and the causal link to performance is not established.\n*   Clarity (10): 6 — While the text is generally well-written, the core visualizations (Figure 2, Figure 3) and the main results table (Table 1) are poorly designed and obscure the findings, significantly impacting clarity.\n*   Confidence (5): 5 — I am highly confident in my review, as the topic aligns with my expertise in LLM analysis and interpretability.",
  "final_review": "1) Summary\nThis paper investigates the internal mechanisms through which reinforcement learning (RL) fine-tuning enhances the capabilities of large language models (LLMs). The authors employ an Edge Attribution Patching (EAP) framework to analyze the internal information flow of four pairs of LLMs before and after RL-based post-training on mathematical reasoning tasks. The core contributions are the identification of two consistent effects of online RL (PPO, GRPO): an increase in the overall activation intensity of internal pathways and a greater diversity in activation patterns, measured by information complexity and distribution kurtosis. These changes suggest RL reshapes the model's internal circuitry to be more flexible. Notably, the study finds that Direct Preference Optimization (DPO) does not consistently produce these effects, highlighting a mechanistic difference between online RL and offline preference-based methods.2) Strengths\n*   **Comprehensive and Diverse Experimental Setup:** The study's conclusions are supported by experiments across a varied set of models and training paradigms, which strengthens the generality of the findings.\n    *   The analysis includes four distinct model families (DeepSeek-Math, Mistral, Distilled-Qwen, Qwen2.5), each with different architectural characteristics (Appendix A, Table 2). This diversity helps to demonstrate that the observed effects are not specific to a single architecture.\n    *   The paper examines models trained with three different RL algorithms (GRPO, PPO, and DPO), allowing for a nuanced comparison between online RL methods and offline preference optimization (Section 4.1).\n    *   The evaluation is conducted on three different mathematical reasoning datasets (MATH, College Math, GSM8K), ensuring the results are not an artifact of a single data distribution (Table 1).*   **Well-Motivated and Computationally Sound Methodology:** The paper adopts a principled and efficient method for analyzing the internal workings of LLMs, clearly justifying its choices.\n    *   The use of the Edge Attribution Patching (EAP) framework is well-motivated by its computational efficiency compared to alternative ablation-based methods like ACDC, making the analysis of large models tractable (Section 3.2, Figure 1c).\n    *   The three proposed metrics—Activation Intensity, Information Complexity, and Distribution Kurtosis—are clearly defined (Equations 14-16) and directly map to the central hypotheses regarding the strength and diversity of internal activations.\n    *   The sample selection and truncation procedure described in Section 3.3 is a thoughtful step to ensure fair comparisons between models by controlling for factors like answer correctness and sequence length (Equations 10-12).*   **Novel and Insightful Findings on RL's Internal Effects:** The paper provides a clear and unified perspective on how online RL alters LLM internals, supported by consistent empirical evidence.\n    *   The primary findings—that online RL increases activation intensity and diversity—are observed across the DeepSeek-Math, Mistral, and Distilled-Qwen model pairs in many settings (Table 1, Section 4.3). This consistency suggests a fundamental mechanism may be at play.\n    *   The paper's use of DPO as a comparative case is a significant strength. By hypothesizing that DPO should not exhibit the same trends as online RL, the paper provides a falsifiable claim that helps isolate the effects of the online training process (Section 4.3). While the data does not uniformly support DPO being a clear negative control (see Weaknesses), this comparative framing is crucial for the paper's argument.\n    *   The interpretation that these changes lead to more \"redundant and more flexible\" information flow provides a plausible mechanistic explanation for the improved generalization capabilities often observed in RL-tuned models (Section 1, Conclusion).*   **High Potential for Reproducibility:** The authors have made a commendable effort to ensure their work can be reproduced and built upon by the community.\n    *   The paper provides links to all open-source models and datasets used in the experiments (Section 4.1, Appendix A).\n    *   A dedicated appendix provides specific implementation details, including system specifications and software versions (Appendix B, Table 3).\n    *   The authors state that their code is open-sourced and provide an anonymous link, which is a best practice for top-tier venues (Abstract, Reproducibility Statement).3) Weaknesses\n*   **Overstated Claims and Inconsistencies Between Text and Data:** The narrative in the results section overstates the consistency of the findings and is, in some cases, contradicted by the data presented in Table 1.\n    *   The claim that Activation Intensity and Information Complexity \"tend to increase\" with online RL (Section 4.3) is undermined by multiple counterexamples. For instance, Activation Intensity consistently *decreases* for DeepSeek-Math on the College Math dataset, and Information Complexity consistently *decreases* for Mistral on the same dataset (Table 1). These are not \"individual exceptions\" but systematic trends for certain model-dataset pairs.\n    *   The central argument that the DPO-tuned model \"fail[s] to exhibit a stable pattern\" (Section 4.3) is contradicted by the Distribution Kurtosis metric. For this metric, the DPO model shows a consistent *decrease* across all datasets and settings, a trend that is as stable as, or more stable than, the online RL models (Table 1).*   **Limited Scope of Analysis and Potential for Task-Specific Bias:** The study's conclusions are drawn exclusively from experiments on mathematical reasoning tasks, which may not generalize to other domains where RL fine-tuning is applied.\n    *   All three datasets used for analysis (MATH, GSM8K, College Math) belong to the mathematical problem-solving domain (Section 4.1). Mathematical reasoning is a highly structured task that may uniquely benefit from or induce the observed increases in activation intensity and diversity.\n    *   It is unclear whether these internal changes would be observed for models fine-tuned on other common tasks like creative writing, dialogue generation, or summarization. The paper's claims about a \"unified view of how RL fine-tuning systematically alters the internal circuitry\" (Abstract) may be overstated without broader task coverage.\n    *   No direct evidence found in the manuscript to suggest the analysis was performed on non-mathematical tasks.*   **Asserted Causality and Ambiguous Interpretation of Metrics:** The paper establishes a correlation between online RL fine-tuning and changes in internal metrics but does not provide causal evidence linking these changes to improved model performance.\n    *   The paper suggests that increased activation intensity and diversity \"may explain its advantage in generalization\" (Abstract) and \"may underlie its superior generalization ability\" (Conclusion), but this link is interpretive rather than experimentally verified.\n    *   The interpretation of the metrics is not critically examined. For instance, higher \"Information Complexity\" (entropy) could alternatively be interpreted as noisier or less efficient information processing, rather than strictly \"more flexible\" flow. The paper does not explore this alternative.\n    *   The analysis does not show a correlation between the magnitude of the metric changes (Table 1) and the magnitude of performance gains on downstream tasks (Appendix C, Table 4). For example, Distilled-Qwen shows massive performance gains but relatively modest changes in some metrics compared to DeepSeek-Math.*   **Poor Clarity of Key Visualizations and Tables:** Several key figures and tables that present the main results are difficult to interpret, hindering the effective communication of the findings.\n    *   Figure 2, a 3D surface plot showing relative change in activation, is visually cluttered and lacks clear axes or reference points, making it difficult to extract quantitative insights. A 2D heatmap or aggregated statistics would likely be more effective.\n    *   Figure 3, which supports the paper's second main conclusion, is difficult to interpret. The scatter plot and bar chart in the main text (Figure 3) are presented in a low-resolution, unconventional format (e.g., a raw text table for panel b). A much clearer version of these plots appears in the conclusion, but should be in the main results section.\n    *   The metrics used in Figure 3, \"Inter-Sample Diversity\" and \"Output Entropy Diversity Score\", are not formally defined in the methodology (Section 4.2). While Section 4.3 provides brief textual descriptions, the lack of formal definitions with equations makes the results in this key figure difficult to verify and interpret.\n    *   Table 1 is extremely dense, presenting results for four model pairs, three datasets, three metrics, and four hyperparameter settings in a single block. This makes it challenging to parse and identify trends.*   **Potential Confounding Factors in Model Comparisons:** The experimental design does not fully control for all variables between the base SFT models and their RL-tuned counterparts, which could confound the interpretation of the results.\n    *   The model pairs are sourced from different providers and may have been trained with different datasets, hyperparameters, and engineering details beyond just the application of a specific RL algorithm. For example, the \"Distilled-Qwen\" pair involves a base model distilled from one large model and an RL model fine-tuned on \"curated math and code datasets\" (Section 4.1), introducing uncontrolled variables.\n    *   The paper states that `AceReason-Nemotron-7B` starts from the same checkpoint as `DeepSeek-R1-Distill-Qwen-7B` (Section 4.1), but does not provide details on the RL training data or other hyperparameters, making it difficult to isolate the effect of the GRPO algorithm itself.\n    *   Without a perfectly controlled setup (e.g., using the exact same reward model, data, and hyperparameters for PPO, GRPO, and DPO on a single base model), it is difficult to attribute the observed differences solely to the choice of RL algorithm.4) Suggestions for Improvement\n*   **Reconcile Claims with Data and Moderate Conclusions:** The narrative should be revised to more accurately reflect the empirical results.\n    *   In Section 4.3, explicitly acknowledge and discuss the cases where the metrics for online RL models move in the opposite direction of the main claim (e.g., the decreases in Activation Intensity and Information Complexity). This would provide a more balanced and credible analysis.\n    *   Revise the characterization of the DPO model. Acknowledge that it shows a consistent trend for Distribution Kurtosis and provide a more nuanced discussion of how its internal signature differs from online RL, rather than claiming a general lack of stable patterns.*   **Broaden the Scope of Analysis:** To validate the generality of the claims, the analysis should be extended to include LLMs fine-tuned on a more diverse set of tasks.\n    *   Apply the same EAP-based analysis to model pairs fine-tuned on non-mathematical domains, such as general instruction-following (e.g., using the Alpaca dataset) or code generation (e.g., CodeLlama models).\n    *   If the same trends hold across different task domains, it would significantly strengthen the paper's central claim of providing a \"unified view.\" If not, a discussion of the task-dependent nature of these internal changes would be a valuable contribution.\n    *   This would directly address the concern that the findings might be an artifact of the mathematical reasoning domain.*   **Strengthen the Link Between Internal Changes and Performance:** The paper should include analysis that more directly connects the observed internal phenomena to the external behavior of the models.\n    *   Provide a correlational analysis between the magnitude of change in the proposed metrics (Activation Intensity, etc.) and the performance improvements reported in Appendix C, Table 4. A strong positive correlation would provide more compelling evidence for the functional importance of these internal changes.\n    *   Discuss the alternative interpretations of the metrics. For example, add a paragraph considering whether high entropy could ever be detrimental and under what circumstances.\n    *   As a more exploratory direction, consider a simple intervention. For example, does the difference in activation patterns between correct and incorrect answers from the same model align with the observed trends (e.g., are correct answers associated with higher intensity/diversity)?*   **Improve Clarity of Visualizations and Tables:** The key results should be re-visualized to be clearer, more professional, and easier to interpret.\n    *   Replace the 3D plot in Figure 2 with a 2D heatmap, perhaps aggregated by layer or module type (e.g., attention vs. FFN), to provide a more structured and quantitative view of where activations are strengthening.\n    *   Re-create Figure 3 using a standard plotting library and move the high-quality versions from the conclusion into the main results section. The scatter plot and bar chart should be rendered with clear labels and professional formatting.\n    *   Formally define the metrics 'Inter-Sample Diversity' and 'Output Entropy Diversity Score' with equations in the methodology section (Section 4.2) to ensure the results in Figure 3 are verifiable.\n    *   To improve the readability of Table 1, consider splitting it into multiple smaller tables (e.g., one per metric) or using color-coding to highlight the increase/decrease patterns more effectively.*   **Acknowledge and Discuss Potential Confounding Factors:** The manuscript should be more transparent about the limitations of its experimental setup and discuss potential confounding variables.\n    *   Add a paragraph in the experimental setup (Section 4.1) or a dedicated limitations section acknowledging that the model pairs are not from perfectly controlled experiments.\n    *   Explicitly list the known differences between the training of the SFT and RL models beyond the core algorithm (e.g., different data mixes, reward models, or training durations) and discuss how these factors might have influenced the results.\n    *   This would add nuance to the claims and demonstrate a more critical evaluation of the experimental design, thereby increasing the credibility of the findings that can be confidently attributed to the RL algorithms themselves.5) Score\n*   Overall (10): 6 — The paper tackles an important problem with a solid methodology, but the main conclusions are overstated and not fully supported by the data in Table 1, and key results are presented unclearly.\n*   Novelty (10): 7 — Applying EAP to systematically compare SFT vs. online RL vs. DPO models and identifying their potential mechanistic differences is a novel and valuable contribution.\n*   Technical Quality (10): 5 — The analysis suffers from significant inconsistencies between the narrative claims and the quantitative results (Table 1), and the causal link to performance is not established.\n*   Clarity (10): 5 — Core visualizations are poorly designed (Figure 2, Figure 3), the main results table is dense (Table 1), and key metrics for Figure 3 are not formally defined, significantly hindering comprehension.\n*   Confidence (5): 5 — I am highly confident in my review, as the topic aligns with my expertise in LLM analysis and interpretability.",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 7,
        "novelty": 7,
        "technical_quality": 7,
        "clarity": 6,
        "confidence": 5
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 6,
        "novelty": 7,
        "technical_quality": 5,
        "clarity": 5,
        "confidence": 5
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "1) Summary\nThis paper investigates the internal mechanisms through which reinforcement learning (RL) fine-tuning enhances the capabilities of large language models (LLMs). The authors employ an Edge Attribution Patching (EAP) framework to analyze the internal information flow of four pairs of LLMs before and after RL-based post-training on mathematical reasoning tasks. The core contributions are the identification of two consistent effects of online RL (PPO, GRPO): an increase in the overall activation intensity of internal pathways and a greater diversity in activation patterns, measured by information complexity and distribution kurtosis. These changes suggest RL reshapes the model's internal circuitry to be more flexible. Notably, the study finds that Direct Preference Optimization (DPO) does not consistently produce these effects, highlighting a mechanistic difference between online RL and offline preference-based methods.2) Strengths\n*   **Comprehensive and Diverse Experimental Setup:** The study's conclusions are supported by experiments across a varied set of models and training paradigms, which strengthens the generality of the findings.\n    *   The analysis includes four distinct model families (DeepSeek-Math, Mistral, Distilled-Qwen, Qwen2.5), each with different architectural characteristics (Appendix A, Table 2). This diversity helps to demonstrate that the observed effects are not specific to a single architecture.\n    *   The paper examines models trained with three different RL algorithms (GRPO, PPO, and DPO), allowing for a nuanced comparison between online RL methods and offline preference optimization (Section 4.1).\n    *   The evaluation is conducted on three different mathematical reasoning datasets (MATH, College Math, GSM8K), ensuring the results are not an artifact of a single data distribution (Table 1).*   **Well-Motivated and Computationally Sound Methodology:** The paper adopts a principled and efficient method for analyzing the internal workings of LLMs, clearly justifying its choices.\n    *   The use of the Edge Attribution Patching (EAP) framework is well-motivated by its computational efficiency compared to alternative ablation-based methods like ACDC, making the analysis of large models tractable (Section 3.2, Figure 1c).\n    *   The three proposed metrics—Activation Intensity, Information Complexity, and Distribution Kurtosis—are clearly defined (Equations 14-16) and directly map to the central hypotheses regarding the strength and diversity of internal activations.\n    *   The sample selection and truncation procedure described in Section 3.3 is a thoughtful step to ensure fair comparisons between models by controlling for factors like answer correctness and sequence length (Equations 10-12).*   **Novel and Insightful Findings on RL's Internal Effects:** The paper provides a clear and unified perspective on how online RL alters LLM internals, supported by consistent empirical evidence.\n    *   The primary findings—that online RL increases activation intensity and diversity—are observed across the DeepSeek-Math, Mistral, and Distilled-Qwen model pairs in many settings (Table 1, Section 4.3). This consistency suggests a fundamental mechanism may be at play.\n    *   The paper's use of DPO as a comparative case is a significant strength. By hypothesizing that DPO should not exhibit the same trends as online RL, the paper provides a falsifiable claim that helps isolate the effects of the online training process (Section 4.3). While the data does not uniformly support DPO being a clear negative control (see Weaknesses), this comparative framing is crucial for the paper's argument.\n    *   The interpretation that these changes lead to more \"redundant and more flexible\" information flow provides a plausible mechanistic explanation for the improved generalization capabilities often observed in RL-tuned models (Section 1, Conclusion).*   **High Potential for Reproducibility:** The authors have made a commendable effort to ensure their work can be reproduced and built upon by the community.\n    *   The paper provides links to all open-source models and datasets used in the experiments (Section 4.1, Appendix A).\n    *   A dedicated appendix provides specific implementation details, including system specifications and software versions (Appendix B, Table 3).\n    *   The authors state that their code is open-sourced and provide an anonymous link, which is a best practice for top-tier venues (Abstract, Reproducibility Statement).3) Weaknesses\n*   **Overstated Claims and Inconsistencies Between Text and Data:** The narrative in the results section overstates the consistency of the findings and is, in some cases, contradicted by the data presented in Table 1.\n    *   The claim that Activation Intensity and Information Complexity \"tend to increase\" with online RL (Section 4.3) is undermined by multiple counterexamples. For instance, Activation Intensity consistently *decreases* for DeepSeek-Math on the College Math dataset, and Information Complexity consistently *decreases* for Mistral on the same dataset (Table 1). These are not \"individual exceptions\" but systematic trends for certain model-dataset pairs.\n    *   The central argument that the DPO-tuned model \"fail[s] to exhibit a stable pattern\" (Section 4.3) is contradicted by the Distribution Kurtosis metric. For this metric, the DPO model shows a consistent *decrease* across all datasets and settings, a trend that is as stable as, or more stable than, the online RL models (Table 1).*   **Limited Scope of Analysis and Potential for Task-Specific Bias:** The study's conclusions are drawn exclusively from experiments on mathematical reasoning tasks, which may not generalize to other domains where RL fine-tuning is applied.\n    *   All three datasets used for analysis (MATH, GSM8K, College Math) belong to the mathematical problem-solving domain (Section 4.1). Mathematical reasoning is a highly structured task that may uniquely benefit from or induce the observed increases in activation intensity and diversity.\n    *   It is unclear whether these internal changes would be observed for models fine-tuned on other common tasks like creative writing, dialogue generation, or summarization. The paper's claims about a \"unified view of how RL fine-tuning systematically alters the internal circuitry\" (Abstract) may be overstated without broader task coverage.\n    *   No direct evidence found in the manuscript to suggest the analysis was performed on non-mathematical tasks.*   **Asserted Causality and Ambiguous Interpretation of Metrics:** The paper establishes a correlation between online RL fine-tuning and changes in internal metrics but does not provide causal evidence linking these changes to improved model performance.\n    *   The paper suggests that increased activation intensity and diversity \"may explain its advantage in generalization\" (Abstract) and \"may underlie its superior generalization ability\" (Conclusion), but this link is interpretive rather than experimentally verified.\n    *   The interpretation of the metrics is not critically examined. For instance, higher \"Information Complexity\" (entropy) could alternatively be interpreted as noisier or less efficient information processing, rather than strictly \"more flexible\" flow. The paper does not explore this alternative.\n    *   The analysis does not show a correlation between the magnitude of the metric changes (Table 1) and the magnitude of performance gains on downstream tasks (Appendix C, Table 4). For example, Distilled-Qwen shows massive performance gains but relatively modest changes in some metrics compared to DeepSeek-Math.*   **Poor Clarity of Key Visualizations and Tables:** Several key figures and tables that present the main results are difficult to interpret, hindering the effective communication of the findings.\n    *   Figure 2, a 3D surface plot showing relative change in activation, is visually cluttered and lacks clear axes or reference points, making it difficult to extract quantitative insights. A 2D heatmap or aggregated statistics would likely be more effective.\n    *   Figure 3, which supports the paper's second main conclusion, is difficult to interpret. The scatter plot and bar chart in the main text (Figure 3) are presented in a low-resolution, unconventional format (e.g., a raw text table for panel b). A much clearer version of these plots appears in the conclusion, but should be in the main results section.\n    *   The metrics used in Figure 3, \"Inter-Sample Diversity\" and \"Output Entropy Diversity Score\", are not formally defined in the methodology (Section 4.2). While Section 4.3 provides brief textual descriptions, the lack of formal definitions with equations makes the results in this key figure difficult to verify and interpret.\n    *   Table 1 is extremely dense, presenting results for four model pairs, three datasets, three metrics, and four hyperparameter settings in a single block. This makes it challenging to parse and identify trends.*   **Potential Confounding Factors in Model Comparisons:** The experimental design does not fully control for all variables between the base SFT models and their RL-tuned counterparts, which could confound the interpretation of the results.\n    *   The model pairs are sourced from different providers and may have been trained with different datasets, hyperparameters, and engineering details beyond just the application of a specific RL algorithm. For example, the \"Distilled-Qwen\" pair involves a base model distilled from one large model and an RL model fine-tuned on \"curated math and code datasets\" (Section 4.1), introducing uncontrolled variables.\n    *   The paper states that `AceReason-Nemotron-7B` starts from the same checkpoint as `DeepSeek-R1-Distill-Qwen-7B` (Section 4.1), but does not provide details on the RL training data or other hyperparameters, making it difficult to isolate the effect of the GRPO algorithm itself.\n    *   Without a perfectly controlled setup (e.g., using the exact same reward model, data, and hyperparameters for PPO, GRPO, and DPO on a single base model), it is difficult to attribute the observed differences solely to the choice of RL algorithm.4) Suggestions for Improvement\n*   **Reconcile Claims with Data and Moderate Conclusions:** The narrative should be revised to more accurately reflect the empirical results.\n    *   In Section 4.3, explicitly acknowledge and discuss the cases where the metrics for online RL models move in the opposite direction of the main claim (e.g., the decreases in Activation Intensity and Information Complexity). This would provide a more balanced and credible analysis.\n    *   Revise the characterization of the DPO model. Acknowledge that it shows a consistent trend for Distribution Kurtosis and provide a more nuanced discussion of how its internal signature differs from online RL, rather than claiming a general lack of stable patterns.*   **Broaden the Scope of Analysis:** To validate the generality of the claims, the analysis should be extended to include LLMs fine-tuned on a more diverse set of tasks.\n    *   Apply the same EAP-based analysis to model pairs fine-tuned on non-mathematical domains, such as general instruction-following (e.g., using the Alpaca dataset) or code generation (e.g., CodeLlama models).\n    *   If the same trends hold across different task domains, it would significantly strengthen the paper's central claim of providing a \"unified view.\" If not, a discussion of the task-dependent nature of these internal changes would be a valuable contribution.\n    *   This would directly address the concern that the findings might be an artifact of the mathematical reasoning domain.*   **Strengthen the Link Between Internal Changes and Performance:** The paper should include analysis that more directly connects the observed internal phenomena to the external behavior of the models.\n    *   Provide a correlational analysis between the magnitude of change in the proposed metrics (Activation Intensity, etc.) and the performance improvements reported in Appendix C, Table 4. A strong positive correlation would provide more compelling evidence for the functional importance of these internal changes.\n    *   Discuss the alternative interpretations of the metrics. For example, add a paragraph considering whether high entropy could ever be detrimental and under what circumstances.\n    *   As a more exploratory direction, consider a simple intervention. For example, does the difference in activation patterns between correct and incorrect answers from the same model align with the observed trends (e.g., are correct answers associated with higher intensity/diversity)?*   **Improve Clarity of Visualizations and Tables:** The key results should be re-visualized to be clearer, more professional, and easier to interpret.\n    *   Replace the 3D plot in Figure 2 with a 2D heatmap, perhaps aggregated by layer or module type (e.g., attention vs. FFN), to provide a more structured and quantitative view of where activations are strengthening.\n    *   Re-create Figure 3 using a standard plotting library and move the high-quality versions from the conclusion into the main results section. The scatter plot and bar chart should be rendered with clear labels and professional formatting.\n    *   Formally define the metrics 'Inter-Sample Diversity' and 'Output Entropy Diversity Score' with equations in the methodology section (Section 4.2) to ensure the results in Figure 3 are verifiable.\n    *   To improve the readability of Table 1, consider splitting it into multiple smaller tables (e.g., one per metric) or using color-coding to highlight the increase/decrease patterns more effectively.*   **Acknowledge and Discuss Potential Confounding Factors:** The manuscript should be more transparent about the limitations of its experimental setup and discuss potential confounding variables.\n    *   Add a paragraph in the experimental setup (Section 4.1) or a dedicated limitations section acknowledging that the model pairs are not from perfectly controlled experiments.\n    *   Explicitly list the known differences between the training of the SFT and RL models beyond the core algorithm (e.g., different data mixes, reward models, or training durations) and discuss how these factors might have influenced the results.\n    *   This would add nuance to the claims and demonstrate a more critical evaluation of the experimental design, thereby increasing the credibility of the findings that can be confidently attributed to the RL algorithms themselves.5) Score\n*   Overall (10): 6 — The paper tackles an important problem with a solid methodology, but the main conclusions are overstated and not fully supported by the data in Table 1, and key results are presented unclearly.\n*   Novelty (10): 7 — Applying EAP to systematically compare SFT vs. online RL vs. DPO models and identifying their potential mechanistic differences is a novel and valuable contribution.\n*   Technical Quality (10): 5 — The analysis suffers from significant inconsistencies between the narrative claims and the quantitative results (Table 1), and the causal link to performance is not established.\n*   Clarity (10): 5 — Core visualizations are poorly designed (Figure 2, Figure 3), the main results table is dense (Table 1), and key metrics for Figure 3 are not formally defined, significantly hindering comprehension.\n*   Confidence (5): 5 — I am highly confident in my review, as the topic aligns with my expertise in LLM analysis and interpretability."
}