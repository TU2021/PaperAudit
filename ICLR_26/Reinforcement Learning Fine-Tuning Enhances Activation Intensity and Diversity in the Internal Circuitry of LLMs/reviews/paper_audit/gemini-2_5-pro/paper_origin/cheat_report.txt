**Integrity and Consistency Review Report**

This report details significant internal inconsistencies and methodological issues identified in the manuscript "REINFORCEMENT LEARNING FINE-TUNING ENHANCES ACTIVATION INTENSITY AND DIVERSITY IN THE INTERNAL CIRCUITRY OF LLMs". The findings below raise concerns about the validity of the paper's central claims.

**Summary of Key Issues:**

The manuscript exhibits critical inconsistencies between the quantitative results presented in its main table and the narrative claims made in the text. Specifically, the core arguments about the effects of online reinforcement learning (RL) are frequently contradicted by the paper's own data. Furthermore, a key results figure relies on metrics that are not defined in the methodology, rendering its conclusions unverifiable.

---

**1. Contradiction Between Main Claims and Tabled Results**

The central claims of the paper are that online RL fine-tuning (using PPO/GRPO) leads to a consistent increase in "Activation Intensity" and "Information Complexity," and a decrease in "Distribution Kurtosis." The text in Section 4.3 ("Results and Analysis") states these trends are "largely consistent" across the Deepseek-Math, Mistral, and Distilled-Qwen models, dismissing counter-examples as "individual exceptions." However, a detailed review of Table 1 reveals that these exceptions are numerous and systematic, undermining the claims of consistency.

*   **Claim vs. Evidence (Activation Intensity):** The paper claims Activation Intensity "tend[s] to increase" (Section 4.3).
    *   **Contradiction:** For the **DeepSeek-Math** model, Activation Intensity *decreases* after RL tuning in multiple instances:
        *   On the College Math dataset, it decreases for α settings 0.03, 0.1, and 0.3 (Table 1).
        *   On the GSM8K dataset, it decreases for the α = 0.03 setting (Table 1).
    *   This pattern of decrease in 4 out of 12 reported cases for DeepSeek-Math is more than an "individual exception" and challenges the robustness of the primary finding.

*   **Claim vs. Evidence (Information Complexity):** The paper claims Information Complexity "tend[s] to increase" (Section 4.3).
    *   **Contradiction:** For the **Mistral** model, Information Complexity frequently *decreases* after RL tuning:
        *   On the College Math dataset, it decreases for α settings 0.03, 0.1, and 0.3 (Table 1).
        *   On the MATH and GSM8K datasets, it decreases for the α = 0.03 setting (Table 1).
    *   The consistent decrease on the College Math dataset for Mistral directly contradicts the paper's general conclusion.

**2. Contradictory Evidence Regarding the DPO Model as an Outlier**

A core part of the paper's narrative is that the DPO-tuned model (Qwen2.5) behaves differently and "fail[s] to exhibit a stable pattern" (Section 4.3), unlike the online RL models. The data for one of the paper's own metrics directly contradicts this assertion.

*   **Claim vs. Evidence (Distribution Kurtosis):** The paper claims DPO is inconsistent, while online RL models show a decrease in Distribution Kurtosis.
    *   **Contradiction:** In Table 1, the **Qwen2.5 (DPO)** model shows a consistent *decrease* in Distribution Kurtosis across all datasets and hyperparameter settings. This trend is just as stable, if not more so, than that of the online RL models it is contrasted with.
    *   This finding directly undermines the central argument that DPO's internal effects are fundamentally different or less consistent than PPO/GRPO, at least as measured by this metric.

**3. Use of Undefined Metrics in a Key Results Figure**

Figure 3 is presented to provide visual evidence for the paper's claims about increased activation diversity. However, the figure relies entirely on two metrics that are not defined or mentioned anywhere in the methodology (Section 4.2).

*   **Issue:** Figure 3a plots "RL Model Inter-Sample Diversity" vs. "Base Model Inter-Sample Diversity," and Figure 3b plots "Output Entropy Diversity Score."
*   **Evidence:** The methodology in Section 4.2 defines only three metrics: "Activation Intensity," "Information Complexity," and "Distribution Kurtosis." There is no description of how "Inter-Sample Diversity" or "Output Entropy Diversity Score" are calculated.
*   **Impact:** Without a methodological definition, the results presented in Figure 3 are uninterpretable and cannot be scientifically validated. As this figure is used to substantiate the paper's second main conclusion ("Online RL fine-tuning diversifies the activation patterns"), this omission represents a critical methodological flaw.

**4. Potentially Misleading "Representative Case" Visualization**

Figure 2 is presented as a "representative case" (Section 4.3) to illustrate that RL fine-tuning strengthens internal connections. While the figure accurately reflects a single data point from Table 1 (Mistral on MATH with α = 0.5), its characterization as "representative" is questionable.

*   **Issue:** The visualization shows a clear increase in activation strength.
*   **Context:** As noted in Point 1, Table 1 contains multiple instances where activation strength *decreases*. By selecting one of the positive cases and labeling it "representative," the authors may be presenting a biased view of the data that obscures the inconsistencies in their results.

**Conclusion**

The manuscript contains significant internal contradictions between its claims, data, and methodology. The conclusions about the consistent effects of online RL are not fully supported by the provided results in Table 1. The central argument that DPO is a clear outlier is undermined by the paper's own data on distribution kurtosis. Finally, the use of undefined metrics in a key figure makes a substantial portion of the evidence unverifiable. These issues materially affect the trustworthiness and scientific validity of the paper's findings.