{
  "baseline_review": "Summary\n- The paper investigates how reinforcement learning (RL) post-training alters the internal information pathways of large language models (LLMs). Building on Edge Attribution Patching (EAP) (Section 3; Eq. 9; Figure 1c), the authors compute edge-level importance using gradients of a self-entropy loss on truncated generations (Section 3.3; Eq. 13), and summarize edge-weight distributions with three metrics: Activation Intensity, Information Complexity (entropy), and Distribution Kurtosis (Section 4.2; Eqs. 14–16). Across four 7B model pairs and three math benchmarks (Section 4.1; Table 1), they report two robust internal effects of online RL (PPO/GRPO): increased activation intensity and increased diversity (higher entropy, lower kurtosis). They also observe that DPO-trained Qwen2.5 deviates from these trends (Section 4.3; Table 1; Figure 3). The paper provides a unified post-training gradient view (Eq. 4), visualizations (Figures 2–3), and open-source code (Abstract; Appendix B).Strengths\n- Boldly unified formalization of post-training updates\n  • Evidence: The gradient-based unification of SFT and RL is formalized in Eq. 4 (Section 2.2), with clear identification of data source D, reward/reference π_ref, and token-level gradient coefficients GC_A. Why it matters: Offers a clean mathematical lens for comparing post-training methods, improving clarity and interpretability of methodological differences (novelty/clarity).\n  • Evidence: The discussion contrasting online RL (PPO/GRPO) with DPO uses the unified view (Section 4.3) to explain static vs. dynamic data generation. Why it matters: Helps connect internal activation changes to training paradigms (impact/interpretability).\n  • Evidence: The introduction situates the work within recent RL post-training advances (Section 1), grounding the unified view in contemporary literature surveyed in Section 2.2 and References. Why it matters: Contextualizes contributions in a fast-evolving space (clarity/impact).- Clear graph-theoretic view of residual computation and edge attribution\n  • Evidence: The residual-stream formulas (Eq. 5; Section 3.1) and graph abstraction (Eqs. 6–7) map sub-modules to nodes and residual paths to edges, with a schematic in Figure 1b. Why it matters: Provides a precise internal representation that supports edge-level analysis (technical soundness/clarity).\n  • Evidence: The EAP linearization (Eq. 9; Figure 1c) is contrasted with ablation-based ACDC (Eq. 8), explaining computational tractability. Why it matters: Justifies choosing EAP for large models (scalability/technical suitability).\n  • Evidence: Section 3.2 explains how EAP can be computed for all edges with a single forward/backward pass (Section 3.2; text below Eq. 9). Why it matters: Ensures the feasibility of the proposed analysis at scale (experimental rigor).- Thoughtful data selection and truncation protocol to control confounds\n  • Evidence: Section 3.3 details filtering to questions correctly answered by both models and constraints on token length variability (Eqs. 10–12). Why it matters: Reduces confounding due to correctness and extreme lengths (experimental rigor).\n  • Evidence: Truncation to T_cut = α·T̄ and self-entropy definition (Eq. 13) explicitly control memory and maintain comparability across models (Section 3.3). Why it matters: Operationalizes a consistent and tractable attribution pipeline (clarity/reproducibility).\n  • Evidence: The authors vary α across {0.03, 0.1, 0.3, 0.5} in Table 1 and analyze robustness as α increases (Section 4.3, first paragraph; Table 1). Why it matters: Demonstrates some sensitivity analysis of truncation length (experimental rigor).- Consistent cross-model, cross-dataset evidence for online RL effects\n  • Evidence: Table 1 shows Activation Intensity increases for DeepSeek-Math (+GRPO), Mistral (+PPO), and Distilled-Qwen (+GRPO) across MATH, College Math, and GSM8K at several α values (Section 4.3; Table 1). Why it matters: Supports the claim of stronger engagement of internal pathways (technical soundness/impact).\n  • Evidence: Table 1 also shows Information Complexity increases and Distribution Kurtosis decreases for the same families and datasets, with exceptions diminishing as α grows (Section 4.3; Table 1). Why it matters: Indicates richer and less concentrated activation patterns after RL (technical soundness/impact).\n  • Evidence: Figure 2 visualizes relative changes in edge activation for Mistral on MATH at α = 0.5, showing widespread strengthening. Why it matters: Offers an intuitive view of network-level changes (clarity/impact).- Clear visualization of diversity changes and per-model patterns\n  • Evidence: Figure 3a (inter-sample diversity plot) highlights that most points lie in the RL improvement region, with a reported 95.1% improvement rate over 41 experiments (Section 4.3; Figure 3a). Why it matters: Summarizes global diversity effects across settings (experimental rigor/clarity).\n  • Evidence: Figure 3b reports large increases in output entropy for several settings (e.g., Deepseek-Math +1246%, +324%, +656%, +25%; Mistral +120%, +180%, etc.), corroborating Table 1 trends (Section 4.3; Figure 3b; Table 1). Why it matters: Reinforces the entropy-based diversity claim (impact/clarity).\n  • Evidence: Conclusion reuses visual cues (Figure 34a–33b) to emphasize the divergence of DPO-trained Qwen2.5 from PPO/GRPO trends (Section 6; Figures 33–34; Section 4.3). Why it matters: Flags algorithmic differences in internal dynamics (impact).- Reproducibility and transparency\n  • Evidence: The paper provides model sources and specifications (Appendix A; Table 2), system configuration (Appendix B; Table 3), and an anonymous code link (Abstract; Appendix B). Why it matters: Facilitates replication and validation (reproducibility).\n  • Evidence: Performance comparisons before/after post-training are summarized across multiple benchmarks in Table 4 (Appendix C). Why it matters: Establishes external competence changes alongside internal analyses (rigor/impact).\n  • Evidence: Ethics and reproducibility statements are included (Appendix; Ethics Statement; Reproducibility Statement). Why it matters: Signals responsible and transparent research practices (clarity).Weaknesses\n- Limited causal validation linking edge attribution changes to behavioral gains\n  • Evidence: The study infers that stronger and more diverse activations “may explain” generalization (Section 6; Section 4.3), but does not perform targeted causal interventions (e.g., edge-level ablation guided by EAP) to verify necessity/sufficiency. Why it matters: Without causal tests, conclusions about mechanisms remain correlational (technical rigor/impact).\n  • Evidence: No correlation analysis between metric changes (Eqs. 14–16) and task performance gains (Table 4) is reported. Why it matters: The link between internal metrics and external improvements is asserted but not quantitatively substantiated (technical rigor).\n  • Evidence: ACDC-style ablation (Eq. 8) is discussed but not used to validate EAP estimates (Section 3.2; Figure 1c). Why it matters: Lacking ground-truth checks reduces confidence in attribution fidelity (technical soundness).- Self-entropy loss and truncation choices may introduce analysis biases\n  • Evidence: Edge attribution uses self-entropy over generated tokens (Eq. 13), not target-correct labels; justification and trade-offs are brief (Section 3.3), with no ablation to compare alternative losses. Why it matters: Self-entropy can be sensitive to decoding stochasticity and may weaken interpretability of gradients (technical soundness).\n  • Evidence: The truncation length α ∈ {0.03, 0.1, 0.3, 0.5} is motivated by memory control (Section 3.3) but not systematically justified; only qualitative statements about robustness with larger α appear (Section 4.3; Table 1). Why it matters: Truncation can change which circuits are probed, affecting conclusions (experimental rigor).\n  • Evidence: Filtering to questions correctly answered by both models and enforcing length balance (Eqs. 10–12) may exclude samples where RL provides the most benefit. Why it matters: The analysis set may be biased toward easier instances, potentially underrepresenting RL’s distinctive behavior on hard cases (generalizability).- Ambiguity in edge-weight matrix construction and granularity\n  • Evidence: Section 4.2 defines W ∈ ℝ^{n × n_o × n_i} but does not specify how n_o and n_i correspond to concrete edges (e.g., per-head, per-layer, attention/FFN splits). Why it matters: Ambiguity hinders reproducibility and interpretability of metrics (clarity/technical soundness).\n  • Evidence: The graph view (Section 3.1; Eqs. 6–7) aggregates sub-modules (A^ℓ, F^ℓ) but does not clarify whether edge-level attribution is computed at head granularity or whole-branch outputs. Why it matters: Coarse aggregation may hide important fine-grained mechanisms (novelty/technical depth).\n  • Evidence: Figures 2–3 do not detail the mapping from “output/input neurons” axes to specific architectural components. Why it matters: Limits insights into which parts of the network are most affected by RL (clarity/impact).- Statistical rigor and reporting\n  • Evidence: Table 1 shows small absolute differences in Act.Intens. (e.g., Mistral MATH α=0.1: 6.76e-4→7.71e-4), but no confidence intervals, standard errors, or statistical tests are provided. Why it matters: Without uncertainty estimates, it is difficult to assess robustness (experimental rigor).\n  • Evidence: The paper acknowledges “Missing values result from GPU memory overflow” in Table 1, but does not quantify potential selection bias or remedy. Why it matters: Incomplete data may skew observations (experimental rigor).\n  • Evidence: Improvement percentages in Figure 3b are reported without variance or sample counts for each bar (e.g., +1246% on small baselines). Why it matters: Large percentage changes on near-zero baselines can be misleading (clarity/rigor).- Decoding and generation settings are insufficiently controlled\n  • Evidence: Section 3.3 defines loss over generated tokens but does not specify decoding strategies (temperature, top-k/p, beam vs. sampling) or random seed controls. Why it matters: Attribution magnitudes and entropy can vary with decoding; lack of control weakens conclusions (experimental rigor).\n  • Evidence: No details on prompt formatting or stopping criteria beyond truncation (Section 3.3) are provided. Why it matters: Variations in prompting can materially affect internal activations (reproducibility).\n  • Evidence: No robustness checks across different decoding schemes are presented. Why it matters: It remains unclear if reported trends generalize across common inference settings (generalizability).- External confounds in model selection and training differences\n  • Evidence: The RL models are third-party releases (Appendix A; Section 4.1), potentially differing in data, hyperparameters, and reward models beyond the algorithm (PPO/GRPO). Why it matters: Attribution of observed changes to “RL” per se may be confounded (technical rigor).\n  • Evidence: The DPO-vs-online RL explanation (Section 4.3) is plausible but not tested with controlled variants (e.g., Online-DPO or different DPO datasets). Why it matters: Conclusions about DPO’s internal dynamics may not generalize without controlled comparisons (generalizability).\n  • Evidence: Distilled-Qwen starts from a distilled checkpoint with strong reasoning (Section 4.1), which may interact with RL effects differently than non-distilled baselines. Why it matters: Heterogeneous starting points complicate cross-family comparisons (clarity/rigor).- Metric design choices may obscure important signal\n  • Evidence: Information Complexity uses entropy over histogram of |W| (Eq. 15), discarding sign information. Why it matters: Signs in residual streams can encode inhibitory vs. excitatory influences; losing sign may reduce interpretability (technical soundness).\n  • Evidence: The number of bins B and ε for entropy are not specified (Section 4.2; Eq. 15). Why it matters: Entropy estimates are sensitive to binning; lack of specification hinders replication (reproducibility).\n  • Evidence: Kurtosis (Eq. 16) is interpreted as “stability” and “uniformity,” but no calibration to known distributions or sanity checks are provided. Why it matters: Misinterpretation of kurtosis can mislead conclusions (clarity/technical soundness).- Scope limited to mathematical reasoning and 7B models\n  • Evidence: Analyses use MATH, College Math, GSM8K (Section 4.1; Table 1), and performance comparisons in Appendix C are math-heavy (Table 4). Why it matters: Internal activation conclusions may be domain-specific (generalizability).\n  • Evidence: All model pairs are ≈7B (Section 4.1; Appendix A; Table 2). Why it matters: Trends may shift for smaller or larger models (impact/generalizability).\n  • Evidence: Claims on generalization to writing/coding (Section 1) are referenced but not probed internally in this work. Why it matters: The mechanistic findings are not validated beyond math (impact/generalizability).Suggestions for Improvement\n- Strengthen causal validation between internal metrics and behavior\n  • Add edge-level causal tests: Use EAP scores (Eq. 9) to identify high-importance edges, then perform ACDC-style ablations (Eq. 8; Section 3.2) to verify that removing them reduces performance on the same samples (Section 4.1; Table 1).\n  • Quantify correlations: Compute statistical associations between metric changes (Eqs. 14–16) and accuracy improvements (Appendix C; Table 4) across models/datasets/α, with confidence intervals.\n  • Include counterfactual patching: Perform activation patching guided by the discovered edges to test whether restoring pre-RL circuits reduces diversity and performance (Figure 3; Section 4.3).- Justify and stress-test the self-entropy loss and truncation protocol\n  • Compare losses: Evaluate EAP under self-entropy (Eq. 13) versus teacher-forced cross-entropy on correct targets to assess sensitivity (Section 3.3).\n  • Systematic α sweeps: Extend α beyond {0.03, 0.1, 0.3, 0.5} and report stability plots for metrics (Table 1; Section 4.3), including memory–fidelity trade-offs.\n  • Expand sample inclusion: Analyze a matched set including hard questions only RL solves (relax Eqs. 10–12) to check whether the internal trends strengthen on more challenging instances (Section 3.3).- Clarify construction and granularity of the edge-weight matrices\n  • Specify mapping: Define n_o, n_i (Section 4.2) with precise architectural correspondence (per head, per layer, attention vs. FFN), and reflect this in figure axes (Figure 2–3).\n  • Provide granularity analyses: Report per-head/per-layer metrics to reveal where RL induces the largest changes (Section 3.1; Eqs. 6–7).\n  • Add ablations: Compare branch-level (A^ℓ, F^ℓ) aggregation vs. head-level attributions to quantify resolution effects on Act.Intens./Info.Complex./Dist.Kurt. (Section 4.2).- Improve statistical rigor and reporting\n  • Report uncertainty: Add confidence intervals or standard errors for Table 1 metrics across runs/seeds, and include statistical tests (Section 4.3; Table 1).\n  • Address missing data: Quantify the impact of “GPU memory overflow” on coverage and consider gradient checkpointing to fill missing entries (Table 1).\n  • Normalize percentage reporting: For Figure 3b, include absolute baselines, sample counts, and variance to contextualize large percentage changes (Section 4.3).- Control and document decoding/generation settings\n  • Specify inference settings: Report temperature, top-k/top-p, seed, and stopping criteria alongside Section 3.3 so gradients under Eq. 13 are interpretable and reproducible.\n  • Robustness checks: Recompute metrics under multiple decoding regimes (e.g., greedy vs. sampling) to assess stability of trends (Table 1; Figure 3).\n  • Prompting controls: Standardize prompt templates and format across models, and include these details in Appendix B to ensure consistency (Appendix B; Table 3).- Reduce external confounds in model selection and training differences\n  • Controlled re-training: Where feasible, start from the same SFT checkpoint and apply standardized PPO/GRPO/DPO settings to isolate algorithmic effects (Section 4.1; Appendix A).\n  • Test DPO variants: Include Online-DPO or different preference datasets to check whether static data is the primary driver of Qwen2.5 deviations (Section 4.3; Table 1).\n  • Match baselines: Compare distilled and non-distilled baselines within the same family (Distilled-Qwen; Section 4.1) to disentangle distillation effects from RL-induced changes.- Refine metric design and reporting\n  • Preserve sign: Compute entropy over signed attributions or complement |W|-based metrics with sign-sensitive measures to capture inhibitory/excitatory dynamics (Eq. 15).\n  • Specify binning: Report B and ε for entropy (Eq. 15), and add sensitivity analyses for different bin counts to ensure robustness (Section 4.2).\n  • Calibrate kurtosis: Include sanity checks (e.g., synthetic distributions or known baselines) to validate the interpretation of Dist.Kurt. (Eq. 16), and relate kurtosis shifts to concrete distributional changes.- Broaden scope beyond math and 7B models\n  • Add domains: Extend the internal analyses to writing and coding tasks (Section 1; Appendix C) to test whether diversity and intensity trends generalize outside math.\n  • Scale variation: Include smaller and larger model sizes to assess scaling effects (Appendix A; Table 2).\n  • Cross-task consistency: Report whether per-model trends hold uniformly across tasks, with per-task visualizations akin to Figures 2–3.Score\n- Overall (10): 6 — Clear EAP-based framework and cross-model analyses (Section 3; Eqs. 9, 13–16; Table 1; Figures 2–3) but limited causal validation and statistical rigor (Section 4.3; Table 1; Appendix C).\n- Novelty (10): 6 — Applies EAP to RL post-training to reveal internal diversity/intensity shifts (Section 3; Table 1; Figure 3) with a unified update view (Eq. 4), though mechanistic claims remain correlational (Section 4.3).\n- Technical Quality (10): 5 — Sound formulation (Eqs. 5–9, 14–16) and careful filtering (Section 3.3), but missing causal checks, uncertainty reporting, and decoding controls (Table 1; Figure 3; Appendix B).\n- Clarity (10): 6 — Well-structured method and visuals (Figures 1–3; Section 4.2), yet ambiguity in W construction/granularity and metric parameterization (Section 4.2; Table 1) reduces interpretability.\n- Confidence (5): 3 — Assessment based on provided text, figures, and tables; lack of some methodological specifics (Section 4.2; Appendix B) and missing statistical details constrain certainty.",
  "final_review": "Summary\n- The paper investigates how reinforcement learning (RL) post-training alters internal information pathways in large language models (LLMs). Using Edge Attribution Patching (EAP) (Section 3; Eq. 9; Figure 1c), the authors compute edge-level importance from gradients of a self-entropy loss on truncated model-generated tokens (Section 3.3; Eq. 13). They summarize edge-weight distributions with Activation Intensity, Information Complexity (entropy), and Distribution Kurtosis (Section 4.2; Eqs. 14–16). Across four 7B model pairs and three math benchmarks (Section 4.1; Table 1), they report two robust effects of online RL (PPO/GRPO): increased activation intensity and increased diversity (higher entropy, lower kurtosis). They also observe that DPO-trained Qwen2.5 deviates from these trends (Section 4.3; Table 1; Figure 3). The paper provides a unified post-training gradient view (Eq. 4), visualizations (Figures 2–3), and open-source code (Abstract; Appendix B).Strengths\n- Boldly unified formalization of post-training updates\n  • Evidence: The gradient-based unification of SFT and RL is formalized in Eq. 4 (Section 2.2), with clear identification of data source D, reward/reference π_ref, and token-level gradient coefficients GC_A. Why it matters: Offers a clean mathematical lens for comparing post-training methods, improving clarity and interpretability of methodological differences (novelty/clarity).\n  • Evidence: The discussion contrasting online RL (PPO/GRPO) with DPO uses the unified view (Section 4.3) to explain static vs. dynamic data generation. Why it matters: Helps connect internal activation changes to training paradigms (impact/interpretability).\n  • Evidence: The introduction situates the work within recent RL post-training advances (Section 1), grounding the unified view in contemporary literature surveyed in Section 2.2 and References. Why it matters: Contextualizes contributions in a fast-evolving space (clarity/impact).- Clear graph-theoretic view of residual computation and edge attribution\n  • Evidence: The residual-stream formulas (Eq. 5; Section 3.1) and graph abstraction (Eqs. 6–7) map sub-modules to nodes and residual paths to edges, with a schematic in Figure 1b. Why it matters: Provides a precise internal representation that supports edge-level analysis (technical soundness/clarity).\n  • Evidence: The EAP linearization (Eq. 9; Figure 1c) is contrasted with ablation-based ACDC (Eq. 8), explaining computational tractability. Why it matters: Justifies choosing EAP for large models (scalability/technical suitability).\n  • Evidence: Section 3.2 explains how EAP can be computed for all edges with a single forward/backward pass (Section 3.2; text below Eq. 9). Why it matters: Ensures the feasibility of the proposed analysis at scale (experimental rigor).- Thoughtful data selection and truncation protocol to control confounds\n  • Evidence: Section 3.3 details filtering to questions correctly answered by both models and constraints on token length variability (Eqs. 10–12). Why it matters: Reduces confounding due to correctness and extreme lengths (experimental rigor).\n  • Evidence: Truncation to T_cut = α·T̄ and self-entropy definition (Eq. 13) explicitly control memory and maintain comparability across models (Section 3.3). Why it matters: Operationalizes a consistent and tractable attribution pipeline (clarity/reproducibility).\n  • Evidence: The authors vary α across {0.03, 0.1, 0.3, 0.5} in Table 1 and analyze robustness as α increases (Section 4.3, first paragraph; Table 1). Why it matters: Demonstrates some sensitivity analysis of truncation length (experimental rigor).- Consistent cross-model, cross-dataset evidence for online RL effects\n  • Evidence: Table 1 shows Activation Intensity increases for DeepSeek-Math (+GRPO), Mistral (+PPO), and Distilled-Qwen (+GRPO) across MATH, College Math, and GSM8K at several α values (Section 4.3; Table 1). Why it matters: Supports the claim of stronger engagement of internal pathways (technical soundness/impact).\n  • Evidence: Table 1 also shows Information Complexity increases and Distribution Kurtosis decreases for the same families and datasets, with exceptions diminishing as α grows (Section 4.3; Table 1). Why it matters: Indicates richer and less concentrated activation patterns after RL (technical soundness/impact).\n  • Evidence: Figure 2 visualizes relative changes in edge activation for Mistral on MATH at α = 0.5, showing widespread strengthening. Why it matters: Offers an intuitive view of network-level changes (clarity/impact).- Clear visualization of diversity changes and per-model patterns\n  • Evidence: Figure 3a (inter-sample diversity plot) highlights that most points lie in the RL improvement region, with a reported 95.1% improvement rate over 41 experiments (Section 4.3; Figure 3a). Why it matters: Summarizes global diversity effects across settings (experimental rigor/clarity).\n  • Evidence: Figure 3b reports large increases in output entropy for several settings (e.g., Deepseek-Math +1246%, +324%, +656%, +25%; Mistral +120%, +180%, etc.), corroborating Table 1 trends (Section 4.3; Figure 3b; Table 1). Why it matters: Reinforces the entropy-based diversity claim (impact/clarity).\n  • Evidence: Conclusion reiterates diversity visuals (Figure 3a–3b) to emphasize the divergence of DPO-trained Qwen2.5 from PPO/GRPO trends (Section 6; Figures in Conclusion; Section 4.3). Why it matters: Flags algorithmic differences in internal dynamics (impact).- Reproducibility and transparency\n  • Evidence: The paper provides model sources and specifications (Appendix A; Table 2), system configuration (Appendix B; Table 3), and an anonymous code link (Abstract; Appendix B). Why it matters: Facilitates replication and validation (reproducibility).\n  • Evidence: Performance comparisons before/after post-training are summarized across multiple benchmarks in Table 4 (Appendix C). Why it matters: Establishes external competence changes alongside internal analyses (rigor/impact).\n  • Evidence: Ethics and reproducibility statements are included (Appendix; Ethics Statement; Reproducibility Statement). Why it matters: Signals responsible and transparent research practices (clarity).Weaknesses\n- Limited causal validation linking edge attribution changes to behavioral gains\n  • Evidence: The study infers that stronger and more diverse activations “may explain” generalization (Section 6; Section 4.3), but does not perform targeted causal interventions (e.g., edge-level ablation guided by EAP) to verify necessity/sufficiency. Why it matters: Without causal tests, conclusions about mechanisms remain correlational (technical rigor/impact).\n  • Evidence: No correlation analysis between metric changes (Eqs. 14–16) and task performance gains (Table 4) is reported. Why it matters: The link between internal metrics and external improvements is asserted but not quantitatively substantiated (technical rigor).\n  • Evidence: ACDC-style ablation (Eq. 8) is discussed but not used to validate EAP estimates (Section 3.2; Figure 1c). Why it matters: Lacking ground-truth checks reduces confidence in attribution fidelity (technical soundness).- Self-entropy loss and truncation choices may introduce analysis biases\n  • Evidence: Edge attribution uses self-entropy over generated tokens (Eq. 13), not target-correct labels; justification and trade-offs are brief (Section 3.3), with no ablation to compare alternative losses. Why it matters: Self-entropy can be sensitive to decoding stochasticity and may weaken interpretability of gradients (technical soundness).\n  • Evidence: The truncation length α ∈ {0.03, 0.1, 0.3, 0.5} is motivated by memory control (Section 3.3) but not systematically justified; only qualitative statements about robustness with larger α appear (Section 4.3; Table 1). Why it matters: Truncation can change which circuits are probed, affecting conclusions (experimental rigor).\n  • Evidence: Filtering to questions correctly answered by both models and enforcing length balance (Eqs. 10–12) may exclude samples where RL provides the most benefit. Why it matters: The analysis set may be biased toward easier instances, potentially underrepresenting RL’s distinctive behavior on hard cases (generalizability).\n  • Evidence: Attribution is computed on each model’s own generated tokens (Section 3.3; Eq. 13), without aligning the token targets across models. Why it matters: Differences in edge attributions may conflate internal circuitry changes with output-token differences (technical rigor/interpretability).- Ambiguity in edge-weight matrix construction and granularity\n  • Evidence: Section 4.2 defines W ∈ ℝ^{n × n_o × n_i} but does not specify how n_o and n_i correspond to concrete edges (e.g., per-head, per-layer, attention/FFN splits). Why it matters: Ambiguity hinders reproducibility and interpretability of metrics (clarity/technical soundness).\n  • Evidence: The graph view (Section 3.1; Eqs. 6–7) aggregates sub-modules (A^ℓ, F^ℓ) but does not clarify whether edge-level attribution is computed at head granularity or whole-branch outputs. Why it matters: Coarse aggregation may hide important fine-grained mechanisms (novelty/technical depth).\n  • Evidence: Figures 2–3 do not detail the mapping from “output/input neurons” axes to specific architectural components (Figure 2; Figure 3). Why it matters: Limits insights into which parts of the network are most affected by RL (clarity/impact).\n  • Evidence: Eq. (1) uses a scaling term √d_k in attention while the text defines d_query and does not introduce d_k (Section 2.1; Eq. 1; text below Eq. 1). Why it matters: Notational inconsistency impacts clarity and technical precision (clarity/technical soundness).- Statistical rigor and reporting\n  • Evidence: Table 1 shows small absolute differences in Act.Intens. (e.g., Mistral MATH α=0.1: 6.76e-4→7.71e-4), but no confidence intervals, standard errors, or statistical tests are provided. Why it matters: Without uncertainty estimates, it is difficult to assess robustness (experimental rigor).\n  • Evidence: The paper acknowledges “Missing values result from GPU memory overflow” in Table 1, but does not quantify potential selection bias or remedy. Why it matters: Incomplete data may skew observations (experimental rigor).\n  • Evidence: Improvement percentages in Figure 3b are reported without variance or sample counts for each bar (e.g., +1246% on small baselines). Why it matters: Large percentage changes on near-zero baselines can be misleading (clarity/rigor).\n  • Evidence: Table 1 row labeling uses “Act.”/“Intens.”/arrows (↑/↓) across α values, which is not explained in Section 4.2. Why it matters: Ambiguous labeling complicates interpretation and verification (clarity/reproducibility).- Decoding and generation settings are insufficiently controlled\n  • Evidence: Section 3.3 defines loss over generated tokens but does not specify decoding strategies (temperature, top-k/p, beam vs. sampling) or random seed controls. Why it matters: Attribution magnitudes and entropy can vary with decoding; lack of control weakens conclusions (experimental rigor).\n  • Evidence: No details on prompt formatting or stopping criteria beyond truncation (Section 3.3) are provided. Why it matters: Variations in prompting can materially affect internal activations (reproducibility).\n  • Evidence: No robustness checks across different decoding schemes are presented. Why it matters: It remains unclear if reported trends generalize across common inference settings (generalizability).- External confounds in model selection and training differences\n  • Evidence: The RL models are third-party releases (Appendix A; Section 4.1), potentially differing in data, hyperparameters, and reward models beyond the algorithm (PPO/GRPO). Why it matters: Attribution of observed changes to “RL” per se may be confounded (technical rigor).\n  • Evidence: The DPO-vs-online RL explanation (Section 4.3) is plausible but not tested with controlled variants (e.g., Online-DPO or different DPO datasets). Why it matters: Conclusions about DPO’s internal dynamics may not generalize without controlled comparisons (generalizability).\n  • Evidence: Distilled-Qwen starts from a distilled checkpoint with strong reasoning (Section 4.1), which may interact with RL effects differently than non-distilled baselines. Why it matters: Heterogeneous starting points complicate cross-family comparisons (clarity/rigor).- Metric design choices may obscure important signal\n  • Evidence: Information Complexity uses entropy over histogram of |W| (Eq. 15), discarding sign information. Why it matters: Signs in residual streams can encode inhibitory vs. excitatory influences; losing sign may reduce interpretability (technical soundness).\n  • Evidence: The number of bins B and ε for entropy are not specified (Section 4.2; Eq. 15). Why it matters: Entropy estimates are sensitive to binning; lack of specification hinders replication (reproducibility).\n  • Evidence: Kurtosis (Eq. 16) is interpreted as “stability” and “uniformity,” but no calibration to known distributions or sanity checks are provided. Why it matters: Misinterpretation of kurtosis can mislead conclusions (clarity/technical soundness).\n  • Evidence: Additional metrics used in Figure 3—inter-sample diversity (panel a) and “entropy of output edge patterns per node” (panel b)—are not defined in Section 4.2. Why it matters: Undefined metrics prevent replication and obscure how improvements were computed (clarity/reproducibility).- Scope limited to mathematical reasoning and 7B models\n  • Evidence: Analyses use MATH, College Math, GSM8K (Section 4.1; Table 1), and performance comparisons in Appendix C are math-heavy (Table 4). Why it matters: Internal activation conclusions may be domain-specific (generalizability).\n  • Evidence: All model pairs are ≈7B (Section 4.1; Appendix A; Table 2). Why it matters: Trends may shift for smaller or larger models (impact/generalizability).\n  • Evidence: Claims on generalization to writing/coding (Section 1) are referenced but not probed internally in this work. Why it matters: The mechanistic findings are not validated beyond math (impact/generalizability).Suggestions for Improvement\n- Strengthen causal validation between internal metrics and behavior\n  • Add edge-level causal tests: Use EAP scores (Eq. 9) to identify high-importance edges, then perform ACDC-style ablations (Eq. 8; Section 3.2) to verify that removing them reduces performance on the same samples (Section 4.1; Table 1).\n  • Quantify correlations: Compute statistical associations between metric changes (Eqs. 14–16) and accuracy improvements (Appendix C; Table 4) across models/datasets/α, with confidence intervals.\n  • Include counterfactual patching: Perform activation patching guided by the discovered edges to test whether restoring pre-RL circuits reduces diversity and performance (Figure 3; Section 4.3).- Justify and stress-test the self-entropy loss and truncation protocol\n  • Compare losses: Evaluate EAP under self-entropy (Eq. 13) versus teacher-forced cross-entropy on correct targets to assess sensitivity (Section 3.3).\n  • Systematic α sweeps: Extend α beyond {0.03, 0.1, 0.3, 0.5} and report stability plots for metrics (Table 1; Section 4.3), including memory–fidelity trade-offs.\n  • Expand sample inclusion: Analyze a matched set including hard questions only RL solves (relax Eqs. 10–12) to check whether the internal trends strengthen on more challenging instances (Section 3.3).\n  • Align tokens across models: Recompute attributions with shared token targets via teacher forcing to disentangle internal changes from output-sequence differences (Section 3.3; Eq. 13).- Clarify construction and granularity of the edge-weight matrices\n  • Specify mapping: Define n_o, n_i (Section 4.2) with precise architectural correspondence (per head, per layer, attention vs. FFN), and reflect this in figure axes (Figure 2–3).\n  • Provide granularity analyses: Report per-head/per-layer metrics to reveal where RL induces the largest changes (Section 3.1; Eqs. 6–7).\n  • Add ablations: Compare branch-level (A^ℓ, F^ℓ) aggregation vs. head-level attributions to quantify resolution effects on Act.Intens./Info.Complex./Dist.Kurt. (Section 4.2).\n  • Fix notation: Correct Eq. (1) by defining d_k explicitly or replacing with the already defined d_query to ensure consistency (Section 2.1; Eq. 1).- Improve statistical rigor and reporting\n  • Report uncertainty: Add confidence intervals or standard errors for Table 1 metrics across runs/seeds, and include statistical tests (Section 4.3; Table 1).\n  • Address missing data: Quantify the impact of “GPU memory overflow” on coverage and consider gradient checkpointing to fill missing entries (Table 1).\n  • Normalize percentage reporting: For Figure 3b, include absolute baselines, sample counts, and variance to contextualize large percentage changes (Section 4.3).\n  • Clarify table labels: Standardize row naming in Table 1 to directly correspond to defined metrics (Eqs. 14–16) and remove or explain arrow rows (↑/↓) for interpretability (Table 1).- Control and document decoding/generation settings\n  • Specify inference settings: Report temperature, top-k/top-p, seed, and stopping criteria alongside Section 3.3 so gradients under Eq. 13 are interpretable and reproducible.\n  • Robustness checks: Recompute metrics under multiple decoding regimes (e.g., greedy vs. sampling) to assess stability of trends (Table 1; Figure 3).\n  • Prompting controls: Standardize prompt templates and format across models, and include these details in Appendix B to ensure consistency (Appendix B; Table 3).- Reduce external confounds in model selection and training differences\n  • Controlled re-training: Where feasible, start from the same SFT checkpoint and apply standardized PPO/GRPO/DPO settings to isolate algorithmic effects (Section 4.1; Appendix A).\n  • Test DPO variants: Include Online-DPO or different preference datasets to check whether static data is the primary driver of Qwen2.5 deviations (Section 4.3; Table 1).\n  • Match baselines: Compare distilled and non-distilled baselines within the same family (Distilled-Qwen; Section 4.1) to disentangle distillation effects from RL-induced changes.- Refine metric design and reporting\n  • Preserve sign: Compute entropy over signed attributions or complement |W|-based metrics with sign-sensitive measures to capture inhibitory/excitatory dynamics (Eq. 15).\n  • Specify binning: Report B and ε for entropy (Eq. 15), and add sensitivity analyses for different bin counts to ensure robustness (Section 4.2).\n  • Calibrate kurtosis: Include sanity checks (e.g., synthetic distributions or known baselines) to validate the interpretation of Dist.Kurt. (Eq. 16), and relate kurtosis shifts to concrete distributional changes.\n  • Define diversity metrics: Formally specify inter-sample diversity and per-node output-edge entropy used in Figure 3, including exact formulas and parameter choices (Section 4.3; Figure 3; Section 4.2).- Broaden scope beyond math and 7B models\n  • Add domains: Extend the internal analyses to writing and coding tasks (Section 1; Appendix C) to test whether diversity and intensity trends generalize outside math.\n  • Scale variation: Include smaller and larger model sizes to assess scaling effects (Appendix A; Table 2).\n  • Cross-task consistency: Report whether per-model trends hold uniformly across tasks, with per-task visualizations akin to Figures 2–3.Score\n- Overall (10): 6 — Clear EAP-based framework and cross-model analyses (Section 3; Eqs. 9, 13–16; Table 1; Figures 2–3) but limited causal validation and statistical rigor (Section 4.3; Table 1; Appendix C).\n- Novelty (10): 6 — Applies EAP to RL post-training to reveal internal diversity/intensity shifts (Section 3; Table 1; Figure 3) with a unified update view (Eq. 4), though mechanistic claims remain correlational (Section 4.3).\n- Technical Quality (10): 5 — Sound formulation (Eqs. 5–9, 14–16) and careful filtering (Section 3.3), but missing causal checks, uncertainty reporting, decoding controls, and alignment of token targets across models (Table 1; Figure 3; Section 3.3).\n- Clarity (10): 5 — Well-structured method and visuals (Figures 1–3; Section 4.2), yet ambiguity in W construction/granularity, undefined diversity metrics in Section 4.2, and Table 1 labeling issues reduce interpretability (Section 4.2; Figure 3; Table 1).\n- Confidence (5): 3 — Assessment based on provided text, figures, and tables; lack of some methodological specifics (Section 4.2; Section 3.3; Appendix B) and missing statistical details constrain certainty.",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 6,
        "novelty": 6,
        "technical_quality": 5,
        "clarity": 6,
        "confidence": 3
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 6,
        "novelty": 6,
        "technical_quality": 5,
        "clarity": 5,
        "confidence": 3
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "Summary\n- The paper investigates how reinforcement learning (RL) post-training alters internal information pathways in large language models (LLMs). Using Edge Attribution Patching (EAP) (Section 3; Eq. 9; Figure 1c), the authors compute edge-level importance from gradients of a self-entropy loss on truncated model-generated tokens (Section 3.3; Eq. 13). They summarize edge-weight distributions with Activation Intensity, Information Complexity (entropy), and Distribution Kurtosis (Section 4.2; Eqs. 14–16). Across four 7B model pairs and three math benchmarks (Section 4.1; Table 1), they report two robust effects of online RL (PPO/GRPO): increased activation intensity and increased diversity (higher entropy, lower kurtosis). They also observe that DPO-trained Qwen2.5 deviates from these trends (Section 4.3; Table 1; Figure 3). The paper provides a unified post-training gradient view (Eq. 4), visualizations (Figures 2–3), and open-source code (Abstract; Appendix B).Strengths\n- Boldly unified formalization of post-training updates\n  • Evidence: The gradient-based unification of SFT and RL is formalized in Eq. 4 (Section 2.2), with clear identification of data source D, reward/reference π_ref, and token-level gradient coefficients GC_A. Why it matters: Offers a clean mathematical lens for comparing post-training methods, improving clarity and interpretability of methodological differences (novelty/clarity).\n  • Evidence: The discussion contrasting online RL (PPO/GRPO) with DPO uses the unified view (Section 4.3) to explain static vs. dynamic data generation. Why it matters: Helps connect internal activation changes to training paradigms (impact/interpretability).\n  • Evidence: The introduction situates the work within recent RL post-training advances (Section 1), grounding the unified view in contemporary literature surveyed in Section 2.2 and References. Why it matters: Contextualizes contributions in a fast-evolving space (clarity/impact).- Clear graph-theoretic view of residual computation and edge attribution\n  • Evidence: The residual-stream formulas (Eq. 5; Section 3.1) and graph abstraction (Eqs. 6–7) map sub-modules to nodes and residual paths to edges, with a schematic in Figure 1b. Why it matters: Provides a precise internal representation that supports edge-level analysis (technical soundness/clarity).\n  • Evidence: The EAP linearization (Eq. 9; Figure 1c) is contrasted with ablation-based ACDC (Eq. 8), explaining computational tractability. Why it matters: Justifies choosing EAP for large models (scalability/technical suitability).\n  • Evidence: Section 3.2 explains how EAP can be computed for all edges with a single forward/backward pass (Section 3.2; text below Eq. 9). Why it matters: Ensures the feasibility of the proposed analysis at scale (experimental rigor).- Thoughtful data selection and truncation protocol to control confounds\n  • Evidence: Section 3.3 details filtering to questions correctly answered by both models and constraints on token length variability (Eqs. 10–12). Why it matters: Reduces confounding due to correctness and extreme lengths (experimental rigor).\n  • Evidence: Truncation to T_cut = α·T̄ and self-entropy definition (Eq. 13) explicitly control memory and maintain comparability across models (Section 3.3). Why it matters: Operationalizes a consistent and tractable attribution pipeline (clarity/reproducibility).\n  • Evidence: The authors vary α across {0.03, 0.1, 0.3, 0.5} in Table 1 and analyze robustness as α increases (Section 4.3, first paragraph; Table 1). Why it matters: Demonstrates some sensitivity analysis of truncation length (experimental rigor).- Consistent cross-model, cross-dataset evidence for online RL effects\n  • Evidence: Table 1 shows Activation Intensity increases for DeepSeek-Math (+GRPO), Mistral (+PPO), and Distilled-Qwen (+GRPO) across MATH, College Math, and GSM8K at several α values (Section 4.3; Table 1). Why it matters: Supports the claim of stronger engagement of internal pathways (technical soundness/impact).\n  • Evidence: Table 1 also shows Information Complexity increases and Distribution Kurtosis decreases for the same families and datasets, with exceptions diminishing as α grows (Section 4.3; Table 1). Why it matters: Indicates richer and less concentrated activation patterns after RL (technical soundness/impact).\n  • Evidence: Figure 2 visualizes relative changes in edge activation for Mistral on MATH at α = 0.5, showing widespread strengthening. Why it matters: Offers an intuitive view of network-level changes (clarity/impact).- Clear visualization of diversity changes and per-model patterns\n  • Evidence: Figure 3a (inter-sample diversity plot) highlights that most points lie in the RL improvement region, with a reported 95.1% improvement rate over 41 experiments (Section 4.3; Figure 3a). Why it matters: Summarizes global diversity effects across settings (experimental rigor/clarity).\n  • Evidence: Figure 3b reports large increases in output entropy for several settings (e.g., Deepseek-Math +1246%, +324%, +656%, +25%; Mistral +120%, +180%, etc.), corroborating Table 1 trends (Section 4.3; Figure 3b; Table 1). Why it matters: Reinforces the entropy-based diversity claim (impact/clarity).\n  • Evidence: Conclusion reiterates diversity visuals (Figure 3a–3b) to emphasize the divergence of DPO-trained Qwen2.5 from PPO/GRPO trends (Section 6; Figures in Conclusion; Section 4.3). Why it matters: Flags algorithmic differences in internal dynamics (impact).- Reproducibility and transparency\n  • Evidence: The paper provides model sources and specifications (Appendix A; Table 2), system configuration (Appendix B; Table 3), and an anonymous code link (Abstract; Appendix B). Why it matters: Facilitates replication and validation (reproducibility).\n  • Evidence: Performance comparisons before/after post-training are summarized across multiple benchmarks in Table 4 (Appendix C). Why it matters: Establishes external competence changes alongside internal analyses (rigor/impact).\n  • Evidence: Ethics and reproducibility statements are included (Appendix; Ethics Statement; Reproducibility Statement). Why it matters: Signals responsible and transparent research practices (clarity).Weaknesses\n- Limited causal validation linking edge attribution changes to behavioral gains\n  • Evidence: The study infers that stronger and more diverse activations “may explain” generalization (Section 6; Section 4.3), but does not perform targeted causal interventions (e.g., edge-level ablation guided by EAP) to verify necessity/sufficiency. Why it matters: Without causal tests, conclusions about mechanisms remain correlational (technical rigor/impact).\n  • Evidence: No correlation analysis between metric changes (Eqs. 14–16) and task performance gains (Table 4) is reported. Why it matters: The link between internal metrics and external improvements is asserted but not quantitatively substantiated (technical rigor).\n  • Evidence: ACDC-style ablation (Eq. 8) is discussed but not used to validate EAP estimates (Section 3.2; Figure 1c). Why it matters: Lacking ground-truth checks reduces confidence in attribution fidelity (technical soundness).- Self-entropy loss and truncation choices may introduce analysis biases\n  • Evidence: Edge attribution uses self-entropy over generated tokens (Eq. 13), not target-correct labels; justification and trade-offs are brief (Section 3.3), with no ablation to compare alternative losses. Why it matters: Self-entropy can be sensitive to decoding stochasticity and may weaken interpretability of gradients (technical soundness).\n  • Evidence: The truncation length α ∈ {0.03, 0.1, 0.3, 0.5} is motivated by memory control (Section 3.3) but not systematically justified; only qualitative statements about robustness with larger α appear (Section 4.3; Table 1). Why it matters: Truncation can change which circuits are probed, affecting conclusions (experimental rigor).\n  • Evidence: Filtering to questions correctly answered by both models and enforcing length balance (Eqs. 10–12) may exclude samples where RL provides the most benefit. Why it matters: The analysis set may be biased toward easier instances, potentially underrepresenting RL’s distinctive behavior on hard cases (generalizability).\n  • Evidence: Attribution is computed on each model’s own generated tokens (Section 3.3; Eq. 13), without aligning the token targets across models. Why it matters: Differences in edge attributions may conflate internal circuitry changes with output-token differences (technical rigor/interpretability).- Ambiguity in edge-weight matrix construction and granularity\n  • Evidence: Section 4.2 defines W ∈ ℝ^{n × n_o × n_i} but does not specify how n_o and n_i correspond to concrete edges (e.g., per-head, per-layer, attention/FFN splits). Why it matters: Ambiguity hinders reproducibility and interpretability of metrics (clarity/technical soundness).\n  • Evidence: The graph view (Section 3.1; Eqs. 6–7) aggregates sub-modules (A^ℓ, F^ℓ) but does not clarify whether edge-level attribution is computed at head granularity or whole-branch outputs. Why it matters: Coarse aggregation may hide important fine-grained mechanisms (novelty/technical depth).\n  • Evidence: Figures 2–3 do not detail the mapping from “output/input neurons” axes to specific architectural components (Figure 2; Figure 3). Why it matters: Limits insights into which parts of the network are most affected by RL (clarity/impact).\n  • Evidence: Eq. (1) uses a scaling term √d_k in attention while the text defines d_query and does not introduce d_k (Section 2.1; Eq. 1; text below Eq. 1). Why it matters: Notational inconsistency impacts clarity and technical precision (clarity/technical soundness).- Statistical rigor and reporting\n  • Evidence: Table 1 shows small absolute differences in Act.Intens. (e.g., Mistral MATH α=0.1: 6.76e-4→7.71e-4), but no confidence intervals, standard errors, or statistical tests are provided. Why it matters: Without uncertainty estimates, it is difficult to assess robustness (experimental rigor).\n  • Evidence: The paper acknowledges “Missing values result from GPU memory overflow” in Table 1, but does not quantify potential selection bias or remedy. Why it matters: Incomplete data may skew observations (experimental rigor).\n  • Evidence: Improvement percentages in Figure 3b are reported without variance or sample counts for each bar (e.g., +1246% on small baselines). Why it matters: Large percentage changes on near-zero baselines can be misleading (clarity/rigor).\n  • Evidence: Table 1 row labeling uses “Act.”/“Intens.”/arrows (↑/↓) across α values, which is not explained in Section 4.2. Why it matters: Ambiguous labeling complicates interpretation and verification (clarity/reproducibility).- Decoding and generation settings are insufficiently controlled\n  • Evidence: Section 3.3 defines loss over generated tokens but does not specify decoding strategies (temperature, top-k/p, beam vs. sampling) or random seed controls. Why it matters: Attribution magnitudes and entropy can vary with decoding; lack of control weakens conclusions (experimental rigor).\n  • Evidence: No details on prompt formatting or stopping criteria beyond truncation (Section 3.3) are provided. Why it matters: Variations in prompting can materially affect internal activations (reproducibility).\n  • Evidence: No robustness checks across different decoding schemes are presented. Why it matters: It remains unclear if reported trends generalize across common inference settings (generalizability).- External confounds in model selection and training differences\n  • Evidence: The RL models are third-party releases (Appendix A; Section 4.1), potentially differing in data, hyperparameters, and reward models beyond the algorithm (PPO/GRPO). Why it matters: Attribution of observed changes to “RL” per se may be confounded (technical rigor).\n  • Evidence: The DPO-vs-online RL explanation (Section 4.3) is plausible but not tested with controlled variants (e.g., Online-DPO or different DPO datasets). Why it matters: Conclusions about DPO’s internal dynamics may not generalize without controlled comparisons (generalizability).\n  • Evidence: Distilled-Qwen starts from a distilled checkpoint with strong reasoning (Section 4.1), which may interact with RL effects differently than non-distilled baselines. Why it matters: Heterogeneous starting points complicate cross-family comparisons (clarity/rigor).- Metric design choices may obscure important signal\n  • Evidence: Information Complexity uses entropy over histogram of |W| (Eq. 15), discarding sign information. Why it matters: Signs in residual streams can encode inhibitory vs. excitatory influences; losing sign may reduce interpretability (technical soundness).\n  • Evidence: The number of bins B and ε for entropy are not specified (Section 4.2; Eq. 15). Why it matters: Entropy estimates are sensitive to binning; lack of specification hinders replication (reproducibility).\n  • Evidence: Kurtosis (Eq. 16) is interpreted as “stability” and “uniformity,” but no calibration to known distributions or sanity checks are provided. Why it matters: Misinterpretation of kurtosis can mislead conclusions (clarity/technical soundness).\n  • Evidence: Additional metrics used in Figure 3—inter-sample diversity (panel a) and “entropy of output edge patterns per node” (panel b)—are not defined in Section 4.2. Why it matters: Undefined metrics prevent replication and obscure how improvements were computed (clarity/reproducibility).- Scope limited to mathematical reasoning and 7B models\n  • Evidence: Analyses use MATH, College Math, GSM8K (Section 4.1; Table 1), and performance comparisons in Appendix C are math-heavy (Table 4). Why it matters: Internal activation conclusions may be domain-specific (generalizability).\n  • Evidence: All model pairs are ≈7B (Section 4.1; Appendix A; Table 2). Why it matters: Trends may shift for smaller or larger models (impact/generalizability).\n  • Evidence: Claims on generalization to writing/coding (Section 1) are referenced but not probed internally in this work. Why it matters: The mechanistic findings are not validated beyond math (impact/generalizability).Suggestions for Improvement\n- Strengthen causal validation between internal metrics and behavior\n  • Add edge-level causal tests: Use EAP scores (Eq. 9) to identify high-importance edges, then perform ACDC-style ablations (Eq. 8; Section 3.2) to verify that removing them reduces performance on the same samples (Section 4.1; Table 1).\n  • Quantify correlations: Compute statistical associations between metric changes (Eqs. 14–16) and accuracy improvements (Appendix C; Table 4) across models/datasets/α, with confidence intervals.\n  • Include counterfactual patching: Perform activation patching guided by the discovered edges to test whether restoring pre-RL circuits reduces diversity and performance (Figure 3; Section 4.3).- Justify and stress-test the self-entropy loss and truncation protocol\n  • Compare losses: Evaluate EAP under self-entropy (Eq. 13) versus teacher-forced cross-entropy on correct targets to assess sensitivity (Section 3.3).\n  • Systematic α sweeps: Extend α beyond {0.03, 0.1, 0.3, 0.5} and report stability plots for metrics (Table 1; Section 4.3), including memory–fidelity trade-offs.\n  • Expand sample inclusion: Analyze a matched set including hard questions only RL solves (relax Eqs. 10–12) to check whether the internal trends strengthen on more challenging instances (Section 3.3).\n  • Align tokens across models: Recompute attributions with shared token targets via teacher forcing to disentangle internal changes from output-sequence differences (Section 3.3; Eq. 13).- Clarify construction and granularity of the edge-weight matrices\n  • Specify mapping: Define n_o, n_i (Section 4.2) with precise architectural correspondence (per head, per layer, attention vs. FFN), and reflect this in figure axes (Figure 2–3).\n  • Provide granularity analyses: Report per-head/per-layer metrics to reveal where RL induces the largest changes (Section 3.1; Eqs. 6–7).\n  • Add ablations: Compare branch-level (A^ℓ, F^ℓ) aggregation vs. head-level attributions to quantify resolution effects on Act.Intens./Info.Complex./Dist.Kurt. (Section 4.2).\n  • Fix notation: Correct Eq. (1) by defining d_k explicitly or replacing with the already defined d_query to ensure consistency (Section 2.1; Eq. 1).- Improve statistical rigor and reporting\n  • Report uncertainty: Add confidence intervals or standard errors for Table 1 metrics across runs/seeds, and include statistical tests (Section 4.3; Table 1).\n  • Address missing data: Quantify the impact of “GPU memory overflow” on coverage and consider gradient checkpointing to fill missing entries (Table 1).\n  • Normalize percentage reporting: For Figure 3b, include absolute baselines, sample counts, and variance to contextualize large percentage changes (Section 4.3).\n  • Clarify table labels: Standardize row naming in Table 1 to directly correspond to defined metrics (Eqs. 14–16) and remove or explain arrow rows (↑/↓) for interpretability (Table 1).- Control and document decoding/generation settings\n  • Specify inference settings: Report temperature, top-k/top-p, seed, and stopping criteria alongside Section 3.3 so gradients under Eq. 13 are interpretable and reproducible.\n  • Robustness checks: Recompute metrics under multiple decoding regimes (e.g., greedy vs. sampling) to assess stability of trends (Table 1; Figure 3).\n  • Prompting controls: Standardize prompt templates and format across models, and include these details in Appendix B to ensure consistency (Appendix B; Table 3).- Reduce external confounds in model selection and training differences\n  • Controlled re-training: Where feasible, start from the same SFT checkpoint and apply standardized PPO/GRPO/DPO settings to isolate algorithmic effects (Section 4.1; Appendix A).\n  • Test DPO variants: Include Online-DPO or different preference datasets to check whether static data is the primary driver of Qwen2.5 deviations (Section 4.3; Table 1).\n  • Match baselines: Compare distilled and non-distilled baselines within the same family (Distilled-Qwen; Section 4.1) to disentangle distillation effects from RL-induced changes.- Refine metric design and reporting\n  • Preserve sign: Compute entropy over signed attributions or complement |W|-based metrics with sign-sensitive measures to capture inhibitory/excitatory dynamics (Eq. 15).\n  • Specify binning: Report B and ε for entropy (Eq. 15), and add sensitivity analyses for different bin counts to ensure robustness (Section 4.2).\n  • Calibrate kurtosis: Include sanity checks (e.g., synthetic distributions or known baselines) to validate the interpretation of Dist.Kurt. (Eq. 16), and relate kurtosis shifts to concrete distributional changes.\n  • Define diversity metrics: Formally specify inter-sample diversity and per-node output-edge entropy used in Figure 3, including exact formulas and parameter choices (Section 4.3; Figure 3; Section 4.2).- Broaden scope beyond math and 7B models\n  • Add domains: Extend the internal analyses to writing and coding tasks (Section 1; Appendix C) to test whether diversity and intensity trends generalize outside math.\n  • Scale variation: Include smaller and larger model sizes to assess scaling effects (Appendix A; Table 2).\n  • Cross-task consistency: Report whether per-model trends hold uniformly across tasks, with per-task visualizations akin to Figures 2–3.Score\n- Overall (10): 6 — Clear EAP-based framework and cross-model analyses (Section 3; Eqs. 9, 13–16; Table 1; Figures 2–3) but limited causal validation and statistical rigor (Section 4.3; Table 1; Appendix C).\n- Novelty (10): 6 — Applies EAP to RL post-training to reveal internal diversity/intensity shifts (Section 3; Table 1; Figure 3) with a unified update view (Eq. 4), though mechanistic claims remain correlational (Section 4.3).\n- Technical Quality (10): 5 — Sound formulation (Eqs. 5–9, 14–16) and careful filtering (Section 3.3), but missing causal checks, uncertainty reporting, decoding controls, and alignment of token targets across models (Table 1; Figure 3; Section 3.3).\n- Clarity (10): 5 — Well-structured method and visuals (Figures 1–3; Section 4.2), yet ambiguity in W construction/granularity, undefined diversity metrics in Section 4.2, and Table 1 labeling issues reduce interpretability (Section 4.2; Figure 3; Table 1).\n- Confidence (5): 3 — Assessment based on provided text, figures, and tables; lack of some methodological specifics (Section 4.2; Section 3.3; Appendix B) and missing statistical details constrain certainty."
}