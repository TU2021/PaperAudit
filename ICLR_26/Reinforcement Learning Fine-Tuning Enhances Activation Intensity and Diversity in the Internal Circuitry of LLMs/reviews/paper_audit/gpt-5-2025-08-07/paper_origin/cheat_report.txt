Academic integrity and internal consistency risk report

Summary of high-impact issues

1) Methodological comparability of edge attribution across models is not justified
- Evidence: Section 3.3 defines the loss used for edge attribution as “self-entropy (cross-entropy of the model with respect to its own output)” (Eq. 13), computed on each model’s own generated tokens s1:Tcut. The filtering requires only that both models answer correctly and have similar sequence lengths (Eqs. 10–12), but not identical token sequences.
- Problem: Because the base and RL models can generate different (even if both correct) token sequences, the gradients ∇H L in Eq. 9 are taken with respect to different targets for different models. This confounds attribution differences with output-token differences rather than purely internal circuitry changes. The paper asserts “maintaining comparability across sequences” (Section 3.3) but only controls for length, not content.
- Impact: This materially affects the main claims about RL increasing activation intensity and diversity, as changes may reflect different generated tokens rather than changes in internal pathways.
- Missing detail: No procedure is described to align or teacher-force both models on the same token targets (e.g., using ground-truth or a shared reference response), nor are generation hyperparameters (temperature, sampling) provided to assess output variability. No direct evidence found in the manuscript that token content was controlled.

2) Metrics used in Results/Figures are not defined in the Metrics section
- Evidence: Section 4.2 defines three metrics only: Activation Intensity (Eq. 14), Information Complexity (Eq. 15), and Distribution Kurtosis (Eq. 16).
- Contradiction: Section 4.3 and Figure 3 introduce additional measures—“inter-sample diversity” described as “one minus the mean correlation of edge-weight matrices between sample pairs” and “output-edge entropy per node”—but neither is defined in Section 4.2.
  - Figure 3 caption references these metrics; panel (a) uses inter-sample diversity, panel (b) uses per-node output entropy and very large percentage improvements (e.g., +1246%).
- Impact: The introduction of undefined metrics prevents replication and makes it unclear how the reported improvements (e.g., “Total Experiments: 41; Average Improvement: 7.6%; Improvement Rate: 95.1%” in Figure 3(a)) were computed. This undermines the trustworthiness of the results.

3) Reproducibility claims vs. missing experimental hyperparameters and procedures
- Evidence: Section 3.3 introduces filtering and truncation hyperparameters β, γ, δ, and α; Section 4.2’s entropy metric requires the number of bins B and ε; Section 4.1/Appendix A reference diverse models; Appendix B is titled “Implementation details.”
- Contradiction: Appendix B lists only hardware/software (OS, CUDA, Python, PyTorch, devices) and provides no analytical hyperparameters (β, γ, δ, α values actually used per experiment), entropy binning choices (B, ε), sample sizes n, generation settings (temperature, top-k/p, max tokens), rounding of Tcut, or details on how edge-weight matrices W were constructed (see Issue 4).
- Impact: The stated “implementation details” (Appendix B) do not include the necessary experimental settings for reproducing Figures/Tables. This contradicts the Reproducibility Statement (Appendix, “we list the implementation details in Appendix B”) and materially affects reproducibility.

4) Construction of the “edge-weight matrix” W and Figure 2 axes is unclear
- Evidence: Section 4.2 defines W(k) ∈ ℝno×ni without specifying what “outputs” and “inputs” index (e.g., which nodes, layers, heads, or edges), nor how I_EAP(O, H) (Eq. 9) is aggregated into W(k).
- Figures: Figure 2 uses axes “Output Neurons” and “Input Neurons,” but there is no definition of “neurons” in the graph-edge attribution context or mapping from edges to a no×ni matrix.
- Impact: Without a precise mapping from graph edges (Section 3.1–3.2) to matrix indices, the metrics (Eqs. 14–16) are ambiguous and cannot be independently verified or reproduced.

5) Overstated generalization about DPO vs. RL without direct methodological evidence
- Evidence: Section 4.3 claims “DPO training cannot activate a broader range of neural pathways,” attributing Qwen2.5’s divergence to DPO’s static data use (Sections 4.2–4.3).
- Problem: The paper provides empirical results for one DPO-trained model pair (Qwen2.5-7B-SFT vs Qwen2.5-7B-DPO; Table 1) but presents a general mechanistic explanation for DPO, while acknowledging only in passing that DPO can be implemented differently (references include Online-DPO-R1 in References).
- Impact: The mechanistic claim extends beyond the evidence presented in the manuscript. It should be framed as a hypothesis consistent with Qwen2.5 observations rather than a general conclusion. No direct evidence found in the manuscript demonstrating that DPO intrinsically fails to broaden pathway engagement beyond the single Qwen2.5 case.

6) Use of undefined variable in the Transformer formulation
- Evidence: Section 2.1 Eq. (1) uses “sqrt(d_k)” in the attention scaling denominator, but the text defines query/key dimensionality as d_query and never defines d_k.
- Impact: This is a technical inconsistency in the mathematical description that should be corrected (e.g., use sqrt(d_query) if that is intended).

7) Table 1 labeling and metric presentation ambiguities
- Evidence: Table 1 lists rows labeled “Act.”, “Intens.”, and “↑” (and “Info.”, “Complex.”, “↑”; “Dist.”, “Kurt.”, “↓”) across multiple α values.
- Problem: It is unclear whether these rows represent the same metric (Act.Intens., Info.Complex., Dist.Kurt.) at different α values or distinct submetrics. The arrow rows (“↑”/“↓”) appear as metric names but are not defined in Section 4.2.
- Impact: Ambiguous labeling complicates interpretation and cross-checking of the numerical trends.

8) Very large percentage improvements in Figure 3(b) conflict with Table 1 scales and lack methodological grounding
- Evidence: Figure 3(b) shows “entropy of output edge patterns per node” with increases such as +1246%, +656%, etc. Table 1’s Information Complexity values for DeepSeek-Math are on the order of 0.17–0.41 per dataset/α, which does not align with these extreme percentages if measuring the same or a closely related entropy quantity.
- Problem: Because the per-node entropy metric is not defined, the basis for the percentage calculation and its relation to Table 1’s Info.Complex. is unclear.
- Impact: The figure’s numerical claims are difficult to validate and may misrepresent the magnitude of effects.

9) Claims about computational tractability vs. observed GPU memory overflow
- Evidence: Section 3.2 states the EAP approach “enables scalable, fine-grained circuit analysis … making it tractable even for very large models.” Table 1 notes “Missing values result from GPU memory overflow.”
- Impact: While not fatal, this inconsistency should be acknowledged and quantified (e.g., memory footprint per α/model) to clarify practical limitations and prevent overstatements of scalability.

Items that could be clarified but are less critical
- Section 3.2’s phrase “single forward and backward pass under the zeroing perturbation” is confusing, since the gradient-based estimate (Eq. 9) does not require actual zeroing ablations; only forward activations and backward gradients are needed.
- Section 3.3’s assertion “maintaining comparability across sequences” would be strengthened by reporting δ, β, γ and the percentage of questions retained per dataset/model pair.

Conclusion
The manuscript presents a promising analysis framework, but several high-impact methodological and reporting inconsistencies materially affect the trustworthiness and reproducibility of the main claims:
- The most serious issue is the lack of control for identical token targets across models when computing edge attribution, which can confound internal changes with output differences (Section 3.3, Eq. 13).
- Additional undefined metrics used in the Results/Figures, missing hyperparameters and procedures, and unclear construction of the edge-weight matrices prevent independent verification.
Addressing these points with precise definitions, aligned-token evaluation or teacher forcing, complete experimental settings, and clarified metric derivations would substantially improve the paper’s scientific validity.