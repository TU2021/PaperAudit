Summary
- The paper addresses contextual faithfulness in RAG by encouraging direct reuse of context text, hypothesizing that higher copying degree reduces hallucinations. It introduces CopyPaste, a two-stage approach: (i) three prompting methods (CP-Order, CP-Link, CP-Refine) to produce high-copying responses, and (ii) CopyPasteLLM, trained via DPO on automatically constructed preference pairs emphasizing contextual trust. The authors define copy coverage κ and copy density δ (Eq. 1) and add an interpretability tool, Context-Parameter Copying Capturing (Algorithm 3), plus a “logits power” analysis (Appendix I). Experiments on FaithEval, ConFiQA, PubMedQA, and RAGTruth claim substantial gains, including large improvements in counterfactual settings with only 365 training samples (Abstract; §4.1.2; Appendix Table 5). The method aims to reduce reliance on parametric knowledge while maintaining contextual representations (Figures 3–4; Appendix Figures 7–9).Strengths
- Bold operationalization of faithfulness via direct copying
  - Clear formalization of copy metrics κ and δ with equation and copy-fragment detection (Eq. 1; Appendix C/Algorithm 4), grounding the core idea in measurable signals. This matters for technical soundness by defining explicit optimization targets.
  - Three complementary prompting regimes—hard (CP-Order), semi-hard (CP-Link), and soft iterative (CP-Refine)—provide controllable ways to elevate copying (Section 3.1; Figure 2; Appendix J.1–J.4). This is novel and clarifies how constraints trade off fluency and faithfulness.
  - Pipeline transforms generated responses into structured preferences and filters by copying strength (κ, δ), faithfulness (AlignScore, MiniCheck), relevance, and fluency (Section 3.2; Figure 2; Appendix Algorithms 1–2). This supports experimental rigor via multi-criteria quality control.
- Data-efficient DPO alignment with automated preference construction
  - Multi-stage selection, Elo-style LLM-as-Judge tournament, and “stamping” gold/wrong answers yield roughly five preference pairs per item (Section 3.2; Figure 2; Algorithm 2). This enhances sample efficiency and impact by amplifying supervision from limited data.
  - The training uses parameter-efficient LoRA with transparent hyperparameters (Appendix D: r=64, α=128, β=0.3; epochs and optimizer settings), supporting reproducibility and technical soundness.
  - The text claims only 365 query-context pairs achieve large gains over strong baselines (Abstract; §4.1.2). If sustained, this would be impactful for practitioners constrained by data.
- Broad and carefully structured evaluation suite
  - Four datasets spanning daily life, science counterfactuals, biomedicine, and Wikidata (Appendix A/Table 4), enabling cross-domain assessment. This matters for impact and generality.
  - Multiple evaluation axes—faithfulness (AlignScore; MiniCheck), hallucination via LLM-as-Judge with Twist and Causal prompts (Appendix A/J.3), fluency (GPT-2 perplexity), copying (κ, δ), and relevance (Qwen3-Embedding) (Appendix A). The breadth strengthens experimental rigor.
  - Non-counterfactual results in Table 3 show strong gains across ConFiQA subsets and PubMedQA for two bases, with a slight drop for Llama-3.1-8B on PubMedQA (Section 4.1.2/Table 3), indicating utility beyond counterfactual stress tests while reflecting dataset-specific variability.
- Mechanistic analysis beyond final answers
  - Context-Parameter Copying Capturing (Section 3.3; Algorithm 3) extends Knowledge Token Capturing to CoT trajectories, collecting top-K tokens’ probabilities and hidden states with/without context. This advances interpretability by moving to sequence-level analysis.
  - Logits power and UMAP visualizations (Figures 3–4; Appendix I/Figures 7–9) show earlier and stronger contextual reliance and distinct parametric distributions for CopyPasteLLM. This provides technical evidence of the claimed “recalibration” of parametric knowledge reliance.
  - Position-aware results note earlier peak contextual usage in CopyPasteLLM than base (Section 4.2; Figures 3–4), aligning with the paper’s thesis about contextual trust dynamics.
- Clear articulation of limitations and ethics
  - Limitations discuss incomplete context scenarios and multimodal extension challenges (Appendix G/H), which supports responsible framing and impact.
  - Ethics statement acknowledges risk of copying incorrect source material (Appendix 7), signaling awareness of deployment trade-offs.
- Comprehensive prompt transparency and procedural formalization
  - Detailed prompts for CP-Order/Link/Refine, baselines, judges, and accuracy/hit rate (Appendix J.1–J.4), aiding reproducibility and clarity.
  - End-to-end algorithms for prompting, preference construction/training, and token-level capturing (Appendix B/Algorithms 1–3) enhance technical soundness and clarity.Weaknesses
- Missing quantitative evidence for Stage 1 claims and correlation details
  - The text references “Table 2” for Stage 1 gains (Section 4.1.1), but the table with concrete numbers is not present. No direct evidence found in the manuscript. This weakens empirical support for claims about CP-Refine/CP-Order/CP-Link comparative performance.
  - The asserted “inverse correlation” between copying degree and hallucination on RAGTruth is shown qualitatively (Figure 1), but no correlation coefficient or statistical test is provided (Section 2.2; Figure 1). This limits novelty/rigor by lacking a quantified relationship.
  - Fluency conclusions (CP-Order poorer; CP-Refine best/second-best) are stated (Section 4.1.1) without the corresponding results table/plots. No direct evidence found in the manuscript. This affects clarity and technical quality of the Stage 1 evaluation.
- Validity of Context-Parameter Copying Capturing heuristics and “logits power”
  - Classification of tokens as “contextual” or “parametric” hinges on membership heuristics (Algorithm 3: lines 12–16), which may conflate common tokens or topical overlaps; the “isMeaningless” function is not defined (Algorithm 3: line 8). This challenges technical soundness.
  - Dependence on top-K selection (Algorithm 3: line 6) lacks justification for K and sensitivity analysis; no ablation is shown to validate robustness across K values. No direct evidence found in the manuscript. This affects rigor.
  - The “commonSubstringMatching” step (Algorithm 3: line 2) is under-specified, which can bias token attribution for parametric vs. contextual knowledge. This reduces interpretability validity.
  - The logits power formula (Appendix I, Eq. 2) is nonstandard and its relation to reliance is not theoretically justified or empirically calibrated; the negation trick for Para. visualization (Figures 3; 7; 59) further complicates interpretability. This impacts technical quality.
  - Figure caption text states “Values above x=0 indicate CTX logits power; values below x=0 indicate Para. logits power,” while the x-axis is response length (Figures 3; Appendix Figure 7), creating an axis/caption inconsistency that undermines clarity of the analysis.
- Baseline fairness and scope in counterfactual evaluation
  - The main text cites “Table 1” for counterfactual gains and 12.2–24.5pp improvements (Section 4.1.2), but Table 1 is missing. No direct evidence found in the manuscript. This hurts clarity and rigor.
  - Comparisons to GPT-4o and others (Appendix Table 5) may involve different prompts/settings, and the paper does not detail standardization across methods (Appendix A/J.4), raising fairness concerns. This affects impact claims.
  - The “50× smaller than Context-DPO (18,000)” claim (Section 4.1.2; Abstract) is strong, but compute budgets, judge usage, and pair-multiplication (Algorithm 2: line 20) could offset data savings; no controlled ablation is provided. No direct evidence found in the manuscript. This affects novelty/technical quality.
- Copying degree measurement may undercount paraphrase faithfulness
  - Copy fragment detection (Algorithm 4) relies on exact subsequence matches, potentially penalizing legitimate paraphrases aligned with context. This matters for evaluation validity.
  - δ uses squared fragment lengths (Eq. 1; Section 2.1), which could over-emphasize very long verbatim spans and bias toward extractive outputs; no study shows optimality of this weighting. No direct evidence found in the manuscript.
  - No human evaluation or error analysis is provided to assess whether high κ/δ aligns with perceived faithfulness or answer quality (Appendix A lists metrics but no human study). This impacts experimental rigor.
  - Algorithm 4 contains a typesetting/implementation inconsistency (line 15 missing closing parenthesis/bracket; Appendix Algorithm 4), which impedes exact reproducibility of κ/δ computations central to the method.
- Reliance on LLM-as-Judge without calibration
  - Elo-style LLM-as-Judge tournament (Section 3.2; Figure 2; Appendix A) uses Qwen3-32B reasoning (Appendix A), but no inter-judge agreement, sensitivity to judge model choice, or calibration experiments are given. This affects technical soundness.
  - Judge prompts instruct “assume the context is absolutely correct” (Appendix J.3), which could bias against legitimate uncertainty or nuanced reasoning and favor style choices. This impacts validity.
  - The tournament diagnostics of “Twist” and “Causal” hallucinations (Section 3.2; Appendix J.3) lack ground-truth labels; thus verdicts are unverified beyond the judge. No direct evidence found in the manuscript. This affects experimental rigor.
- Reproducibility and availability inconsistencies
  - Abstract claims “All codes are available” (Abstract), while Appendix 8 says “will be made available upon publication,” creating ambiguity on current availability. This affects reproducibility.
  - The claimed 365-sample preference data is central (Section 4.1.2), but the constructed dataset, multi-criteria thresholds, and filtering statistics (Algorithm 2: line 7) are not released or fully tabulated. No direct evidence found in the manuscript.
  - Critical hyperparameters for filtering (faithfulness thresholds for AlignScore/MiniCheck; κ/δ cutoffs; perplexity bounds) are referenced (Section 3.2) but exact values are missing; only training hyperparameters are reported (Appendix D), affecting verifiability.
  - Dataset accounting is unclear: Table 3’s caption reports PubMedQA evaluated on “20,000” samples (Section 4.1.2/Table 3) whereas Appendix A/Table 4 lists PubMedQA size “1,000” and Appendix I reports “1000/1000 (100.0%)” analyzed (Appendix Figure 7), creating a numerical inconsistency that undermines clarity.
  - Training/evaluation roles are not consistently documented: Appendix A/Table 4 marks FaithEval and PubMedQA as “Train + Eval” for RQ2 while §4.1.2 emphasizes “only 365” training samples; the manuscript does not reconcile how these roles and counts relate (Appendix A/Table 4; §4.1.2), raising potential ambiguity about splits.Suggestions for Improvement
- Provide full Stage 1 quantitative evidence and correlation analyses
  - Include the missing “Table 2” with concrete numbers for hallucination, AlignScore/MiniCheck, perplexity, κ/δ, and relevance across CP-Order/Link/Refine vs. baselines (Section 4.1.1).
  - Report statistical correlation coefficients (e.g., Pearson/Spearman) and significance tests for the inverse relationship between copying degree and hallucination (Section 2.2; Figure 1), plus scatter plots with trend lines.
  - Add plots/tables supporting fluency claims (Section 4.1.1), including perplexity distributions and human readability ratings, to complement Figure 5–6.
- Strengthen and validate Context-Parameter Copying Capturing and “logits power”
  - Define “isMeaningless” precisely, list excluded token classes, and justify K with sensitivity/ablation results (Algorithm 3: lines 6–9), including performance/interpretability curves over K.
  - Provide robustness checks comparing alternative attribution heuristics (e.g., attention-over-context tokens, edit distance matching), and show convergence across methods (Algorithm 3: lines 12–16).
  - Detail “commonSubstringMatching” (Algorithm 3: line 2) and evaluate how it influences token attribution, including edge cases with partial overlaps and subword tokenization.
  - The logits power formula (Appendix I, Eq. 2) should be theoretically motivated or replaced with a standard measure (e.g., cumulative probability mass on context tokens), plus ablations contrasting different aggregation schemes; report calibration against ground-truth attributions where possible.
  - Correct axis/caption text in Figures 3 and Appendix Figure 7 to refer to the y-axis for CTX/Para. separation and explicitly document the negation used for Para. visualization; ensure captions and plotted axes are self-consistent.
- Expand baseline fairness and controlled comparisons in counterfactual settings
  - Include the missing “Table 1” with per-model prompt templates (Appendix J.4), temperature/decoding settings, and exact evaluation splits (Section 4.1.2) to ensure comparability.
  - Run matched-prompt, same-judge settings for all baselines (Appendix A/J.3–J.4), and report sensitivity to prompt variations to mitigate unfair advantages.
  - Provide controlled ablations isolating data size vs. pair amplification (Algorithm 2: line 20) and judge/tournament effects to substantiate the “50× smaller” efficiency claim (Abstract; §4.1.2).
- Complement κ/δ with paraphrase-aware faithfulness measures
  - Augment Algorithm 4 with fuzzy matching (e.g., token-level stemming or semantic overlap via embeddings) and report κ/δ variants that capture paraphrases (Appendix C).
  - Evaluate alternative δ weightings (linear, log-length) and show how different choices affect the faithfulness–fluency trade-off (Section 2.1), including calibration on human judgments.
  - Add a small-scale human evaluation assessing perceived faithfulness and readability for high κ/δ outputs vs. paraphrase-heavy faithful answers to validate metric alignment (Appendix A).
  - Correct Algorithm 4 typesetting (Appendix Algorithm 4, line 15) and provide the exact implementation used for κ/δ in code to ensure reproducibility.
- Calibrate the LLM-as-Judge pipeline
  - Report inter-judge agreement by comparing multiple judges and show verdict stability under model changes and prompt perturbations (Appendix J.3; Section 3.2).
  - Introduce control tasks with ground-truth hallucination labels to validate Twist/Causal detection accuracy and calibrate Elo ratings (Appendix J.3).
  - Allow judges to express uncertainty or tie-breakers based on explicit criteria rather than enforcing “context is absolutely correct” in all cases; add an “Unknown/Unsure” outcome and analyze its frequency (Appendix J.3).
- Resolve reproducibility and availability gaps
  - Align the Abstract and Appendix 8 statements by releasing the full code and scripts now, including the preference dataset constructed from 365 samples (Abstract; Appendix 8).
  - Publish the multi-criteria filtering thresholds and distributions (Section 3.2; Algorithm 2: line 7), plus per-dataset acceptance rates, so others can replicate selection.
  - Provide configuration files for all models, judge prompts, and decoding parameters (Appendix D; Appendix J), and deposit the exact train/eval splits and random seeds used.
  - Reconcile the PubMedQA sample count across sections by correcting Table 3’s caption or the appendix to a consistent number (Section 4.1.2/Table 3; Appendix A/Table 4; Appendix Figure 7).
  - Explicitly list training/validation/test splits for RQ2 and clarify how the “365” training samples were derived relative to datasets marked “Train + Eval” (Appendix A/Table 4; §4.1.2), ensuring no train–test leakage.Score
- Overall (10): 7 — Strong idea with broad evaluation and clear gains (Table 3; Appendix Table 5; Figures 3–4), but missing Stage 1 table and some methodological validations, plus caption/axis and dataset-accounting inconsistencies (Section 4.1.1; Appendix I; Section 4.1.2/Table 3; Appendix A/Table 4).
- Novelty (10): 8 — Two-stage copy-paste preference alignment and token-level CoT attribution are distinct contributions (Sections 3.1–3.3; Algorithms 1–3; Figure 2).
- Technical Quality (10): 6 — Solid pipeline and DPO implementation (Appendix D; Algorithm 2) but incomplete Stage 1 evidence, heuristic attribution, and limited calibration (Section 4.1.1; Algorithm 3; Appendix I).
- Clarity (10): 6 — Clear formalization and prompts (Eq. 1; Appendix J; Algorithms 1–4), yet key tables are missing and there are caption/axis and dataset-count inconsistencies (Section 4.1.1; Figures 3/Appendix Figure 7; Section 4.1.2/Table 3; Appendix A/Table 4).
- Confidence (5): 4 — Good detail across methods and datasets (Appendix A/D/J), but absent Stage 1 table and attribution validation, and unresolved dataset-accounting issues temper certainty (Section 4.1.1; Appendix I; Section 4.1.2/Table 3; Appendix A/Table 4).