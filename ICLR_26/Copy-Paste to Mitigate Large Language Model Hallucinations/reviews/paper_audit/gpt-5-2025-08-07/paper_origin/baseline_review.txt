Summary
- The paper addresses contextual faithfulness in RAG by encouraging models to directly reuse context text, positing that higher copying degree reduces hallucinations. It introduces CopyPaste, a two-stage approach: (i) three prompting methods (CP-Order, CP-Link, CP-Refine) to produce high-copying responses, and (ii) CopyPasteLLM, trained via DPO on automatically constructed preference pairs emphasizing contextual trust. The authors define copy coverage κ and copy density δ (Eq. 1) and add an interpretability tool, Context-Parameter Copying Capturing (Algorithm 3), plus a “logits power” analysis (Appendix I). Experiments on FaithEval, ConFiQA, PubMedQA, and RAGTruth claim substantial gains, including large improvements in counterfactual settings with only 365 training samples (Abstract; §4.1.2; Table 3; Appendix Table 5). The method aims to reduce reliance on parametric knowledge while maintaining contextual representations (Figures 3–4; Appendix Figures 7–9).Strengths
- Bold operationalization of faithfulness via direct copying
  - Clear formalization of copy metrics κ and δ with equation and copy-fragment detection (Eq. 1; Appendix C/Algorithm 4), grounding the core idea in measurable signals. This matters for technical soundness by defining explicit optimization targets.
  - Three complementary prompting regimes—hard (CP-Order), semi-hard (CP-Link), and soft iterative (CP-Refine)—provide controllable ways to elevate copying (Section 3.1; Figures 13; 10). This is novel and clarifies how constraints trade off fluency and faithfulness.
  - Pipeline transforms generated responses into structured preferences and filters by copying strength (κ, δ), faithfulness (AlignScore, MiniCheck), relevance, and fluency (Section 3.2; Figure 14). This supports experimental rigor via multi-criteria quality control.- Data-efficient DPO alignment with automated preference construction
  - Multi-stage selection, Elo-style LLM-as-Judge tournament, and “stamping” gold/wrong answers yield roughly five preference pairs per item (Section 3.2; Figure 14; Algorithm 2). This enhances sample efficiency and impact by amplifying supervision from limited data.
  - The training uses parameter-efficient LoRA with transparent hyperparameters (Appendix D: r=64, α=128, β=0.3; epochs and optimizer settings), supporting reproducibility and technical soundness.
  - The text claims only 365 query-context pairs achieve large gains over strong baselines (Abstract; §4.1.2). If sustained, this would be impactful for practitioners constrained by data.- Broad and carefully structured evaluation suite
  - Four datasets spanning daily life, science counterfactuals, biomedicine, and Wikidata (Appendix A/Table 4), enabling cross-domain assessment. This matters for impact and generality.
  - Multiple evaluation axes—faithfulness (AlignScore; MiniCheck), hallucination via LLM-as-Judge with Twist and Causal prompts (Appendix A/J.3), fluency (GPT-2 perplexity), copying (κ, δ), and relevance (Qwen3-Embedding) (Appendix A). The breadth strengthens experimental rigor.
  - Non-counterfactual performance gains in Table 3 across ConFiQA subsets and PubMedQA (Section 4.1.2/Table 3), demonstrating utility beyond counterfactual stress tests.- Mechanistic analysis beyond final answers
  - Context-Parameter Copying Capturing (Section 3.3; Algorithm 3) extends Knowledge Token Capturing to CoT trajectories, collecting top-K tokens’ probabilities and hidden states with/without context. This advances interpretability by moving to sequence-level analysis.
  - Logits power and UMAP visualizations (Figures 3–4; Appendix I/Figure 7–9) show earlier and stronger contextual reliance and distinct parametric distributions for CopyPasteLLM. This provides technical evidence of the claimed “recalibration” of parametric knowledge reliance.
  - Position-aware results note earlier peak contextual usage in CopyPasteLLM than base (Section 4.2; Figures 3–4), aligning with the paper’s thesis about contextual trust dynamics.- Clear articulation of limitations and ethics
  - Limitations discuss incomplete context scenarios and multimodal extension challenges (Appendix G/H), which supports responsible framing and impact.
  - Ethics statement acknowledges risk of copying incorrect source material (Appendix 7), signaling awareness of deployment trade-offs.- Comprehensive prompt transparency and procedural formalization
  - Detailed prompts for CP-Order/Link/Refine, baselines, judges, and accuracy/hit rate (Appendix J.1–J.4), aiding reproducibility and clarity.
  - End-to-end algorithms for prompting, preference construction/training, and token-level capturing (Appendix B/Algorithms 1–3) enhance technical soundness and clarity.Weaknesses
- Missing quantitative evidence for Stage 1 claims and correlation details
  - The text references “Table 2” for Stage 1 gains (Section 4.1.1), but the table with concrete numbers is not present. No direct evidence found in the manuscript. This weakens empirical support for claims about CP-Refine/CP-Order/CP-Link comparative performance.
  - The asserted “inverse correlation” between copying degree and hallucination on RAGTruth is shown qualitatively (Figure 1), but no correlation coefficient or statistical test is provided (Section 2.2; Figure 1). This limits novelty/rigor by lacking a quantified relationship.
  - Fluency conclusions (CP-Order poorer; CP-Refine best/second-best) are stated (Section 4.1.1) without the corresponding results table/plots. No direct evidence found in the manuscript. This affects clarity and technical quality of the Stage 1 evaluation.- Validity of Context-Parameter Copying Capturing heuristics and “logits power”
  - Classification of tokens as “contextual” or “parametric” hinges on membership heuristics (Algorithm 3: lines 12–16), which may conflate common tokens or topical overlaps; the “isMeaningless” function is not defined (Algorithm 3: line 8). This challenges technical soundness.
  - Dependence on top-K selection (Algorithm 3: line 6) lacks justification for K and sensitivity analysis; no ablation is shown to validate robustness across K values. No direct evidence found in the manuscript. This affects rigor.
  - The “commonSubstringMatching” step (Algorithm 3: line 2) is under-specified, which can bias token attribution for parametric vs. contextual knowledge. This reduces interpretability validity.
  - The logits power formula (Appendix I, Eq. 2) is nonstandard and its relation to reliance is not theoretically justified or empirically calibrated; the negation trick for Para. visualization (Figures 3; 25; 59) further complicates interpretability. This impacts technical quality.- Baseline fairness and scope in counterfactual evaluation
  - The main text cites “Table 1” for counterfactual gains and 12.2–24.5pp improvements (Section 4.1.2), but Table 1 is missing. No direct evidence found in the manuscript. This hurts clarity and rigor.
  - Comparisons to GPT-4o and others (Appendix Table 5) may involve different prompts/settings, and the paper does not detail standardization across methods (Appendix A/J.4), raising fairness concerns. This affects impact claims.
  - The “50× smaller than Context-DPO (18,000)” claim (Section 4.1.2; Abstract) is strong, but compute budgets, judge usage, and pair-multiplication (Algorithm 2: line 20) could offset data savings; no controlled ablation is provided. No direct evidence found in the manuscript. This affects novelty/technical quality.- Copying degree measurement may undercount paraphrase faithfulness
  - Copy fragment detection (Algorithm 4) relies on exact subsequence matches, potentially penalizing legitimate paraphrases aligned with context. This matters for evaluation validity.
  - δ uses squared fragment lengths (Eq. 1; Section 2.1), which could over-emphasize very long verbatim spans and bias toward extractive outputs; no study shows optimality of this weighting. No direct evidence found in the manuscript.
  - No human evaluation or error analysis is provided to assess whether high κ/δ aligns with perceived faithfulness or answer quality (Appendix A lists metrics but no human study). This impacts experimental rigor.- Reliance on LLM-as-Judge without calibration
  - Elo-style LLM-as-Judge tournament (Section 3.2; Figure 14) uses Qwen3-32B reasoning (Appendix A), but no inter-judge agreement, sensitivity to judge model choice, or calibration experiments are given. This affects technical soundness.
  - Judge prompts instruct “assume the context is absolutely correct” (Appendix J.3), which could bias against legitimate uncertainty or nuanced reasoning and favor style choices. This impacts validity.
  - The tournament diagnostics of “Twist” and “Causal” hallucinations (Section 3.2; Appendix J.3) lack ground-truth labels; thus verdicts are unverified beyond the judge. No direct evidence found in the manuscript. This affects experimental rigor.- Reproducibility and availability inconsistencies
  - Abstract claims “All codes are available” (Abstract), while Appendix 8 says “will be made available upon publication,” creating ambiguity on current availability. This affects reproducibility.
  - The claimed 365-sample preference data is central (Section 4.1.2), but the constructed dataset, multi-criteria thresholds, and filtering statistics (Algorithm 2: line 7) are not released or fully tabulated. No direct evidence found in the manuscript.
  - Critical hyperparameters for filtering (faithfulness thresholds for AlignScore/MiniCheck; κ/δ cutoffs; perplexity bounds) are referenced (Section 3.2) but exact values are missing; only training hyperparameters are reported (Appendix D), affecting verifiability.Suggestions for Improvement
- Provide full Stage 1 quantitative evidence and correlation analyses
  - Include the missing “Table 2” with concrete numbers for hallucination, AlignScore/MiniCheck, perplexity, κ/δ, and relevance across CP-Order/Link/Refine vs. baselines (Section 4.1.1).
  - Report statistical correlation coefficients (e.g., Pearson/Spearman) and significance tests for the inverse relationship between copying degree and hallucination (Section 2.2; Figure 1), plus scatter plots with trend lines.
  - Add plots/tables supporting fluency claims (Section 4.1.1), including perplexity distributions and human readability ratings, to complement Figure 5–6.- Strengthen and validate Context-Parameter Copying Capturing and “logits power”
  - Define “isMeaningless” precisely, list excluded token classes, and justify K with sensitivity/ablation results (Algorithm 3: lines 6–9), including performance/interpretability curves over K.
  - Provide robustness checks comparing alternative attribution heuristics (e.g., attention-over-context tokens, edit distance matching), and show convergence across methods (Algorithm 3: lines 12–16).
  - Detail “commonSubstringMatching” (Algorithm 3: line 2) and evaluate how it influences token attribution, including edge cases with partial overlaps and subword tokenization.
  - The logits power formula (Appendix I, Eq. 2) should be theoretically motivated or replaced with a standard measure (e.g., cumulative probability mass on context tokens), plus ablations contrasting different aggregation schemes; report calibration against ground-truth attributions where possible.- Expand baseline fairness and controlled comparisons in counterfactual settings
  - Include the missing “Table 1” with per-model prompt templates (Appendix J.4), temperature/decoding settings, and exact evaluation splits (Section 4.1.2) to ensure comparability.
  - Run matched-prompt, same-judge settings for all baselines (Appendix A/J.3–J.4), and report sensitivity to prompt variations to mitigate unfair advantages.
  - Provide controlled ablations isolating data size vs. pair amplification (Algorithm 2: line 20) and judge/tournament effects to substantiate the “50× smaller” efficiency claim (Abstract; §4.1.2).- Complement κ/δ with paraphrase-aware faithfulness measures
  - Augment Algorithm 4 with fuzzy matching (e.g., token-level stemming or semantic overlap via embeddings) and report κ/δ variants that capture paraphrases (Appendix C).
  - Evaluate alternative δ weightings (linear, log-length) and show how different choices affect the faithfulness–fluency trade-off (Section 2.1), including calibration on human judgments.
  - Add a small-scale human evaluation assessing perceived faithfulness and readability for high κ/δ outputs vs. paraphrase-heavy faithful answers to validate metric alignment (Appendix A).- Calibrate the LLM-as-Judge pipeline
  - Report inter-judge agreement by comparing multiple judges and show verdict stability under model changes and prompt perturbations (Appendix J.3; Section 3.2).
  - Introduce control tasks with ground-truth hallucination labels to validate Twist/Causal detection accuracy and calibrate Elo ratings (Appendix J.3).
  - Allow judges to express uncertainty or tie-breakers based on explicit criteria rather than enforcing “context is absolutely correct” in all cases; add an “Unknown/Unsure” outcome and analyze its frequency (Appendix J.3).- Resolve reproducibility and availability gaps
  - Align the Abstract and Appendix 8 statements by releasing the full code and scripts now, including the preference dataset constructed from 365 samples (Abstract; Appendix 8).
  - Publish the multi-criteria filtering thresholds and distributions (Section 3.2; Algorithm 2: line 7), plus per-dataset acceptance rates, so others can replicate selection.
  - Provide configuration files for all models, judge prompts, and decoding parameters (Appendix D; Appendix J), and deposit the exact train/eval splits and random seeds used.Score
- Overall (10): 7 — Strong idea with broad evaluation and clear gains (Table 3; Appendix Table 5; Figures 3–4), but missing Stage 1 table and some methodological validations (Section 4.1.1; Appendix I).
- Novelty (10): 8 — Two-stage copy-paste preference alignment and token-level CoT attribution are distinct contributions (Sections 3.1–3.3; Algorithms 1–3; Figure 14).
- Technical Quality (10): 6 — Solid pipeline and DPO implementation (Appendix D; Algorithm 2) but incomplete Stage 1 evidence, heuristic attribution, and limited calibration (Section 4.1.1; Algorithm 3; Appendix I).
- Clarity (10): 7 — Clear formalization and prompts (Eq. 1; Appendix J; Algorithms 1–4), yet some key tables/thresholds are missing and availability statements conflict (Section 4.1.1; Appendix 8).
- Confidence (5): 4 — Good detail across methods and datasets (Appendix A/D/J), but absent Stage 1 table and attribution validation temper certainty (Section 4.1.1; Appendix I).