1) Summary
This paper addresses the problem of contextual faithfulness in Retrieval-Augmented Generation (RAG), where large language models (LLMs) may hallucinate by ignoring provided context in favor of their internal parametric knowledge. The authors observe an inverse correlation between the degree to which a model copies from the context and its hallucination rate. Based on this, they propose CopyPasteLLM, a two-stage framework. First, three "CopyPaste-Prompting" methods are used to generate high-copying responses, which serve as preferred examples. Second, this preference data is used to fine-tune a base LLM using Direct Preference Optimization (DPO), teaching it to favor context-grounded, high-copying outputs. The resulting model shows significant improvements in faithfulness on several benchmarks, notably achieving 12.2%-24.5% higher accuracy on FaithEval with 50x less training data than a strong baseline. A novel analysis method is proposed to reveal the model's knowledge sourcing.2) Strengths
*   **Novel and Well-Motivated Core Idea**
    *   The central hypothesis—that a higher degree of copying from the context can serve as a proxy for and a mechanism to improve contextual faithfulness—is intuitive and compelling. This idea is grounded in an empirical observation of an inverse correlation between copying degree and hallucination on the RAGTruth dataset (Figure 1, Section 2.2).
    *   This approach elegantly connects the problem of faithfulness to the problem of attribution. By design, responses with a high copy degree are inherently attributable to the source context, addressing a key challenge in RAG systems (Section 1, Section 5).
    *   The formalization of "copying degree" using Copy Coverage (κ) and Copy Density (δ) provides a quantifiable and optimizable objective for the proposed methods (Section 2.1, Equation 1).*   **High Reported Performance and Data Efficiency**
    *   The proposed CopyPasteLLM is reported to achieve state-of-the-art results, significantly outperforming strong baselines on challenging counterfactual benchmarks. On FaithEval, it is claimed to improve accuracy by 12.2 to 24.5 percentage points over the best baseline across three different models (Table 1, Section 4.1.2).
    *   The method is presented as remarkably data-efficient. The authors report achieving these results using preference data generated from only 365 query-context pairs, which is stated to be 1/50th of the data used by the Context-DPO baseline (Section 4.1.2).
    *   The performance gains are reported to be consistent across multiple models (Mistral-7B, Llama-3-8B, Llama-3.1-8B) and datasets, including both counterfactual (FaithEval, ConFiQA) and standard QA settings (PubMedQA) (Table 1, Table 3).*   **Comprehensive and Automated Methodological Pipeline**
    *   The two-stage framework is well-structured. Stage 1 provides a systematic way to generate high-quality preference data through three distinct prompting strategies (CP-Order, CP-Link, CP-Refine) that explore a spectrum from hard to soft constraints (Section 3.1, Figure 2).
    *   The preference data construction pipeline in Stage 2 is fully automated, incorporating multi-criteria filtering, an LLM-as-judge tournament for ranking, and an "answer stamping" technique to create informative preference pairs (Section 3.2, Algorithm 2). This removes the need for expensive human annotation.
    *   The use of Direct Preference Optimization (DPO) is a suitable and effective choice for internalizing the desired high-copying behavior into the model's policy (Section 3.2, Algorithm 2).*   **Proposes a Novel Method for Mechanistic Analysis**
    *   The paper proposes the Context-Parameter Copying Capturing algorithm, a tool for token-level analysis of knowledge sourcing during generation (Section 3.3, Algorithm 3).
    *   This method extends prior work by applying the analysis to the entire Chain-of-Thought reasoning process, rather than just short final answers, which could provide more granular insights (Section 3.3).
    *   The algorithm is designed to capture and differentiate between contextual and parametric knowledge usage by running generation with and without context and analyzing the top-K tokens, which is a reasonable approach for this type of analysis (Algorithm 3).3) Weaknesses
*   **Critical Issues in Experimental Reporting and Validity**
    *   There is a major, unexplained contradiction regarding the training data size. The abstract and main text repeatedly claim training was performed with data derived from "only 365 training samples" (Abstract, Section 4.1.2), but Appendix A states that RAGTruth (839 samples), FaithEval (1,000 samples), and PubMedQA (1,000 samples) were all used for training (Table 4). This discrepancy fundamentally undermines the central claim of data efficiency.
    *   The experimental design suggests potential data contamination. Table 4 in Appendix A indicates that the FaithEval and PubMedQA datasets were used for both "Train" and "Eval". The manuscript does not state whether disjoint splits were used, raising serious concerns that the model may have been evaluated on data it was trained on, which would invalidate the reported performance metrics.*   **Potential Integrity Issues in Analysis Results**
    *   The UMAP visualizations presented in the appendix appear to contain duplicated figures, which calls the integrity of the mechanistic analysis into question. The plots for the Mistral-7B-Instruct-v0.2 model are visually identical for the FaithEval dataset (Appendix Figure 8) and the RAGTruth dataset (Appendix Figure 9). It is highly improbable for UMAP projections on two different datasets to be identical, suggesting a reporting error where one result was copied over the other. This undermines the conclusions drawn in Section 4.2 about how the model works.*   **Unclear Scalability and Cost of Preference Generation**
    *   The preference data generation in Stage 1 relies on multiple LLM calls per sample, especially for the iterative CP-Refine method (Section 3.1, Algorithm 1). The computational cost of this process for generating larger-scale datasets is not discussed.
    *   The quality of the generated preference data may be highly dependent on the capability of the generator model (DeepSeek-V3-0324, Appendix D). The robustness of the prompting methods across different generator models is not explored.
    *   The "50x" data efficiency claim is ambiguous. The paper compares its "365 query-context pairs" to the baseline's "18,000 samples" (Section 4.1.2), but the proposed method generates ~5 preference pairs per source sample (Algorithm 2). It is unclear if the baseline's 18,000 samples refers to source pairs or final preference pairs, making the comparison difficult to verify.*   **Potential Negative Trade-offs are Under-explored**
    *   The strong emphasis on copying might negatively impact other desirable qualities like conciseness, synthesis of information, and linguistic naturalness. The paper lacks a qualitative or human evaluation of the final CopyPasteLLM outputs to assess these potential trade-offs.
    *   The framework's effectiveness is predicated on the context containing a complete and correct answer. The paper acknowledges this limitation in the appendix (Appendix G), but it is a critical point for real-world RAG applications that deserves more prominent discussion.
    *   The evaluation metrics for RQ2 (Accuracy and Hit Rate) are primarily focused on correctness in multiple-choice or short-answer settings (Appendix A, Section 4.1.2), which may not fully capture the quality of longer, free-form responses.*   **Ambiguities in the Analysis Methodology**
    *   The Context-Parameter Copying Capturing algorithm (Algorithm 3) contains several implementation details that lack clarity. For instance, the criterion for identifying "meaningless tokens" (line 8) is not defined.
    *   The logic for categorizing a token as contextual (`x_j in C`) versus parametric (`x_j in A_para`) is presented as mutually exclusive, but common words could appear in both, and the algorithm's handling of this overlap is not specified (Algorithm 3, lines 12-17).
    *   The "logits power" metric is defined late in the appendix (Appendix I, Equation 2) and its mathematical motivation is not explained, making the interpretation of Figure 3 less grounded.4) Suggestions for Improvement
*   **Clarify Experimental Setup and Ensure Valid Evaluation**
    *   Please resolve the contradiction regarding the training data size. The manuscript must clearly state the exact origin and composition of the 365 samples and explain how this relates to the datasets listed as "Train" in Table 4.
    *   Please explicitly confirm whether strictly disjoint splits of FaithEval and PubMedQA were used for training and evaluation. If there was overlap, the experiments must be re-run with a clean separation to ensure the validity of the results.*   **Verify and Correct the Mechanistic Analysis**
    *   Please carefully review and correct the UMAP visualizations in the appendix. The apparent duplication between Figure 8 and Figure 9 for the Mistral-7B model must be addressed. If it is an error, the correct figure should be provided, and the conclusions in Section 4.2 should be revisited based on the corrected results.*   **Discuss Scalability and Refine Efficiency Claims**
    *   Please add a discussion about the computational cost (e.g., number of API calls, tokens processed per sample) of the Stage 1 preference generation pipeline to help readers assess its practicality.
    *   Please include a brief ablation or discussion on how the choice of the generator model affects the quality of the preference data and the final performance.
    *   Please clarify the basis for the "50x" data efficiency claim by specifying whether the baseline's "18,000 samples" refers to source documents or final preference pairs, ensuring an apples-to-apples comparison.*   **Conduct a More Holistic Evaluation of Trade-offs**
    *   Please consider adding a small-scale human evaluation comparing outputs from CopyPasteLLM and a baseline on dimensions like naturalness, conciseness, and ability to synthesize information.
    *   Please move the discussion of key limitations, particularly the assumption of context completeness, from Appendix G into a dedicated "Limitations" section in the main paper.
    *   Consider adding a qualitative analysis with a few examples of generated outputs in the appendix to showcase the model's behavior and illustrate the differences between the CopyPasteLLM and baseline responses.*   **Clarify the Analysis Algorithm and Metrics**
    *   Please refine the description of Algorithm 3 by explicitly defining what constitutes a "meaningless token" (e.g., a list of stop words).
    *   Please clarify the logic in Algorithm 3 for handling tokens that could be classified as both contextual and parametric.
    *   Please introduce the "logits power" formula (Equation 2) in the main methodology section (e.g., Section 3.3) and include a brief justification for its specific mathematical form.5) Score
*   Overall (10): 3 — The paper proposes an interesting idea, but critical flaws in experimental reporting, potential data contamination, and apparent duplication of results severely undermine the validity of its central claims (Table 4, Figure 8, Figure 9).
*   Novelty (10): 6 — The core heuristic of using high copy-degree as a preference signal for DPO is a novel engineering contribution, though the broader framework of using preference optimization for faithfulness may have precedents.
*   Technical Quality (10): 2 — The technical quality is severely compromised by a major contradiction in reported training data size, potential train-test contamination, and duplicated figures in the analysis (Table 4, Appendix A, Figure 8, Figure 9).
*   Clarity (10): 5 — While generally well-written, the paper contains critical ambiguities and contradictions regarding the experimental setup that make it difficult to understand what was actually done (Section 4.1.2 vs. Table 4).
*   Confidence (5): 5 — I am highly confident in my assessment, as the identified issues are based on direct contradictions and evidence found within the manuscript and its appendix.