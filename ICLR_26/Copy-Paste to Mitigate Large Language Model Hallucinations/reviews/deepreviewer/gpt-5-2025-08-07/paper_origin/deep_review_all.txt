Review 1

Summary
The paper proposes CopyPasteLLM, a two-stage framework to improve contextual faithfulness in RAG: Stage 1 generates high-copying answers via three prompting strategies (CP-Order, CP-Link, CP-Refine); Stage 2 converts these into preference pairs and trains with DPO to internalize “trust in context.” Faithfulness is operationalized by copy coverage κ and copy density δ (Eq. 1), and an interpretability tool, Context-Parameter Copying Capturing (Algorithm 3), is introduced to probe reliance on contextual vs parametric knowledge during CoT. The method reports large gains on FaithEval and ConFiQA while using only 365 training samples (Tables 1, 3; Appendix Table 5).

Soundness
- The core hypothesis—that higher copying reduces context-unfaithful hallucinations—is plausible and supported by an exploratory analysis on RAGTruth (Figure 1; §2.2), but causality is not established and the correlation is not quantified (no coefficient or statistical test reported).
- The two-stage pipeline is logically consistent: (i) produce high-copy candidates under hard/soft constraints (Algorithm 1; §3.1), then (ii) align preferences via multi-criteria filtering, Elo LLM-as-judge, and answer stamping before DPO (Algorithm 2; §3.2). The alignment relies on multiple metrics (AlignScore, MiniCheck, κ, δ, embedding relevance, perplexity; §3.2; Appendix A–D), which is sensible, though perplexity with GPT-2 may be a weak fluency proxy for modern LLM outputs (Appendix A).
- The interpretability method (Algorithm 3) extends KTC to token-level CoT trajectories; however, the proxy for “parametric knowledge” (top-K tokens preferred in context-free runs) and the “isMeaningless” filter are heuristic, and validation is primarily visual (Figures 3–4; Appendix Figures 7–9) without formal statistical testing (§4.2; Appendix I).
- Training claims “not introducing additional parametric knowledge through LoRA,” yet LoRA modifies parameters; the intended claim is that external knowledge is not added (but the statement could be clarified, §4.1.2; Appendix D, §3.2).

Presentation
- The paper is clearly organized with a motivating figure (Figure 1), a pipeline diagram (Figure 2), and step-by-step algorithms (Algorithms 1–4), making the approach easy to follow (§3; Appendix B–D).
- Empirical sections reference major numbers and datasets, and figures summarizing copying degree and query relevance are helpful (Appendix Figures 5–6), but several central tables are only referred to without being shown in the main body (Table 2, Table 1), which complicates verification (§4.1–4.2; Appendix F).
- Minor inconsistencies: “All codes are available at” (Abstract) vs “implementation will be made available upon publication” (Appendix §8), and Table 4 indicates “Train + Eval” for PubMedQA while §4.1.2 states “none used for training,” which should be reconciled (Appendix A; §4.1.2).

Contribution
- Conceptual: Treat copying degree as an operational proxy for contextual faithfulness and internalize it via preference training (§3.1–3.2); provide an interpretable probe for context vs parametric usage (§3.3).
- Empirical: Large improvements on counterfactual FaithEval and ConFiQA with extreme data efficiency—365 samples producing ~5× preference pairs (§4.1.2; Appendix D).
- Practical: A fully automated pipeline, including LLM-as-judge tournaments and multi-criteria filtering, potentially useful for practitioners (Figure 2; §3.2; Appendix J).

Strengths
- Strong, consistent gains across multiple datasets and models, including difficult counterfactual settings (Table 3; Appendix Table 5; §4.1.2).
- Data-efficient DPO via preference pair inflation and careful candidate curation (Algorithm 2; §3.2; Appendix D).
- Clear, reproducible prompting recipes and algorithms (Appendix J; Algorithms 1–4).
- Insightful qualitative mechanism: recalibration of parametric knowledge confidence rather than altering contextual representations (Figure 4; §4.2).

Weaknesses
- Causality between copying and hallucination is not rigorously established; the motivating observation remains correlational (Figure 1; §2.2).
- Evaluation depends heavily on LLM-as-judge (Elo tournaments for Twist/Causal), which may introduce bias; human evaluation or objective checks are limited (Appendix A, J.3; §3.2).
- Perplexity with GPT-2 is an outdated fluency metric for long-form RAG responses (Appendix A).
- Minor but notable inconsistencies in data usage and code availability statements (Appendix A vs §4.1.2; Abstract vs Appendix §8).

Questions
1. Can you report the quantitative correlation (e.g., Pearson/Spearman with confidence intervals) between κ/δ and hallucination rates on RAGTruth, controlling for answer length (Figure 1; §2.2)?
2. How sensitive are results to the choice of LLM-as-judge and tournament protocol; do adjudications agree across different judges (Appendix J.3; §3.2)?
3. Could you replace GPT-2 perplexity with a contemporary fluency metric or human readability ratings to validate the trade-off (Appendix A)?
4. Please clarify PubMedQA training usage (Appendix Table 4 vs §4.1.2) and reconcile the code availability statements (Abstract vs Appendix §8).
5. Is there an ablation on “answer stamping” (gold/wrong append) to show its incremental benefit over standard preference alignment (Algorithm 2; §3.2)?

Rating
- Overall (10): 8 — Strong empirical improvements with an elegant pipeline and interpretable analysis, but core hypothesis is supported by correlation rather than causation (Figure 1; §4.1.2; Figure 2; §3.3).
- Novelty (10): 7 — High-copy prompting combined with automated preference alignment and a token-level CoT probe is a fresh synthesis, though each component relates to existing lines (Algorithms 1–3; §3; §5).
- Technical Quality (10): 7 — Methodology is coherent with multi-criteria filtering and DPO, yet reliance on LLM-as-judge and heuristic interpretability entail validation gaps (§3.2; §4.2; Appendix A, I).
- Clarity (10): 8 — The pipeline and algorithms are clearly presented with helpful figures, but missing main-body tables and minor inconsistencies hamper verification (Figure 2; §4; Appendix A, §8).
- Confidence (5): 4 — High confidence based on thorough reading of methods, algorithms, and Appendix, but some empirical details (tables, evaluation choices) are not fully presented in the main text.


Review 2

Summary
The paper argues that increasing verbatim reuse from context leads to fewer hallucinations in RAG, and builds a two-stage system: generate high-copy candidates (CP-Order/Link/Refine) and train CopyPasteLLM via DPO on preference pairs filtered by faithfulness, relevance, and fluency. It introduces Context-Parameter Copying Capturing to study knowledge source reliance and reports large data-efficient gains on FaithEval, ConFiQA, and PubMedQA (§3–4; Appendix).

Soundness
- The CopyPaste-Prompting procedures are reasonable and enforce lexical fidelity, but the optimization of copying can trade off readability and coverage of nuanced facts (§3.1; Appendix J.1).
- The filtering and Elo-style judging provide a principled way to select preferences, yet judgment quality and bias are under-discussed; agreement among judges or calibration isn’t reported (Appendix A; J.3; §3.2).
- The interpretability approach (Algorithm 3) uses a heuristic mapping of tokens to context/parametric sources and relies on top-K probabilities; without ground truth for token-level provenance, conclusions remain suggestive (Figures 3–4; §4.2).
- Some reporting inconsistencies raise questions: PubMedQA is “Train + Eval” in Appendix Table 4, but §4.1.2 says none of PubMedQA was used for training; code is “available” (Abstract) vs “will be made available” (Appendix §8).

Presentation
- The paper is generally well-structured with explicit algorithms and pipeline diagrams (Figure 2; Algorithms 1–4).
- However, several key empirical artifacts are referenced but not shown in the main body (e.g., Table 2 for Stage 1, Table 1 for FaithEval baselines), pushing critical evidence to the Appendix, which weakens the immediate verifiability (§4.1; §4.1.2; Appendix F).
- Figures are informative but some captions are overly general or rely on placeholders in the Appendix (Appendix Figures 8–9).

Contribution
- Offers a clear operational lever—copying degree—for improving faithfulness and an automated preference-training pipeline to internalize this behavior (§3.1–3.2).
- Proposes a token-level CoT probe that extends KTC beyond short answers (§3.3).
- Demonstrates strong performance with notably small training data (§4.1.2), which is practically impactful.

Strengths
- Data efficiency and large gains in counterfactual settings (FaithEval; ConFiQA MR/MC) suggest the approach targets a real failure mode (§4.1.2; Table 3; Appendix Table 5).
- Method is reproducible in principle with detailed prompting templates and algorithms (Appendix J; Algorithms 1–4).
- A unified multi-criteria filter avoids myopically maximizing copy rates at the expense of relevance/fluency (§3.2; Appendix A).

Weaknesses
- Over-reliance on LLM-as-judge for ranking hallucination severity may bias preference construction, and inter-judge reliability is not measured (Appendix J.3; §3.2).
- Copy metrics κ, δ reward verbatim reuse but may not penalize irrelevant copying; the paper mitigates with relevance filters but does not show error analyses of “high copy yet wrong” cases (Appendix A; §3.1).
- The interpretability findings are primarily visual and lack statistical tests or counterfactual controls (Figures 3–4; Appendix I).
- Reporting and availability inconsistencies (Appendix A vs §4.1.2; Abstract vs Appendix §8).

Questions
1. Do you have inter-annotator (inter-judge) agreement statistics for Elo rankings across different LLM judges or seeds (Appendix J.3; §3.2)?
2. Can you provide an error analysis of cases where κ/δ are high but answers are incorrect, to understand failure modes of the copy-first strategy (Appendix Figures 5–6; §4.1.1)?
3. What is the correlation between copying degree and answer length; is length a confound (Figure 1; §2.2)?
4. Please reconcile PubMedQA training usage and code availability statements (Appendix A; §4.1.2; Abstract; Appendix §8).
5. Could you include baselines with copy-aware decoding (e.g., CoCoLex) in Stage 1 and additional non-copying strong finetunes in Stage 2 for completeness (Appendix A; §4.1.2)?

Rating
- Overall (10): 6 — Useful, data-efficient method with strong empirical gains, but evaluation relies on LLM-as-judge and several reporting inconsistencies reduce trust (Figure 2; §4.1.2; Appendix A, J.3).
- Novelty (10): 6 — Combining high-copy prompting with preference alignment is an incremental but practical idea, and the CoT probe is a modest extension of KTC (§3.1–3.3; §5).
- Technical Quality (10): 5 — The pipeline is sound, yet causality is not demonstrated, interpretability validation is heuristic, and robustness to judge choices is not shown (Figure 1; §4.2; Appendix I, J.3).
- Clarity (10): 7 — Clear algorithms and diagrams, but key results are relegated to the Appendix and some statements conflict (Figure 2; §4; Appendix A, §8).
- Confidence (5): 4 — Reasonably confident based on detailed reading, but missing main-body tables and reliance on judge-based evaluation prevent higher confidence.


Review 3

Summary
This work introduces a copy-paste paradigm for RAG and argues that encouraging verbatim reuse of context (quantified by κ and δ) fosters contextual trust and reduces hallucinations. It implements three prompting variants to produce high-copy responses and trains CopyPasteLLM via DPO on curated preference pairs. To explain effects, the authors propose Context-Parameter Copying Capturing (Algorithm 3) to track token-level reliance on contextual vs parametric knowledge during CoT; they report earlier and stronger contextual engagement and suppressed parametric confidence (Figures 3–4; §4.2).

Soundness
- The copy-first assumption is defensible in counterfactual RAG, but it is not theoretically guaranteed to maximize faithfulness when context is incomplete or when essential facts require abstraction; the authors acknowledge this limitation (Appendix G).
- The interpretability method approximates “parametric knowledge” by top-K tokens preferred without context; this conflates general language priors and domain parametrics and depends on a heuristic “isMeaningless” filter (Algorithm 3; §3.3).
- The logits power metric (Appendix I, Eq. 2) is introduced without derivation or sensitivity analysis; squaring logits and scaling by √n is ad hoc and might overemphasize high-variance positions.
- The empirical mechanism narrative (clearer separation of CTX/Para in UMAP, earlier contextual peak) is compelling but lacks statistical testing (e.g., cluster separability metrics, time-to-peak comparisons with CIs) (Figures 3–4; Appendix Figures 7–9).

Presentation
- Strengths: Detailed algorithms, illustrative figures, and explicit datasets/metrics tables (Figure 2; Algorithms 1–4; Appendix A).
- Weaknesses: Reliance on visual evidence for interpretability and absence of quantitative tests; some appendix figures contain placeholder-like captions and limited axis details (Appendix Figures 8–9; §4.2).
- Minor inconsistencies in availability and dataset usage (Abstract vs Appendix §8; Appendix Table 4 vs §4.1.2) distract from the narrative.

Contribution
- Proposes a practical, automated pipeline translating high-copy behaviors into preference alignment, and shows sizeable gains with small training sets (365 samples) on FaithEval and ConFiQA (§4.1.2; Table 3; Appendix Table 5).
- Extends token-level provenance probing to full CoT trajectories and frames the effect as parametric confidence recalibration (§3.3; §4.2).

Strengths
- Clear, reproducible prompting and training pipeline with strong data efficiency (Figure 2; Algorithms 1–2; Appendix D).
- Mechanistic analysis attempts to move beyond final answers, providing a richer picture of knowledge source dynamics (§4.2; Figures 3–4).
- Demonstrated effectiveness across multiple models and datasets, including challenging counterfactual scenarios (Table 3; Appendix Table 5).

Weaknesses
- Interpretability relies on heuristic proxies and bespoke metrics without statistical validation or ablations (Algorithm 3; Appendix I).
- The connection between high copying and reduced hallucinations remains correlational (Figure 1; §2.2).
- Fluency assessment uses GPT-2 perplexity, which is misaligned with modern instruction outputs (Appendix A).

Questions
1. Can you validate the UMAP findings with quantitative cluster separability (e.g., silhouette, Davies–Bouldin) and compare time-to-peak contextual utilization with confidence intervals (Figures 3–4; §4.2)?
2. How sensitive are results to K in Algorithm 3 and to the “isMeaningless” filter; could you report robustness curves (Algorithm 3; Appendix I)?
3. Can you compare the logits power metric to alternatives (e.g., normalized odds, entropy-based measures) and justify Eq. 2 theoretically (Appendix I)?
4. Do ablations that remove answer stamping, Elo tournaments, or mini-checking materially degrade performance (Algorithm 2; §3.2)?
5. Could you incorporate human judgments of readability to ensure high-copy answers remain usable (Appendix A; §4.1.1)?

Rating
- Overall (10): 7 — Effective and practical framework with an interesting mechanistic story, but interpretability and causality claims need quantitative backing (§4.1.2; §4.2; Figure 1).
- Novelty (10): 8 — The synthesis of copy-first preference training and token-level CoT probing is novel in framing and practice (§3.1–3.3; §5).
- Technical Quality (10): 6 — Solid engineering and training, but heuristic interpretability and limited statistical validation reduce rigor (Algorithm 3; Appendix I; §4.2).
- Clarity (10): 7 — Clear pipeline and algorithms, though empirical interpretability is largely visual and some reporting inconsistencies remain (Figure 2; §4.2; Appendix A, §8).
- Confidence (5): 3 — Moderate confidence; conclusions hinge on heuristic proxies and missing statistical tests despite detailed algorithms and appendices.


Review 4

Summary
The paper advances RAG faithfulness by directly copying context into answers and training LLMs to prefer such responses. It introduces three copy-focused prompting schemes (CP-Order/Link/Refine), an automated DPO pipeline using multi-criteria filtering and Elo tournaments, and a token-level CoT probe for contextual vs parametric reliance. Large, data-efficient gains are reported on FaithEval and ConFiQA, with smaller but consistent improvements in non-counterfactual PubMedQA (§3–4).

Soundness
- The approach is methodologically coherent: high-copy generation to create preferred behaviors and DPO to internalize them (Algorithms 1–2; Figure 2). Multi-criteria filtering helps prevent degenerate “copy-only” outputs (§3.2).
- The motivating observation is correlational; while reasonable, it does not prove that copying causes lower hallucination (Figure 1; §2.2).
- Evaluation spans counterfactual and original contexts, with accuracy and hit rate metrics; however, heavy dependence on LLM-as-judge for hallucination ranking introduces potential bias (Appendix A; J.3).
- The interpretability tool offers useful qualitative insight but is not rigorously validated (Algorithm 3; §4.2; Appendix I).

Presentation
- The manuscript is generally clear and well-structured, with intuitive diagrams and explicit algorithms (Figure 2; Algorithms 1–4).
- Some key quantitative results are not in the main text (Stage 1 Table 2; FaithEval Table 1), and minor inconsistencies in dataset usage and code availability need correction (Appendix A; §4.1.2; Abstract vs Appendix §8).
- Visualizations are readable and support claims, though more precise captions and statistics would help (Figures 3–4; Appendix Figures 5–6).

Contribution
- Practical contribution: an automated, data-efficient pipeline that materially improves faithfulness without large-scale fine-tuning (§3.2; §4.1.2).
- Conceptual: reframing faithfulness through copying degree and integrating this into training objectives (§3.1–3.2).
- Interpretability: a sequential probe of knowledge reliance during CoT (§3.3; §4.2).

Strengths
- Significant gains with minimal training data (365 samples), including challenging counterfactual benchmarks (Table 3; Appendix Table 5; §4.1.2).
- Detailed recipes and algorithms enhance reproducibility (Appendix J; Algorithms 1–4; Appendix D).
- The writer–reviewer loop (CP-Refine) balances copying with fluency and relevance (§3.1; Appendix J.1.4).

Weaknesses
- Overdependence on LLM-as-judge with no reliability checks (Appendix J.3; §3.2).
- Copy metrics focus on lexical overlap and may miss semantic misalignments; more robust checks could be added (Appendix A; §2.1).
- Minor reporting and availability inconsistencies (Appendix A; §4.1.2; Abstract vs Appendix §8).

Questions
1. Could you add inter-judge agreement and robustness to different judges or prompts in Elo tournaments (Appendix J.3; §3.2)?
2. Please provide the missing main-body tables for Stage 1 and FaithEval comparisons, and reconcile dataset usage and code availability statements (Table 2; Table 1; Appendix A; §4.1.2; Abstract vs Appendix §8).
3. How does the approach perform when context is noisy or partially incorrect; can the pipeline detect and downweight “bad context” (Appendix G; §3.2)?
4. Beyond GPT-2 perplexity, can you include human readability or modern LM-based fluency metrics (Appendix A)?

Rating
- Overall (10): 8 — Clear, useful pipeline with strong empirical gains and practical recipes, tempered by reliance on LLM-as-judge and correlational motivation (Figure 2; §4.1.2; Figure 1).
- Novelty (10): 7 — The end-to-end integration of copy-first prompting with preference training and sequential probing is a meaningful contribution (§3.1–3.3; §5).
- Technical Quality (10): 7 — Solid engineering and experiments, though evaluation/interpretability would benefit from robustness and statistical tests (§3.2; §4.2; Appendix A, I).
- Clarity (10): 8 — Well-structured with explicit algorithms and figures, but key tables should be surfaced and minor inconsistencies fixed (Figure 2; §4; Appendix A, §8).
- Confidence (5): 4 — High-level confidence in the pipeline and reported gains, with reservations about judge-based evaluation and correlational motivation.