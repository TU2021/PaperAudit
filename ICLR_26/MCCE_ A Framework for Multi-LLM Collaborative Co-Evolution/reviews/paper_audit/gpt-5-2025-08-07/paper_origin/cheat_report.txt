Academic integrity and consistency risk report

Summary of high-impact issues with explicit evidence anchors:

1) Unspecified and potentially non-reproducible objective scoring for DRD2, GSK3β, and JNK3
- Where it matters: The core optimization task depends on five objectives (QED, SAscore, DRD2 binding, GSK3β binding, JNK3 binding).
- Evidence:
  - Section 4.1 (Stage 3; Eq. 9–10) defines a generic multi-objective scoring vector and normalization but does not specify how DRD2, GSK3β, and JNK3 are quantified (predictive models, docking, QSAR predictors, scales, thresholds, versions, or validation).
  - Section 5.1 states the models were “assessed against five standard drug-likeness objectives” without describing the evaluators or pipelines.
  - Appendix A.1 provides textual heuristics but no actual scoring method or implementation details.
- Risk: Without explicit evaluators and their calibration, results cannot be verified or reproduced, and HV comparisons (Section 5.2) may be uninterpretable.
- Missing details: No direct evidence found in the manuscript on the evaluators used (model names, training data, metrics, versions, and parameter settings).

2) Hypervolume (HV) computation lacks essential specifics (reference point and normalization policy), compromising comparability
- Evidence:
  - Section 5.1 uses HV as the main metric.
  - Section 4.1 (Eq. 10) indicates per-generation normalization by current population mean and std for each objective, but it is unclear whether HV is computed on raw or normalized scores, and the HV reference point is not specified.
- Risk: HV depends critically on a fixed reference point and consistent objective scales. Per-generation normalization can change the Pareto geometry, making across-run or across-method comparisons potentially invalid.
- Missing details: No direct evidence found in the manuscript specifying the HV reference point, whether HV uses raw or normalized objectives, and how objectives are scaled across methods/runs.

3) Mismatch between claimed API models used and reported results
- Evidence:
  - Section 5.1: “we leveraged the GPT-4o-2024-05-13 and Gemini-2.5-flash-nothinking models through their APIs.”
  - Table 1 (Section 5.2.1; Block #34) and figures (Figures 2–3) report results for GPT-4o and Qwen2.5 only; no Gemini-2.5 results are shown.
- Risk: Claims of using multiple closed-source LLMs are not supported by displayed results; readers cannot assess Gemini’s contribution or performance.
- Missing details: No direct evidence found in the manuscript presenting Gemini-2.5 performance or its role in the collaborative variants.

4) Suspicious uniformity of “Top100auc” across all models in Table 1
- Evidence:
  - Table 1 (Section 5.2.1; Block #34): “Top100auc” shows identical or near-identical means (3.93) across all rows, with minimal variation in std.
- Risk: Such uniformity is highly implausible across heterogeneous models and training paradigms. It suggests potential copy-paste or calculation issues. If “Top100auc” feeds into conclusions, this could materially affect the claimed superiority.
- Missing details: No definition of “Top100auc” is provided, preventing validation of whether identical values are expected.

5) Conceptual inconsistency about “enhancing capabilities” of a frozen closed-source LLM
- Evidence:
  - Abstract (Section: Abstract, Block #2): “enhances the capabilities of both models through mutual inspiration.”
  - Introduction (Blocks #3–4): Emphasizes that closed-source LLMs cannot be fine-tuned or update parameters.
  - Table 1 caption (Section 5.2.1; Block #34): “enhances the performance of both the local trainable model and the API-based frozen LLM … This demonstrates that mutual learning benefits both components.”
- Risk: A frozen API model cannot internally learn via parameter updates. If improvement refers to system-level behavior (e.g., better parent selection or prompts), this should be clarified. As phrased, it implies parameter-level enhancement of the frozen model, contradicting earlier statements and potentially misleading readers about the mechanism of improvement.

6) Ambiguity/inconsistency in the definition of similarity sim(c, q) used for DPO triplet construction
- Evidence:
  - Section 4.3 (Notation; Block #21): sim(c, q) is defined via fingerprint-based metrics (e.g., Tanimoto on Morgan fingerprints) between a candidate molecule c and the prompt q.
  - Section 4.1 (Stage 2; Block #15): prompts are constructed as prompt(p1, p2) with parent molecules; Appendix A.1 shows a generic prompt without explicit parent molecules included.
- Risk: Fingerprint similarity requires molecular structures, not text. It is unclear whether q encodes parent molecules, how multiple parents are handled (similarity to p1, p2, or both?), or how similarity is computed when q is a text-only prompt (Appendix A.1). Since DPO pair construction (Algorithm 1/2; Blocks #24 and #48) hinges on this similarity, the method becomes non-reproducible.
- Missing details: No direct evidence found in the manuscript specifying the exact mapping from prompt q to molecular representations for sim(c, q), or aggregation over multiple parents.

7) Key DPO training hyperparameters are not provided
- Evidence:
  - Section 4.3 (Dataset and hyperparameters; Block #25) mentions L (recent prompts), r (pairs per prompt), α, similarity intervals I1–I3, and μ ± σ band.
  - Appendix A.3 (Training Details; Block #46) discusses f (update frequency), |D|, β, and general sweeps.
- Risk: Absent concrete values for L, r, f, β, and dataset sizes, training cannot be reproduced; stability and performance claims (Section 4.2 and Section 5.2) cannot be verified.
- Missing details: No direct evidence found in the manuscript giving the actual hyperparameter values used in the main experiments (L, r, f, β, |D|).

8) Reproducibility claims not substantiated by the manuscript
- Evidence:
  - Appendix C (Reproducibility Statement; Block #51): claims “All code and datasets have been made publicly available in an anonymous repository,” and “hardware details … described in detail in the paper.”
- Risk: The manuscript provides no repository link or identifier and does not detail hardware or compute settings (GPUs, memory, batch sizes). Reproducibility claims are therefore unsupported.
- Missing details: No direct evidence found in the manuscript with code/data access or hardware configurations.

9) Loss behavior claim partially inconsistent with the provided loss plot
- Evidence:
  - Appendix A.2 (Loss Analysis; Blocks #43–45) describes non-monotonic loss with spikes diminishing over time.
  - Appendix A.3 (Training Details; Block #46) states “we observe a monotonically decreasing average loss curve.”
  - Figure 4 (Appendix A.2; Block #44) shows sharp spikes; no “average loss” curve is shown to support monotonicity.
- Risk: The stronger claim of monotonic decrease of average loss is not evidenced by the shown figure. While not central to conclusions, it weakens confidence in the training stability narrative.

10) Analysis with “no-parent prompt” may not be directly comparable to the main evolutionary setting
- Evidence:
  - Section 5.2.2: Output distribution analysis uses a “no-parent prompt” to sample 1,000 molecules from each model.
  - Section 4.1 (Stage 2; Block #15) defines generation via parent-based prompts prompt(p1, p2).
- Risk: The distribution shift between parent-based generation and no-parent prompting could affect the comparability of the depicted distributions and the claimed learning effect. Clarification is needed to ensure the analysis aligns with the primary optimization regime.

Additional clarifications needed:
- Definitions for metrics in Table 1: “Top1F,” “Top10F,” “Top100F,” “Top1auc,” “Top10auc,” “Top100auc,” “Uniqueness,” “Diversity,” “Validity” are not defined in the manuscript. No direct evidence found specifying these metrics’ formulas or computation procedures. This affects interpretation of Table 1 and figures.

If addressed, these issues could significantly improve the paper’s scientific validity and reproducibility. As it stands, several critical methodological and reporting gaps materially affect trustworthiness, especially the missing evaluator details for binding objectives, HV computation specifics, unclear similarity definition, and the suspicious Table 1 “Top100auc” values.

If none of the above are errors and the authors can supply the missing details (evaluators, HV settings, similarity mapping, hyperparameters, and code/data/hardware), many of the integrity risks would be mitigated.