# Global Summary
- Problem: Optimize in vast, discrete, multi-objective spaces (demonstrated in drug design), where traditional evolutionary algorithms risk premature convergence and struggle with diversity-quality trade-offs.
- Core approach: Multi-LLM Collaborative Co-evolution (MCCE) coupling a frozen closed-source LLM (for global exploration) with a lightweight trainable local LLM (for experience-driven adaptation). The system maintains trajectory memory and periodically refines the local model via Direct Preference Optimization (DPO) using similarity-based preference pair synthesis to stabilize training.
- Evaluation scope: Five-objective molecular optimization (QED, SAscore, DRD2, GSK3β, JNK3). Baselines include single-model runs (Qwen2.5-7B-Instruct, GPT-4o-2024-05-13), simple collaboration without training, and co-evolution variants with SFT or RL.
- Key findings: MCCE with DPO achieves state-of-the-art Pareto front quality. Reported Hypervolume (HV) for dpo_coevolve is "0.847 ± 0.138" (10 runs), exceeding GPT-4o ("0.661 ± 0.214") and Qwen ("0.516 ± 0.102"). Top1F for dpo_coevolve is "4.35 ± 0.17", higher than baselines. Both components benefit: dpo_coevolve:api HV "0.855 ± 0.135"; dpo_coevolve:local HV "0.826 ± 0.126".
- Explicit caveats stated by authors: SFT led to catastrophic forgetting (loss of uniqueness/novelty); RL training was unstable with collapse due to negative rewards; RAG cannot update parameters; closed-source LLMs cannot internalize experience; small open models lack broad knowledge. Other limitations (e.g., compute budget, hardware, licensing) are Not specified.

# Abstract
- Problem framing: Multi-objective discrete optimization (e.g., molecular design) is hard; evolutionary algorithms can get stuck in local optima; expert knowledge helps.
- LLM motivation: Closed-source LLMs have strong exploration but are frozen (no parameter updates); small open models are trainable but weaker in knowledge/reasoning.
- Proposed framework: MCCE combines a frozen closed-source LLM with a trainable small model. Maintains a trajectory memory; the small model is refined via reinforcement learning (implemented as DPO later), and both models co-support exploration. Emphasis on "mutual inspiration" rather than distillation.
- Claims: On multi-objective drug design benchmarks, MCCE achieves state-of-the-art Pareto front quality and consistently outperforms baselines.
- Quantitative details: Not specified in this section.

# Introduction
- Challenges: Discrete/multi-objective optimization has high-dimensional, unstructured spaces; traditional evolutionary methods risk premature convergence and poor diversity.
- LLM limitations: Single LLMs may converge to their own distribution (reducing diversity); RAG is limited by context size and cannot update parameters; effective optimization needs parameter training.
- Motivation: Closed-source LLMs strong but untrainable; open models trainable but weaker; hybrid needed.
- Proposal: MCCE—frozen closed-source LLM plus lightweight trainable local model. Alternating roles as evolutionary operators: large model explores, local model learns from experience to target searches; periodic refinement using breakthrough trajectories; mutual inspiration (beyond distillation).
- Context: Prior ExLLM and MoLLEO used LLM operators but relied on a single frozen LLM and fewer objectives; MCCE extends to five objectives for more realistic testing.
- Contributions:
  - A collaborative co-evolution framework integrating closed-source and trainable models for discrete, multi-objective optimization.
  - Experience-driven learning using breakthrough trajectories to refine local search directions.
  - Demonstrated efficacy and extensibility beyond drug discovery.

# Related Work
- Multi-model collaboration: Studies show multiple LLMs can improve reasoning via diverse perspectives (e.g., adaptive branching MCTS; ensemble strategies). Prior methods largely treat models as static; this work enables dynamic co-evolution with learning.
- Experience learning: RL and hybrid strategies incrementally improve reasoning (e.g., ML-agent, CALM, Evo-Tune, RL-PLUS, LUFFY, ReLIFT, TAPO). Prior works focus on single-model improvement; MCCE enables collaborative experience learning across models.
- Evolutionary algorithms: LLMs as heuristic generators and reflective search (FunSearch, EoH, MEoH, REEVO, ML-master, AlphaEvolve, Hsevo). Typical approaches lack parameter-level adaptation and multi-model dynamics; MCCE closes this gap.

# Preliminaries
- Reinforcement Learning (RL): Objective J(π) = E_{τ∼π}[∑ γ^t r(s_t, a_t)] (1).
- Direct Preference Optimization (DPO): Pairwise preference loss L_DPO(π) with reference model π_ref and sharpness β; uses sigmoid σ (2).
- Supervised Fine-Tuning (SFT): NLL over outputs y given prompt x (3).
- Generative Flow Networks (GFlowNets): Generate diverse trajectories with probability proportional to terminal reward R(s_T), enforced via flow matching constraints (4–5).
- Quantitative hyperparameters beyond formulas: Not specified in this section.

# Method
- Problem setting: Five-objective molecular optimization targeting QED, SAscore, DRD2 binding, GSK3β binding, JNK3 binding; extends prior three-objective evaluations.
- Overall framework (four stages):
  - Initialization: Population P_0 = {c_1,…,c_M}, c_i ∼ π_init (6). Each operator call generates exactly two candidates; building a population of size M requires M/2 prompt generations (8).
  - Candidate generation: Select parents (e.g., tournament/fitness-proportional); LLM operator produces two offspring (7). Operator alternates between frozen API LLM and trainable local LLM.
  - Multi-objective evaluation: Compute scores s(c) = [s_1,…,s_K] (9); normalize via per-objective mean μ_k and std σ_k (10).
  - Update and learning: Pareto selection to form P_{t+1} and maintain diversity. After every N generated candidates, store successful trajectories as experience D and update local model parameters via Update(π_LL M, D) (11).
- Training paradigm exploration:
  - SFT: Treat breakthrough generations as positive examples; led to catastrophic forgetting, reduced uniqueness.
  - RL: Using scoring as reward was unstable; negative rewards caused collapse and invalid outputs.
  - DPO: Adopted for stable, sample-efficient optimization. Initial same-prompt high-vs-low pairing caused loss oscillations; resolved by similarity-based data synthesis.
- Similarity-based data synthesis:
  - Phase 1: Compute similarity stats S = {sim(c,q)} across both models; mean μ and std σ (12); global filter F = {c | μ − σ ≤ sim(c,q) ≤ μ + σ} (13).
  - Phase 2: Score stratification: quantile α = 0.3 to define T_high (top α) and T_low (bottom α). Similarity intervals: I_1 = [μ + 2/3 σ, μ + σ], I_2 = [μ + 1/3 σ, μ + σ], I_3 = [μ, μ + σ] (14).
  - Phase 3: Per-prompt triplet construction (q, τ^+, τ^-): filter by F; choose τ^+ from high-score pool within I_1 → I_2 → I_3 → Top-50%; choose τ^- from low-score pool within I_1 → I_2 → I_3 → Bottom-50%; fallback rules when unavailable.
  - Dataset and hyperparameters: Use most recent L prompts; r pairs per prompt (default r = 1); D ≤ L·r. Key hyperparameters: α (=0.3), I_1–I_3 windows, global band μ ± σ.
  - Rationale: Using similarity stats from both models and narrow similarity windows reduces distribution shift and contradictory same-prompt responses.

# Experiments
- Experimental setup:
  - Models: Frozen closed-source LLMs GPT-4o-2024-05-13 and Gemini-2.5-flash-nothinking (API); local trainable model Qwen2.5-7B-Instruct.
  - Initialization: 100 molecules randomly sampled from ZINC.
  - Objectives: Five drug-likeness objectives; multi-objective scoring.
  - Metrics: Hypervolume Indicator (HV) as main metric; additional metrics reported include Top1F, Top10F, Top100F, Top1auc, Top10auc, Top100auc, Uniqueness, Diversity, Validity.
  - Training implementations: SFT and RL baselines via verl; DPO via trl.
- Main results (Table 1; mean ± std over 10 runs):
  - qwen2.5-7b-instruct: Top1F "4.07 ± 0.04"; Top10F "4.05 ± 0.04"; Top100F "3.99 ± 0.04"; Top1auc "3.95 ± 0.03"; Top10auc "3.91 ± 0.03"; Uniqueness "0.576 ± 0.018"; Diversity "0.543 ± 0.047"; HV "0.516 ± 0.102"; Validity "0.838 ± 0.025"; Top100auc "3.86 ± 0.02".
  - gpt-4o-2024-05-13: Top1F "4.16 ± 0.15"; Top10F "4.14 ± 0.12"; Top100F "4.09 ± 0.10"; Top1auc "4.02 ± 0.10"; Top10auc "3.99 ± 0.08"; Uniqueness "0.702 ± 0.056"; Diversity "0.497 ± 0.035"; HV "0.661 ± 0.214"; Validity "0.902 ± 0.022"; Top100auc "3.93 ± 0.05".
  - collaboration (no training): Top1F "4.19 ± 0.15"; Top10F "4.13 ± 0.12"; Top100F "4.07 ± 0.09"; Top1auc "4.00 ± 0.08"; Top10auc "3.96 ± 0.06"; Uniqueness "0.750 ± 0.041"; Diversity "0.524 ± 0.048"; HV "0.695 ± 0.189"; Validity "0.838 ± 0.024"; Top100auc "3.90 ± 0.04".
  - rl_coevolve: Top1F "4.19 ± 0.17"; Top10F "4.16 ± 0.15"; Top100F "4.10 ± 0.13"; Top1auc "4.03 ± 0.12"; Top10auc "3.99 ± 0.09"; Uniqueness "0.683 ± 0.045"; Diversity "0.509 ± 0.059"; HV "0.709 ± 0.219"; Validity "0.893 ± 0.021"; Top100auc "3.93 ± 0.07".
  - sft_coevolve: Top1F "4.24 ± 0.25"; Top10F "4.20 ± 0.22"; Top100F "4.13 ± 0.19"; Top1auc "4.03 ± 0.14"; Top10auc "3.99 ± 0.11"; Uniqueness "0.571 ± 0.047"; Diversity "0.478 ± 0.070"; HV "0.709 ± 0.288"; Validity "0.905 ± 0.020"; Top100auc "3.93 ± 0.08".
  - dpo_coevolve: Top1F "4.35 ± 0.17"; Top10F "4.28 ± 0.15"; Top100F "4.19 ± 0.13"; Top1auc "4.07 ± 0.11"; Top10auc "4.02 ± 0.09"; Uniqueness "0.660 ± 0.018"; Diversity "0.484 ± 0.063"; HV "0.847 ± 0.138"; Validity "0.820 ± 0.022"; Top100auc "3.93 ± 0.06".
  - dpo_coevolve:local: Top1F "4.27 ± 0.16"; Top10F "4.22 ± 0.14"; Top100F "4.09 ± 0.10"; Top1auc "4.06 ± 0.03"; Top10auc "4.01 ± 0.03"; Uniqueness "0.633 ± 0.025"; Diversity "0.555 ± 0.055"; HV "0.826 ± 0.126"; Validity "0.759 ± 0.030"; Top100auc "3.93 ± 0.03".
  - dpo_coevolve:api: Top1F "4.35 ± 0.17"; Top10F "4.28 ± 0.14"; Top100F "4.17 ± 0.12"; Top1auc "4.08 ± 0.09"; Top10auc "4.03 ± 0.07"; Uniqueness "0.784 ± 0.016"; Diversity "0.505 ± 0.062"; HV "0.855 ± 0.135"; Validity "0.907 ± 0.016"; Top100auc "3.93 ± 0.06".
- Co-evolutionary curve and distribution analysis:
  - Curves (Figures 2–3) show avg_top1 and hypervolume (mean ± std) evolving over the optimization process; MCCE with DPO increases both metrics steadily and surpasses all baselines.
  - Output distribution analysis: 1,000 molecules sampled per model (no-parent prompt). The trained local model’s fitness distribution shifts toward higher scores compared to frozen LLM and untrained local model.
- Additional quantitative notes:
  - Plots show “all_unique_mols” up to approximately 5,000 on the x-axis; specific run budgets beyond 10 runs are Not specified.
  - Training budget, hardware, and wall-clock times: Not specified.

# Conclusion
- Summary: MCCE establishes a closed feedback loop combining global exploration (frozen LLM) with experience-driven adaptation (trainable local LLM), achieving mutual reinforcement rather than one-way distillation.
- Claims: Extensive experiments in multi-objective drug design show state-of-the-art performance and significant improvements over baselines.
- Broader principle: Hybrid systems pairing static powerful models with adaptive trainable counterparts can unlock capabilities in complex discrete optimization.
- Future directions: Extend to other domains; explore more adaptive inter-model communication and dynamic balance mechanisms.

# References
- Cited domains: Multi-LLM collaboration and ensemble strategies; RL and hybrid policy optimization for experience learning; LLM-integrated evolutionary algorithm design and reflective mechanisms; multi-objective heuristic evolution; surveys of self-evolving agents and LLM agents.
- Specific references include FunSearch (Nature 2024), EoH/MEoH, REEVO, ML-master, AlphaEvolve, RL-PLUS, LUFFY, CALM, ReLIFT, TAPO, and prior LLM-driven drug design works (ExLLM/MoLLM/MoLLEO).
- Quantitative details of cited works: Not specified in this section.

# Appendix
- Prompt (A.1): Concrete multi-objective molecule-generation instruction balancing SA, DRD2, QED, GSK3β, JNK3, with output constrained to exactly two SMILES wrapped in tags.
- DPO loss analysis (A.2): Figure 4 shows DPO loss decreasing over training steps with diminishing peak magnitudes; sharp peaks arise when new synthesized data is introduced.
- Training details (A.3):
  - DPO objective uses π_ref = initial untrained local model π_{θ0} to prevent reference drift (15).
  - Hyperparameters discussed: update frequency f and dataset size |D| trade off adaptation speed vs stability; exact values are Not specified.
  - Comparative sweeps across SFT, offline RL, GFlowNets, DPO show DPO higher Perf(m) and robustness; exact sweep sizes and configs Not specified.
- Detailed algorithm for similarity-based data synthesis (A.4): Full pseudocode including fallback rules for selecting τ^+, τ^- within similarity intervals and score pools; stores optional s(τ^{±}) and sim(τ^{±}, q).
- Additional co-evolutionary curves (A.5): Four figures showing consistent superiority of collaborative trajectories over single models across runs.
- Ethics statement (B): Compliance with ICLR Code of Ethics; ZINC used under guidelines; no human/animal studies; no PII; commitment to avoiding bias.
- Reproducibility statement (C): Code and datasets available in an anonymous repository; detailed setup provided; exact hardware and links Not specified.
- LLM usage disclosure (D): LLMs used for writing/polishing only; no role in ideation, methodology, or analysis.