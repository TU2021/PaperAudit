1) Summary
This paper introduces Multi-LLM Collaborative Co-evolution (MCCE), a framework for multi-objective discrete optimization. The core problem addressed is the trade-off between powerful but static closed-source Large Language Models (LLMs) and less capable but trainable open-source models. MCCE combines a frozen, closed-source LLM for global exploration with a lightweight, trainable local model for experience-driven learning. The framework operates as an evolutionary algorithm where both LLMs act as genetic operators. The local model is continuously refined using Direct Preference Optimization (DPO) on "breakthrough" trajectories, which are curated via a novel similarity-based data synthesis method to ensure stable training. Experiments on a five-objective molecular design benchmark show that MCCE achieves state-of-the-art performance, outperforming single-model baselines and alternative training paradigms. The results suggest a synergistic co-evolution where both models' capabilities are enhanced through collaboration.2) Strengths
*   **Novel and Well-Motivated Framework:** The paper proposes a compelling solution to a well-defined problem in LLM-based optimization: the inability of powerful API-based models to learn from experience. The MCCE framework, which pairs a frozen explorer with a trainable learner, is a novel and intuitive architecture for combining the strengths of both model types.
    *   The introduction clearly articulates the dilemma between powerful, non-trainable closed-source LLMs and weaker, trainable open-source models (Section 1, Page 2).
    *   The framework is presented as a form of "mutual inspiration" rather than simple one-way distillation, which is a more sophisticated and powerful concept (Abstract, Section 1).
    *   The overall architecture depicted in Figure 1 is clear, generalizable, and applicable to a wide range of discrete optimization problems beyond the tested domain of molecular design (Section 4.1).*   **Rigorous Technical Contribution for Stable Learning:** The paper's primary technical contribution is the well-justified and carefully designed learning mechanism for the local model. The authors provide a systematic analysis of why simpler methods like Supervised Fine-Tuning (SFT) and standard Reinforcement Learning (RL) fail in this context.
    *   The discussion in Section 4.2 provides clear empirical reasoning for choosing DPO, noting that SFT led to catastrophic forgetting and RL was unstable. This methodical approach adds credibility to the final design choice.
    *   The proposed "Similarity-Based Data Synthesis" pipeline (Section 4.3) is a non-trivial and clever solution to stabilize DPO training by mitigating distributional shift and avoiding confusing the model with contradictory prompt-response pairs.
    *   The detailed algorithm (Algorithm 1, Algorithm 2 in Appendix) and the resulting stable DPO loss curve (Figure 4) provide strong evidence for the technical soundness and effectiveness of this learning component.*   **Strong Empirical Results and Comprehensive Ablations:** The experimental evaluation is thorough and demonstrates the effectiveness of the proposed MCCE framework. The results consistently show superior performance over relevant baselines.
    *   MCCE with DPO (`dpo_coevolve`) significantly outperforms single-model baselines (`qwen2.5-7b-instruct`, `gpt-4o-2024-05-13`) and a simple collaboration baseline without training (`collaboration`) on the key Hypervolume (HV) metric (Table 1).
    *   The framework is also shown to be superior to co-evolutionary variants using SFT and RL for training (`sft_coevolve`, `rl_coevolve`), validating the specific choice of the DPO-based learning paradigm (Table 1, Figure 2).
    *   The output distribution analysis (Figure 3, Right) provides clear evidence that the local model successfully learns to generate higher-quality candidates, confirming that the experience-driven learning mechanism is working as intended.*   **Clear Evidence of Synergistic Co-Evolution:** The paper provides compelling evidence for its central claim that the models "co-evolve" and mutually benefit, rather than the local model simply distilling knowledge from the larger one.
    *   Table 1 includes a performance breakdown for molecules generated by the local model (`dpo_coevolve:local`) and the API model (`dpo_coevolve:api`) within the MCCE system.
    *   The results show that the API model's generations (`dpo_coevolve:api`, HV 0.855) are significantly better than when it operates alone (`gpt-4o-2024-05-13`, HV 0.661). This suggests that the improving local model provides better parent candidates to the API model, enhancing its exploration.
    *   Similarly, the local model's performance (`dpo_coevolve:local`, HV 0.826) far exceeds its standalone capability (`qwen2.5-7b-instruct`, HV 0.516), demonstrating the benefit of guidance from the API model. This two-way improvement is a significant strength of the framework.3) Weaknesses
*   **Lack of Clarity in Key "Co-evolutionary Curve" Visualizations:** The plots intended to visualize the collaborative dynamics between the two models are confusing and poorly explained, undermining a central piece of evidence for the "co-evolution" narrative.
    *   In Figure 3 (Left), the plot shows two curves labeled "qwen2.5-7b-instruct" and "gpt-4o-2024-05-13". These labels correspond to the standalone baseline models in Table 1, making it appear as if the plot compares two independent runs rather than the interaction within a single MCCE system.
    *   The caption and main text (Section 5.2.2) describe a dynamic interplay and complementarity, but the visualization does not clearly depict this. It is unclear what the y-axis (`avg_top1`) represents in the context of two interacting models.
    *   This ambiguity is repeated in the additional plots in the Appendix (Figure 5), indicating a systematic issue with this form of visualization.*   **High Variance in Reported Results:** While the mean performance of MCCE is superior, the reported standard deviations for the primary metric (Hypervolume) are very high across all methods, which weakens the claims of significant improvement.
    *   In Table 1, the best-performing method (`dpo_coevolve`) has a Hypervolume of `0.847 ± 0.138`. The second-best method (`rl_coevolve`) has `0.709 ± 0.219`. The large standard deviations suggest a substantial overlap in the performance distributions.
    *   The paper makes strong claims about superiority ("significantly surpasses all single-model baselines") but does not provide any statistical significance tests (e.g., t-tests) to formally support these claims in light of the high variance.
    *   The source of this high variance is not discussed, leaving open questions about the stability and robustness of the optimization process across different runs.*   **Missing Implementation Details on Model Alternation:** The paper states that the LLM operator "alternates between a frozen API model and a locally trainable model" (Section 4.1), but the specific strategy for this alternation is not described.
    *   It is unclear how the workload is divided between the two models. Is it a fixed 50/50 split for generating new candidates in each generation? Is it an adaptive strategy?
    *   This is a critical hyperparameter of the framework that governs the balance between exploration (API model) and exploitation (local model), and its omission hinders reproducibility and a full understanding of the method's dynamics. No direct evidence found in the manuscript.*   **Limited Comparison to External State-of-the-Art Methods:** The experimental comparison is largely internal, focusing on ablations of the MCCE framework (different training methods) and single-model baselines. The comparison to the broader state-of-the-art in LLM-based molecular optimization is limited.
    *   The related work section (Section 2) discusses several other relevant approaches (e.g., FunSearch, EoH), but these are not included as experimental baselines.
    *   While the paper builds on prior work like ExLLM and MoLLEO (Section 1), a direct quantitative comparison to other contemporary methods would better situate the performance of MCCE and strengthen its claim of achieving state-of-the-art results.4) Suggestions for Improvement
*   **Revise and Clarify the Co-evolutionary Curve Visualizations:** The authors should completely revise the plots in Figure 3 (Left) and Figure 5 to make them interpretable.
    *   The legend should be changed to accurately reflect what is being plotted. For example, if the curves represent the quality of candidates generated by each model type *within a single MCCE run*, the labels should be explicit (e.g., "API-generated candidates in MCCE", "Local model-generated candidates in MCCE").
    *   The caption and the corresponding text in Section 5.2.2 must be rewritten to unambiguously explain what the plot shows and how it supports the claim of collaborative improvement.
    *   Consider an alternative visualization, such as plotting the population's Pareto front at different generations, color-coding the points by which model generated them.*   **Address and Analyze High Result Variance:** The authors should acknowledge the high variance in the results and provide a more rigorous statistical analysis.
    *   Add statistical significance tests (e.g., paired t-tests or a non-parametric equivalent like the Wilcoxon signed-rank test) to Table 1 to compare `dpo_coevolve` against the main baselines. This would provide formal evidence for the claimed improvements.
    *   Include a brief discussion in the results section (Section 5.2) about the potential sources of this variance (e.g., stochasticity of LLM outputs, sensitivity to the initial population) and its implications for the method's robustness.*   **Specify the Model Alternation Strategy:** The manuscript should be updated to include the missing details about how the two LLM operators are alternated.
    *   In Section 4.1, explicitly state the strategy used in the experiments. For example: "In each generation, we generate M/2 new pairs of candidates. For each pair, the operator (API or local model) is chosen uniformly at random," or "we assign 50% of the parent pairs to the API model and 50% to the local model."
    *   This detail is essential for reproducibility and for other researchers to build upon this work.*   **Broaden the Set of Experimental Baselines:** To better contextualize the results, the authors should consider adding at least one strong, external baseline from the recent literature on LLM-based optimization.
    *   If computationally feasible, implementing and comparing against a non-evolutionary LLM optimization method or a different LLM-based evolutionary approach cited in Section 2 would make the claim of "state-of-the-art performance" more convincing.
    *   If direct implementation is not possible, a thorough discussion comparing MCCE's results to those reported in other relevant papers on the same or similar benchmarks would be a valuable addition.5) Score
- Overall (10): 8 — The paper presents a novel, well-motivated framework with a strong technical contribution and impressive results, though clarity issues in key figures and high result variance need to be addressed.
- Novelty (10): 9 — The concept of co-evolving a frozen API model with a trainable local model via similarity-stabilized DPO is a highly novel and impactful idea for LLM-based optimization.
- Technical Quality (10): 8 — The methodical development of the DPO-based learning paradigm is excellent (Section 4.2, 4.3), but the high variance in results (Table 1) and missing implementation details slightly reduce the technical quality.
- Clarity (10): 7 — The paper is generally well-written, but the confusing "co-evolutionary curve" visualizations (Figure 3 Left, Figure 5) and the omission of the model alternation strategy significantly hinder clarity and reproducibility.
- Confidence (5): 5 — I am highly confident in my assessment, having thoroughly reviewed the methodology, results, and appendices.