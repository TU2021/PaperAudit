# Global Summary
This paper introduces Multi-LLM Collaborative Co-evolution (MCCE), a framework for multi-objective discrete optimization. The core problem is that traditional evolutionary algorithms get stuck in local optima, while single LLM approaches either cannot learn from experience (closed-source models) or lack broad knowledge (open-source models). MCCE addresses this by combining a frozen, powerful, closed-source LLM for global exploration with a lightweight, trainable local model for experience-driven adaptation. The local model is progressively refined using Direct Preference Optimization (DPO) on "breakthrough" search trajectories, which are curated using a novel similarity-based data synthesis method to ensure stable training. The framework is evaluated on a challenging five-objective molecular design task. Experiments show that MCCE with DPO achieves a state-of-the-art Hypervolume (HV) score of 0.847 ± 0.138, significantly outperforming single-model baselines like GPT-4o (0.661 ± 0.214) and alternative training paradigms like SFT and RL. The key claim is that this hybrid, co-evolutionary approach creates a mutually reinforcing synergy, enabling continual learning and more effective optimization.

# Abstract
- The paper addresses challenges in multi-objective discrete optimization, such as molecular design, where traditional evolutionary algorithms often fail.
- It proposes Multi-LLM Collaborative Co-evolution (MCCE), a hybrid framework combining a frozen closed-source LLM with a lightweight trainable model.
- The system uses a trajectory memory of past searches to refine the small model via reinforcement learning, allowing the two models to complement each other in global exploration.
- This process is described as mutual inspiration, distinct from model distillation, enhancing both models' capabilities.
- Experiments on multi-objective drug design benchmarks show MCCE achieves state-of-the-art Pareto front quality and outperforms baselines, demonstrating a new paradigm for continual evolution in hybrid LLM systems.

# Introduction
- Discrete and multi-objective optimization problems are difficult due to vast, unstructured search spaces. Traditional evolutionary algorithms suffer from premature convergence and poor diversity-quality trade-offs.
- LLMs are promising optimizers due to their reasoning and prior knowledge, but single LLMs tend to reduce solution diversity, and RAG-based approaches cannot internalize experience via parameter updates.
- The paper argues that parameter training is essential for accumulating experience. This creates a dilemma between powerful but non-trainable closed-source LLMs and trainable but less capable open-source models.
- MCCE is proposed as a solution: a frozen closed-source LLM performs global exploration while a trainable local model performs targeted searches, learning from accumulated experience.
- The framework is contrasted with prior work like ExLLM, which uses a single frozen LLM and is thus limited in its ability to learn.
- Contributions are: (1) The MCCE framework, (2) an experience-driven learning paradigm using breakthrough trajectories, and (3) demonstrated SOTA performance in multi-objective drug design.

# Related Work
- **Multi-model Collaboration:** Cites works using multiple LLMs in MCTS or ensemble strategies. It notes that these methods typically treat models as static entities, whereas MCCE focuses on dynamic co-evolution where models learn from shared experience.
- **Experience Learning:** Discusses LLM agents that learn from experience using reinforcement learning (e.g., ML-agent, CALM, RL-PLUS). The paper claims these methods are limited by the capability ceiling of a single model, which MCCE overcomes through multi-model collaboration.
- **Evolutionary Algorithms:** Mentions works integrating LLMs with evolutionary algorithms (e.g., FunSearch, EoH, REEVO). It states that these approaches generally lack parameter-level adaptation or multi-model dynamics, a gap that MCCE aims to fill.

# Preliminaries
- **Reinforcement Learning (RL) and Direct Preference Optimization (DPO):** Defines the standard RL objective to maximize cumulative reward. Introduces DPO as an alternative that uses pairwise preferences over trajectories (τ⁺, τ⁻) and optimizes a logistic loss function without an explicit reward model.
- **Supervised Fine-Tuning (SFT):** Defines SFT as adapting a pre-trained LLM by minimizing the negative log-likelihood of reference outputs given a prompt.
- **Generative Flow Networks (GFlowNets):** Describes GFlowNets as a method to generate diverse trajectories with probability proportional to a reward function, enforced by a flow matching constraint.

# Method
- The proposed framework is Multi-LLM Collaborative Co-evolution (MCCE), which uses a frozen LLM for global exploration and a trainable local model for refinement.
- The framework is demonstrated on a five-objective molecular optimization task: QED, synthetic accessibility (SAscore), DRD2 binding, GSK3β binding, and JNK3 binding.
- **Overall Framework:** The process is an iterative loop with four stages:
    1.  **Initialization:** An initial population of M molecules is created.
    2.  **Candidate Generation:** Two parent molecules are selected to form a prompt, and an LLM operator (alternating between the frozen and local model) generates two new candidates.
    3.  **Multi-Objective Evaluation:** Each candidate is scored on K objectives, with scores normalized.
    4.  **Update and Learning:** The next generation is formed using Pareto front selection. Successful trajectories are stored as experience to refine the trainable LLM.
- **Choice of Training Paradigm:** The paper explored several methods to train the local model:
    - SFT led to catastrophic forgetting and a drop in the uniqueness of generated molecules.
    - RL was highly unstable, with the model collapsing due to strong negative rewards.
    - DPO was chosen for its stability. Initial DPO attempts with simple high-vs-low score pairs were also unstable, leading to the development of a specialized data synthesis method.
- **Similarity-based Data Synthesis for DPO:** A three-phase pipeline to create stable (query, chosen, rejected) triplets for DPO training.
    1.  **Similarity Statistics:** Compute the mean (μ) and standard deviation (σ) of molecular similarity (e.g., Tanimoto on Morgan fingerprints) between prompts and generated candidates from both models. A global filter retains candidates within one standard deviation (μ ± σ).
    2.  **Score Stratification:** Candidates are sorted by score. Top (T_high) and bottom (T_low) pools are created using a quantile threshold α = 0.3. Nested similarity intervals (I_1, I_2, I_3) are defined to prioritize candidates similar to the prompt.
    3.  **Pair Construction:** For each prompt, the algorithm tries to find a preferred candidate (τ⁺) from the high-score pool and a rejected one (τ⁻) from the low-score pool, using progressively relaxed similarity intervals and score ranges as fallbacks.

# Experiments
- **Experimental Setup:**
    - **Task:** Multi-objective drug design with 5 objectives.
    - **Models:** Frozen models: GPT-4o-2024-05-13, Gemini-2.5-flash-nothinking. Trainable local model: Qwen2.5-7B-Instruct.
    - **Initialization:** 100 molecules randomly sampled from the ZINC dataset.
    - **Metric:** Hypervolume Indicator (HV).
    - **Implementation:** SFT/RL baselines with `verl`, DPO with `trl`.
- **Main Results:**
    - MCCE with DPO (`dpo_coevolve`) achieves the highest performance across multiple metrics.
    - From Table 1 (mean ± std over 10 runs):
        - `dpo_coevolve` HV: 0.847 ± 0.138.
        - `gpt-4o-2024-05-13` HV: 0.661 ± 0.214.
        - `qwen2.5-7b-instruct` HV: 0.516 ± 0.102.
        - `sft_coevolve` HV: 0.709 ± 0.288.
        - `rl_coevolve` HV: 0.709 ± 0.219.
    - The `dpo_coevolve` method also achieves the highest Top1F score (average score of the best molecule) of 4.35 ± 0.17.
    - Figure 2 shows that `dpo_coevolve` consistently outperforms all baselines on both `avg_top1` and `hypervolume` throughout the optimization process.
    - Figure 3 (Right) shows that the fine-tuned local model's output distribution is shifted towards higher fitness scores compared to the frozen LLM and the initial local model, confirming that it learns from experience.

# Conclusion
- The paper presents MCCE, a collaborative co-evolutionary framework combining a frozen LLM for exploration and a trainable model for experience-driven learning.
- The synergy between the models is described as mutually reinforcing, not one-way distillation.
- Experiments in multi-objective drug design show MCCE achieves state-of-the-art performance, outperforming baselines.
- The work suggests a broader principle for hybrid AI systems: combining powerful static models with adaptive, trainable ones can unlock new problem-solving capabilities.
- Future work includes extending MCCE to other optimization domains and exploring more adaptive inter-model communication.

# References
- This section contains a list of references cited in the paper. No summary is needed.

# Appendix
- **A.1 Prompt:** Provides the full prompt text used for molecular generation. It specifies the 5 optimization objectives (decrease SA, DRD2, GSK3β; increase QED, JNK3), gives brief explanations for each, and dictates the output format (two molecules in SMILES form, enclosed in `¡mol¿...¡/mol¿`).
- **A.2 DPO Loss Analysis:** Figure 4 shows the DPO training loss curve. The loss generally decreases over time, with occasional spikes corresponding to the introduction of new training data. The diminishing magnitude of these peaks is presented as evidence of stable learning.
- **A.3 Training Details:** The reference model `π_ref` in the DPO loss is fixed to the initial untrained local model `π_{θ0}` to prevent distributional drift and instability. The paper notes that DPO consistently achieved higher performance than SFT and offline RL across hyperparameter sweeps.
- **A.4 Detailed Algorithm for Similarity-based Data Synthesis:** Provides Algorithm 2, a detailed pseudocode for the DPO pair construction process, including all fallback rules for selecting preferred and rejected candidates.
- **A.5 Additional Co-evolutionary Curves:** Figure 5 presents four additional plots showing the collaborative performance of the frozen and local models, reinforcing the main text's claim that their combined trajectory surpasses either model alone.
- **B Ethics Statement:** States that the work adheres to the ICLR Code of Ethics, used public datasets (ZINC) in compliance with guidelines, and involved no human/animal subjects or PII.
- **C Reproducibility Statement:** Claims that all code and datasets are publicly available in an anonymous repository and that the experimental setup is described in detail to ensure reproducibility.
- **D. LLM Usage:** Discloses that an LLM was used to aid in writing and polishing the manuscript's language but was not involved in the research ideas, methodology, or experiments. The authors take full responsibility for all content.