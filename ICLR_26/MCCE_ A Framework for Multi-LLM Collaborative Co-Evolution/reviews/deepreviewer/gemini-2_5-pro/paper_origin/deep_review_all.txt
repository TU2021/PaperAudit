Here are four distinct reviews of the paper "MCCE: A FRAMEWORK FOR MULTI-LLM COLLABORATIVE CO-EVOLUTION".

***

### **Review 1**

**Summary**

This paper introduces the Multi-LLM Collaborative Co-evolution (MCCE) framework, designed to address multi-objective discrete optimization problems. The core idea is to combine a large, frozen, closed-source LLM for broad exploration with a smaller, trainable, open-source LLM for experience-driven local adaptation. The framework uses an evolutionary loop where the small model is continuously refined using Direct Preference Optimization (DPO) on "breakthrough" trajectories identified during the search. A novel similarity-based data synthesis method is proposed to construct stable training pairs for DPO. The authors demonstrate the effectiveness of MCCE on a challenging five-objective molecular design task, showing that it achieves state-of-the-art performance compared to single-model baselines and alternative training paradigms.

**Soundness**

The methodological approach is sound and well-justified. The core premise—that a hybrid system can leverage the strengths of both large, static models and smaller, adaptable ones—is compelling. The paper provides a convincing rationale for choosing DPO over SFT and RL (Section 4.2), citing issues like catastrophic forgetting and training instability, which are common pitfalls. The proposed similarity-based data synthesis for DPO (Section 4.3) is a clever solution to the problem of training signal stability and distribution shift, which often plagues preference-based learning in generative tasks. The experimental setup is rigorous, employing a challenging 5-objective benchmark, standard evaluation metrics (Hypervolume), and multiple runs to ensure statistical significance. The ablation studies effectively demonstrate the necessity of the learning component and the superiority of the DPO-based approach.

**Presentation**

The paper is well-written and clearly structured. The introduction effectively motivates the problem and situates the work within the existing literature. The methodology is described in a logical sequence, from the high-level framework to the specific details of the DPO training. Figure 2 provides a clear and compelling visualization of the main results, effectively showing MCCE's superiority. Table 1 is comprehensive and provides a detailed breakdown of performance across multiple metrics. While Figure 1 gives a good overview, it is slightly cluttered. The "co-evolutionary curve" in Figure 3 (left) could be more intuitive; it currently shows two separate performance curves rather than a direct visualization of collaborative dynamics. Overall, however, the presentation is strong and effectively communicates the paper's contributions.

**Contribution**

The primary contribution is the MCCE framework itself—a novel and practical paradigm for continuous learning in LLM-based optimization. By enabling a small model to learn from experience while being guided by a powerful but static large model, the work charts a path around the key limitation of closed-source APIs. This concept of "mutual inspiration" (Section 2) where the system's overall capability grows is a significant step beyond simple model distillation or ensembling. The similarity-based data synthesis technique is also a valuable technical contribution for stabilizing DPO in complex generative domains. The demonstration of state-of-the-art performance on a difficult real-world problem underscores the practical significance of this work.

**Strengths**

1.  **Novel Framework:** The MCCE framework presents an innovative and well-reasoned architecture for combining the strengths of different LLM types for optimization.
2.  **Strong Empirical Results:** The method achieves state-of-the-art performance on a challenging multi-objective drug design benchmark, clearly outperforming well-chosen baselines (Table 1, Figure 2).
3.  **Justified Methodological Choices:** The paper provides a strong rationale for using DPO and develops a novel data synthesis technique (Section 4.3) to make it work effectively, addressing common training issues.
4.  **Generalizability:** The proposed framework is presented as a general-purpose optimization tool, with potential applications far beyond the tested domain of molecular design.

**Weaknesses**

1.  **"Co-evolution" Claim:** The term "co-evolution" might be slightly overstated, as the large, closed-source LLM is frozen and does not have its parameters updated. While its inputs (prompts) evolve with the population, the model itself does not. The learning is one-directional at the parameter level.
2.  **Visualization of Collaboration:** The "co-evolutionary curve" in Figure 3 (left) does not fully visualize the collaborative dynamic. It shows that both models contribute to finding good solutions over time, but not necessarily *how* they interact or complement each other at different stages.

**Questions**

1.  The paper argues for "mutual inspiration" and "co-evolution." Could the authors elaborate on how the frozen large model is "inspired" or "evolves"? Is it purely through the evolving quality of the parent solutions it receives in its prompts, or is there a more direct mechanism?
2.  In the candidate generation stage (Section 4.1), how is the alternation between the frozen API model and the local trainable model managed? Is it a fixed 50/50 split, or is the ratio adapted during the evolutionary process?
3.  The similarity-based data synthesis introduces several hyperparameters (α, similarity windows, etc.). How sensitive is the framework's performance to these settings? A brief sensitivity analysis would strengthen the paper's claims of robustness.

**Rating**

-   Overall (10): 9 — The paper presents a novel, well-motivated framework with strong empirical validation on a challenging task.
-   Novelty (10): 9 — The hybrid co-evolutionary concept with DPO-based learning is highly original and significant.
-   Technical Quality (10): 9 — The methodology is sound, and the DPO data synthesis technique (Section 4.3) is a strong technical contribution.
-   Clarity (10): 8 — The paper is mostly clear, but some visualizations like Figure 3 (left) could be improved to better support the narrative.
-   Confidence (5): 5 — I am highly confident in my assessment, as I am familiar with research in LLM-based optimization and evolutionary algorithms.

***

### **Review 2**

**Summary**

This paper proposes MCCE, a framework for multi-objective optimization that uses a frozen, large LLM (the "explorer") in collaboration with a smaller, trainable LLM (the "adapter"). The system follows an evolutionary algorithm structure, where the two LLMs act as operators to generate new candidate solutions. The core novelty lies in training the smaller model via Direct Preference Optimization (DPO) using data synthesized from the search history. This synthesis method, based on molecular similarity and score stratification, aims to create stable preference pairs. The authors evaluate MCCE on a 5-objective molecular design problem and report superior performance over baselines, including single-model approaches and variants using SFT or RL for training.

**Soundness**

The overall concept is interesting, but the technical soundness of the proposed training method raises several concerns. The "Similarity-based Data Synthesis" (Section 4.3) is complex and introduces a considerable number of hyperparameters (α, I₁, I₂, I₃, L, r, the choice of similarity metric). The paper does not provide a sensitivity analysis for these, making it difficult to assess the method's robustness and generalizability. The stability of this process seems fragile; for instance, the fallback rules (Algorithm 1 and 2) suggest that finding ideal pairs is not guaranteed.

Furthermore, the justification for DPO over other methods is somewhat incomplete. While SFT and RL issues are noted, the appendix (Section A.3) mentions GFlowNets, a paradigm explicitly designed for generating diverse candidates proportional to a reward. Why was this not included as a main baseline in the experiments, given its relevance?

Finally, many cited works are from "2025" (e.g., Sun et al., 2025; Zhao et al., 2025; Ran et al., 2025). While these are likely preprints, this is an unusual and potentially confusing citation practice that should be standardized to the year of publication/preprint.

**Presentation**

The presentation has several weaknesses that hinder a full understanding of the method. Section 4.3, which contains the core technical contribution, is very dense and difficult to parse. The simplified Algorithm 1 is too abstract to be useful, forcing the reader to hunt for details in the appendix (Algorithm 2). The main text should provide a clearer, more self-contained description of this crucial component.

The visualizations are also problematic. The flowchart in Figure 1 is busy, and the mermaid diagram in Block 16 is completely different from the graphical version in Block 19, which is confusing. More importantly, the "co-evolutionary curve" (Figure 3, left) is misleading. It simply plots the `avg_top1` score for each model's generated candidates over time. This does not show "collaboration" or "complementarity" but rather that the system as a whole is improving. A more insightful plot might show, for instance, the diversity of proposals from the API model versus the average quality of proposals from the local model over time.

**Contribution**

The paper's central idea—a learning-based collaboration between heterogeneous LLMs—is novel. The attempt to create a stable DPO training pipeline for this context is also a contribution. However, the significance is undermined by the methodological opacity and potential fragility of the data synthesis process. The claim of "co-evolution" is not fully supported, as the large model is static. The contribution is more accurately described as a guided, online fine-tuning of a small model within a larger evolutionary system. While the results are strong, the complexity and lack of robustness analysis of the method may limit its practical adoption.

**Strengths**

1.  **Interesting Core Idea:** The concept of using a trainable small model to continuously adapt and specialize within a search process guided by a large generalist model is promising.
2.  **Strong Empirical Benchmark:** The use of a 5-objective molecular design task provides a challenging and relevant testbed for the proposed method.
3.  **Good Baseline Comparison:** The paper includes important baselines, such as single models and alternative training methods (SFT, RL), which helps contextualize the performance of the proposed DPO approach (Table 1).

**Weaknesses**

1.  **Complex and Under-analyzed Training Method:** The similarity-based data synthesis (Section 4.3) is convoluted and introduces many hyperparameters without any sensitivity analysis, questioning its robustness.
2.  **Misleading Visualizations:** The "co-evolutionary curve" (Figure 3, left) does not adequately support the claims of collaboration and complementarity.
3.  **Overstated "Co-evolution" Claim:** The learning is one-way (only the small model's parameters are updated), making the term "co-evolution" an overstatement. The system evolves, but the models do not "co-evolve" in a symmetric sense.
4.  **Incomplete Baseline Comparison:** GFlowNets are mentioned as a relevant paradigm (Appendix A.3) but are not compared against in the main experiments, which is a missed opportunity for a more thorough evaluation.

**Questions**

1.  Can the authors provide a sensitivity analysis for the key hyperparameters in the similarity-based data synthesis pipeline (e.g., α, the similarity thresholds for I₁, I₂, I₃)? How much tuning was required to achieve the reported results?
2.  Why were GFlowNets, which seem highly relevant for generating diverse, high-reward samples, mentioned in the appendix but not included as a baseline in the main experimental comparison in Section 5.2?
3.  The DPO training relies on a scalarized score or ranking for stratification (Section 4.3). How is this scalarization performed for the 5-objective problem, and how does the choice of scalarization method impact the learning process?
4.  Could you clarify the discrepancy between the mermaid diagram (Block 16) and the graphical flowchart (Block 19)? Which one more accurately represents the framework?

**Rating**

-   Overall (10): 6 — The core idea is interesting, but the technical execution is complex and lacks rigorous analysis, and the presentation has significant clarity issues.
-   Novelty (10): 7 — The hybrid learning framework is novel, but the "co-evolution" claim is overstated.
-   Technical Quality (10): 5 — The data synthesis method is opaque, introduces many unanalyzed hyperparameters, and the experimental comparison is missing a key baseline (GFlowNets).
-   Clarity (10): 5 — Key methodological sections are dense, and several figures are confusing or misleading.
-   Confidence (5): 5 — I am very confident in my assessment, having expertise in both machine learning methodology and its application to scientific discovery.

***

### **Review 3**

**Summary**

This paper proposes MCCE, a hybrid AI framework for multi-objective optimization. It combines a large, static, API-based LLM with a smaller, fine-tunable LLM in an evolutionary search loop. The large model acts as a general-purpose explorer, while the small model is continuously trained on successful search trajectories using Direct Preference Optimization (DPO). The goal is to create a system that learns from experience to improve its optimization capability over time. The authors apply this framework to a multi-objective drug design problem and show that it outperforms several baselines, including systems that use only one model or do not involve parameter updates.

**Soundness**

The methodology appears sound from a theoretical standpoint. The idea of using a powerful but expensive model for exploration and a cheaper, adaptable model for exploitation/local search is a classic strategy in optimization, and its application here is logical. The use of DPO to learn from preferences ("this molecule is better than that one") is more robust than trying to learn from absolute scores via standard RL, as argued convincingly in Section 4.2. The experimental results, particularly the significant improvement in Hypervolume shown in Figure 2 and Table 1, suggest the method is effective in practice for the chosen task.

**Presentation**

The paper is generally well-presented, with a clear narrative flow. The introduction sets up the problem well, and the results section clearly highlights the superior performance of the MCCE framework. However, the paper is missing a crucial discussion on the practical aspects of the framework, namely computational cost and efficiency. The use of a state-of-the-art API model like GPT-4o implies significant financial cost and latency. This is a major barrier to real-world adoption and should be discussed. For example, how many API calls are made during a typical run? What is the wall-clock time for an experiment compared to the baselines? This information is essential for evaluating the practical viability of MCCE.

**Contribution**

The main contribution is a practical framework that enables continuous learning in LLM-based optimizers by cleverly circumventing the fine-tuning restrictions of powerful, closed-source models. This is a significant contribution, as it provides a tangible path for building smarter optimization tools that improve with use. The state-of-the-art results on a complex drug design benchmark demonstrate the real-world potential of this approach. The work's value lies not just in the novelty of the algorithm but in its demonstrated effectiveness and its potential to be generalized to other complex engineering and scientific design problems.

**Strengths**

1.  **Practical Effectiveness:** The framework achieves state-of-the-art results on a realistic and challenging problem (Table 1, Figure 2), demonstrating its practical value.
2.  **Clever Use of Resources:** MCCE provides a smart solution to leverage the power of closed-source LLMs while still incorporating experience-based learning through a smaller, open model.
3.  **Generalizable Paradigm:** The framework is not inherently tied to molecular design and could be adapted to other discrete optimization domains where expert knowledge (encoded in the LLM) is valuable.
4.  **Robust Learning Signal:** The choice of DPO over RL or SFT is well-justified and likely a key reason for the method's success and stability.

**Weaknesses**

1.  **No Discussion of Cost/Efficiency:** The paper completely ignores the practical costs (financial, computational, time) associated with using a premium API like GPT-4o. This is a major omission for a paper proposing a practical framework.
2.  **Limited Generalization Evidence:** While the paper claims the framework is broadly applicable (Contribution 1), it is only demonstrated on a single domain (molecular design). A discussion of what would be required to adapt it to a different domain (e.g., logistics, chip design) would strengthen this claim.
3.  **Dependence on API Model:** The system's performance is heavily dependent on the quality of the frozen LLM. If the API provider changes the model or its pricing, it could drastically affect the framework's utility. This dependency is a practical risk that is not discussed.

**Questions**

1.  Could you provide an analysis of the computational cost of MCCE? Specifically, what is the approximate number of API calls to the large LLM and the wall-clock time required for a full experimental run compared to the baselines?
2.  The framework's success seems to depend on the large LLM's ability to perform "global exploration." How do you think MCCE would perform if a less capable (and cheaper) API model were used as the frozen component?
3.  You claim the framework is broadly applicable. What do you foresee as the main challenges in adapting MCCE to a completely different discrete optimization domain, such as vehicle routing or hardware design? What components would need to be redesigned?

**Rating**

-   Overall (10): 8 — A highly effective and innovative framework, but its practical value is hard to fully assess without a discussion of computational costs.
-   Novelty (10): 8 — The hybrid learning architecture is a novel and practical approach to LLM-based optimization.
-   Technical Quality (10): 8 — The method is technically sound and empirically validated, though a cost analysis is missing.
-   Clarity (10): 7 — The paper is clearly written, but the omission of any discussion on cost and efficiency is a significant clarity gap for a practical method.
-   Confidence (5): 5 — I am confident in my review, with a background in applied AI and optimization.

***

### **Review 4**

**Summary**

This paper introduces MCCE, a framework that combines a large, frozen LLM with a smaller, trainable LLM for multi-objective discrete optimization. The system operates as an evolutionary algorithm where the models generate candidate solutions. The smaller model is periodically updated via Direct Preference Optimization (DPO), learning from high-performing trajectories. To facilitate this, the authors propose a "similarity-based data synthesis" method to create preference pairs. Experiments on a molecular design task show that this collaborative, learning-based approach outperforms single-model systems and other training strategies.

**Soundness**

The conceptual foundation of the paper seems plausible. The idea of combining a generalist explorer with a specialist learner is a solid design pattern. The empirical results presented in Figure 2 and Table 1 appear strong, suggesting the method is effective. However, the clarity of the methodological description is poor in places, which makes it difficult to fully verify the soundness of the implementation. For example, the description of the similarity-based data synthesis in Section 4.3 is convoluted, and the simplified Algorithm 1 is so high-level it's almost uninformative, deferring all crucial details to the appendix. A clearer explanation is needed to be fully confident in the technical execution.

**Presentation**

The presentation of this paper requires significant improvement. While the overall structure is logical, there are numerous issues with clarity and visual aids.

1.  **Figure 1:** The main workflow diagram (Block 19) is cluttered and contains a typo ("experiencce"). Furthermore, it is inconsistent with the mermaid diagram provided in the text (Block 16), which describes a different flow. This is confusing for the reader.
2.  **Figure 3 (left):** The "co-evolutionary curve" is poorly named and visualized. It simply overlays two performance curves. It fails to illustrate the "collaboration" or "complementary" roles the text claims it shows. This is a key claim of the paper that is not supported by its visualization.
3.  **Figure 3 (right):** The image provided in the paper text (Block 34, right side) is a duplicate of the left-side image. The actual fitness distribution plot is only visible in the extracted image block (Block 37). This is a formatting error that needs to be fixed.
4.  **Methodology Description (Section 4.3):** This section is the paper's core technical novelty but is written in an overly dense and formula-heavy way that obscures intuition. The concepts of similarity windows and fallback rules need to be explained more clearly in the main text, perhaps with a small, concrete example.
5.  **Algorithm 1:** This algorithm is too simplified. It omits all the key logic (filtering, relaxation, fallbacks), making it a poor summary of the actual method detailed in Algorithm 2 in the appendix. The main text should contain a more representative pseudocode.

**Contribution**

The paper's proposed contribution—a co-evolutionary framework for LLM-based optimization—is interesting and potentially impactful. The idea of enabling continuous learning in a system that leverages a static, closed-source model is novel. However, the paper's contribution is currently obscured by its poor presentation. A reader must work very hard to understand the technical details of the data synthesis method, and key claims about "collaboration" are not well-supported by the figures. The paper would be much stronger if it were revised for clarity.

**Strengths**

1.  **Core Concept:** The central idea of a hybrid, learning-based LLM optimizer is strong and well-motivated.
2.  **Main Results Visualization:** Figure 2 (avg_top1 and hypervolume curves) is clear, standard, and effectively communicates the primary finding that MCCE outperforms baselines.
3.  **Comprehensive Results Table:** Table 1 provides a wealth of data comparing the different methods across many relevant metrics.

**Weaknesses**

1.  **Poor Clarity in Methodology:** The description of the novel data synthesis method (Section 4.3) is dense and difficult to follow.
2.  **Misleading and Inconsistent Figures:** Figure 1 is cluttered and inconsistent. Figure 3 (left) is misleadingly labeled and does not support the paper's claims about collaboration. There are also formatting errors with Figure 3 (right).
3.  **Over-reliance on Appendix:** Key details of the algorithm are relegated to the appendix, making the main paper less self-contained than it should be.
4.  **Minor Errors:** The paper contains typos (e.g., "experiencce" in Figure 1) that suggest a lack of careful proofreading.

**Questions**

1.  Could you please revise Figure 1 for clarity and ensure it is consistent with the text description? What is the intended difference between the diagram in Block 16 and the one in Block 19?
2.  Can you provide a better visualization to support the claim that the two models "complement each other" (Section 5.2.2)? The current "co-evolutionary curve" in Figure 3 (left) is not sufficient. For example, a plot showing the diversity of proposals from the API model versus the quality of proposals from the local model over time might be more illustrative.
3.  Could you provide a more intuitive, step-by-step explanation of the similarity-based data synthesis method in the main text, perhaps with a small example, to make Section 4.3 more accessible?
4.  The image for Figure 3 (right) appears to be a duplicate of Figure 3 (left) in the main text block (Block 34). Please correct this formatting error.

**Rating**

-   Overall (10): 5 — The paper has a promising core idea and strong results, but is severely hampered by poor presentation, unclear methodology, and misleading figures.
-   Novelty (10): 7 — The framework concept is novel, but its description and substantiation are lacking.
-   Technical Quality (10): 6 — The results seem strong, but the opaqueness of the method's description makes it difficult to fully assess its technical soundness and reproducibility.
-   Clarity (10): 3 — The paper suffers from significant clarity issues in the methodology section and in its choice of visualizations, which is a major flaw.
-   Confidence (5): 4 — I am confident in my assessment of the presentation quality, though less confident about the technical quality due to the lack of clarity.