### Summary

This submission presents a **benchmarking study and toolbox** for **fairness-constrained training of deep neural networks** framed as a constrained ERM / stochastic programming problem. Built on **US Census / Folktables (ACS)** data, the benchmark aims to provide a standardized and reproducible testbed for comparing **stochastic approximation / constrained optimization algorithms** under common group fairness constraints. The paper (i) offers a structured **review and taxonomy** of relevant constrained stochastic optimization methods, (ii) implements several recently proposed but previously unavailable-in-code algorithms (e.g., **Stochastic Ghost**, **SSL-ALM / linearized augmented Lagrangian**, **stochastic switching subgradient**; with some reviewers also mentioning an ALM variant), and (iii) evaluates them under multiple fairness notions (e.g., **independence/statistical parity**, **separation/equalized odds**, **sufficiency**, and additional distributional criteria such as **Wasserstein distance**) while tracking optimization performance and fairness violations.

Across reviewers, there is agreement that a **unified, open benchmark/toolkit** for fairness-constrained deep learning is useful and that the paper’s effort toward reproducibility is valuable. One reviewer praises the clarity and the structured literature table/taxonomy, and another highlights that the unified implementations enable meaningful head-to-head comparisons. However, several reviewers question whether the contribution is **too incremental for ICLR**, emphasizing that the paper largely **implements and compares existing algorithms** on an already established dataset family (ACS/Folktables), with limited evidence for broad generality beyond the presented setup. Key recurring concerns include **scope limitations of the empirical validation**, **insufficient justification of novelty/positioning**, and requests for clearer benchmark description, better result presentation, and more rigorous hyperparameter search (which the authors claim to have expanded in revision, including multi-group experiments and hyperparameter sensitivity analyses).

### Strengths

* **Fills a tooling gap for fairness-constrained optimization:** Reviewers acknowledge value in providing a **reproducible and extensible benchmark framework** and an accompanying toolbox for constrained training, addressing the lack of unified evaluation platforms in this area.
* **Clear exposition and structured review (for some reviewers):** The writing and notation are considered consistent by at least one reviewer, and the **taxonomy-style table summarizing methods and assumptions** is highlighted as particularly helpful.
* **Unified implementation enables fair comparisons:** Running multiple constrained optimization algorithms under identical pipelines/datasets/metrics is viewed as a strong practical contribution, helping practitioners and researchers understand trade-offs.
* **Covers multiple fairness notions and constraints:** Evaluating several widely used group fairness criteria (independence/separation/sufficiency, plus an additional distributional metric) is seen as a positive, enabling nuanced trade-off analysis.

### Weaknesses

* **Perceived limited novelty / incremental contribution:** Multiple reviewers argue the paper primarily **implements and benchmarks existing algorithms** rather than proposing new methodology, which may limit perceived research contribution even in a benchmark track.
* **Positioning and contribution clarity issues:** Some reviewers find the paper’s structure and messaging unclear—too much space is devoted to literature review relative to clearly stating **what is new in the benchmark**, why it matters, and what the main empirical takeaways are.
* **Empirical scope concerns:** Reviewers note heavy reliance on **ACS/Folktables-derived tasks**; some view this as narrow for claims of general benchmarking, and ask for broader domains (e.g., vision/language) or clearer justification of why the chosen tasks are sufficiently “challenging” and representative.
* **Experimental presentation and readability:** Figures/results are described as hard to read in the initial version, and one reviewer requests clearer reporting and better integration of results.
* **Hyperparameter selection and fairness-threshold sensitivity:** At least one reviewer flags that missing or insufficient hyperparameter search could confound conclusions (algorithm differences vs. tuning differences). Reviewers also ask how thresholds (e.g., δ) and other constraint hyperparameters affect convergence and outcomes.
* **Scalability and realism gaps vs. stated claims:** While the paper mentions very large subgroup spaces, reviewers note the experiments do not convincingly demonstrate behavior at that scale, and request clearer discussion of **computational cost, runtime, and memory**, especially for methods involving QP subproblems.
