Summary
The paper proposes a benchmarking framework for fairness-constrained training of deep neural networks under stochastic approximation. It formalizes fairness constraints as bounds on expected loss differences between subgroups (Table 1, Eq. (4)), reviews recent constrained stochastic optimization methods (Table 2), and implements four algorithms—Stochastic Ghost (StGh, Sec. 3.2), Stochastic Smoothed and Linearized Augmented Lagrangian (SSL-ALM, Sec. 3.3), a plain ALM variant, and Stochastic Switching Subgradient (SSw, Sec. 3.4). Experiments on the ACSIncome dataset (Oklahoma) evaluate optimization behavior and fairness metrics (independence, separation, sufficiency), comparing to SGD baselines and a differentiable fairness regularizer (Fairret).

Soundness
The methodological setup is reasonable for empirical benchmarking but has conceptual and technical caveats. The fairness constraint used in training is the difference in expected loss across groups (Eq. (4)), while evaluation employs standard probability-based fairness metrics (Ind/Sp/Sf; Sec. 2)—this mismatch complicates interpreting causal effects of the constraints on the metrics. The adaptation of SSL-ALM (Sec. 3.3) from linear equality/inequality constraints to nonlinear inequality constraints via slack variables is plausible but lacks theoretical justification in this setting, which the authors openly acknowledge (Limitations). StGh requires gradients of C and F (Eqs. (7–8)) that are not everywhere C^1 with ReLU networks (Sec. 4.2 network spec), further underscoring the absence of guarantees. The experimental design uses 10 runs and reports means/variances (Table 3), which is good practice, but hyperparameter search is limited and SSw is reported as sensitive (Sec. 4.2; Appendix B). The constrained sampling strategy balances subgroups (Sec. 4.2), which is appropriate for estimating constraints. A notable omission is the value of δ in Eq. (4); figures suggest ±0.1 bounds (Figs. 1–2 captions mention “Constraint bound”), but the paper does not explicitly specify δ, making reproducibility and interpretation harder.

Presentation
The paper is mostly clear and well organized, with a crisp review of methods (Sec. 3.1, Table 2) and algorithm pseudocode (Appendix A). Figures of optimization trajectories (Figs. 1–2) and fairness results (Fig. 3, Table 3, Fig. 4) support claims, although figures are small and hard to read in places (e.g., many overlapping curves in Figs. 1–2; small density plots in Figs. 35–40). There is a minor inconsistency between the abstract (“three recently proposed, but as-of-yet unimplemented, algorithms”) and Sec. 4.2 reporting four algorithms including a plain ALM baseline. The code link in the abstract appears as https://github.com/humancompatible/train, which may be a placeholder and should be verified.

Contribution
The main contribution is an empirical benchmark and toolbox focused specifically on fairness-constrained ERM with stochastic approximation—an underserved area relative to differentiable fairness regularization or post-processing (Sec. 2). The comparative analysis across algorithms designed for stochastic constrained optimization (Table 2; Sec. 3.2–3.4) and their empirical behavior on a real dataset (Sec. 4) provides valuable insights for practitioners and researchers about practical trade-offs (e.g., ALM/SSL-ALM stabilizing constraints; StGh achieving stronger fairness at accuracy cost; SSw sensitivity). The claim of “first benchmark” (Conclusion) is qualified in Sec. 2 by contrasting with FFB [30], which focuses on differentiable methods; the niche is credible. However, the empirical scope (single dataset, single architecture, single fairness constraint form) limits generality.

Strengths
- Focused benchmark on constrained optimization approaches for fairness (Sec. 3.1; Table 2), filling a gap vis-à-vis differentiable-only toolkits/benchmarks (Sec. 2).
- Clear algorithmic exposition and availability of pseudocode (Appendix A; Eqs. (7–11)).
- Sensible experimental protocol with multiple runs and reporting of means/variances (Table 3).
- Practical insights on algorithm behavior (ALM/SSL-ALM vs. StGh vs. SSw) and constraint satisfaction vs. accuracy trade-offs (Sec. 4.2, Figs. 1–2; Table 3).

Weaknesses
- Training constraint (loss-gap bound; Eq. (4)) does not directly optimize the evaluated fairness metrics (Ind/Sp/Sf; Sec. 2), making causal interpretation tenuous.
- Missing explicit δ specification for constraints (Eq. (4)); only implicit bounds appear in figures (Figs. 1–2), hindering reproducibility.
- Limited empirical scope: single state/task (ACSIncome Oklahoma), single network architecture, no multi-dataset or multi-constraint ablations despite claiming up to 5.7B protected subgroup definitions (Sec. 1).
- Theoretical adaptation of SSL-ALM to nonlinear stochastic inequality constraints is not justified; no convergence guarantees (Limitations; Sec. 3.3).
- SSw sensitivity noted but hyperparameter exploration seems narrow (Sec. 4.2; Appendix B).
- Code link in abstract may be a placeholder; needs verification.

Questions
- What is the exact value of δ used in Eq. (4) for train/test, and how was it selected (Sec. 4.2; Figs. 1–2 show bounds)? Please report δ explicitly and conduct sensitivity analysis.
- Why choose loss-difference constraints (Table 1/Eq. (4)) rather than constraints directly tied to Ind/Sp/Sf (rates), given [16]’s framework? Could you add experiments with rate constraints?
- How robust are the findings across different Folktables tasks/states and architectures? Any results on ACSEmployment or ACSMobility?
- Can you clarify the code repository URL and provide instructions/scripts for reproducing Table 3 and Figs. 1–4?
- For SSL-ALM, can you elaborate on the effect of slack variables and smoothing on feasibility and stationarity in the nonlinear inequality case (Sec. 3.3), and include empirical checks of KKT residuals?
- Could you add ablations on mini-batch sizes for StGh (Eq. (8–9)) and on subgroup sampling strategies (Sec. 4.2)?
- How is the classification threshold set for computing Ind/Sp/Sf? Is calibration applied to improve sufficiency?

Rating
- Overall (10): 6 — Useful empirical benchmark and toolbox targeted at constrained fairness methods with clear comparative insights, but limited scope and a mismatch between training constraint and evaluation metrics (Sec. 3.1–3.4; Sec. 4.2; Table 3).
- Novelty (10): 6 — Distinct focus on constrained stochastic optimization for fairness relative to differentiable-only benchmarks/toolkits (Sec. 2; Table 2), though empirical setup is standard.
- Technical Quality (10): 5 — Sound empirical methodology with multiple runs, but lacking explicit δ, limited ablations, and no guarantees or justification for nonlinear constraints in SSL-ALM (Eq. (4); Sec. 3.3; Limitations).
- Clarity (10): 7 — Generally well written with helpful tables/pseudocode (Table 2; Appendix A), though figures are small and a minor inconsistency in algorithm count exists (Abstract vs. Sec. 4.2).
- Confidence (5): 4 — High familiarity with fairness-constrained optimization and benchmarks; assessment grounded in specific sections/figures/tables, though code verification not performed.


Summary
This paper introduces a benchmarking package and empirical study for training deep neural networks under fairness constraints formulated as bounds on expected loss differences between protected and non-protected groups (Table 1, Eq. (4)). It reviews constrained stochastic optimization methods (Table 2) and implements Stochastic Ghost (Sec. 3.2), SSL-ALM (Sec. 3.3), ALM, and SSw (Sec. 3.4), comparing them to SGD and Fairret on the ACSIncome task (Sec. 4), reporting optimization behavior (Figs. 1–2) and fairness metrics (Table 3, Fig. 4).

Soundness
The experimental design is coherent and highlights key trade-offs, but the methodological choices require scrutiny. The fairness constraint targets expected loss differences (Eq. (4)), while evaluation uses probability-based metrics (Ind/Sp/Sf; Sec. 2), so improvements in constraints do not directly imply improvements in the reported fairness criteria. The theoretical coverage of the chosen algorithms for nonconvex, nonsmooth, stochastic inequality constraints is incomplete, as the authors acknowledge (Sec. 3.1; Limitations). In particular, SSL-ALM is designed for linear constraints (Sec. 3.3) and its application to nonlinear constraints is heuristic; StGh requires differentiability of C and F (Eqs. (7–8)), which is violated by ReLU-based networks (Sec. 4.2). The SSw implementation shows sensitivity (Sec. 4.2, Appendix B) and the paper indicates difficulty finding parameters that jointly enforce constraints and minimize the objective. Reporting of δ is missing; only constraint-bound lines appear in figures (Figs. 1–2), obstructing reproducibility. Still, the empirical reporting over 10 runs and subgroup-balanced constraint sampling (Sec. 4.2) are solid.

Presentation
The manuscript is well-structured with clear motivation (Sec. 1), background (Sec. 2), method review (Sec. 3.1), algorithm details (Sec. 3.2–3.4; Appendix A), and empirical results (Sec. 4). Figures and tables are informative, though figure readability is limited by scale and overlapping lines (Figs. 1–2, 3). Minor inconsistencies exist: the abstract mentions three implemented algorithms while Sec. 4.2 evaluates four (ALM included). The repository URL should be verified and a reproducibility checklist or script references would help.

Contribution
The work adds value by centering constrained stochastic optimization methods in fairness benchmarking, differentiating from existing benchmarks focusing on differentiable penalties or post-processing (Sec. 2; Table 2). Providing pseudocode and parameterizations aids practitioners. Empirical insights—ALM/SSL-ALM offering better constraint satisfaction, StGh stronger fairness at accuracy cost, SSw sensitivity (Sec. 4.2; Table 3)—are useful. However, the scope is narrow (single dataset/task/architecture; single constraint form), and the training constraint does not directly align with fairness metrics used for evaluation, limiting interpretability.

Strengths
- Focused benchmarking of constrained stochastic algorithms, a relatively underexplored niche (Table 2; Sec. 3.1).
- Transparent reporting with repeated runs and variance (Table 3).
- Practical pseudocode and parameter disclosures (Appendix A; Sec. 4.2).
- Balanced qualitative analysis of algorithm behavior (Sec. 4.2; Appendix B).

Weaknesses
- Mismatch between training constraints and evaluation metrics (Eq. (4) vs. Sec. 2), weakening causal claims about fairness improvements.
- Missing explicit δ; constraint bounds only visible in figures (Figs. 1–2).
- Narrow empirical scope; no multi-dataset or multi-constraint evaluation despite claims of many subgroup definitions (Sec. 1).
- Minimal theoretical support for SSL-ALM with nonlinear stochastic inequality constraints (Sec. 3.3).
- SSw parameter sensitivity; limited exploration (Sec. 4.2; Appendix B).
- Minor narrative inconsistency regarding the number of implemented algorithms (Abstract vs. Sec. 4.2).

Questions
- Please specify δ values used in Eq. (4) and provide sensitivity analysis across δ; how do δ choices impact Table 3 metrics?
- Could you add experiments using rate-based constraints (Ind/Sp) as in [16] to better align training with evaluated metrics?
- What is the impact of network architecture and calibration on sufficiency (Sf)? Did you consider calibrated thresholds?
- Can you expand the benchmark to additional Folktables tasks, multiple states, and different protected attributes to substantiate the “challenging benchmark” claim (Sec. 1)?
- What are the exact mini-batch sizes used for constraints and objective across algorithms (Sec. 4.2), and how sensitive are outcomes to these sizes?
- Please confirm the repository link and include scripts to reproduce Figs. 1–4 and Table 3.

Rating
- Overall (10): 5 — The benchmark addresses a valuable niche with informative comparisons, but limited scope, missing δ specification, and training–evaluation mismatch constrain its impact (Eq. (4); Sec. 4.2; Table 3).
- Novelty (10): 5 — Moderate novelty in centering constrained stochastic optimization for fairness benchmarking (Sec. 2; Table 2), though empirical setup is conventional.
- Technical Quality (10): 4 — Competent empirical work but sparse ablations, lack of δ, and heuristic adaptation of SSL-ALM to nonlinear constraints (Sec. 3.3; Limitations).
- Clarity (10): 6 — Generally clear, with useful tables/pseudocode, but figures are small and minor inconsistencies exist (Figs. 1–3; Abstract vs. Sec. 4.2).
- Confidence (5): 3 — Reasonable confidence based on close reading of methods and results; did not verify code/external repo.


Summary
The paper builds a benchmark and toolkit for fairness-constrained ERM, framing fairness as constraints on expected loss differences (Table 1; Eq. (4)) and empirically comparing four stochastic approximation algorithms (StGh, SSL-ALM, ALM, SSw) to SGD baselines on ACSIncome (Sec. 4). It reviews algorithmic assumptions (Table 2), details stochastic formulations (Eqs. (7–11)), and reports optimization and fairness metrics (Figs. 1–2; Table 3; Fig. 4).

Soundness
The approach is suitable for empirical insights but has conceptual limitations. Training goals (loss-gap constraints; Eq. (4)) are not the same as evaluation metrics (Ind/Sp/Sf; Sec. 2); thus improvements in constraints may not translate to improved fairness criteria. Algorithmic applicability to nonlinear inequality constraints is partly heuristic (SSL-ALM; Sec. 3.3) and differentiability assumptions for StGh (Eqs. (7–8)) conflict with ReLU nonsmoothness (Sec. 4.2). The paper honestly states the lack of general guarantees (Sec. 3.1; Limitations). Experimental methods are sound (10 runs, subgroup-balanced constraint sampling; Sec. 4.2), and results are consistent with expected trade-offs (ALM/SSL-ALM best compromise; StGh improves Ind/Sp but degrades accuracy; SSw sensitive—Table 3; Appendix B). Absence of explicit δ undermines precision and reproducibility.

Presentation
The manuscript is clear, with a thorough method review (Table 2), explicit algorithm steps (Appendix A), and comprehensive figures/tables. Some plots are dense and small (Figs. 1–2), and figure numbering/captions around multiple images could be tightened. The claim “three recently proposed” in the abstract vs. evaluating four algorithms (Sec. 4.2) should be reconciled. The code URL requires confirmation.

Contribution
This work’s main value is focusing a benchmark on constrained stochastic optimization methods for fairness, contrasting with existing differentiable-only benchmarks/tools (Sec. 2). It offers empirical guidance on algorithm selection and parameterization under fairness constraints, which is relevant for practice. However, the empirical coverage is narrow (one dataset, one constraint form), and the constraint–metric mismatch limits generality of conclusions.

Strengths
- Timely focus on constrained fairness optimization; useful comparative insights (Sec. 3.1; Table 2; Sec. 4.2).
- Clear algorithmic descriptions and pseudocode (Appendix A).
- Careful empirical reporting with multiple runs and dispersion (Table 3).
- Practical findings on constraint satisfaction and accuracy trade-offs (Figs. 1–2; Table 3; Appendix B).

Weaknesses
- Training constraint (loss-gap) misaligned with evaluated fairness metrics (Eq. (4) vs. Sec. 2).
- δ not specified; only implied in figures (Figs. 1–2).
- Limited datasets, architecture variety, and constraint types; no demonstration of “5.7 billion subgroups” in practice (Sec. 1).
- Incomplete theoretical support for SSL-ALM in the nonlinear stochastic inequality setting (Sec. 3.3).
- SSw sensitivity and limited hyperparameter exploration (Sec. 4.2; Appendix B).
- Minor inconsistency in algorithm count (Abstract vs. Sec. 4.2).

Questions
- Please report δ and its selection rationale; include sensitivity experiments across δ values.
- Can you add experiments with constraints directly on rates (Ind/Sp) as in [16] for alignment with evaluation metrics?
- What subgroup definitions beyond binary race (“white” vs. “non-white”) were tried? Any multi-attribute protected groups?
- How do results change with different architectures or regularization? Any calibration to improve sufficiency?
- What mini-batch sizes and sampling strategies were used for objective vs. constraints (Sec. 4.2), and how do they affect StGh/SSL-ALM behavior?
- Can you confirm the repository and provide a reproducibility checklist?

Rating
- Overall (10): 7 — A well-executed empirical benchmark in a niche area with clear insights, but scope and constraint–metric alignment issues reduce impact (Sec. 3.1–3.4; Sec. 4.2; Table 3).
- Novelty (10): 7 — Stronger novelty on the benchmarking focus for constrained stochastic methods vs. prior differentiable-only benchmarks (Sec. 2; Table 2).
- Technical Quality (10): 6 — Solid empirical work with repeated runs; theoretical guarantees are absent and δ unreported (Eq. (4); Sec. 3.3; Limitations).
- Clarity (10): 7 — Clear structure and helpful tables/pseudocode (Table 2; Appendix A), though figure readability is limited and minor text inconsistencies remain.
- Confidence (5): 4 — Good confidence from detailed reading and cross-referencing sections/figures; code not independently verified.


Summary
The paper develops a benchmark and open-source toolbox for training DNNs under fairness constraints using stochastic approximation, reviewing recent methods (Table 2) and implementing StGh (Sec. 3.2), SSL-ALM (Sec. 3.3), ALM, and SSw (Sec. 3.4). On ACSIncome (Oklahoma), the study compares optimization performance (Figs. 1–2) and fairness metrics (Table 3; Fig. 4) against SGD baselines, highlighting trade-offs between constraint satisfaction and accuracy.

Soundness
The empirical methodology is competent, with multiple runs and fairness metric reporting. However, the fairness constraint enforced (expected loss-gap; Eq. (4)) is not the same as evaluated fairness definitions (Ind/Sp/Sf; Sec. 2), weakening the inference that constraint enforcement improves these metrics. Algorithm suitability is mixed: SSL-ALM is designed for linear constraints and lacks theory in the nonlinear setting used (Sec. 3.3), StGh requires differentiability that ReLU networks lack (Eqs. (7–8); Sec. 4.2), and SSw shows sensitivity (Sec. 4.2; Appendix B). The paper candidly admits the absence of guarantees (Sec. 3.1; Limitations). Missing explicit δ further limits reproducibility and precise interpretation. Nonetheless, subgroup-balanced sampling for constraints (Sec. 4.2) and reporting of Wasserstein distances (Table 3) add robustness.

Presentation
The manuscript is readable and structured, with effective use of tables/pseudocode (Table 2; Appendix A). Figures are sometimes small and dense (Figs. 1–2), and the abstract’s count of implemented algorithms conflicts slightly with Sec. 4.2. The code URL should be verified and detailed reproduction instructions provided.

Contribution
The work’s distinct contribution is focusing a benchmark on constrained stochastic optimization methods for fairness—rare in current literature that tends to emphasize differentiable penalties/post-processing (Sec. 2). The implementation of StGh/SSL-ALM/SSw and public toolbox can catalyze further research. Yet, the empirical scope is narrow and the training constraint–metric mismatch hampers generality of conclusions.

Strengths
- Addresses an important gap by benchmarking constrained stochastic methods for fairness (Table 2; Sec. 3.1).
- Multiple runs with dispersion metrics (Table 3); subgroup-balanced constraint estimation (Sec. 4.2).
- Clear algorithmic exposition and pseudocode (Appendix A).
- Conveys practical trade-offs among methods (Sec. 4.2; Figs. 1–2; Table 3).

Weaknesses
- Training constraint misaligned with evaluated fairness metrics (Eq. (4) vs. Sec. 2).
- δ not disclosed; constraint bounds only visible in plots (Figs. 1–2).
- Limited evaluation scope (single dataset/task/architecture; single constraint form); no demonstration of broader subgroup configurations (Sec. 1).
- Theoretical fit of SSL-ALM to nonlinear stochastic inequality constraints is not substantiated (Sec. 3.3).
- SSw sensitivity with limited hyperparameter search (Sec. 4.2; Appendix B).
- Minor inconsistency in algorithm count (Abstract vs. Sec. 4.2); small figures reduce readability.

Questions
- Please report the δ used in Eq. (4) and detail how it was chosen; add sensitivity analyses.
- Can you include experiments with rate-based constraints (e.g., Ind/Sp constraints) to align training with evaluation metrics?
- How do results vary across other Folktables tasks/states and other protected attributes? Any multi-attribute subgroup experiments?
- What are the computational costs/throughput differences among methods (Sec. 4.2 hardware), and how do mini-batch sizes affect StGh variance (Eqs. (8–9))?
- Is the toolbox integrated with Fairlearn/AIF360 for metric computation, and can it generate multiple constraint formulations (Table 1) out-of-the-box?
- Please confirm the repository URL and add scripts for reproducing figures/tables.

Rating
- Overall (10): 6 — Valuable benchmark and toolbox with informative comparisons, but narrow scope and constraint–metric misalignment limit impact (Sec. 3.1–3.4; Sec. 4.2; Table 3).
- Novelty (10): 6 — Novel focus on constrained stochastic methods vs. differentiable-only benchmarks (Sec. 2; Table 2).
- Technical Quality (10): 5 — Solid empirical practices, but missing δ, limited ablations, and heuristic adaptation of methods to nonlinear constraints (Eq. (4); Sec. 3.3; Limitations).
- Clarity (10): 7 — Clear and organized, though figure readability and minor textual inconsistencies could be improved (Figs. 1–2; Abstract vs. Sec. 4.2).
- Confidence (5): 4 — Confident based on detailed cross-referencing of sections/figures; code not independently checked.