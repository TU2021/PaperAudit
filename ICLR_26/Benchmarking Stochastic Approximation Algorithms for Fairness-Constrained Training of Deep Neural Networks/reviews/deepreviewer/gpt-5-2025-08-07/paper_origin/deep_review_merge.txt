Summary
The paper presents a benchmarking framework and open-source toolbox for training deep neural networks under fairness constraints using stochastic approximation. It formalizes fairness constraints as bounds on differences in expected loss across protected and non-protected subgroups and reviews constrained stochastic optimization methods. Four algorithms are implemented—Stochastic Ghost (StGh), Stochastic Smoothed and Linearized Augmented Lagrangian (SSL-ALM), a plain Augmented Lagrangian Method (ALM), and Stochastic Switching Subgradient (SSw)—and compared against SGD and a differentiable fairness regularizer (Fairret). Experiments on the ACSIncome (Oklahoma) task evaluate optimization dynamics and fairness metrics (independence, separation, sufficiency), using subgroup-balanced sampling to estimate constraints and reporting means and variances across 10 runs. The study highlights trade-offs among methods: ALM/SSL-ALM tend to stabilize constraints; StGh often achieves stronger fairness at an accuracy cost; SSw exhibits sensitivity to hyperparameters.

Strengths
- Clear and focused contribution: a benchmark centered on constrained stochastic optimization for fairness, addressing a gap relative to toolkits and benchmarks that primarily emphasize differentiable penalties or post-processing.
- Transparent methodological exposition: concise review of algorithms and assumptions, with explicit stochastic formulations and pseudocode that make the implementations accessible to practitioners.
- Sensible and careful empirical protocol: multiple independent runs with dispersion reporting, as well as subgroup-balanced sampling for constraint estimation to improve reliability of constraint measurements.
- Informative comparative insights: the empirical analysis surfaces practical trade-offs between constraint satisfaction and predictive performance across methods, offering guidance for algorithm selection in fairness-constrained ERM.
- Overall presentation is organized and readable, with tables and figures supporting the narrative despite some readability limitations.

Weaknesses
- Conceptual mismatch between training constraints and evaluation metrics: the enforced constraint is a bound on expected loss differences between groups, while evaluation focuses on rate-based fairness criteria (independence, separation, sufficiency). This weakens causal interpretation of improvements in constraint satisfaction with respect to the reported fairness metrics.
- Missing specification of the constraint bound δ: the exact δ used in training and evaluation is not explicitly stated, appearing only implicitly in figures. This omission hinders reproducibility and makes results harder to interpret and compare.
- Limited empirical scope: experiments are restricted to a single dataset/state (ACSIncome Oklahoma), one network architecture, and a single form of fairness constraint. There are no ablations across datasets, tasks, protected attribute definitions, architectures, or alternative constraint formulations (e.g., rate-based constraints), limiting generality.
- Incomplete theoretical support for applied algorithms: the adaptation of SSL-ALM from linear to nonlinear stochastic inequality constraints via slack variables is heuristic and lacks convergence guarantees; StGh assumes differentiability of the constraint/objective that is not satisfied by ReLU networks; overall, guarantees are not provided for the nonconvex, nonsmooth, stochastic setting used.
- Sensitivity and limited hyperparameter exploration: SSw is reported as sensitive, and the hyperparameter search appears narrow, leaving uncertainty about whether stronger performance could be achieved with tuning. Related ablations (e.g., mini-batch sizes for objective vs. constraints, subgroup sampling strategies) are not reported.
- Reproducibility and presentation issues: figures are dense and sometimes hard to read; a minor inconsistency exists between the abstract’s count of implemented algorithms and the evaluation section; the repository URL appears potentially placeholder and reproducibility scripts/checklists are not clearly documented.
