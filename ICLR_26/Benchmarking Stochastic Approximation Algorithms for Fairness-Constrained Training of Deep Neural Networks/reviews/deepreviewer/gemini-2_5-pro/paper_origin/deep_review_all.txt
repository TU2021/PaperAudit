Here are four distinct reviews of the research paper.

***

### **Review 1**

**Summary**
This paper presents a benchmark for fairness-constrained training of deep neural networks. The authors frame fairness as a constrained empirical risk minimization (ERM) problem. They review existing stochastic approximation algorithms suitable for this setting, identify theoretical gaps, and then implement and evaluate three recently proposed algorithms (Stochastic Ghost, SSL-ALM, Stochastic Switching Subgradient) on a real-world task derived from the US Census data. The paper is accompanied by a Python package to facilitate further research and comparison.

**Soundness**
The methodological soundness has some significant issues.
1.  The core problem is a non-convex ERM with stochastic, non-convex, and potentially non-smooth inequality constraints (Section 3, eq. 5). The paper correctly identifies the lack of algorithms with convergence guarantees for this general setting.
2.  However, the choice of algorithms for the benchmark is questionable. For instance, SSL-ALM [32] is explicitly designed for *linear* inequality constraints (Section 3.3). The authors acknowledge this but use it anyway for the non-linear constraints derived from the fairness formulation (eq. 4) without justifying why this is a valid or meaningful adaptation. This mismatch between the algorithm's assumptions and the problem's structure undermines the credibility of the SSL-ALM and ALM results.
3.  The experimental results for the Stochastic Switching Subgradient (SSw) method show that it fails to satisfy the constraints with the chosen parameters (Figure 1, col. 4), and the authors admit they were "unable to identify parameter settings for SSw that simultaneously satisfy the constraints and minimize the objective function" (Section 4.2). While this is an honest report, it suggests either a flaw in the implementation, a very difficult tuning process that was not sufficiently explored, or that the algorithm is not practical for this problem class. The appendix (Section B) shows a different parameter setting where constraints are met but the objective is not minimized, reinforcing this issue.
4.  The Stochastic Ghost method requires computing an unbiased estimate of the search direction `d(x_k)` using a complex sampling scheme involving geometrically distributed batch sizes (Section 3.2, eq. 9). The high variance observed in its performance (Figure 1, col. 1) is likely a direct consequence of this estimator, and it raises questions about its practical utility without further variance reduction techniques.

**Presentation**
The paper is generally well-written and structured. The introduction and related work sections provide good context. The review of algorithms in Section 3.1 and Table 2 is a valuable overview. However, there are several areas for improvement:
-   The figures, particularly Figures 1 and 2, have y-axis labels ("Loss", "C(x)") that are cut off or illegible. The plots themselves are quite noisy, and while showing quartiles is good, the visual clutter makes it hard to discern clear trends, especially for StGh.
-   The description of the algorithms in Section 3 is very dense. For example, the explanation of the Stochastic Ghost method's direction computation (eq. 9) is difficult to parse without referring to the original paper.
-   The parameters for the algorithms are listed in a dense paragraph (Section 4.2), which makes them hard to find and parse. A table would be much clearer.

**Contribution**
The main contribution is the creation of a benchmark and an open-source toolbox for a challenging and important class of problems. While other fairness toolkits exist (AIF360, FairLearn), this work's focus on modern stochastic *constrained optimization* algorithms is a valuable and more specialized niche. The paper successfully highlights the practical difficulties of applying these advanced algorithms to real-world deep learning tasks. The literature survey and the identification of theoretical gaps (Table 2) are also useful contributions.

**Strengths**
-   **Problem Formulation:** The paper clearly formulates fairness in DNNs as a stochastic constrained optimization problem (eq. 1, 4), which is a principled and important approach.
-   **Comprehensive Review:** The literature review of stochastic constrained optimization methods (Section 3.1, Table 2) is thorough and provides a clear overview of the state-of-the-art and its limitations.
-   **Open-Source Benchmark:** The release of the code as a Python package is a significant strength, enabling reproducibility and fostering further research in this area.
-   **Honest Reporting:** The authors are transparent about the difficulties they faced, particularly with tuning the SSw algorithm (Section 4.2), which is commendable.

**Weaknesses**
-   **Technical Mismatch:** The application of an algorithm designed for linear constraints (SSL-ALM) to a problem with non-linear constraints is a major technical weakness that is not adequately justified.
-   **Experimental Rigor:** The failure to find a working parameter regime for SSw and the high variance of StGh suggest that the experimental evaluation may be premature. A more thorough hyperparameter search and analysis are needed to make definitive claims about these algorithms' performance.
-   **Limited Scope of Experiments:** The evaluation is conducted on a single dataset and a single small neural network architecture. The benchmark would be more compelling if it included a wider variety of datasets, model architectures, and protected attributes.

**Questions**
1.  Could you elaborate on the decision to use SSL-ALM, an algorithm for linear constraints, on your non-linear problem? Did you consider linearization of the constraints at each step, and if so, how does that relate to the theory of the original algorithm?
2.  Regarding the SSw method, the results suggest a fundamental difficulty in balancing objective minimization and constraint satisfaction. Do you believe this is an inherent issue with the algorithm's design (e.g., the switching condition) when applied to DNNs, or a matter of needing a more sophisticated hyperparameter tuning strategy (e.g., Bayesian optimization)?
3.  The Stochastic Ghost method is known to have high variance. Did you experiment with any variance reduction techniques beyond the baseline algorithm, or consider how the large, geometrically-sampled batches impact computational cost compared to the other methods in terms of total gradient computations, not just wall-clock time?

**Rating**
-   Overall (10): 5 — The paper addresses an important problem and provides a useful tool, but the experimental soundness is undermined by a technical mismatch in one key algorithm and inconclusive results for another (Section 3.3, Section 4.2).
-   Novelty (10): 7 — While fairness benchmarks exist, the specific focus on implementing and comparing modern, complex stochastic constrained optimization algorithms is novel and timely (Section 3).
-   Technical Quality (10): 4 — The application of SSL-ALM to non-linear constraints without justification and the failure to properly tune SSw represent significant technical shortcomings (Section 3.3, Figure 1).
-   Clarity (10): 7 — The paper is mostly well-written, but the algorithm descriptions are dense and some figures have presentation issues like illegible labels (Section 3.2, Figure 1).
-   Confidence (5): 5 — I am very confident in my assessment, based on my expertise in stochastic optimization and machine learning.

***

### **Review 2**

**Summary**
This paper introduces a benchmark for evaluating stochastic optimization algorithms on fairness-constrained deep learning problems. The authors formulate fairness goals, such as demographic parity, as constraints on the empirical risk difference between demographic groups. They review relevant optimization literature, implement three recent algorithms, and compare their performance on a binary classification task using the Folktables dataset. The work is supported by a publicly available Python package.

**Soundness**
The paper's methodology is generally sound from a fairness perspective. The use of the Folktables dataset [22] is a good choice, as it is a standard and realistic dataset for fairness research. The formulation of fairness constraints as bounds on the difference in group-wise loss (Table 1, eq. 4) is a valid and common "in-processing" technique. The evaluation using multiple standard fairness metrics (Independence, Separation, Sufficiency) in addition to inaccuracy (Table 3) provides a multi-faceted view of the models' performance.

However, there are some points that could be strengthened. The constraint is on the *difference in loss*, which is a proxy for fairness. This is not the same as directly constraining a fairness metric like the demographic parity gap itself. While related, the connection is not one-to-one, especially with complex loss functions and models. The authors could have discussed the implications of this choice more.

The experimental setup is reasonable, comparing the constrained methods against an unconstrained SGD baseline and a regularization-based baseline (SGD-Fairret). This allows for a clear demonstration of the trade-offs involved. The stratification of the train/test split with respect to the protected attribute is good practice.

**Presentation**
The paper is clearly written and easy to follow. The introduction provides excellent motivation for the problem. The background on fairness metrics in the preliminaries (Sections 2 and "Fairness metrics") is concise and accurate. The experimental results are presented with both plots (Figures 1-4) and a detailed table (Table 3), which is helpful for interpretation.

Some presentation elements could be improved.
-   Figure 3, showing the distribution of predictions, is a very effective visualization. However, the individual plots are too small, and the caption could be more descriptive.
-   The spider plots in Figure 4 are a good way to visualize the multi-objective trade-offs between accuracy and the three fairness metrics.
-   The paper mentions implementing four algorithms in the contributions ("implements four algorithms"), but the abstract says three, and the experiments section title is "assessing the performance of four algorithms", which includes a plain ALM alongside SSL-ALM. This minor inconsistency should be clarified.

**Contribution**
The primary contribution is a practical and much-needed benchmark focused on the *optimization challenges* of fairness-constrained learning. While toolkits like Fairlearn [8] provide access to various fairness algorithms, they often abstract away the underlying optimization. This paper dives into the optimization machinery, implementing and testing algorithms that are not yet standard tools. By providing an easy-to-use framework and open-sourcing the code, the authors make a valuable contribution to both the machine learning optimization and algorithmic fairness communities. It serves as a bridge between the theoretical optimization literature and the practical needs of fairness practitioners.

**Strengths**
-   **Practical Focus:** The paper tackles the real-world problem of implementing and benchmarking complex optimization algorithms for fairness, moving beyond purely theoretical analysis.
-   **Useful Tool:** The release of a Python package is a major strength, lowering the barrier for other researchers to test their own algorithms or replicate the results.
-   **Good Fairness Evaluation:** The use of multiple, standard fairness metrics (Ind, Sp, Sf) and clear baselines provides a nuanced evaluation of the different methods' impacts (Table 3, Figure 4).
-   **Clear Motivation:** The paper does an excellent job of motivating why constrained optimization is a desirable approach for fairness compared to regularization (Section 2).

**Weaknesses**
-   **Indirect Fairness Control:** The paper constrains the difference in loss, not a direct fairness metric. A discussion on the pros and cons of this choice versus directly constraining, for example, the statistical parity difference, would strengthen the paper.
-   **Limited Dataset Diversity:** The experiments are limited to one task on one dataset (ACSIncome) with one protected attribute (a binarized version of race). The benchmark's value would be greatly enhanced by including more diverse tasks and datasets (e.g., COMPAS, German Credit) to see if the relative performance of the algorithms holds.
-   **Lack of Post-processing Comparison:** The paper focuses on in-processing methods. While this is the stated scope, a brief comparison to a simple post-processing method (e.g., threshold tuning) could have provided additional context on the performance-fairness trade-off.

**Questions**
1.  You chose to constrain the difference in group-wise loss. Could you comment on the challenges or benefits of this approach compared to directly constraining a standard fairness metric, such as the statistical parity gap defined in your "Independence (Ind)" section?
2.  The results in Table 3 show that while the constrained methods improve Independence and Separation, they worsen Sufficiency compared to the SGD baseline. Is this an expected trade-off, and could you provide some intuition for why this occurs?
3.  Your benchmark currently uses the ACSIncome task. Do you have plans to extend the package to include other standard fairness datasets? How easily can a user of your package integrate a new dataset and fairness task?

**Rating**
-   Overall (10): 8 — This is a strong paper that makes a valuable practical contribution by benchmarking and open-sourcing complex optimization algorithms for fairness, despite a somewhat limited experimental scope.
-   Novelty (10): 8 — The creation of a benchmark specifically for modern stochastic constrained optimization algorithms in the fairness context is a novel and needed contribution (Abstract, Section 1).
-   Technical Quality (10): 7 — The experimental setup is solid and the fairness evaluation is well-executed, though the choice of algorithms could be better justified from an optimization theory standpoint (Section 4.2, Table 3).
-   Clarity (10): 9 — The paper is very well-written and organized, with clear explanations of fairness concepts and experimental results (Sections 2 & 4).
-   Confidence (5): 5 — I am highly confident in my review, as my research focuses on algorithmic fairness and its practical applications.

***

### **Review 3**

**Summary**
This paper presents a new benchmark and an associated Python toolbox for training deep neural networks with fairness constraints. The authors survey several recent stochastic optimization algorithms designed for constrained problems, implement a selection of them, and compare their performance on a real-world fairness task from the Folktables dataset. The goal is to bridge the gap between theoretical optimization research and the practical need for fair machine learning models.

**Soundness**
The paper's methodology seems sound and well-reasoned. The authors correctly identify the challenges of the problem: stochastic, non-convex, and non-smooth functions (Section 3). Their review of existing methods in Table 2 is very informative and justifies their selection of algorithms to test, as these are the ones that most closely match the problem's characteristics (stochastic objective and constraints, inequality constraints).

The experimental setup is well-designed. They use a relevant dataset (ACSIncome) and compare the specialized constrained optimization algorithms against sensible baselines: standard unconstrained training (SGD) and a regularization-based approach (SGD-Fairret). The use of 10 runs to report mean and variance (Section 4.2) adds robustness to the empirical claims. The analysis is thorough, covering both optimization performance (loss and constraint violation over time, Figures 1 & 2) and fairness performance (various metrics, Table 3).

**Presentation**
The presentation is excellent. The paper is logically structured, starting with a broad introduction, moving to technical preliminaries and algorithm descriptions, and finishing with a clear experimental evaluation. As someone interested in this field, I found the literature review in Section 3.1 and Table 2 to be particularly helpful in navigating a complex landscape of algorithms. The descriptions of the chosen algorithms (StGh, SSL-ALM, SSw) are concise but provide enough detail to grasp the main ideas.

The figures and tables are used effectively to support the text. Figures 1 and 2 give a good sense of the optimization dynamics, while Figure 3 (prediction distributions) and Figure 4 (spider plots) provide intuitive visualizations of the fairness outcomes. The code release is a fantastic addition that greatly enhances the paper's value.

**Contribution**
This paper makes a very strong contribution. There is a clear need for standardized tools and benchmarks in the fairness-constrained ML space. While many algorithms are proposed in theory, practitioners often don't know which ones work best or how to implement them. This paper addresses that gap directly by:
1.  Providing a clear survey of the algorithmic landscape (Table 2).
2.  Implementing three complex, recent algorithms that were previously not available in a standard toolkit (Abstract).
3.  Creating an easy-to-use benchmark on a real-world dataset.
4.  Open-sourcing the code to allow the community to build upon this work.

This is exactly the kind of work that helps move a field forward from theory to practice.

**Strengths**
-   **Timely and Relevant:** The paper addresses the important and timely problem of operationalizing fairness in machine learning through constrained optimization.
-   **Excellent Literature Review:** The survey of algorithms in Section 3.1 is clear, comprehensive, and very useful for understanding the state of the art.
-   **Practical Contribution:** The implementation and benchmarking of these algorithms is a significant piece of work. The public release of the code is a major plus.
-   **Clear Experiments:** The experimental section is well-structured, with appropriate baselines and metrics, leading to clear and interpretable results about the trade-offs between different methods (Section 4).

**Weaknesses**
-   The paper could benefit from a slightly more detailed discussion of the hyperparameter tuning process. The chosen parameters are listed (Section 4.2), but it's unclear how sensitive the algorithms are to these choices or how much effort was required to find them. This is especially relevant for SSw, which seemed hard to tune.
-   The computational cost of the different algorithms isn't explicitly compared. While plots are vs. wall-clock time, a discussion of complexity in terms of, for example, gradient evaluations per iteration would be informative, especially for the Stochastic Ghost method with its variable batch sizes.

**Questions**
1.  In your implementation, how did you handle the computation of gradients for the constraint function `c(x, ζ)`? Since the constraint is an average loss over a subgroup, did you use PyTorch's autograd on a computation graph that includes the subgroup filtering, or was a more manual approach needed?
2.  The Stochastic Ghost algorithm seems computationally intensive due to the large batch sizes sampled from the geometric distribution. Could you comment on the practical trade-offs in terms of wall-clock time vs. convergence quality you observed when comparing it to the other, simpler methods like SSL-ALM?
3.  Thank you for releasing the code! Are there any plans to add more algorithms to the benchmark in the future, for example, some of the other methods listed in Table 2?

**Rating**
-   Overall (10): 9 — An excellent paper that provides a valuable, well-executed, and much-needed benchmark and toolbox for the community.
-   Novelty (10): 8 — The paper is novel in its focus on benchmarking recent, complex stochastic constrained optimizers for fairness, and in providing a public implementation (Abstract).
-   Technical Quality (10): 9 — The methodology, implementation, and experiments are of high quality and support the paper's claims well (Section 4).
-   Clarity (10): 10 — The paper is exceptionally clear, well-organized, and easy to read from start to finish.
-   Confidence (5): 4 — I am confident in my assessment, though I am not a deep expert in the specific optimization theories of all the benchmarked algorithms.

***

### **Review 4**

**Summary**
The paper proposes a benchmark for comparing stochastic approximation algorithms for fairness-constrained training of deep neural networks. It reviews existing methods, implements three algorithms (Stochastic Ghost, SSL-ALM, Stochastic Switching Subgradient) plus a standard ALM, and evaluates them on a fairness task using the Folktables dataset. The authors claim this is the first such benchmark and release their code as a Python package.

**Soundness**
The paper's claims and methodology require more careful justification.
1.  The claim in the conclusion of providing the "first benchmark for assessing the performance of optimization methods on real-world instances of fairness constrained training" (Section 5) seems overstated. The paper itself cites [30], which presents "FFB: A Fair Fairness Benchmark for In-Processing Group Fairness Methods" and evaluates a wide range of methods. The authors must more clearly differentiate their contribution from existing benchmarks like FFB.
2.  There is an inconsistency in the number of algorithms. The abstract mentions implementing "three recently proposed, but as-of-yet unimplemented, algorithms". The contributions list "implements four algorithms". The experiments section evaluates four constrained methods (StGh, SSL-ALM, ALM, SSw). Plain ALM is a classic method, not a recent proposal. This should be clarified.
3.  The description of the SSw parameters is confusing: "ε_k = 10^{−4} if k < 500, ε_k = 0.97 ε_{k−1} for every k ≥ 500 at each epoch" (Section 4.2). What does "at each epoch" mean in the context of an iterative update `k`? This lacks precision.
4.  The experimental results for SSw are presented as a failure (Figure 1, col. 4), which calls into question its inclusion. If the algorithm cannot be made to work on this canonical problem, it is not a useful benchmark component. Simply stating that parameters could not be found is insufficient; the authors should analyze *why* it fails.

**Presentation**
The quality of the presentation is mixed and needs significant improvement.
-   **Figures are poor quality:** In Figures 1 and 2, the y-axis labels are cropped and unreadable. The plots are very dense with overlapping lines for mean, median, and quartiles, making them hard to interpret. Figure 3 consists of six tiny, unlabeled plots; the caption only lists the algorithms and does not explain what the plot shows (distribution of predictions). Figure 4 contains two spider plots, presumably for train and test sets, but they are not labeled (a) and (b), and the caption only refers to the test set plot in the text. The images in the appendix (Figure 5) are also poorly arranged and captioned.
-   **Inconsistent Terminology:** The paper uses `x` for the optimization variable in Section 3 but `θ` in Section 2. While common, sticking to one would improve consistency.
-   **Clarity of Algorithm Descriptions:** The descriptions in Section 3 are rushed. For SSL-ALM, the updates for `x` and `z` are given, but the variables themselves are not clearly defined for the problem at hand (e.g., what is `z` in the context of training a DNN?). The update for `y` seems to be a simple gradient ascent step on the Lagrangian, but this is not explicitly stated.

**Contribution**
The claimed contribution is a new benchmark and toolbox. While a useful endeavor, the novelty is questionable given existing work like [30]. The main value seems to be the focus on a specific class of stochastic constrained optimization algorithms, but the paper fails to demonstrate that these advanced algorithms are actually superior or even practical. Two of the main methods tested (StGh, SSw) show high variance or fail to converge properly (Figure 1), and the third (SSL-ALM) is theoretically mismatched with the problem. Therefore, the paper serves more as a cautionary tale than a successful demonstration of these methods. The literature review in Table 2 is useful, but this is a secondary contribution.

**Strengths**
-   The paper tackles an important problem at the intersection of optimization and fair ML.
-   The literature review in Table 2 provides a good, condensed summary of the theoretical assumptions of various algorithms.
-   The code is open-sourced, which allows for verification and extension of the work.

**Weaknesses**
-   **Overstated Novelty:** The claim of being the "first benchmark" is not well-supported and the paper does not adequately distinguish itself from prior work (e.g., [30]).
-   **Poor Presentation Quality:** The figures are low quality, poorly labeled, and difficult to interpret (Figures 1, 2, 3, 4).
-   **Inconclusive Experimental Results:** The benchmarked algorithms perform poorly (high variance for StGh, failure to satisfy constraints for SSw) or are theoretically mismatched (SSL-ALM), which undermines the paper's goal of demonstrating their utility.
-   **Lack of Precision:** Key details, such as hyperparameter schedules and algorithm variables, are described imprecisely (Section 4.2, Section 3.3).

**Questions**
1.  How does your benchmark contribution differ from or improve upon the "Fair Fairness Benchmark" (FFB) [30], which also evaluates in-processing fairness methods on benchmark datasets?
2.  Can you clarify the inconsistency regarding the number of "newly implemented" algorithms? Is the plain ALM considered one of the main contributions, and if so, why?
3.  Given the poor performance of SSw in your experiments, do you still consider it a valuable component of a benchmark for practitioners? What insights should a user take away from this result?
4.  Please clarify the role of the variable `z` in the SSL-ALM method (Section 3.3). What does it represent in the context of training a neural network, and how was it initialized?

**Rating**
-   Overall (10): 3 — The paper has a good goal but is undermined by overstated novelty, very poor presentation of results, and inconclusive experiments that fail to demonstrate the value of the benchmarked methods.
-   Novelty (10): 4 — The idea is not entirely new, and the paper fails to adequately differentiate its contribution from existing fairness benchmarks like [30].
-   Technical Quality (10): 3 — The experiments are inconclusive, one of the main algorithms is used outside its theoretical scope, and another fails to work, indicating significant technical issues (Section 4.2).
-   Clarity (10): 4 — The paper suffers from numerous presentation flaws, including unreadable figures, inconsistent statements, and imprecise descriptions (Figures 1-4, Section 4.2).
-   Confidence (5): 5 — I am very confident in my assessment of the paper's weaknesses in presentation and its relationship to the existing literature.