1) Summary
This paper introduces a benchmark for evaluating stochastic approximation algorithms on fairness-constrained deep neural network training tasks. The authors formulate fairness criteria, such as demographic parity, as constraints in an empirical risk minimization (ERM) problem. They provide a literature review of relevant constrained optimization algorithms, highlighting the theoretical challenges posed by non-convex, non-smooth, and stochastic objectives and constraints. The core contribution is a publicly released Python package that facilitates benchmarking on a large-scale dataset derived from US Census data (Folktables). The authors demonstrate the benchmark's utility by implementing and comparing three recent algorithms—Stochastic Ghost (StGh), Stochastic Smoothed and Linearized AL Method (SSL-ALM), and Stochastic Switching Subgradient (SSw)—evaluating their optimization performance and their effectiveness in improving fairness metrics.2) Strengths
*   **Addresses a Timely and Important Problem**
    *   The paper tackles the critical task of training machine learning models with explicit fairness constraints, which is of growing importance in both research and practice (Section 1, Section 2).
    *   The motivation is well-grounded in real-world needs, referencing legislative frameworks like the EU AI Act and applications in high-risk domains (Section 1, Section 2).
    *   By framing fairness as a constrained optimization problem (Equation 1, Table 1), the work connects the field of responsible AI with the established literature on stochastic optimization, providing a principled approach.*   **Significant Practical Contribution via a Public Benchmark and Toolbox**
    *   The primary contribution is a reusable, open-source benchmark and Python package for comparing constrained optimization algorithms on fairness tasks (Abstract, Section 1). This addresses a stated gap in the literature, where no standard toolkit exists for such comparisons (Section 1).
    *   The benchmark is built on a challenging, real-world dataset (ACSIncome from Folktables), which is known to exhibit biases, making the task realistic and relevant (Section 4.1, [30]).
    *   The toolbox is designed to be flexible, allowing users to define protected subgroups and select from various fairness constraints, which should facilitate future research in this area (Section 1).*   **Systematic Review and Principled Algorithm Selection**
    *   The paper provides a concise but comprehensive review of recent stochastic constrained optimization algorithms, systematically categorizing them based on the theoretical assumptions they make about the objective and constraint functions (Section 3.1, Table 2).
    *   This review effectively highlights the gap between existing theory and the practical challenges of fairness-constrained DNN training (non-convexity, non-smoothness, stochastic inequality constraints) (Section 3.1).
    *   The selection of StGh, SSL-ALM, and SSw for empirical evaluation is well-justified as these are among the few recent methods that can handle stochastic inequality constraints (Table 2).*   **Thorough and Multi-faceted Experimental Evaluation**
    *   The experiments compare the constrained optimization methods against relevant baselines: standard unconstrained SGD and a regularization-based fairness method (SGD-Fairret) (Section 4.2).
    *   The evaluation is comprehensive, analyzing both optimization performance (loss and constraint violation over time, Figures 1 and 2) and fairness outcomes using multiple standard metrics (Independence, Separation, Sufficiency in Table 3 and Figure 4).
    *   The analysis considers performance on both training and test sets, providing insights into generalization (Figures 1 vs. 2, Table 3).
    *   The results provide a nuanced conclusion, identifying ALM and SSL-ALM as offering the best trade-off between fairness improvement and accuracy degradation among the tested methods (Section 4.2, Table 3).3) Weaknesses
*   **Unclear Justification for Hyperparameter Selection**
    *   The paper lists the hyperparameters used for each algorithm but provides no details on how these values were selected or tuned (Section 4.2). The performance of these complex optimization algorithms is known to be highly sensitive to such choices.
    *   The poor performance of the SSw method is explicitly linked to the choice of parameters, with the authors noting they "were unable to identify parameter settings for SSw that simultaneously satisfy the constraints and minimize the objective function" (Section 4.2). This raises questions about whether the comparison is entirely fair, as SSw may have been disadvantaged by a suboptimal parameter configuration.
    *   The appendix shows that an alternative parameterization for SSw satisfies constraints but fails to minimize the objective (Appendix B, Figure 5), further highlighting the sensitivity and the lack of a systematic tuning process.*   **Mismatch Between Algorithm Assumptions and Problem Characteristics**
    *   The paper correctly identifies that fairness-constrained ERM involves non-convex and potentially non-smooth functions (Section 3). However, the chosen algorithms are applied without a full discussion of how their theoretical assumptions are met or adapted.
    *   Specifically, SSL-ALM was originally described for problems with *linear* constraints [32], but it is applied here to a problem with non-linear constraints derived from a neural network (Section 3.3). The paper acknowledges this ("Although problem (1) has non-linear inequality constraints, we use the SSL-ALM due to the lack of algorithms..."), but does not explain the necessary modifications (e.g., linearization) for this application.
    *   The review in Table 2 shows that most algorithms, including those tested, assume C^1 or C^2 continuity, which may not hold for DNNs with ReLU activations. While the paper mentions the need for algorithms that work on "tame" functions [19], the connection to the implemented methods is not made explicit.*   **Limited Scope of the Empirical Comparison**
    *   While the literature review in Table 2 is broad, the empirical evaluation is limited to three recently proposed algorithms, described as "as-of-yet unimplemented" (Abstract).
    *   The benchmark would be more comprehensive if it included more established or simpler baseline algorithms for constrained optimization, such as a basic stochastic primal-dual method or a penalty method, to better contextualize the performance of the newer, more complex methods.
    *   The introduction and contributions list mention a toolbox that "implements four algorithms" (Section 1), but the methods section details three (StGh, SSL-ALM, SSw) and the experiments evaluate these three plus a variant (ALM). This slight inconsistency could be clarified.*   **Clarity of Results Presentation**
    *   The plots showing optimization performance (Figures 1 and 2) are difficult to interpret. The y-axis scales for the loss plots (top row) differ across columns, hindering direct visual comparison of the final loss values achieved by different algorithms.
    *   The trajectories for the StGh algorithm are extremely noisy, making it hard to discern the underlying convergence trend (Figures 1 and 2, column 1).
    *   In the main text, the caption for Table 3 appears between the text referencing Figure 3 and the text referencing Figure 4, which disrupts the flow and creates confusion (Section 4.2, page 8).4) Suggestions for Improvement
*   **Provide Details on Hyperparameter Tuning**
    *   To improve reproducibility and strengthen the fairness of the comparison, please add a paragraph or an appendix section detailing the hyperparameter selection process. This should include the range of values explored for key parameters and the methodology used for selection (e.g., grid search on a validation set).
    *   For the SSw method, it would be more convincing to either conduct a more thorough search for effective parameters or to provide a more detailed analysis of why the method struggles on this specific problem structure.
    *   Showing results for more than two parameter settings for SSw would help demonstrate the trade-offs involved more clearly than the binary choice presented in the main text and Appendix B.*   **Clarify Adaptations for Mismatched Assumptions**
    *   Please add a more detailed discussion on the practical implementation of the algorithms, particularly where the problem characteristics do not align with the methods' original theoretical assumptions.
    *   For SSL-ALM, explicitly describe how the non-linear constraints are handled. For example, clarify if a linearization of the constraint function `c(x, ζ)` is used at each iteration, as the name "Linearized AL Method" might suggest.
    *   Briefly comment on the non-smoothness issue (e.g., due to ReLUs) and how the subgradient-based SSw method is suited for it, while also acknowledging that the other C^1-requiring methods are applied using standard auto-differentiation, which is a common practice but technically a heuristic in this context.*   **Broaden the Set of Evaluated Algorithms**
    *   To make the benchmark's initial results more impactful, consider including one or two additional, more standard constrained optimization baselines from the literature (e.g., a simple stochastic penalty method or a Lagrangian method like [42]). This would provide a more complete picture of the state of the art.
    *   Please clarify the "four algorithms" statement in the introduction by explicitly listing them and explaining that ALM is presented as a variant of SSL-ALM.*   **Enhance the Presentation of Figures**
    *   Please revise Figures 1 and 2 to use a consistent y-axis scale for all loss plots (top row) to allow for direct visual comparison.
    *   To improve the readability of the StGh plots, consider adding a smoothed trend line (e.g., using a moving average) over the noisy raw data to better visualize the convergence behavior.
    *   Please correct the ordering of captions and text in Section 4.2 so that the text referencing each figure and table is followed directly by the corresponding element. A summary table reporting the mean and standard deviation of the final loss and constraint violation for each method could be a valuable addition to complement the time-series plots.5) Score
*   Overall (10): 7 — The paper makes a valuable contribution by providing a much-needed benchmark and toolbox, supported by a solid literature review and a thorough experimental demonstration (Section 1, Section 4).
*   Novelty (10): 7 — The primary novelty lies in the creation of a specific, open-source benchmark for fairness-constrained optimization and the comparative evaluation of recent algorithms on it (Abstract, Section 1).
*   Technical Quality (10): 6 — The experimental setup is strong, but the lack of a clear hyperparameter tuning strategy and the unaddressed mismatch between algorithm theory and practical application (e.g., SSL-ALM for non-linear constraints) are notable weaknesses (Section 3.3, Section 4.2).
*   Clarity (10): 7 — The paper is generally well-written, but the presentation of results could be improved, particularly in the figures and the ordering of captions (Figures 1 & 2, Section 4.2).
*   Confidence (5): 5 — I am highly confident in my assessment, as my expertise aligns well with the topics of optimization, fairness, and benchmarking in machine learning.