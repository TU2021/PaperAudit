Summary
- The paper studies constrained empirical risk minimization for training deep neural networks under group fairness constraints formulated as expectations (Eq. (1), Table 1). It identifies practical and theoretical challenges (nonconvexity, inequality constraints, stochasticity; Section 3) and reviews relevant stochastic approximation algorithms (Table 2). The authors implement three recently proposed methods—Stochastic Ghost (Section 3.2; Eq. (8)–(9), Alg. 1), Stochastic Smoothed and Linearized Augmented Lagrangian Method (SSL-ALM; Section 3.3; Eq. (10)–(11), Alg. 2), and Stochastic Switching Subgradient (SSw; Section 3.4; Alg. 3)—and compare them on a fairness-constrained training task built on the ACSIncome dataset from Folktables (Section 4.1). Experiments contrast constrained methods with ERM baselines (SGD and SGD with a differentiable fairness regularizer; Section 4.2) and report optimization traces (Figures 1–2), distributional shifts (Figure 3), and multiple fairness metrics alongside inaccuracy and Wasserstein distances (Table 3). The code is announced as a Python package (Abstract; Section 1).Strengths
- Bold, well-scoped problem formulation
  - The constrained ERM setup with expectation constraints is clearly defined (Eq. (1); Section 1) and instantiated for fairness using loss-difference bounds (Eq. (4); Table 1), which makes the study’s setting precise and reproducible.
  - The paper explicitly discusses challenges—stochasticity, inequality constraints, nonconvexity/nonsmoothness (Section 3)—helping situate the algorithms’ design choices in a realistic DNN context (technical soundness).
  - The mapping from fairness notions to constraints is illustrated concretely for demographic parity, equal opportunity, and equalized odds (Table 1), aiding clarity and potential impact for practitioners.- Useful synthesis of algorithms and assumptions
  - Table 2 succinctly contrasts assumptions made by recent methods (e.g., stochastic objectives/constraints, smoothness classes such as C1/C2/C3, linear vs nonlinear constraints), which is a valuable reference for the community (clarity/impact).
  - The review in Section 3.1 explains why few methods accommodate stochastic inequality constraints and nonsmoothness, motivating the selection of StGh, SSL-ALM, and SSw for implementation (novelty/technical positioning).
  - The paper highlights open theoretical gaps (tame locally Lipschitz objectives/constraints; Section 3.1) linking to [19], clarifying where guarantees are lacking (technical soundness).- Nontrivial implementations of recent, under-implemented methods
  - The stochastic adaptation of the Ghost method is implemented with unbiased direction estimation via multilevel sampling (Eq. (8)–(9); Algorithm 1; Section 3.2), bridging theory to practice (technical contribution).
  - SSL-ALM is adapted to inequality constraints via slack variables and smoothing (Section 3.3; Eq. (10)–(11); Algorithm 2), despite the method’s original scope being stochastic linear constraints (novelty/engineering value).
  - SSw is implemented with a slight generalization allowing different step sizes for objective and constraint updates (Section 3.4; Algorithm 3), which may broaden applicability (practical contribution).- Clear experimental protocol with multiple baselines and metrics
  - The dataset choice and preprocessing are transparent (ACSIncome, Oklahoma; 17,917 samples; protected attribute RAC1P binarized; stratified split; StandardScaler; Section 4.1), aiding reproducibility (clarity).
  - Baselines include both plain ERM (SGD) and ERM with a fairness regularizer (SGD-Fairret; Section 4.2), enabling comparisons between constrained and penalization approaches (experimental rigor).
  - Fairness metrics from three families (Ind, Sp, Sf) and inaccuracy are reported on both train and test with means and standard deviations over 10 runs (Table 3; Figures 3–4), which underscores the fairness–accuracy trade-off (experimental rigor/clarity).- Informative analysis of optimization behavior
  - Time-evolution plots for loss and constraint values (Figures 1–2) show variability and feasibility trends, e.g., ALM/SSL-ALM achieving more stable feasibility than StGh/SSw on train (Section 4.2), supporting claims with visual evidence (experimental rigor).
  - The ablation for SSw with equal step sizes illustrates a trade-off between feasibility and objective progress (Appendix B; Figure 5; Section 4.2), improving transparency about tuning (clarity).
  - The paper candidly reports that SSw was hard to tune for jointly low loss and feasibility (Section 4.2), which is valuable for practitioners (practical impact).- Distributional diagnostics beyond scalar metrics
  - Figure 3 compares groupwise prediction distributions, complemented by Wasserstein distances (Table 3), providing granular insights into distribution shifts under different training regimes (clarity/impact).
  - The qualitative shift for constrained models towards closer inter-group distributions (Figure 3; Section 4.2) triangulates with Ind/Sp improvements (Table 3), supporting the fairness intent (experimental rigor).
  - The observation that ALM/SSL-ALM resemble SGD’s distribution more than StGh/SSw (Figure 3; Section 4.2) helps interpret trade-offs between preserving predictive signal and fairness constraints (clarity).- Publicly announced code and benchmark intent
  - The paper releases a Python package (Abstract; Section 1) and describes automated construction of ERM formulations using PyTorch/TensorFlow computation graphs with subgroup definitions (Section 1), which can catalyze adoption (impact).
  - The benchmark leverages Folktables (Section 4.1) and claims support for many subgroup definitions (Section 1), aligning with fairness auditing needs in practice (impact).
  - Including pseudocode (Algorithms 1–3; Appendix A) improves the transparency of the implemented methods (clarity/reproducibility).- Responsible framing and limitations
  - The Conclusion and Limitations acknowledge the lack of general guarantees and caution against viewing fair ML tooling as a “silver bullet” (Conclusion; Limitations), which reflects good scholarly practice (ethical clarity).
  - The discussion of generalization behavior of constraints on unseen data is appropriately scoped (Section 4.2 referencing [12]), avoiding over-claims (technical humility).
  - The paper positions the work as a seed for future algorithmic advances and benchmarking, rather than as a final verdict (Conclusion), supporting constructive community engagement (impact).Weaknesses
- Limited empirical breadth of the “benchmark” demonstration
  - Single dataset and task: The experimental evaluation focuses on ACSIncome for one state (Oklahoma) with one binary classification label (> $50k; Section 4.1), which underutilizes the stated goal of a “challenging benchmark” on real-world fairness tasks (impact/external validity).
  - Single protected-attribute configuration: Only RAC1P binarized into “white” vs “non-white” is used (Section 4.1), despite the claim of supporting “up to 5.7 billion protected subgroups” (Section 1), limiting demonstration of subgroup scalability (impact).
  - Single constraint family used in practice: Although Table 1 lists demographic parity, equal opportunity, and equalized odds formulations, experiments enforce a loss-difference bound (Eq. (4); Section 4.2) and do not instantiate the other fairness constraints (completeness).- Missing or ambiguous experimental details hinder reproducibility
  - Constraint bound δ not specified: While Eq. (4) defines a two-sided bound with δ and Figures 1–2 show a dashed “constraint bound,” δ’s numerical value is not reported in Section 4.2 or captions (reproducibility/technical clarity).
  - Mini-batch sizes and sampling policies are incomplete: SSw requires a mini-batch size J (Algorithm 3; Appendix A) but Section 4.2 does not state J; for ALM/SSL-ALM sampling is described (single samples; Section 3.3) but the exact settings used in experiments are not reiterated in the experimental section (reproducibility).
  - Stopping/iteration budget is not standardized: Plots are reported versus time (seconds) (Figures 1–2) without iteration/epoch counts or per-iteration cost, complicating algorithmic comparison independent of hardware (methodology).- Model specification appears inconsistent and underspecified
  - Parameter count mismatch: Section 4.2 states “2 hidden layers of sizes 64 and 32 … with a total of 194 parameters,” which is inconsistent with typical parameter counts for a 9–64–32–1 MLP (orders of magnitude larger), risking confusion about the true capacity (technical clarity/reproducibility).
  - Architectural details are minimal: No information on biases, regularization (dropout/weight decay), or normalization beyond StandardScaler is provided (Section 4.1–4.2), making the stated parameter count and behavior hard to interpret (reproducibility).
  - Output layer and thresholding are not fully specified: While BCEWithLogitsLoss is used (Eq. (12); Section 4.2), the decision threshold for computing Ind/Sp/Sf (which depend on binary predictions) is not described (Section 2; Table 3), which affects reported metrics (technical clarity).- Misalignment between enforced constraints and reported fairness metrics
  - Constraint is loss-difference, metrics are Ind/Sp/Sf: The enforced constraint targets the difference in empirical loss across groups (Eq. (4)), while reported fairness metrics reflect independence, separation, and sufficiency (Table 3; Section 2), which are not necessarily optimized by the constraint (evaluation alignment).
  - Sufficiency degrades for constrained models: The constrained models have higher Sf values than SGD/SGD-Fairret (Table 3; both train and test), but this is neither quantified against δ nor explicitly analyzed beyond a brief remark (Section 4.2), leaving ambiguity about trade-offs (analysis depth).
  - No aggregate constraint satisfaction summaries: Constraint satisfaction is visualized over time (Figures 1–2), but there is no tabulated final/average violation or feasibility rate across runs, hindering a clear comparison to fairness metrics (experimental rigor).- Evaluation methodology overly reliant on wall-clock time and lacks scalability analysis
  - Time-based x-axis confounds comparisons: Figures 1–2 plot versus seconds on a specific laptop (Section 4.2), making results sensitive to implementation details and hardware rather than algorithmic complexity (methodology).
  - No per-iteration complexity reporting: Methods have different sampling regimes (e.g., StGh’s geometric mini-batches; Section 3.2; Algorithm 1), but per-iteration cost or effective sample usage is not quantified (scalability/technical rigor).
  - Limited sensitivity analyses: Aside from one SSw ablation (Appendix B), there is no systematic sensitivity to key hyperparameters (e.g., p0, τ, β in StGh; μ, ρ in SSL-ALM; tolerance/step schedules in SSw) (experimental completeness).- Benchmark/tooling claims not fully substantiated by experiments
  - Massive subgroup support not demonstrated: The claim of “up to 5.7 billion protected subgroups” (Section 1) is not exercised in experiments (Section 4), leaving the scalability claim unvalidated (impact).
  - Sparse benchmark description: Beyond stating automated ERM construction and dataset use (Section 1), the paper does not detail the benchmark’s APIs, configuration files, or evaluation protocol for easy extension (No direct evidence found in the manuscript), limiting immediate usability (reproducibility/impact).
  - Limited algorithmic coverage vs. survey breadth: While Table 2 reviews many methods, the toolbox evaluates four (StGh, SSL-ALM, ALM, SSw; Section 4.2), and does not demonstrate modular comparisons across key design choices (e.g., sampling strategies, globalization, restarts) highlighted in Section 1 (completeness).Suggestions for Improvement
- Broaden the empirical scope of the benchmark demonstration
  - Evaluate across multiple ACS states/tasks from Folktables (Section 4.1) to show robustness across distributions and to substantiate the “challenging benchmark” claim.
  - Include additional protected-attribute configurations (e.g., multi-category race without binarization, gender, age bands) to exercise the claimed subgroup scalability (Section 1).
  - Instantiate constraints aligned with Table 1 beyond loss-difference, such as demographic parity or equal opportunity constraints, and compare their effects alongside Eq. (4) within the same benchmark protocol (Sections 1–2; Table 1).- Provide missing experimental details to enhance reproducibility
  - Report the exact numeric value of δ used in Eq. (4) and in Figures 1–2 (constraint bound), and describe how it was selected (Section 4.2; Figures 1–2).
  - State the mini-batch sizes and sampling policies for every algorithm in Section 4.2, including J for SSw (Algorithm 3; Appendix A), and reiterate ALM/SSL-ALM’s per-iteration sampling choices (Section 3.3) as actually used.
  - Standardize and report iteration/epoch counts and, in addition to time, provide convergence plots versus iterations to make comparisons implementation- and hardware-agnostic (Figures 1–2).- Clarify and tighten the model specification
  - Recompute and report the exact parameter count for the stated 2-layer MLP (Section 4.2) including biases; if 194 is correct, specify the architecture that yields it (e.g., smaller hidden sizes) to resolve the apparent mismatch.
  - Document architectural choices (biases, weight decay, dropout, batchnorm), optimizer settings, and any additional regularization to enable replication (Sections 4.1–4.2).
  - Specify the decision threshold used to compute Ind/Sp/Sf (Section 2; Table 3), and report if threshold tuning (e.g., 0.5 vs. calibrated) was applied on validation data.- Align evaluation metrics with enforced constraints and report feasibility clearly
  - Complement Ind/Sp/Sf with a directly aligned metric—e.g., final/average group loss difference—to match Eq. (4) and tabulate it per method on train and test (Sections 4.2; Figures 1–2).
  - Analyze and discuss the observed degradation in sufficiency (Sf) for constrained models (Table 3), quantifying trade-offs against δ and inaccuracy to inform users of practical implications.
  - Add a feasibility table with mean ± std of final constraint violation across 10 runs for each method (train/test), enabling straightforward comparisons beyond time-series plots (Figures 1–2; Table 3).- Strengthen methodological evaluation and scalability analysis
  - Report per-iteration computational cost and effective sample usage (e.g., expected batch size in StGh via p0; Section 3.2) to contextualize time-based plots (Figures 1–2; Section 4.2).
  - Include convergence plots versus iterations and ablations on key hyperparameters (p0, τ, β, μ, ρ, step schedules) to assess algorithm sensitivity (Sections 3.2–3.4; Appendix B).
  - Provide memory/runtime scaling with dataset size and number of groups to substantiate claims of practicality for large-scale fairness-constrained training (Sections 1, 4.1).- Substantiate the benchmark/tooling contribution
  - Demonstrate subgroup scalability by running at least one experiment varying the number/structure of subgroups, validating “up to 5.7 billion” feasibility in practice (Section 1).
  - Add a dedicated section detailing the benchmark API, configuration files, extensibility points (e.g., how to plug in a new optimizer), and provide example scripts (No direct evidence found in the manuscript).
  - Expand the comparative study to include modular design choices highlighted in Section 1 (sampling strategies, globalization, restarts), even for one method, to showcase the toolbox’s breadth beyond the four fixed algorithms (Section 1; Table 2).Score
- Overall (10): 6 — Useful implementations and comparative study of three recent stochastic constrained methods with clear results on ACSIncome, but limited empirical breadth and some missing details (Sections 4.1–4.2; Figures 1–3; Table 3; Table 2).
- Novelty (10): 6 — Implementing and adapting StGh, SSL-ALM, and SSw for fairness-constrained DNN training is practically novel, though the benchmark demonstration is narrow and constraints align with prior formulations (Sections 3.2–3.4; Eq. (4); Table 1).
- Technical Quality (10): 5 — Algorithms are accurately described and implemented with pseudocode (Algorithms 1–3), but experimental reporting lacks some key details (e.g., δ, batch sizes, iteration counts) and alignment between enforced constraints and evaluation metrics could be improved (Sections 3–4; Figures 1–2; Table 3).
- Clarity (10): 7 — The manuscript is generally well-organized with clear formulations (Eq. (1), (4)), a helpful assumptions table (Table 2), and informative figures/tables, albeit with a few ambiguities (parameter count, δ value) (Sections 3–4).
- Confidence (5): 4 — Assessment is based on the provided manuscript with multiple concrete anchors (Sections 3–4; Figures 1–3; Table 3), but some reproduction-critical details are missing (δ, batch sizes), so confidence is high but not absolute.