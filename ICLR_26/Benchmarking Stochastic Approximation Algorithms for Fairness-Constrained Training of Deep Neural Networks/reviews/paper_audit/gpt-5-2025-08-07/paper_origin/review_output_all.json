{
  "baseline_review": "Summary\n- The paper studies constrained empirical risk minimization for training deep neural networks under group fairness constraints formulated as expectations (Eq. (1), Table 1). It identifies practical and theoretical challenges (nonconvexity, inequality constraints, stochasticity; Section 3) and reviews relevant stochastic approximation algorithms (Table 2). The authors implement three recently proposed methods‚ÄîStochastic Ghost (Section 3.2; Eq. (8)‚Äì(9), Alg. 1), Stochastic Smoothed and Linearized Augmented Lagrangian Method (SSL-ALM; Section 3.3; Eq. (10)‚Äì(11), Alg. 2), and Stochastic Switching Subgradient (SSw; Section 3.4; Alg. 3)‚Äîand compare them on a fairness-constrained training task built on the ACSIncome dataset from Folktables (Section 4.1). Experiments contrast constrained methods with ERM baselines (SGD and SGD with a differentiable fairness regularizer; Section 4.2) and report optimization traces (Figures 1‚Äì2), distributional shifts (Figure 3), and multiple fairness metrics alongside inaccuracy and Wasserstein distances (Table 3). The code is announced as a Python package (Abstract; Section 1).Strengths\n- Bold, well-scoped problem formulation\n  - The constrained ERM setup with expectation constraints is clearly defined (Eq. (1); Section 1) and instantiated for fairness using loss-difference bounds (Eq. (4); Table 1), which makes the study‚Äôs setting precise and reproducible.\n  - The paper explicitly discusses challenges‚Äîstochasticity, inequality constraints, nonconvexity/nonsmoothness (Section 3)‚Äîhelping situate the algorithms‚Äô design choices in a realistic DNN context (technical soundness).\n  - The mapping from fairness notions to constraints is illustrated concretely for demographic parity, equal opportunity, and equalized odds (Table 1), aiding clarity and potential impact for practitioners.- Useful synthesis of algorithms and assumptions\n  - Table 2 succinctly contrasts assumptions made by recent methods (e.g., stochastic objectives/constraints, smoothness classes such as C1/C2/C3, linear vs nonlinear constraints), which is a valuable reference for the community (clarity/impact).\n  - The review in Section 3.1 explains why few methods accommodate stochastic inequality constraints and nonsmoothness, motivating the selection of StGh, SSL-ALM, and SSw for implementation (novelty/technical positioning).\n  - The paper highlights open theoretical gaps (tame locally Lipschitz objectives/constraints; Section 3.1) linking to [19], clarifying where guarantees are lacking (technical soundness).- Nontrivial implementations of recent, under-implemented methods\n  - The stochastic adaptation of the Ghost method is implemented with unbiased direction estimation via multilevel sampling (Eq. (8)‚Äì(9); Algorithm 1; Section 3.2), bridging theory to practice (technical contribution).\n  - SSL-ALM is adapted to inequality constraints via slack variables and smoothing (Section 3.3; Eq. (10)‚Äì(11); Algorithm 2), despite the method‚Äôs original scope being stochastic linear constraints (novelty/engineering value).\n  - SSw is implemented with a slight generalization allowing different step sizes for objective and constraint updates (Section 3.4; Algorithm 3), which may broaden applicability (practical contribution).- Clear experimental protocol with multiple baselines and metrics\n  - The dataset choice and preprocessing are transparent (ACSIncome, Oklahoma; 17,917 samples; protected attribute RAC1P binarized; stratified split; StandardScaler; Section 4.1), aiding reproducibility (clarity).\n  - Baselines include both plain ERM (SGD) and ERM with a fairness regularizer (SGD-Fairret; Section 4.2), enabling comparisons between constrained and penalization approaches (experimental rigor).\n  - Fairness metrics from three families (Ind, Sp, Sf) and inaccuracy are reported on both train and test with means and standard deviations over 10 runs (Table 3; Figures 3‚Äì4), which underscores the fairness‚Äìaccuracy trade-off (experimental rigor/clarity).- Informative analysis of optimization behavior\n  - Time-evolution plots for loss and constraint values (Figures 1‚Äì2) show variability and feasibility trends, e.g., ALM/SSL-ALM achieving more stable feasibility than StGh/SSw on train (Section 4.2), supporting claims with visual evidence (experimental rigor).\n  - The ablation for SSw with equal step sizes illustrates a trade-off between feasibility and objective progress (Appendix B; Figure 5; Section 4.2), improving transparency about tuning (clarity).\n  - The paper candidly reports that SSw was hard to tune for jointly low loss and feasibility (Section 4.2), which is valuable for practitioners (practical impact).- Distributional diagnostics beyond scalar metrics\n  - Figure 3 compares groupwise prediction distributions, complemented by Wasserstein distances (Table 3), providing granular insights into distribution shifts under different training regimes (clarity/impact).\n  - The qualitative shift for constrained models towards closer inter-group distributions (Figure 3; Section 4.2) triangulates with Ind/Sp improvements (Table 3), supporting the fairness intent (experimental rigor).\n  - The observation that ALM/SSL-ALM resemble SGD‚Äôs distribution more than StGh/SSw (Figure 3; Section 4.2) helps interpret trade-offs between preserving predictive signal and fairness constraints (clarity).- Publicly announced code and benchmark intent\n  - The paper releases a Python package (Abstract; Section 1) and describes automated construction of ERM formulations using PyTorch/TensorFlow computation graphs with subgroup definitions (Section 1), which can catalyze adoption (impact).\n  - The benchmark leverages Folktables (Section 4.1) and claims support for many subgroup definitions (Section 1), aligning with fairness auditing needs in practice (impact).\n  - Including pseudocode (Algorithms 1‚Äì3; Appendix A) improves the transparency of the implemented methods (clarity/reproducibility).- Responsible framing and limitations\n  - The Conclusion and Limitations acknowledge the lack of general guarantees and caution against viewing fair ML tooling as a ‚Äúsilver bullet‚Äù (Conclusion; Limitations), which reflects good scholarly practice (ethical clarity).\n  - The discussion of generalization behavior of constraints on unseen data is appropriately scoped (Section 4.2 referencing [12]), avoiding over-claims (technical humility).\n  - The paper positions the work as a seed for future algorithmic advances and benchmarking, rather than as a final verdict (Conclusion), supporting constructive community engagement (impact).Weaknesses\n- Limited empirical breadth of the ‚Äúbenchmark‚Äù demonstration\n  - Single dataset and task: The experimental evaluation focuses on ACSIncome for one state (Oklahoma) with one binary classification label (> $50k; Section 4.1), which underutilizes the stated goal of a ‚Äúchallenging benchmark‚Äù on real-world fairness tasks (impact/external validity).\n  - Single protected-attribute configuration: Only RAC1P binarized into ‚Äúwhite‚Äù vs ‚Äúnon-white‚Äù is used (Section 4.1), despite the claim of supporting ‚Äúup to 5.7 billion protected subgroups‚Äù (Section 1), limiting demonstration of subgroup scalability (impact).\n  - Single constraint family used in practice: Although Table 1 lists demographic parity, equal opportunity, and equalized odds formulations, experiments enforce a loss-difference bound (Eq. (4); Section 4.2) and do not instantiate the other fairness constraints (completeness).- Missing or ambiguous experimental details hinder reproducibility\n  - Constraint bound Œ¥ not specified: While Eq. (4) defines a two-sided bound with Œ¥ and Figures 1‚Äì2 show a dashed ‚Äúconstraint bound,‚Äù Œ¥‚Äôs numerical value is not reported in Section 4.2 or captions (reproducibility/technical clarity).\n  - Mini-batch sizes and sampling policies are incomplete: SSw requires a mini-batch size J (Algorithm 3; Appendix A) but Section 4.2 does not state J; for ALM/SSL-ALM sampling is described (single samples; Section 3.3) but the exact settings used in experiments are not reiterated in the experimental section (reproducibility).\n  - Stopping/iteration budget is not standardized: Plots are reported versus time (seconds) (Figures 1‚Äì2) without iteration/epoch counts or per-iteration cost, complicating algorithmic comparison independent of hardware (methodology).- Model specification appears inconsistent and underspecified\n  - Parameter count mismatch: Section 4.2 states ‚Äú2 hidden layers of sizes 64 and 32 ‚Ä¶ with a total of 194 parameters,‚Äù which is inconsistent with typical parameter counts for a 9‚Äì64‚Äì32‚Äì1 MLP (orders of magnitude larger), risking confusion about the true capacity (technical clarity/reproducibility).\n  - Architectural details are minimal: No information on biases, regularization (dropout/weight decay), or normalization beyond StandardScaler is provided (Section 4.1‚Äì4.2), making the stated parameter count and behavior hard to interpret (reproducibility).\n  - Output layer and thresholding are not fully specified: While BCEWithLogitsLoss is used (Eq. (12); Section 4.2), the decision threshold for computing Ind/Sp/Sf (which depend on binary predictions) is not described (Section 2; Table 3), which affects reported metrics (technical clarity).- Misalignment between enforced constraints and reported fairness metrics\n  - Constraint is loss-difference, metrics are Ind/Sp/Sf: The enforced constraint targets the difference in empirical loss across groups (Eq. (4)), while reported fairness metrics reflect independence, separation, and sufficiency (Table 3; Section 2), which are not necessarily optimized by the constraint (evaluation alignment).\n  - Sufficiency degrades for constrained models: The constrained models have higher Sf values than SGD/SGD-Fairret (Table 3; both train and test), but this is neither quantified against Œ¥ nor explicitly analyzed beyond a brief remark (Section 4.2), leaving ambiguity about trade-offs (analysis depth).\n  - No aggregate constraint satisfaction summaries: Constraint satisfaction is visualized over time (Figures 1‚Äì2), but there is no tabulated final/average violation or feasibility rate across runs, hindering a clear comparison to fairness metrics (experimental rigor).- Evaluation methodology overly reliant on wall-clock time and lacks scalability analysis\n  - Time-based x-axis confounds comparisons: Figures 1‚Äì2 plot versus seconds on a specific laptop (Section 4.2), making results sensitive to implementation details and hardware rather than algorithmic complexity (methodology).\n  - No per-iteration complexity reporting: Methods have different sampling regimes (e.g., StGh‚Äôs geometric mini-batches; Section 3.2; Algorithm 1), but per-iteration cost or effective sample usage is not quantified (scalability/technical rigor).\n  - Limited sensitivity analyses: Aside from one SSw ablation (Appendix B), there is no systematic sensitivity to key hyperparameters (e.g., p0, œÑ, Œ≤ in StGh; Œº, œÅ in SSL-ALM; tolerance/step schedules in SSw) (experimental completeness).- Benchmark/tooling claims not fully substantiated by experiments\n  - Massive subgroup support not demonstrated: The claim of ‚Äúup to 5.7 billion protected subgroups‚Äù (Section 1) is not exercised in experiments (Section 4), leaving the scalability claim unvalidated (impact).\n  - Sparse benchmark description: Beyond stating automated ERM construction and dataset use (Section 1), the paper does not detail the benchmark‚Äôs APIs, configuration files, or evaluation protocol for easy extension (No direct evidence found in the manuscript), limiting immediate usability (reproducibility/impact).\n  - Limited algorithmic coverage vs. survey breadth: While Table 2 reviews many methods, the toolbox evaluates four (StGh, SSL-ALM, ALM, SSw; Section 4.2), and does not demonstrate modular comparisons across key design choices (e.g., sampling strategies, globalization, restarts) highlighted in Section 1 (completeness).Suggestions for Improvement\n- Broaden the empirical scope of the benchmark demonstration\n  - Evaluate across multiple ACS states/tasks from Folktables (Section 4.1) to show robustness across distributions and to substantiate the ‚Äúchallenging benchmark‚Äù claim.\n  - Include additional protected-attribute configurations (e.g., multi-category race without binarization, gender, age bands) to exercise the claimed subgroup scalability (Section 1).\n  - Instantiate constraints aligned with Table 1 beyond loss-difference, such as demographic parity or equal opportunity constraints, and compare their effects alongside Eq. (4) within the same benchmark protocol (Sections 1‚Äì2; Table 1).- Provide missing experimental details to enhance reproducibility\n  - Report the exact numeric value of Œ¥ used in Eq. (4) and in Figures 1‚Äì2 (constraint bound), and describe how it was selected (Section 4.2; Figures 1‚Äì2).\n  - State the mini-batch sizes and sampling policies for every algorithm in Section 4.2, including J for SSw (Algorithm 3; Appendix A), and reiterate ALM/SSL-ALM‚Äôs per-iteration sampling choices (Section 3.3) as actually used.\n  - Standardize and report iteration/epoch counts and, in addition to time, provide convergence plots versus iterations to make comparisons implementation- and hardware-agnostic (Figures 1‚Äì2).- Clarify and tighten the model specification\n  - Recompute and report the exact parameter count for the stated 2-layer MLP (Section 4.2) including biases; if 194 is correct, specify the architecture that yields it (e.g., smaller hidden sizes) to resolve the apparent mismatch.\n  - Document architectural choices (biases, weight decay, dropout, batchnorm), optimizer settings, and any additional regularization to enable replication (Sections 4.1‚Äì4.2).\n  - Specify the decision threshold used to compute Ind/Sp/Sf (Section 2; Table 3), and report if threshold tuning (e.g., 0.5 vs. calibrated) was applied on validation data.- Align evaluation metrics with enforced constraints and report feasibility clearly\n  - Complement Ind/Sp/Sf with a directly aligned metric‚Äîe.g., final/average group loss difference‚Äîto match Eq. (4) and tabulate it per method on train and test (Sections 4.2; Figures 1‚Äì2).\n  - Analyze and discuss the observed degradation in sufficiency (Sf) for constrained models (Table 3), quantifying trade-offs against Œ¥ and inaccuracy to inform users of practical implications.\n  - Add a feasibility table with mean ¬± std of final constraint violation across 10 runs for each method (train/test), enabling straightforward comparisons beyond time-series plots (Figures 1‚Äì2; Table 3).- Strengthen methodological evaluation and scalability analysis\n  - Report per-iteration computational cost and effective sample usage (e.g., expected batch size in StGh via p0; Section 3.2) to contextualize time-based plots (Figures 1‚Äì2; Section 4.2).\n  - Include convergence plots versus iterations and ablations on key hyperparameters (p0, œÑ, Œ≤, Œº, œÅ, step schedules) to assess algorithm sensitivity (Sections 3.2‚Äì3.4; Appendix B).\n  - Provide memory/runtime scaling with dataset size and number of groups to substantiate claims of practicality for large-scale fairness-constrained training (Sections 1, 4.1).- Substantiate the benchmark/tooling contribution\n  - Demonstrate subgroup scalability by running at least one experiment varying the number/structure of subgroups, validating ‚Äúup to 5.7 billion‚Äù feasibility in practice (Section 1).\n  - Add a dedicated section detailing the benchmark API, configuration files, extensibility points (e.g., how to plug in a new optimizer), and provide example scripts (No direct evidence found in the manuscript).\n  - Expand the comparative study to include modular design choices highlighted in Section 1 (sampling strategies, globalization, restarts), even for one method, to showcase the toolbox‚Äôs breadth beyond the four fixed algorithms (Section 1; Table 2).Score\n- Overall (10): 6 ‚Äî Useful implementations and comparative study of three recent stochastic constrained methods with clear results on ACSIncome, but limited empirical breadth and some missing details (Sections 4.1‚Äì4.2; Figures 1‚Äì3; Table 3; Table 2).\n- Novelty (10): 6 ‚Äî Implementing and adapting StGh, SSL-ALM, and SSw for fairness-constrained DNN training is practically novel, though the benchmark demonstration is narrow and constraints align with prior formulations (Sections 3.2‚Äì3.4; Eq. (4); Table 1).\n- Technical Quality (10): 5 ‚Äî Algorithms are accurately described and implemented with pseudocode (Algorithms 1‚Äì3), but experimental reporting lacks some key details (e.g., Œ¥, batch sizes, iteration counts) and alignment between enforced constraints and evaluation metrics could be improved (Sections 3‚Äì4; Figures 1‚Äì2; Table 3).\n- Clarity (10): 7 ‚Äî The manuscript is generally well-organized with clear formulations (Eq. (1), (4)), a helpful assumptions table (Table 2), and informative figures/tables, albeit with a few ambiguities (parameter count, Œ¥ value) (Sections 3‚Äì4).\n- Confidence (5): 4 ‚Äî Assessment is based on the provided manuscript with multiple concrete anchors (Sections 3‚Äì4; Figures 1‚Äì3; Table 3), but some reproduction-critical details are missing (Œ¥, batch sizes), so confidence is high but not absolute.",
  "final_review": "Summary\n- The paper studies constrained empirical risk minimization for training deep neural networks under group fairness constraints formulated as expectations (Eq. (1), Table 1). It identifies practical and theoretical challenges (nonconvexity, inequality constraints, stochasticity; Section 3) and reviews relevant stochastic approximation algorithms (Table 2). The authors implement three recently proposed methods‚ÄîStochastic Ghost (Section 3.2; Eq. (8)‚Äì(9), Alg. 1), Stochastic Smoothed and Linearized Augmented Lagrangian Method (SSL-ALM; Section 3.3; Eq. (10)‚Äì(11), Alg. 2), and Stochastic Switching Subgradient (SSw; Section 3.4; Alg. 3)‚Äîand compare them on a fairness-constrained training task built on the ACSIncome dataset from Folktables (Section 4.1). Experiments contrast constrained methods with ERM baselines (SGD and SGD with a differentiable fairness regularizer; Section 4.2) and report optimization traces (Figures 1‚Äì2), distributional shifts (Figure 3), and multiple fairness metrics alongside inaccuracy and Wasserstein distances (Table 3). The code is announced as a Python package (Abstract; Section 1).Strengths\n- Bold, well-scoped problem formulation\n  - The constrained ERM setup with expectation constraints is clearly defined (Eq. (1); Section 1) and instantiated for fairness using loss-difference bounds (Eq. (4); Table 1), which makes the study‚Äôs setting precise and reproducible.\n  - The paper explicitly discusses challenges‚Äîstochasticity, inequality constraints, nonconvexity/nonsmoothness (Section 3)‚Äîhelping situate the algorithms‚Äô design choices in a realistic DNN context (technical soundness).\n  - The mapping from fairness notions to constraints is illustrated concretely for demographic parity, equal opportunity, and equalized odds (Table 1), aiding clarity and potential impact for practitioners.\n- Useful synthesis of algorithms and assumptions\n  - Table 2 succinctly contrasts assumptions made by recent methods (e.g., stochastic objectives/constraints, smoothness classes such as C1/C2/C3, linear vs nonlinear constraints), which is a valuable reference for the community (clarity/impact).\n  - The review in Section 3.1 explains why few methods accommodate stochastic inequality constraints and nonsmoothness, motivating the selection of StGh, SSL-ALM, and SSw for implementation (novelty/technical positioning).\n  - The paper highlights open theoretical gaps (tame locally Lipschitz objectives/constraints; Section 3.1) linking to [19], clarifying where guarantees are lacking (technical soundness).\n- Nontrivial implementations of recent, under-implemented methods\n  - The stochastic adaptation of the Ghost method is implemented with unbiased direction estimation via multilevel sampling (Eq. (8)‚Äì(9); Algorithm 1; Section 3.2), bridging theory to practice (technical contribution).\n  - SSL-ALM is adapted from its original scope to the nonlinear inequality-constrained setting using a proximal augmented Lagrangian with smoothing (Section 3.3; Eq. (10)‚Äì(11); Algorithm 2), reflecting noteworthy engineering effort (novelty/engineering value).\n  - SSw is implemented with a slight generalization allowing different step sizes for objective and constraint updates (Section 3.4; Algorithm 3), which may broaden applicability (practical contribution).\n- Clear experimental protocol with multiple baselines and metrics\n  - The dataset choice and preprocessing are transparent (ACSIncome, Oklahoma; 17,917 samples; protected attribute RAC1P binarized; stratified split; StandardScaler; Section 4.1), aiding reproducibility (clarity).\n  - Baselines include both plain ERM (SGD) and ERM with a fairness regularizer (SGD-Fairret; Section 4.2), enabling comparisons between constrained and penalization approaches (experimental rigor).\n  - Fairness metrics from three families (Ind, Sp, Sf) and inaccuracy are reported on both train and test with means and standard deviations over 10 runs (Table 3; Figures 3‚Äì4), which underscores the fairness‚Äìaccuracy trade-off (experimental rigor/clarity).\n- Informative analysis of optimization behavior\n  - Time-evolution plots for loss and constraint values (Figures 1‚Äì2) show variability and feasibility trends, e.g., ALM/SSL-ALM achieving more stable feasibility than StGh/SSw on train (Section 4.2), supporting claims with visual evidence (experimental rigor).\n  - The ablation for SSw with equal step sizes illustrates a trade-off between feasibility and objective progress (Appendix B; Figure 5; Section 4.2), improving transparency about tuning (clarity).\n  - The paper candidly reports that SSw was hard to tune for jointly low loss and feasibility (Section 4.2), which is valuable for practitioners (practical impact).\n- Distributional diagnostics beyond scalar metrics\n  - Figure 3 compares groupwise prediction distributions, complemented by Wasserstein distances (Table 3), providing granular insights into distribution shifts under different training regimes (clarity/impact).\n  - The qualitative shift for constrained models towards closer inter-group distributions (Figure 3; Section 4.2) triangulates with Ind/Sp improvements (Table 3), supporting the fairness intent (experimental rigor).\n  - The observation that ALM/SSL-ALM resemble SGD‚Äôs distribution more than StGh/SSw (Figure 3; Section 4.2) helps interpret trade-offs between preserving predictive signal and fairness constraints (clarity).\n- Publicly announced code and benchmark intent\n  - The paper releases a Python package (Abstract; Section 1) and describes automated construction of ERM formulations using PyTorch/TensorFlow computation graphs with subgroup definitions (Section 1), which can catalyze adoption (impact).\n  - The benchmark leverages Folktables (Section 4.1) and claims support for many subgroup definitions (Section 1), aligning with fairness auditing needs in practice (impact).\n  - Including pseudocode (Algorithms 1‚Äì3; Appendix A) improves the transparency of the implemented methods (clarity/reproducibility).\n- Responsible framing and limitations\n  - The Conclusion and Limitations acknowledge the lack of general guarantees and caution against viewing fair ML tooling as a ‚Äúsilver bullet‚Äù (Conclusion; Limitations), which reflects good scholarly practice (ethical clarity).\n  - The discussion of generalization behavior of constraints on unseen data is appropriately scoped (Section 4.2 referencing [12]), avoiding over-claims (technical humility).\n  - The paper positions the work as a seed for future algorithmic advances and benchmarking, rather than as a final verdict (Conclusion), supporting constructive community engagement (impact).Weaknesses\n- Limited empirical breadth of the ‚Äúbenchmark‚Äù demonstration\n  - Single dataset and task: The experimental evaluation focuses on ACSIncome for one state (Oklahoma) with one binary classification label (> $50k; Section 4.1), which underutilizes the stated goal of a ‚Äúchallenging benchmark‚Äù on real-world fairness tasks (impact/external validity).\n  - Single protected-attribute configuration: Only RAC1P binarized into ‚Äúwhite‚Äù vs ‚Äúnon-white‚Äù is used (Section 4.1), despite the claim of supporting ‚Äúup to 5.7 billion protected subgroups‚Äù (Section 1), limiting demonstration of subgroup scalability (impact).\n  - Single constraint family used in practice: Although Table 1 lists demographic parity, equal opportunity, and equalized odds formulations, experiments enforce a loss-difference bound (Eq. (4); Section 4.2) and do not instantiate the other fairness constraints (completeness).\n- Missing or ambiguous experimental details hinder reproducibility\n  - Constraint bound Œ¥ not specified: While Eq. (4) defines a two-sided bound with Œ¥ and Figures 1‚Äì2 show a dashed ‚Äúconstraint bound,‚Äù Œ¥‚Äôs numerical value is not reported in Section 4.2 or captions (reproducibility/technical clarity).\n  - Mini-batch sizes and sampling policies are incomplete: SSw requires a mini-batch size J (Algorithm 3; Appendix A) but Section 4.2 does not state J; for ALM/SSL-ALM sampling is described (single samples; Section 3.3) but the exact settings used in experiments are not reiterated in the experimental section (reproducibility).\n  - Stopping/iteration budget is not standardized: Plots are reported versus time (seconds) (Figures 1‚Äì2) without iteration/epoch counts or per-iteration cost, complicating algorithmic comparison independent of hardware (methodology).\n  - Undefined parameter: Section 4.2 lists a StGh parameter Œª = 0.5, but Œª is not defined in Section 3.2 or Algorithm 1 (reproducibility/clarity).\n  - Inconsistent batch-size/exponent notation in StGh: Eq. (9) uses 2N+1 whereas the surrounding text and Algorithm 1 use 2^{N+1} for the mini-batch size (Section 3.2; Eq. (9); Algorithm 1), creating ambiguity in the unbiased direction construction (technical clarity).\n- Model specification appears inconsistent and underspecified\n  - Parameter count mismatch: Section 4.2 states ‚Äú2 hidden layers of sizes 64 and 32 ‚Ä¶ with a total of 194 parameters,‚Äù which is inconsistent with typical parameter counts for a 9‚Äì64‚Äì32‚Äì1 MLP (orders of magnitude larger), risking confusion about the true capacity (technical clarity/reproducibility).\n  - Architectural details are minimal: No information on biases, regularization (dropout/weight decay), or normalization beyond StandardScaler is provided (Section 4.1‚Äì4.2), making the stated parameter count and behavior hard to interpret (reproducibility).\n  - Output layer and thresholding are not fully specified: While BCEWithLogitsLoss is used (Eq. (12); Section 4.2), the decision threshold for computing Ind/Sp/Sf (which depend on binary predictions) is not described (Section 2; Table 3), which affects reported metrics (technical clarity).\n- Misalignment between enforced constraints and reported fairness metrics\n  - Constraint is loss-difference, metrics are Ind/Sp/Sf: The enforced constraint targets the difference in empirical loss across groups (Eq. (4)), while reported fairness metrics reflect independence, separation, and sufficiency (Table 3; Section 2), which are not necessarily optimized by the constraint (evaluation alignment).\n  - Sufficiency degrades for constrained models: The constrained models have higher Sf values than SGD/SGD-Fairret (Table 3; both train and test), but this is neither quantified against Œ¥ nor explicitly analyzed beyond a brief remark (Section 4.2); additionally, the statement ‚ÄúSGD-Fairret slightly improves sufficiency relative to the SGD model ‚Ä¶ Similar observations hold for metrics on the test set‚Äù conflicts with Table 3, where test Sf is higher for SGD-Fairret than SGD (Table 3), creating confusion (analysis depth/consistency).\n  - No aggregate constraint satisfaction summaries: Constraint satisfaction is visualized over time (Figures 1‚Äì2), but there is no tabulated final/average violation or feasibility rate across runs, hindering a clear comparison to fairness metrics (experimental rigor).\n- Evaluation methodology overly reliant on wall-clock time and lacks scalability analysis\n  - Time-based x-axis confounds comparisons: Figures 1‚Äì2 plot versus seconds on a specific laptop (Section 4.2), making results sensitive to implementation details and hardware rather than algorithmic complexity (methodology).\n  - No per-iteration complexity reporting: Methods have different sampling regimes (e.g., StGh‚Äôs geometric mini-batches; Section 3.2; Algorithm 1), but per-iteration cost or effective sample usage is not quantified (scalability/technical rigor).\n  - Limited sensitivity analyses: Aside from one SSw ablation (Appendix B), there is no systematic sensitivity to key hyperparameters (e.g., p0, œÑ, Œ≤ in StGh; Œº, œÅ in SSL-ALM; tolerance/step schedules in SSw) (experimental completeness).\n- Benchmark/tooling claims not fully substantiated by experiments\n  - Massive subgroup support not demonstrated: The claim of ‚Äúup to 5.7 billion protected subgroups‚Äù (Section 1) is not exercised in experiments (Section 4), leaving the scalability claim unvalidated (impact).\n  - Sparse benchmark description: Beyond stating automated ERM construction and dataset use (Section 1), the paper does not detail the benchmark‚Äôs APIs, configuration files, or evaluation protocol for easy extension (No direct evidence found in the manuscript), limiting immediate usability (reproducibility/impact).\n  - Limited algorithmic coverage vs. survey breadth: While Table 2 reviews many methods, the toolbox evaluates four (StGh, SSL-ALM, ALM, SSw; Section 4.2), and does not demonstrate modular comparisons across key design choices (e.g., sampling strategies, globalization, restarts) highlighted in Section 1 (completeness).\n- Ambiguity in SSL-ALM inequality handling and multiplier/projection details\n  - Slack variables are mentioned but absent from equations and updates: Section 3.3 states inequalities are handled ‚Äúwith slack variables‚Äù and minimizes over ùìß = ‚Ñù^n √ó ‚Ñù_{‚â•0}^m, yet LœÅ, KœÅ,Œº, G, and updates (Eq. (10)‚Äì(11)) contain no slack variable s and no equality form with s (technical soundness; Section 3.3; Algorithm 2).\n  - Multiplier and projection handling are unclear: Eq. (11) applies proj_ùìß to x, although ùìß was defined to include a nonnegative component; Algorithm 2 truncates y to zero when ‚à•y‚à• ‚â• M_y (Algorithm 2 lines 3‚Äì6) without projection to y ‚â• 0, leaving the treatment of inequality multipliers ambiguous (clarity/technical soundness).\n  - Resulting ambiguity: It is unclear which precise inequality-constrained problem SSL-ALM solves relative to (1)/(5), affecting interpretability of the comparisons (Section 3.3; impact).Suggestions for Improvement\n- Broaden the empirical scope of the benchmark demonstration\n  - Evaluate across multiple ACS states/tasks from Folktables (Section 4.1) to show robustness across distributions and to substantiate the ‚Äúchallenging benchmark‚Äù claim.\n  - Include additional protected-attribute configurations (e.g., multi-category race without binarization, gender, age bands) to exercise the claimed subgroup scalability (Section 1).\n  - Instantiate constraints aligned with Table 1 beyond loss-difference, such as demographic parity or equal opportunity constraints, and compare their effects alongside Eq. (4) within the same benchmark protocol (Sections 1‚Äì2; Table 1).\n- Provide missing experimental details to enhance reproducibility\n  - Report the exact numeric value of Œ¥ used in Eq. (4) and in Figures 1‚Äì2 (constraint bound), and describe how it was selected (Section 4.2; Figures 1‚Äì2).\n  - State the mini-batch sizes and sampling policies for every algorithm in Section 4.2, including J for SSw (Algorithm 3; Appendix A), and reiterate ALM/SSL-ALM‚Äôs per-iteration sampling choices (Section 3.3) as actually used.\n  - Standardize and report iteration/epoch counts and, in addition to time, provide convergence plots versus iterations to make comparisons implementation- and hardware-agnostic (Figures 1‚Äì2).\n  - Define any additional method parameters used in experiments (e.g., Œª in StGh in Section 4.2) or remove them if unused to avoid confusion (Sections 3.2‚Äì4.2; Algorithm 1).\n  - Correct Eq. (9) to use 2^{N+1} (or clarify the intended construction) consistently with Algorithm 1 and the surrounding text to eliminate ambiguity in the unbiased estimator (Section 3.2; Eq. (9); Algorithm 1).\n- Clarify and tighten the model specification\n  - Recompute and report the exact parameter count for the stated 2-layer MLP (Section 4.2) including biases; if 194 is correct, specify the architecture that yields it (e.g., smaller hidden sizes) to resolve the apparent mismatch.\n  - Document architectural choices (biases, weight decay, dropout, batchnorm), optimizer settings, and any additional regularization to enable replication (Sections 4.1‚Äì4.2).\n  - Specify the decision threshold used to compute Ind/Sp/Sf (Section 2; Table 3), and report if threshold tuning (e.g., 0.5 vs. calibrated) was applied on validation data.\n- Align evaluation metrics with enforced constraints and report feasibility clearly\n  - Complement Ind/Sp/Sf with a directly aligned metric‚Äîe.g., final/average group loss difference‚Äîto match Eq. (4) and tabulate it per method on train and test (Sections 4.2; Figures 1‚Äì2).\n  - Analyze and discuss the observed degradation in sufficiency (Sf) for constrained models (Table 3), quantifying trade-offs against Œ¥ and inaccuracy to inform users of practical implications; also reconcile the narrative with Table 3 where test Sf for SGD-Fairret is higher than for SGD (Table 3).\n  - Add a feasibility table with mean ¬± std of final constraint violation across 10 runs for each method (train/test), enabling straightforward comparisons beyond time-series plots (Figures 1‚Äì2; Table 3).\n- Strengthen methodological evaluation and scalability analysis\n  - Report per-iteration computational cost and effective sample usage (e.g., expected batch size in StGh via p0; Section 3.2) to contextualize time-based plots (Figures 1‚Äì2; Section 4.2).\n  - Include convergence plots versus iterations and ablations on key hyperparameters (p0, œÑ, Œ≤, Œº, œÅ, step schedules) to assess algorithm sensitivity (Sections 3.2‚Äì3.4; Appendix B).\n  - Provide memory/runtime scaling with dataset size and number of groups to substantiate claims of practicality for large-scale fairness-constrained training (Sections 1, 4.1).\n- Substantiate the benchmark/tooling contribution\n  - Demonstrate subgroup scalability by running at least one experiment varying the number/structure of subgroups, validating ‚Äúup to 5.7 billion‚Äù feasibility in practice (Section 1).\n  - Add a dedicated section detailing the benchmark API, configuration files, extensibility points (e.g., how to plug in a new optimizer), and provide example scripts (No direct evidence found in the manuscript).\n  - Expand the comparative study to include modular design choices highlighted in Section 1 (sampling strategies, globalization, restarts), even for one method, to showcase the toolbox‚Äôs breadth beyond the four fixed algorithms (Section 1; Table 2).\n- Clarify SSL-ALM formulation and multiplier/projection handling\n  - If slack variables are used to convert inequalities to equalities, include them explicitly in LœÅ/KœÅ,Œº, the gradient estimator G, and the updates; otherwise, remove the slack-variable claim (Section 3.3; Eq. (10)‚Äì(11); Algorithm 2).\n  - If inequalities are enforced directly, project multipliers onto y ‚â• 0 and define the projection sets unambiguously (e.g., proj on x vs. (x, s)), and clarify the rationale for resetting y to zero at ‚à•y‚à• ‚â• M_y (Algorithm 2; Eq. (11)).\n  - State precisely which constrained problem SSL-ALM optimizes in experiments and how it aligns with (1)/(5), so readers can interpret and reproduce the results (Section 3.3; Section 4.2).Score\n- Overall (10): 6 ‚Äî Useful implementations and comparative study of three recent stochastic constrained methods with clear results on ACSIncome, but limited empirical breadth and some missing/ambiguous details (Sections 4.1‚Äì4.2; Figures 1‚Äì3; Table 3; Table 2).\n- Novelty (10): 6 ‚Äî Implementing and adapting StGh, SSL-ALM, and SSw for fairness-constrained DNN training is practically novel, though the benchmark demonstration is narrow and constraints align with prior formulations (Sections 3.2‚Äì3.4; Eq. (4); Table 1).\n- Technical Quality (10): 4 ‚Äî Method and experiment reporting contain notable gaps and inconsistencies (unspecified Œ¥; SSL-ALM inequality handling and multiplier/projection ambiguity in Section 3.3/Eqs. (10)‚Äì(11)/Alg. 2; parameter-count mismatch in Section 4.2; notation/parameter issues in StGh Eq. (9)/Alg. 1 and Œª in Section 4.2), which hinder reproducibility and interpretation.\n- Clarity (10): 7 ‚Äî The manuscript is generally well-organized with clear formulations (Eq. (1), (4)), a helpful assumptions table (Table 2), and informative figures/tables, albeit with ambiguities such as Œ¥, parameter counts, and SSL-ALM detail (Sections 3‚Äì4; Figures 1‚Äì2; Table 3).\n- Confidence (5): 4 ‚Äî Assessment is based on the provided manuscript with multiple concrete anchors (Sections 3‚Äì4; Figures 1‚Äì3; Table 3), but some reproduction-critical details are missing (e.g., Œ¥, certain hyperparameters), so confidence is high but not absolute.",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 6,
        "novelty": 6,
        "technical_quality": 5,
        "clarity": 7,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 6,
        "novelty": 6,
        "technical_quality": 4,
        "clarity": 7,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "Summary\n- The paper studies constrained empirical risk minimization for training deep neural networks under group fairness constraints formulated as expectations (Eq. (1), Table 1). It identifies practical and theoretical challenges (nonconvexity, inequality constraints, stochasticity; Section 3) and reviews relevant stochastic approximation algorithms (Table 2). The authors implement three recently proposed methods‚ÄîStochastic Ghost (Section 3.2; Eq. (8)‚Äì(9), Alg. 1), Stochastic Smoothed and Linearized Augmented Lagrangian Method (SSL-ALM; Section 3.3; Eq. (10)‚Äì(11), Alg. 2), and Stochastic Switching Subgradient (SSw; Section 3.4; Alg. 3)‚Äîand compare them on a fairness-constrained training task built on the ACSIncome dataset from Folktables (Section 4.1). Experiments contrast constrained methods with ERM baselines (SGD and SGD with a differentiable fairness regularizer; Section 4.2) and report optimization traces (Figures 1‚Äì2), distributional shifts (Figure 3), and multiple fairness metrics alongside inaccuracy and Wasserstein distances (Table 3). The code is announced as a Python package (Abstract; Section 1).Strengths\n- Bold, well-scoped problem formulation\n  - The constrained ERM setup with expectation constraints is clearly defined (Eq. (1); Section 1) and instantiated for fairness using loss-difference bounds (Eq. (4); Table 1), which makes the study‚Äôs setting precise and reproducible.\n  - The paper explicitly discusses challenges‚Äîstochasticity, inequality constraints, nonconvexity/nonsmoothness (Section 3)‚Äîhelping situate the algorithms‚Äô design choices in a realistic DNN context (technical soundness).\n  - The mapping from fairness notions to constraints is illustrated concretely for demographic parity, equal opportunity, and equalized odds (Table 1), aiding clarity and potential impact for practitioners.\n- Useful synthesis of algorithms and assumptions\n  - Table 2 succinctly contrasts assumptions made by recent methods (e.g., stochastic objectives/constraints, smoothness classes such as C1/C2/C3, linear vs nonlinear constraints), which is a valuable reference for the community (clarity/impact).\n  - The review in Section 3.1 explains why few methods accommodate stochastic inequality constraints and nonsmoothness, motivating the selection of StGh, SSL-ALM, and SSw for implementation (novelty/technical positioning).\n  - The paper highlights open theoretical gaps (tame locally Lipschitz objectives/constraints; Section 3.1) linking to [19], clarifying where guarantees are lacking (technical soundness).\n- Nontrivial implementations of recent, under-implemented methods\n  - The stochastic adaptation of the Ghost method is implemented with unbiased direction estimation via multilevel sampling (Eq. (8)‚Äì(9); Algorithm 1; Section 3.2), bridging theory to practice (technical contribution).\n  - SSL-ALM is adapted from its original scope to the nonlinear inequality-constrained setting using a proximal augmented Lagrangian with smoothing (Section 3.3; Eq. (10)‚Äì(11); Algorithm 2), reflecting noteworthy engineering effort (novelty/engineering value).\n  - SSw is implemented with a slight generalization allowing different step sizes for objective and constraint updates (Section 3.4; Algorithm 3), which may broaden applicability (practical contribution).\n- Clear experimental protocol with multiple baselines and metrics\n  - The dataset choice and preprocessing are transparent (ACSIncome, Oklahoma; 17,917 samples; protected attribute RAC1P binarized; stratified split; StandardScaler; Section 4.1), aiding reproducibility (clarity).\n  - Baselines include both plain ERM (SGD) and ERM with a fairness regularizer (SGD-Fairret; Section 4.2), enabling comparisons between constrained and penalization approaches (experimental rigor).\n  - Fairness metrics from three families (Ind, Sp, Sf) and inaccuracy are reported on both train and test with means and standard deviations over 10 runs (Table 3; Figures 3‚Äì4), which underscores the fairness‚Äìaccuracy trade-off (experimental rigor/clarity).\n- Informative analysis of optimization behavior\n  - Time-evolution plots for loss and constraint values (Figures 1‚Äì2) show variability and feasibility trends, e.g., ALM/SSL-ALM achieving more stable feasibility than StGh/SSw on train (Section 4.2), supporting claims with visual evidence (experimental rigor).\n  - The ablation for SSw with equal step sizes illustrates a trade-off between feasibility and objective progress (Appendix B; Figure 5; Section 4.2), improving transparency about tuning (clarity).\n  - The paper candidly reports that SSw was hard to tune for jointly low loss and feasibility (Section 4.2), which is valuable for practitioners (practical impact).\n- Distributional diagnostics beyond scalar metrics\n  - Figure 3 compares groupwise prediction distributions, complemented by Wasserstein distances (Table 3), providing granular insights into distribution shifts under different training regimes (clarity/impact).\n  - The qualitative shift for constrained models towards closer inter-group distributions (Figure 3; Section 4.2) triangulates with Ind/Sp improvements (Table 3), supporting the fairness intent (experimental rigor).\n  - The observation that ALM/SSL-ALM resemble SGD‚Äôs distribution more than StGh/SSw (Figure 3; Section 4.2) helps interpret trade-offs between preserving predictive signal and fairness constraints (clarity).\n- Publicly announced code and benchmark intent\n  - The paper releases a Python package (Abstract; Section 1) and describes automated construction of ERM formulations using PyTorch/TensorFlow computation graphs with subgroup definitions (Section 1), which can catalyze adoption (impact).\n  - The benchmark leverages Folktables (Section 4.1) and claims support for many subgroup definitions (Section 1), aligning with fairness auditing needs in practice (impact).\n  - Including pseudocode (Algorithms 1‚Äì3; Appendix A) improves the transparency of the implemented methods (clarity/reproducibility).\n- Responsible framing and limitations\n  - The Conclusion and Limitations acknowledge the lack of general guarantees and caution against viewing fair ML tooling as a ‚Äúsilver bullet‚Äù (Conclusion; Limitations), which reflects good scholarly practice (ethical clarity).\n  - The discussion of generalization behavior of constraints on unseen data is appropriately scoped (Section 4.2 referencing [12]), avoiding over-claims (technical humility).\n  - The paper positions the work as a seed for future algorithmic advances and benchmarking, rather than as a final verdict (Conclusion), supporting constructive community engagement (impact).Weaknesses\n- Limited empirical breadth of the ‚Äúbenchmark‚Äù demonstration\n  - Single dataset and task: The experimental evaluation focuses on ACSIncome for one state (Oklahoma) with one binary classification label (> $50k; Section 4.1), which underutilizes the stated goal of a ‚Äúchallenging benchmark‚Äù on real-world fairness tasks (impact/external validity).\n  - Single protected-attribute configuration: Only RAC1P binarized into ‚Äúwhite‚Äù vs ‚Äúnon-white‚Äù is used (Section 4.1), despite the claim of supporting ‚Äúup to 5.7 billion protected subgroups‚Äù (Section 1), limiting demonstration of subgroup scalability (impact).\n  - Single constraint family used in practice: Although Table 1 lists demographic parity, equal opportunity, and equalized odds formulations, experiments enforce a loss-difference bound (Eq. (4); Section 4.2) and do not instantiate the other fairness constraints (completeness).\n- Missing or ambiguous experimental details hinder reproducibility\n  - Constraint bound Œ¥ not specified: While Eq. (4) defines a two-sided bound with Œ¥ and Figures 1‚Äì2 show a dashed ‚Äúconstraint bound,‚Äù Œ¥‚Äôs numerical value is not reported in Section 4.2 or captions (reproducibility/technical clarity).\n  - Mini-batch sizes and sampling policies are incomplete: SSw requires a mini-batch size J (Algorithm 3; Appendix A) but Section 4.2 does not state J; for ALM/SSL-ALM sampling is described (single samples; Section 3.3) but the exact settings used in experiments are not reiterated in the experimental section (reproducibility).\n  - Stopping/iteration budget is not standardized: Plots are reported versus time (seconds) (Figures 1‚Äì2) without iteration/epoch counts or per-iteration cost, complicating algorithmic comparison independent of hardware (methodology).\n  - Undefined parameter: Section 4.2 lists a StGh parameter Œª = 0.5, but Œª is not defined in Section 3.2 or Algorithm 1 (reproducibility/clarity).\n  - Inconsistent batch-size/exponent notation in StGh: Eq. (9) uses 2N+1 whereas the surrounding text and Algorithm 1 use 2^{N+1} for the mini-batch size (Section 3.2; Eq. (9); Algorithm 1), creating ambiguity in the unbiased direction construction (technical clarity).\n- Model specification appears inconsistent and underspecified\n  - Parameter count mismatch: Section 4.2 states ‚Äú2 hidden layers of sizes 64 and 32 ‚Ä¶ with a total of 194 parameters,‚Äù which is inconsistent with typical parameter counts for a 9‚Äì64‚Äì32‚Äì1 MLP (orders of magnitude larger), risking confusion about the true capacity (technical clarity/reproducibility).\n  - Architectural details are minimal: No information on biases, regularization (dropout/weight decay), or normalization beyond StandardScaler is provided (Section 4.1‚Äì4.2), making the stated parameter count and behavior hard to interpret (reproducibility).\n  - Output layer and thresholding are not fully specified: While BCEWithLogitsLoss is used (Eq. (12); Section 4.2), the decision threshold for computing Ind/Sp/Sf (which depend on binary predictions) is not described (Section 2; Table 3), which affects reported metrics (technical clarity).\n- Misalignment between enforced constraints and reported fairness metrics\n  - Constraint is loss-difference, metrics are Ind/Sp/Sf: The enforced constraint targets the difference in empirical loss across groups (Eq. (4)), while reported fairness metrics reflect independence, separation, and sufficiency (Table 3; Section 2), which are not necessarily optimized by the constraint (evaluation alignment).\n  - Sufficiency degrades for constrained models: The constrained models have higher Sf values than SGD/SGD-Fairret (Table 3; both train and test), but this is neither quantified against Œ¥ nor explicitly analyzed beyond a brief remark (Section 4.2); additionally, the statement ‚ÄúSGD-Fairret slightly improves sufficiency relative to the SGD model ‚Ä¶ Similar observations hold for metrics on the test set‚Äù conflicts with Table 3, where test Sf is higher for SGD-Fairret than SGD (Table 3), creating confusion (analysis depth/consistency).\n  - No aggregate constraint satisfaction summaries: Constraint satisfaction is visualized over time (Figures 1‚Äì2), but there is no tabulated final/average violation or feasibility rate across runs, hindering a clear comparison to fairness metrics (experimental rigor).\n- Evaluation methodology overly reliant on wall-clock time and lacks scalability analysis\n  - Time-based x-axis confounds comparisons: Figures 1‚Äì2 plot versus seconds on a specific laptop (Section 4.2), making results sensitive to implementation details and hardware rather than algorithmic complexity (methodology).\n  - No per-iteration complexity reporting: Methods have different sampling regimes (e.g., StGh‚Äôs geometric mini-batches; Section 3.2; Algorithm 1), but per-iteration cost or effective sample usage is not quantified (scalability/technical rigor).\n  - Limited sensitivity analyses: Aside from one SSw ablation (Appendix B), there is no systematic sensitivity to key hyperparameters (e.g., p0, œÑ, Œ≤ in StGh; Œº, œÅ in SSL-ALM; tolerance/step schedules in SSw) (experimental completeness).\n- Benchmark/tooling claims not fully substantiated by experiments\n  - Massive subgroup support not demonstrated: The claim of ‚Äúup to 5.7 billion protected subgroups‚Äù (Section 1) is not exercised in experiments (Section 4), leaving the scalability claim unvalidated (impact).\n  - Sparse benchmark description: Beyond stating automated ERM construction and dataset use (Section 1), the paper does not detail the benchmark‚Äôs APIs, configuration files, or evaluation protocol for easy extension (No direct evidence found in the manuscript), limiting immediate usability (reproducibility/impact).\n  - Limited algorithmic coverage vs. survey breadth: While Table 2 reviews many methods, the toolbox evaluates four (StGh, SSL-ALM, ALM, SSw; Section 4.2), and does not demonstrate modular comparisons across key design choices (e.g., sampling strategies, globalization, restarts) highlighted in Section 1 (completeness).\n- Ambiguity in SSL-ALM inequality handling and multiplier/projection details\n  - Slack variables are mentioned but absent from equations and updates: Section 3.3 states inequalities are handled ‚Äúwith slack variables‚Äù and minimizes over ùìß = ‚Ñù^n √ó ‚Ñù_{‚â•0}^m, yet LœÅ, KœÅ,Œº, G, and updates (Eq. (10)‚Äì(11)) contain no slack variable s and no equality form with s (technical soundness; Section 3.3; Algorithm 2).\n  - Multiplier and projection handling are unclear: Eq. (11) applies proj_ùìß to x, although ùìß was defined to include a nonnegative component; Algorithm 2 truncates y to zero when ‚à•y‚à• ‚â• M_y (Algorithm 2 lines 3‚Äì6) without projection to y ‚â• 0, leaving the treatment of inequality multipliers ambiguous (clarity/technical soundness).\n  - Resulting ambiguity: It is unclear which precise inequality-constrained problem SSL-ALM solves relative to (1)/(5), affecting interpretability of the comparisons (Section 3.3; impact).Suggestions for Improvement\n- Broaden the empirical scope of the benchmark demonstration\n  - Evaluate across multiple ACS states/tasks from Folktables (Section 4.1) to show robustness across distributions and to substantiate the ‚Äúchallenging benchmark‚Äù claim.\n  - Include additional protected-attribute configurations (e.g., multi-category race without binarization, gender, age bands) to exercise the claimed subgroup scalability (Section 1).\n  - Instantiate constraints aligned with Table 1 beyond loss-difference, such as demographic parity or equal opportunity constraints, and compare their effects alongside Eq. (4) within the same benchmark protocol (Sections 1‚Äì2; Table 1).\n- Provide missing experimental details to enhance reproducibility\n  - Report the exact numeric value of Œ¥ used in Eq. (4) and in Figures 1‚Äì2 (constraint bound), and describe how it was selected (Section 4.2; Figures 1‚Äì2).\n  - State the mini-batch sizes and sampling policies for every algorithm in Section 4.2, including J for SSw (Algorithm 3; Appendix A), and reiterate ALM/SSL-ALM‚Äôs per-iteration sampling choices (Section 3.3) as actually used.\n  - Standardize and report iteration/epoch counts and, in addition to time, provide convergence plots versus iterations to make comparisons implementation- and hardware-agnostic (Figures 1‚Äì2).\n  - Define any additional method parameters used in experiments (e.g., Œª in StGh in Section 4.2) or remove them if unused to avoid confusion (Sections 3.2‚Äì4.2; Algorithm 1).\n  - Correct Eq. (9) to use 2^{N+1} (or clarify the intended construction) consistently with Algorithm 1 and the surrounding text to eliminate ambiguity in the unbiased estimator (Section 3.2; Eq. (9); Algorithm 1).\n- Clarify and tighten the model specification\n  - Recompute and report the exact parameter count for the stated 2-layer MLP (Section 4.2) including biases; if 194 is correct, specify the architecture that yields it (e.g., smaller hidden sizes) to resolve the apparent mismatch.\n  - Document architectural choices (biases, weight decay, dropout, batchnorm), optimizer settings, and any additional regularization to enable replication (Sections 4.1‚Äì4.2).\n  - Specify the decision threshold used to compute Ind/Sp/Sf (Section 2; Table 3), and report if threshold tuning (e.g., 0.5 vs. calibrated) was applied on validation data.\n- Align evaluation metrics with enforced constraints and report feasibility clearly\n  - Complement Ind/Sp/Sf with a directly aligned metric‚Äîe.g., final/average group loss difference‚Äîto match Eq. (4) and tabulate it per method on train and test (Sections 4.2; Figures 1‚Äì2).\n  - Analyze and discuss the observed degradation in sufficiency (Sf) for constrained models (Table 3), quantifying trade-offs against Œ¥ and inaccuracy to inform users of practical implications; also reconcile the narrative with Table 3 where test Sf for SGD-Fairret is higher than for SGD (Table 3).\n  - Add a feasibility table with mean ¬± std of final constraint violation across 10 runs for each method (train/test), enabling straightforward comparisons beyond time-series plots (Figures 1‚Äì2; Table 3).\n- Strengthen methodological evaluation and scalability analysis\n  - Report per-iteration computational cost and effective sample usage (e.g., expected batch size in StGh via p0; Section 3.2) to contextualize time-based plots (Figures 1‚Äì2; Section 4.2).\n  - Include convergence plots versus iterations and ablations on key hyperparameters (p0, œÑ, Œ≤, Œº, œÅ, step schedules) to assess algorithm sensitivity (Sections 3.2‚Äì3.4; Appendix B).\n  - Provide memory/runtime scaling with dataset size and number of groups to substantiate claims of practicality for large-scale fairness-constrained training (Sections 1, 4.1).\n- Substantiate the benchmark/tooling contribution\n  - Demonstrate subgroup scalability by running at least one experiment varying the number/structure of subgroups, validating ‚Äúup to 5.7 billion‚Äù feasibility in practice (Section 1).\n  - Add a dedicated section detailing the benchmark API, configuration files, extensibility points (e.g., how to plug in a new optimizer), and provide example scripts (No direct evidence found in the manuscript).\n  - Expand the comparative study to include modular design choices highlighted in Section 1 (sampling strategies, globalization, restarts), even for one method, to showcase the toolbox‚Äôs breadth beyond the four fixed algorithms (Section 1; Table 2).\n- Clarify SSL-ALM formulation and multiplier/projection handling\n  - If slack variables are used to convert inequalities to equalities, include them explicitly in LœÅ/KœÅ,Œº, the gradient estimator G, and the updates; otherwise, remove the slack-variable claim (Section 3.3; Eq. (10)‚Äì(11); Algorithm 2).\n  - If inequalities are enforced directly, project multipliers onto y ‚â• 0 and define the projection sets unambiguously (e.g., proj on x vs. (x, s)), and clarify the rationale for resetting y to zero at ‚à•y‚à• ‚â• M_y (Algorithm 2; Eq. (11)).\n  - State precisely which constrained problem SSL-ALM optimizes in experiments and how it aligns with (1)/(5), so readers can interpret and reproduce the results (Section 3.3; Section 4.2).Score\n- Overall (10): 6 ‚Äî Useful implementations and comparative study of three recent stochastic constrained methods with clear results on ACSIncome, but limited empirical breadth and some missing/ambiguous details (Sections 4.1‚Äì4.2; Figures 1‚Äì3; Table 3; Table 2).\n- Novelty (10): 6 ‚Äî Implementing and adapting StGh, SSL-ALM, and SSw for fairness-constrained DNN training is practically novel, though the benchmark demonstration is narrow and constraints align with prior formulations (Sections 3.2‚Äì3.4; Eq. (4); Table 1).\n- Technical Quality (10): 4 ‚Äî Method and experiment reporting contain notable gaps and inconsistencies (unspecified Œ¥; SSL-ALM inequality handling and multiplier/projection ambiguity in Section 3.3/Eqs. (10)‚Äì(11)/Alg. 2; parameter-count mismatch in Section 4.2; notation/parameter issues in StGh Eq. (9)/Alg. 1 and Œª in Section 4.2), which hinder reproducibility and interpretation.\n- Clarity (10): 7 ‚Äî The manuscript is generally well-organized with clear formulations (Eq. (1), (4)), a helpful assumptions table (Table 2), and informative figures/tables, albeit with ambiguities such as Œ¥, parameter counts, and SSL-ALM detail (Sections 3‚Äì4; Figures 1‚Äì2; Table 3).\n- Confidence (5): 4 ‚Äî Assessment is based on the provided manuscript with multiple concrete anchors (Sections 3‚Äì4; Figures 1‚Äì3; Table 3), but some reproduction-critical details are missing (e.g., Œ¥, certain hyperparameters), so confidence is high but not absolute."
}