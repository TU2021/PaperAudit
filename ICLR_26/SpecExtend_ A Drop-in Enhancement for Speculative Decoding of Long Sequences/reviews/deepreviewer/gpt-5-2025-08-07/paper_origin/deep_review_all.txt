Summary
The paper introduces SpecExtend, a training-free, drop-in enhancement for speculative decoding on long sequences. It combines efficient attention (FlashAttention for both target and draft prefill; Hybrid Tree Attention for verification) with Cross-model Retrieval (CMR), a KV cache eviction strategy that uses the target model’s attention scores to retain the most relevant input chunks in the draft model’s cache. Experiments across long summarization (GovReport, PG-19, BookSum) and long reasoning (AIME-24) show speedups up to 2.84× and 3.86×, respectively, while maintaining short-input performance. Ablations (Table 4, Table 5) analyze component contributions and retrieval parameters; overhead of CMR is reported to be minimal (Table 8).

Soundness
The methodological core is sound at the systems level. Using target attention to guide draft KV eviction is plausible because verification already computes attention; reusing the last-layer scores minimizes extra compute (§3.2.1 and Table 8). The theoretical framing relies on a standard speedup expression (Eq. 1) and correctly identifies the moderate-length regime where weights dominate memory (Figure 2), motivating reduction of draft KV size while keeping high acceptance length τ. The algorithm (Algorithm 1) integrates CMR without violating lossless verification. However, there are assumptions: (i) relevance is inferred from the last accepted token’s attention in the final layer, which may miss multi-hop or cross-layer dependencies (§3.2.1); (ii) HTA does not produce full matrices, so switching to standard attention at the final layer introduces a coupling that may affect verification latency, though authors report small overhead (Table 8). Empirical evidence across varied models (Vicuna, LongChat, Llama-3.1, EAGLE/EAGLE-3) is consistent and supports claims (Table 2, Figure 5, Table 6, Figure 6). A notable gap is the absence of output-quality metrics for summarization (e.g., ROUGE) and reasoning accuracy (AIME pass rate), leaving correctness preservation unquantified beyond the theoretical losslessness of speculation.

Presentation
The paper is clearly structured with informative figures and tables. Figure 1 gives a good overview; Figures 4–6 and Table 2 comprehensively report speed and τ across lengths and setups; Table 4 and Table 5 offer useful ablations; Table 8 clarifies overhead. Cross-referencing is consistent, and the narrative explains how the components interact (§3.1–3.2). Minor clarity issues include lack of precise chunk selection heuristics (e.g., tie-breaking, normalization across layers) and limited detail on HTA integration mechanics (§3.1), but overall readability is high.

Contribution
CMR is a novel, training-free way to align the draft cache with target attention, directly addressing early degradation in moderate-length regimes (Figure 2, §1). Integrating FlashAttention and HTA in a cohesive pipeline for speculative decoding is practical and impactful. Compared to prior long-context speculation that drafts with the base model (TriForce; §2), SpecExtend maintains speed by drafting with a small model while leveraging target signals. The paper’s contribution is primarily systems-engineering and empirical, not theoretical; the speedup analysis is adapted from prior work (Eq. 1), and no formal result links CMR to τ improvements beyond empirical evidence.

Strengths
- Clear identification of the moderate-length degradation regime and its causes (Figure 2; §1).
- Practical, training-free enhancement with consistent speedups across models and tasks (Table 2, Figure 5, Figure 6).
- Strong component and parameter ablations that isolate contributions (Table 4, Table 5).
- Minimal overhead for retrieval and integration with HTA/FlashAttention (Table 8; §3.2.1).
- Evidence that CMR improves both acceptance length and divergence to target (Figure 3; Table 1).

Weaknesses
- No quality metrics for task outputs (ROUGE for summarization; AIME accuracy) despite extensive speed reporting (Table 2, Figure 5, Figure 6).
- Reliance on final-layer attention of the last accepted token may underperform on multi-hop or dispersed dependency tasks (§3.2.1), and this is not stress-tested beyond Needle Retrieval (Table 1).
- Baseline fairness: naive AR appears without FlashAttention/FlashDecoding baselines in some comparisons, potentially inflating reported speedups (Table 3; §4.1.1).
- Limited analysis of CMR’s behavior under domain shift or adversarial context where attention may be noisy; retrieval frequency and chunk granularity are only empirically tuned (Table 5).
- Evaluation uses 20 sampled inputs per length (Appendix A.2), which may be statistically thin for variance-heavy long-context tasks.

Questions
1. Can the authors report summarization quality (e.g., ROUGE/LONG-ROUGE) and AIME accuracy to confirm that altered draft behavior does not affect final outputs in practice (Tables 2, 3, 6)?
2. How sensitive is CMR to using final-layer attention versus aggregating attention across layers or heads? Any comparison to multi-layer/attention-aggregated retrieval?
3. Does computing standard attention in the last layer ever become a bottleneck for larger targets or multi-GPU settings? Please provide scaling results beyond single A100 (Table 8; Appendix A.2).
4. How does CMR perform on tasks demanding multi-hop retrieval or dispersed evidence (e.g., LongBench variants), beyond Needle Retrieval (Table 1)?
5. What heuristic is used for chunk score normalization and tie-breaking during SELECTCHUNKS in Algorithm 1? Is recency bias or decay applied?
6. Can SpecExtend integrate with sliding-window or attention-sink architectures without degrading τ, and what are best practices for working cache size (Table 5) across models?

Rating
- Overall (10): 8 — Strong systems contribution with consistent speedups and minimal overhead, though output-quality metrics are missing and attention-score reliance is simplified (Table 2, Figure 5, Table 8, §3.2.1).
- Novelty (10): 7 — CMR’s target-guided draft cache eviction is new and practical, but built on existing efficient attention and speculative decoding frameworks (Figure 1, §3.2).
- Technical Quality (10): 7 — Sound engineering and extensive experiments, yet limited theoretical analysis and missing task-quality evaluations (Eq. 1, Table 2, Figure 6).
- Clarity (10): 8 — Clear exposition with helpful figures/tables, minor details omitted for chunk selection and HTA integration (Figure 1, §3.1–3.2, Algorithm 1).
- Confidence (5): 4 — High confidence based on detailed review of provided sections, but absence of quality metrics and limited statistical depth temper certainty.


Summary
SpecExtend proposes Cross-model Retrieval (CMR) to dynamically prune the draft model’s KV cache using target model attention, alongside adopting FlashAttention and Hybrid Tree Attention (HTA) to speed prefill and verification. The aim is to counter early speculative-decoding slowdown on moderately long inputs where model weights dominate memory (Figure 2). Results across Vicuna/LongChat/Llama-3.1 and EAGLE/EAGLE-3 setups claim up to 2.84× (summarization) and 3.86× (reasoning) speedups with preserved short-input performance (Table 2, Figure 5, Figure 6).

Soundness
The approach is coherent: verification yields attention signals that can guide the draft’s cache; pruning reduces draft latency while keeping relevant context, thus increasing the accepted length τ (Eq. 1; §3.2.2). Algorithm 1 correctly interleaves retrieval and verification. The needle-retrieval study indicates CMR improves accuracy dramatically compared to StreamingLLM and approaches TriForce (Table 1), corroborating the alignment hypothesis. However, several aspects are under-specified or fragile: (i) using only the last accepted token’s attention to rank chunks may ignore multi-token conditioning; (ii) final-layer-only attention may not capture lower-layer syntactic/positional cues; (iii) verification uses HTA but attention extraction falls back to standard attention, introducing potential latency and semantic mismatch; (iv) speedups are reported primarily as tokens/s and τ without task-level correctness.

Presentation
The exposition is generally clear and well-supported visually (Figure 1, Figures 4–6). Tables provide thorough cross-length comparisons (Table 2) and ablations (Table 4, Table 5). The paper would benefit from a more explicit description of SELECTCHUNKS and UPDATEDRAFTCACHE, including normalization, tie-breaks, and how cache compaction is implemented to avoid fragmentation (§3.2.1). Theoretical analysis is brief and mainly reprises existing formulas (Eq. 1).

Contribution
The main contribution is a practical, training-free mechanism to use target attention for draft cache retention, addressing the moderate-length regime where prior methods perform poorly (TriForce, MagicDec; Table 3). Integrating FlashAttention and HTA in speculative decoding pipelines is also useful, though incremental. The work targets an important operational pain point and offers reproducible gains (code link; Appendix A.2).

Strengths
- Addresses a concrete, underexplored performance dip before KV cache dominates (Figure 2; §1).
- Simple, effective cache-retention mechanism with strong empirical gains across models and tasks (Table 2, Figure 6).
- Comprehensive ablation of components and parameters (Table 4, Table 5).
- Minimal retrieval overhead demonstrated (Table 8).
- Preserves short-input performance (Figure 5; §4.1.1).

Weaknesses
- Lack of end-task metrics (ROUGE for summarization; AIME accuracy) to ensure that speedups do not mask degraded quality (Tables 2, 3).
- CMR relies on attention as relevance; attention may be noisy or non-causal in some settings; robustness analysis beyond Needle Retrieval is limited (Table 1; Figure 3).
- Baselines may be unfavorable: TriForce/MagicDec draft with the larger base model, but naive AR is not reported with FlashAttention, potentially skewing speedup comparisons (Table 3).
- Theoretical contribution is limited; no bound or model explaining when attention-guided pruning improves τ (Eq. 1; §3.2.2).
- Experimental sampling size per length is small (Appendix A.2), which may limit statistical robustness.

Questions
1. Please report summarization ROUGE/LONG-ROUGE and AIME accuracies to substantiate quality preservation, at least for the configurations in Table 2 and Figure 6.
2. How would results change if attention from multiple recent queries (e.g., sliding window of accepted tokens) were aggregated to rank chunks?
3. Can you compare full-matrix attention aggregation across layers/heads vs final-layer-only selection (§3.2.1) and quantify τ and latency trade-offs?
4. Is CMR still beneficial when the draft model uses attention sinks or sliding windows (StreamingLLM hybrids), and can you show an ablation combining sinks with CMR?
5. What mechanisms ensure cache consistency when removing/adding chunks mid-decoding—do positional encodings or rotary embeddings introduce drift?

Rating
- Overall (10): 7 — A practical, well-engineered solution with clear speed benefits but missing task-quality metrics and limited theoretical depth (Table 2, Table 3, §3.2).
- Novelty (10): 7 — Target-guided cache eviction for drafts is new in the moderate-length regime, though built on existing attention/verification tools (Figure 1, §3.2).
- Technical Quality (10): 6 — Solid systems evidence and ablations but lacks correctness metrics and deeper analysis of attention-based relevance (Table 1, Table 4, Table 5).
- Clarity (10): 8 — Clear figures and tables; algorithms are understandable but some implementation details are glossed over (Figure 1, Algorithm 1, §3.2.1).
- Confidence (5): 4 — High confidence in understanding the presented material; conclusions tempered by missing quality evaluations and baseline fairness checks.


Summary
This work targets the early degradation of speculative decoding when inputs become moderately long. SpecExtend combines FlashAttention for prefill and HTA for verification with Cross-model Retrieval (CMR), which prunes the draft model’s KV cache using target attention scores from the latest verification step. The method is training-free, aims to maintain high accepted length τ, and shows strong speedups across models and datasets (Table 2, Figure 5), with detailed component and parameter ablations (Table 4, Table 5) and low overhead (Table 8).

Soundness
The systems rationale is credible: speculative decoding benefits from higher τ and faster draft passes; CMR reduces draft KV size while retaining context judged relevant by the target. Algorithm 1 respects verification correctness, and the re-use of attention signals minimizes overhead. The needle task results (Table 1) validate the idea that small drafts can leverage target-selected context. Nonetheless, the decision to rely on final-layer attention of the last accepted token could fail in settings where relevance is distributed or where attention does not reflect causally needed evidence; this is acknowledged indirectly via ablations but not analyzed theoretically. Comparisons (Table 3) show clear wins, yet the baselines do not include naive AR with FlashAttention, leaving some uncertainty about fair end-to-end baselines.

Presentation
The paper is well-organized, with a clear pipeline figure (Figure 1) and multi-angle empirical reporting (Figures 4–6, Table 2). Section 3 succinctly explains the integration of FlashAttention/HTA and CMR. However, more detail on cache management (e.g., positional encoding impacts when chunks are dropped/retained, exact procedure for cache compaction) would improve reproducibility; Algorithm 1 abstracts UPDATEDRAFTCACHE but the mechanics matter for correctness and speed.

Contribution
The key contribution is a simple but effective training-free mechanism (CMR) that uses target attention to guide draft KV retention, enabling speedups in the moderate-length regime that prior methods struggle with (Figure 2; Table 3). It also demonstrates compatibility with newer draft architectures (EAGLE-3; Table 6) and extremely long inputs (Table 7). The theoretical component is limited to restating an existing speedup formula, but the empirical contribution is substantial.

Strengths
- Addresses a practical bottleneck that impacts real deployments (Figure 2; §1).
- Broad empirical validation across datasets and models, including newer Llama-3.1/EAGLE-3 and very long contexts (Table 6, Table 7).
- Component contribution and parameter sensitivity are well documented (Table 4, Table 5).
- Minimal overhead of retrieval and preserved short-input performance (Table 8; Figure 5).
- Clear, drop-in integration that can be adopted by existing frameworks.

Weaknesses
- Missing task-quality metrics (ROUGE, AIME accuracy) despite strong speed claims (Table 2, Figure 6).
- Limited theoretical analysis of why attention-guided chunking improves τ; no robustness guarantees for attention noise (Eq. 1; §3.2.2).
- Potentially favorable baseline choices; naive AR baselines are not explicitly paired with FlashAttention (Table 3).
- The reliance on last-token, final-layer attention may be brittle for reasoning chains requiring earlier evidence; only Needle Retrieval is deeply analyzed (Table 1).

Questions
1. Please add end-task quality metrics for summarization and AIME to establish that the method is strictly lossless in practice (beyond theory), given the changed draft dynamics.
2. Can you provide evidence that multi-token query attention or cumulative attention over recent accepted tokens materially changes τ or latency?
3. How does CMR interact with positional encodings (e.g., RoPE) when chunks are pruned—are there re-indexing or re-scaling issues?
4. Are gains consistent on multi-GPU or CPU-offloaded KV cache settings common in serving systems?
5. Could you contrast CMR with heuristic retrieval signals (e.g., token entropy, surprise) shown in Figure 3 and assess combined signals?

Rating
- Overall (10): 7 — Strong systems paper with practical impact, but lacks task-quality metrics and deeper theoretical grounding (Table 2, Figure 6, §3.2).
- Novelty (10): 7 — Using target attention to guide draft KV pruning in speculative decoding is a neat, training-free idea (Figure 1, §3.2.1).
- Technical Quality (10): 7 — Solid engineering and experiments; some baseline fairness and robustness gaps remain (Table 3, Table 1).
- Clarity (10): 8 — Clear pipeline and results; some implementation specifics omitted (Figure 1, Algorithm 1).
- Confidence (5): 4 — Confident in assessment based on thorough reading; quality metrics omission limits certainty.


Summary
The paper proposes SpecExtend, a drop-in enhancement for speculative decoding on long contexts. It integrates FlashAttention to accelerate prefill and Hybrid Tree Attention to speed verification, and introduces Cross-model Retrieval (CMR), which uses attention scores from the target model to select top-k input chunks to retain in the draft’s KV cache. This aims to preserve high acceptance length τ while reducing draft latency. Empirical results demonstrate consistent speedups across datasets and models, including EAGLE-3 and Llama-3.1 setups (Table 2, Table 6; Figure 5–6), with component ablations (Table 4), parameter sensitivity (Table 5), and negligible overhead (Table 8).

Soundness
SpecExtend is methodologically coherent and compatible with lossless speculative decoding. The key assumption—target attention indicating relevance—is tested via Needle Retrieval (Table 1), acceptance/divergence analyses (Figure 3), and broad speed/τ improvements (Table 2). The system design carefully minimizes overhead by reusing verification attention and only computing final-layer scores when needed (§3.2.1, Table 8). Still, the paper does not analyze scenarios where attention is misaligned with required evidence or where longer-range compositional dependencies dominate; nor does it quantify the trade-off between retrieval frequency and latency beyond tokens/s aggregates (Table 5). The theoretical section (Eq. 1) frames the problem but provides no new formal results.

Presentation
The exposition is strong: figures communicate the pipeline and performance well (Figure 1, Figures 4–6); tables are comprehensive and well-annotated (Table 2, Table 4–5). Algorithm 1 is readable, although UPDATEDRAFTCACHE and SELECTCHUNKS are abstracted. The paper provides hardware, dataset sampling, and tree-expansion details (Appendix A.2), which aids reproducibility. Minor wording issues include occasional reliance on qualitative claims (e.g., locality of context, §3.2.1) without quantitative backing.

Contribution
The paper’s main novelty lies in training-free, target-guided draft cache eviction that improves speculative decoding in the moderate-length regime—an underexplored setting identified by the authors (Figure 2; §1). Integrating FlashAttention/HTA into speculative pipelines adds practical value. Compared to prior work that drafts with the base model (TriForce) or focuses on extreme-length KV bottlenecks (MagicDec), SpecExtend offers a balanced, practical improvement with consistent empirical gains (Table 3).

Strengths
- Clear identification of the problem regime and corresponding design choices (Figure 2; §1).
- Strong empirical validation across diverse setups and lengths, including newer EAGLE-3 (Table 6) and 128K inputs (Table 7).
- Thorough ablation and parameter studies (Table 4, Table 5) and overhead reporting (Table 8).
- Simple, drop-in nature with code availability; preserves short-input performance (Figure 5).

Weaknesses
- No task-quality metrics (e.g., ROUGE, AIME pass rate) provided alongside speedups (Table 2, Figure 6).
- Attention-only relevance may be brittle; lack of alternative signals or multi-layer aggregation analysis (§3.2.1, Figure 3).
- Baseline fairness concerns: comparisons do not show naive AR with FlashAttention/FlashDecoding (Table 3).
- Limited statistical analysis due to small sample sizes per length (Appendix A.2), and absence of variance/error bars.

Questions
1. Please include task-quality metrics for summarization and reasoning to confirm practical losslessness under SpecExtend (Table 2, Figure 6).
2. Have you tried aggregating attention over multiple recent accepted tokens or layers; if so, what are the τ/latency trade-offs?
3. Can you report results with naive AR + FlashAttention to contextualize speedup numbers (Table 3)?
4. How does CMR behave on inputs with scattered evidence or contradictory cues (e.g., LongBench subsets), beyond Needle Retrieval (Table 1)?
5. What are the memory implications of frequent cache updates for very long outputs; do you observe fragmentation or allocator overhead?

Rating
- Overall (10): 8 — Effective, well-executed systems contribution with compelling speedups; key missing quality metrics and limited theory prevent a higher score (Table 2, Figure 5–6, §3.2).
- Novelty (10): 7 — Target-guided draft cache pruning is a useful and new idea in this setting (Figure 1, §3.2).
- Technical Quality (10): 7 — Strong engineering and evaluations; baseline fairness and robustness analyses need strengthening (Table 3, Table 1).
- Clarity (10): 8 — Clear writing and visuals; some implementation details abstracted (Figure 1, Algorithm 1).
- Confidence (5): 4 — Confident based on breadth of evidence; lack of task-quality metrics reduces certainty.