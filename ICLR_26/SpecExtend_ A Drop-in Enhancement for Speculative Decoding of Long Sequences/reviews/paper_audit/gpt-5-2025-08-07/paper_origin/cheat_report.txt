Academic integrity and internal consistency risk report

Summary of high-impact issues identified

1) Misdefined “speedup” in Equation 1 and undefined variables
- Evidence: Section 3.2.2 (Equation 1) defines “the speedup of standard speculative decoding” as T_avg^sd / T_t = (1/τ(n,d)) (d·T_d/T_t + T_v(n)/T_t).
- Problems:
  - The quantity T_avg^sd / T_t is a normalized latency ratio, not a speedup. Speedup should be T_t / T_avg^sd, i.e., the reciprocal of the expression shown. Labeling the ratio as “speedup” is a clear conceptual error that can mislead interpretation of results.
  - The variable d appears in Equation 1 but is not defined in the surrounding text. Earlier, K is used as the drafted block size (Algorithm 1, Section 3.2.1), creating a notation mismatch and ambiguity about whether d=K or something else.
- Impact: Mislabeling the core theoretical metric and leaving variables undefined undermines the correctness and interpretability of the speedup analysis that the paper relies on.

2) Inconsistent baselines for “speedup” across text, figures, and tables
- Evidence:
  - Abstract and Conclusion (Blocks #1, #34): “SpecExtend accelerates speculative decoding by up to 2.84× on 16K-token long summarization and up to 3.86× on long reasoning.”
  - Section 4.1.1 (Block #18): “SpecExtend accelerates standard speculative decoding with LLM draft models by 2.37× and 2.22×, yielding overall speedups of 2.39× and 2.87× over naive autoregressive generation.”
  - Table 2 (Block #22): “Speedup is measured relative to naive autoregressive generation.”
  - Table 6 (Block #32): Llama-3.1-8B/Instruct with EAGLE-3 at 16K shows Tokens/s 18.71 (No) vs 53.18 (Yes); ratio ≈ 2.84× (this is speedup over the standard setting, not over AR).
  - Figure 5 (Block #22, caption): “Speedup comparison of standard speculative decoding and SpecExtend…,” but the plotted numbers (e.g., 1.38 and 2.82 at 16K) match Table 2’s speedups vs AR.
- Problems:
  - The paper mixes at least two baselines—standard speculative decoding and naive autoregressive generation—without consistently specifying which one is used in each “speedup” statement. For example, the headline “up to 2.84× on 16K summarization” matches Table 6 as speedup over the standard setup, while Table 2 and Figure 5 report speedups vs AR; both are called “speedup” in the narrative.
  - Table 2 contains several speedups >2.84× vs AR (e.g., GovReport, LC-7B/EAGLE, 16K: 3.21×; BookSum, V-7B/EAGLE, 16K: 3.18×), contradicting the “up to 2.84×” phrasing if the baseline is AR. The claimed “up to 2.84×” maximum is only tenable if the baseline is the standard speculative decoding configuration (as in Table 6), but this is not made explicit in the abstract/conclusion.
- Impact: Ambiguity in baselines impedes fair comparison and can overstate or understate results depending on interpretation. It materially affects trust in the reported gains.

3) Large internal inconsistencies in Table 2 across different model configurations
- Evidence: Table 2 (Block #22) reports Tokens/s and speedups for multiple Target/Draft settings (V-7B/V-68M vs LC-7B/LC-68M; V-7B/EAGLE vs LC-7B/EAGLE) on GovReport/PG-19/BookSum. Multiple rows across different target models have identical Tokens/s while reporting different speedups:
  - GovReport, 1K, “No” entries:
    - V-7B/V-68M: Tok/s 100.31, Speedup 1.78×
    - LC-7B/LC-68M: Tok/s 100.31, Speedup 1.78× (identical Tok/s repeated for different base models)
  - GovReport, 16K, “No” entries:
    - V-7B/V-68M: Tok/s 16.56, Speedup 1.38×
    - LC-7B/LC-68M: Tok/s 16.56, Speedup 1.51×
    - Same Tok/s across different target models, but speedups differ, implying different AR baselines that are not shown or explained.
  - Similar repetitions appear in other datasets and in EAGLE rows (e.g., GovReport 1K “No” Tok/s 131.06 for both V-7B/EAGLE and LC-7B/EAGLE).
- Problems:
  - Identical performance (Tokens/s) reported across distinct target model configurations is implausible without explanation and strongly suggests copy/paste or reporting errors.
  - Speedup values differ while Tokens/s are identical across settings, implying unreported changes in the AR baselines per target model; these baselines are neither tabulated nor discussed.
- Impact: This undermines the credibility of the comparative evaluation and makes it impossible to verify the claimed improvements per configuration.

4) Unsupported claim referencing EAGLE-1
- Evidence: Section 4.1.2 “Decoding” (Block #23) states “EAGLE-3 … draft accuracy drops sharply beyond 2K tokens, even falling below EAGLE-1 (Table 6).”
- Problem: Table 6 (Block #32) contains only EAGLE and EAGLE-3. No results for EAGLE-1 are provided. No direct evidence found in the manuscript to substantiate the comparison to EAGLE-1.
- Impact: The claim is not supported by the cited evidence and should be removed or supported with the appropriate table/figure.

5) Cross-reference error in experimental details
- Evidence: Appendix A.2 (Block #40) says, “We use the optimal working KV cache size and retrieval parameters described in Section 5.”
- Problem: Section 5 is the Conclusion (Block #34) and does not contain retrieval parameter details. The relevant ablations are in Section 4.3.2 (Block #28).
- Impact: Misreferencing critical experimental settings hinders reproducibility.

6) Potential contradiction about “no additional forward passes” for attention scores
- Evidence: Section 3.2.1 (Block #10) states: “the target model’s attention scores are obtained directly from the most recent verification step, requiring no additional forward passes… To address [FlashDecoding’s lack of full scores], we compute standard attention and extract attention scores of only the final layer.”
- Problem: The phrase “requiring no additional forward passes” conflicts with “we compute standard attention,” which implies changing the verification computation from FlashDecoding to standard attention (at least in the final layer). Whether this is a separate pass or a modified pass is unclear. Table 8 (Block #39) shows a small overhead for “Target Forward w/ Retrieval,” suggesting the same pass with altered computation, but the wording remains contradictory.
- Impact: Ambiguity about added computation in the critical verification path affects claims of “training-free, drop-in” overhead and should be clarified to avoid misinterpretation of latency costs.

7) Minor numerical mismatch in text vs table for extremely long inputs
- Evidence: Section 4.4.2 (Block #33) text: “yields a 2.67× speedup over the standard setting.” Table 7 (Block #33) shows “No” Tok/s 8.45 vs “Yes” Tok/s 22.59 at 128K, ratio ≈ 2.67×, but at 32K the ratio is ≈ 2.73× (23.05/8.45), while the table’s “speedup” column for 32K (2.08×) is vs AR.
- Problem: Mixed baselines in the same section (vs standard in the text, vs AR in the table) and small numeric inconsistency (2.73× vs reported 2.67× for 32K if using standard as baseline).
- Impact: While not large, the inconsistency adds to the general confusion about baselines and reported gains.

Assessment

The manuscript presents promising ideas, but there are several substantive, evidence-based inconsistencies and ambiguities that materially affect interpretability and trustworthiness:

- Theoretical metric mislabeling (Equation 1) and undefined variables.
- Systematic baseline confusion for speedup reporting across sections, figures, and tables.
- Apparent copy/paste or reporting errors in Table 2 (identical Tokens/s across different model configurations with differing speedups).
- Unsupported claims (EAGLE-3 vs EAGLE-1) and misreferenced sections for key experimental setup details.
- Ambiguity regarding additional computation in verification steps when extracting attention scores.

Recommendations to address the issues

- Correct Equation 1: explicitly define d, use consistent notation with K, and label the ratio as normalized latency; present speedup as its reciprocal, and clearly state the baseline (standard vs AR).
- Standardize “speedup” reporting: for each table/figure and in the abstract/conclusion, state the baseline (standard speculative decoding vs naive AR) and avoid mixing baselines within the same section.
- Audit and fix Table 2: re-run or re-tabulate Tokens/s per configuration to avoid duplicated values across different target models; provide AR baseline Tokens/s used to compute each speedup to ensure transparency.
- Remove or substantiate the EAGLE-1 comparison: add the missing EAGLE-1 results or delete the claim.
- Fix cross-references: change Appendix A.2’s reference from Section 5 to Section 4.3.2.
- Clarify the verification computation: explicitly state whether attention score extraction is done within the same forward pass (final layer only) and quantify the overhead; align wording (“no additional forward passes”) with the actual implementation.

If these issues are resolved, the paper’s core contributions can be evaluated more reliably. As it stands, the inconsistencies could significantly impact readers’ confidence in the reported gains and the correctness of the analysis.