# Global Summary
- Problem: Speculative decoding in LLMs accelerates inference by drafting tokens with a small model and verifying with the target model, but performance degrades early as input length grows (moderate-length regime), before the KV cache dominates memory. Identified causes are quadratic attention latency in prefill/verification and reduced draft accuracy on long contexts (small drafts trained on short sequences).
- Core approach: SpecExtend is a training-free, drop-in enhancement combining efficient attention (FlashAttention for prefill; Hybrid Tree Attention for tree-based verification) and Cross-model Retrieval (CMR), a KV cache eviction strategy that uses target-model attention scores to select top-k relevant chunks for the draft model’s cache.
- Evaluation scope: Long summarization (GovReport, PG-19, BookSum) at up to 16K tokens; long reasoning (AIME-24) where outputs grow very long; newer configuration with Llama-3.1-8B-Instruct with EAGLE/EAGLE-3; extremely long inputs up to 128K tokens. Drafts include off-the-shelf 68M LLMs and EAGLE/EAGLE-3; targets include Vicuna-7B-16K, LongChat-7B-16K, Llama-3.1-8B-Instruct, and DeepSeek-R1-Distill-Llama-8B.
- Key findings: SpecExtend achieves up to “2.84×” speedup on 16K summarization and up to “3.86×” on long reasoning. It improves average accepted length by up to “2.55×” (draft accuracy) and preserves short-input performance. In AIME-24 with DeepSeek-R1-Distill-Llama-8B/EAGLE-3, tokens/s increased from “30.34” to “117.21”, and average accepted length rose from “1.89” to “5.95” (speedup “3.86×” over standard; “3.73×” over naive autoregressive).
- Quantitative support: Across GovReport/PG-19/BookSum, SpecExtend consistently boosts tokens/s and speedups versus standard speculative decoding (e.g., GovReport Vicuna-7B/Vicuna-68M at 16K: “2.82×” speedup; LC-7B/LC-68M at 16K: “2.84×”). Extremely long PG-19 inputs (32K–128K) show tokens/s rising from “8.45” to “23.05” at 32K (speedup “2.08×”), with naive AR OOM beyond 64K.
- Caveats and overhead: SpecExtend is training-free and drop-in; it computes final-layer attention to obtain scores, adding minimal latency (target forward “53.76 ms” vs “54.11 ms”; retrieval cache update “0.34 ms”). Retrieval can be applied adaptively/less frequently. Naive AR runs out of memory beyond 64K tokens in the 128K PG-19 test.

# Abstract
- Problem framing: Speculative decoding speedups drop at moderate input lengths; early degradation largely underexplored.
- Method: SpecExtend integrates FlashAttention (prefill) and Hybrid Tree Attention (verification) for efficiency; proposes Cross-model Retrieval (KV cache eviction) informed by the target model’s attention to retain relevant context in the draft cache without retraining.
- Claims: Training-free; improves draft accuracy and speed on long inputs; preserves short-input performance of SOTA frameworks.
- Quantitative highlights: Accelerates speculative decoding by “up to 2.84×” on 16K summarization and “up to 3.86×” on long reasoning.
- Availability: Code at github.com/jycha98/SpecExtend.

# Method
- Overview: SpecExtend’s components are (1) efficient attention (FlashAttention, Hybrid Tree Attention) and (2) Cross-model Retrieval (CMR) for training-free draft cache management. Theoretical speedup analysis (Equation 1) and in-depth CMR effectiveness analysis are provided.
- Efficient attention:
  - Prefill: FlashAttention applied to target and draft models to reduce quadratic memory and latency.
  - Verification: Hybrid Tree Attention enables FlashDecoding compatibility with tree-structured speculative verification; applied to target model to accelerate verification.
- Cross-model Retrieval (CMR):
  - Goal: Reduce draft KV cache size to speed drafting while preserving globally relevant context for accuracy on long inputs without retraining.
  - Procedure: Divide input prefix into fixed-size chunks; rank chunks by average attention scores from the target model using the last accepted token as query; select top-k chunks; update draft cache; draft K candidate tokens; verify with target model which also produces attention scores for subsequent retrieval; compute standard attention at final layer to extract scores (FlashDecoding otherwise avoids full matrices).
  - Overhead: Table 8 reports minimal additional latency (target forward “53.76 ms” vs “54.11 ms”; retrieval cache update “0.34 ms”; draft forward “0.84 ms”). Retrieval updates can be applied adaptively/less frequently.
  - Effect: Improves average accepted length by “up to 2.55×” on inputs up to 16K tokens; outperforming static eviction (StreamingLLM).
- Algorithmic details:
  - Speculative loop with optional retrieval, DRAFT to produce K token distributions p_i, sampling candidate tokens, VERIFY against target q_i, CORRECT on mismatch; select top-k chunks c_1…c_k via SELECTCHUNKS(s); UPDATEDRAFTCACHE invoked on retrieval steps.
- Theoretical speedup (Equation 1):
  - Speedup depends on target per-token latency T_t, draft per-token latency T_d, verification cost T_v(n), and average accepted length τ(n,d).
  - In moderate-length regime, model weights dominate memory; speedup requires low T_d and high τ; SpecExtend reduces T_d via smaller draft and reduced KV cache, maintains τ via CMR, reduces T_v via Hybrid Tree Attention, and mitigates prefill dilution via FlashAttention.
- Effectiveness analysis:
  - Needle Retrieval task with Vicuna-7B/160M vs TriForce (Vicuna-7B for drafting+verification):
    - Perplexity: Full KV “8.311”, StreamingLLM “2.435”, CMR “2.237”, TriForce “2.191”.
    - Accuracy: Full KV “0.081”, StreamingLLM “0.166”, CMR “0.823”, TriForce “0.976”.
  - Acceptance rate and divergence:
    - Acceptance rates: Hard tokens ~“62%” for both StreamingLLM and CMR; Easy tokens ~“72%” (StreamingLLM) vs ~“77%” (CMR).
    - Natural divergence D_n: 1st token ~“0.4” vs ~“0.25”; 2nd ~“0.4” vs ~“0.35”; 3rd ~“0.4” vs ~“0.35”; Resampled ~“0.9” vs ~“0.75” (StreamingLLM vs CMR).
  - Interpretation stated by authors: CMR supplies target-guided, fine-grained context, reducing draft–target divergence and helping both hard and easy tokens; ablation in long summarization is in Section 4.3.1.

# Experiments
- Setup:
  - Tasks: Long summarization (very long input from start) and long reasoning (short input but very long generation).
  - Models: Base targets Vicuna-7B-16K, LongChat-7B-16K; drafts EAGLE and off-the-shelf Vicuna-68M/LLaMA-68M; dynamic tree expansion drafting; generation length 256 tokens with temperature 0 for summarization.
  - Long reasoning: Target DeepSeek-R1-Distill-Llama-8B; draft EAGLE-3; AIME-24 benchmark; max generation length 32K; temperature 0.5.
  - Hardware: Single A100 80GB GPU; further details in Appendix A.2.
- Main findings across datasets:
  - Average accepted length improvements (Vicuna-7B/68M, Figure 4a):
    - Standard: declines from ~“3.5” (1K) to ~“1.8” (16K).
    - StreamingLLM: ~“3.5” to ~“2.6”.
    - SpecExtend: ~“3.8” (1K), ~“3.0–3.5” (2K), ~“2.8–3.0” (8K–16K).
  - End-to-end latency breakdown on 16K (Figure 4b): Standard ~“0–18 s”; SpecExtend reduces to ~“0–8 s” by lowering target/draft prefill and verification latencies.
- Long summarization results (Table 2; examples):
  - GovReport, Vicuna-7B/Vicuna-68M:
    - Speedup vs AR: 1K “1.78×” → “2.28×” with SpecExtend; 16K “1.38×” → “2.82×”.
    - Tokens/s: 1K “100.31” → “128.59”; 16K “16.56” → “33.84”.
    - Average τ: 1K “2.73” → “3.80”; 16K “1.59” → “3.07”.
  - GovReport, Vicuna-7B/EAGLE:
    - Speedup vs AR: 1K “2.57×” → “2.58×”; 16K “1.61×” → “3.08×”.
    - Tokens/s: 1K “144.77” → “145.53”; 16K “19.35” → “37.05”.
    - Average τ: 1K “4.61” → “4.58”; 16K “2.00” → “3.51”.
  - GovReport, LongChat-7B/LC-68M:
    - Speedup vs AR: 16K “1.51×” → “2.84×”.
  - PG-19, Vicuna-7B/Vicuna-68M:
    - Speedup vs AR: 8K “1.01×” → “2.39×”; 16K “1.29×” → “2.87×”.
    - Tokens/s: 8K “21.80” → “47.64”; 16K “14.73” → “32.88”.
    - Average τ: 8K “1.55” → “2.65”; 16K “1.54” → “2.70”.
  - PG-19, Vicuna-7B/EAGLE:
    - Speedup vs AR: 8K “1.32×” → “2.67×”; 16K “1.48×” → “3.09×”.
  - BookSum, Vicuna-7B/Vicuna-68M:
    - Speedup vs AR: 16K “1.30×” → “2.98×”.
  - BookSum, Vicuna-7B/EAGLE:
    - Speedup vs AR: 16K “1.57×” → “3.18×”.
- Long reasoning (AIME-24, DeepSeek-R1-Distill-Llama-8B/EAGLE-3):
  - Average accepted length: Naive AR “1.00”; EAGLE-3 “1.89”; EAGLE-3 + SpecExtend “5.95”.
  - Tokens/s: Naive AR “31.42”; EAGLE-3 “30.34”; EAGLE-3 + SpecExtend “117.21”.
  - Speedups: SpecExtend vs standard “3.86×”; vs naive AR “3.73×”.
  - Note: Authors state EAGLE-3 draft accuracy drops sharply beyond “2K” tokens; SpecExtend recovers long-input accuracy while preserving short-input strength.
- Comparison with other methods (Table 3; Vicuna-7B/Vicuna-68M across lengths):
  - FlashDecoding: speedups up to “1.58×” (BookSum 16K).
  - TriForce: speedups mostly “~1.1–1.2×” and fall near “1.0×” at 16K GovReport.
  - MagicDec: speedups “~1.03–1.24×”.
  - Standard speculative decoding: e.g., GovReport 1K “1.78×”, 16K “1.38×”.
  - Standard + SpecExtend: GovReport 1K “2.28×”, 16K “2.65×”; PG-19 16K “2.70×”; BookSum 16K “2.81×”.
- Ablations (Table 4; Vicuna-7B/68M on GovReport; speedups vs standard):
  - FlashAttention: up to “1.25×” speedup at “8K–16K” (tokens/s “34.33” at 8K; “22.07” at 16K).
  - Hybrid Tree Attention: minor overhead at short lengths; up to “1.19×” beyond 8K (16K tokens/s “20.95”).
  - StreamingLLM: improves at longer lengths (16K tokens/s “22.39”, “1.27×”).
  - CMR: largest gains; 16K tokens/s “25.82”, “1.46×” speedup; τ increases from “1.72” to “2.93”.
- Retrieval parameters (Table 5; GovReport 8K; Vicuna-7B target; drafts Vicuna-68M/EAGLE):
  - Optimal working cache sizes: ~“1024” tokens for Vicuna-68M (tokens/s “33.69”), ~“2048” for EAGLE (tokens/s “45.33”).
  - Chunk size: “32” best (Vicuna-68M “33.52”; EAGLE “49.68”).
  - Top-k: “32” (Vicuna-68M “33.28”; EAGLE “47.21”) or “64” (EAGLE “48.09”).
  - Retrieval frequency: “4” steps (Vicuna-68M “33.59”; EAGLE “48.17”) or “8” (EAGLE “48.52”).
- Newer configuration (Table 6; Llama-3.1-8B-Instruct):
  - EAGLE baseline: 16K tokens/s “23.08”, speedup “1.02×”; with SpecExtend: tokens/s “41.66”, speedup “1.85×”.
  - EAGLE-3 baseline: 4K tokens/s “47.30”, speedup “1.15×”; at 8K speedup “0.96×”; at 16K “0.83×”. With SpecExtend: 4K “89.93” (“2.18×”), 8K “66.52” (“2.08×”), 16K “53.18” (“2.36×”). Authors state EAGLE-3 draft accuracy improves by “up to 2.55×” and overall speedup up to “2.84×”.
- Extremely long inputs (Table 7; PG-19; Llama-3.1-8B-Instruct/EAGLE):
  - 32K: τ “1.73” → “2.73”; tokens/s “8.45” → “23.05”; speedup “2.08×”.
  - 64K: τ “1.72” → “2.71”; tokens/s “8.46” → “22.76”.
  - 128K: τ “1.73” → “2.72”; tokens/s “8.45” → “22.59”.
  - Naive AR runs out of memory beyond 64K; speedups vs AR not reported at 64K or 128K.

# Introduction
- Motivation: Early degradation of speculative decoding performance at moderate lengths is underexplored; retraining draft models on long contexts is costly; drop-in solution should preserve short-input performance.
- Contributions:
  - First training-free solution addressing moderate-length speculative decoding degradation.
  - Cross-model Retrieval: novel KV cache eviction that improves draft accuracy “by up to 2.55×” and speed on long inputs; outperforms static eviction policies.
  - SpecExtend: accelerates speculative decoding “up to 2.84×” on 16K summarization and “up to 3.86×” on long reasoning; preserves short-input performance.

# Related Work
- Speculative decoding: Drafting multiple candidate tokens with a smaller model and parallel verification by the target; guarantees identical output distribution with proper verification/correction. Extensions include tree-based drafting/verification (SpecInfer), draft models from target subsets (Medusa/EAGLE), dynamic draft trees (EAGLE-2, OPT-Tree), and scaled training (EAGLE-3).
- Efficient long-sequence attention: FlashAttention mitigates quadratic memory/latency via tiling/online softmax; FlashDecoding parallelizes across KV length to speed LLM decoding.
- Long sequence speculative methods: Prior works address KV cache bottlenecks in extreme lengths (MagicDec, TriForce), often drafting with the slow base model in moderate regimes, yielding marginal speedups. LongSpec trains draft models specifically for long inputs. SpecExtend differs by offering a training-free drop-in enhancement that improves long-sequence performance while preserving existing framework benefits.

# Discussion
- Conclusion: SpecExtend combines efficient attention with Cross-model Retrieval to accelerate speculative decoding across stages and enhance draft accuracy without retraining. Reported results show “up to 2.84×” speedup in long summarization and “3.86×” in long reasoning, with robustness across input lengths and compatibility with various speculative setups, while preserving short-input performance.

# References
- Cited datasets and benchmarks: GovReport, PG-19, BookSum, AIME-24.
- Key methods/frameworks and techniques: Speculative decoding (Xia et al., Leviathan et al.), SpecInfer, EAGLE/EAGLE-2/EAGLE-3, OPT-Tree, Medusa, MagicDec, TriForce, LongSpec; FlashAttention, FlashDecoding; StreamingLLM; surveys on efficient LLM inference and speculative decoding.
- Newer models: Llama-3.1-8B-Instruct; DeepSeek-R1-Distill-Llama-8B; Vicuna; LongChat; Qwen2.5 technical report mentioned.
- Not specified in this section: Full bibliographic details beyond the cited items above (see manuscript’s reference list).

# Appendix
- A.1 Latency overhead of CMR:
  - Target forward: “53.76 ms”.
  - Target forward with retrieval: “54.11 ms”.
  - Draft forward: “0.84 ms”.
  - Retrieval cache update: “0.34 ms”.
- A.2 Experiment details:
  - EAGLE models for vicuna-7b-v1.5-16k and longchat-7b-16k trained on ShareGPT with default settings using “4 A100 40GB GPUs”.
  - For each input length (1K–16K), “20 inputs” sampled, “run each input twice”, metrics averaged.
  - OPT-Tree dynamic tree expansion settings: “50 total nodes”, “maximum depth 10”, “threshold 0.7”.
  - Retrieval working KV cache sizes and parameters follow Section 5’s optimal settings.
  - License: EAGLE models publicly available under Apache 2.0.