Summary
The paper studies short-horizon causal reasoning framed as strict conjunctive deduction in a Horn-clause subset of first-order logic. The authors construct a SimpleLogic-style dataset with explicit control over reasoning depth and provide two out-of-distribution (OOD) test splits: (i) deeper natural-language (NL) sequences than seen in training, and (ii) a non-natural-language (NNL) variant that replaces lexical items with randomized tokens intended to ablate natural-language cues. They compare zero-/few-shot in-context learning (ICL) performance of several closed-weight decoder-only APIs with fine-tuned encoder-only and encoder–decoder models, plus one fine-tuned open-weight decoder. Across accuracy and discrimination metrics, they report that small decoder-only models struggle with increasing depth and OOD conditions, while fine-tuned encoders/encoder–decoders generalize better to deeper chains and to NNL inputs. Very large decoder-only models can achieve near-perfect performance but with higher latency and compute cost. The paper also proposes a mechanistic interpretability probe based on “curvature similarity” to argue that encoders preserve more stable geometric invariants with increasing reasoning depth, which the authors suggest may underpin their robustness. The work emphasizes practical implications: for short-horizon deductive tasks, fine-tuned encoders/encoder–decoders may be more efficient and reliable than smaller decoder-only models, unless one employs extremely large decoders.

Strengths
- Clear problem framing: the paper crisply defines the target capability as strict conjunctive, multi-hop deductive inference and motivates why this task may interact differently with encoder vs decoder architectures.
- Dataset design with control and stress tests: a depth-stratified corpus with proof chains enables targeted supervision and analysis; the OOD splits include a deeper NL test and an NNL variant that attempts to ablate lexical shortcuts.
- Comprehensive evaluation views: results are broken down by depth and reported with multiple metrics (accuracy, precision/recall/F1, AUROC), including ROC curves and depth-wise performance trends.
- Practical perspective: the comparison includes inference-time and cost considerations, highlighting trade-offs between robustness and efficiency across model families and scales.
- Consistent empirical pattern: fine-tuned encoders/encoder–decoders maintain better discrimination as depth increases and exhibit stronger robustness on the NNL split than smaller decoder-only models; very large decoders can match performance but at higher latency.
- Initial mechanistic link: the attempt to relate behavioral robustness to representational geometry (via curvature similarity) is timely and potentially insightful, aligning with emerging interpretability work.

Weaknesses
- Architecture–training confound: encoders and encoder–decoders are fine-tuned (with supervision that includes proof chains), whereas most decoder-only models are evaluated via zero-/few-shot ICL; only one open-weight decoder is fine-tuned. This conflates architecture with training regime and supervision, weakening causal attribution of performance differences to architecture.
- Failure-handling bias: failed API or parsing calls are defaulted to a negative label after retries. This can skew class distributions and inflate/depress metrics unevenly across models, especially for those with higher failure rates, and compromises fair comparison.
- Tokenization confound on NNL: the NNL split is constructed with randomized tokens “likely unseen by the tokenizers,” but tokenizer types and token lengths differ across models (byte-level vs subword). Without controls or diagnostics, observed brittleness may partly reflect tokenization/preprocessing rather than reasoning capability.
- Unequal prompting/completion budgets and evaluation conditions: differences in maximum tokens, prompting formats, and API vs on-prem execution introduce extraneous variance that can favor or penalize certain models, complicating both accuracy and latency comparisons.
- Statistical rigor gaps: AUROC is incorrectly described as a measure of statistical significance; no confidence intervals, hypothesis tests, or variability estimates are reported. Discrepancies such as AUCs near chance alongside moderate accuracies suggest thresholding and class-imbalance effects that are not explicitly analyzed or calibrated.
- Under-specified mechanistic method: the “curvature similarity” analysis lacks methodological detail (e.g., formal definition of curvature, trajectory construction, layer selection, normalization, similarity metric, and variance across runs), limiting interpretability and reproducibility and reducing the evidentiary weight of the geometric claims.
- Reporting inconsistencies and plausibility issues: several figures/tables contain malformed or approximate values and repeated/mismatched entries; some results (e.g., uniformly perfect performance for a large decoder across depths and splits) appear implausible without additional context or verification, undermining confidence in quantitative claims.
- Speculative architectural attribution: for closed-source models, architectural labels and assumptions are sometimes conjectural. Attributing outcomes to architecture without firm documentation adds interpretive risk.
- Overstated scope of conclusions: claims that ICL “alone is insufficient” are stronger than warranted given the mismatch in training regimes, the limited set of fine-tuned decoders, and the lack of ablations isolating supervision type or model scale.
- Incomplete handling of class balance and thresholding: the paper does not clearly document class distributions per depth/split or decision-threshold selection, creating ambiguity in how accuracy and discrimination metrics should be interpreted relative to imbalance and calibration.
