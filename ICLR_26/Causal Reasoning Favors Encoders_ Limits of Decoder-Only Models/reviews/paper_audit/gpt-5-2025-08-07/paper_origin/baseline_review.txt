Summary
- The paper studies causal reasoning—operationalized as multi-hop, conjunctive first-order logic (FOL) deduction—in different model families (encoder-only, encoder–decoder, decoder-only). It builds a SimpleLogic-derived training set (depths 0–7) and two out-of-distribution (OOD) test sets: a Natural Language (NL) split and a Non-Natural Language (NNL) split with randomized tokens (depths 0–11) (Section 4.1; Figure 2). Models include BERT/BART/Flan-T5 (fine-tuned), Qwen3-1.7B (fine-tuned), and API-based GPT-4.1, Claude Opus 4.1, GPT-5 (Section 4.2; Table 2). Metrics are accuracy, depth-wise precision/recall/F1, and AUROC (Section 4.3). Results show encoder-based models generally maintain better discrimination and depth robustness after fine-tuning (Section 5.2; Figures 4, 8), while large decoder-only GPT-5 excels but with high latency (Section 6.2; Table 1). A mechanistic probe using curvature similarity suggests encoders preserve geometric invariants with depth (Section 7; Figure 5).Strengths
- Bold, well-scoped problem framing on causal reasoning and multi-hop composition
  - The paper clearly defines the demands of causal reasoning (multi-hop composition and strict conjunctive control) and motivates evaluating logical deduction as a proxy (Section 1; Section 3.1). This clear focus improves impact by isolating reasoning-specific behavior from world knowledge (novelty/clarity).
  - The distinction between encoder global projection vs decoder autoregressive aggregation is articulated (Section 3.2), anchoring the hypothesis and guiding the experimental design (technical soundness/impact).
  - The choice to stratify difficulty by reasoning depth emphasizes composition over lexical knowledge (Section 3.1), enhancing the rigor of the evaluation (experimental rigor).- Carefully constructed datasets and OOD splits with depth control and proof annotations
  - Training and test datasets are depth-balanced and cover 0–11 steps; training max depth 7 and OOD test up to 11 (Section 4.1; Figure 2), enabling controlled generalization analysis (experimental rigor/impact).
  - The NNL split ablates lexical cues by randomized character vocabularies (Section 4.1; Figure 2), directly testing reliance on lexical correlations (novelty/impact).
  - Proof chains and failure traces are generated algorithmically (Appendix A.1–A.2), increasing supervision signal and supporting interpretability (technical soundness/clarity).
  - Quality checks (well-formedness, sound labels, bounded depth) are listed (Appendix A.2), strengthening data reliability (experimental rigor).- Broad, multi-family model comparison across encoder-only, encoder–decoder, and decoder-only
  - The study compares BERT/BART/Flan-T5, Qwen variants, GPT-4.1/5, Claude Opus 4.1 (Section 4.2; Table 2), providing a broad view across architectures (impact/clarity).
  - Both zero-/few-shot ICL and fine-tuning regimes are contrasted (Sections 5.1–5.2), allowing conclusions about ICL vs finetuned behavior (technical soundness).
  - Depth-wise breakdowns for multiple metrics (Figures 4, 9–13; Tables 6–13) reveal consistent degradation with depth and architecture-specific patterns (experimental rigor).- Multiple evaluation lenses, including discrimination metrics and depth-wise analyses
  - Accuracy, per-depth precision/recall/F1, and AUROC are reported (Section 4.3; Figures 7–8), offering complementary perspectives beyond point accuracy (experimental rigor/clarity).
  - ROC/AUC plots for finetuned vs non-finetuned models (Figures 7–8; Appendix C) highlight discrimination improvements for encoders (technical soundness).
  - Depth-wise OLS fits quantify degradation slopes (Section 6.1; Figures 30–31), adding quantitative analysis to qualitative trends (experimental rigor).- Ablations on inference efficiency and depth vs accuracy
  - Inference time and “efficiency” (accuracy/hour) are compared across models (Section 6.2; Table 1), surfacing practical trade-offs where encoders are cost-effective (impact).
  - Depth-wise analysis demonstrates sharper deterioration in NNL for some decoders vs steadier declines for encoders (Section 6.1; Figure 4; Figures 30–31), supporting the architectural hypothesis (technical soundness).- Mechanistic interpretability using curvature similarity as a geometric probe
  - Curvature similarity is introduced to test stability of logical update flows (Section 7), a novel angle aligned with recent theory (novelty/impact).
  - BERT maintains higher curvature similarity across depths 6–11 compared to Flan-T5/BART/Qwen (Figure 5), consistent with the global-context hypothesis (technical soundness).
  - The mechanistic results mirror the performance ordering, strengthening the narrative that encoders preserve structural invariants (Section 7; Figure 5) (impact).- Reproducibility considerations and training details
  - Fine-tuning setup (epochs, batch size, learning rate) is reported (Appendix B.3), and learning curves are shown (Appendix B.6; Figure 6; Figure 57), aiding replication (clarity/rigor).
  - Random seeds and canonical graph logs are maintained (Appendix B.4), supporting exact regeneration (experimental rigor).
  - Code/data availability is promised (Section 9–10; footnote link in Section 1), improving transparency (impact).Weaknesses
- Limited statistical rigor and conflation of AUROC with statistical significance
  - The paper states “we utilize [AUROC] as our measure of statistical significance” (Section 4.3), but AUROC is a performance metric, not a statistical test; no confidence intervals, hypothesis tests, or variance estimates are provided (technical soundness/rigor).
  - Figures and tables lack error bars or uncertainty quantification (e.g., Figures 4, 7–8, 30–31; Tables 6–13), making it hard to assess whether differences are statistically meaningful (experimental rigor/impact).
  - Several claimed advantages (e.g., encoder discrimination superiority in NL: BERT-Base AUC 0.759 vs Flan-T5 0.663 and BART 0.623; Section 5.2; Figure 66) are not tested for significance, risking overinterpretation (clarity/impact).- Ambiguities and potential biases in parsing-error handling and label defaults
  - Section 4.3 says failed calls are retried and “default any failed calls to 0”; yet Figure 3 reports a class “-1” for parsing errors, suggesting inconsistent logging vs scoring (clarity/technical soundness).
  - The caption of Figure 3 reports dramatic shifts in “label compliance” (e.g., 1.3%→99.8% for BART-Base in NL), while also noting persistent label skewness, implying pipeline or metric-definition inconsistencies (clarity/rigor).
  - Defaulting failed calls to class 0 can systematically bias accuracy/ROC, especially when parsing errors differ by model/split (Section 4.3; Figure 3), confounding comparisons (experimental rigor/impact).- Fairness and comparability issues in API LLM evaluations and efficiency analysis
  - Table 2 marks key API models with unknown architectures/parameters (?), and the paper assumes decoder-only when undocumented (Section 4.2; Table 2), which risks misclassification (clarity/technical soundness).
  - Completion token budgets differ substantially (128 vs 5,000 for “reasoning models”; Appendix B.2), making the ICL comparison uneven (experimental rigor/impact).
  - Efficiency (accuracy/hour) is computed across heterogeneous hardware and API backends and acknowledged as “estimates at best” (Section 6.2; Table 1), reducing reliability of the cost-effectiveness claims (technical soundness/clarity).- Mechanistic curvature analysis lacks methodological detail and validation
  - Section 7 does not specify how hidden states were extracted (which layers, averaging strategy, per-depth alignment), nor the exact curvature computation pipeline; only a table of similarities is shown (Figure 5) (clarity/technical soundness).
  - No robustness checks (e.g., alternative layers, datasets, or randomization baselines) are provided to validate curvature similarity as an invariant of logical flow (Section 7; Figure 5) (experimental rigor/impact).
  - Claims of “mechanistic evidence” (Section 7; Conclusion) are strong relative to the sparse methods description and single-figure result, risking overclaiming (clarity/novelty).- NNL dataset design introduces confounds beyond lexical ablation
  - The NNL corpus uses random character strings with punctuation (Figure 2), which likely alters tokenization patterns and sequence lengths, as reflected in increased inference time (Section 6.2; Table 1), confounding the target of “lexical ablation” (technical soundness/rigor).
  - No control experiments ensure parity in token count/length distribution between NL and NNL instances, so performance drops can be due to tokenization inefficiency rather than reasoning differences (Section 4.1; Figure 2) (experimental rigor/impact).
  - The NNL vocabulary is entirely unseen, changing both surface form and tokenizer behavior simultaneously—an OOD that mixes multiple factors (Section 4.1), complicating attribution (clarity/technical soundness).- Scope limitations and overgeneralized conclusions about causal reasoning and ICL
  - The dataset targets a subset of FOL with Horn clauses and excludes disjunctions (Section 4.1), limiting coverage of classical causal inference constructs (e.g., interventions, counterfactuals) (impact/novelty).
  - The paper equates causal reasoning with logical deduction without evaluating intervention robustness or graph-level causal operations (Section 3.1), constraining claims about “causal reasoning” (clarity/impact).
  - Conclusions that “ICL alone is insufficient for reliable causal reasoning” (Abstract; Conclusion) overgeneralize from this specific synthetic benchmark; exceptions like GPT-5’s perfect scores (Section 5.1; Table 1) weaken the blanket statement (clarity/impact).Suggestions for Improvement
- Strengthen statistical rigor and avoid conflating metrics with significance
  - Replace “AUROC as a measure of statistical significance” (Section 4.3) with proper statistical tests: report bootstrap confidence intervals for accuracy/AUC and conduct pairwise significance tests across models per depth (e.g., DeLong tests for AUC) with clear visualization (Figures 4, 7–8, 30–31).
  - Add error bars or shaded confidence bands to depth-wise curves (Figures 4, 30–31) and ROC plots (Figures 7–8, 66–67) to show variability.
  - When asserting encoder advantages (e.g., BERT-Base AUC 0.759 vs 0.663/0.623; Section 5.2), include statistical significance results and effect sizes to substantiate claims.- Clarify and de-bias parsing-error handling and labeling pipeline
  - Reconcile the discrepancy between defaulting failed calls to 0 (Section 4.3) and reporting -1 “parsing errors” (Figure 3); explicitly define how -1 is treated in metrics and ensure consistency across all analyses.
  - Report per-model, per-split parsing-error rates and evaluate metrics both with and without failed calls (e.g., filtered analysis vs 0-imputation) to quantify bias (Figure 3).
  - Consider neutral imputation (e.g., excluding failed calls from accuracy and computing separate “format adherence” metrics) or balanced imputation to avoid class-0 bias.- Improve fairness and comparability in API LLM and efficiency evaluations
  - Avoid assuming architectural types when undocumented (Section 4.2; Table 2); instead, report models as “undisclosed architecture” and refrain from architecture-specific conclusions for those entries.
  - Standardize completion token budgets and shot settings across models (Appendix B.2), or report matched-budget results alongside “best effort” settings to isolate ICL vs capacity effects.
  - For efficiency (Section 6.2; Table 1), normalize by input tokens and measured output tokens; provide per-instance latency distributions and, where APIs are used, repeated runs to estimate variance; clearly state that cross-hardware comparisons are not statistically comparable.- Expand methodological detail and validation for curvature-based mechanistic analysis
  - Document the exact curvature computation: layer(s) used, how trajectories are constructed across depths, alignment procedures, normalization, and similarity metric; include ablations across layers and seeds (Section 7; Figure 5).
  - Validate the probe with synthetic controls (e.g., shuffled rule orders, token-level perturbations) and report whether curvature similarity tracks reasoning degradation (cross-check with Figures 4, 30–31).
  - Extend the mechanistic analysis to NNL data and additional encoder–decoder/decoder layers to test robustness; include statistical summaries (means, confidence intervals) per depth.- Reduce confounds in the NNL dataset and isolate lexical effects
  - Match NL and NNL instances on token count and clause structure; constrain NNL tokens to letter-only strings without punctuation to minimize tokenizer fragmentation (Section 4.1; Figure 2).
  - Report token-length distributions and token-type profiles for NL vs NNL to quantify differences; control inference-time comparisons by normalizing for token counts (Section 6.2; Table 1).
  - Add a third OOD variant with permuted natural-language atoms (e.g., synonymized or random word assignments) that preserves tokenization behavior while ablating semantics, enabling cleaner attribution to lexical cues.- Calibrate the scope of claims and broaden causal reasoning coverage
  - Explicitly qualify conclusions to this FOL Horn-clause setting; revise “ICL alone is insufficient” statements (Abstract; Conclusion) to reflect observed exceptions (GPT-5 near-perfect in both splits; Section 5.1; Table 1) and the synthetic nature of the task.
  - Incorporate evaluations that exercise causal notions beyond deduction (e.g., intervention consistency, counterfactual queries, causal graph operations) or cite this as a limitation with plans for future work (Section 3.1; 4.1).
  - Discuss how proof-chain supervision (Appendix B.3) relates to causal planning and whether models learn general proof strategies; consider testing transfer to benchmarks like FOLIO/P-FOLIO (References) to support broader claims.Score
- Overall (10): 6 — The paper presents a well-motivated, multi-architecture study with controlled datasets and insightful depth-wise analyses but has methodological gaps (statistical rigor, parsing-error handling, NNL confounds) that temper the strength of the conclusions (Sections 4.1–4.3; 5.2; 6.1–6.2; Figures 2–5, 7–8; Table 1).
- Novelty (10): 5 — The encoder-vs-decoder comparison on SimpleLogic-style data and the curvature probe are interesting, but core elements (FOL depth control, ICL vs finetune comparisons) build on established paradigms; mechanistic analysis is promising but underdeveloped (Sections 4.1–4.2; 7; Figure 5).
- Technical Quality (10): 5 — Strong dataset construction and multi-metric evaluation are offset by lack of statistical significance testing, parsing-error handling ambiguities, and fairness issues in API comparisons and efficiency analysis (Sections 4.3; 5.1–5.2; 6.2; Figure 3; Table 1).
- Clarity (10): 6 — The paper is generally clear, with comprehensive figures/tables, but some contradictions (e.g., default-to-0 vs -1 parsing errors) and sparse mechanistic-method details hinder interpretability (Section 4.3; Section 7; Figure 5; Figure 3).
- Confidence (5): 4 — Confidence is relatively high due to the extensive reported analyses and anchors, but reliance on API models with undisclosed details and pending code release (“upon publication”) plus noted methodological issues warrant caution (Section 4.2; Table 2; Sections 9–10).