{
  "baseline_review": "Summary\n- The paper studies causal reasoning—operationalized as multi-hop, conjunctive first-order logic (FOL) deduction—in different model families (encoder-only, encoder–decoder, decoder-only). It builds a SimpleLogic-derived training set (depths 0–7) and two out-of-distribution (OOD) test sets: a Natural Language (NL) split and a Non-Natural Language (NNL) split with randomized tokens (depths 0–11) (Section 4.1; Figure 2). Models include BERT/BART/Flan-T5 (fine-tuned), Qwen3-1.7B (fine-tuned), and API-based GPT-4.1, Claude Opus 4.1, GPT-5 (Section 4.2; Table 2). Metrics are accuracy, depth-wise precision/recall/F1, and AUROC (Section 4.3). Results show encoder-based models generally maintain better discrimination and depth robustness after fine-tuning (Section 5.2; Figures 4, 8), while large decoder-only GPT-5 excels but with high latency (Section 6.2; Table 1). A mechanistic probe using curvature similarity suggests encoders preserve geometric invariants with depth (Section 7; Figure 5).Strengths\n- Bold, well-scoped problem framing on causal reasoning and multi-hop composition\n  - The paper clearly defines the demands of causal reasoning (multi-hop composition and strict conjunctive control) and motivates evaluating logical deduction as a proxy (Section 1; Section 3.1). This clear focus improves impact by isolating reasoning-specific behavior from world knowledge (novelty/clarity).\n  - The distinction between encoder global projection vs decoder autoregressive aggregation is articulated (Section 3.2), anchoring the hypothesis and guiding the experimental design (technical soundness/impact).\n  - The choice to stratify difficulty by reasoning depth emphasizes composition over lexical knowledge (Section 3.1), enhancing the rigor of the evaluation (experimental rigor).- Carefully constructed datasets and OOD splits with depth control and proof annotations\n  - Training and test datasets are depth-balanced and cover 0–11 steps; training max depth 7 and OOD test up to 11 (Section 4.1; Figure 2), enabling controlled generalization analysis (experimental rigor/impact).\n  - The NNL split ablates lexical cues by randomized character vocabularies (Section 4.1; Figure 2), directly testing reliance on lexical correlations (novelty/impact).\n  - Proof chains and failure traces are generated algorithmically (Appendix A.1–A.2), increasing supervision signal and supporting interpretability (technical soundness/clarity).\n  - Quality checks (well-formedness, sound labels, bounded depth) are listed (Appendix A.2), strengthening data reliability (experimental rigor).- Broad, multi-family model comparison across encoder-only, encoder–decoder, and decoder-only\n  - The study compares BERT/BART/Flan-T5, Qwen variants, GPT-4.1/5, Claude Opus 4.1 (Section 4.2; Table 2), providing a broad view across architectures (impact/clarity).\n  - Both zero-/few-shot ICL and fine-tuning regimes are contrasted (Sections 5.1–5.2), allowing conclusions about ICL vs finetuned behavior (technical soundness).\n  - Depth-wise breakdowns for multiple metrics (Figures 4, 9–13; Tables 6–13) reveal consistent degradation with depth and architecture-specific patterns (experimental rigor).- Multiple evaluation lenses, including discrimination metrics and depth-wise analyses\n  - Accuracy, per-depth precision/recall/F1, and AUROC are reported (Section 4.3; Figures 7–8), offering complementary perspectives beyond point accuracy (experimental rigor/clarity).\n  - ROC/AUC plots for finetuned vs non-finetuned models (Figures 7–8; Appendix C) highlight discrimination improvements for encoders (technical soundness).\n  - Depth-wise OLS fits quantify degradation slopes (Section 6.1; Figures 30–31), adding quantitative analysis to qualitative trends (experimental rigor).- Ablations on inference efficiency and depth vs accuracy\n  - Inference time and “efficiency” (accuracy/hour) are compared across models (Section 6.2; Table 1), surfacing practical trade-offs where encoders are cost-effective (impact).\n  - Depth-wise analysis demonstrates sharper deterioration in NNL for some decoders vs steadier declines for encoders (Section 6.1; Figure 4; Figures 30–31), supporting the architectural hypothesis (technical soundness).- Mechanistic interpretability using curvature similarity as a geometric probe\n  - Curvature similarity is introduced to test stability of logical update flows (Section 7), a novel angle aligned with recent theory (novelty/impact).\n  - BERT maintains higher curvature similarity across depths 6–11 compared to Flan-T5/BART/Qwen (Figure 5), consistent with the global-context hypothesis (technical soundness).\n  - The mechanistic results mirror the performance ordering, strengthening the narrative that encoders preserve structural invariants (Section 7; Figure 5) (impact).- Reproducibility considerations and training details\n  - Fine-tuning setup (epochs, batch size, learning rate) is reported (Appendix B.3), and learning curves are shown (Appendix B.6; Figure 6; Figure 57), aiding replication (clarity/rigor).\n  - Random seeds and canonical graph logs are maintained (Appendix B.4), supporting exact regeneration (experimental rigor).\n  - Code/data availability is promised (Section 9–10; footnote link in Section 1), improving transparency (impact).Weaknesses\n- Limited statistical rigor and conflation of AUROC with statistical significance\n  - The paper states “we utilize [AUROC] as our measure of statistical significance” (Section 4.3), but AUROC is a performance metric, not a statistical test; no confidence intervals, hypothesis tests, or variance estimates are provided (technical soundness/rigor).\n  - Figures and tables lack error bars or uncertainty quantification (e.g., Figures 4, 7–8, 30–31; Tables 6–13), making it hard to assess whether differences are statistically meaningful (experimental rigor/impact).\n  - Several claimed advantages (e.g., encoder discrimination superiority in NL: BERT-Base AUC 0.759 vs Flan-T5 0.663 and BART 0.623; Section 5.2; Figure 66) are not tested for significance, risking overinterpretation (clarity/impact).- Ambiguities and potential biases in parsing-error handling and label defaults\n  - Section 4.3 says failed calls are retried and “default any failed calls to 0”; yet Figure 3 reports a class “-1” for parsing errors, suggesting inconsistent logging vs scoring (clarity/technical soundness).\n  - The caption of Figure 3 reports dramatic shifts in “label compliance” (e.g., 1.3%→99.8% for BART-Base in NL), while also noting persistent label skewness, implying pipeline or metric-definition inconsistencies (clarity/rigor).\n  - Defaulting failed calls to class 0 can systematically bias accuracy/ROC, especially when parsing errors differ by model/split (Section 4.3; Figure 3), confounding comparisons (experimental rigor/impact).- Fairness and comparability issues in API LLM evaluations and efficiency analysis\n  - Table 2 marks key API models with unknown architectures/parameters (?), and the paper assumes decoder-only when undocumented (Section 4.2; Table 2), which risks misclassification (clarity/technical soundness).\n  - Completion token budgets differ substantially (128 vs 5,000 for “reasoning models”; Appendix B.2), making the ICL comparison uneven (experimental rigor/impact).\n  - Efficiency (accuracy/hour) is computed across heterogeneous hardware and API backends and acknowledged as “estimates at best” (Section 6.2; Table 1), reducing reliability of the cost-effectiveness claims (technical soundness/clarity).- Mechanistic curvature analysis lacks methodological detail and validation\n  - Section 7 does not specify how hidden states were extracted (which layers, averaging strategy, per-depth alignment), nor the exact curvature computation pipeline; only a table of similarities is shown (Figure 5) (clarity/technical soundness).\n  - No robustness checks (e.g., alternative layers, datasets, or randomization baselines) are provided to validate curvature similarity as an invariant of logical flow (Section 7; Figure 5) (experimental rigor/impact).\n  - Claims of “mechanistic evidence” (Section 7; Conclusion) are strong relative to the sparse methods description and single-figure result, risking overclaiming (clarity/novelty).- NNL dataset design introduces confounds beyond lexical ablation\n  - The NNL corpus uses random character strings with punctuation (Figure 2), which likely alters tokenization patterns and sequence lengths, as reflected in increased inference time (Section 6.2; Table 1), confounding the target of “lexical ablation” (technical soundness/rigor).\n  - No control experiments ensure parity in token count/length distribution between NL and NNL instances, so performance drops can be due to tokenization inefficiency rather than reasoning differences (Section 4.1; Figure 2) (experimental rigor/impact).\n  - The NNL vocabulary is entirely unseen, changing both surface form and tokenizer behavior simultaneously—an OOD that mixes multiple factors (Section 4.1), complicating attribution (clarity/technical soundness).- Scope limitations and overgeneralized conclusions about causal reasoning and ICL\n  - The dataset targets a subset of FOL with Horn clauses and excludes disjunctions (Section 4.1), limiting coverage of classical causal inference constructs (e.g., interventions, counterfactuals) (impact/novelty).\n  - The paper equates causal reasoning with logical deduction without evaluating intervention robustness or graph-level causal operations (Section 3.1), constraining claims about “causal reasoning” (clarity/impact).\n  - Conclusions that “ICL alone is insufficient for reliable causal reasoning” (Abstract; Conclusion) overgeneralize from this specific synthetic benchmark; exceptions like GPT-5’s perfect scores (Section 5.1; Table 1) weaken the blanket statement (clarity/impact).Suggestions for Improvement\n- Strengthen statistical rigor and avoid conflating metrics with significance\n  - Replace “AUROC as a measure of statistical significance” (Section 4.3) with proper statistical tests: report bootstrap confidence intervals for accuracy/AUC and conduct pairwise significance tests across models per depth (e.g., DeLong tests for AUC) with clear visualization (Figures 4, 7–8, 30–31).\n  - Add error bars or shaded confidence bands to depth-wise curves (Figures 4, 30–31) and ROC plots (Figures 7–8, 66–67) to show variability.\n  - When asserting encoder advantages (e.g., BERT-Base AUC 0.759 vs 0.663/0.623; Section 5.2), include statistical significance results and effect sizes to substantiate claims.- Clarify and de-bias parsing-error handling and labeling pipeline\n  - Reconcile the discrepancy between defaulting failed calls to 0 (Section 4.3) and reporting -1 “parsing errors” (Figure 3); explicitly define how -1 is treated in metrics and ensure consistency across all analyses.\n  - Report per-model, per-split parsing-error rates and evaluate metrics both with and without failed calls (e.g., filtered analysis vs 0-imputation) to quantify bias (Figure 3).\n  - Consider neutral imputation (e.g., excluding failed calls from accuracy and computing separate “format adherence” metrics) or balanced imputation to avoid class-0 bias.- Improve fairness and comparability in API LLM and efficiency evaluations\n  - Avoid assuming architectural types when undocumented (Section 4.2; Table 2); instead, report models as “undisclosed architecture” and refrain from architecture-specific conclusions for those entries.\n  - Standardize completion token budgets and shot settings across models (Appendix B.2), or report matched-budget results alongside “best effort” settings to isolate ICL vs capacity effects.\n  - For efficiency (Section 6.2; Table 1), normalize by input tokens and measured output tokens; provide per-instance latency distributions and, where APIs are used, repeated runs to estimate variance; clearly state that cross-hardware comparisons are not statistically comparable.- Expand methodological detail and validation for curvature-based mechanistic analysis\n  - Document the exact curvature computation: layer(s) used, how trajectories are constructed across depths, alignment procedures, normalization, and similarity metric; include ablations across layers and seeds (Section 7; Figure 5).\n  - Validate the probe with synthetic controls (e.g., shuffled rule orders, token-level perturbations) and report whether curvature similarity tracks reasoning degradation (cross-check with Figures 4, 30–31).\n  - Extend the mechanistic analysis to NNL data and additional encoder–decoder/decoder layers to test robustness; include statistical summaries (means, confidence intervals) per depth.- Reduce confounds in the NNL dataset and isolate lexical effects\n  - Match NL and NNL instances on token count and clause structure; constrain NNL tokens to letter-only strings without punctuation to minimize tokenizer fragmentation (Section 4.1; Figure 2).\n  - Report token-length distributions and token-type profiles for NL vs NNL to quantify differences; control inference-time comparisons by normalizing for token counts (Section 6.2; Table 1).\n  - Add a third OOD variant with permuted natural-language atoms (e.g., synonymized or random word assignments) that preserves tokenization behavior while ablating semantics, enabling cleaner attribution to lexical cues.- Calibrate the scope of claims and broaden causal reasoning coverage\n  - Explicitly qualify conclusions to this FOL Horn-clause setting; revise “ICL alone is insufficient” statements (Abstract; Conclusion) to reflect observed exceptions (GPT-5 near-perfect in both splits; Section 5.1; Table 1) and the synthetic nature of the task.\n  - Incorporate evaluations that exercise causal notions beyond deduction (e.g., intervention consistency, counterfactual queries, causal graph operations) or cite this as a limitation with plans for future work (Section 3.1; 4.1).\n  - Discuss how proof-chain supervision (Appendix B.3) relates to causal planning and whether models learn general proof strategies; consider testing transfer to benchmarks like FOLIO/P-FOLIO (References) to support broader claims.Score\n- Overall (10): 6 — The paper presents a well-motivated, multi-architecture study with controlled datasets and insightful depth-wise analyses but has methodological gaps (statistical rigor, parsing-error handling, NNL confounds) that temper the strength of the conclusions (Sections 4.1–4.3; 5.2; 6.1–6.2; Figures 2–5, 7–8; Table 1).\n- Novelty (10): 5 — The encoder-vs-decoder comparison on SimpleLogic-style data and the curvature probe are interesting, but core elements (FOL depth control, ICL vs finetune comparisons) build on established paradigms; mechanistic analysis is promising but underdeveloped (Sections 4.1–4.2; 7; Figure 5).\n- Technical Quality (10): 5 — Strong dataset construction and multi-metric evaluation are offset by lack of statistical significance testing, parsing-error handling ambiguities, and fairness issues in API comparisons and efficiency analysis (Sections 4.3; 5.1–5.2; 6.2; Figure 3; Table 1).\n- Clarity (10): 6 — The paper is generally clear, with comprehensive figures/tables, but some contradictions (e.g., default-to-0 vs -1 parsing errors) and sparse mechanistic-method details hinder interpretability (Section 4.3; Section 7; Figure 5; Figure 3).\n- Confidence (5): 4 — Confidence is relatively high due to the extensive reported analyses and anchors, but reliance on API models with undisclosed details and pending code release (“upon publication”) plus noted methodological issues warrant caution (Section 4.2; Table 2; Sections 9–10).",
  "final_review": "Summary\n- The paper investigates multi-hop, conjunctive first-order logic (FOL) deduction as a proxy for causal reasoning across encoder-only, encoder–decoder, and decoder-only model families. It constructs a SimpleLogic-derived training corpus (40,000 samples, depths 0–7) and two OOD test sets: Natural Language (NL) and Non-Natural Language (NNL) with randomized tokens (3,600 samples each, depths 0–11) (Section 4.1; Figure 2). Evaluated models include BERT/BART/Flan-T5/Qwen3-1.7B (fine-tuned) and API-based GPT-4.1, GPT-5, Claude Opus 4.1, Qwen 2.5 (Section 4.2; Table 2). Metrics are accuracy, per-depth precision/recall/F1, and AUROC (Section 4.3). Results show fine-tuned encoders and encoder–decoders generally exhibit stronger discrimination and depth robustness, while GPT-5 attains near-perfect performance with high latency (Sections 5.1–5.2; 6.2; Table 1). A mechanistic probe using curvature similarity suggests encoders better preserve geometric invariants with depth (Section 7; Figure 5).Strengths\n- Bold, well-scoped problem framing on causal reasoning and multi-hop composition\n  - The paper clearly defines causal reasoning requirements (multi-hop composition and strict conjunctive control) and motivates evaluating logical deduction as a proxy (Section 1; Section 3.1), focusing on reasoning-specific behavior independent of world knowledge (novelty/clarity).\n  - The architectural hypothesis distinguishes encoder global projection versus decoder autoregressive aggregation (Section 3.2), guiding experimental design (technical soundness/impact).\n  - Depth stratification emphasizes composition over lexical knowledge (Section 3.1), reinforcing controlled difficulty (experimental rigor).- Carefully constructed datasets and OOD splits with depth control and proof annotations\n  - Training/test sets are depth-balanced and span 0–11 steps; training max depth 7 with OOD test to 11 (Section 4.1; Figure 2), enabling generalization analysis (experimental rigor/impact).\n  - The NNL split ablates lexical cues via randomized character vocabularies (Section 4.1; Figure 2), directly probing reliance on lexical correlations (novelty/impact).\n  - Algorithmic generation of proof chains and failure traces (Appendix A.1–A.2) adds structured supervision and interpretability (technical soundness/clarity).\n  - Quality checks (well-formedness, sound labels, bounded depth) are enumerated (Appendix A.2), strengthening data reliability (experimental rigor).- Broad, multi-family model comparison across encoder-only, encoder–decoder, and decoder-only\n  - The study covers BERT/BART/Flan-T5, Qwen variants, GPT-4.1/5, Claude Opus 4.1 (Section 4.2; Table 2), offering a broad architectural view (impact/clarity).\n  - Zero-/few-shot ICL and fine-tuning regimes are contrasted (Sections 5.1–5.2), informing differences in adaptation mechanisms (technical soundness).\n  - Depth-wise breakdowns across metrics (Figures 4, 9–13; Tables 6–13) reveal systematic degradation with depth and architecture-specific patterns (experimental rigor).- Multiple evaluation lenses, including discrimination metrics and depth-wise analyses\n  - Accuracy, per-depth precision/recall/F1, and AUROC are reported (Section 4.3; Figures 7–8), providing complementary views beyond point accuracy (experimental rigor/clarity).\n  - ROC/AUC plots for finetuned vs non-finetuned models (Figures 7–8; Appendix C) highlight discrimination gains for encoders after fine-tuning (technical soundness).\n  - Depth-wise OLS fits quantify degradation slopes (Section 6.1; Figures 30–31), adding quantitative support to qualitative trends (experimental rigor).- Ablations on inference efficiency and depth vs accuracy\n  - Inference time and an “efficiency” metric (accuracy/hour) are compared (Section 6.2; Table 1), surfacing practical performance–cost trade-offs (impact).\n  - Depth-wise analysis shows sharper deterioration in NNL for some decoders versus steadier declines for encoders (Section 6.1; Figure 4; Figures 30–31), supporting the architectural hypothesis (technical soundness).- Mechanistic interpretability using curvature similarity as a geometric probe\n  - Curvature similarity is introduced as a mechanistic indicator of stable internal logical updates (Section 7), offering a novel angle aligned with recent geometric reasoning frameworks (novelty/impact).\n  - BERT maintains higher curvature similarity across depths 6–11 compared to Flan-T5/BART/Qwen (Figure 5), consistent with the global-context hypothesis (technical soundness).\n  - Mechanistic results mirror performance ordering, linking geometric invariants to empirical behavior (Section 7; Figure 5) (impact).- Reproducibility considerations and training details\n  - Fine-tuning setup (epochs, batch size, learning rate) and loss curves are reported (Appendix B.3, B.6; Figure 6; Figure 57), aiding replication (clarity/rigor).\n  - Random seeds and canonical graph logs are maintained (Appendix B.4), enabling exact regeneration (experimental rigor).\n  - Code/data release is stated (Section 9–10; link in Section 1), improving transparency (impact).Weaknesses\n- Limited statistical rigor and conflation of AUROC with statistical significance\n  - The paper states “we utilize [AUROC] as our measure of statistical significance” (Section 4.3), but AUROC is a performance metric; no confidence intervals, hypothesis tests, or variance estimates are provided (technical soundness/rigor).\n  - Figures/tables lack uncertainty quantification (e.g., Figures 4, 7–8, 30–31; Tables 6–13), impeding assessment of whether differences are statistically meaningful (experimental rigor/impact).\n  - Several asserted advantages (e.g., encoder discrimination superiority: NL AUC BERT-Base 0.759 vs Flan-T5 0.663 and BART 0.623; Figure 66; Section 5.2) are not tested for significance (clarity/impact).\n  - The manuscript does not specify how continuous decision scores were obtained for ROC/AUC when models emit discrete 0/1 JSON outputs (Appendix B.5; Prompt 1; Section 4.3); No direct evidence found in the manuscript describing probability/logit extraction for ROC threshold sweeps (technical soundness/reproducibility).- Ambiguities and potential biases in parsing-error handling and label defaults\n  - Section 4.3 says failed calls are retried and “default any failed calls to 0,” yet Figure 3/its panels include a “-1” parsing-error class, indicating inconsistent logging versus scoring (clarity/technical soundness).\n  - The caption and panels in Figure 3 report dramatic shifts in “label compliance” (e.g., NL: 1.3%→99.8% for BART-Base; NNL: 0%→91.2%), while also showing irregular class distributions; several entries appear malformed or non-summing (e.g., NL Qwen-3 1.7B and NNL panels), undermining reliability (clarity/rigor).\n  - Defaulting failed calls to class 0 can bias accuracy/ROC, especially if parse/error rates differ by model/split (Section 4.3; Figure 3), confounding comparisons (experimental rigor/impact).- Fairness and comparability issues in API LLM evaluations and efficiency analysis\n  - Table 2 marks key API models with undisclosed architectures/parameters (?), and the paper conjectures decoder-only types when undocumented (Section 4.2; Table 2), risking misclassification (clarity/technical soundness).\n  - Completion token budgets differ markedly (128 vs 5,000 for “reasoning models”; Appendix B.2), making ICL comparisons uneven (experimental rigor/impact).\n  - The efficiency metric mixes heterogeneous hardware/API backends and is acknowledged as “estimates at best” (Section 6.2; Table 1); additionally, the textual description (“over an hour to obtain a one-percent accuracy point”) is hard to reconcile with the defined Accuracy/Hour ratio (clarity/technical soundness).\n  - Claims that “all our calls were made through the Azure OpenAI API” (Appendix B.2) conflict with evaluations of non-Azure/API models (e.g., Claude Opus 4.1; Qwen open-weight runs on local GPU; Table 1; Table 2), creating pipeline ambiguity (clarity/reproducibility).- Mechanistic curvature analysis lacks methodological detail and validation\n  - Section 7 does not specify how hidden states were extracted (layers, token aggregation, alignment), nor the full curvature computation pipeline; only similarities are reported (Figure 5) (clarity/technical soundness).\n  - No robustness checks (e.g., alternative layers, datasets, randomization baselines) are provided to validate curvature similarity as an invariant of logical flow (Section 7; Figure 5) (experimental rigor/impact).\n  - Strong wording around “mechanistic evidence” (Section 7; Conclusion) is not commensurate with sparse procedural detail and a single-figure result (clarity/novelty).\n  - Fine-tuning is described as supervised on both proof chain and final label (Appendix B.3), but the integration details (objectives, loss terms, architecture-specific handling—especially for encoder-only classification) are not specified; No direct evidence found in the manuscript explaining how proof-chain supervision affects inference outputs reported in Sections 5.1–5.2 (reproducibility/technical soundness).- NNL dataset design introduces confounds beyond lexical ablation\n  - The NNL corpus uses random character strings with punctuation (Figure 2), likely altering tokenization and sequence lengths, reflected in higher inference times (Section 6.2; Table 1) and potentially confounding lexical ablation (technical soundness/rigor).\n  - No control experiments ensure matched token count/length distributions between NL and NNL instances; performance drops could arise from tokenization inefficiency versus reasoning differences (Section 4.1; Figure 2) (experimental rigor/impact).\n  - The NNL vocabulary is entirely unseen, changing surface form and tokenizer behavior simultaneously—an OOD that mixes multiple factors (Section 4.1), complicating attribution (clarity/technical soundness).- Scope limitations and overgeneralized conclusions about causal reasoning and ICL\n  - The dataset targets a Horn-fragment (no disjunctions) (Section 4.1), while Section 3.2 frames logical programs including clause-level disjunction, creating a conceptual–experimental mismatch (clarity/novelty).\n  - The paper equates causal reasoning with logical deduction without evaluating interventions/counterfactuals or causal graph operations (Section 3.1), limiting claims about “causal reasoning” (clarity/impact).\n  - Conclusions that “ICL alone is insufficient for reliable causal reasoning” (Abstract; Conclusion) generalize from a specific synthetic benchmark; GPT-5’s near-perfect scores (Section 5.1; Table 1; Tables 8, 11) complicate blanket statements (clarity/impact).Suggestions for Improvement\n- Strengthen statistical rigor and avoid conflating metrics with significance\n  - Replace “AUROC as a measure of statistical significance” (Section 4.3) with proper statistical tests: report bootstrap confidence intervals for accuracy/AUC and conduct pairwise significance tests per depth (e.g., DeLong for AUC) with clear visualization (Figures 4, 7–8, 30–31; Tables 6–13).\n  - Add error bars or shaded confidence bands to depth-wise curves (Figures 4, 30–31) and ROC plots (Figures 7–8, 66–67) to show variability.\n  - When asserting encoder advantages (e.g., BERT-Base AUC 0.759 vs 0.663/0.623; Section 5.2; Figure 66), include significance tests and effect sizes.\n  - Explicitly document how ROC/AUC are computed from continuous decision scores (probabilities/logits), or provide calibrated decision scores for models currently emitting discrete 0/1 JSON (Appendix B.5; Section 4.3).- Clarify and de-bias parsing-error handling and labeling pipeline\n  - Reconcile the discrepancy between defaulting failed calls to 0 (Section 4.3) and reporting -1 “parsing errors” (Figure 3); define how -1 is treated in metrics and ensure consistent handling across analyses.\n  - Report per-model, per-split parsing-error rates and evaluate metrics both with and without failed calls (e.g., filtered analysis vs 0-imputation) to quantify bias (Figure 3).\n  - Consider neutral imputation (exclude failed calls from accuracy; add separate “format adherence” metrics) or balanced imputation to avoid class-0 bias; correct malformed/non-summing entries in Figure 3 and ensure all percentages sum to 100%.- Improve fairness and comparability in API LLM and efficiency evaluations\n  - Avoid assuming architectural types when undocumented (Section 4.2; Table 2); report models as “undisclosed architecture” and refrain from architecture-specific conclusions for those entries.\n  - Standardize completion token budgets and shot settings (Appendix B.2), or report matched-budget results alongside “best effort” settings to isolate ICL vs capacity effects.\n  - For efficiency (Section 6.2; Table 1), clarify unit interpretation and align textual descriptions with the Accuracy/Hour ratio; normalize by input/output tokens, provide per-instance latency distributions, and repeat API runs to estimate variance; explicitly reconcile the “Azure-only” claim (Appendix B.2) with non-Azure/API/local runs (Table 1; Table 2) to remove pipeline ambiguity.- Expand methodological detail and validation for curvature-based mechanistic analysis\n  - Document the curvature pipeline: layers used, trajectory construction across depths, alignment/normalization, and similarity metrics; include ablations across layers/seeds (Section 7; Figure 5).\n  - Validate the probe with synthetic controls (e.g., shuffled rule orders, token-level perturbations) and test whether curvature similarity tracks reasoning degradation (Figures 4, 30–31).\n  - Extend the mechanistic analysis to NNL data and additional encoder–decoder/decoder layers; include statistical summaries per depth; specify how proof-chain supervision (Appendix B.3) is implemented (objectives/losses/architecture treatment) and its effect on reported metrics.- Reduce confounds in the NNL dataset and isolate lexical effects\n  - Match NL and NNL instances on token count and clause structure; constrain NNL tokens to minimize tokenizer fragmentation (Section 4.1; Figure 2).\n  - Report token-length distributions and token-type profiles for NL vs NNL; normalize inference-time comparisons for token counts (Section 6.2; Table 1).\n  - Add a third OOD variant with permuted natural-language atoms (e.g., synonymized/random word assignments) preserving tokenizer behavior while ablating semantics, enabling cleaner attribution to lexical cues.- Calibrate the scope of claims and broaden causal reasoning coverage\n  - Align theoretical framing with the evaluated dataset: explicitly note that disjunction is excluded in experiments and revise Section 3.2 accordingly, or add disjunctive cases to the data (Sections 3.2; 4.1).\n  - Qualify conclusions to the Horn-clause setting; revise “ICL alone is insufficient” (Abstract; Conclusion) to reflect observed exceptions (GPT-5 near-perfect; Section 5.1; Tables 8, 11) and the synthetic nature of tasks.\n  - Incorporate evaluations of causal notions beyond deduction (e.g., intervention consistency, counterfactual queries, causal graph operations) or note this as a limitation with concrete future plans (Sections 3.1; 4.1).Score\n- Overall (10): 6 — Well-motivated cross-architecture study with controlled datasets and depth-wise analyses, but methodological gaps (statistics, parsing/error handling, API/efficiency comparability) limit conclusions (Sections 4.1–4.3; 5.2; 6.1–6.2; Figures 2–5, 7–8; Table 1).\n- Novelty (10): 5 — Empirical synthesis (encoder vs decoder) and curvature probe are interesting, yet core elements (FOL depth control, ICL vs fine-tuning) build on established paradigms; mechanistic analysis is underdeveloped (Sections 4.1–4.2; 7; Figure 5).\n- Technical Quality (10): 4 — Dataset construction and multi-metric evaluation are offset by misuse of AUROC as significance, unclear ROC/AUC computation from discrete outputs, parsing-error handling inconsistencies, and fairness issues in API/efficiency comparisons (Section 4.3; Figure 3; Appendix B.2–B.5; Table 1).\n- Clarity (10): 5 — Generally clear with extensive figures/tables, but contradictions (default-to-0 vs -1 parsing errors; “Azure-only” vs mixed pipelines), garbled class distribution entries (Figure 3), and sparse mechanistic/proof-chain details hinder interpretability (Section 4.3; Section 7; Appendix B.2–B.3).\n- Confidence (5): 4 — Confidence is relatively high due to extensive reported analyses and anchors, but undisclosed API details, approximate timing, and noted methodological inconsistencies warrant caution (Section 4.2; Table 2; Sections 6.2; 9–10).",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 6,
        "novelty": 5,
        "technical_quality": 5,
        "clarity": 6,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 6,
        "novelty": 5,
        "technical_quality": 4,
        "clarity": 5,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "Summary\n- The paper investigates multi-hop, conjunctive first-order logic (FOL) deduction as a proxy for causal reasoning across encoder-only, encoder–decoder, and decoder-only model families. It constructs a SimpleLogic-derived training corpus (40,000 samples, depths 0–7) and two OOD test sets: Natural Language (NL) and Non-Natural Language (NNL) with randomized tokens (3,600 samples each, depths 0–11) (Section 4.1; Figure 2). Evaluated models include BERT/BART/Flan-T5/Qwen3-1.7B (fine-tuned) and API-based GPT-4.1, GPT-5, Claude Opus 4.1, Qwen 2.5 (Section 4.2; Table 2). Metrics are accuracy, per-depth precision/recall/F1, and AUROC (Section 4.3). Results show fine-tuned encoders and encoder–decoders generally exhibit stronger discrimination and depth robustness, while GPT-5 attains near-perfect performance with high latency (Sections 5.1–5.2; 6.2; Table 1). A mechanistic probe using curvature similarity suggests encoders better preserve geometric invariants with depth (Section 7; Figure 5).Strengths\n- Bold, well-scoped problem framing on causal reasoning and multi-hop composition\n  - The paper clearly defines causal reasoning requirements (multi-hop composition and strict conjunctive control) and motivates evaluating logical deduction as a proxy (Section 1; Section 3.1), focusing on reasoning-specific behavior independent of world knowledge (novelty/clarity).\n  - The architectural hypothesis distinguishes encoder global projection versus decoder autoregressive aggregation (Section 3.2), guiding experimental design (technical soundness/impact).\n  - Depth stratification emphasizes composition over lexical knowledge (Section 3.1), reinforcing controlled difficulty (experimental rigor).- Carefully constructed datasets and OOD splits with depth control and proof annotations\n  - Training/test sets are depth-balanced and span 0–11 steps; training max depth 7 with OOD test to 11 (Section 4.1; Figure 2), enabling generalization analysis (experimental rigor/impact).\n  - The NNL split ablates lexical cues via randomized character vocabularies (Section 4.1; Figure 2), directly probing reliance on lexical correlations (novelty/impact).\n  - Algorithmic generation of proof chains and failure traces (Appendix A.1–A.2) adds structured supervision and interpretability (technical soundness/clarity).\n  - Quality checks (well-formedness, sound labels, bounded depth) are enumerated (Appendix A.2), strengthening data reliability (experimental rigor).- Broad, multi-family model comparison across encoder-only, encoder–decoder, and decoder-only\n  - The study covers BERT/BART/Flan-T5, Qwen variants, GPT-4.1/5, Claude Opus 4.1 (Section 4.2; Table 2), offering a broad architectural view (impact/clarity).\n  - Zero-/few-shot ICL and fine-tuning regimes are contrasted (Sections 5.1–5.2), informing differences in adaptation mechanisms (technical soundness).\n  - Depth-wise breakdowns across metrics (Figures 4, 9–13; Tables 6–13) reveal systematic degradation with depth and architecture-specific patterns (experimental rigor).- Multiple evaluation lenses, including discrimination metrics and depth-wise analyses\n  - Accuracy, per-depth precision/recall/F1, and AUROC are reported (Section 4.3; Figures 7–8), providing complementary views beyond point accuracy (experimental rigor/clarity).\n  - ROC/AUC plots for finetuned vs non-finetuned models (Figures 7–8; Appendix C) highlight discrimination gains for encoders after fine-tuning (technical soundness).\n  - Depth-wise OLS fits quantify degradation slopes (Section 6.1; Figures 30–31), adding quantitative support to qualitative trends (experimental rigor).- Ablations on inference efficiency and depth vs accuracy\n  - Inference time and an “efficiency” metric (accuracy/hour) are compared (Section 6.2; Table 1), surfacing practical performance–cost trade-offs (impact).\n  - Depth-wise analysis shows sharper deterioration in NNL for some decoders versus steadier declines for encoders (Section 6.1; Figure 4; Figures 30–31), supporting the architectural hypothesis (technical soundness).- Mechanistic interpretability using curvature similarity as a geometric probe\n  - Curvature similarity is introduced as a mechanistic indicator of stable internal logical updates (Section 7), offering a novel angle aligned with recent geometric reasoning frameworks (novelty/impact).\n  - BERT maintains higher curvature similarity across depths 6–11 compared to Flan-T5/BART/Qwen (Figure 5), consistent with the global-context hypothesis (technical soundness).\n  - Mechanistic results mirror performance ordering, linking geometric invariants to empirical behavior (Section 7; Figure 5) (impact).- Reproducibility considerations and training details\n  - Fine-tuning setup (epochs, batch size, learning rate) and loss curves are reported (Appendix B.3, B.6; Figure 6; Figure 57), aiding replication (clarity/rigor).\n  - Random seeds and canonical graph logs are maintained (Appendix B.4), enabling exact regeneration (experimental rigor).\n  - Code/data release is stated (Section 9–10; link in Section 1), improving transparency (impact).Weaknesses\n- Limited statistical rigor and conflation of AUROC with statistical significance\n  - The paper states “we utilize [AUROC] as our measure of statistical significance” (Section 4.3), but AUROC is a performance metric; no confidence intervals, hypothesis tests, or variance estimates are provided (technical soundness/rigor).\n  - Figures/tables lack uncertainty quantification (e.g., Figures 4, 7–8, 30–31; Tables 6–13), impeding assessment of whether differences are statistically meaningful (experimental rigor/impact).\n  - Several asserted advantages (e.g., encoder discrimination superiority: NL AUC BERT-Base 0.759 vs Flan-T5 0.663 and BART 0.623; Figure 66; Section 5.2) are not tested for significance (clarity/impact).\n  - The manuscript does not specify how continuous decision scores were obtained for ROC/AUC when models emit discrete 0/1 JSON outputs (Appendix B.5; Prompt 1; Section 4.3); No direct evidence found in the manuscript describing probability/logit extraction for ROC threshold sweeps (technical soundness/reproducibility).- Ambiguities and potential biases in parsing-error handling and label defaults\n  - Section 4.3 says failed calls are retried and “default any failed calls to 0,” yet Figure 3/its panels include a “-1” parsing-error class, indicating inconsistent logging versus scoring (clarity/technical soundness).\n  - The caption and panels in Figure 3 report dramatic shifts in “label compliance” (e.g., NL: 1.3%→99.8% for BART-Base; NNL: 0%→91.2%), while also showing irregular class distributions; several entries appear malformed or non-summing (e.g., NL Qwen-3 1.7B and NNL panels), undermining reliability (clarity/rigor).\n  - Defaulting failed calls to class 0 can bias accuracy/ROC, especially if parse/error rates differ by model/split (Section 4.3; Figure 3), confounding comparisons (experimental rigor/impact).- Fairness and comparability issues in API LLM evaluations and efficiency analysis\n  - Table 2 marks key API models with undisclosed architectures/parameters (?), and the paper conjectures decoder-only types when undocumented (Section 4.2; Table 2), risking misclassification (clarity/technical soundness).\n  - Completion token budgets differ markedly (128 vs 5,000 for “reasoning models”; Appendix B.2), making ICL comparisons uneven (experimental rigor/impact).\n  - The efficiency metric mixes heterogeneous hardware/API backends and is acknowledged as “estimates at best” (Section 6.2; Table 1); additionally, the textual description (“over an hour to obtain a one-percent accuracy point”) is hard to reconcile with the defined Accuracy/Hour ratio (clarity/technical soundness).\n  - Claims that “all our calls were made through the Azure OpenAI API” (Appendix B.2) conflict with evaluations of non-Azure/API models (e.g., Claude Opus 4.1; Qwen open-weight runs on local GPU; Table 1; Table 2), creating pipeline ambiguity (clarity/reproducibility).- Mechanistic curvature analysis lacks methodological detail and validation\n  - Section 7 does not specify how hidden states were extracted (layers, token aggregation, alignment), nor the full curvature computation pipeline; only similarities are reported (Figure 5) (clarity/technical soundness).\n  - No robustness checks (e.g., alternative layers, datasets, randomization baselines) are provided to validate curvature similarity as an invariant of logical flow (Section 7; Figure 5) (experimental rigor/impact).\n  - Strong wording around “mechanistic evidence” (Section 7; Conclusion) is not commensurate with sparse procedural detail and a single-figure result (clarity/novelty).\n  - Fine-tuning is described as supervised on both proof chain and final label (Appendix B.3), but the integration details (objectives, loss terms, architecture-specific handling—especially for encoder-only classification) are not specified; No direct evidence found in the manuscript explaining how proof-chain supervision affects inference outputs reported in Sections 5.1–5.2 (reproducibility/technical soundness).- NNL dataset design introduces confounds beyond lexical ablation\n  - The NNL corpus uses random character strings with punctuation (Figure 2), likely altering tokenization and sequence lengths, reflected in higher inference times (Section 6.2; Table 1) and potentially confounding lexical ablation (technical soundness/rigor).\n  - No control experiments ensure matched token count/length distributions between NL and NNL instances; performance drops could arise from tokenization inefficiency versus reasoning differences (Section 4.1; Figure 2) (experimental rigor/impact).\n  - The NNL vocabulary is entirely unseen, changing surface form and tokenizer behavior simultaneously—an OOD that mixes multiple factors (Section 4.1), complicating attribution (clarity/technical soundness).- Scope limitations and overgeneralized conclusions about causal reasoning and ICL\n  - The dataset targets a Horn-fragment (no disjunctions) (Section 4.1), while Section 3.2 frames logical programs including clause-level disjunction, creating a conceptual–experimental mismatch (clarity/novelty).\n  - The paper equates causal reasoning with logical deduction without evaluating interventions/counterfactuals or causal graph operations (Section 3.1), limiting claims about “causal reasoning” (clarity/impact).\n  - Conclusions that “ICL alone is insufficient for reliable causal reasoning” (Abstract; Conclusion) generalize from a specific synthetic benchmark; GPT-5’s near-perfect scores (Section 5.1; Table 1; Tables 8, 11) complicate blanket statements (clarity/impact).Suggestions for Improvement\n- Strengthen statistical rigor and avoid conflating metrics with significance\n  - Replace “AUROC as a measure of statistical significance” (Section 4.3) with proper statistical tests: report bootstrap confidence intervals for accuracy/AUC and conduct pairwise significance tests per depth (e.g., DeLong for AUC) with clear visualization (Figures 4, 7–8, 30–31; Tables 6–13).\n  - Add error bars or shaded confidence bands to depth-wise curves (Figures 4, 30–31) and ROC plots (Figures 7–8, 66–67) to show variability.\n  - When asserting encoder advantages (e.g., BERT-Base AUC 0.759 vs 0.663/0.623; Section 5.2; Figure 66), include significance tests and effect sizes.\n  - Explicitly document how ROC/AUC are computed from continuous decision scores (probabilities/logits), or provide calibrated decision scores for models currently emitting discrete 0/1 JSON (Appendix B.5; Section 4.3).- Clarify and de-bias parsing-error handling and labeling pipeline\n  - Reconcile the discrepancy between defaulting failed calls to 0 (Section 4.3) and reporting -1 “parsing errors” (Figure 3); define how -1 is treated in metrics and ensure consistent handling across analyses.\n  - Report per-model, per-split parsing-error rates and evaluate metrics both with and without failed calls (e.g., filtered analysis vs 0-imputation) to quantify bias (Figure 3).\n  - Consider neutral imputation (exclude failed calls from accuracy; add separate “format adherence” metrics) or balanced imputation to avoid class-0 bias; correct malformed/non-summing entries in Figure 3 and ensure all percentages sum to 100%.- Improve fairness and comparability in API LLM and efficiency evaluations\n  - Avoid assuming architectural types when undocumented (Section 4.2; Table 2); report models as “undisclosed architecture” and refrain from architecture-specific conclusions for those entries.\n  - Standardize completion token budgets and shot settings (Appendix B.2), or report matched-budget results alongside “best effort” settings to isolate ICL vs capacity effects.\n  - For efficiency (Section 6.2; Table 1), clarify unit interpretation and align textual descriptions with the Accuracy/Hour ratio; normalize by input/output tokens, provide per-instance latency distributions, and repeat API runs to estimate variance; explicitly reconcile the “Azure-only” claim (Appendix B.2) with non-Azure/API/local runs (Table 1; Table 2) to remove pipeline ambiguity.- Expand methodological detail and validation for curvature-based mechanistic analysis\n  - Document the curvature pipeline: layers used, trajectory construction across depths, alignment/normalization, and similarity metrics; include ablations across layers/seeds (Section 7; Figure 5).\n  - Validate the probe with synthetic controls (e.g., shuffled rule orders, token-level perturbations) and test whether curvature similarity tracks reasoning degradation (Figures 4, 30–31).\n  - Extend the mechanistic analysis to NNL data and additional encoder–decoder/decoder layers; include statistical summaries per depth; specify how proof-chain supervision (Appendix B.3) is implemented (objectives/losses/architecture treatment) and its effect on reported metrics.- Reduce confounds in the NNL dataset and isolate lexical effects\n  - Match NL and NNL instances on token count and clause structure; constrain NNL tokens to minimize tokenizer fragmentation (Section 4.1; Figure 2).\n  - Report token-length distributions and token-type profiles for NL vs NNL; normalize inference-time comparisons for token counts (Section 6.2; Table 1).\n  - Add a third OOD variant with permuted natural-language atoms (e.g., synonymized/random word assignments) preserving tokenizer behavior while ablating semantics, enabling cleaner attribution to lexical cues.- Calibrate the scope of claims and broaden causal reasoning coverage\n  - Align theoretical framing with the evaluated dataset: explicitly note that disjunction is excluded in experiments and revise Section 3.2 accordingly, or add disjunctive cases to the data (Sections 3.2; 4.1).\n  - Qualify conclusions to the Horn-clause setting; revise “ICL alone is insufficient” (Abstract; Conclusion) to reflect observed exceptions (GPT-5 near-perfect; Section 5.1; Tables 8, 11) and the synthetic nature of tasks.\n  - Incorporate evaluations of causal notions beyond deduction (e.g., intervention consistency, counterfactual queries, causal graph operations) or note this as a limitation with concrete future plans (Sections 3.1; 4.1).Score\n- Overall (10): 6 — Well-motivated cross-architecture study with controlled datasets and depth-wise analyses, but methodological gaps (statistics, parsing/error handling, API/efficiency comparability) limit conclusions (Sections 4.1–4.3; 5.2; 6.1–6.2; Figures 2–5, 7–8; Table 1).\n- Novelty (10): 5 — Empirical synthesis (encoder vs decoder) and curvature probe are interesting, yet core elements (FOL depth control, ICL vs fine-tuning) build on established paradigms; mechanistic analysis is underdeveloped (Sections 4.1–4.2; 7; Figure 5).\n- Technical Quality (10): 4 — Dataset construction and multi-metric evaluation are offset by misuse of AUROC as significance, unclear ROC/AUC computation from discrete outputs, parsing-error handling inconsistencies, and fairness issues in API/efficiency comparisons (Section 4.3; Figure 3; Appendix B.2–B.5; Table 1).\n- Clarity (10): 5 — Generally clear with extensive figures/tables, but contradictions (default-to-0 vs -1 parsing errors; “Azure-only” vs mixed pipelines), garbled class distribution entries (Figure 3), and sparse mechanistic/proof-chain details hinder interpretability (Section 4.3; Section 7; Appendix B.2–B.3).\n- Confidence (5): 4 — Confidence is relatively high due to extensive reported analyses and anchors, but undisclosed API details, approximate timing, and noted methodological inconsistencies warrant caution (Section 4.2; Table 2; Sections 6.2; 9–10)."
}