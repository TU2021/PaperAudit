Academic Integrity and Consistency Risk Report

Scope: This report flags high-impact, evidence-based inconsistencies, contradictions, and missing details that materially affect the scientific correctness and trustworthiness of the manuscript. Anchors to the manuscript are provided for each item.

1) Concept–dataset mismatch on disjunction
- Evidence:
  - Section 3.2 (Block #14) introduces logical programs of the form “(literals) ⇒ (clause-level disjunction) ⇒ (global conjunction)” and frames the work around multi-hop reasoning that includes disjunction (e.g., “X ∨ Y”).
  - Methods 4.1 (Block #16) explicitly state the benchmark (SimpleLogic) is a subset of FOL that excludes disjunctions, using only Horn-style rules with [AND] and [IMPLY].
- Issue: The theoretical premise emphasizes testing clause-level disjunction, but the experimental dataset excludes disjunction. This is a clear internal inconsistency between the claims about what is being tested and what is actually evaluated.
- Impact: Undermines the central claim about evaluating strict conjunctive control in the presence of disjunction; conclusions about handling disjunction are not supported by the dataset used.

2) Prompting, parsing, and API-use inconsistencies
- Evidence:
  - Evaluation pipeline: Section 4.3 (Block #19) says failed calls are defaulted to 0. Yet Figure 3’s class distributions (Block #21) include a “-1” bin labeled as parsing errors with very high percentages (e.g., BART-Base: -1 = 100.0% on NNL; Flan-T5-Base: -1 = 100.0% on NNL; BERT-Base: -1 = 82.3% on NNL). This contradicts the stated default-to-0 policy for failed calls.
  - Attribution of failures: Prompt 1 note (Block #50) states “the parse failures mostly stemmed from API errors.” However, the models with dominant “-1” rates include BERT and BART, which are local models in this study (non-API), and were finetuned locally (Section B.3, Block #51; Table 1, Block #32).
  - API claim: Section B.2 (Block #48) states “All our calls were made through the Azure OpenAI API.” This is incompatible with the reported use of Claude Opus 4.1 (Anthropic, Block #40), and open-weight Qwen models (Block #17, Block #47, Table 2 in Block #49), which are not accessed via Azure OpenAI. Table 1 (Block #32) also lists Qwen2 as “API” and multiple local GPU runs, contradicting the “all calls via Azure” statement.
- Issue: Contradictory handling of failed/parsing outputs and inconsistent reporting of API versus local execution pipelines.
- Impact: Reliability of the reported class distributions and error handling is unclear; reproducibility and fairness of comparisons (API vs local) are compromised.

3) Invalid or corrupted class distribution numbers
- Evidence:
  - Figure 3 tables (Block #21) show percentages that do not sum to 100%, and include garbled entries. Examples:
    - “BART-Base: -1: 98.5%, 0: 0.0%, 1: 0.8%” (sums to 99.3%).
    - “Qwen-3 1.7B: -1: 0.0%, 0: 4.1%, 1: 100.0%” (sums to 104.1%).
    - “Qwen-3 1.7B: 0: 44.0%, 1: 63.2%, 70.5%, 46.4%” (extra unlabeled numbers).
- Issue: Numerical inconsistencies and malformed entries in a core figure.
- Impact: Calls into question the accuracy and integrity of class distribution analyses; any downstream interpretations relying on these distributions are unreliable.

4) Incomplete methodology for non-finetuned encoders and ROC/AUC computation
- Evidence:
  - Prompting: Section 4.3 (Block #19) references “how outputs are requested for each model family,” and Section B.5 (Block #53) provides a JSON-formatted prompt solely for LLMs. No method is described for how non-generative encoders (e.g., BERT) produced zero-shot predictions in 5.1 (Block #21). No direct evidence found in the manuscript on how BERT/BART/Flan-T5 were prompted without fine-tuning to emit 0/1 JSON, or how parsing applies to them.
  - ROC/AUC: Evaluation metrics (Block #18) rely on AUROC and ROC curves. With discrete 0/1 outputs (Prompt 1, Block #50), the manuscript does not describe how continuous decision scores were obtained for ROC calculation (e.g., probabilities, logits, confidence scores). No direct evidence found in the manuscript detailing how ROC thresholds were swept for models that emit hard labels only.
- Issue: Critical methodological gaps for obtaining zero-shot outputs from encoders and computing ROC/AUC from models that are described as outputting discrete labels.
- Impact: AUROC results may be invalid or irreproducible; comparisons across architectures may not be methodologically sound.

5) Misuse of AUROC as “measure of statistical significance”
- Evidence:
  - Section 4.3 (Block #18): “We utilize [AUROC] as our measure of statistical significance.”
- Issue: AUROC is a performance metric, not a statistical significance test. No confidence intervals, hypothesis tests, or variance analyses are provided.
- Impact: The manuscript’s claims about significance are not substantiated; the inferential rigor of conclusions is weakened.

6) Fine-tuning with proof-chain supervision: missing technical details
- Evidence:
  - Section B.3 (Block #51): “We have finetuned explicitly with both components: the reasoning path (proof chain) and the final prediction… supervised not only on the direct label but also on the intermediate reasoning steps.”
- Issue: The manuscript does not specify architecture modifications, loss functions, sequence-to-graph mapping, or how chain supervision is integrated for each model family (especially for encoder-only BERT). No direct evidence found in the manuscript clarifying how models generated or consumed proof chains during training/inference, nor how chain supervision affects outputs reported in Sections 5.1–5.2.
- Impact: Reproducibility and validity of the claimed “mechanistic” improvement are unclear; results attributing robustness to chain supervision cannot be independently verified.

7) Reporting inconsistency on shot settings
- Evidence:
  - Section 5.1 (Block #21): “we observed marginal changes (+0.5% average)… thus we only report zero-shot.” Immediately afterward, the text reports five-shot figures for Claude Opus 4.1 and distinguishes zero vs five-shot accuracies (“65% and 66% in the NNL dataset (zero and five-shot, respectively)”).
  - Figures in Appendix (e.g., Figure 9, Block #64; Figures 68–69, Blocks #68–#69) present both zero- and five-shot curves after stating only zero-shot would be reported.
- Issue: The manuscript states only zero-shot results are reported, but proceeds to mix and present both zero- and five-shot results.
- Impact: Confusing and inconsistent reporting that hinders clear interpretation of the experimental comparisons.

8) Efficiency metric and cross-hardware comparisons
- Evidence:
  - Table 1 (Block #32) defines “Efficiency (Accuracy/Hour)” with large values (e.g., 640 for BART-Base). The narrative states GPT-5 had efficiency of 1.1 “over an hour to obtain a one-percent accuracy point.” The table mixes local GPU runs with API calls and notes estimates and unknown hardware.
- Issue: The efficiency metric is non-standard, unit interpretation is unclear (percent vs fraction), and cross-hardware/API comparisons are acknowledged as “not fully comparable.” The textual explanation (“over an hour per one-percent point”) does not clearly align with the defined metric.
- Impact: Conclusions about model efficiency and cost-effectiveness are not well-founded; cross-system comparisons may be misleading.

9) Mechanistic curvature similarity lacks procedural detail
- Evidence:
  - Section 7 and Figure 5 (Blocks #33–#34; #36): Curvature similarity values are reported for BERT, Flan-T5, BART, and Qwen across depths 6–11 on the NL dataset.
- Issue: No description is provided on how hidden representations were extracted (which layers, how token-level states were aggregated), how curvature was computed, how similarity was defined, or how the pipeline was applied to closed-source/API models versus local models. No direct evidence found in the manuscript describing the concrete computation steps.
- Impact: The mechanistic claims cannot be reproduced and their validity cannot be independently assessed.

10) Discrepancies in class balance and perfect metrics at depth 0
- Evidence:
  - Prompt optimization results (Table 5, Block #56) show for depth 0 on both NL and NNL: accuracy = 100.0, recall(Label 0) = 0.00, recall(Label 1) = 100.0.
- Issue: Perfect accuracy with zero recall for Label 0 implies the dataset at depth 0 may contain only Label 1 samples, but the manuscript does not document per-depth label distributions. This clashes with the design that introduces negatives and balanced curricula (Appendix A.2, Block #45).
- Impact: Without disclosure of per-depth class balance, per-depth metrics (especially perfect scores) are difficult to interpret and may overstate performance at shallow depths.

Summary Assessment
The manuscript contains multiple high-impact inconsistencies and missing methodological details that affect the validity and trustworthiness of its conclusions, notably:
- Conceptual–experimental mismatch on disjunction (Sections 3.2 vs 4.1).
- Contradictions and corrupted numbers in class distribution reporting (Figure 3).
- Unclear/contradictory handling of parsing failures and API/local execution.
- Methodological gaps for zero-shot encoders and ROC/AUC computation from hard labels.
- Misuse of AUROC as “statistical significance.”
- Missing details on proof-chain supervision and curvature computation.

Recommendations for Remediation
- Align theoretical framing with the dataset capable of testing those claims (either include disjunction or revise claims).
- Correct and fully document the parsing/error handling policy; remove “-1” bins if failed calls are defaulted to 0, and ensure all percentages sum to 100%.
- Provide precise, model-family-specific methods for producing outputs (especially for non-generative encoders) and for obtaining continuous scores for ROC/AUC.
- Replace AUROC-as-significance with appropriate statistical tests or provide confidence intervals/bootstraps.
- Detail the proof-chain training pipeline (objectives, loss terms, architecture changes) and the curvature similarity computation procedure.
- Cleanly separate and report zero- vs few-shot results as stated; avoid mixing settings within the same summary claims.
- Clarify efficiency metric units and avoid cross-hardware/API comparisons unless normalized.

Given the breadth and severity of the above issues, the current manuscript’s integrity and reproducibility are substantially compromised.