Summary
- The paper investigates multi-hop, conjunctive first-order logic (FOL) deduction as a proxy for causal reasoning across encoder-only, encoder–decoder, and decoder-only model families. It constructs a SimpleLogic-derived training corpus (40,000 samples, depths 0–7) and two OOD test sets: Natural Language (NL) and Non-Natural Language (NNL) with randomized tokens (3,600 samples each, depths 0–11) (Section 4.1; Figure 2). Evaluated models include BERT/BART/Flan-T5/Qwen3-1.7B (fine-tuned) and API-based GPT-4.1, GPT-5, Claude Opus 4.1, Qwen 2.5 (Section 4.2; Table 2). Metrics are accuracy, per-depth precision/recall/F1, and AUROC (Section 4.3). Results show fine-tuned encoders and encoder–decoders generally exhibit stronger discrimination and depth robustness, while GPT-5 attains near-perfect performance with high latency (Sections 5.1–5.2; 6.2; Table 1). A mechanistic probe using curvature similarity suggests encoders better preserve geometric invariants with depth (Section 7; Figure 5).Strengths
- Bold, well-scoped problem framing on causal reasoning and multi-hop composition
  - The paper clearly defines causal reasoning requirements (multi-hop composition and strict conjunctive control) and motivates evaluating logical deduction as a proxy (Section 1; Section 3.1), focusing on reasoning-specific behavior independent of world knowledge (novelty/clarity).
  - The architectural hypothesis distinguishes encoder global projection versus decoder autoregressive aggregation (Section 3.2), guiding experimental design (technical soundness/impact).
  - Depth stratification emphasizes composition over lexical knowledge (Section 3.1), reinforcing controlled difficulty (experimental rigor).- Carefully constructed datasets and OOD splits with depth control and proof annotations
  - Training/test sets are depth-balanced and span 0–11 steps; training max depth 7 with OOD test to 11 (Section 4.1; Figure 2), enabling generalization analysis (experimental rigor/impact).
  - The NNL split ablates lexical cues via randomized character vocabularies (Section 4.1; Figure 2), directly probing reliance on lexical correlations (novelty/impact).
  - Algorithmic generation of proof chains and failure traces (Appendix A.1–A.2) adds structured supervision and interpretability (technical soundness/clarity).
  - Quality checks (well-formedness, sound labels, bounded depth) are enumerated (Appendix A.2), strengthening data reliability (experimental rigor).- Broad, multi-family model comparison across encoder-only, encoder–decoder, and decoder-only
  - The study covers BERT/BART/Flan-T5, Qwen variants, GPT-4.1/5, Claude Opus 4.1 (Section 4.2; Table 2), offering a broad architectural view (impact/clarity).
  - Zero-/few-shot ICL and fine-tuning regimes are contrasted (Sections 5.1–5.2), informing differences in adaptation mechanisms (technical soundness).
  - Depth-wise breakdowns across metrics (Figures 4, 9–13; Tables 6–13) reveal systematic degradation with depth and architecture-specific patterns (experimental rigor).- Multiple evaluation lenses, including discrimination metrics and depth-wise analyses
  - Accuracy, per-depth precision/recall/F1, and AUROC are reported (Section 4.3; Figures 7–8), providing complementary views beyond point accuracy (experimental rigor/clarity).
  - ROC/AUC plots for finetuned vs non-finetuned models (Figures 7–8; Appendix C) highlight discrimination gains for encoders after fine-tuning (technical soundness).
  - Depth-wise OLS fits quantify degradation slopes (Section 6.1; Figures 30–31), adding quantitative support to qualitative trends (experimental rigor).- Ablations on inference efficiency and depth vs accuracy
  - Inference time and an “efficiency” metric (accuracy/hour) are compared (Section 6.2; Table 1), surfacing practical performance–cost trade-offs (impact).
  - Depth-wise analysis shows sharper deterioration in NNL for some decoders versus steadier declines for encoders (Section 6.1; Figure 4; Figures 30–31), supporting the architectural hypothesis (technical soundness).- Mechanistic interpretability using curvature similarity as a geometric probe
  - Curvature similarity is introduced as a mechanistic indicator of stable internal logical updates (Section 7), offering a novel angle aligned with recent geometric reasoning frameworks (novelty/impact).
  - BERT maintains higher curvature similarity across depths 6–11 compared to Flan-T5/BART/Qwen (Figure 5), consistent with the global-context hypothesis (technical soundness).
  - Mechanistic results mirror performance ordering, linking geometric invariants to empirical behavior (Section 7; Figure 5) (impact).- Reproducibility considerations and training details
  - Fine-tuning setup (epochs, batch size, learning rate) and loss curves are reported (Appendix B.3, B.6; Figure 6; Figure 57), aiding replication (clarity/rigor).
  - Random seeds and canonical graph logs are maintained (Appendix B.4), enabling exact regeneration (experimental rigor).
  - Code/data release is stated (Section 9–10; link in Section 1), improving transparency (impact).Weaknesses
- Limited statistical rigor and conflation of AUROC with statistical significance
  - The paper states “we utilize [AUROC] as our measure of statistical significance” (Section 4.3), but AUROC is a performance metric; no confidence intervals, hypothesis tests, or variance estimates are provided (technical soundness/rigor).
  - Figures/tables lack uncertainty quantification (e.g., Figures 4, 7–8, 30–31; Tables 6–13), impeding assessment of whether differences are statistically meaningful (experimental rigor/impact).
  - Several asserted advantages (e.g., encoder discrimination superiority: NL AUC BERT-Base 0.759 vs Flan-T5 0.663 and BART 0.623; Figure 66; Section 5.2) are not tested for significance (clarity/impact).
  - The manuscript does not specify how continuous decision scores were obtained for ROC/AUC when models emit discrete 0/1 JSON outputs (Appendix B.5; Prompt 1; Section 4.3); No direct evidence found in the manuscript describing probability/logit extraction for ROC threshold sweeps (technical soundness/reproducibility).- Ambiguities and potential biases in parsing-error handling and label defaults
  - Section 4.3 says failed calls are retried and “default any failed calls to 0,” yet Figure 3/its panels include a “-1” parsing-error class, indicating inconsistent logging versus scoring (clarity/technical soundness).
  - The caption and panels in Figure 3 report dramatic shifts in “label compliance” (e.g., NL: 1.3%→99.8% for BART-Base; NNL: 0%→91.2%), while also showing irregular class distributions; several entries appear malformed or non-summing (e.g., NL Qwen-3 1.7B and NNL panels), undermining reliability (clarity/rigor).
  - Defaulting failed calls to class 0 can bias accuracy/ROC, especially if parse/error rates differ by model/split (Section 4.3; Figure 3), confounding comparisons (experimental rigor/impact).- Fairness and comparability issues in API LLM evaluations and efficiency analysis
  - Table 2 marks key API models with undisclosed architectures/parameters (?), and the paper conjectures decoder-only types when undocumented (Section 4.2; Table 2), risking misclassification (clarity/technical soundness).
  - Completion token budgets differ markedly (128 vs 5,000 for “reasoning models”; Appendix B.2), making ICL comparisons uneven (experimental rigor/impact).
  - The efficiency metric mixes heterogeneous hardware/API backends and is acknowledged as “estimates at best” (Section 6.2; Table 1); additionally, the textual description (“over an hour to obtain a one-percent accuracy point”) is hard to reconcile with the defined Accuracy/Hour ratio (clarity/technical soundness).
  - Claims that “all our calls were made through the Azure OpenAI API” (Appendix B.2) conflict with evaluations of non-Azure/API models (e.g., Claude Opus 4.1; Qwen open-weight runs on local GPU; Table 1; Table 2), creating pipeline ambiguity (clarity/reproducibility).- Mechanistic curvature analysis lacks methodological detail and validation
  - Section 7 does not specify how hidden states were extracted (layers, token aggregation, alignment), nor the full curvature computation pipeline; only similarities are reported (Figure 5) (clarity/technical soundness).
  - No robustness checks (e.g., alternative layers, datasets, randomization baselines) are provided to validate curvature similarity as an invariant of logical flow (Section 7; Figure 5) (experimental rigor/impact).
  - Strong wording around “mechanistic evidence” (Section 7; Conclusion) is not commensurate with sparse procedural detail and a single-figure result (clarity/novelty).
  - Fine-tuning is described as supervised on both proof chain and final label (Appendix B.3), but the integration details (objectives, loss terms, architecture-specific handling—especially for encoder-only classification) are not specified; No direct evidence found in the manuscript explaining how proof-chain supervision affects inference outputs reported in Sections 5.1–5.2 (reproducibility/technical soundness).- NNL dataset design introduces confounds beyond lexical ablation
  - The NNL corpus uses random character strings with punctuation (Figure 2), likely altering tokenization and sequence lengths, reflected in higher inference times (Section 6.2; Table 1) and potentially confounding lexical ablation (technical soundness/rigor).
  - No control experiments ensure matched token count/length distributions between NL and NNL instances; performance drops could arise from tokenization inefficiency versus reasoning differences (Section 4.1; Figure 2) (experimental rigor/impact).
  - The NNL vocabulary is entirely unseen, changing surface form and tokenizer behavior simultaneously—an OOD that mixes multiple factors (Section 4.1), complicating attribution (clarity/technical soundness).- Scope limitations and overgeneralized conclusions about causal reasoning and ICL
  - The dataset targets a Horn-fragment (no disjunctions) (Section 4.1), while Section 3.2 frames logical programs including clause-level disjunction, creating a conceptual–experimental mismatch (clarity/novelty).
  - The paper equates causal reasoning with logical deduction without evaluating interventions/counterfactuals or causal graph operations (Section 3.1), limiting claims about “causal reasoning” (clarity/impact).
  - Conclusions that “ICL alone is insufficient for reliable causal reasoning” (Abstract; Conclusion) generalize from a specific synthetic benchmark; GPT-5’s near-perfect scores (Section 5.1; Table 1; Tables 8, 11) complicate blanket statements (clarity/impact).Suggestions for Improvement
- Strengthen statistical rigor and avoid conflating metrics with significance
  - Replace “AUROC as a measure of statistical significance” (Section 4.3) with proper statistical tests: report bootstrap confidence intervals for accuracy/AUC and conduct pairwise significance tests per depth (e.g., DeLong for AUC) with clear visualization (Figures 4, 7–8, 30–31; Tables 6–13).
  - Add error bars or shaded confidence bands to depth-wise curves (Figures 4, 30–31) and ROC plots (Figures 7–8, 66–67) to show variability.
  - When asserting encoder advantages (e.g., BERT-Base AUC 0.759 vs 0.663/0.623; Section 5.2; Figure 66), include significance tests and effect sizes.
  - Explicitly document how ROC/AUC are computed from continuous decision scores (probabilities/logits), or provide calibrated decision scores for models currently emitting discrete 0/1 JSON (Appendix B.5; Section 4.3).- Clarify and de-bias parsing-error handling and labeling pipeline
  - Reconcile the discrepancy between defaulting failed calls to 0 (Section 4.3) and reporting -1 “parsing errors” (Figure 3); define how -1 is treated in metrics and ensure consistent handling across analyses.
  - Report per-model, per-split parsing-error rates and evaluate metrics both with and without failed calls (e.g., filtered analysis vs 0-imputation) to quantify bias (Figure 3).
  - Consider neutral imputation (exclude failed calls from accuracy; add separate “format adherence” metrics) or balanced imputation to avoid class-0 bias; correct malformed/non-summing entries in Figure 3 and ensure all percentages sum to 100%.- Improve fairness and comparability in API LLM and efficiency evaluations
  - Avoid assuming architectural types when undocumented (Section 4.2; Table 2); report models as “undisclosed architecture” and refrain from architecture-specific conclusions for those entries.
  - Standardize completion token budgets and shot settings (Appendix B.2), or report matched-budget results alongside “best effort” settings to isolate ICL vs capacity effects.
  - For efficiency (Section 6.2; Table 1), clarify unit interpretation and align textual descriptions with the Accuracy/Hour ratio; normalize by input/output tokens, provide per-instance latency distributions, and repeat API runs to estimate variance; explicitly reconcile the “Azure-only” claim (Appendix B.2) with non-Azure/API/local runs (Table 1; Table 2) to remove pipeline ambiguity.- Expand methodological detail and validation for curvature-based mechanistic analysis
  - Document the curvature pipeline: layers used, trajectory construction across depths, alignment/normalization, and similarity metrics; include ablations across layers/seeds (Section 7; Figure 5).
  - Validate the probe with synthetic controls (e.g., shuffled rule orders, token-level perturbations) and test whether curvature similarity tracks reasoning degradation (Figures 4, 30–31).
  - Extend the mechanistic analysis to NNL data and additional encoder–decoder/decoder layers; include statistical summaries per depth; specify how proof-chain supervision (Appendix B.3) is implemented (objectives/losses/architecture treatment) and its effect on reported metrics.- Reduce confounds in the NNL dataset and isolate lexical effects
  - Match NL and NNL instances on token count and clause structure; constrain NNL tokens to minimize tokenizer fragmentation (Section 4.1; Figure 2).
  - Report token-length distributions and token-type profiles for NL vs NNL; normalize inference-time comparisons for token counts (Section 6.2; Table 1).
  - Add a third OOD variant with permuted natural-language atoms (e.g., synonymized/random word assignments) preserving tokenizer behavior while ablating semantics, enabling cleaner attribution to lexical cues.- Calibrate the scope of claims and broaden causal reasoning coverage
  - Align theoretical framing with the evaluated dataset: explicitly note that disjunction is excluded in experiments and revise Section 3.2 accordingly, or add disjunctive cases to the data (Sections 3.2; 4.1).
  - Qualify conclusions to the Horn-clause setting; revise “ICL alone is insufficient” (Abstract; Conclusion) to reflect observed exceptions (GPT-5 near-perfect; Section 5.1; Tables 8, 11) and the synthetic nature of tasks.
  - Incorporate evaluations of causal notions beyond deduction (e.g., intervention consistency, counterfactual queries, causal graph operations) or note this as a limitation with concrete future plans (Sections 3.1; 4.1).Score
- Overall (10): 6 — Well-motivated cross-architecture study with controlled datasets and depth-wise analyses, but methodological gaps (statistics, parsing/error handling, API/efficiency comparability) limit conclusions (Sections 4.1–4.3; 5.2; 6.1–6.2; Figures 2–5, 7–8; Table 1).
- Novelty (10): 5 — Empirical synthesis (encoder vs decoder) and curvature probe are interesting, yet core elements (FOL depth control, ICL vs fine-tuning) build on established paradigms; mechanistic analysis is underdeveloped (Sections 4.1–4.2; 7; Figure 5).
- Technical Quality (10): 4 — Dataset construction and multi-metric evaluation are offset by misuse of AUROC as significance, unclear ROC/AUC computation from discrete outputs, parsing-error handling inconsistencies, and fairness issues in API/efficiency comparisons (Section 4.3; Figure 3; Appendix B.2–B.5; Table 1).
- Clarity (10): 5 — Generally clear with extensive figures/tables, but contradictions (default-to-0 vs -1 parsing errors; “Azure-only” vs mixed pipelines), garbled class distribution entries (Figure 3), and sparse mechanistic/proof-chain details hinder interpretability (Section 4.3; Section 7; Appendix B.2–B.3).
- Confidence (5): 4 — Confidence is relatively high due to extensive reported analyses and anchors, but undisclosed API details, approximate timing, and noted methodological inconsistencies warrant caution (Section 4.2; Table 2; Sections 6.2; 9–10).