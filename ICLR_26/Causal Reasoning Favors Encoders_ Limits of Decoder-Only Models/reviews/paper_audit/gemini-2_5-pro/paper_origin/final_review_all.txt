1) Summary
This paper investigates the architectural underpinnings of multi-hop logical reasoning, hypothesizing that encoder-based models are inherently better suited for such tasks than decoder-only models. The authors argue that encoders' ability to create a global representation of the input facilitates the compositional and conjunctive control required for logical deduction. To test this, they create a synthetic first-order logic dataset with two out-of-distribution splits: one with deeper reasoning chains (NL) and another with randomized, non-natural language tokens (NNL). They compare fine-tuned encoder-only (BERT) and encoder-decoder (BART, Flan-T5) models against various decoder-only LLMs under zero- and few-shot in-context learning, as well as a fine-tuned decoder-only model (Qwen3-1.7B). Results show that fine-tuned encoder-based models generalize more robustly and efficiently, particularly on the NNL split and at greater reasoning depths, a finding supported by a mechanistic analysis of representation geometry.2) Strengths
*   **Clear and Well-Motivated Hypothesis:** The paper is built around a clear and plausible hypothesis that an encoder's ability to globally project the entire input sequence is architecturally advantageous for multi-hop logical reasoning compared to the recursive, token-by-token processing of decoders.
    *   The informal argument in Section 3.2 provides a solid conceptual foundation for the experimental work, contrasting the "instant global information sharing" of encoders with the sequential propagation in decoders.
    *   This hypothesis directly addresses a known weakness in LLMs—their struggle with strict, compositional reasoning (Section 1)—and proposes a concrete architectural explanation.
    *   The framing connects the architectural design to specific reasoning requirements, namely "multi-hop composition and strict conjunctive control" (Abstract, Section 1).*   **Comprehensive and Rigorous Experimental Design:** The study employs a thorough evaluation framework to test its central hypothesis, covering multiple model families, learning paradigms, and data distributions.
    *   The comparison includes encoder-only, encoder-decoder, and decoder-only architectures, providing a broad view of the architectural landscape (Section 4.2, Table 2).
    *   The work evaluates models under both fine-tuning (BERT, BART, Qwen3) and in-context learning (GPT, Claude, Qwen), which are the dominant paradigms for smaller and larger models, respectively (Figure 1, Section 5).
    *   The dataset design is a key strength, featuring controlled reasoning depth and two distinct OOD test sets (NL and NNL) to disentangle logical structure from lexical cues (Section 4.1, Figure 2). This allows for a nuanced analysis of generalization.
    *   A comprehensive set of metrics is used, including overall accuracy, per-depth F1 scores, and AUROC, to provide a multi-faceted view of model performance (Section 4.3).*   **Novel Mechanistic Interpretability Analysis:** The paper goes beyond standard input-output evaluations by including a mechanistic analysis to provide deeper insight into *how* different architectures process logical information.
    *   The use of "curvature similarity" as a probe to measure the stability of internal transformations is a novel and sophisticated technique in this context (Section 7).
    *   The results of this analysis (Figure 5) provide compelling evidence that aligns with the paper's main hypothesis: BERT (encoder-only) maintains the most stable geometric transformations across increasing reasoning depths, suggesting it re-applies a consistent internal logic.
    *   This analysis directly links the architectural feature (bidirectional context) to an observable internal property (stable curvature), strengthening the paper's central claim that encoders "preserve structural invariants" (Section 7).*   **Strong and Consistent Empirical Results:** The experimental findings consistently support the paper's hypothesis across various tests, demonstrating the robustness of the conclusions.
    *   Fine-tuned encoder-based models, particularly BERT-Base, show superior generalization on the challenging NNL dataset compared to the fine-tuned decoder-only model (Section 5.2, Figure 4 right). BERT-Base degrades much more slowly with depth.
    *   The AUROC analysis shows that fine-tuned encoder and encoder-decoder models achieve better discrimination than the fine-tuned decoder-only model on the NL dataset (Section 5.2, Figure 8/63).
    *   The efficiency analysis in Section 6.2 (Table 1) makes a strong practical point, showing that smaller, fine-tuned encoder/encoder-decoder models can achieve robust performance at a fraction of the computational cost of large decoder-only models, though the comparison methodology has flaws.3) Weaknesses
*   **Overstated Framing of "Causal Reasoning":** The paper is framed around "causal reasoning" (Title, Abstract, Section 1), but the task is strictly one of logical deduction on a subset of first-order logic.
    *   The task, derived from SimpleLogic (Section 4.1), involves determining logical entailment from facts and rules. It does not include core components of causal reasoning like interventions, counterfactuals, or causal discovery from data, as defined in foundational works like Pearl (2009).
    *   While the paper briefly acknowledges that it focuses on the "deductive backbone" of causal reasoning (Section 3.1), the persistent use of the broader term "causal reasoning" throughout the paper is an overclaim that could mislead readers about the scope of the findings.
    *   No direct evidence found in the manuscript that the task evaluates anything beyond logical deduction.*   **Limited Comparison of Fine-Tuned Decoder-Only Models:** The central claim about the superiority of fine-tuned encoders rests on a comparison with only a single fine-tuned decoder-only model (Qwen3-1.7B).
    *   The fine-tuning experiments compare two BERT variants and two BART variants (plus Flan-T5) against just one decoder-only architecture (Section 5.2).
    *   This narrow sample size for the decoder-only family makes it difficult to generalize the conclusion that the observed performance gap is due to the architecture itself, rather than idiosyncrasies of the specific Qwen model chosen.
    *   The lack of other popular open-weight decoder-only models (e.g., from the Llama or Mistral families) in the fine-tuning comparison is a significant limitation.*   **Severe Inconsistencies in Results Reporting:** The presentation of results contains numerous contradictions between text, tables, and plots, which undermines the credibility of the findings and hinders interpretation.
    *   The class distribution data in Figure 3 contains severe inconsistencies. For example, the text table for non-finetuned NNL models claims Qwen-3 1.7B predicts label '1' 100% of the time, while the corresponding plot shows it has 82.3% parsing errors ('-1') (Block #21 vs. Block #25).
    *   The fine-tuning loss curves also show contradictions. The text table for Figure 6 states Qwen3-1.7B's loss converges to 0.15, while the plot shows it converging near 0.55, a value supported by the text in Appendix B.6 (Block #55 vs. Block #57, Block #54).
    *   The model Flan-T5-Base is a key part of the fine-tuned results (Section 5.2, Figure 4) but is omitted from the main "Models Used" section (Section 4.2) and only appears in the appendix (Table 2).
    *   Key figures supporting main-text claims, such as the ROC curves for finetuned models (Figure 8/63), are relegated to the appendix, separating them from the textual discussion of AUC scores (Section 5.2).*   **Use of Hypothetical and Unverifiable Model Citations:** The paper evaluates and cites models with future publication dates, such as "GPT-5 (OpenAI, 2025b)" and "Claude Opus 4.1 (Anthropic, 2025)".
    *   This is non-standard scientific practice and makes the results related to these models impossible to verify or reproduce. The specific versions and capabilities of these purported models are unknown to the public and the research community (Section 4.2, Appendix B.1, References).
    *   Citing future work from model providers (e.g., OpenAI, 2025b; Anthropic, 2025) for models that are not yet announced or documented undermines the scientific credibility of the claims, particularly the finding that GPT-5 achieves "100% accuracy" (Section 5.1).
    *   This practice obscures which actual models were used for the experiments, preventing a fair comparison and replication.*   **Flawed Efficiency Analysis:** The inference time and efficiency comparison presented in Section 6.2 is methodologically unsound, making its conclusions unreliable.
    *   The analysis in Table 1 directly compares the inference times of locally run, fine-tuned models (e.g., BART-Base) against zero-shot API calls for large, proprietary models (e.g., GPT-5).
    *   The table caption explicitly states the analysis is for "zero-shot" inference, which is directly contradicted by a footnote indicating that three of the models are the "finetuned version" (Table 1).
    *   This "apples-to-oranges" comparison renders the "Efficiency (Accuracy/Hour)" metric and the associated conclusions about cost-effectiveness (Section 6.2, Conclusion) misleading.4) Suggestions for Improvement
*   **Refine Terminology to Accurately Reflect the Task:** To improve precision and avoid overclaiming, re-frame the paper's contribution as being about "multi-hop logical deduction" or "compositional logical reasoning" rather than "causal reasoning."
    *   The title, abstract, and introduction should be revised to use the more specific terminology.
    *   The connection to the broader field of causal reasoning can be maintained as a motivating point in the introduction, but the core claims and conclusions should be grounded in the logical deduction task that was actually performed.
    *   This change would align the paper's framing with its technical substance.*   **Broaden the Scope of Fine-Tuned Decoder-Only Models:** To make the architectural comparison more convincing, expand the set of fine-tuned models to include more contemporary and diverse open-source decoder-only architectures.
    *   Fine-tune and evaluate at least one or two additional decoder-only models, such as those from the Llama or Mistral families, at parameter counts comparable to the BERT-Base/Large and BART-Base/Large models.
    *   This would provide a much stronger basis for concluding that the observed performance differences are attributable to the encoder vs. decoder architecture rather than model-specific factors.*   **Resolve Contradictions and Restructure Results:** Thoroughly audit all results to resolve the numerous contradictions and reorganize the presentation for clarity.
    *   Correct all numerical inconsistencies between text, tables, and plots, ensuring that data presented in different formats (e.g., the text table and bar charts in Figure 3) are identical.
    *   Ensure the data in the loss curve table (Figure 6 text) matches the plot (Figure 6 plot) and the textual description (Appendix B.6).
    *   Ensure all models evaluated are properly introduced in the main methods section (Section 4.2), including Flan-T5-Base.
    *   Move essential figures, like the ROC curves for finetuned models (Figure 8/63), into the main body of the paper to be co-located with their corresponding discussion.*   **Use Standard and Verifiable Model Naming and Citations:** Replace all future-dated and hypothetical model names/citations with the official, publicly documented model versions that were used at the time of the experiments.
    *   For example, if "GPT-4.1" refers to a specific version of GPT-4-Turbo, use that name and cite the appropriate technical report or API documentation.
    *   If pre-release models were accessed, this should be stated transparently, but citing non-existent papers from "2025" is not acceptable. Grounding the work in verifiable artifacts is essential for reproducibility and scientific integrity.*   **Correct the Efficiency Analysis:** Revise the efficiency analysis to be methodologically sound and transparent.
    *   The efficiency comparison should be based on consistent conditions (e.g., all fine-tuned or all zero-shot, on comparable hardware where possible).
    *   The text, tables, and captions must be corrected to accurately describe the conditions under which each model's inference time was measured (Table 1).
    *   Conclusions drawn from the efficiency metric should be re-evaluated and potentially tempered based on a corrected, methodologically sound comparison.5) Score
*   Overall (10): 4 — The paper has a novel hypothesis and a strong experimental concept, but the execution is undermined by critical flaws, including unverifiable model citations, severe data inconsistencies, and a methodologically unsound efficiency analysis.
*   Novelty (10): 8 — The architectural comparison for logical reasoning is well-motivated, and the use of a non-natural language dataset (Section 4.1) and curvature similarity probes (Section 7) are novel contributions.
*   Technical Quality (10): 4 — While the dataset design is a strength, the technical quality is severely compromised by the use of unverifiable models (Section 5.1), major contradictions in reported results (Figure 3, Figure 6), and a flawed efficiency comparison (Table 1).
*   Clarity (10): 4 — The core argument is understandable, but the paper is difficult to trust due to pervasive numerical contradictions between text, tables, and plots (e.g., Figure 3, Figure 6), and inconsistent model descriptions (Section 4.2).
*   Confidence (5): 5 — I have carefully reviewed the entire manuscript and am highly confident in my assessment, as the identified weaknesses are supported by clear and direct evidence within the paper.