{
  "paper": "Causal Reasoning Favors Encoders_ Limits of Decoder-Only Models",
  "runs": [
    {
      "config": {
        "judge_model": "gpt-5-2025-08-07",
        "review_agent": "paper_audit",
        "ai_model": "gpt-5-2025-08-07",
        "baseline_file": "baseline_review.txt",
        "final_file": "final_review_all.txt",
        "temperature": 0.0,
        "max_tokens": 20000,
        "timeout_s": 600,
        "metric": "regression_v2_minimal",
        "prompt_hash": "2142c04332"
      },
      "config_key": "1a46f14f5902ebb0fc2c14773cec6644d2fa5338",
      "inputs": {
        "baseline_review": "reviews/paper_audit/gpt-5-2025-08-07/paper_origin/baseline_review.txt",
        "final_review": "reviews/paper_audit/gpt-5-2025-08-07/paper_origin/final_review_all.txt"
      },
      "regression": {
        "score_delta": {
          "baseline_score": 6.0,
          "final_score": 6.0,
          "delta": 0.0,
          "scale_hint": "1-10"
        },
        "differences": [
          {
            "diff_type": "new_critique",
            "summary": "Unclear ROC/AUC computation from discrete 0/1 outputs",
            "paperaudit_types": [
              "EXPERIMENTAL_DESIGN_PROTOCOL",
              "METHOD_LOGIC_CONSISTENCY"
            ],
            "why_impacts_score": "Undermines validity of ROC/AUC results and technical rigor",
            "evidence": {
              "baseline_quote": "AUROC is a performance metric, not a statistical test; no confidence intervals or hypothesis tests provided.",
              "final_quote": "Manuscript does not specify how continuous scores for ROC/AUC were obtained from discrete 0/1 JSON outputs; no evidence of probability/logit extraction."
            }
          },
          {
            "diff_type": "intensified_critique",
            "summary": "Parsing-error handling now flagged as malformed/non-summing",
            "paperaudit_types": [
              "EVIDENCE_DATA_INTEGRITY",
              "EXPERIMENTAL_DESIGN_PROTOCOL",
              "METHOD_LOGIC_CONSISTENCY"
            ],
            "why_impacts_score": "Calls metric reliability/comparability into question",
            "evidence": {
              "baseline_quote": "Figure 3 reports a class '-1' for parsing errors, suggesting inconsistent logging vs scoring.",
              "final_quote": "Several entries appear malformed or non-summing in Figure 3 panels, undermining reliability."
            }
          },
          {
            "diff_type": "new_critique",
            "summary": "Pipeline contradictions (Azure-only claim vs mixed runs) and efficiency metric mismatch",
            "paperaudit_types": [
              "EXPERIMENTAL_DESIGN_PROTOCOL",
              "METHOD_LOGIC_CONSISTENCY",
              "CONTEXT_MISALIGNMENT_INCOHERENCE"
            ],
            "why_impacts_score": "Undermines fairness, comparability, and clarity of methodology",
            "evidence": {
              "baseline_quote": "Efficiency (accuracy/hour) is computed across heterogeneous hardware and API backends and acknowledged as 'estimates at best'.",
              "final_quote": "Claims 'all our calls' used Azure OpenAI API conflict with non-Azure/API evaluations, creating pipeline ambiguity."
            }
          },
          {
            "diff_type": "new_critique",
            "summary": "Proof-chain supervision integration details missing and effects unclear",
            "paperaudit_types": [
              "EXPERIMENTAL_DESIGN_PROTOCOL",
              "METHOD_LOGIC_CONSISTENCY"
            ],
            "why_impacts_score": "Hurts reproducibility and interpretation of reported metrics",
            "evidence": {
              "baseline_quote": "Section 7 does not specify hidden state extraction or the exact curvature computation pipeline.",
              "final_quote": "Fine-tuning supervised on proof chain and final label, but integration details and effects on outputs are not specified."
            }
          },
          {
            "diff_type": "new_critique",
            "summary": "Conceptual–experimental mismatch about disjunction in Section 3.2 vs Horn-fragment data",
            "paperaudit_types": [
              "CONTEXT_MISALIGNMENT_INCOHERENCE",
              "CLAIM_RESULT_DISTORTION"
            ],
            "why_impacts_score": "Misalignment weakens theoretical framing and claims",
            "evidence": {
              "baseline_quote": "Dataset targets a subset of FOL with Horn clauses and excludes disjunctions.",
              "final_quote": "Section 3.2 frames programs including clause-level disjunction, creating a conceptual–experimental mismatch."
            }
          },
          {
            "diff_type": "score_rationale_shift",
            "summary": "Final lowers Technical Quality sub-score citing added methodological issues",
            "paperaudit_types": [
              "EXPERIMENTAL_DESIGN_PROTOCOL",
              "METHOD_LOGIC_CONSISTENCY",
              "EVIDENCE_DATA_INTEGRITY"
            ],
            "why_impacts_score": "Explicitly downgrades technical quality based on new concerns",
            "evidence": {
              "baseline_quote": "Technical Quality (10): 5 — Strong dataset construction and multi-metric evaluation offset by lack of statistical significance testing.",
              "final_quote": "Technical Quality (10): 4 — Misuse of AUROC, unclear ROC/AUC from discrete outputs, parsing/error inconsistencies, fairness issues."
            }
          }
        ]
      },
      "generated_at": "2026-01-05T20:38:06"
    }
  ]
}