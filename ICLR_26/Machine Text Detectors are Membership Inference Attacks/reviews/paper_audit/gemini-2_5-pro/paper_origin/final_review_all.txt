1) Summary
This paper investigates the relationship between Membership Inference Attacks (MIAs) and Machine-Generated Text Detection (MGTD), arguing they are fundamentally similar tasks that have been studied in separate communities. The authors provide a theoretical unification, proposing that the likelihood ratio test is asymptotically optimal for both tasks. They frame a large number of existing methods from both fields as approximations of this optimal test. Through large-scale experiments on 12 methods across 13 domains and 10 generators, the authors demonstrate a strong performance correlation (Spearman's ρ > 0.6) between the two tasks. A key finding is that Binoculars, a state-of-the-art MGTD method, also achieves state-of-the-art performance on MIA benchmarks. To facilitate future cross-task research, the authors introduce MINT, a unified evaluation suite.2) Strengths
*   **Novel and Impactful Thesis:** The paper's central claim—that MIAs and MGTD are two sides of the same coin—is a novel and important contribution. It challenges the siloed research in these areas and encourages a more unified approach, which could accelerate progress in both.
    *   The introduction clearly motivates this gap by highlighting the independent development of nearly identical methods like Neighborhood attack and DetectGPT (Section 1, Section 2.2).
    *   The conclusion calls for "greater cross-task awareness and collaboration" (Abstract, Section 5), a direct and impactful implication of the work.
    *   The release of the MINT unified evaluation suite provides a concrete tool for the community to act on this thesis (Abstract, Section 1).*   **Principled Theoretical Framing:** The paper provides a principled theoretical framework that formally connects the two tasks. This goes beyond prior work that only noted anecdotal similarities between specific methods.
    *   Theorem 2.3 proposes that a likelihood ratio test is asymptotically optimal for both MIA and MGTD, providing a shared theoretical foundation for unifying the two tasks (Section 2.3).
    *   The classification of existing methods as either "Approximation via External Reference" or "Approximation via Text Sampling" is a valuable conceptual contribution that organizes a disparate set of techniques under a single lens (Section 2.4).
    *   Table 1 effectively demonstrates how 13 different methods can be reformulated to fit this unified framework, making the theoretical claim concrete and easy to understand.*   **Comprehensive and Rigorous Empirical Evaluation:** The experimental setup is extensive and robust, providing strong evidence for the paper's claims of transferability.
    *   The study includes a large number of methods (7 MIA, 5 MGTD, plus 4 baselines) (Section 3.1, Table 1).
    *   The evaluation is conducted on large-scale, established benchmarks for both tasks (MIMIR for MIA, RAID for MGTD), covering numerous domains and model sizes (Section 3.1).
    *   The use of Spearman's rank correlation is an appropriate and effective way to quantify the primary claim of performance transferability (Figure 2).
    *   Experiments are conducted in both white-box and black-box settings, demonstrating the relevance of the findings to different real-world scenarios (Section 3.2, Figure 3).*   **Compelling and Surprising Key Finding:** The paper presents a clear, practical demonstration of its thesis by showing that a method from one field can achieve state-of-the-art results in the other.
    *   The finding that Binoculars, an MGTD method, outperforms all tested MIA methods on the MIMIR MIA benchmark is a powerful and surprising result (Figure 3, Section 3.2).
    *   This result directly supports the claim that "current evaluations in MIAs may be biased by overlooking stronger methods from machine text detection" (Section 3.2).
    *   The similarity in score distributions between top-performing methods from each field (Min-K%++ and Binoculars) further reinforces their fundamental connection (Figure 1).*   **Nuanced Analysis of Outliers:** The authors strengthen their argument by analyzing a case of limited transferability, which demonstrates a deep understanding of the tasks' subtle differences.
    *   The paper identifies Zlib as a method that performs well on MIA but poorly on MGTD (Figure 2, Section 3.3).
    *   The analysis in Section 3.3 and Figure 4 provides a convincing explanation: the priors are different. In MIA, both classes are human text, while in MGTD, machine text is systematically more compressible, which confounds the Zlib metric. This thoughtful analysis adds credibility to the overall thesis.3) Weaknesses
*   **Flawed Theoretical Unification and Unclear Assumptions:** The core theoretical result (Theorem 2.3) appears to be based on a flawed mathematical premise and relies on strong, under-specified assumptions, undermining the rigor of the unification.
    *   The proposed test statistic in Theorem 2.3 is a *ratio* of negative log-likelihoods (NLLs), `L(x;M)/L(x;Q)`. However, the Neyman-Pearson lemma, which is cited to prove optimality, applies to the *likelihood ratio*, `P(x|H1)/P(x|H0)`. The log of this ratio is a *difference* of log-likelihoods, not a ratio of NLLs. This invalidates the proof of optimality for the proposed statistic (Section 2.3).
    *   This theoretical claim is contradicted by the paper's own classification of methods. Table 1 shows that top-performing approximations like Reference and DetectGPT are based on a *difference* of NLLs (e.g., `L(x;M) - L(x;M_ref)`), which correctly approximates the true log-likelihood ratio, not the ratio of NLLs proposed in the theorem.
    *   The theorem statement refers to "standard asymptotic regularity conditions" (Section 2.3) without explicitly defining them in the main text. The proof relies on idealizations like "infinite training samples" (Section 2.3), and the potential disconnect between this asymptotic theory and the finite-sample reality of LLM pre-training is not sufficiently addressed in the main discussion.*   **Incomplete Theoretical Framework:** The proposed likelihood ratio framework, while elegant, does not encompass all the methods tested, including some of the top-performing ones.
    *   The authors explicitly state that metrics like DC-PDD and Min-k% "fall outside of our core framing" (Section 2.5).
    *   Min-K% and Min-K%++ are among the best-performing MIA methods and transfer reasonably well (Figure 2, Figure 3), yet the theory does not explain their success or connection to the likelihood ratio principle.
    *   This gap between the theory and the empirical results for some key methods leaves the unification story feeling incomplete.*   **Oversimplified Presentation of Main Results:** The primary results in the main paper are presented as average AUROC scores, which obscures performance variance and potential inconsistencies.
    *   Figure 3 uses simple bar charts showing the mean performance averaged over all domains and models for each task. This makes it difficult to assess how consistently one method outperforms another. For example, while Binoculars has the highest average MIA score, the chart does not show if this holds for all model sizes or domains.
    *   The paper silently omits the `DetectGPT` method from the main visualizations (Figure 2, Figure 3), despite listing it as a tested method (Section 3.1) and showing its (identical to `Neighborhood`) results in the appendix (Tables 2, 3, 4). This lack of explanation could cause confusion.
    *   While full results are available in the appendix (Tables 2, 3, 4), including a measure of variance (e.g., error bars or box plots) in the main figure would provide a more complete and honest picture of the methods' relative performance.*   **Lack of Computational Cost Analysis:** The paper compares numerous methods on performance without a systematic discussion of their computational efficiency, a critical factor for practical application.
    *   Methods like Neighborhood and DetectGPT require generating and scoring many text perturbations, making them significantly more expensive than single-pass methods like Loss, Zlib, or Binoculars (descriptions in Appendix A).
    *   The paper makes claims about the "practical impact of the transferability" (Abstract), but without a cost-benefit analysis, it is difficult for a reader to make informed decisions about which method to use.
    *   No direct evidence found in the manuscript for a systematic comparison of computational costs.4) Suggestions for Improvement
*   **Correct and Clarify the Theoretical Framework:** To improve the technical soundness of the paper, the authors should correct the core theorem and be more explicit about its assumptions.
    *   Revise Theorem 2.3 to use the correct log-likelihood ratio statistic, which is based on a *difference* of NLLs (e.g., `L(x;Q) - L(x;M)`).
    *   This correction would resolve the internal contradiction where the methods in Table 1 correctly use a difference-based formulation while the theorem incorrectly proposes a ratio-based one.
    *   In the statement of Theorem 2.3, briefly enumerate the key assumptions (e.g., model as a maximum likelihood estimator, convergence under infinite data) instead of relying on the general phrase "standard asymptotic regularity conditions."*   **Discuss Methods Outside the Unification Framework:** The authors should briefly discuss the high-performing methods that do not fit their proposed framework to provide a more complete picture.
    *   In Section 2.5, add a few sentences hypothesizing why methods like Min-K%++ work well. For example, could they be viewed as approximating a different, but related, statistical test that focuses on "outlier" tokens as evidence of non-membership, rather than the overall likelihood?
    *   This would demonstrate a deeper engagement with the full landscape of methods and acknowledge the limitations of the current theoretical model.*   **Enhance Visualization and Reporting of Results:** The main results figures and text should be updated to include information about performance variance and clarify the set of tested methods.
    *   In Figure 3, replace the current bar charts with box plots or add error bars to show the distribution of AUROC scores across the different domains and models for each method.
    *   Add a brief note in the experimental setup (Section 3.1) or figure captions explaining that `DetectGPT` was omitted from main figures because its performance is identical to `Neighborhood`, to ensure full transparency.*   **Incorporate Computational Cost Analysis:** To improve the practical utility of the paper, the authors should add a discussion of the methods' computational complexity.
    *   Add a column to Table 1 indicating the relative computational cost of each method (e.g., number of forward passes required per sample).
    *   Include a brief paragraph in the analysis (Section 3.3) discussing the performance-vs-efficiency trade-offs. For instance, it could be noted that Binoculars is not only the top performer but also one of the most efficient methods, further strengthening its position.5) Score
*   Overall (10): 7 — The paper presents a novel and impactful thesis with strong empirical support, but a core theoretical claim is based on a flawed proof.
*   Novelty (10): 9 — The conceptual unification of MIA and MGTD and the large-scale study of transferability are highly novel and insightful.
*   Technical Quality (10): 6 — The empirical methodology is rigorous, but the central theoretical proof is incorrect and there are minor inconsistencies in the experimental reporting (Theorem 2.3, Figure 3).
*   Clarity (10): 8 — The paper is generally well-written, but the flawed theorem and the unexplained omission of a method from key figures reduce clarity.
*   Confidence (5): 5 — I am highly confident in my assessment as the topic is within my area of expertise and the paper provides sufficient detail for a thorough evaluation.