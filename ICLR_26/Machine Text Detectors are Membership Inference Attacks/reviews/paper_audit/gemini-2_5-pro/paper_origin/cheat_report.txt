## Integrity and Consistency Review Report

This report details several high-impact internal inconsistencies and logical problems identified within the manuscript. These issues materially affect the validity of the paper's theoretical claims and the clarity of its empirical results.

### 1. Fundamental Flaw in the Theoretical Unification (Section 2.3)

The central theoretical contribution of the paper—that both Membership Inference Attacks (MIAs) and Machine-Generated Text Detection (MGTD) share the same asymptotically optimal metric—is based on a mathematically incorrect application of the Neyman-Pearson lemma.

-   **Evidence:** In Section 2.3, Theorem 2.3, the proposed optimal test statistic is defined as the ratio of negative log-likelihoods (NLLs):
    $$\Lambda(x) = \frac{\sum_{i=1}^{n} \log P_\mathcal{M}(x_i \mid x_{<i})}{\sum_{i=1}^{n} \log P_Q(x_i \mid x_{<i})} = \frac{\mathcal{L}(x; \mathcal{M})}{\mathcal{L}(x; Q)}$$
-   **Problem:** The Neyman-Pearson lemma, which is cited to prove optimality, applies to the **likelihood ratio**, which is $\frac{P(x|H_1)}{P(x|H_0)}$. The log of this ratio is the difference in log-likelihoods, i.e., $\log P(x|H_1) - \log P(x|H_0)$, which is equivalent to $\mathcal{L}(x; H_0) - \mathcal{L}(x; H_1)$. The paper's proposed statistic, a *ratio* of NLLs, is not the likelihood ratio test statistic. Therefore, the appeal to the Neyman-Pearson lemma is invalid, and the proof of optimality for $\Lambda(x)$ is incorrect. This fundamental error undermines the entire theoretical framework presented in Section 2.

### 2. Contradiction Between Theoretical Framework and Method Classification (Table 1)

The paper's attempt to unify existing methods under its proposed theoretical framework is internally inconsistent. Several key methods are classified as approximations of the proposed ratio-based metric, yet their mathematical forms are based on differences, which align with the correct log-likelihood ratio test that the paper misrepresents.

-   **Evidence:** Table 1 presents "Unified formulations" of various methods.
    -   `Reference` is formulated as: $\mathcal{L}(x; \mathcal{M}) - \mathcal{L}(x; \mathcal{M}_{\text{ref}})$
    -   `DetectGPT` and `Neighborhood` are formulated as: $\mathcal{L}(x; \mathcal{M}) - \mathbb{E}_{\tilde{x} \sim \phi(x)}\!\left[\mathcal{L}(\tilde{x}; \mathcal{M})\right]$
-   **Problem:** These methods are based on a **difference** of NLLs, which approximates the true log-likelihood ratio. This directly contradicts the paper's central theoretical claim that the optimal metric is a **ratio** of NLLs. The paper's framework fails to account for the actual mathematical structure of the very methods it claims to unify.

### 3. Inconsistent Reporting of Experimental Methods and Results (Section 3 vs. Appendix C)

There is a significant discrepancy between the methods described as tested and those presented in the main figures, which could mislead the reader about the scope and results of the evaluation.

-   **Evidence:**
    1.  Section 3.1 ("Methods Tested") states that 5 machine text detection methods are considered, explicitly listing `DetectGPT`.
    2.  The paper repeatedly notes that `Neighborhood` (an MIA method) and `DetectGPT` are "approximately identical" (Section 2.2) and presents them with the exact same formula in Table 1.
    3.  The full results in the appendix (Table 2, Table 3, Table 4) include separate entries for both `Neighborhood` and `DetectGPT`, showing identical performance scores for both across all experiments.
    4.  However, the main results visualizations (Figure 2 and Figure 3) silently **omit** `DetectGPT` entirely, presenting results for only 15 methods instead of the 16 that were clearly evaluated. In Figure 2, the single data point representing both methods is colored blue, classifying it as an MIA method (`Neighborhood`).
-   **Problem:** This inconsistent reporting is problematic. By selectively removing one method from a duplicate pair in the main figures without explanation, the paper obscures the full scope of its analysis and presents a potentially confusing narrative. The choice to label the shared data point as an MIA method in Figure 2 is an arbitrary representational choice that is not explicitly justified.

### 4. Direct Numerical Contradiction in Experimental Setup Description

The manuscript provides conflicting information about the number of models used in the black-box detection experiments, calling into question the reliability of the reported averages.

-   **Evidence:**
    -   The caption for Figure 3 states that the black-box results are "averaged over eight domains and **five models**."
    -   However, Section 3.3 ("Transferability in Real-World Scenarios") describes the same experiment as evaluating performance "on texts generated by **ChatGPT and GPT-4**." This implies two, not five, generator models.
-   **Problem:** This is a direct and unexplained numerical contradiction in the description of the experimental methodology. It is unclear how the average results reported in Figure 3 were calculated.

### 5. Flawed Formal Problem Definition (Section 2.1)

The theoretical analysis is built upon a poorly defined formal problem statement for membership inference.

-   **Evidence:** In Section 2.1, the hypotheses for membership inference are given as:
    $H_0 : x \in \mathcal{X}$ vs. $H_1 : x \in \mathcal{D}_{train}$
-   **Problem:** The null hypothesis $H_0 : x \in \mathcal{X}$ (the text is a sequence of tokens) is trivially true for any input text $x$ and does not represent a meaningful alternative to the "member" hypothesis ($H_1$). The standard formulation for a non-member is a sample drawn from the data distribution that was not included in the training set. This flawed setup further weakens the already problematic theoretical claims.

### Conclusion

The manuscript contains significant, evidence-based inconsistencies that affect its core contributions. The theoretical framework is based on a fundamental mathematical error, the classification of existing methods contradicts this framework, and the reporting of the experimental setup and results is inconsistent and contradictory. These issues must be addressed to ensure the scientific validity and trustworthiness of the work.