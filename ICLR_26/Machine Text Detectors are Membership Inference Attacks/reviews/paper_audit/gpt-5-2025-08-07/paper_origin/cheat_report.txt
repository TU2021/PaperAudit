Academic integrity and internal consistency risk report

Summary of high-impact issues
The manuscript presents a strong empirical story, but several core theoretical derivations and multiple reporting/definition inconsistencies materially affect the paper‚Äôs correctness, reproducibility, and trustworthiness. The most serious problem is a mis-specification of the ‚Äúoptimal‚Äù likelihood ratio test in Section 2.3, which underpins the main theoretical claim. Additional issues concern the formulation of membership inference hypotheses, baseline metric definitions, inconsistent method counts and averaging regimes, misattribution, and incomplete tables/footnotes.

Major theoretical inconsistencies

1) Mis-specified ‚Äúlikelihood ratio test‚Äù (ratio of log-likelihoods rather than a log-likelihood difference or density ratio)
- Evidence: Section 2.3 Theorem 2.3 (Block 11) defines Œõ(x) as
  Œõ(x) = [‚àëi log P_M(x_i|x_<i)] / [‚àëi log P_Q(x_i|x_<i)] = L(x;M)/L(x;Q).
  In Step 1 (Block 11), the paper states: ‚ÄúThe likelihood ratio test for [H0: x~P_Q vs H1: x~P_M] is Œõ_MGT(x) = L(x;M)/L(x;Q), which exactly coincides with the proposed statistic Œõ(x).‚Äù
- Problem: The uniformly most powerful Neyman‚ÄìPearson test compares the likelihood ratio p_M(x)/p_Q(x), or equivalently the log-likelihood difference ‚àëi log P_M(x_i|x_<i) ‚àí ‚àëi log P_Q(x_i|x_<i). The ratio of sums of log-likelihoods (or ratio of negative log-likelihoods) used here is not monotone in the true likelihood ratio; it is not an equivalent test statistic. This invalidates the central optimality claim.
- Impact: This affects the headline theoretical claim that the same ‚Äúoptimal metric‚Äù achieves asymptotically highest performance on both tasks (¬ß2.3), and the subsequent framing that methods approximating Œõ(x) will transfer well (¬ß2.5).

2) Membership inference hypotheses are not distributional, and the LRT derivation is unsupported
- Evidence:
  - Section 2.1 (Block 9): MIA hypotheses are stated as ‚ÄúH0: x ‚àà X, H1: x ‚àà D_train,‚Äù i.e., set membership, not distributions.
  - Section 2.3 Step 2 (Block 12): ‚ÄúSince M is trained via cross-entropy, it is a maximum likelihood estimator‚Ä¶ ùìõ(x;ùìú) converges to the likelihood of x under H1. Therefore, the likelihood ratio test ‚Ä¶ is also Œõ_MI(x) = ùìõ(x;ùìú)/ùìõ(x;ùì†).‚Äù
- Problem: A likelihood ratio test requires distributions under each hypothesis. ‚Äúx is in the training set‚Äù versus ‚Äúx is not‚Äù are not probability models. The standard MIA LRT (e.g., Carlini et al., 2022) compares an ‚Äúin‚Äù model vs an ‚Äúout‚Äù model trained on datasets with/without x; the manuscript does not derive or justify replacing that with P_Q in the denominator. The statement that ùìõ(x;ùìú) ‚Äúconverges to the likelihood of x under H1‚Äù is also conceptually incorrect: cross-entropy training makes P_M approximate the data distribution, but does not define a distribution ‚Äúunder H1: x ‚àà D_train‚Äù in a way that yields the proposed ratio.
- Impact: This undermines the claimed unified optimality for MIA (Block 11‚Äì12) and the theoretical foundation for transferability.

3) Incorrect training objective description in the theorem statement
- Evidence: Section 2.3 Theorem 2.3 (Block 11): ‚ÄúLet ùìú be a language model trained with cross-entropy to minimize the likelihood of some training set‚Ä¶‚Äù
- Problem: Cross-entropy training maximizes likelihood (minimizes negative log-likelihood). Saying ‚Äúminimize the likelihood‚Äù is wrong and causes confusion in the theorem‚Äôs assumptions.
- Impact: Although a sign error, it appears in the formal theorem and contributes to ambiguity in subsequent equalities involving ùìõ.

4) Advantage bound derivation is stated for Œõ(x) without proving Œõ is Bayes-optimal
- Evidence: Section 2.3 Step 3 (Block 13): ‚ÄúFor a binary hypothesis test with statistic Œõ(x), the Bayes error is Œµ* = (1 ‚àí TV(P_M, P_Q))/2 ‚Ä¶ yields the stated bound ‚Ä¶‚Äù
- Problem: Œµ* is the minimum achievable error over all tests; it does not automatically apply to an arbitrary statistic Œõ(x). The manuscript does not show that the proposed Œõ(x) achieves Bayes optimality (and given Issue 1, it does not).
- Impact: Overstates performance guarantees associated with Œõ(x).

Metric definitions and reproducibility issues

5) Entropy baseline is mis-defined
- Evidence: Appendix A.1 (Block 46) defines Entropy as f(x;M) = E_{tilde x ‚àº M}[ùìõ(tilde x;M)] and describes it as ‚Äúexpected likelihood of the next token ‚Ä¶ at each time step.‚Äù
- Problem: The formula given is an unconditional expectation over samples from M, not the conditional token-level entropy H[p_M(¬∑|x_{<i})] computed on the given sequence x. As written, it would be constant per model and cannot yield per-sample ROC curves shown in Figures/Tables (e.g., Figure 3; Tables 3‚Äì4).
- Impact: This discrepancy suggests either the implementation differs from the paper‚Äôs definition or results are not reproducible from the description.

6) Min-K% sign inconsistency
- Evidence: Table 1 (Block 19) expresses Min-k% using ùìõ(x_i;M) (negative log-likelihood), whereas Appendix A.2 (Block 48) defines Min-K% using log p(x_i|x_{<i};M) without the negative sign.
- Problem: These are not the same statistic; the sign change affects ordering and thresholds.
- Impact: Ambiguity in how Min-K% was implemented and evaluated affects reproducibility.

7) Zlib ‚Äúequivalent reformulation‚Äù lacks derivation
- Evidence: Appendix A.2 (Block 48): Zlib is ‚Äúequivalently rewritten‚Äù as ùìõ(x;M)/E_{x‚àºM_ref}[ùìõ(x;M_ref)] where M_ref is the ‚Äúempirical substring distribution.‚Äù
- Problem: No derivation is provided to connect zlib compression entropy to an expectation of negative log-likelihood under an explicit probabilistic model M_ref. The statement that this is an ‚Äúequivalent‚Äù reformulation is asserted but not shown.
- Impact: The categorization of Zlib as an ‚Äúexternal reference likelihood ratio‚Äù (Section 2.4, Block 17) rests on this equivalence; without proof, the unification is not substantiated. No direct evidence found in the manuscript.

Reporting and attribution inconsistencies

8) Method count mismatch in rank-correlation analysis
- Evidence:
  - Methods listed: Section 3.1 (Block 23) includes 7 MIA methods + 5 detectors + 4 baselines (Loss, Rank, LogRank, Entropy) = 16 methods.
  - Section 3.2 (Block 26) states: ‚ÄúWe compute the rank correlation over all 15 methods‚Ä¶‚Äù
- Problem: The reported number of methods used in correlation does not match the list provided. Figure 2 (Block 28) appears to include both MIA methods and detectors plus baselines, but the exact count is not reconciled.
- Impact: Ambiguity in what was included in the correlation undermines the statistical claim (œÅ = 0.66).

9) Averaging regime mismatch: ‚Äúthree generators‚Äù vs ‚Äúfive models‚Äù
- Evidence:
  - Section 3.2 (Block 26): ‚Äúrankings ‚Ä¶ averaged ‚Ä¶ on machine text detection (across eight domains and three generators).‚Äù
  - Figure 3 caption (Block 32): white-box and black-box detection results ‚Äúaveraged over eight domains and five models.‚Äù
- Problem: The paper oscillates between ‚Äúthree generators‚Äù (white-box models) and ‚Äúfive models‚Äù (white-box + two closed-source), without a clear statement of which setting the correlation/averages use.
- Impact: Confuses the scope of the main correlation and averages.

10) Incomplete/damaged text in dataset description
- Evidence: Section 3.1 (Block 22): ‚Äúas well as 5 models: 1 (OpenAI et al., 2024) as closed-source models.‚Äù The phrase is truncated/garbled.
- Impact: Obscures the experimental setup; readers cannot unambiguously identify models counted in ‚Äúfive.‚Äù

11) Misattribution of DetectLLM
- Evidence:
  - Table 1 (Block 19) and References (Block 44) correctly list DetectLLM as Su et al., 2023.
  - Appendix B.2 (Block 60) refers to ‚ÄúDetectLLM (Mitchell et al., 2023).‚Äù
- Problem: Inconsistent citation that mixes DetectGPT (Mitchell et al., 2023) and DetectLLM (Su et al., 2023).
- Impact: Citation accuracy and clarity for replication.

12) Broken/ambiguous footnote for reference models
- Evidence: Appendix B.2 (Block 58): ‚ÄúGPT2-XL-Chat‚Å∑ ‚Ä¶ ‚Å∑ lgaalves/gpt2-xl_llma LLaMA-2-70B-Chat.‚Äù
- Problem: The footnote conflates GPT-2 and LLaMA resources; the identifier ‚Äúgpt2-xl_llma‚Äù is unclear, and the line wraps join two different models.
- Impact: Reproducibility risk in setting up Binoculars reference models.

13) Table truncation
- Evidence: Appendix Table 4 (Block 65) ends mid-row: ‚ÄúWikipedia‚Ä¶ GPT-4‚Ä¶ Llama | Wikipedia‚Ä¶ G‚Äù
- Impact: The black-box results table is incomplete, hindering verification and reuse.

14) Abstract vs body mismatch on scope numbers
- Evidence:
  - Abstract (Block 2): ‚Äúacross 13 domains and 10 generators.‚Äù
  - Section 3.1‚Äì3.2 and Figures/Tables (Blocks 22, 26, 32, 64): 13 domains (5 MIMIR + 8 RAID) are consistent; however, the manuscript uses three white-box generators and two closed-source (total five) in detection, and five target models in MIAs (not ‚Äúgenerators‚Äù). ‚Äú10 generators‚Äù is not reconciled in the body.
- Impact: Creates confusion about the experimental breadth claimed in the abstract.

Other notable internal inconsistencies

15) Nomenclature inconsistency: Lastde/Lastde++
- Evidence: Table 1 (Block 19) lists both Lastde and Lastde++; Methods tested (Block 23) include Lastde++ only; in Table 2 (Block 63) the label appears as ‚ÄúLLaSDe++,‚Äù which looks like a typo.
- Impact: Minor, but harms clarity.

16) Notational inconsistency for Q vs ùì†
- Evidence: Section 2.3 Step 2 (Block 12) uses ùì† in denominator, while earlier (Blocks 8‚Äì11) use Q.
- Impact: Minor notation inconsistency inside the key theorem/proof.

Recommendations
- Correct the theoretical derivation: Replace Œõ(x) = ratio of (log-)likelihood sums with the proper log-likelihood ratio, i.e., ‚àëi log P_M(x_i|x_<i) ‚àí ‚àëi log P_Q(x_i|x_<i), or p_M(x)/p_Q(x). Revisit claims of optimality and downstream ‚Äúunification‚Äù in Table 1 and Sections 2.4‚Äì2.5 accordingly.
- Provide a rigorous distributional formulation for MIA (H0/H1) and justify any use of P_Q in a membership test, or else align with established LRT approaches that use ‚Äúin/out‚Äù shadow models.
- Fix the cross-entropy objective statement (‚Äúmaximize likelihood‚Äù/‚Äúminimize negative log-likelihood‚Äù).
- Correct the Entropy baseline definition to the per-token conditional entropy on the given sequence x (and ensure the implemented metric matches the described formula).
- Resolve method count and averaging discrepancies (Blocks 23 vs 26; Blocks 26 vs 32). State exactly which methods and models are included in each analysis.
- Repair truncated/garbled text (Blocks 22, 65) and footnotes (Block 58) to ensure reproducibility.
- Correct DetectLLM attribution (Block 60).

If these issues are addressed, the empirical transferability story could remain compelling. As written, the theoretical core and several implementation/reporting details require correction before the conclusions can be considered reliable.

No direct evidence found in the manuscript
- Formal derivation showing Zlib‚Äôs compression entropy equals an expectation of negative log-likelihood under an explicit surrogate distribution (Appendix A.2, Block 48).