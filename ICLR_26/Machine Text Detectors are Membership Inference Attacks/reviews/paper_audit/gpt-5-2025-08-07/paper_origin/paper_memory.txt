# Global Summary
The paper investigates the theoretical and empirical connection between membership inference attacks (MIAs) and machine-generated text detection (MGTD) for language models. It proves that both tasks share the same asymptotically optimal decision statistic: a likelihood ratio between the target model and the true human text distribution. It unifies many existing metrics as approximations of this ratio and hypothesizes that the quality of this approximation predicts cross-task transferability. Large-scale experiments evaluate 7 MIA methods and 5 MGTD methods across 13 domains and 10 generators, computing AUROC-based rankings and Spearman rank correlations between tasks. The authors report a statistically significant cross-task rank correlation of ρ = 0.66 (p < 0.01) across 15 methods and ρ = 0.78 (p < 0.01) among the top-10 MIA methods. Binoculars, a MGTD method, achieves the best average AUROC on both tasks, notably outperforming MIA methods on MIA benchmarks. The work introduces MINT, a unified evaluation suite implementing 15 recent methods. Caveats include reliance on asymptotic assumptions in theory, differences in task priors (e.g., Zlib’s limited transfer), and feasibility limits for MIAs in black-box settings for closed-source models where training data is unknown.

# Abstract
- Problem: MIAs identify whether a text came from a model’s training data; MGTD distinguishes human vs machine-generated text. Both often exploit model probability distributions, yet have been studied separately.
- Theory: Proves the same metric achieves asymptotically highest performance for both tasks—the likelihood ratio between the target model distribution and the true population distribution. Hypothesis: a method’s accuracy in approximating this metric correlates with transferability.
- Empirics: Tests 7 SOTA MIA methods and 5 SOTA MGTD methods across 13 domains and 10 generators; finds very strong rank correlation in cross-task performance (ρ > 0.6).
- Key finding: Binoculars, designed for MGTD, achieves state-of-the-art performance on MIA benchmarks.
- Resource: Introduces MINT, a unified evaluation suite with implementations of 15 methods. GitHub: https://github.com/ryuryukke/mint.
- Visual evidence: Score distributions of Min-K%++ and Binoculars are strikingly similar across tasks, suggesting transferability.
- Status: Preprint.

# Introduction
- Context: LLMs provide benefits but raise privacy (PII leakage, copyright) and authenticity concerns (propaganda, academic cheating).
- Tasks:
  - MIAs: classify whether a text is in a model’s training set (e.g., likelihood-based signals).
  - MGTD: distinguish human vs machine-generated texts (likelihood/entropy baselines).
- Shared methodology: Both tasks use model-derived likelihood signals; Neighborhood attack (MIA) and DetectGPT (MGTD) are essentially identical via text perturbation-based probability curvature.
- Motivation: Independent study may bias evaluations and miss stronger methods from the other task.
- Contributions:
  - Theory: Both tasks share the same asymptotically optimal metric—the likelihood ratio between model and true human distribution (Section 2.3).
  - Empirics: Large-scale evaluation of 7 MIA and 5 MGTD methods across 13 domains and 10 generators, computing cross-task Spearman rank correlations (ρ > 0.6).
  - Finding: Binoculars achieves SOTA on MIAs, indicating practical impact of transferability.

# Method
- Unified Optimality (Theorem 2.3):
  - Statistic: Λ(x) = L(x; M) / L(x; Q), where L denotes summed log likelihood over tokens under model M or oracle Q.
  - It achieves optimal accuracy at a given false positive rate for both MGTD and MIA under standard asymptotic regularity conditions.
  - Advantage bound: adv ≤ sqrt(D_KL(P_Q || P_M) / 8).
- Proof sketch:
  - MGTD: Likelihood ratio test between H0: x ∼ P_Q and H1: x ∼ P_M; Λ_MGT(x) equals Λ(x). By Neyman–Pearson, it is uniformly most powerful for any fixed Type I error.
  - MIA: Test H0: x ∈ X vs H1: x ∈ D_train; with cross-entropy training, L(x; M) converges to likelihood under H1; Λ_MI(x) equals Λ(x) asymptotically.
  - Error bound via total variation and Pinsker’s inequality leads to the stated advantage bound.
- Implication: Any method closely approximating Λ(x) should transfer well across MIAs and MGTD.
- Approximation strategies (Section 2.4):
  - External reference approximation: Replace L(x; Q) with L(x; M_ref). Examples: Reference (MIA), Zlib (MIA via Huffman entropy), DC-PDD (MIA via corpus token frequencies), Binoculars (MGTD via cross-model entropy).
  - Text sampling approximation: Approximate L(x; Q) via the expected likelihood of perturbed texts E_{tilde{x}∼φ(·|x)}[L(tilde{x}; M)]. Examples: Neighborhood (MIA), DetectGPT and Fast-DetectGPT (MGTD).
- Unified formulations (Table 1):
  - Lists equational forms for methods including Reference, Zlib, DetectLLM, ReCaLL, DC-PDD, Binoculars, DetectGPT, Neighborhood, Fast-DetectGPT, Min-K%, Min-K%++, Lastde, Lastde++.
- Discussion (Section 2.5):
  - Some metrics (e.g., single-quantity methods like DC-PDD, Min-K%) fall outside pure ratio framing; transferability assessed empirically.
  - Zlib, despite being a ratio approximation, performs poorly on MGTD; hypotheses for this are explored in analysis (Section 3.3).
  - Practical note: Real LMs may not meet asymptotic assumptions; creative approximations may outperform direct likelihoods in practice.

# Preliminaries
- Task formalization:
  - Let X be token sequences; Q be an oracle for P_Q (human text); M be LM distribution P_M; D_train ⊂ X be unknown training set.
- Hypothesis tests:
  - MGTD: H0: x ∼ P_Q; H1: x ∼ P_M.
  - MIA: H0: x ∈ X; H1: x ∈ D_train.
- Goal: Develop f(x; M) to accept/reject null with maximum power.

# Motivation
- Identical pairs:
  - Neighborhood (MIA): Neighborhood(x; M; φ) = L(x; M) − (1/n) Σ_i L(tilde{x}^{(i)}; M), with perturbations tilde{x}^{(i)} ∼ φ(x).
  - DetectGPT (MGTD): DetectGPT(x; M; φ) = L(x; M) − E_{tilde{x} ∼ φ(x)}[L(tilde{x}; M)].
- Observation: The two metrics are approximately identical (finite-sample vs expectation), motivating broader exploration of cross-task metric equivalence and transferability.

# Experiments
- Objective: Quantify cross-task transferability via rank correlation of method performance on MIAs and MGTD.
- Datasets:
  - MIMIR (MIAs): Large-scale benchmark with 5 Pile domains—Wikipedia (knowledge), Pile CC (general web), PubMed Central and ArXiv (academic), HackerNews (dialogue). Members and non-members drawn from Pile train/test; 13-gram deduplication filtering prevents leakage.
  - RAID (MGTD): Benchmark with 8 domains—Wikipedia, News, Abstracts, Recipes, Reddit, Poetry, Books, Reviews. White-box target models include GPT-2-XL, MPT-30B-Chat, LLaMA-2-70B-Chat (Table 3). Black-box generators include ChatGPT and GPT-4 (Table 4).
- Target/surrogate models:
  - MIAs: PYTHIA suite—five models with 160M, 1.4B, 2.8B, 6.7B/6.9B, and 12B parameters (6.7B vs 6.9B appears inconsistently across sections).
  - Black-box MGTD: Surrogate models PYTHIA-160M and LLaMA-3-3.2B; report averages across surrogates.
- Methods tested:
  - MIAs (7): Reference, Zlib, Neighborhood, Min-K%, Min-K%++, ReCaLL, DC-PDD.
  - MGTD (5): DetectGPT, Fast-DetectGPT, Binoculars, DetectLLM, Lastde++.
  - Baselines: Loss, Rank, LogRank, Entropy.
- Evaluation:
  - Metric: AUROC.
  - Transferability measure: Spearman’s rank correlation between AUROC-based rankings on MIAs and MGTD.
- Main results (Section 3.2):
  - Cross-task rank correlation: ρ = 0.66 (p < 0.01) across all 15 methods; ρ = 0.78 (p < 0.01) among top-10 MIA methods.
  - Top performance: Binoculars achieves best average AUROC in both MIAs and MGTD.
- Average AUROC summaries (Figure 3):
  - MIAs (averaged over five domains and five models):
    - Binoculars 54.45; Fast-DetectGPT 54.44; Min-K%++ 54.29; DetectLLM 53.98; Lastde++ 53.97; Neighborhood 53.93; Min-K% 53.74; Reference 53.70; ReCaLL 53.59; Zlib 53.53; Loss 53.44; DC-PDD 53.43; LogRank 53.41; Rank 52.94; Entropy 52.42.
  - MGTD white-box (averaged over eight domains and reported generators/models):
    - Binoculars 99.87; DetectLLM 99.40; Fast-DetectGPT 99.36; Min-K% 99.03; Lastde++ 98.49; Min-K%++ 98.44; LogRank 97.55; Loss 97.26; DC-PDD 84.46; ReCaLL 82.52; Rank 82.16; Neighborhood 81.90; Entropy 81.58; Zlib 81.20; Reference 63.06.
  - MGTD black-box (averaged over eight domains and two generators, across surrogates):
    - Binoculars 88.60; Loss 85.20; LogRank 84.20; Min-K% 83.50; DC-PDD 80.80; DetectLLM 79.70; Fast-DetectGPT 79.30; Min-K%++ 79.20; Entropy 78.00; Lastde++ 77.80; Rank 76.80; Zlib 73.00; Neighborhood 71.80; ReCaLL 71.30; Reference 56.60.
- Additional analyses (Section 3.3):
  - Real-world transfer: In black-box MGTD on ChatGPT/GPT-4, Binoculars remains best; MIA methods (Min-K%, DC-PDD) reach parity or exceed strong detectors in some domains.
  - Distributional similarity: Jensen–Shannon distance between Min-K%++ and Binoculars score distributions is 0.14 (MIAs) and 0.11 (MGTD) on Wikipedia with PYTHIA-12B.
  - Zlib outlier: Zlib ranks 10th in MIAs but 14th in MGTD; compression entropy distributions converge between classes in MIAs (means: 676.9 non-members vs 680.4 members) but diverge in MGTD (means: 804.4 humans vs 561.8 machines); machine texts are more compressible, diminishing discriminative signal when combined with loss.
- Scenarios and caveats:
  - Main rank-correlation analysis uses white-box settings (access to token probabilities).
  - MIAs on closed-source models are infeasible due to unknown training data; therefore MIAs are evaluated only in white-box.
  - Reported averages and model/domain counts are summarized per figures/tables; some counts differ across sections (e.g., 3 vs 5 white-box models), exact reconciliation not specified.

# Related Work
- Proximity noted before: Neighborhood (MIA) and DetectGPT (MGTD) are essentially identical (curvature via perturbations); prior observations suggested MIAs might act as MGTD by misclassifying machine-generated non-members as members.
- Likelihood ratio optimality for MIAs: Carlini et al. (2022) introduced shadow-model-based LiRA approximating LRT; strong performance at low FPR; practical constraints for foundation models noted.
- MIA methods: likelihood-based and curvature/min-k variants (Carlini et al., 2021; Mattern et al., 2023; Shi et al., 2024; Zhang et al., 2025; Xie et al., 2024; Zhang et al., 2024).
- MGTD methods: supervised classifiers (n-grams, neural) and zero-shot detectors leveraging model statistics (entropy, likelihood, curvature, cross-model entropy). This work focuses on zero-shot detectors for transferability alignment.

# Conclusion
- Theoretical: Many methods from MIAs and MGTD reduce to measuring discrepancy between surprisal under a target model and under the true distribution; the likelihood ratio yields shared optimality.
- Empirical: Substantial cross-task rank correlation (ρ = 0.66; top-10 ρ = 0.78) validates transferability; methods originally proposed for one task perform well on the other.
- Practical impact: Binoculars, designed for MGTD, surpasses state-of-the-art MIA methods on MIA benchmarks and leads in MGTD, highlighting the need for cross-task awareness and fair evaluation.
- Call to action: Encourage closer collaboration and unified evaluation across the two communities; MINT suite provided to facilitate such efforts.

# Appendix
- Author contributions: Koike and Dugan co-designed; Koike implemented and ran experiments and drafted; Dugan proved theory and co-wrote; Kaneko, Callison-Burch, Okazaki supervised and edited.
- Acknowledgments: Thanks to Shannon Sequiera for derivation checks; support from DARPA SciFy (HR00112520300), ODNI/IARPA HIATUS (#2022-22072200005), NICT Japan (No. 22501), JST SPRING (JPMJSP2106); views do not reflect official policies.
- Methods (Appendix A):
  - Baselines: Loss (L(x; M)); Entropy (E_{tilde{x}∼M}[L(tilde{x}; M)]); Rank (average token rank); LogRank (average log rank).
  - MIAs:
    - Reference: L(x; M) − L(x; M_ref); smaller-size references per target.
    - Zlib: L(x; M) / zlib(x); equivalently L(x; M) / E_{x∼M_ref}[L(x; M_ref)] with empirical substring distribution.
    - Neighborhood: L(x; M) − (1/n) Σ_i L(tilde{x}^{(i)}; M); perturbations via φ(x).
    - Min-K%: average log-likelihood of k% lowest-probability tokens (k = 20).
    - Min-K%++: standardized version of Min-K% over vocabulary (k = 20); normalization by standard deviation φ shorthand Φ(x) = x / σ_x.
    - ReCaLL: E_{tilde{x}∼φ(x)}[L(tilde{x}; M)] / L(x; M), φ(x) = P ⊕ x with non-member prefixes (n = 10).
    - DC-PDD: E_{tilde{x}∼M}[L(tilde{x}; M_ref)] using unigram frequencies from reference corpus (Laplace-smoothed).
  - MGTD:
    - DetectGPT: log-likelihood curvature via perturbations (T5-Large, mask rate 0.3).
    - Fast-DetectGPT: curvature with normalization (Φ) using perturbation model likelihoods.
    - Binoculars: L(x; M) / E_{x∼M}[L(x; M_ref)] with a reference model similar in performance.
    - DetectLLM (NPR): E_{tilde{x}∼φ(x)}[R(tilde{x}; M)] / R(x; M), where R denotes log rank.
    - Lastde++: variance/diversity-based metric with sampling and normalization; Lastde uses L(x; M) divided by stddev of diversity-entropy across scales; default settings s = 4, ε = 8, τ' = 15.
- Implementation details (Appendix B):
  - Reference model choices per target/surrogate; examples include GPT2-Small for GPT2-XL and LLaMA-2-70B for LLaMA-2-70B-Chat; PYTHIA-70M as reference for PYTHIA models.
  - Neighborhood and DetectGPT: T5-Large mask filling, mask rate 0.3 across settings.
  - Min-K% and Min-K%++: k = 20.
  - ReCaLL: number of prefixes n = 10; prefixes from non-member texts or human-written texts depending on task.
  - DC-PDD: reference unigram distribution built from C4 subsets with model-specific tokenizers.
  - Fast-DetectGPT and DetectLLM: target model used for scoring and perturbation; surrogates in black-box.
  - Binoculars: uses official code; target/reference model pairing per task; e.g., LLaMA-2-70B as reference for LLaMA-2-70B-Chat; PYTHIA-deduped as reference for PYTHIA in MIAs.
- Full performance tables (Appendix C):
  - Table 2: AUROC per domain/model for MIAs across PYTHIA sizes (160M, 1.4B, 2.8B, 6.9B, 12B), including textual domains; averages broadly match Figure 3 summary.
  - Table 3: AUROC per domain for MGTD white-box across GPT-2-XL, MPT-30B-Chat, LLaMA-2-70B-Chat; many methods exceed 99 AUROC in multiple domains; Binoculars often reaches 100.0.
  - Table 4: AUROC per domain for MGTD black-box across ChatGPT and GPT-4 with LLaMA-3-3.2B and PYTHIA-160M surrogates; Binoculars reaches up to 100.0 in some settings; variability across domains and surrogates documented.

# References
- Core theoretical and methodological references: Neyman–Pearson (1933) for uniformly most powerful tests; Carlini et al. (2022) for LRT-based MIAs (LiRA); Carlini et al. (2021) for extracting training data and MIA baselines; Mattern et al. (2023) for Neighborhood; Mitchell et al. (2023) for DetectGPT; Bao et al. (2024) for Fast-DetectGPT; Hans et al. (2024) for Binoculars; Su et al. (2023) for DetectLLM; Xu et al. (2025) for Lastde++.
- Benchmarks and datasets: Duan et al. (2024) MIMIR (MIAs); Dugan et al. (2024) RAID (MGTD); Gao et al. (2020) The Pile; Raffel et al. (2023) T5; Grattafiori et al. (2024) LLaMA-3; Biderman et al. (2023) PYTHIA.
- Contextual studies: privacy leakage (Lukas et al., 2023), copyright (Wei et al., 2024), memorization (Morris et al., 2025); machine text detection surveys and supervised approaches (Ippolito et al., 2020; Crothers et al., 2023; Yang et al., 2024; Li et al., 2024; Wang et al., 2024).
- Additional findings: machine text compressibility (Tulchinskii et al., 2023; Mao et al., 2025) supporting Zlib analysis.