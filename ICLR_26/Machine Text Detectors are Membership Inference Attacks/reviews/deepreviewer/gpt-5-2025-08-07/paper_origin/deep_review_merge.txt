Summary
The paper investigates the relationship between membership inference attacks (MIAs) on language models and machine-generated text detection (MGTD). It proposes that both tasks hinge on comparing a target model’s distribution to a reference or “true” distribution and claims a theoretical unification via a statistic Λ that is presented as (asymptotically) optimal for both. The authors map many published methods to approximations of this contrast (via reference models or sampling) and conduct a large-scale empirical study spanning 15 methods across numerous domains and generators, evaluating on MIMIR (MIAs) and RAID (MGTD). The main empirical findings are substantial cross-task transferability—strong rank correlation between methods’ performances across tasks (overall Spearman ρ around 0.66 and higher among top methods)—and the observation that Binoculars, originally a detector, achieves state-of-the-art performance on MIAs. The work releases MINT, a unified evaluation suite with implementations, configurations, and datasets to standardize comparisons. While the empirical evidence is persuasive and broadly reproducible, the central theoretical claim has significant issues: Λ is defined as a ratio of negative log-likelihoods, which is not the uniformly most powerful likelihood ratio test statistic for MGTD, and the membership inference optimality argument lacks a rigorous training algorithm/dataset model and conflates asymptotic model fit with the distribution of “in” examples.

Strengths
- Clear empirical contribution and practical impact:
  - Comprehensive, carefully engineered evaluation across tasks: 15 methods (7 MIAs, 5 detectors among them), many domains and multiple generators/models, covering both white-box and black-box detection.
  - Strong evidence of cross-task transferability, with substantial rank correlation across methods; methods that approximate a two-distribution contrast tend to perform well on both tasks.
  - Identification of Binoculars as a strong MIA method provides an actionable baseline for privacy auditing.
  - Empirical MIA results align with recent literature: small but consistent gains (typical AUROCs around 0.53–0.63), increasing with model scale.
  - Black-box detection results against ChatGPT/GPT-4 are informative, with MIAs like Binoculars and Min-K% competitive.
- Unifying perspective:
  - A useful taxonomy that reinterprets diverse methods as approximations to a common distributional contrast (reference-based or sampling-based), bringing clarity to a fragmented literature and facilitating cross-community understanding.
  - Illuminating connections among methods (e.g., DetectGPT and Neighborhood) that help explain transfer patterns.
- Diagnostic analyses and insights:
  - The Zlib case study effectively surfaces failure modes and underscores differences in priors/problem formulations between MI and MGTD.
- Reproducibility and tooling:
  - Public release of MINT with code, standardized configurations, and benchmarks enables fair comparison and future extensions.
  - Thorough documentation of implementation details and hyperparameters.

Weaknesses
- Theoretical issues with the central unification:
  - For MGTD, the uniformly most powerful test for H0: x ∼ PQ vs H1: x ∼ PM thresholds the likelihood ratio pM(x)/pQ(x), equivalently the log-likelihood difference log pM(x) − log pQ(x) (or LQ − LM). The paper defines Λ as a ratio of negative log-likelihoods L(x;M)/L(x;Q) and invokes Neyman–Pearson optimality, but this statistic is not generally monotone in the true likelihood ratio. As stated, the “exactly coincides” optimality claim is incorrect.
  - For MIAs, the theoretical argument assumes that cross-entropy training makes L(x;M) converge to P(x|in), conflating model fit to PQ with the distribution of included examples; in the standard asymptotic regime, M → PQ, so membership signals vanish without an explicit dataset/algorithm inclusion model. The claim of asymptotic optimality for MIAs lacks a precise threat model and appears invalid under standard learning assumptions.
  - The conditions under which Λ would be optimal are vague, and certain tested methods (e.g., Min-k%, DC-PDD) do not fit cleanly into the proposed LRT framework.
- Evaluation design and reporting concerns:
  - White-box MGTD is near-trivial under the chosen setup (access to target logits), yielding AUROCs near 99–100. These ceiling effects limit discriminative power, risk inflating cross-task rank correlations, and provide limited insight into realistic detection scenarios.
  - Aggregation and reporting inconsistencies (e.g., “eight domains and three generators” vs “eight domains and five models,” differing accounts of models/generators between text and figure captions) make it hard to interpret averaged metrics and the exact composition underlying the reported correlations.
  - Lack of uncertainty quantification: no confidence intervals for AUROC or for rank correlations; this is especially important when MIA AUROCs are close together.
- Limited robustness analyses:
  - Sparse ablations on key hyperparameters and design choices (e.g., k in Min-K%, perturbation strength/generator for DetectGPT/Neighborhood, reference model strength for Binoculars).
  - Insufficient analysis of factors that can drive apparent transferability, such as sequence length, domain effects, or surrogate-model mismatch in black-box detection.
  - No calibration-sensitive metrics or operating point analyses (e.g., FPR at low TPR), which would be informative for privacy risk assessment.
- Presentation and notation:
  - Notational slippage in calling Λ a likelihood ratio while defining it on negative log-likelihoods; sign and monotonicity need clarification.
  - Minor typos and labeling inconsistencies (e.g., “LLaSDe++”) and formatting artifacts in appendices.
