### Summary

The paper introduces **SafeAgents**, a framework designed to evaluate the security vulnerabilities in **multi-agent LLM systems (MAS)** under adversarial prompting. The authors present **DHARMA**, a diagnostic metric that helps identify weak links in MAS by classifying failures at various levels (planner, sub-agent, orchestration). Their analysis covers five widely used MAS architectures (centralized, decentralized, and hybrid) and four benchmarks that span different types of web tasks, tool use, and code generation. The findings highlight significant vulnerabilities within common design patterns, revealing how design decisions such as autonomy levels, task decomposition, and context sharing affect the security of these systems.

---

### Strengths

1. **Important Research Gap**: The paper addresses the critical issue of security in **multi-agent systems**, a largely underexplored area compared to single-agent systems. The shift from single-agent to multi-agent systems brings new security risks, which this paper seeks to investigate.

2. **Unified Framework for Security Assessment**: The introduction of **SafeAgents** offers a novel and comprehensive framework for evaluating **MAS security** across various architectures, providing a **unified metric** (DHARMA) to assess vulnerabilities in a granular way.

3. **Valuable Findings**: The study uncovers actionable insights, such as the discovery that **centralized systems** with low sub-agent autonomy are more vulnerable because delegating only atomic tasks can obscure harmful objectives from sub-agents. This finding is crucial for system designers aiming to mitigate vulnerabilities in **MAS**.

4. **Empirical Evaluation**: The paper performs an **extensive empirical study** across multiple **MAS architectures** and **datasets**, offering concrete evidence of the vulnerabilities introduced by different system designs.

---

### Weaknesses

1. **Reliability of the DHARMA Metric (LLM-as-Judge)**:

   * The paper depends on an **LLM-as-judge** for classifying failures in multi-agent systems, which raises concerns about **reproducibility** and **reliability**. The authors did not provide validation studies for the accuracy of the LLM judge used in DHARMA.
   * While the authors conducted a **validation study** with **human-LLM agreement**, the absence of a second judge model and error bars leaves the results vulnerable to **judge-specific artifacts**. The paper needs a clearer evaluation of **inter-rater reliability**.

2. **Internal Inconsistencies in DHARMA Taxonomy**:

   * The **DHARMA classification** includes categories that are not fully mutually exclusive. For example, the **Planner-Failed** category simultaneously describes both **plan generation failure** and cases where **execution continues despite a valid plan**, which introduces ambiguity.
   * The authors need to clarify this inconsistency in their taxonomy and provide **explicit rules** for **state transitions** and **tie-breaker priorities**.

3. **Lack of Mechanistic Analysis**:

   * While the paper attributes security issues to **design primitives** (e.g., sub-agent autonomy, context sharing), it does not fully **disentangle** these variables from other system-level configurations. The authors argue that **sub-agent autonomy** is a critical factor, but other system design choices (e.g., **planning strategy**, **context organization**) also vary across frameworks, making it difficult to pinpoint the root cause of security vulnerabilities.

4. **Limited Novelty in Contribution**:

   * The paper's main contribution is a **framework for security evaluation** and a **taxonomy of failures** in MAS, which provides valuable insights but lacks **algorithmic innovation**. While the study highlights important findings, the novelty primarily lies in **system design** and **taxonomic classification** rather than introducing new algorithms or mechanisms for securing multi-agent systems.

5. **Writing and Clarity**:

   * The **writing** and **clarity** of the paper need improvement. There are instances of **self-contradictory sentences** (e.g., "planner fails to generate a valid plan, yet continues despite a valid plan") and grammatical issues that affect readability and understanding.
   * **Redundancy** in discussions (e.g., "Integration Complexity" vs. "Lack of Systematic Comparison") could be streamlined to improve focus and coherence.

6. **Insufficient Attack Model Comparison**:

   * The paper presents results across **four benchmarks**, but it lacks an **explicit comparison across different threat models**. The authors should analyze which **MAS architectures** perform best under specific types of **adversarial attacks** (e.g., **prompt injection**, **code execution**) to make the findings more actionable for the research community.
