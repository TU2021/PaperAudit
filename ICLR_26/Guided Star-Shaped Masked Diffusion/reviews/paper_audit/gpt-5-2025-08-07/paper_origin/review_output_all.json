{
  "baseline_review": "Summary\n- The paper proposes Guided Star-Shaped Masked Diffusion (G-Star) for discrete diffusion models, addressing the irreversibility of masked diffusion (Equation 2, Section 2). It reformulates the forward process into a star-shaped structure where all latents are conditionally independent given x0 (Equation 4, Section 3.1) and samples by predicting x̂0 and remasking via q(·|x̂0) (Equation 5). A lightweight learned error predictor gφ targets likely erroneous tokens (Algorithms 1–2, Section 3.2). The method is designed to be compatible with pre-trained MDLMs (Section 3.1, Equation 6; Appendix A). Experiments on OpenWebText (Section 4, Figure 2, Table 1), code generation (Table 2), and a 7B instruction-tuned model (Table 3) show improved MAUVE and perplexity, especially in few-step regimes (Figure 3) and loop-based refinement schedules (Section 4.4; Table 1). Ablations cover activation timing t_on (Figure 2), loop size (Appendix C.2, Figure 5), and predictor temperature (Appendix C.3, Figure 6).Strengths\n- Boldly motivated problem and clear formulation of irreversibility in masked diffusion\n  - The paper precisely identifies the core limitation of absorbing masked diffusion—deterministic preservation of unmasked tokens—via Equation 2 (Section 2), which prevents error correction. This clear articulation strengthens the problem framing (clarity/impact).\n  - The connection to prior remedial approaches (ReMDM schedules in Equation 3, Section 2) contextualizes the need for targeted revision (clarity/novelty).\n  - The empirical manifestation of early-stage instability under pure star sampling (Figure 1 right, Section 4.2) aligns the theoretical limitation with observed dynamics (technical soundness/impact).- Star-shaped reformulation with compatible training objective\n  - The star-shaped joint factorization q(x1:T|x0)=∏t q(xt|x0) (Equation 4, Section 3.1) enables non-monotonic transitions and revisiting earlier decisions (novelty/technical soundness).\n  - The sampling rule pθ(xt−1|xt)=q(xt−1|x0=x̂0) (Equation 5, Section 3.1) operationalizes iterative refinement, allowing remasking and correction (technical soundness/impact).\n  - Claim 1 (Equation 6, Section 3.1; Appendix A derivation Equations 7–17) shows the VLB reduces to a weighted cross-entropy with timestep-dependent weights, suggesting compatibility with MDLM training (technical soundness/practical impact).- Targeted, learnable error correction module\n  - The error predictor gφ is trained to flag tokens likely mispredicted by fθ (Algorithm 1, Section 3.2), a principled approach to focus corrections (novelty/impact).\n  - Guided remasking uses Gumbel-Top-K sampling over error logits (Algorithm 2 steps 7–10, Section 3.2), improving correction efficiency per step (technical soundness/experimental rigor).\n  - Qualitative visualizations (Appendix E–F, Figure 7 and Figures 8–9) indicate coherent phrase-level edits emerging from guided clusters, beyond token-wise randomness (clarity/impact).- Comprehensive analysis isolating effective regimes and schedules\n  - Two-phase hypothesis validated: Star performs poorly early but excels in late refinement; optimal switch at t_on≈0.3 (Figure 2, Section 4.2), matching the observed similarity/perplexity dynamics (Figure 1, Section 4.2) (experimental rigor/clarity).\n  - Few-step regime advantage of guidance (Figure 3, Section 4.3): G-Star+ consistently outperforms unguided Star+ and MDLM on MAUVE/PPL (technical soundness/impact).\n  - Loop schedule study (Section 4.4; Table 1) shows strong refinement at constant α and the quality–diversity trade-off with step budgets (Appendix C.2, Figure 5) (experimental rigor/impact).- Strong empirical results across tasks with parameter-efficient guidance\n  - On OWT (Table 1), guided loop configurations (G-Star-loop variants) reach MAUVE 57.3 at 128 steps and PPL 9.9 at 512 steps, outperforming tuned ReMDM in low-step regimes (experimental strength/impact).\n  - On Conala (Table 2), G-Star-loop and G-Star+ reduce conditional PPL against Qwen2.5B-Coder, indicating improved fluency and relevance (impact/experimental rigor).\n  - On Dream-Instruct 7B (Table 3; Appendix D.2–D.3), freezing the 7B backbone and training only a head yields consistent improvements across seven benchmarks, showcasing practical applicability at scale (impact/practicality).- Parameter efficiency and practical guidance\n  - Head-only predictor (12B, H) performs comparably to full fine-tuning (12B, F) (Table 1, Section 4.5), minimizing overhead (practical impact).\n  - Heuristic advice—allocate ~10% of steps to refinement loops—emerges from ablations (Section 4.4 Practical implications), aiding practitioners (clarity/impact).\n  - Temperature analysis (Appendix C.3, Figure 6) provides a tunable trade-off between diversity and quality, with MAUVE peaking at T≈4–32 (clarity/practicality).Weaknesses\n- Limited theoretical rigor and equivalence claims\n  - The text asserts mathematical equivalence to ReMDM for a specific σt (“mathematically equivalent… when σt = 1 − αs”, Section 3.1) without a formal derivation or theorem; pointers to Appendix C.1 and Section 4.4 are empirical/tuning-focused rather than proofs (novelty/technical soundness). No direct evidence found in the manuscript.\n  - The proof of Claim 1 (Appendix A, Equations 11–14) simplifies the KL by assuming x0 is a non-mask token and reduces to −αt−1 log pθ(x0|xt), but does not discuss edge cases (e.g., sequences with [MASK] present in x0) or conditions under which the simplification holds exactly (technical completeness).\n  - The claim that pre-trained MDLM weights can be reused “without any fine-tuning” (Section 3.1, after Equation 6) is not substantiated with an explicit experiment isolating zero fine-tuning of fθ; the OWT setup notes fine-tuning a public MDLM checkpoint (Section 4.1), and predictor fine-tuning is reported (Section 4.5). No direct evidence found in the manuscript.- Reproducibility and reporting gaps\n  - The OWT fine-tuning details for the baseline MDLM used in Section 4 (e.g., learning rate, optimizer, training steps, temperatures, seed counts) are not provided in Section 4.1; Appendix D focuses on Conala and Dream-Instruct but not OWT (experimental rigor/reproducibility). No direct evidence found in the manuscript.\n  - Statistical significance or variance reporting is limited; although Figure 3 shows shaded regions, the number of seeds and statistical tests are not described, and Table 3 improvements are small (e.g., +0.1–2.9 points) without confidence intervals (experimental rigor).\n  - Inference-time and training-time overheads of gφ are described qualitatively as “negligible” and “lightweight” (Section 4.5), but no runtime, memory, or wall-clock metrics are reported across OWT, Conala, or Dream-Instruct (experimental rigor/practicality). No direct evidence found in the manuscript.- Baseline coverage and metric choices\n  - Comparative baselines emphasize ReMDM variants (Section 4.4; Table 1) but do not include other recent inference-time correctors/guidance for discrete diffusion cited in the references (e.g., Schiff et al., 2024; Nisonoff et al., 2024; Zhao et al., 2024b), limiting breadth (experimental rigor/impact). No direct evidence found in the manuscript.\n  - The Dream-Instruct baseline uses the repository’s “entropy” selection (Appendix D.2) while the proposed approach adds a trained head and loop steps; more advanced baseline decoders or scaling recipes (Appendix D.2 cites inference-time settings but not stronger baselines) are not considered (fairness/impact).\n  - Conala evaluation relies on conditional PPL under Qwen2.5B-Coder (Table 2); standard code metrics (pass@k on HumanEval/MBPP, functional correctness) are not used for Conala, making practical impact harder to assess (experimental rigor/impact).- Consistency issues in reported results\n  - Table 1 reports identical numbers for “ReMDM-loop η=0.1” and “Star-loop” across steps (e.g., MAUVE 67.7, PPL 20.7, DIV 39.3 at 512 steps), which may indicate result reuse for theoretical equivalence rather than independent runs; this raises concerns about empirical validation of distinct samplers (experimental rigor/clarity) (Table 1).\n  - The unusually low PPL 11.7 at 128 steps for “Star+t_on=0.2” (Table 1) is not explained; this is substantially lower than other methods and merits clarification (experimental rigor).\n  - Differences between protocols (Star+, Star-loop, G-Star+) are not fully cross-referenced in early analysis figures (Figure 1 and Figure 2 focus on Star+/MDLM, while Table 1 introduces loop schedules); the mapping between regimes may confuse readers (clarity).- Missing or ambiguous method/hyperparameter details\n  - Algorithm 2 introduces pnucleus, τdenoiser, τremask and NucleusFilter (Section 3.2), but default values and ablations for OWT are not specified in Section 4; Appendix D covers Dream-Instruct values and Conala specifics, leaving OWT unaddressed (clarity/reproducibility). No direct evidence found in the manuscript.\n  - The rule N=⌈(1−αt−1)·L⌉ (Algorithm 2 step 8, Section 3.2) conflicts with Appendix D.2 where N=15 tokens are remasked per refinement step in Dream-Instruct, potentially indicating heuristic overrides without systematic justification (clarity/technical soundness).\n  - The choice of α_on per benchmark (Table 4) lacks rationale or sensitivity analysis beyond a general heuristic (Section 4.4 Practical implications), limiting guidance for practitioners (clarity/practicality).- Scope limitations and sensitivity\n  - The method cannot perform insertion/deletion edits (Appendix G), constraining the types of corrections in natural language and code, especially for structural errors (impact).\n  - The error predictor requires a separate training stage (Appendix G), adding pipeline complexity; joint training or end-to-end alternatives are not explored (practicality).\n  - The performance is sensitive to predictor temperature T_remask (Appendix C.3, Figure 6), with visible quality–diversity trade-offs; while a range (T≈4–32) is suggested, auto-tuning or task-adaptive strategies are not provided (practicality/robustness).Suggestions for Improvement\n- Strengthen theoretical grounding and equivalence claims\n  - Provide a formal theorem and proof of equivalence between star-shaped sampling and ReMDM under the specified σt (Section 3.1), including assumptions and boundary conditions; include derivations analogous to Appendix A.\n  - Extend Appendix A to cover edge cases (e.g., presence of [MASK] in x0, multi-token dependencies) and clarify conditions ensuring the weighted cross-entropy form (Equations 11–14).\n  - Add an explicit experiment demonstrating reusing pre-trained MDLM weights “without any fine-tuning” of fθ (Section 3.1), isolating the effect of gφ and confirming performance parity; if fine-tuning is necessary, revise the claim accordingly.- Improve reproducibility and reporting\n  - Document full OWT training/sampling configurations (Section 4.1), including optimizer, learning rates, training steps, temperatures, and seed counts; align with the detail level in Appendix D for Conala/Dream-Instruct.\n  - Report variance across multiple seeds and include statistical significance tests for the gains in Table 1–3; describe the shaded regions in Figure 3 and their computation.\n  - Provide runtime and memory overhead measurements for gφ during training and inference across OWT, Conala, and Dream-Instruct; include wall-clock comparisons at matched step budgets and publish code for replication.- Expand baselines and adopt standard metrics\n  - Include comparisons to discrete guidance/corrector baselines cited (e.g., Schiff et al., 2024; Nisonoff et al., 2024; Zhao et al., 2024b) under the same step budgets and schedules (Section 4.4; Table 1).\n  - For Dream-Instruct, test stronger baseline decoders or alternative schedule recipes (Appendix D.2), ensuring fairness by matching any additional training or refinement applied to G-Star.\n  - On Conala (Table 2), report functional correctness metrics (e.g., exact match or pass@k) in addition to PPL to substantiate practical gains; optionally add HumanEval/MBPP-style evaluations for the trained Conala model.- Clarify and verify reported results\n  - Re-run “Star-loop” independently from “ReMDM-loop η=0.1” and report distinct results or add a footnote explaining exact identity due to proven equivalence; ensure all entries in Table 1 reflect independent experiments with seed counts.\n  - Investigate and explain the low PPL 11.7 for “Star+t_on=0.2” at 128 steps (Table 1), including diagnostics, seeds, and protocol differences; add an ablation isolating the effect of t_on on PPL.\n  - Cross-reference protocol differences (Star+, Star-loop, G-Star+) earlier in Section 4, and add a summary figure/table mapping regimes to the figures and Table 1 to improve reader navigation.- Specify and ablate method hyperparameters\n  - Provide default values and OWT ablations for pnucleus, τdenoiser, τremask (Algorithm 2), paralleling Appendix D’s specificity; include sensitivity plots showing their effect on MAUVE/PPL/DIV.\n  - Reconcile the N selection rule (Algorithm 2, step 8) with the fixed N=15 used in Appendix D.2 by stating when each heuristic applies; add an experiment varying N under Dream-Instruct to validate robustness.\n  - Justify α_on choices (Table 4) with sensitivity analysis per benchmark, potentially referencing Figure 6-like curves where α_on affects the balance of diversity/quality.- Address scope limitations and robustness\n  - Explore insertion/deletion operations within the star-shaped framework, e.g., by augmenting q(xt−1|x̂0) with shift operators or learned edit types; add preliminary experiments (Appendix G).\n  - Investigate joint or end-to-end training of fθ and gφ to reduce pipeline complexity and potentially improve synergy; report training stability and gains.\n  - Propose an automatic temperature tuning strategy for T_remask (Appendix C.3, Figure 6), such as optimizing MAUVE on a validation split or adapting temperature to predicted error distribution entropy; provide default heuristics per task.Score\n- Overall (10): 7 — The paper introduces a practical star-shaped sampler with targeted guidance (Equations 4–5; Algorithms 1–2) and demonstrates consistent empirical gains (Figure 3; Table 1–3), but theoretical equivalence and reporting gaps temper confidence (Section 3.1; Appendix A/C/D).\n- Novelty (10): 7 — Adapting star-shaped sampling to masked discrete diffusion and learning an error-targeted remask module (Section 3.1–3.2) is a meaningful contribution, though related refinement/guidance ideas exist and formal equivalence to ReMDM is not proven (Section 2–3; references).\n- Technical Quality (10): 6 — Analyses and ablations are strong (Figure 2; Appendix C.2–C.3), but missing proofs for equivalence (Section 3.1), limited reproducibility details (Section 4.1), and metric/baseline coverage reduce rigor (Table 1–3; Appendix D).\n- Clarity (10): 8 — The paper is well-written with clear algorithms (Algorithms 1–2), illustrative figures (Figure 1–3; Appendix E–F), and practical guidance (Section 4.4), though some protocol/result consistency issues remain (Table 1; Section 4).\n- Confidence (5): 4 — High familiarity with discrete diffusion and MDLM literature; conclusions grounded in provided figures/tables/equations, but some claims lack formal evidence (Section 3.1; Appendix A/C), and reproducibility details are incomplete (Section 4.1; Appendix D).",
  "final_review": "Summary\n- The paper proposes Guided Star-Shaped Masked Diffusion (G-Star) for discrete diffusion models, addressing irreversibility in masked diffusion (Equation 2, Section 2). It reformulates the forward process into a star-shaped structure with conditionally independent latents given x0 (Equation 4, Section 3.1) and samples by predicting x̂0 and remasking via q(·|x̂0) (Equation 5). A lightweight learned error predictor gφ targets likely erroneous tokens (Algorithms 1–2, Section 3.2). The method is designed to be compatible with pre-trained MDLMs (Section 3.1, Equation 6; Appendix A). Experiments on OpenWebText (Section 4, Figure 2, Table 1), code generation (Table 2), and a 7B instruction-tuned model (Table 3) show improved MAUVE and perplexity, especially in few-step regimes (Figure 3) and loop-based refinement schedules (Section 4.4; Table 1). Ablations cover activation timing t_on (Figure 2), loop size (Appendix C.2, Figure 5), and predictor temperature (Appendix C.3, Figure 6).Strengths\n- Boldly motivated problem and clear formulation of irreversibility in masked diffusion\n  - The paper precisely identifies the core limitation of absorbing masked diffusion—deterministic preservation of unmasked tokens—via Equation 2 (Section 2), which prevents error correction. This clear articulation strengthens the problem framing (clarity/impact).\n  - The connection to prior remedial approaches (ReMDM schedules in Equation 3, Section 2) contextualizes the need for targeted revision (clarity/novelty).\n  - The empirical manifestation of early-stage instability under pure star sampling (Figure 1 right, Section 4.2) aligns the theoretical limitation with observed dynamics (technical soundness/impact).\n- Star-shaped reformulation with compatible training objective\n  - The star-shaped joint factorization q(x1:T|x0)=∏t q(xt|x0) (Equation 4, Section 3.1) enables non-monotonic transitions and revisiting earlier decisions (novelty/technical soundness).\n  - The sampling rule pθ(xt−1|xt)=q(xt−1|x0=x̂0) (Equation 5, Section 3.1) operationalizes iterative refinement, allowing remasking and correction (technical soundness/impact).\n  - Claim 1 (Equation 6, Section 3.1; Appendix A derivation Equations 7–17) shows the VLB reduces to a weighted cross-entropy with timestep-dependent weights, suggesting compatibility with MDLM training (technical soundness/practical impact).\n- Targeted, learnable error correction module\n  - The error predictor gφ is trained to flag tokens likely mispredicted by fθ (Algorithm 1, Section 3.2), a principled approach to focus corrections (novelty/impact).\n  - Guided remasking uses Gumbel-Top-K sampling over error logits (Algorithm 2 steps 7–10, Section 3.2), improving correction efficiency per step (technical soundness/experimental rigor).\n  - Qualitative visualizations (Appendix E–F, Figure 7 and Figures 8–9) indicate coherent phrase-level edits emerging from guided clusters, beyond token-wise randomness (clarity/impact).\n- Comprehensive analysis isolating effective regimes and schedules\n  - Two-phase hypothesis validated: Star performs poorly early but excels in late refinement; optimal switch at t_on≈0.3 (Figure 2, Section 4.2), matching the observed similarity/perplexity dynamics (Figure 1, Section 4.2) (experimental rigor/clarity).\n  - Few-step regime advantage of guidance (Figure 3, Section 4.3): G-Star+ consistently outperforms unguided Star+ and MDLM on MAUVE/PPL (technical soundness/impact).\n  - Loop schedule study (Section 4.4; Table 1) shows strong refinement at constant α and the quality–diversity trade-off with step budgets (Appendix C.2, Figure 5) (experimental rigor/impact).\n- Strong empirical results across tasks with parameter-efficient guidance\n  - On OWT (Table 1), guided loop configurations (G-Star-loop variants) reach MAUVE 57.3 at 128 steps and PPL 9.9 at 512 steps, outperforming tuned ReMDM in low-step regimes (experimental strength/impact).\n  - On Conala (Table 2), G-Star-loop and G-Star+ reduce conditional PPL against Qwen2.5B-Coder, indicating improved fluency and relevance (impact/experimental rigor).\n  - On Dream-Instruct 7B (Table 3; Appendix D.2–D.3), freezing the 7B backbone and training only a head yields consistent improvements across seven benchmarks, showcasing practical applicability at scale (impact/practicality).\n- Parameter efficiency and practical guidance\n  - Head-only predictor (12B, H) performs comparably to full fine-tuning (12B, F) (Table 1, Section 4.5), minimizing overhead (practical impact).\n  - Heuristic advice—allocate ~10% of steps to refinement loops—emerges from ablations (Section 4.4 Practical implications), aiding practitioners (clarity/impact).\n  - Temperature analysis (Appendix C.3, Figure 6) provides a tunable trade-off between diversity and quality, with MAUVE peaking at T≈4–32 (clarity/practicality).Weaknesses\n- Limited theoretical rigor and equivalence claims\n  - The text asserts mathematical equivalence to ReMDM for a specific σt (“mathematically equivalent… when σt = 1 − αs”, Section 3.1), but the variable s is undefined and no formal derivation/theorem is provided; pointers to Appendix C.1 and Section 4.4 are empirical/tuning-focused rather than proofs (novelty/technical soundness). No direct evidence found in the manuscript.\n  - The proof of Claim 1 (Appendix A, Equations 11–14) simplifies the KL by assuming x0 is a non-mask token and reduces to −αt−1 log pθ(x0|xt), but does not discuss edge cases (e.g., sequences with [MASK] present in x0) or clarify whether x̂0 is treated as a soft distribution vs one-hot consistently (technical completeness/clarity).\n  - The claim that pre-trained MDLM weights can be reused “without any fine-tuning” (Section 3.1, after Equation 6) is not substantiated with an explicit experiment isolating zero fine-tuning of fθ; the OWT setup notes fine-tuning a public MDLM checkpoint (Section 4.1), and predictor fine-tuning is reported (Section 4.5). No direct evidence found in the manuscript.\n- Reproducibility and reporting gaps\n  - The OWT fine-tuning details for the baseline MDLM used in Section 4 (e.g., learning rate, optimizer, training steps, temperatures, seed counts) are not provided in Section 4.1; Appendix D focuses on Conala and Dream-Instruct but not OWT (experimental rigor/reproducibility). No direct evidence found in the manuscript.\n  - Statistical significance or variance reporting is limited; although Figure 3 shows shaded regions, the number of seeds and statistical tests are not described, and Table 3 improvements are small (e.g., +0.1–2.9 points) without confidence intervals (experimental rigor).\n  - Inference-time and training-time overheads of gφ are described qualitatively as “negligible” and “lightweight” (Section 4.5), but no runtime, memory, or wall-clock metrics are reported across OWT, Conala, or Dream-Instruct (experimental rigor/practicality). No direct evidence found in the manuscript.\n- Baseline coverage and metric choices\n  - Comparative baselines emphasize ReMDM variants (Section 4.4; Table 1) but do not include other recent inference-time correctors/guidance for discrete diffusion cited in the references (e.g., Schiff et al., 2024; Nisonoff et al., 2024; Zhao et al., 2024b), limiting breadth (experimental rigor/impact). No direct evidence found in the manuscript.\n  - The Dream-Instruct baseline uses the repository’s “entropy” selection (Appendix D.2) while the proposed approach adds a trained head and loop steps; more advanced baseline decoders or scaling recipes (Appendix D.2 cites inference-time settings but not stronger baselines) are not considered (fairness/impact).\n  - Conala evaluation relies on conditional PPL under Qwen2.5B-Coder (Table 2); standard code metrics (pass@k on HumanEval/MBPP, functional correctness) are not used for Conala, making practical impact harder to assess (experimental rigor/impact).\n- Consistency issues in reported results\n  - Table 1 reports identical numbers for “ReMDM-loop η=0.1” and “Star-loop” across steps (e.g., MAUVE 67.7, PPL 20.7, DIV 39.3 at 512 steps), which may indicate result reuse for theoretical equivalence rather than independent runs; this raises concerns about empirical validation of distinct samplers (experimental rigor/clarity) (Table 1).\n  - The unusually low and non-monotonic PPL values for “Star+t_on=0.2” across step budgets (11.7 at 128, 24.5 at 256, 14.7 at 512; Table 1) are not explained and are inconsistent with broader trends (experimental rigor).\n  - Ambiguities in figure/text references: Section 4.2 maps “pure Star” to t_on=1.0 and “pure MDLM” to t_on=0.0 while stating the sampler switches to Star after t_on (Section 4.2; Figure 2), and Section 4.3 mislabels Figure 3 panels (“MAUVE scores (left panel)… perplexity curves (right panel)” whereas left is Perplexity and right is MAUVE; Figures 20–21), risking reader confusion (clarity).\n- Missing or ambiguous method/hyperparameter details\n  - Algorithm 2 introduces pnucleus, τdenoiser, τremask and NucleusFilter (Section 3.2), but default values and ablations for OWT are not specified in Section 4; Appendix D covers Dream-Instruct values and Conala specifics, leaving OWT unaddressed (clarity/reproducibility). No direct evidence found in the manuscript.\n  - The rule N=⌈(1−αt−1)·L⌉ (Algorithm 2 step 8, Section 3.2) conflicts with Appendix D.2 where N=15 tokens are remasked per refinement step in Dream-Instruct, potentially indicating heuristic overrides without systematic justification (clarity/technical soundness).\n  - Algorithms omit explicit time conditioning in some steps (Algorithm 1 step 8 uses fθ(xt) and Algorithm 2 step 4 uses fθ(xt) rather than fθ(xt, t), and q(·|x0) in Algorithm 1 step 7 does not show dependence on t; Sections 3.2 and 3.1), creating ambiguity about whether t is continuous vs discrete and how it enters training/sampling (clarity/reproducibility).\n- Scope limitations and sensitivity\n  - The method cannot perform insertion/deletion edits (Appendix G), constraining the types of corrections in natural language and code, especially for structural errors (impact).\n  - The error predictor requires a separate training stage (Appendix G), adding pipeline complexity; joint training or end-to-end alternatives are not explored (practicality).\n  - The performance is sensitive to predictor temperature T_remask (Appendix C.3, Figure 6), with visible quality–diversity trade-offs; while a range (T≈4–32) is suggested, auto-tuning or task-adaptive strategies are not provided (practicality/robustness).Suggestions for Improvement\n- Strengthen theoretical grounding and equivalence claims\n  - Provide a formal theorem and proof of equivalence between star-shaped sampling and ReMDM under the specified σt (Section 3.1), define the notation precisely (e.g., resolve “σt = 1 − αs”), and state assumptions/boundary conditions; include derivations analogous to Appendix A.\n  - Extend Appendix A to cover edge cases (e.g., presence of [MASK] in x0, multi-token dependencies) and clarify whether x̂0 is treated as a soft distribution in the KL; make the soft-vs-one-hot assumption explicit and justify its use (Appendix A, Equations 10–14).\n  - Add an explicit experiment demonstrating reusing pre-trained MDLM weights “without any fine-tuning” of fθ (Section 3.1), isolating the effect of gφ and confirming performance parity; if fine-tuning is necessary, revise the claim accordingly.\n- Improve reproducibility and reporting\n  - Document full OWT training/sampling configurations (Section 4.1), including optimizer, learning rates, training steps, temperatures, and seed counts; align with the detail level in Appendix D for Conala/Dream-Instruct.\n  - Report variance across multiple seeds and include statistical significance tests for the gains in Table 1–3; describe the shaded regions in Figure 3 and their computation.\n  - Provide runtime and memory overhead measurements for gφ during training and inference across OWT, Conala, and Dream-Instruct; include wall-clock comparisons at matched step budgets and publish code for replication.\n- Expand baselines and adopt standard metrics\n  - Include comparisons to discrete guidance/corrector baselines cited (e.g., Schiff et al., 2024; Nisonoff et al., 2024; Zhao et al., 2024b) under the same step budgets and schedules (Section 4.4; Table 1).\n  - For Dream-Instruct, test stronger baseline decoders or alternative schedule recipes (Appendix D.2), ensuring fairness by matching any additional training or refinement applied to G-Star.\n  - On Conala (Table 2), report functional correctness metrics (e.g., exact match or pass@k) in addition to PPL to substantiate practical gains; optionally add HumanEval/MBPP-style evaluations for the trained Conala model.\n- Clarify and verify reported results\n  - Re-run “Star-loop” independently from “ReMDM-loop η=0.1” and report distinct results or add a footnote explaining exact identity due to proven equivalence; ensure all entries in Table 1 reflect independent experiments with seed counts (Table 1; Section 4.4).\n  - Investigate and explain the low/non-monotonic PPL values for “Star+t_on=0.2” across steps (Table 1), including diagnostics, seeds, and protocol differences; add an ablation isolating the effect of t_on on PPL.\n  - Resolve reference ambiguities by unambiguously defining the t_on switching rule and mapping to “pure” cases (Section 4.2; Figure 2), and correct panel references in Section 4.3 so that Figure 3 text matches left=Perplexity and right=MAUVE (Figures 20–21); add a summary mapping between protocols (Star+, Star-loop, G-Star+) and the figures/tables.\n- Specify and ablate method hyperparameters\n  - Provide default values and OWT ablations for pnucleus, τdenoiser, τremask (Algorithm 2), and clarify the time conditioning in Algorithms 1–2 (discrete vs continuous t, use of fθ(xt, t) and q(·|x0, t)); include sensitivity plots showing their effect on MAUVE/PPL/DIV.\n  - Reconcile the N selection rule (Algorithm 2, step 8) with the fixed N=15 used in Appendix D.2 by stating when each heuristic applies; add an experiment varying N under Dream-Instruct to validate robustness.\n  - Justify α_on choices (Table 4) with sensitivity analysis per benchmark, potentially referencing Figure 6-like curves where α_on affects the balance of diversity/quality.\n- Address scope limitations and robustness\n  - Explore insertion/deletion operations within the star-shaped framework, e.g., by augmenting q(xt−1|x̂0) with shift operators or learned edit types; add preliminary experiments (Appendix G).\n  - Investigate joint or end-to-end training of fθ and gφ to reduce pipeline complexity and potentially improve synergy; report training stability and gains.\n  - Propose an automatic temperature tuning strategy for T_remask (Appendix C.3, Figure 6), such as optimizing MAUVE on a validation split or adapting temperature to predicted error distribution entropy; provide default heuristics per task.Score\n- Overall (10): 7 — Practical star-shaped sampler with targeted guidance (Equations 4–5; Algorithms 1–2) and consistent empirical gains (Figure 3; Table 1–3), but equivalence/notation gaps and reporting inconsistencies temper confidence (Section 3.1; Figure 2; Appendix A/D).\n- Novelty (10): 7 — Adapting star-shaped sampling to masked discrete diffusion and learning an error-targeted remask module (Section 3.1–3.2) is a meaningful contribution, though related refinement/guidance ideas exist and formal equivalence to ReMDM is not proven (Section 2–3; references).\n- Technical Quality (10): 6 — Analyses and ablations are strong (Figure 2; Appendix C.2–C.3), but missing proofs for equivalence (Section 3.1), limited reproducibility details (Section 4.1), and metric/baseline coverage reduce rigor (Table 1–3; Appendix D).\n- Clarity (10): 7 — Clear algorithms (Algorithms 1–2) and illustrative figures (Figure 1–3; Appendix E–F), but contradictions/misreferences (Section 4.2; Figure 2; Section 4.3 panels) and table duplications (Table 1) affect readability.\n- Confidence (5): 4 — High familiarity with discrete diffusion and MDLM literature; conclusions grounded in provided figures/tables/equations, but some claims lack formal evidence (Section 3.1; Appendix A) and reproducibility details are incomplete (Section 4.1; Appendix D).",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 7,
        "novelty": 7,
        "technical_quality": 6,
        "clarity": 8,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 7,
        "novelty": 7,
        "technical_quality": 6,
        "clarity": 7,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "Summary\n- The paper proposes Guided Star-Shaped Masked Diffusion (G-Star) for discrete diffusion models, addressing irreversibility in masked diffusion (Equation 2, Section 2). It reformulates the forward process into a star-shaped structure with conditionally independent latents given x0 (Equation 4, Section 3.1) and samples by predicting x̂0 and remasking via q(·|x̂0) (Equation 5). A lightweight learned error predictor gφ targets likely erroneous tokens (Algorithms 1–2, Section 3.2). The method is designed to be compatible with pre-trained MDLMs (Section 3.1, Equation 6; Appendix A). Experiments on OpenWebText (Section 4, Figure 2, Table 1), code generation (Table 2), and a 7B instruction-tuned model (Table 3) show improved MAUVE and perplexity, especially in few-step regimes (Figure 3) and loop-based refinement schedules (Section 4.4; Table 1). Ablations cover activation timing t_on (Figure 2), loop size (Appendix C.2, Figure 5), and predictor temperature (Appendix C.3, Figure 6).Strengths\n- Boldly motivated problem and clear formulation of irreversibility in masked diffusion\n  - The paper precisely identifies the core limitation of absorbing masked diffusion—deterministic preservation of unmasked tokens—via Equation 2 (Section 2), which prevents error correction. This clear articulation strengthens the problem framing (clarity/impact).\n  - The connection to prior remedial approaches (ReMDM schedules in Equation 3, Section 2) contextualizes the need for targeted revision (clarity/novelty).\n  - The empirical manifestation of early-stage instability under pure star sampling (Figure 1 right, Section 4.2) aligns the theoretical limitation with observed dynamics (technical soundness/impact).\n- Star-shaped reformulation with compatible training objective\n  - The star-shaped joint factorization q(x1:T|x0)=∏t q(xt|x0) (Equation 4, Section 3.1) enables non-monotonic transitions and revisiting earlier decisions (novelty/technical soundness).\n  - The sampling rule pθ(xt−1|xt)=q(xt−1|x0=x̂0) (Equation 5, Section 3.1) operationalizes iterative refinement, allowing remasking and correction (technical soundness/impact).\n  - Claim 1 (Equation 6, Section 3.1; Appendix A derivation Equations 7–17) shows the VLB reduces to a weighted cross-entropy with timestep-dependent weights, suggesting compatibility with MDLM training (technical soundness/practical impact).\n- Targeted, learnable error correction module\n  - The error predictor gφ is trained to flag tokens likely mispredicted by fθ (Algorithm 1, Section 3.2), a principled approach to focus corrections (novelty/impact).\n  - Guided remasking uses Gumbel-Top-K sampling over error logits (Algorithm 2 steps 7–10, Section 3.2), improving correction efficiency per step (technical soundness/experimental rigor).\n  - Qualitative visualizations (Appendix E–F, Figure 7 and Figures 8–9) indicate coherent phrase-level edits emerging from guided clusters, beyond token-wise randomness (clarity/impact).\n- Comprehensive analysis isolating effective regimes and schedules\n  - Two-phase hypothesis validated: Star performs poorly early but excels in late refinement; optimal switch at t_on≈0.3 (Figure 2, Section 4.2), matching the observed similarity/perplexity dynamics (Figure 1, Section 4.2) (experimental rigor/clarity).\n  - Few-step regime advantage of guidance (Figure 3, Section 4.3): G-Star+ consistently outperforms unguided Star+ and MDLM on MAUVE/PPL (technical soundness/impact).\n  - Loop schedule study (Section 4.4; Table 1) shows strong refinement at constant α and the quality–diversity trade-off with step budgets (Appendix C.2, Figure 5) (experimental rigor/impact).\n- Strong empirical results across tasks with parameter-efficient guidance\n  - On OWT (Table 1), guided loop configurations (G-Star-loop variants) reach MAUVE 57.3 at 128 steps and PPL 9.9 at 512 steps, outperforming tuned ReMDM in low-step regimes (experimental strength/impact).\n  - On Conala (Table 2), G-Star-loop and G-Star+ reduce conditional PPL against Qwen2.5B-Coder, indicating improved fluency and relevance (impact/experimental rigor).\n  - On Dream-Instruct 7B (Table 3; Appendix D.2–D.3), freezing the 7B backbone and training only a head yields consistent improvements across seven benchmarks, showcasing practical applicability at scale (impact/practicality).\n- Parameter efficiency and practical guidance\n  - Head-only predictor (12B, H) performs comparably to full fine-tuning (12B, F) (Table 1, Section 4.5), minimizing overhead (practical impact).\n  - Heuristic advice—allocate ~10% of steps to refinement loops—emerges from ablations (Section 4.4 Practical implications), aiding practitioners (clarity/impact).\n  - Temperature analysis (Appendix C.3, Figure 6) provides a tunable trade-off between diversity and quality, with MAUVE peaking at T≈4–32 (clarity/practicality).Weaknesses\n- Limited theoretical rigor and equivalence claims\n  - The text asserts mathematical equivalence to ReMDM for a specific σt (“mathematically equivalent… when σt = 1 − αs”, Section 3.1), but the variable s is undefined and no formal derivation/theorem is provided; pointers to Appendix C.1 and Section 4.4 are empirical/tuning-focused rather than proofs (novelty/technical soundness). No direct evidence found in the manuscript.\n  - The proof of Claim 1 (Appendix A, Equations 11–14) simplifies the KL by assuming x0 is a non-mask token and reduces to −αt−1 log pθ(x0|xt), but does not discuss edge cases (e.g., sequences with [MASK] present in x0) or clarify whether x̂0 is treated as a soft distribution vs one-hot consistently (technical completeness/clarity).\n  - The claim that pre-trained MDLM weights can be reused “without any fine-tuning” (Section 3.1, after Equation 6) is not substantiated with an explicit experiment isolating zero fine-tuning of fθ; the OWT setup notes fine-tuning a public MDLM checkpoint (Section 4.1), and predictor fine-tuning is reported (Section 4.5). No direct evidence found in the manuscript.\n- Reproducibility and reporting gaps\n  - The OWT fine-tuning details for the baseline MDLM used in Section 4 (e.g., learning rate, optimizer, training steps, temperatures, seed counts) are not provided in Section 4.1; Appendix D focuses on Conala and Dream-Instruct but not OWT (experimental rigor/reproducibility). No direct evidence found in the manuscript.\n  - Statistical significance or variance reporting is limited; although Figure 3 shows shaded regions, the number of seeds and statistical tests are not described, and Table 3 improvements are small (e.g., +0.1–2.9 points) without confidence intervals (experimental rigor).\n  - Inference-time and training-time overheads of gφ are described qualitatively as “negligible” and “lightweight” (Section 4.5), but no runtime, memory, or wall-clock metrics are reported across OWT, Conala, or Dream-Instruct (experimental rigor/practicality). No direct evidence found in the manuscript.\n- Baseline coverage and metric choices\n  - Comparative baselines emphasize ReMDM variants (Section 4.4; Table 1) but do not include other recent inference-time correctors/guidance for discrete diffusion cited in the references (e.g., Schiff et al., 2024; Nisonoff et al., 2024; Zhao et al., 2024b), limiting breadth (experimental rigor/impact). No direct evidence found in the manuscript.\n  - The Dream-Instruct baseline uses the repository’s “entropy” selection (Appendix D.2) while the proposed approach adds a trained head and loop steps; more advanced baseline decoders or scaling recipes (Appendix D.2 cites inference-time settings but not stronger baselines) are not considered (fairness/impact).\n  - Conala evaluation relies on conditional PPL under Qwen2.5B-Coder (Table 2); standard code metrics (pass@k on HumanEval/MBPP, functional correctness) are not used for Conala, making practical impact harder to assess (experimental rigor/impact).\n- Consistency issues in reported results\n  - Table 1 reports identical numbers for “ReMDM-loop η=0.1” and “Star-loop” across steps (e.g., MAUVE 67.7, PPL 20.7, DIV 39.3 at 512 steps), which may indicate result reuse for theoretical equivalence rather than independent runs; this raises concerns about empirical validation of distinct samplers (experimental rigor/clarity) (Table 1).\n  - The unusually low and non-monotonic PPL values for “Star+t_on=0.2” across step budgets (11.7 at 128, 24.5 at 256, 14.7 at 512; Table 1) are not explained and are inconsistent with broader trends (experimental rigor).\n  - Ambiguities in figure/text references: Section 4.2 maps “pure Star” to t_on=1.0 and “pure MDLM” to t_on=0.0 while stating the sampler switches to Star after t_on (Section 4.2; Figure 2), and Section 4.3 mislabels Figure 3 panels (“MAUVE scores (left panel)… perplexity curves (right panel)” whereas left is Perplexity and right is MAUVE; Figures 20–21), risking reader confusion (clarity).\n- Missing or ambiguous method/hyperparameter details\n  - Algorithm 2 introduces pnucleus, τdenoiser, τremask and NucleusFilter (Section 3.2), but default values and ablations for OWT are not specified in Section 4; Appendix D covers Dream-Instruct values and Conala specifics, leaving OWT unaddressed (clarity/reproducibility). No direct evidence found in the manuscript.\n  - The rule N=⌈(1−αt−1)·L⌉ (Algorithm 2 step 8, Section 3.2) conflicts with Appendix D.2 where N=15 tokens are remasked per refinement step in Dream-Instruct, potentially indicating heuristic overrides without systematic justification (clarity/technical soundness).\n  - Algorithms omit explicit time conditioning in some steps (Algorithm 1 step 8 uses fθ(xt) and Algorithm 2 step 4 uses fθ(xt) rather than fθ(xt, t), and q(·|x0) in Algorithm 1 step 7 does not show dependence on t; Sections 3.2 and 3.1), creating ambiguity about whether t is continuous vs discrete and how it enters training/sampling (clarity/reproducibility).\n- Scope limitations and sensitivity\n  - The method cannot perform insertion/deletion edits (Appendix G), constraining the types of corrections in natural language and code, especially for structural errors (impact).\n  - The error predictor requires a separate training stage (Appendix G), adding pipeline complexity; joint training or end-to-end alternatives are not explored (practicality).\n  - The performance is sensitive to predictor temperature T_remask (Appendix C.3, Figure 6), with visible quality–diversity trade-offs; while a range (T≈4–32) is suggested, auto-tuning or task-adaptive strategies are not provided (practicality/robustness).Suggestions for Improvement\n- Strengthen theoretical grounding and equivalence claims\n  - Provide a formal theorem and proof of equivalence between star-shaped sampling and ReMDM under the specified σt (Section 3.1), define the notation precisely (e.g., resolve “σt = 1 − αs”), and state assumptions/boundary conditions; include derivations analogous to Appendix A.\n  - Extend Appendix A to cover edge cases (e.g., presence of [MASK] in x0, multi-token dependencies) and clarify whether x̂0 is treated as a soft distribution in the KL; make the soft-vs-one-hot assumption explicit and justify its use (Appendix A, Equations 10–14).\n  - Add an explicit experiment demonstrating reusing pre-trained MDLM weights “without any fine-tuning” of fθ (Section 3.1), isolating the effect of gφ and confirming performance parity; if fine-tuning is necessary, revise the claim accordingly.\n- Improve reproducibility and reporting\n  - Document full OWT training/sampling configurations (Section 4.1), including optimizer, learning rates, training steps, temperatures, and seed counts; align with the detail level in Appendix D for Conala/Dream-Instruct.\n  - Report variance across multiple seeds and include statistical significance tests for the gains in Table 1–3; describe the shaded regions in Figure 3 and their computation.\n  - Provide runtime and memory overhead measurements for gφ during training and inference across OWT, Conala, and Dream-Instruct; include wall-clock comparisons at matched step budgets and publish code for replication.\n- Expand baselines and adopt standard metrics\n  - Include comparisons to discrete guidance/corrector baselines cited (e.g., Schiff et al., 2024; Nisonoff et al., 2024; Zhao et al., 2024b) under the same step budgets and schedules (Section 4.4; Table 1).\n  - For Dream-Instruct, test stronger baseline decoders or alternative schedule recipes (Appendix D.2), ensuring fairness by matching any additional training or refinement applied to G-Star.\n  - On Conala (Table 2), report functional correctness metrics (e.g., exact match or pass@k) in addition to PPL to substantiate practical gains; optionally add HumanEval/MBPP-style evaluations for the trained Conala model.\n- Clarify and verify reported results\n  - Re-run “Star-loop” independently from “ReMDM-loop η=0.1” and report distinct results or add a footnote explaining exact identity due to proven equivalence; ensure all entries in Table 1 reflect independent experiments with seed counts (Table 1; Section 4.4).\n  - Investigate and explain the low/non-monotonic PPL values for “Star+t_on=0.2” across steps (Table 1), including diagnostics, seeds, and protocol differences; add an ablation isolating the effect of t_on on PPL.\n  - Resolve reference ambiguities by unambiguously defining the t_on switching rule and mapping to “pure” cases (Section 4.2; Figure 2), and correct panel references in Section 4.3 so that Figure 3 text matches left=Perplexity and right=MAUVE (Figures 20–21); add a summary mapping between protocols (Star+, Star-loop, G-Star+) and the figures/tables.\n- Specify and ablate method hyperparameters\n  - Provide default values and OWT ablations for pnucleus, τdenoiser, τremask (Algorithm 2), and clarify the time conditioning in Algorithms 1–2 (discrete vs continuous t, use of fθ(xt, t) and q(·|x0, t)); include sensitivity plots showing their effect on MAUVE/PPL/DIV.\n  - Reconcile the N selection rule (Algorithm 2, step 8) with the fixed N=15 used in Appendix D.2 by stating when each heuristic applies; add an experiment varying N under Dream-Instruct to validate robustness.\n  - Justify α_on choices (Table 4) with sensitivity analysis per benchmark, potentially referencing Figure 6-like curves where α_on affects the balance of diversity/quality.\n- Address scope limitations and robustness\n  - Explore insertion/deletion operations within the star-shaped framework, e.g., by augmenting q(xt−1|x̂0) with shift operators or learned edit types; add preliminary experiments (Appendix G).\n  - Investigate joint or end-to-end training of fθ and gφ to reduce pipeline complexity and potentially improve synergy; report training stability and gains.\n  - Propose an automatic temperature tuning strategy for T_remask (Appendix C.3, Figure 6), such as optimizing MAUVE on a validation split or adapting temperature to predicted error distribution entropy; provide default heuristics per task.Score\n- Overall (10): 7 — Practical star-shaped sampler with targeted guidance (Equations 4–5; Algorithms 1–2) and consistent empirical gains (Figure 3; Table 1–3), but equivalence/notation gaps and reporting inconsistencies temper confidence (Section 3.1; Figure 2; Appendix A/D).\n- Novelty (10): 7 — Adapting star-shaped sampling to masked discrete diffusion and learning an error-targeted remask module (Section 3.1–3.2) is a meaningful contribution, though related refinement/guidance ideas exist and formal equivalence to ReMDM is not proven (Section 2–3; references).\n- Technical Quality (10): 6 — Analyses and ablations are strong (Figure 2; Appendix C.2–C.3), but missing proofs for equivalence (Section 3.1), limited reproducibility details (Section 4.1), and metric/baseline coverage reduce rigor (Table 1–3; Appendix D).\n- Clarity (10): 7 — Clear algorithms (Algorithms 1–2) and illustrative figures (Figure 1–3; Appendix E–F), but contradictions/misreferences (Section 4.2; Figure 2; Section 4.3 panels) and table duplications (Table 1) affect readability.\n- Confidence (5): 4 — High familiarity with discrete diffusion and MDLM literature; conclusions grounded in provided figures/tables/equations, but some claims lack formal evidence (Section 3.1; Appendix A) and reproducibility details are incomplete (Section 4.1; Appendix D)."
}