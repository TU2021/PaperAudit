{
  "baseline_review": "1) Summary\nThe paper addresses the limitation of standard masked diffusion models, where irreversible token generation prevents iterative error correction. The authors propose Guided Star-Shaped Masked Diffusion (G-Star), a novel sampling framework compatible with pre-trained models. The method first reformulates the generation process using a \"star-shaped\" paradigm, where each reverse step involves predicting the clean data and then re-noising it, inherently allowing for token revision. To make this process efficient, they introduce a lightweight, learnable error predictor that guides the re-masking to target likely incorrect tokens. The authors demonstrate empirically that G-Star significantly improves sample quality, particularly in few-step generation regimes, for tasks including text and code generation. They also show its ability to enhance the performance of a large-scale, 7B parameter instruction-tuned language model on various downstream benchmarks.2) Strengths\n*   **Principled and Well-Motivated Method for Error Correction:** The paper introduces a clear and theoretically sound approach to enable iterative refinement in masked diffusion. The shift from a standard Markovian process to a star-shaped one (Section 3.1) is a principled way to break the chain of irreversible decisions. Augmenting this with a learned error predictor (Section 3.2) is a logical and intelligent improvement over prior random re-masking strategies.\n    *   The method is shown to be compatible with pre-trained models, as the star-shaped variational lower bound simplifies to a weighted cross-entropy objective similar to the standard one (Claim 1, Appendix A). This is a significant practical advantage.\n    *   The paper clearly connects its unguided sampler to the ReMDM framework, showing it is equivalent to a specific ReMDM configuration but without the need for sensitive hyperparameter tuning (Section 3.1, Appendix C.1).\n    *   The training procedure for the error predictor is straightforward and intuitive, involving simulating the denoising process and training a classifier to spot mismatches with the ground truth (Algorithm 1).*   **Thorough and Insightful Ablation Studies:** The authors provide an excellent deconstruction of their method, offering deep insights into why and when each component is effective. This analytical rigor significantly strengthens the paper.\n    *   The analysis of the two distinct generation phases (structure-building vs. late-stage refinement) is particularly compelling. Figures 1 and 2 clearly demonstrate that a hybrid approach—using the standard MDLM sampler initially and switching to the star-shaped sampler for refinement—is optimal (Section 4.2).\n    *   The paper effectively demonstrates the value of guided re-masking over an unguided approach, especially in computationally constrained, few-step regimes (Figure 3, Section 4.3).\n    *   The analysis of the error predictor's capacity shows that a lightweight, parameter-efficient model is sufficient, confirming the efficiency of the guidance mechanism (Section 4.5, Table 1).\n    *   The appendices provide valuable analyses of the quality-diversity trade-off, controlled by the refinement loop size (Appendix C.2, Figure 5) and the error predictor temperature (Appendix C.3, Figure 6), offering practical guidance for users.*   **Comprehensive and Strong Empirical Evaluation:** The experiments are extensive, covering multiple domains and scales, and robustly demonstrate the proposed method's effectiveness and versatility.\n    *   G-Star shows substantial gains over strong baselines like MDLM and ReMDM on unconditional text generation, especially in low-step scenarios (128 and 256 steps), where it achieves a much better MAUVE score (Table 1).\n    *   The evaluation on conditional code generation using the Conala benchmark shows that the method generalizes beyond natural language, achieving lower (better) perplexity than baselines (Table 2, Section 5.2).\n    *   A key strength is the successful application of G-Star to a 7B instruction-tuned model (Dream-Instruct). The method yields consistent improvements across a diverse set of seven downstream benchmarks, including complex reasoning (MMLU, GPQA) and instruction following (IFEval) tasks (Table 3, Section 5.1). This demonstrates the real-world impact and scalability of the proposed sampler.3) Weaknesses\n*   **Clarity of the \"Loop\" Scheduling Implementation:** The paper's best results rely on a \"loop schedule\" protocol, but the implementation details are somewhat fragmented, making it difficult to fully grasp the experimental setup and its implications for computational cost.\n    *   The core description of the three-phase loop schedule is in Section 4.4, but crucial parameters, such as the specific allocation of steps for each phase, are only mentioned in an example in the appendix (Appendix C.2).\n    *   The main text provides a general heuristic to \"allocate 10% of the total sampling steps to the refinement loop\" (Section 4.4), but it is not specified how the remaining 90% of steps are divided between the initial drafting and final completion phases.\n    *   For the large-scale model evaluation, the interaction between the G-Star loop schedule and the baseline's \"one mask is denoised at each step\" schedule is unclear (Appendix D.2). It is not explicitly stated whether the total number of forward passes is held constant, which is critical for a fair comparison of efficiency.*   **Insufficient Quantification of Computational Overhead:** While the paper highlights the parameter efficiency of the error predictor (Section 4.5), it does not provide a clear analysis of the overall computational overhead of the G-Star sampling procedure compared to baselines.\n    *   Each G-Star sampling step requires a forward pass through both the main model `fθ` and the error predictor `gφ` (Algorithm 2). The additional cost of the `gφ` pass is not quantified in terms of wall-clock time or FLOPs.\n    *   It is ambiguous whether the \"Steps\" reported in tables (e.g., Table 1, Table 2) refer to the number of denoising timesteps or the total number of model forward passes. If the loop schedule involves more forward passes for the same number of \"steps,\" the comparison of sample quality versus compute is not direct.\n    *   The training of the error predictor is an additional stage in the overall pipeline (Section G). The paper does not report the computational cost of this stage (e.g., in GPU hours), which is necessary for a complete understanding of the method's resource requirements.*   **Potential Brittleness of the Hybrid Schedule and Hyperparameters:** The method's strong performance appears to depend on a carefully configured hybrid schedule, which may introduce new sensitivities, partially undermining the criticism of ReMDM's tuning-dependent nature.\n    *   The analysis of the `t_on` hyperparameter for the Star+ sampler shows a sharp performance peak around `t_on ≈ 0.3` (Figure 2), suggesting that finding the optimal transition point could require careful tuning.\n    *   The method introduces several new hyperparameters, including the loop size, the noise level for the loop (`α_on`), and the re-masking temperature (`τ_remask`). The paper provides a heuristic for loop size (10% of steps) but does not fully explore the sensitivity to these parameters across different tasks.\n    *   The choice of `α_on` for the various benchmarks in the large-scale evaluation (Table 4) is presented without justification, leaving it unclear whether these values were found through extensive tuning or a more robust heuristic.4) Suggestions for Improvement\n*   **Consolidate and Clarify Scheduling Details:**\n    *   In the main text (e.g., Section 4.4), please provide a clear, self-contained description or pseudocode for the full G-Star-loop sampling algorithm. This should explicitly define how the total step budget is allocated among the initial, loop, and final phases.\n    *   To ensure fair and unambiguous comparisons, please clarify in the captions or main text of all experimental tables (Table 1, 2, 3) whether \"Steps\" refers to the number of denoising timesteps or the total number of forward passes of the main model.\n    *   In Appendix D.2, please elaborate on how the loop schedule was integrated into the baseline's sampling process for the 7B model, confirming whether the total number of forward passes was kept identical.*   **Provide an Explicit Analysis of Computational Overhead:**\n    *   Please add a brief analysis quantifying the inference overhead of the error predictor `gφ` relative to the main model `fθ`. This could be reported as a percentage increase in wall-clock time per step or in FLOPs.\n    *   Confirm that the \"Steps\" axis in all plots and tables corresponds to an equivalent computational budget (e.g., total forward passes) across all compared methods. If not, this should be explicitly stated and justified.\n    *   In Appendix D, please add a short note on the training cost of the error predictor (e.g., GPU hours on a specific hardware) to provide a complete picture of the method's resource footprint.*   **Discuss Hyperparameter Sensitivity and Selection:**\n    *   Please add a consolidated discussion in the main text or appendix that acknowledges the new hyperparameters introduced (`t_on`, loop size, `α_on`, `τ_remask`) and provides more robust guidance on their selection.\n    *   In Appendix D.2, please provide a justification for the choice of `α_on` values used for the different benchmarks in Table 4. For example, was there a heuristic based on sequence length, or were these values tuned on a validation set?\n    *   To further strengthen the claims of robustness compared to ReMDM, consider adding a brief sensitivity analysis for a key new hyperparameter (e.g., loop size or `α_on`) on one of the main tasks, similar to the analysis provided for ReMDM's `η` in Appendix C.1.5) Score\n*   Overall (10): 8 — The paper presents a novel, well-motivated, and effective method with strong empirical backing across diverse tasks (Tables 1, 2, 3).\n*   Novelty (10): 9 — The combination of the star-shaped paradigm for masked diffusion and a learned, targeted error predictor is a highly novel and impactful contribution (Section 3).\n*   Technical Quality (10): 8 — The method is technically sound and the analysis is thorough (Section 4, Appendix A), though clarity on computational comparisons could be improved.\n*   Clarity (10): 7 — The core ideas are well-explained, but crucial implementation details regarding the sampling schedule and computational budget are scattered and sometimes ambiguous (Section 4.4, Appendix D.2).\n*   Confidence (5): 5 — I am highly confident in my assessment, having thoroughly reviewed the manuscript and its appendices.",
  "final_review": "1) Summary\nThe paper addresses the limitation of standard masked diffusion models, where irreversible token generation prevents iterative error correction. The authors propose Guided Star-Shaped Masked Diffusion (G-Star), a novel sampling framework compatible with pre-trained models. The method first reformulates the generation process using a \"star-shaped\" paradigm, where each reverse step involves predicting the clean data and then re-noising it, inherently allowing for token revision. To make this process efficient, they introduce a lightweight, learnable error predictor that guides the re-masking to target likely incorrect tokens. The authors demonstrate empirically that G-Star improves sample quality, particularly in few-step generation regimes, for tasks including text and code generation. They also show its ability to enhance the performance of a large-scale, 7B parameter instruction-tuned language model on various downstream benchmarks.2) Strengths\n*   **Principled and Well-Motivated Method for Error Correction:** The paper introduces a clear and theoretically sound approach to enable iterative refinement in masked diffusion. The shift from a standard Markovian process to a star-shaped one (Section 3.1) is a principled way to break the chain of irreversible decisions. Augmenting this with a learned error predictor (Section 3.2) is a logical and intelligent improvement over prior random re-masking strategies.\n    *   The method is shown to be compatible with pre-trained models, as the star-shaped variational lower bound simplifies to a weighted cross-entropy objective similar to the standard one (Claim 1, Appendix A). This is a significant practical advantage.\n    *   The paper clearly connects its unguided sampler to the ReMDM framework, showing it is equivalent to a specific ReMDM configuration but without the need for sensitive hyperparameter tuning (Section 3.1, Appendix C.1).\n    *   The training procedure for the error predictor is straightforward and intuitive, involving simulating the denoising process and training a classifier to spot mismatches with the ground truth (Algorithm 1).*   **Detailed Ablation Studies:** The authors provide a detailed deconstruction of their method, offering insights into why and when each component is effective. This analytical rigor, if supported by consistent results, would significantly strengthen the paper.\n    *   The analysis of the two distinct generation phases (structure-building vs. late-stage refinement) is compelling. Figures 1 and 2 demonstrate that a hybrid approach—using the standard MDLM sampler initially and switching to the star-shaped sampler for refinement—is optimal (Section 4.2).\n    *   The paper demonstrates the value of guided re-masking over an unguided approach, especially in computationally constrained, few-step regimes (Figure 3, Section 4.3).\n    *   The analysis of the error predictor's capacity shows that a lightweight, parameter-efficient model can be sufficient, confirming the efficiency of the guidance mechanism (Section 4.5, Table 1).\n    *   The appendices provide valuable analyses of the quality-diversity trade-off, controlled by the refinement loop size (Appendix C.2, Figure 5) and the error predictor temperature (Appendix C.3, Figure 6), offering practical guidance for users.*   **Extensive Empirical Evaluation Across Multiple Domains:** The experiments cover multiple domains and scales, and aim to demonstrate the proposed method's effectiveness and versatility.\n    *   G-Star is reported to show substantial gains over strong baselines like MDLM and ReMDM on unconditional text generation, especially in low-step scenarios (128 and 256 steps), where it achieves a much better MAUVE score (Table 1).\n    *   The evaluation on conditional code generation using the Conala benchmark shows that the method generalizes beyond natural language, achieving lower (better) perplexity than baselines (Table 2, Section 5.2).\n    *   A key strength is the application of G-Star to a 7B instruction-tuned model (Dream-Instruct). The method is reported to yield consistent improvements across a diverse set of seven downstream benchmarks, including complex reasoning (MMLU, GPQA) and instruction following (IFEval) tasks (Table 3, Section 5.1). This demonstrates the potential real-world impact and scalability of the proposed sampler.3) Weaknesses\n*   **Significant Inconsistencies in Reported Experimental Results:** The manuscript contains several critical inconsistencies and unexplained results that undermine the validity of the core empirical claims.\n    *   In Table 1, the results reported for the proposed `Star-loop` sampler are identical across all nine metrics to the results for the baseline `ReMDM-loopη=0.1`. This is highly improbable and is not explained, especially given the claim that the star-shaped formulation avoids the need for tuning `η` (Section 4.4).\n    *   The textual analysis of the error predictor's capacity in Section 4.5 appears to mischaracterize the data in Table 1. The text claims the lightweight predictor is \"slightly less performant\" at 128 steps, but Table 1 shows a large MAUVE gap (44.8 vs. 57.3). It also claims the \"performance gap vanishes at higher step counts,\" but at 256 steps, the lightweight model (MAUVE 65.0) actually outperforms the larger models (63.8 and 60.9).\n    *   There is a direct contradiction between Figure 3 (right panel) and Table 1 regarding the perplexity of the guided (`G-Star+`) versus the unguided (`Star+`) sampler. Figure 3 shows `G-Star+` achieving lower (better) perplexity at 128 steps, while Table 1 reports that `Star+` has a much better perplexity (11.7 vs. 19.5).*   **Clarity of the \"Loop\" Scheduling Implementation:** The paper's best results rely on a \"loop schedule\" protocol, but the implementation details are somewhat fragmented, making it difficult to fully grasp the experimental setup and its implications for computational cost.\n    *   The core description of the three-phase loop schedule is in Section 4.4, but crucial parameters, such as the specific allocation of steps for each phase, are only mentioned in an example in the appendix (Appendix C.2).\n    *   The main text provides a general heuristic to \"allocate 10% of the total sampling steps to the refinement loop\" (Section 4.4), but it is not specified how the remaining 90% of steps are divided between the initial drafting and final completion phases.\n    *   For the large-scale model evaluation, the interaction between the G-Star loop schedule and the baseline's \"one mask is denoised at each step\" schedule is unclear (Appendix D.2). It is not explicitly stated whether the total number of forward passes is held constant, which is critical for a fair comparison of efficiency.*   **Insufficient Quantification of Computational Overhead:** While the paper highlights the parameter efficiency of the error predictor (Section 4.5), it does not provide a clear analysis of the overall computational overhead of the G-Star sampling procedure compared to baselines.\n    *   Each G-Star sampling step requires a forward pass through both the main model `fθ` and the error predictor `gφ` (Algorithm 2). The additional cost of the `gφ` pass is not quantified in terms of wall-clock time or FLOPs.\n    *   It is ambiguous whether the \"Steps\" reported in tables (e.g., Table 1, Table 2) refer to the number of denoising timesteps or the total number of model forward passes. If the loop schedule involves more forward passes for the same number of \"steps,\" the comparison of sample quality versus compute is not direct.\n    *   The training of the error predictor is an additional stage in the overall pipeline (Appendix G). The paper does not report the computational cost of this stage (e.g., in GPU hours), which is necessary for a complete understanding of the method's resource requirements.*   **Potential Brittleness of the Hybrid Schedule and Hyperparameters:** The method's strong performance appears to depend on a carefully configured hybrid schedule, which may introduce new sensitivities, partially undermining the criticism of ReMDM's tuning-dependent nature.\n    *   The analysis of the `t_on` hyperparameter for the Star+ sampler shows a sharp performance peak around `t_on ≈ 0.3` (Figure 2), suggesting that finding the optimal transition point could require careful tuning.\n    *   The method introduces several new hyperparameters, including the loop size, the noise level for the loop (`α_on`), and the re-masking temperature (`τ_remask`). The paper provides a heuristic for loop size (10% of steps) but does not fully explore the sensitivity to these parameters across different tasks.\n    *   The choice of `α_on` for the various benchmarks in the large-scale evaluation (Table 4) is presented without justification, leaving it unclear whether these values were found through extensive tuning or a more robust heuristic.4) Suggestions for Improvement\n*   **Re-verify and Clarify All Experimental Results:**\n    *   Please either explain the identical results for `Star-loop` and `ReMDM-loopη=0.1` in Table 1 or correct them if this is a reporting error. This is crucial for substantiating the claim of being a more robust, tuning-free alternative.\n    *   Please revise the textual analysis in Section 4.5 to accurately reflect the results presented in Table 1, particularly concerning the performance trade-offs of the 1B vs. 12B error predictors.\n    *   Please resolve the contradiction between Figure 3 and Table 1 regarding perplexity. Clarify which result is correct and ensure that all figures, tables, and textual claims are consistent throughout the manuscript.*   **Consolidate and Clarify Scheduling Details:**\n    *   In the main text (e.g., Section 4.4), please provide a clear, self-contained description or pseudocode for the full G-Star-loop sampling algorithm. This should explicitly define how the total step budget is allocated among the initial, loop, and final phases.\n    *   To ensure fair and unambiguous comparisons, please clarify in the captions or main text of all experimental tables (Table 1, 2, 3) whether \"Steps\" refers to the number of denoising timesteps or the total number of forward passes of the main model.\n    *   In Appendix D.2, please elaborate on how the loop schedule was integrated into the baseline's sampling process for the 7B model, confirming whether the total number of forward passes was kept identical.*   **Provide an Explicit Analysis of Computational Overhead:**\n    *   Please add a brief analysis quantifying the inference overhead of the error predictor `gφ` relative to the main model `fθ`. This could be reported as a percentage increase in wall-clock time per step or in FLOPs.\n    *   Confirm that the \"Steps\" axis in all plots and tables corresponds to an equivalent computational budget (e.g., total forward passes) across all compared methods. If not, this should be explicitly stated and justified.\n    *   In Appendix D, please add a short note on the training cost of the error predictor (e.g., GPU hours on a specific hardware) to provide a complete picture of the method's resource footprint.*   **Discuss Hyperparameter Sensitivity and Selection:**\n    *   Please add a consolidated discussion in the main text or appendix that acknowledges the new hyperparameters introduced (`t_on`, loop size, `α_on`, `τ_remask`) and provides more robust guidance on their selection.\n    *   In Appendix D.2, please provide a justification for the choice of `α_on` values used for the different benchmarks in Table 4. For example, was there a heuristic based on sequence length, or were these values tuned on a validation set?\n    *   To further strengthen the claims of robustness compared to ReMDM, consider adding a brief sensitivity analysis for a key new hyperparameter (e.g., loop size or `α_on`) on one of the main tasks, similar to the analysis provided for ReMDM's `η` in Appendix C.1.5) Score\n*   Overall (10): 4 — The paper proposes a novel idea, but the core empirical claims are undermined by significant and unexplained inconsistencies in the reported results (Table 1, Figure 3, Section 4.5).\n*   Novelty (10): 8 — The core concept of a guided, star-shaped sampler for error correction in masked diffusion is novel and well-motivated (Section 3).\n*   Technical Quality (10): 3 — The technical quality is severely compromised by major inconsistencies in the experimental reporting, including duplicated results and contradictions between text, tables, and figures (Table 1, Figure 3, Section 4.5).\n*   Clarity (10): 3 — The paper is difficult to evaluate due to critical contradictions between different parts of the experimental section, which makes the central claims unclear and unsupported (e.g., Figure 3 vs. Table 1).\n*   Confidence (5): 5 — I am highly confident in my assessment, having cross-referenced the text, tables, and figures in the manuscript.",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 8,
        "novelty": 9,
        "technical_quality": 8,
        "clarity": 7,
        "confidence": 5
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 4,
        "novelty": 8,
        "technical_quality": 3,
        "clarity": 3,
        "confidence": 5
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "1) Summary\nThe paper addresses the limitation of standard masked diffusion models, where irreversible token generation prevents iterative error correction. The authors propose Guided Star-Shaped Masked Diffusion (G-Star), a novel sampling framework compatible with pre-trained models. The method first reformulates the generation process using a \"star-shaped\" paradigm, where each reverse step involves predicting the clean data and then re-noising it, inherently allowing for token revision. To make this process efficient, they introduce a lightweight, learnable error predictor that guides the re-masking to target likely incorrect tokens. The authors demonstrate empirically that G-Star improves sample quality, particularly in few-step generation regimes, for tasks including text and code generation. They also show its ability to enhance the performance of a large-scale, 7B parameter instruction-tuned language model on various downstream benchmarks.2) Strengths\n*   **Principled and Well-Motivated Method for Error Correction:** The paper introduces a clear and theoretically sound approach to enable iterative refinement in masked diffusion. The shift from a standard Markovian process to a star-shaped one (Section 3.1) is a principled way to break the chain of irreversible decisions. Augmenting this with a learned error predictor (Section 3.2) is a logical and intelligent improvement over prior random re-masking strategies.\n    *   The method is shown to be compatible with pre-trained models, as the star-shaped variational lower bound simplifies to a weighted cross-entropy objective similar to the standard one (Claim 1, Appendix A). This is a significant practical advantage.\n    *   The paper clearly connects its unguided sampler to the ReMDM framework, showing it is equivalent to a specific ReMDM configuration but without the need for sensitive hyperparameter tuning (Section 3.1, Appendix C.1).\n    *   The training procedure for the error predictor is straightforward and intuitive, involving simulating the denoising process and training a classifier to spot mismatches with the ground truth (Algorithm 1).*   **Detailed Ablation Studies:** The authors provide a detailed deconstruction of their method, offering insights into why and when each component is effective. This analytical rigor, if supported by consistent results, would significantly strengthen the paper.\n    *   The analysis of the two distinct generation phases (structure-building vs. late-stage refinement) is compelling. Figures 1 and 2 demonstrate that a hybrid approach—using the standard MDLM sampler initially and switching to the star-shaped sampler for refinement—is optimal (Section 4.2).\n    *   The paper demonstrates the value of guided re-masking over an unguided approach, especially in computationally constrained, few-step regimes (Figure 3, Section 4.3).\n    *   The analysis of the error predictor's capacity shows that a lightweight, parameter-efficient model can be sufficient, confirming the efficiency of the guidance mechanism (Section 4.5, Table 1).\n    *   The appendices provide valuable analyses of the quality-diversity trade-off, controlled by the refinement loop size (Appendix C.2, Figure 5) and the error predictor temperature (Appendix C.3, Figure 6), offering practical guidance for users.*   **Extensive Empirical Evaluation Across Multiple Domains:** The experiments cover multiple domains and scales, and aim to demonstrate the proposed method's effectiveness and versatility.\n    *   G-Star is reported to show substantial gains over strong baselines like MDLM and ReMDM on unconditional text generation, especially in low-step scenarios (128 and 256 steps), where it achieves a much better MAUVE score (Table 1).\n    *   The evaluation on conditional code generation using the Conala benchmark shows that the method generalizes beyond natural language, achieving lower (better) perplexity than baselines (Table 2, Section 5.2).\n    *   A key strength is the application of G-Star to a 7B instruction-tuned model (Dream-Instruct). The method is reported to yield consistent improvements across a diverse set of seven downstream benchmarks, including complex reasoning (MMLU, GPQA) and instruction following (IFEval) tasks (Table 3, Section 5.1). This demonstrates the potential real-world impact and scalability of the proposed sampler.3) Weaknesses\n*   **Significant Inconsistencies in Reported Experimental Results:** The manuscript contains several critical inconsistencies and unexplained results that undermine the validity of the core empirical claims.\n    *   In Table 1, the results reported for the proposed `Star-loop` sampler are identical across all nine metrics to the results for the baseline `ReMDM-loopη=0.1`. This is highly improbable and is not explained, especially given the claim that the star-shaped formulation avoids the need for tuning `η` (Section 4.4).\n    *   The textual analysis of the error predictor's capacity in Section 4.5 appears to mischaracterize the data in Table 1. The text claims the lightweight predictor is \"slightly less performant\" at 128 steps, but Table 1 shows a large MAUVE gap (44.8 vs. 57.3). It also claims the \"performance gap vanishes at higher step counts,\" but at 256 steps, the lightweight model (MAUVE 65.0) actually outperforms the larger models (63.8 and 60.9).\n    *   There is a direct contradiction between Figure 3 (right panel) and Table 1 regarding the perplexity of the guided (`G-Star+`) versus the unguided (`Star+`) sampler. Figure 3 shows `G-Star+` achieving lower (better) perplexity at 128 steps, while Table 1 reports that `Star+` has a much better perplexity (11.7 vs. 19.5).*   **Clarity of the \"Loop\" Scheduling Implementation:** The paper's best results rely on a \"loop schedule\" protocol, but the implementation details are somewhat fragmented, making it difficult to fully grasp the experimental setup and its implications for computational cost.\n    *   The core description of the three-phase loop schedule is in Section 4.4, but crucial parameters, such as the specific allocation of steps for each phase, are only mentioned in an example in the appendix (Appendix C.2).\n    *   The main text provides a general heuristic to \"allocate 10% of the total sampling steps to the refinement loop\" (Section 4.4), but it is not specified how the remaining 90% of steps are divided between the initial drafting and final completion phases.\n    *   For the large-scale model evaluation, the interaction between the G-Star loop schedule and the baseline's \"one mask is denoised at each step\" schedule is unclear (Appendix D.2). It is not explicitly stated whether the total number of forward passes is held constant, which is critical for a fair comparison of efficiency.*   **Insufficient Quantification of Computational Overhead:** While the paper highlights the parameter efficiency of the error predictor (Section 4.5), it does not provide a clear analysis of the overall computational overhead of the G-Star sampling procedure compared to baselines.\n    *   Each G-Star sampling step requires a forward pass through both the main model `fθ` and the error predictor `gφ` (Algorithm 2). The additional cost of the `gφ` pass is not quantified in terms of wall-clock time or FLOPs.\n    *   It is ambiguous whether the \"Steps\" reported in tables (e.g., Table 1, Table 2) refer to the number of denoising timesteps or the total number of model forward passes. If the loop schedule involves more forward passes for the same number of \"steps,\" the comparison of sample quality versus compute is not direct.\n    *   The training of the error predictor is an additional stage in the overall pipeline (Appendix G). The paper does not report the computational cost of this stage (e.g., in GPU hours), which is necessary for a complete understanding of the method's resource requirements.*   **Potential Brittleness of the Hybrid Schedule and Hyperparameters:** The method's strong performance appears to depend on a carefully configured hybrid schedule, which may introduce new sensitivities, partially undermining the criticism of ReMDM's tuning-dependent nature.\n    *   The analysis of the `t_on` hyperparameter for the Star+ sampler shows a sharp performance peak around `t_on ≈ 0.3` (Figure 2), suggesting that finding the optimal transition point could require careful tuning.\n    *   The method introduces several new hyperparameters, including the loop size, the noise level for the loop (`α_on`), and the re-masking temperature (`τ_remask`). The paper provides a heuristic for loop size (10% of steps) but does not fully explore the sensitivity to these parameters across different tasks.\n    *   The choice of `α_on` for the various benchmarks in the large-scale evaluation (Table 4) is presented without justification, leaving it unclear whether these values were found through extensive tuning or a more robust heuristic.4) Suggestions for Improvement\n*   **Re-verify and Clarify All Experimental Results:**\n    *   Please either explain the identical results for `Star-loop` and `ReMDM-loopη=0.1` in Table 1 or correct them if this is a reporting error. This is crucial for substantiating the claim of being a more robust, tuning-free alternative.\n    *   Please revise the textual analysis in Section 4.5 to accurately reflect the results presented in Table 1, particularly concerning the performance trade-offs of the 1B vs. 12B error predictors.\n    *   Please resolve the contradiction between Figure 3 and Table 1 regarding perplexity. Clarify which result is correct and ensure that all figures, tables, and textual claims are consistent throughout the manuscript.*   **Consolidate and Clarify Scheduling Details:**\n    *   In the main text (e.g., Section 4.4), please provide a clear, self-contained description or pseudocode for the full G-Star-loop sampling algorithm. This should explicitly define how the total step budget is allocated among the initial, loop, and final phases.\n    *   To ensure fair and unambiguous comparisons, please clarify in the captions or main text of all experimental tables (Table 1, 2, 3) whether \"Steps\" refers to the number of denoising timesteps or the total number of forward passes of the main model.\n    *   In Appendix D.2, please elaborate on how the loop schedule was integrated into the baseline's sampling process for the 7B model, confirming whether the total number of forward passes was kept identical.*   **Provide an Explicit Analysis of Computational Overhead:**\n    *   Please add a brief analysis quantifying the inference overhead of the error predictor `gφ` relative to the main model `fθ`. This could be reported as a percentage increase in wall-clock time per step or in FLOPs.\n    *   Confirm that the \"Steps\" axis in all plots and tables corresponds to an equivalent computational budget (e.g., total forward passes) across all compared methods. If not, this should be explicitly stated and justified.\n    *   In Appendix D, please add a short note on the training cost of the error predictor (e.g., GPU hours on a specific hardware) to provide a complete picture of the method's resource footprint.*   **Discuss Hyperparameter Sensitivity and Selection:**\n    *   Please add a consolidated discussion in the main text or appendix that acknowledges the new hyperparameters introduced (`t_on`, loop size, `α_on`, `τ_remask`) and provides more robust guidance on their selection.\n    *   In Appendix D.2, please provide a justification for the choice of `α_on` values used for the different benchmarks in Table 4. For example, was there a heuristic based on sequence length, or were these values tuned on a validation set?\n    *   To further strengthen the claims of robustness compared to ReMDM, consider adding a brief sensitivity analysis for a key new hyperparameter (e.g., loop size or `α_on`) on one of the main tasks, similar to the analysis provided for ReMDM's `η` in Appendix C.1.5) Score\n*   Overall (10): 4 — The paper proposes a novel idea, but the core empirical claims are undermined by significant and unexplained inconsistencies in the reported results (Table 1, Figure 3, Section 4.5).\n*   Novelty (10): 8 — The core concept of a guided, star-shaped sampler for error correction in masked diffusion is novel and well-motivated (Section 3).\n*   Technical Quality (10): 3 — The technical quality is severely compromised by major inconsistencies in the experimental reporting, including duplicated results and contradictions between text, tables, and figures (Table 1, Figure 3, Section 4.5).\n*   Clarity (10): 3 — The paper is difficult to evaluate due to critical contradictions between different parts of the experimental section, which makes the central claims unclear and unsupported (e.g., Figure 3 vs. Table 1).\n*   Confidence (5): 5 — I am highly confident in my assessment, having cross-referenced the text, tables, and figures in the manuscript."
}