Summary
The paper proposes Guided Star-Shaped Masked Diffusion (G-Star), a sampling algorithm for pre-trained masked diffusion language models (MDLMs) that reintroduces reversibility and targeted iterative refinement. The key idea is a star-shaped factorization of the forward process, where each noisy state x_t is independently conditioned on x_0, allowing previously committed tokens to be remasked and revised. A lightweight error predictor g_φ is trained to estimate token-level error probabilities by contrasting model hypotheses x̂_0 with ground truth x_0; at inference, high-error tokens are selected (via Gumbel-Top-K with calibrated temperatures and optional nucleus filtering) for remasking and denoising. The training objective for the star-shaped process reduces to a weighted cross-entropy, preserving compatibility with pre-trained MDLMs.

Empirical analyses show a two-phase generation dynamic: star-shaped sampling harms early-stage coherence but improves late-stage refinement. Accordingly, the method performs best when activated late in the trajectory or applied in loop schedules, yielding improvements on OpenWebText (few-step regime and loop refinement), improved conditional perplexity on Conala, and small but consistent gains across Dream-Instruct 7B benchmarks. The paper also offers practical guidance on activation timing and loop allocation (e.g., a simple 10% loop heuristic) and notes that parameter-efficient, head-only training of g_φ can recover most of the gains.

Strengths
- Clear motivation and principled method: addresses MDLM’s irreversibility by adopting a star-shaped formulation that enables remasking of already-decided tokens while remaining compatible with pre-trained models.
- Sound objective and implementation: the training loss derivation for the star-shaped process is correct and reduces to a weighted cross-entropy; the guided remasking pipeline (training labels from x̂_0 ≠ x_0; Gumbel-Top-K selection) is well-specified and operationally coherent.
- Targeted refinement is effective in practice: guidance focuses editing on likely errors, yielding strong benefits in few-step regimes and during late-stage or loop refinement, with clear analyses on activation timing and loop schedules.
- Comprehensive analysis of phases and schedules: empirical evidence supports a two-phase hypothesis (early drafting vs late refinement), along with a documented quality–diversity trade-off under loop refinement.
- Breadth and practicality: demonstrated across unconditional text, code, and instruction-tuned LMs; improvements are consistent though modest on downstream tasks; parameter-efficient head-only training performs competitively.
- Generally clear presentation with concise algorithms and informative figures/appendices, including visualizations of remasking patterns and ablations on schedule choices.

Weaknesses
- Table inconsistency: Table 1 appears to duplicate “Star-loop” results with “ReMDM-loop η=0.1,” undermining comparative clarity and credibility. This likely labeling/error must be corrected and validated.
- Incomplete theory: the claimed equivalence between the star-shaped sampler and ReMDM under a particular remasking probability (σ_t = 1 − α_s) is asserted but not formally derived. A step-by-step proof mapping the star-shaped posterior to ReMDM’s would strengthen the contribution.
- Missing runtime evidence: despite efficiency claims, there are no wall-clock or throughput comparisons versus MDLM/ReMDM, nor quantification of the overhead from g_φ scoring and Gumbel-Top-K selection. Practical deployment benefits remain unsubstantiated.
- Limited baseline coverage: while ReMDM variants are compared, other informed correctors or discrete guidance methods cited in related work are not included in empirical comparisons, limiting the breadth of evaluation.
- Statistical rigor: downstream gains on Dream-Instruct are small and no confidence intervals or significance tests are reported, leaving open whether improvements exceed run-to-run variance.
- Diversity trade-offs: the method can reduce diversity in loop regimes; while this is acknowledged and partially analyzed, practical user controls and policies (e.g., adaptive remask size or thresholds) are not deeply explored.
- Minor clarity gaps: occasional notation ambiguities (σ_t vs α_s) and scattered hyperparameter details (e.g., selection of N, τ_remask, nucleus filtering settings) impede reproducibility and should be centralized and cross-referenced across experiments.
