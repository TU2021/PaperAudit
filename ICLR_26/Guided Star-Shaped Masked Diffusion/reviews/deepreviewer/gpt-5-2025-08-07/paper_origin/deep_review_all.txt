Summary
The paper proposes Guided Star-Shaped Masked Diffusion (G-Star), a sampling algorithm for pre-trained masked diffusion language models (MDLMs) that enables iterative error correction with targeted remasking. The core idea is to adopt a star-shaped formulation in which all noisy states x_t are independently conditioned on x_0 (Eq. 4–5), thereby permitting remasking of already-unmasked tokens. To make remasking efficient, the authors introduce an error predictor g_φ trained to identify likely token errors produced by the base denoiser f_θ (Algorithms 1–2). Empirically, the method is shown to work best in late-generation phases and under loop schedules; it outperforms baselines (MDLM and ReMDM variants) on OpenWebText (Table 1), improves conditional perplexity on Conala (Table 2), and yields consistent small gains on several Dream-Instruct 7B downstream benchmarks (Table 3).

Soundness
- The star-shaped reparameterization for discrete masked diffusion is coherent and ties to known continuous counterparts (Okhotin et al., 2023), with a clear generative recipe (Sec. 3.1, Eq. 5) and a training objective shown to reduce to a weighted cross-entropy (Claim 1; Appendix A). The derivation in Appendix A is consistent and correctly reduces KL terms to per-timestep weighted CE under categorical interpolations with the mask.
- The guidance mechanism via g_φ is well-defined and operationally sound: it trains on sampled model hypotheses x̂_0 against ground truth x_0 to produce token-level error probabilities; at inference it selects N = ⌈(1−α_{t−1})L⌉ tokens for remasking via Gumbel-Top-K (Algorithm 2). This integrates naturally with the star sampler and aligns with the intuition that late-stage refinement benefits from selective edits.
- The claimed equivalence between the star-shaped sampler and ReMDM under a particular re-masking probability (Sec. 3.1: “mathematically equivalent… when σ_t = 1 − α_s”) is plausible but not formally proved in the paper; the Appendix C.1 analyses focus on η-tuning sensitivity rather than a derivation of equivalence. This leaves a small theoretical gap that would benefit from explicit algebra showing the mapping from Eq. 5 to ReMDM’s posterior (Eq. 3) under the stated σ_t.
- The hybrid schedule findings (Sec. 4.2; Fig. 2, Fig. 1) are internally consistent: early-phase star sampling degrades coherence (lower step-to-step similarity) while late-phase activation reduces perplexity, matching the two-phase hypothesis. The loop-schedule analysis (Sec. 4.4) coherently explains MAUVE dynamics via a quality–diversity trade-off (Appendix C.2).

Presentation
- The paper is clearly structured: motivation (Sec. 1), preliminaries (Sec. 2), method (Sec. 3), analysis and ablations (Sec. 4), broader evaluation (Sec. 5), and conclusion. Algorithms 1–2 succinctly specify training and inference procedures.
- Figures (Fig. 1–3) communicate the two-phase dynamics and the guided-vs-unguided gains. Appendices (A, C, D, E–F) provide proofs, tuning analyses, and qualitative visualizations of remasking patterns (Figures 7–10).
- A notable presentation issue appears in Table 1: the “Star-loop” row duplicates the exact numbers of “ReMDM-loop η=0.1” (Steps 128/256/512 columns), which is likely a copy/paste error or mislabeling. This undermines clarity and requires correction.
- Minor notation/scheduling details (e.g., σ_t vs α_s; selection of N in Algorithm 2; nucleus filtering parameters) could be more consistently cross-referenced with the noise schedules and activation times t_on reported in Sec. 4.2/4.3.

Contribution
- Conceptually, the paper adapts star-shaped diffusion to discrete masked diffusion, enabling reversible sampling compatible with pre-trained MDLMs (Sec. 3.1). While star-shaped diffusion is known in continuous spaces, its discrete masked instantiation with equivalence to a remasking posterior is valuable.
- The main practical novelty lies in targeted error correction via a learned error predictor g_φ (Algorithms 1–2), delivering substantial efficiency/quality improvements in few-step regimes and in loop refinement schedules (Sec. 4.3–4.4).
- The work provides actionable guidance on when and how to switch to refinement (Fig. 2; “10% loop” heuristic, Sec. 4.4/4.5, Appendix C.2/C.3) and demonstrates applicability to code generation and instruction-tuned LLMs with modest gains (Tables 2–3). The parameter-efficient head-only training is a useful observation (Sec. 4.5).

Strengths
- Clear identification of the masked diffusion limitation (irreversible commitments) and a principled method to reintroduce iterative refinement (Sec. 2–3.1).
- Practical, lightweight guidance module that is plug-and-play with pre-trained MDLMs; head-only training suffices (Sec. 4.5; Table 1).
- Thorough empirical analysis of schedules, phases, and trade-offs (Sec. 4.2–4.4; Appendix C.1–C.3), with convincing late-stage benefits and few-step dominance over unguided remasking (Fig. 3).
- Demonstrated breadth: unconditional text (OWT), conditional code (Conala), and integration into a 7B instruction-tuned model with consistent improvements (Sec. 5; Tables 2–3).

Weaknesses
- Table 1 likely contains a labeling/duplication error for “Star-loop” identical to “ReMDM-loop η=0.1,” which obscures comparative interpretation and needs correction.
- The stated mathematical equivalence to ReMDM (Sec. 3.1) is not fully derived; the paper would benefit from a precise, step-by-step proof mapping σ_t to the star posterior and clarifying α_s vs α_t notation.
- Runtime and wall-clock efficiency are asserted but not reported quantitatively (no throughput or latency comparisons vs MDLM/ReMDM), especially important given g_φ scoring and Gumbel-Top-K sampling overhead (Sec. 3.2).
- Statistical significance for downstream improvements (Table 3) is not reported; gains are modest (e.g., +0.1 on GSM8K, +1.3 on MMLU) and could be within variance.
- Limited baselines: other informed correctors/discrete guidance methods (e.g., Zhao et al., 2024b; Schiff et al., 2024; Nisonoff et al., 2024) are cited but not empirically compared.
- The diversity reduction is acknowledged (Sec. 4.4, Appendix C.2/C.3) but practical implications for creative generation are not deeply explored; a user-tunable trade-off policy would strengthen applicability.

Questions
- Can the authors provide a formal derivation of the claimed equivalence between the star-shaped sampler and ReMDM at σ_t = 1 − α_s, starting from Eq. (3) and Eq. (5)?
- Is the duplication in Table 1 (“Star-loop” matching “ReMDM-loop η=0.1”) a typographical error? If so, what are the correct Star-loop numbers?
- What is the wall-clock overhead of g_φ in practice (token-level scoring + Gumbel-Top-K), and how does total decoding time compare to MDLM/ReMDM at equal steps?
- How sensitive is performance to the exact choice of N (Algorithm 2, Step 8)? Have alternatives (e.g., learning N or adaptive N via confidence thresholds) been tried?
- For Table 3, can the authors report confidence intervals across runs or bootstrap tests to establish significance of the observed gains?
- Could informed correctors or discrete guidance baselines be added for a broader empirical comparison in loop regimes?

Rating
- Overall (10): 7 — Strong practical idea with clear analyses and consistent gains, but a table error (Table 1) and missing formal equivalence proof in Sec. 3.1/Appendix limit confidence.
- Novelty (10): 6 — Discrete masked instantiation of star-shaped diffusion plus a learned token error predictor is useful but conceptually incremental over ReMDM and prior guidance; see Sec. 3.1 and Algorithms 1–2.
- Technical Quality (10): 7 — Sound derivation of the training objective (Appendix A), coherent algorithms and schedules (Sec. 3–4), yet lacking runtime metrics and full equivalence proof; Table 1 duplication needs correction.
- Clarity (10): 7 — Clear structure and helpful figures (Fig. 1–3), but the Table 1 issue and occasional notation ambiguities (σ_t vs α_s; Algorithm 2 N) should be fixed.
- Confidence (5): 4 — I reviewed equations (Eq. 1–6), algorithms (Alg. 1–2), and appendices; the results appear plausible, but the noted table inconsistency reduces confidence slightly.



Summary
This paper introduces a reversible, guided sampling strategy for masked diffusion LMs. The star-shaped formulation (q(x_1:T|x_0) factorized; Eq. 4–5) allows remasking of previously fixed tokens, and a lightweight error predictor g_φ targets likely mistakes (Algorithms 1–2). The authors show that late-stage activation and loop schedules are most beneficial, report strong few-step performance on OWT (Table 1, Fig. 3), improved perplexity for Conala (Table 2), and small yet consistent gains on Dream-Instruct 7B benchmarks (Table 3).

Soundness
- The derivation of the training objective for the star-shaped process is correct and matches the form of weighted CE (Appendix A; Eq. 7–17). The KL simplification to −α_{t−1} log p_θ(x_0|x_t) is standard under categorical interpolations with an absorbing mask.
- The guided remasking pipeline is methodologically sound and aligns with the intuition of focusing edits where f_θ errs most (Algorithm 1 training labels y_i = I(x̂_{0,i} ≠ x_{0,i}); Algorithm 2 targeted sampling via logits_err and Gumbel-Top-K).
- The two-phase hypothesis is substantiated by step-to-step similarity and perplexity dynamics (Fig. 1) and the t_on sweep (Fig. 2), making the late-stage switch well-supported.
- The equivalence statement to ReMDM is asserted (Sec. 3.1) but not formally demonstrated; the analyses in Sec. 4.4/Appendix C.1 deal with η sensitivity rather than showing σ_t mapping to the star posterior.

Presentation
- Exposition is clear and well-organized; algorithms are concise; ablations and appendices are informative (Sec. 4; Appendix C).
- Visualizations of remasking patterns (Appendix F; Figures 7–10) qualitatively support the targeted correction claim.
- A critical presentation flaw exists in Table 1: “Star-loop” appears to duplicate “ReMDM-loop η=0.1” exactly, contradicting text stating the star-shaped sampler performs competitively without η tuning (Sec. 4.4). This must be corrected for clarity and credibility.
- Minor missing details: runtime/latency; exact nucleus filtering parameters (Algorithm 2) and τ_remask settings per experiment; statistical error bars beyond Figure 3.

Contribution
- Practical sampling improvement for masked diffusion via reversible star-shaped transitions and learned token-level error targeting; provides an alternative to hyperparameter-sensitive ReMDM schedules and demonstrates robust few-step refinement.
- Offers actionable guidance on when to switch sampling regimes and how to allocate refinement loops (Sec. 4.2–4.4; Appendix C.2), including a simple “10% loop” heuristic.
- Demonstrates cross-domain applicability (text, code, LLM benchmarks), and parameter-efficient predictors (Sec. 4.5) that retain gains.

Strengths
- Clear motivation and precise method; compatible with pre-trained MDLMs.
- Strong and well-motivated ablations on activation timing, steps, loop size, and temperature (Fig. 2–3; Appendix C.2–C.3).
- Few-step regime advantages are consistent and practically meaningful (Sec. 4.3; Fig. 3).
- Head-only predictor training achieves near parity with full fine-tuning (Table 1; Sec. 4.5).

Weaknesses
- Table 1 duplication/mislabeling (Star-loop vs ReMDM-loop η=0.1) undermines experimental clarity.
- No quantitative runtime analysis; efficiency claims (Sec. 3.2, 4.5) are not backed by wall-clock or throughput measurements.
- Equivalence to ReMDM not formally derived; lack of comprehensive baseline comparisons to informed correctors/discrete guidance (Zhao et al., 2024b; Schiff et al., 2024).
- Diversity reduction can be substantial at high loop sizes; more systematic user controls (beyond τ_remask) and reporting of impact on creative tasks would be helpful.

Questions
- Please correct Table 1 and clarify whether “Star-loop” results are distinct from “ReMDM-loop η=0.1.” If identical, why?
- Can you provide a concrete wall-clock speed comparison for MDLM, Star+, G-Star+, and ReMDM across a fixed step budget?
- Could you include additional baselines (e.g., informed correctors, discrete guidance) in the loop regime?
- Can you present a formal proof of the stated equivalence to ReMDM σ_t and clarify α_s vs α_t notation?
- For Dream-Instruct gains (Table 3), can you add statistical significance and report variance across seeds?

Rating
- Overall (10): 7 — Effective method with solid analyses and cross-domain gains, but a key table error (Table 1) and missing runtime/significance reduce impact.
- Novelty (10): 6 — Star-shaped adaptation plus learned error predictor is practical but incremental relative to remasking and guidance ideas; see Sec. 3.1 and Algorithms 1–2.
- Technical Quality (10): 7 — Sound objective (Appendix A) and ablations (Sec. 4), yet lacking formal equivalence proof and runtime metrics; Table 1 needs correction.
- Clarity (10): 7 — Generally clear with strong visuals; the Table 1 duplication and some parameter omissions should be fixed (Sec. 4.4, Algorithm 2).
- Confidence (5): 4 — Careful reading of equations, algorithms, and figures; the table inconsistency lowers confidence slightly but overall evidence is compelling.



Summary
The work tackles the irreversibility of masked diffusion sampling by reparameterizing the forward process into a star-shaped form (Eq. 4–5) and adding a learned error predictor to guide remasking (Algorithms 1–2). It argues for a two-phase generation strategy—draft with MDLM, refine with star-shaped guided remasking—and demonstrates empirical gains in few-step regimes, late-phase refinement, and loop schedules on OWT, Conala, and Dream-Instruct 7B.

Soundness
- The star-shaped factorization is logically consistent and yields a straightforward reverse transition via x̂_0 prediction followed by posterior sampling (Eq. 5). The training objective derivation (Appendix A) is correct and matches standard masked diffusion formulations, preserving compatibility with pre-trained weights.
- The guidance mechanism is plausible and well-specified; training labels y_i are defined by mismatches between sampled x̂_0 and x_0 (Algorithm 1, Step 10), and the inference selection via Gumbel-Top-K without replacement (Algorithm 2) is appropriate for controlling remasking cardinality.
- The empirical two-phase hypothesis is substantiated with step-to-step similarity (Fig. 1 right) and perplexity trajectories (Fig. 1 left), explaining why star-shaped sampling harms early structure but helps late corrections; the t_on sweep (Fig. 2) corroborates the optimal activation window.
- Some theoretical claims (equivalence to ReMDM under σ_t = 1 − α_s) are asserted but not fully derived; the paper should include an explicit mapping to Eq. 3 to avoid ambiguity.

Presentation
- The paper reads well, with a coherent flow and compact algorithmic descriptions. The ablation suite and appendix visualizations are accessible and instructive (Appendix F figures).
- However, Table 1 contains an apparent duplication error: “Star-loop” equals “ReMDM-loop η=0.1” entry-wise, conflicting with narrative claims about robustness without η tuning (Sec. 4.4). This must be corrected.
- Some parameter details are scattered: NucleusFilter/p_nucleus (Algorithm 2) and τ_remask values are not consistently linked to each reported experiment; adding a summary table of hyperparameters per setting would aid reproducibility.

Contribution
- Practical: a guided, reversible sampler for discrete masked diffusion that can be retrofitted to pre-trained MDLMs and improves few-step/loop refinement without schedule-specific hyperparameter brittleness (Appendix C.1).
- Empirical: comprehensive phase scheduling analysis, guidance efficacy in few-step regimes (Fig. 3), and portability to code and instruction-tuned LMs (Tables 2–3).
- Methodological: highlighting quality–diversity trade-offs and providing a usable heuristic (10% loop) for practitioners (Sec. 4.4, 4.5; Appendix C.2/C.3).

Strengths
- Clear diagnosis of MDLM’s irreversibility and an elegant fix via star-shaped sampling compatible with existing models (Sec. 3.1, Eq. 5–6; Claim 1).
- Strong, targeted guidance that improves efficiency particularly at low/medium step budgets (Sec. 4.3; Fig. 3).
- Parameter-efficient predictor design validated (Sec. 4.5; head-only training nearly matches full fine-tuning).
- Breadth of evaluation including code and downstream reasoning/instruction tasks (Sec. 5; Tables 2–3).

Weaknesses
- Table 1 duplication/mislabeling compromises interpretability; exact comparative performance of Star-loop vs ReMDM-loop remains unclear.
- No timing measurements; practical claims of efficiency are not backed by wall-clock results relative to MDLM/ReMDM.
- Limited comparison to other discrete guidance/informed corrector methods; focus is primarily on ReMDM schedules.
- Small magnitude improvements on Dream-Instruct benchmarks (Table 3) lack statistical validation; could be within run variance.

Questions
- Please fix Table 1 and clarify the true Star-loop numbers. If identical to ReMDM-loop η=0.1, why?
- What is the actual decoding-time overhead of g_φ scoring and Gumbel-Top-K per step relative to MDLM and ReMDM?
- Could you provide a formal derivation of the equivalence σ_t mapping and clarify α_s notation?
- How robust is g_φ across domains (text vs code vs instructions)—do predictor heads need task-specific training or can they generalize?
- Can τ_remask and N be adaptively controlled (e.g., via entropy thresholds), and does that improve the quality–diversity trade-off?

Rating
- Overall (10): 7 — A well-motivated, practical improvement with strong analyses and consistent gains, tempered by a table error and missing runtime/significance details.
- Novelty (10): 6 — Guided reversible sampling is valuable but incremental over prior remasking and guidance work; core star-shaped idea is adapted from continuous domains; see Sec. 3.1 and Algorithms 1–2.
- Technical Quality (10): 7 — Solid objective derivation and ablations; needs formal equivalence proof and efficiency measurements; Table 1 issue lowers confidence.
- Clarity (10): 7 — Generally clear and well-illustrated, but Table 1 duplication and scattered hyperparameter specifics should be addressed (Sec. 4.4; Algorithm 2).
- Confidence (5): 4 — Cross-checked equations, algorithms, figures, and appendices; remaining concerns are fixable but currently reduce certainty.



Summary
The paper proposes G-Star, a guided star-shaped sampler for masked diffusion LMs that enables error correction by predicting full x̂_0 at each step (Eq. 5) and selectively remasking tokens flagged by an error predictor (Algorithms 1–2). The method is found to be most effective when activated late in generation or under loop refinement schedules, yielding improved MAUVE/PPL on OWT (Table 1; Figs. 1–3), better Conala conditional perplexity (Table 2), and small but consistent improvements on Dream-Instruct 7B benchmarks (Table 3).

Soundness
- The star-shaped formulation is internally consistent and the training objective reduction (Appendix A) is correct, preserving compatibility with pre-trained MDLMs (Eq. 6).
- The learned error predictor is trained with simulated denoising mismatches and used to sample token positions via calibrated temperatures (τ_denoiser, τ_remask) and nucleus filtering; this pipeline is standard and consistent with the targeted correction premise.
- The phase analysis (Sec. 4.2; Fig. 2) and loop-schedule experiments (Sec. 4.4) support the claim that late-stage refinement is the most beneficial regime; the quality–diversity trade-off is well-documented (Appendix C.2/C.3).
- The claimed equivalence to ReMDM (Sec. 3.1) would be stronger with a formal derivation; currently it is stated but not proved.

Presentation
- Writing is clear and concise; figures illustrate key dynamics; algorithms are easy to follow.
- A serious presentation error exists in Table 1: “Star-loop” matches “ReMDM-loop η=0.1” exactly. This needs immediate correction and rechecking of all reported numbers.
- Reporting lacks runtime metrics and significance testing; some hyperparameters (p_nucleus, τ_remask per task) are not centrally summarized, making replication harder despite Appendix D details.

Contribution
- Practical contribution: reversible masked diffusion sampling guided by token error prediction, reducing reliance on brittle remasking schedules (Appendix C.1) and improving few-step performance.
- Methodological insights: when to switch samplers (t_on), how many loop steps to allocate (10% heuristic), and how temperature controls quality-diversity (Appendix C.3).
- Applicability across domains (text/code/LLM); parameter-efficient predictor training is an attractive deployment feature.

Strengths
- Strong, targeted refinement with clear empirical benefits in constrained step budgets (Fig. 3).
- Compatibility with pre-trained MDLMs and parameter-efficient predictor training (Sec. 3.1, 4.5).
- Comprehensive analyses of schedules and trade-offs, with instructive visualizations (Appendix F; Figures 7–10).
- Consistent improvements on varied tasks (Tables 2–3), supporting generality.

Weaknesses
- Table 1 duplication/mislabeling undermines trust in comparative claims and requires correction.
- No wall-clock efficiency data; given added scoring/sampling, practical deployment benefits are not quantitatively demonstrated.
- Limited baseline breadth beyond ReMDM; informed correctors and discrete guidance baselines are not compared experimentally.
- Dream-Instruct gains are modest and unaccompanied by statistical testing.

Questions
- Please correct Table 1 and explain the duplication; provide verified Star-loop results distinct from ReMDM-loop.
- What is the runtime overhead of g_φ + Gumbel-Top-K per step? Can you report tokens/sec and total latency vs MDLM/ReMDM?
- Can you formally derive the equivalence to ReMDM σ_t and clarify α_s notation?
- How sensitive are results to p_nucleus and τ_denoiser; can you provide a sensitivity analysis?
- For creative generation tasks, can you propose a practical policy (e.g., adaptive τ_remask/N) to manage the diversity reductions?

Rating
- Overall (10): 7 — A useful and well-analyzed sampling improvement with broad applicability, but the table error and missing runtime/significance temper the impact.
- Novelty (10): 6 — Star-shaped adaptation plus learned error targeting is practical but incremental; see Sec. 3.1, Algorithms 1–2, and Appendix C.1.
- Technical Quality (10): 7 — Correct objective derivation and strong ablations; lacks formal equivalence proof and efficiency metrics; Table 1 issue requires fix.
- Clarity (10): 7 — Clear presentation overall; the Table 1 duplication and scattered hyperparameter reporting should be remedied (Sec. 4.4; Appendix D).
- Confidence (5): 4 — Verified equations, algorithms, and appendices; the noted inconsistencies reduce confidence slightly but are fixable.