Here are four distinct reviews of the paper.

***

### **Review 1**

**Summary**

This paper introduces Adaptive SVGD (Ad-SVGD), a novel method for improving the performance of Stein Variational Gradient Descent (SVGD). The core idea is to address the critical sensitivity of SVGD to its kernel parameters. Instead of relying on static heuristics like the median heuristic, Ad-SVGD dynamically adapts the kernel by alternating between the standard SVGD particle update and a gradient ascent step on the Kernelized Stein Discrepancy (KSD) with respect to the kernel parameters. The authors provide a theoretical analysis in the mean-field limit that motivates this approach by showing that maximizing the KSD corresponds to the steepest descent direction for the KL divergence. The method is validated empirically on several tasks, demonstrating significant improvements over standard SVGD, particularly in capturing posterior variance.

**Soundness**

The methodology is sound and well-motivated. The intuition that maximizing the KSD at each step should lead to faster convergence is clearly explained and linked to the foundational theory of SVGD (Block #12, Eq. 2). The proposed algorithm (Algorithm 1) is a direct and practical implementation of this idea. The theoretical analysis (Section 4) correctly extends existing convergence results [32] to the proposed adaptive setting. While the analysis relies on an assumption about the accuracy of the inner KSD maximization (Assumption 4), this is a reasonable idealization for motivating the algorithm, and the strong empirical results (Sections 5, A) provide compelling evidence that the practical implementation works as intended. The experiments are well-designed, comparing against a strong baseline (Med-SVGD) on a variety of problems, including a challenging inverse problem and a standard Bayesian logistic regression benchmark.

**Presentation**

The paper is exceptionally well-written and easy to follow. The motivation is clearly laid out in the introduction, and the necessary background is concisely presented in the preliminaries. The core method is explained with clarity, and Algorithm 1 provides a precise description of the implementation. The figures are a major strength; for example, Figure 3 and Figure 7 vividly illustrate the primary benefit of Ad-SVGD—its ability to avoid the variance collapse that plagues Med-SVGD. The structure is logical, progressing from motivation and theory to detailed empirical validation.

**Contribution**

The paper makes a significant and practical contribution to the field of approximate Bayesian inference. While SVGD is a popular algorithm, its sensitivity to the kernel has been a long-standing practical barrier. This work provides a principled, easy-to-implement, and effective solution. By isolating the effect of kernel selection, the authors convincingly argue that this single factor is a primary determinant of SVGD's performance (Block #6). The proposed Ad-SVGD method has the potential to become the new default for applying SVGD, replacing the often-unreliable median heuristic.

**Strengths**

1.  **Principled and Intuitive Method:** The core idea of maximizing KSD to accelerate KL convergence is elegant and well-grounded in SVGD theory.
2.  **Strong Empirical Performance:** The experiments consistently and convincingly demonstrate that Ad-SVGD outperforms the standard median heuristic, especially in higher dimensions and in its ability to capture posterior uncertainty (e.g., Fig 3, Fig 10, Fig 7).
3.  **Flexibility:** The method is applicable to any parameterized kernel family, and the use of dimension-wise bandwidths is shown to be highly effective (Block #21, Fig 4).
4.  **Clarity of Exposition:** The paper is very well-written, with clear explanations and excellent supporting figures that make the arguments easy to understand.

**Weaknesses**

1.  **Computational Cost:** While the authors note that the overhead is manageable (Block #13), a more detailed analysis of the wall-clock time trade-off would be beneficial for practitioners. For example, how does the runtime of Ad-SVGD compare to Med-SVGD in the experiments presented?
2.  **Hyperparameter Sensitivity:** The method introduces new hyperparameters for the inner optimization loop (e.g., step size `s`, `nstepstheta`). A brief discussion or ablation study on the sensitivity to these parameters would strengthen the paper's practical guidance.

**Questions**

1.  In your experiments, you update the kernel parameters every 100 particle updates for the ODE problem (Block #24). Have you investigated how performance is affected by the frequency of this update? Is there a clear trade-off between computational cost and approximation quality?
2.  The method uses gradient ascent on the KSD. Did you encounter any issues with local minima or instability in this inner optimization, especially for more complex kernel parameterizations or target distributions?
3.  Could you elaborate on the potential of using more advanced kernel families, as mentioned in Block #21? For instance, how would Ad-SVGD work with low-rank or neural network-based kernels?

**Rating**

- Overall (10): 9 — The paper presents a simple, principled, and highly effective solution to a major known problem in SVGD, supported by strong theory and compelling experiments.
- Novelty (10): 8 — While related to prior work on multi-kernel SVGD [1], the proposed continuous optimization over general parameter spaces is a significant and novel extension.
- Technical Quality (10): 9 — The theoretical analysis is solid and the empirical evaluation is thorough, rigorous, and convincing.
- Clarity (10): 10 — The paper is exceptionally clear, well-structured, and effectively uses figures to convey its main points.
- Confidence (5): 5 — I am highly confident in my assessment; I have expertise in this area and have reviewed the paper thoroughly.

***

### **Review 2**

**Summary**

The paper proposes Ad-SVGD, a variant of Stein Variational Gradient Descent that adaptively selects kernel parameters during optimization. The method alternates between particle updates and updating the kernel parameters via gradient ascent on the Kernelized Stein Discrepancy (KSD). The authors claim this approach improves convergence and mitigates the variance underestimation common in SVGD. A convergence analysis is provided in the mean-field limit, and experiments are conducted to compare Ad-SVGD with the standard median heuristic.

**Soundness**

The soundness of the paper's theoretical claims is questionable due to its reliance on the unverified Assumption 4. The central convergence result, Theorem 3, states that the maximal KSD converges to zero, but this hinges on the assumption that the error from the inexact KSD maximization, $\varepsilon_n$, vanishes as $n \to \infty$. The paper provides no argument or proof that the gradient ascent scheme used in practice (Algorithm 1) satisfies this condition. Without this link, the theoretical analysis (Section 4) serves only as motivation for an idealized algorithm (Eq. 9) and does not formally apply to the algorithm that is actually implemented and tested. The KSD objective function with respect to kernel parameters $\theta$ is not necessarily concave, and a simple gradient ascent might get stuck in poor local optima, which would violate Assumption 4. While the empirical results are positive, they do not substitute for a sound theoretical foundation for the proposed practical algorithm.

**Presentation**

The paper is generally well-written, but the organization of the theoretical section could be improved. The key weakness—the gap between the idealized analysis and the practical algorithm—is only acknowledged late in the paper (Section 6, Block #39). This should be discussed more transparently alongside the presentation of the theory in Section 4. The connection between the simplified model in Eq. (9) and the alternating scheme in Eq. (10) and Algorithm 1 could be made more explicit earlier on. The captions for some figures are split or incomplete (e.g., Figure 2, Figure 3, Figure 5), which slightly hinders readability.

**Contribution**

The contribution is moderate. The idea of optimizing kernel parameters is a natural one and has been explored before in a discrete setting (e.g., Ai et al. [1]). The main novelty is the extension to continuous parameter spaces via gradient ascent on the KSD. However, given the theoretical gap, the primary contribution is empirical: the paper demonstrates that this specific heuristic for adapting kernels works well on a few test problems. While useful, this is less impactful than a fully-supported, principled method.

**Strengths**

1.  **Good Empirical Results:** The experiments clearly show that Ad-SVGD can outperform the median heuristic, particularly in avoiding variance collapse in moderate dimensions (e.g., Fig 3b, Table 2).
2.  **Clear Motivation:** The intuition of maximizing KSD to achieve the steepest descent on the KL divergence is well-explained (Block #12).

**Weaknesses**

1.  **Unsupported Theoretical Claims:** The main convergence analysis relies critically on Assumption 4, which is not shown to hold for the proposed gradient ascent procedure. This creates a significant gap between theory and practice.
2.  **Potential Instability of Inner Optimization:** The paper does not discuss the optimization landscape of KSD with respect to the kernel parameters. It is unclear if the gradient ascent is stable or if it reliably finds a good kernel.
3.  **Computational Overhead:** The method introduces an inner optimization loop that adds computational cost. This cost is not thoroughly analyzed or compared to the baseline in terms of wall-clock time.

**Questions**

1.  Can you provide any theoretical argument or empirical evidence to suggest that Assumption 4 holds for the gradient ascent scheme in Algorithm 1? For example, is the function $\theta \mapsto \mathrm{KSD}_{\theta}^2(\mu \mid \pi)$ (for a fixed $\mu$) convex or unimodal for the RBF kernel family you use?
2.  The analysis in Section 4 is for the mean-field limit. How do you expect the KSD maximization to behave in the finite-particle regime? The empirical KSD is a biased estimator of the true KSD, which could complicate the inner optimization.
3.  In Algorithm 1, you perform a fixed number of gradient ascent steps (`nstepstheta`). How sensitive is the algorithm's performance to this number? Does a single step suffice, or are many steps needed to approximate the maximum and satisfy the spirit of Assumption 4?

**Rating**

- Overall (10): 5 — The paper presents an interesting heuristic with good empirical results, but the theoretical claims are not fully substantiated for the practical algorithm.
- Novelty (10): 6 — The idea is a natural extension of prior work, but the application to continuous kernel parameter spaces is a fair contribution.
- Technical Quality (10): 4 — The theoretical analysis is incomplete due to the unverified Assumption 4, which is a major weakness.
- Clarity (10): 7 — The paper is mostly clear, but the disconnect between theory and practice should be handled more transparently.
- Confidence (5): 5 — I am very confident in my assessment, having expertise in variational inference and its theory.

***

### **Review 3**

**Summary**

This paper proposes a method, Ad-SVGD, to automatically tune the kernel bandwidth in Stein Variational Gradient Descent. The standard approach uses a "median heuristic" which often performs poorly, especially when the problem is high-dimensional or requires different length scales in different dimensions. Ad-SVGD replaces this heuristic with an adaptive procedure that optimizes dimension-wise kernel bandwidths by performing gradient ascent on the Kernelized Stein Discrepancy (KSD). The authors show through several experiments, including on an ODE-based inverse problem and Bayesian logistic regression, that their method is much better at capturing the true posterior variance and generally outperforms the median heuristic.

**Soundness**

From a practical perspective, the method appears very sound. The core problem it addresses—the poor performance of the median heuristic—is a real and significant issue for anyone using SVGD. The proposed solution is implemented and tested on a good range of problems. The results are compelling: Ad-SVGD consistently avoids the severe variance underestimation that Med-SVGD suffers from (Fig 3b, Fig 7, Table 1 vs Table 2). The fact that the method works well across different problems (Gaussian mixtures, inverse problems, logistic regression) and dimensions (from 1D up to 55D in the BLR case) is strong evidence of its robustness. The theoretical section provides a good high-level motivation, even if some of the formal details might not be fully ironed out. For a practitioner, the empirical proof is what matters most, and it is very strong here.

**Presentation**

The paper is well-organized and the presentation is clear. Algorithm 1 is easy to understand and implement. The experimental sections are particularly well done. The use of multiple, complementary metrics (Wasserstein distance, MMD, test accuracy, marginal variance plots, Q-Q plots) gives a comprehensive picture of the algorithm's performance. The visualizations are excellent; for example, the comparison of the reconstructed GP in Figure 2 (and Figure 25) and the covariance matrices in Figure 7 (and Figures 40-41) immediately conveys the superiority of Ad-SVGD. The appendix is also very valuable, providing additional experiments that strengthen the paper's claims (e.g., the scaling with dimension in Appendix A.2).

**Contribution**

The paper makes a very valuable practical contribution. It offers a "set-it-and-forget-it" alternative to the brittle median heuristic, making SVGD a more robust and reliable tool for Bayesian inference. The ability to learn dimension-specific bandwidths (Block #21) is a key feature that allows SVGD to handle anisotropic distributions, which is a common scenario in practice. This work could significantly improve the out-of-the-box performance of SVGD and encourage its wider adoption.

**Strengths**

1.  **Solves a Real Practical Problem:** The failure of the median heuristic is a well-known pain point for SVGD users. This paper offers a robust solution.
2.  **Impressive Empirical Gains:** The method shows dramatic improvements in capturing posterior variance (Fig 3b, Fig 7), which is a critical aspect of Bayesian inference.
3.  **Handles Anisotropy:** The use of a product kernel with per-dimension bandwidths is a simple but powerful idea that is shown to work very well (Fig 4).
4.  **Thorough Experiments:** The authors have done an excellent job of validating their method on a diverse set of tasks and using multiple evaluation metrics.

**Weaknesses**

1.  **Lack of Wall-Clock Time Comparison:** The paper states there was "no significant runtime difference" (Block #24) when updating the kernel infrequently. However, a quantitative comparison of wall-clock times would be much more informative. How much slower is Ad-SVGD if the kernel is updated more frequently?
2.  **Guidance on New Hyperparameters:** The method introduces new hyperparameters (`s`, `nstepstheta`, update frequency). The paper provides the values used but offers little general guidance on how to set them. A sensitivity analysis would make the method much easier for others to adopt.
3.  **Code Availability:** The paper does not mention if the code will be made available. Given the practical nature of the contribution, releasing the code would be a great service to the community.

**Questions**

1.  Could you provide a rough wall-clock time comparison for the experiments in Section 5.3 and 5.4? For example, how much longer does one run of Ad-SVGD take compared to Med-SVGD?
2.  How did you choose the step size `s` for the kernel parameter updates? Is there a risk that a poorly chosen `s` could lead to unstable optimization and worse performance than the fixed heuristic?
3.  In Algorithm 1, the decision to update the parameters (`paramupdate`) is left abstract. In your experiments, you used a fixed frequency. Have you considered adaptive criteria for this decision, for example, based on the change in KSD or the particle locations?

**Rating**

- Overall (10): 8 — A very strong paper with a significant practical contribution, backed by extensive and convincing experiments.
- Novelty (10): 7 — The adaptive mechanism is a novel and practical way to address a known limitation of SVGD.
- Technical Quality (10): 8 — The experiments are of high quality; the theory provides good motivation, though it's not the main focus from a practical viewpoint.
- Clarity (10): 9 — The paper is very clearly written, with excellent figures and a logical flow.
- Confidence (5): 4 — I am confident in my assessment, based on my experience applying similar methods in practice.

***

### **Review 4**

**Summary**

This paper presents an adaptive kernel selection strategy for Stein Variational Gradient Descent (SVGD), termed Ad-SVGD. The method aims to overcome the limitations of fixed kernel heuristics by dynamically optimizing kernel parameters. This is achieved by alternating between a standard SVGD update for the particles and a few steps of gradient ascent on the Kernelized Stein Discrepancy (KSD) with respect to the kernel parameters. The authors provide a mean-field convergence analysis that extends prior work to this adaptive setting, showing that the maximal KSD over the considered class of kernels converges to zero. This theoretical motivation is supported by numerical experiments on several Bayesian inference tasks.

**Soundness**

The technical approach is well-conceived. The theoretical analysis in Section 4 correctly identifies the connection between maximizing KSD and maximizing the single-step KL reduction (Block #12). The extension of the convergence proof framework from Salim et al. [32] to the adaptive setting is a valid and non-trivial exercise. The introduction of the idealized scheme (Eq. 9) and the subsequent analysis of the practical, inexact scheme (Eq. 10) is a standard and reasonable way to structure the argument.

The main theoretical weakness is the introduction of Assumption 4, which posits that the error in the KSD maximization sub-problem vanishes. This assumption is crucial for the main convergence result (Theorem 3) but is not justified for the gradient ascent method used in practice. This is a notable gap.

However, the refined analysis in Appendix B is a significant strength. Theorem 4, which leverages a generalized Stein Log-Sobolev Inequality (Assumption 5), provides a much more nuanced picture. It explicitly connects the rate of decay of the optimization error $\varepsilon_n$ to the rate of convergence of the KL divergence. This result is elegant and provides a clear theoretical path forward: proving bounds on $\varepsilon_n$ for the inner optimizer would directly translate to overall convergence guarantees. This part of the analysis is of high technical quality.

**Presentation**

The paper is written with mathematical precision. The notation is standard and clearly defined in Section 2. The flow of the theoretical argument in Section 4 is logical, starting from the fixed-kernel case and building up to the adaptive, inexact case. The proofs of Theorem 3 and Theorem 4 are concise and correct, given their assumptions. The separation of the more advanced theoretical results into Appendix B is appropriate for maintaining the flow of the main text. The empirical sections are also clearly presented, with sufficient detail to understand the experimental setups.

**Contribution**

The paper's contribution is twofold. First, it introduces a practical and effective algorithm, Ad-SVGD. Second, it provides a novel theoretical analysis for SVGD with adaptive kernels. The theoretical novelty lies in analyzing the convergence of the *maximal* KSD (Corollary 2) and, more importantly, in establishing a quantitative relationship between the accuracy of the kernel optimization and the convergence rate of the KL divergence (Theorem 4). This extends our understanding of the geometry of SVGD (cf. [8]) by considering a dynamically changing RKHS. While related to [1], the focus on continuous optimization over a parameter space and the corresponding gradient-based analysis is a distinct and valuable contribution.

**Strengths**

1.  **Novel Theoretical Framework:** The analysis of an SVGD-like algorithm where the underlying geometry (the RKHS) changes at each step is novel and interesting.
2.  **Refined Convergence Analysis:** Theorem 4 in Appendix B is a strong theoretical result, providing explicit iteration complexity bounds that depend on the quality of the inner-loop optimization. This is a sophisticated piece of analysis.
3.  **Relaxed LSI Condition:** The generalized Stein LSI (Assumption 5) is an interesting theoretical object. The requirement that only *one* kernel in the class needs to satisfy the LSI to get a global inequality is a powerful idea that merits further investigation.
4.  **Clear Link Between Theory and Practice:** The method is directly inspired by the theoretical insight that maximizing KSD is beneficial, providing a good example of theory-driven algorithm design.

**Weaknesses**

1.  **Unverified Central Assumption:** As noted, Assumption 4 is the main theoretical weakness. The paper would be significantly stronger if it could provide conditions on the kernel family and target under which gradient ascent on the KSD is guaranteed to converge to the maximum, or at least provide a rate for $\varepsilon_n$.
2.  **Mean-Field Analysis:** The analysis is restricted to the mean-field (infinite-particle) limit. While standard for this line of work, it does not address the complexities of using empirical estimates of KSD with a finite number of particles.

**Questions**

1.  Regarding Assumption 5 (generalized Stein LSI): Is this known to hold for any non-trivial kernel classes $\{k_\theta\}$? For instance, if we have a class of RBF kernels, and we know that the LSI holds for a specific bandwidth, does this imply the generalized LSI holds over the whole class?
2.  The KSD maximization is a non-concave optimization problem in general. Could the authors comment on the structure of the objective function $\theta \mapsto \mathrm{KSD}_{\theta}^2(\mu_n \mid \pi)$ for the product-of-RBF kernels used in the experiments? Does it exhibit properties that make it amenable to gradient ascent?
3.  Theorem 4 provides several convergence rates depending on the decay of $\varepsilon_n$. Based on your empirical results, could you speculate which regime ($\varepsilon_n \leq \bar{\varepsilon}$, polynomial decay, or exponential decay) seems most plausible for the practical Ad-SVGD algorithm?

**Rating**

- Overall (10): 7 — A solid paper with a valuable practical method and a novel, interesting theoretical analysis, despite a key assumption being unverified.
- Novelty (10): 8 — The theoretical analysis of SVGD with adaptive geometry and the refined results in Appendix B are novel contributions.
- Technical Quality (10): 7 — The theoretical development is of high quality, but is penalized by the gap left by Assumption 4. The experiments are well-executed.
- Clarity (10): 9 — The paper is written with high clarity and precision, especially in the mathematical sections.
- Confidence (5): 5 — I am highly confident in my evaluation, based on my expertise in the theory of particle-based inference methods.