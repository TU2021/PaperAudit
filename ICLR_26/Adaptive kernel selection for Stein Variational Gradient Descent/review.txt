### Summary

This paper tackles a well-known limitation of Stein Variational Gradient Descent (SVGD): the algorithmâ€™s performance is highly sensitive to the kernel choice (especially bandwidth), and the widely used median heuristic can be unreliable, particularly in higher-dimensional or multimodal problems. The authors propose **Adaptive SVGD (Ad-SVGD)**, which **adaptively tunes kernel parameters by maximizing the kernelized Stein discrepancy (KSD)** while performing standard SVGD particle updates. The paper provides convergence analysis extending KSD-based results from fixed-kernel SVGD to the adaptive-kernel setting, and demonstrates empirically that Ad-SVGD can mitigate variance collapse / particle degeneracy and improve approximation quality relative to median-heuristic SVGD. Overall, reviewers find the idea intuitive and well-motivated, and appreciate the clean algorithmic modification and generally clear presentation. However, multiple reviewers raise concerns about **limited empirical validation (mostly synthetic/toy settings), insufficient discussion of computational overhead and scalability, incomplete positioning against existing SVGD variants, and theory relying on assumptions that are hard to verify in practice**.

### Strengths

* **Well-motivated problem and intuitive approach:** Kernel choice is a central bottleneck for SVGD, and adapting kernel parameters via KSD maximization is a natural, principled strategy.
* **Simple, lightweight modification:** The method augments SVGD with an additional kernel-parameter optimization step and can be integrated into existing SVGD pipelines without changing the core particle update rule.
* **Theoretical contribution:** Reviewers generally agree the mathematical formulation is clear and the convergence analysis is technically sound within its stated assumptions, extending prior KSD-based analyses to an adaptive kernel setting.
* **Empirical signal of improved robustness:** On the presented synthetic examples (e.g., Gaussian mixtures, inverse problems), Ad-SVGD appears to better preserve posterior variance, reduce collapse, and outperform the median heuristic qualitatively and quantitatively.

### Weaknesses

* **Empirical evaluation is not comprehensive enough:** Several reviewers emphasize that experiments are dominated by toy/synthetic and relatively low-dimensional settings, limiting confidence in real-world usefulness. Stronger evidence would come from standard Bayesian inference benchmarks (e.g., Bayesian logistic regression, Bayesian neural networks, higher-dimensional posteriors).
* **Computational cost and scalability are under-discussed:** The adaptive kernel step introduces extra computation (KSD estimation and kernel derivatives), yet the original evaluation lacks runtime/complexity reporting and clear guidance on trade-offs (e.g., how often to update kernels, how many inner steps).
* **Positioning relative to existing methods is incomplete:** Reviewers note limited comparison to other adaptive/multi-kernel or direction-learning SVGD variants, which makes it harder to assess novelty and practical advantage versus the broader SVGD literature.
* **Theoretical assumptions may be strong and difficult to verify:** Multiple reviewers question the practicality of assumptions required for the convergence guarantees (notably the ability to approximately maximize KSD per iteration, and related regularity/inequality conditions), and request clearer interpretation when these are only approximately satisfied.
* **Claims about efficiency/stronger convergence metrics need tighter support:** One reviewer notes the introduction suggests stronger-metric convergence and faster KL decay, but the original theory emphasizes asymptotic convergence of maximal KSD and does not directly quantify efficiency gains without additional conditions; iteration-complexity comparisons are requested.
* **Minor presentation issues:** Reviewers report typos and ask for clearer discussion of hyperparameter sensitivity (kernel learning rate, number of inner steps) and stability under aggressive kernel updates or poor initialization.
