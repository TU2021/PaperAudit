1) Summary
This paper addresses realistic long-tailed semi-supervised learning (RTSSL), where the class distribution of unlabeled data is unknown and differs from that of the labeled data. The authors propose a two-stage approach. In the first stage, they frame the problem as one of non-ignorable missingness and use a doubly robust (DR) estimator, drawing from semi-parametric efficiency theory, to accurately estimate the unlabeled class distribution. This estimator comes with strong theoretical guarantees of asymptotic normality and efficiency. In the second stage, this accurate distribution estimate is plugged into existing state-of-the-art pseudo-labeling methods like SimPro and BOAT to improve their final classification performance. Experiments on CIFAR-10/100-LT, STL-10, and ImageNet-127 show that the DR estimator is more accurate than alternatives and that the two-stage method consistently improves the classification accuracy of the base SSL algorithms.2) Strengths
*   **Novel and Principled Methodological Contribution:** The paper introduces a novel approach to RTSSL by framing it as a non-ignorable missing data problem and applying a doubly robust (DR) estimator to estimate the unlabeled class distribution. This is a significant departure from prior work that often relies on biased on-the-fly estimates.
    *   The connection to non-ignorable missingness (Section 2) provides a solid theoretical foundation for the problem, grounding it in established statistical literature.
    *   The proposed DR estimator (Equation 13) is principled, as it combines two different "nuisance" models—a classifier `P(Y|X)` and a missingness mechanism `P(A|Y)`—to produce an estimate that is consistent if either of the nuisance models is correctly specified.
    *   The method is supported by strong theoretical guarantees (Theorem 3.2), proving that the DR estimator is asymptotically normal and the most efficient regular estimator. This adds significant technical depth and credibility to the proposed approach.*   **Comprehensive Empirical Validation:** The paper presents extensive experiments that rigorously validate both stages of the proposed algorithm across multiple datasets and settings.
    *   The first-stage distribution estimation is thoroughly evaluated (Section 4.1), comparing multiple training models (Supervised, MLE, EM, SimPro) and estimators (IPW, OR, DR). The results in Table 1, Table 2, and Table 7 consistently show that the `SimPro + DR` combination yields the lowest Total Variation Distance to the ground truth, demonstrating its superior accuracy.
    *   The second-stage classification performance is evaluated on four benchmarks (CIFAR-10/100-LT, STL-10, ImageNet-127). The results show consistent and sometimes significant accuracy improvements over strong baselines (SimPro, BOAT) in the majority of settings (Table 3, Table 5, Table 6).
    *   The ablation study in Section 4.3 effectively justifies the two-stage design. It demonstrates that the proposed method outperforms alternatives like a batch-update approach or using a doubly-robust risk for training, highlighting the practical benefits of separating distribution estimation from classifier training (Table 8).*   **High Clarity and Good Structure:** The paper is well-written, clearly motivated, and easy to follow despite the technical nature of the topic.
    *   The motivation is established effectively from the start with Figure 1, which visually contrasts the proposed estimator's accuracy against a strong baseline (SimPro), immediately highlighting the core problem and the paper's contribution.
    *   The overall two-stage pipeline is clearly illustrated in a diagram (Figure 2), giving the reader a high-level map of the proposed algorithm.
    *   The authors clearly explain the relationship between their EM formulation and the recent SimPro method (Section 3.2), which helps to position their work within the existing literature and clarifies their implementation choices.3) Weaknesses
*   **Gap Between Theory and Practical Implementation:** The theoretical guarantees are a key strength, but several assumptions underlying the theory are not fully reconciled with the experimental setup.
    *   Assumption 3.1 requires the nuisance models to converge at a `o_p(N^{-1/4})` rate. While the paper cites literature suggesting this holds for neural networks (Section 3.3.1), it is not discussed whether the specific EM/SimPro training procedure used is known to satisfy this, or how violations might impact the estimator's quality.
    *   The proof of Theorem 3.2 in Appendix A relies on Assumption A.1, which includes sample splitting ("auxiliary samples independent of samples used for the sample averaging"). However, the main text and experimental details (Section 4, Appendix C) do not state whether sample splitting was used in practice. If not, this creates a mismatch between the conditions under which the theory is proven and the actual implementation.
    *   Assumption A.1 also requires the missingness probability `P(A=1|Y)` to be bounded away from zero. This can be a strong assumption in highly imbalanced settings where tail classes may have very few labeled samples. The paper mentions clipping as a potential solution (Appendix A) but does not discuss if it was necessary in the experiments or how it might affect the estimate's bias.*   **Insufficient Analysis of CIFAR-100 Results:** The performance on CIFAR-100 is notably weaker than on other datasets, and the paper does not provide a deep analysis of this phenomenon.
    *   The caption for Table 4 states, "Despite poor estimation in stage 1, our approach does not degrade the accuracy for most of the settings." This is a crucial observation that is not explored further. It is unclear why the method is robust to poor estimation in this case, or what causes the initial estimation to be poor on CIFAR-100.
    *   The accuracy gains on CIFAR-100 (Table 4) are marginal compared to those on CIFAR-10 (Table 3). For example, SimPro improves from 42.5% to 42.8% in one setting. A discussion on why the benefits are diminished on this more challenging dataset would strengthen the paper's conclusions.
    *   The Total Variation Distance scores in Table 2 for CIFAR-100 are substantially higher than those for CIFAR-10 in Table 1, confirming the poor estimation quality. This suggests potential scalability issues with the number of classes, which is a critical aspect to discuss.*   **Ambiguity in Key Stage-2 Implementation Details:** Some important details about how the Stage 1 estimate is integrated into the Stage 2 models are either relegated to the appendix or not fully explained.
    *   Appendix C reveals a critical detail: for SimPro+, the logit adjustment loss (Equation 9) still uses a running estimate of the combined class distribution `P(Y)`, because using the more accurate Stage 1 estimate reportedly "results in lower accuracy." This is counter-intuitive and the reasoning behind this design choice is not explored, despite its importance for reproducibility and understanding the method.
    *   The paper states that for BOAT+, it replaces a term `Δ_c` from the original BOAT paper (Appendix C). For readers not intimately familiar with BOAT, this description is opaque. A brief explanation in the main text would improve the paper's self-containedness.
    *   The use of a smaller model for Stage 1 and a larger one for Stage 2 on ImageNet-127 (Section 4) is an interesting and practical design choice. However, it is not discussed whether this is a general recommendation or ablated to show its importance.4) Suggestions for Improvement
*   **Strengthen the Connection Between Theory and Practice:**
    *   Please add a brief discussion in Section 3.3.1 or the appendix regarding the `o_p(N^{-1/4})` convergence rate (Assumption 3.1) in the context of the EM/SimPro training algorithm.
    *   Please clarify in the experimental setup (Section 4 or Appendix C) whether sample splitting (Assumption A.1) was implemented. If it was not, please add a sentence discussing this limitation and the potential impact of the dependency between nuisance model training and estimation samples.
    *   Please discuss the practical implications of the `P(A=1|Y) > ε` assumption (Assumption A.1). State whether clipping was used in the experiments and, if so, provide the value. This would also be a good place to connect this assumption to the instability observed in the DR-risk baseline (Section 4.3).*   **Provide a Deeper Analysis of CIFAR-100 Performance:**
    *   In the main results section (Section 4.2), please elaborate on the observation from the Table 4 caption. Offer a hypothesis for why the method is robust to poor distribution estimates on CIFAR-100 and what factors (e.g., high number of classes, extreme scarcity of labeled data per class) might contribute to the higher estimation error.
    *   Please add a discussion analyzing the smaller accuracy gains on CIFAR-100. This could involve examining per-class performance or the quality of the nuisance function estimates on this dataset compared to CIFAR-10.
    *   Consider adding a short paragraph discussing the scalability of the distribution estimation method with respect to the number of classes, using the contrasting results from CIFAR-10 and CIFAR-100 as evidence.*   **Clarify Stage-2 Implementation Details in the Main Text:**
    *   Please move the important detail about using a running average for the logit adjustment loss in SimPro+ from Appendix C to the main experimental section (Section 4.2). It would be beneficial to include a brief hypothesis on why a more accurate static estimate might be suboptimal for this specific loss component.
    *   In Section 4.2, when introducing BOAT+, please add a short phrase explaining that `Δ_c` is the log-ratio of class priors used for logit adjustment, which would make the modification clearer.
    *   Please add a brief discussion on the rationale for using different model sizes for Stage 1 and Stage 2 on ImageNet-127. If space permits, a small ablation showing the result of using the same model for both stages would be very insightful.5) Score
- Overall (10): 8 — The paper introduces a novel, theoretically-grounded, and empirically effective two-stage method for a challenging and practical problem, with strong results across multiple benchmarks (Table 3, 5, 6).
- Novelty (10): 9 — The application of doubly robust estimation from semi-parametric theory to estimate the unlabeled class distribution in RTSSL is highly novel and provides a new perspective on the problem (Section 3.3).
- Technical Quality (10): 8 — The method is technically sound with strong theoretical backing (Theorem 3.2) and extensive experiments, though there is a minor gap between the theoretical assumptions and the practical implementation (Assumption A.1).
- Clarity (10): 9 — The paper is very well-written and structured, with clear motivation and effective visualizations (Figure 1, Figure 2), although some key implementation details are in the appendix (Appendix C).
- Confidence (5): 5 — I am highly confident in my assessment, as the topic aligns with my area of expertise.