{
  "baseline_review": "Summary\n- The paper addresses realistic long-tailed semi-supervised learning where the unlabeled class distribution P(Y|A=0) differs from the labeled distribution and is unknown. It proposes a two-stage approach: Stage 1 uses a label-shift EM formulation to estimate the classifier P(Y|X) and the missingness mechanism P(A|Y), and then computes the unlabeled class distribution via a doubly robust estimator of P(Y) (Equations 10–13, Section 3.1–3.3). Under Assumption 3.1 with fourth-root-n rates and sample splitting (Appendix A), Theorem 3.2 shows asymptotic normality and efficiency of the DR estimator (Section 3.3.1, Equation 15, Appendix A). Stage 2 plugs the estimated P(Y|A=0) into existing pseudo-labeling methods (SimPro, BOAT), yielding accuracy gains across CIFAR-10, STL-10, and ImageNet-127 (Tables 3, 6, 8), and improved estimation quality in total variation (Tables 1, 7; Figures 1–2).Strengths\n- **Clear formulation of label-shift EM and its link to pseudo-labeling**\n  - The derivation of the observed-data likelihood and EM lower bound is explicit (Equation 4 to Equation 5, Section 3.1), showing how unlabeled terms induce pseudo-labels via ω^t(x,c) (Equation 7). This clarifies the connection to FixMatch and SimPro (Section 3.2), improving clarity and conceptual grounding.\n  - The decomposition P(A|Y)P(Y|X)P(X) avoids generative modeling of P(X|Y) while respecting label-shift invariance (Section 3.1), which is technically sound and practical.\n  - Demonstrating that SimPro’s parameterization is a reparameterization of the same EM objective (Equation 8–9, Section 3.2) helps unify prior methods under a common framework, aiding impact and clarity.- **Principled doubly robust estimator with strong theoretical guarantees**\n  - The DR estimator of P(Y) (Equation 13) combines outcome regression and inverse-probability weighting, with efficiency and asymptotic normality proved under Assumption 3.1 (Section 3.3.1, Equation 15; Appendix A).\n  - The proof details assumptions like sample splitting and bounded weights (Appendix A, Assumption A.1), and derives the efficient influence function and asymptotic linearity (Appendix A, Equations 16–27), demonstrating technical soundness.\n  - Positioning this in the context of semi-parametric efficiency and double machine learning (Section 2; References) supports novelty of application to RTSSL distribution estimation and impact on downstream pseudo-labeling.- **Effective two-stage design that improves existing methods**\n  - The pipeline in Figure 2 (Section 3.3) operationalizes Stage 1 EM estimates into a DR distribution estimate, then plugs into Stage 2 (SimPro, BOAT), balancing estimation quality and training stability—clear algorithmic contribution.\n  - CIFAR-10 gains are consistent: SimPro+ improves SimPro in 9/10 settings, BOAT+ improves BOAT in 8/10 settings (Table 3), indicating practical impact.\n  - ImageNet-127 shows improvements for both resolutions (Table 6), and the ablation (Table 8) demonstrates the benefit of the 2-stage approach over batch-update DR and DR-risk training, reinforcing robustness.- **Improved estimation of unlabeled class distribution**\n  - The paper quantifies P(Y|A=0) estimation using total variation distance on CIFAR-10 and ImageNet-127 (Tables 1, 7), with DR+SimPro frequently best or competitive, especially in consistent and head-tail settings (Table 1; Figures 1–2), demonstrating empirical rigor and impact.\n  - Figure 1 highlights specific failure modes of SimPro (overshoot of head classes) and the improved estimates using DR (Section “Preliminaries”), which aids clarity and motivation.\n  - The exploration across multiple estimators (OR, IPW, DR) and model choices (Supervised, MLE, EM, SimPro) (Section 4.1; Table 1) provides comprehensive evidence.- **Thoughtful discussion of related DR-risk approaches and instability**\n  - The paper contrasts DR estimation of P(Y) vs DR-risk training losses (Section 2; Appendix B), identifying inverse-weighting instability (Appendix B; Section 4.3) and empirically supporting this with lower accuracy in DR-risk (Table 8), adding technical insight and practical guidance.\n  - The stability issues in reversed settings due to small P(A|Y) are explicitly discussed (Section 4.3), which matters for technical soundness and reproducibility.- **Practical design choices and efficiency considerations**\n  - Using a smaller Stage-1 model (Wide ResNet-28-2) and a larger Stage-2 model (ResNet-50) on ImageNet-127 (Section 4, Training paragraph) demonstrates that finite-dimensional distribution estimation can be done with lighter models, which affects efficiency and impact.\n  - The paper retains running estimates in SimPro’s logit-adjustment loss while plugging DR estimates into the E-step (Appendix C), balancing theoretical and empirical considerations; although ad hoc, it contributes to practical performance (Tables 3, 6).Weaknesses\n- **Mismatch between theoretical assumptions and experimental protocol**\n  - Theorem 3.2 requires sample splitting (Appendix A, Assumption A.1) and fourth-root-n convergence (Assumption 3.1; Equation 14), but Appendix C does not report use of auxiliary samples or explicit sample splitting for estimating P(Y|X) and P(A|Y), creating a gap between theory and practice (Appendix C).\n  - Bounded-away weights (ε > 0) are assumed (Appendix A, Assumption A.1), but the training section (Appendix C) lacks explicit clipping or diagnostics for P(A=1|Y), potentially undermining the claimed stability in estimation.\n  - The theoretical result targets P(Y), while the downstream use is P(Y|A=0) via decomposition P(Y)=∑_a P(A=a)P(Y|A=a) (Section 3.3); the paper does not provide an explicit error propagation bound from P(Y) to P(Y|A=0) or to final accuracy, beyond citing external Theorem 3.1 of (Wei et al., 2024) without details (Section 3.3).- **Clarity and notation concerns**\n  - In Section 3.2, “Equation (1) without the 3 operators is just the unlabeled term in Equation (5) and P(Y|A=1)=P(Y|A=0)=P(Y|uniform)” is confusing; FixMatch does not typically assume uniform class distribution for labeled and unlabeled sets, and the equality may be an artifact of parameterization rather than an assumption (Section 3.2).\n  - Equation (9) uses “log{P(Y=c|X, uniform, θ) + P(Y=c)}”; this additive form inside a log is unusual for logit adjustment and is insufficiently explained compared to Equation (3) (Section 3.2), risking misinterpretation.\n  - The narrative toggles between non-ignorable missingness (A depends on X and Y, Section “Non-ignorable missingness”) and label shift (P(X|Y,A)=P(X|Y), Section 2), yet the model decomposition assumes A depends only on Y (Section 3.1). The scope of dependence and identifiability is not consistently clarified.- **Limited baselines and breadth of empirical comparison**\n  - While FixMatch, CReST+, DASO, SimPro, and BOAT are included (Table 3), several recent RTSSL methods cited (e.g., DC-SSL (Zhao et al., 2022), RDA (Duan et al., 2022), and other methods (Hu et al., 2022; Duan et al., 2023; Wei & Gan, 2023; Ma et al., 2024; Gan et al., 2024)) are not comprehensively benchmarked in stage-2 integration, limiting claims of broad applicability.\n  - Stage-1 distribution estimation on CIFAR-100 appears weak (Section 4.2 notes “Despite poor estimation in stage 1”), and Table 2 is truncated in the manuscript view, making it hard to assess TVD improvements across all settings and estimators (Table 2).\n  - ImageNet-127 stage-2 experiments are run once (Appendix C), which limits statistical confidence relative to CIFAR runs (which are run three times).- **Strong reliance on uniform test distribution and label-shift assumptions**\n  - The paper assumes a uniform test distribution throughout (Section 2, Notation), and uses logit adjustment toward uniform (Equation 3; Section 3.2; Appendix C), which narrows applicability in scenarios where test distributions are non-uniform or unknown.\n  - The label-shift assumption P(X|Y,A)=P(X|Y) (Equation 2) is central, yet the experimental section does not test robustness when this assumption is violated (e.g., domain shifts in X), limiting practical generality.- **Incomplete analysis of estimation-to-accuracy linkage**\n  - Although Section 3.3 cites a bound in (Wei et al., 2024) relating distribution estimation error to pseudo-label accuracy, there is no direct empirical analysis linking TVD improvements to accuracy gains across settings (e.g., correlation analysis between Table 1/7 and Table 3/6), weakening the causal narrative.\n  - Some settings show relatively modest TVD differences across estimators (e.g., SimPro OR vs DR in head-tail settings; Table 1), yet stage-2 accuracy improvements are small; without analysis, it is unclear when distribution estimation quality matters most.- **Practical training details and ablations could be expanded**\n  - The treatment of weight clipping, ε selection, and sensitivity to class ratios in reversed settings is only qualitatively discussed (Section 4.3; Appendix B), lacking quantitative ablations on ε and robustness to extreme imbalance.\n  - The stage-1 vs stage-2 architecture choice is demonstrated only on ImageNet-127 (Section 4, Training paragraph) without broader runtime or resource comparisons, leaving open questions about the computational overhead of two-stage training.- **Reproducibility gaps**\n  - Appendix C outlines high-level hyperparameters and hardware but does not specify code availability, random seeds, or detailed training schedules, particularly for EM updates of P(A|Y) and moving averages (Appendix C).\n  - Some tables lack standard deviations (e.g., Table 6, stage-2 ImageNet-127 runs once), which reduces reproducibility and confidence in gains.Suggestions for Improvement\n- **Align theoretical assumptions with experimental protocol**\n  - Explicitly implement and document sample splitting for estimating P(Y|X) and P(A|Y) versus the sample averages used in Equations 10–13 (Appendix A, Assumption A.1), and rerun key experiments to quantify any changes in TVD (Tables 1, 7) and accuracy (Tables 3, 6, 8).\n  - Add diagnostics for bounded-away weights (ε clipping) during estimation (Appendix A) and report the ε used, its sensitivity, and its impact on reversed settings stability (Section 4.3; Table 8).\n  - Provide a derivation or bound showing how error in P(Y) estimation propagates to P(Y|A=0) and then to classification error, ideally with finite-sample terms; complement this with empirical correlation plots between TVD and accuracy across settings (linking Tables 1/7→Tables 3/6).- **Improve clarity and notation**\n  - Clarify the statement in Section 3.2 regarding “P(Y|A=1)=P(Y|A=0)=P(Y|uniform)” and whether this is a parameterization choice in SimPro’s training loop rather than an assumption about the datasets.\n  - Revisit Equation (9) and provide a derivation from Equation (3) or SimPro’s parameterization (Equation 8) to justify the additive term inside the log; include a short explanatory subsection or appendix snippet to avoid confusion.\n  - Harmonize the discussion of non-ignorable missingness (Section “Non-ignorable missingness”) with the label-shift and decomposition assumptions used (Section 3.1), clearly stating when A may depend on X and how identifiability is guaranteed in the proposed model.- **Broaden empirical baselines and comparisons**\n  - Include stage-2 integration results for a broader set of RTSSL methods cited (e.g., DC-SSL (Zhao et al., 2022), RDA (Duan et al., 2022), Hu et al., 2022; Duan et al., 2023; Wei & Gan, 2023; Ma et al., 2024), reporting top-1 accuracy with and without plugging DR-estimated P(Y|A=0) (extend Table 3/4/6).\n  - Ensure full reporting of CIFAR-100 distribution estimation (complete Table 2) and analyze why stage-1 estimation is weak; consider larger Stage-1 models or alternative training for P(A|Y), and report corresponding TVD improvements.\n  - For ImageNet-127 stage-2, run multiple seeds and report mean ± std (extend Table 6) to improve statistical confidence.- **Relax assumptions and test robustness**\n  - Add experiments where the test distribution is non-uniform or unknown, and investigate whether plugging DR estimates into methods that target non-uniform test distributions (or post-hoc label-shift adaptation) yields similar gains (extend Section 4.2; Table 3/6).\n  - Introduce settings with mild covariate shift (violating P(X|Y,A)=P(X|Y), Equation 2) to test robustness of the approach; discuss any observed degradation and potential remedies (e.g., hybrid decompositions that partially model P(X|Y)).- **Link estimation quality to downstream accuracy**\n  - Provide per-setting analyses that quantify how much improvement in TVD (Tables 1, 7) is needed to achieve measurable accuracy gains (Tables 3, 6), including scatter plots or per-class calibration analyses; this would substantiate the role of distribution estimation in pseudo-label quality (Section 3.3).\n  - Identify settings (e.g., middle vs reversed) where accurate P(Y|A=0) matters most and discuss how pseudo-label thresholds/augmentations (FixMatch-style) interact with distribution estimates (Section 4.1–4.2).- **Expand practical ablations and efficiency reporting**\n  - Add ablations on ε clipping and class ratio sensitivity, including quantitative comparisons across reversed and head-tail settings, to complement the qualitative discussion of instability in DR-risk (Section 4.3; Appendix B; Table 8).\n  - Report training time and GPU hours for Stage-1 and Stage-2 per dataset, and compare to single-stage baselines to quantify overhead; include results with varying Stage-1 model sizes beyond ImageNet-127 (Appendix C).- **Enhance reproducibility**\n  - Provide code availability (or a statement of intent), random seeds, and detailed hyperparameters per dataset; describe the exact implementation of EM updates for P(A|Y) and moving averages, and list any data augmentations applied (Appendix C).\n  - For all tables, include standard deviations (especially Table 6) and specify the number of runs, improving confidence and enabling independent verification.Score\n- Overall (10): 7 — Strong theoretical grounding for DR estimation (Section 3.3.1; Appendix A) and consistent empirical gains in multiple settings (Tables 3, 6, 8), with some gaps in aligning assumptions to practice (Appendix C) and breadth of baselines (Table 3/4).\n- Novelty (10): 6 — Applies doubly robust estimation to unlabeled class distribution within a two-stage RTSSL pipeline and unifies EM/SimPro (Sections 3.1–3.2), but DR itself is established (Section 2; References) and related DR-risk approaches exist (Appendix B).\n- Technical Quality (10): 7 — Clear EM derivation (Equations 4–7), rigorous DR theory (Equation 15; Appendix A), and comprehensive estimators/ablations (Tables 1, 7, 8), with limitations in assumption verification (Appendix A vs Appendix C) and error propagation to P(Y|A=0).\n- Clarity (10): 7 — Generally well-written with diagrams and tables (Figure 2; Tables 1–8), but some notation/derivation ambiguities (Section 3.2; Equation 9) and mixed missingness narratives (Section 2) reduce clarity.\n- Confidence (5): 4 — High confidence in understanding and assessment based on detailed equations, proofs, and extensive experiments (Sections 3–4; Appendix A–C), with slight uncertainty due to truncated Table 2 and lack of multiple seeds in some results (Table 6).",
  "final_review": "Summary\n- The paper addresses realistic long-tailed semi-supervised learning where the unlabeled class distribution P(Y|A=0) differs from the labeled distribution and is unknown. It proposes a two-stage approach: Stage 1 uses a label-shift EM formulation to estimate the classifier P(Y|X) and the missingness mechanism P(A|Y), and then computes the unlabeled class distribution via a doubly robust estimator of P(Y) (Equations 10–13, Section 3.1–3.3). Under Assumption 3.1 with fourth-root-n rates and sample splitting (Appendix A), Theorem 3.2 shows asymptotic normality and efficiency of the DR estimator (Section 3.3.1, Equation 15, Appendix A). Stage 2 plugs the estimated P(Y|A=0) into existing pseudo-labeling methods (SimPro, BOAT), yielding accuracy gains across CIFAR-10, STL-10, and ImageNet-127 (Tables 3, 6, 8), and improved estimation quality in total variation (Tables 1, 7; Figures 1–2).Strengths\n- **Clear formulation of label-shift EM and its link to pseudo-labeling**\n  - The derivation of the observed-data likelihood and EM lower bound is explicit (Equation 4 to Equation 5, Section 3.1), showing how unlabeled terms induce pseudo-labels via ω^t(x,c) (Equation 7). This clarifies the connection to FixMatch and SimPro (Section 3.2), improving clarity and conceptual grounding.\n  - The decomposition P(A|Y)P(Y|X)P(X) avoids generative modeling of P(X|Y) while respecting label-shift invariance (Section 3.1), which is technically sound and practical.\n  - Demonstrating that SimPro’s parameterization is a reparameterization of the same EM objective (Equation 8, Section 3.2) helps unify prior methods under a common framework, aiding impact and clarity.\n- **Principled doubly robust estimator with strong theoretical guarantees**\n  - The DR estimator of P(Y) (Equation 13) combines outcome regression and inverse-probability weighting, with efficiency and asymptotic normality proved under Assumption 3.1 (Section 3.3.1, Equation 15; Appendix A).\n  - The proof details assumptions like sample splitting and bounded weights (Appendix A, Assumption A.1), and derives the efficient influence function and asymptotic linearity (Appendix A, Equations 16–27), demonstrating technical soundness.\n  - Positioning this in the context of semi-parametric efficiency and double machine learning (Section 2; References) supports novelty of application to RTSSL distribution estimation and impact on downstream pseudo-labeling.\n- **Effective two-stage design that improves existing methods**\n  - The pipeline in Figure 2 (Section 3.3) operationalizes Stage 1 EM estimates into a DR distribution estimate, then plugs into Stage 2 (SimPro, BOAT), balancing estimation quality and training stability—clear algorithmic contribution.\n  - CIFAR-10 gains are consistent: SimPro+ improves SimPro in 9/10 settings, BOAT+ improves BOAT in 8/10 settings (Table 3), indicating practical impact.\n  - ImageNet-127 shows improvements for both resolutions (Table 6), and the ablation (Table 8) demonstrates the benefit of the 2-stage approach over batch-update DR and DR-risk training, reinforcing robustness.\n- **Improved estimation of unlabeled class distribution**\n  - The paper quantifies P(Y|A=0) estimation using total variation distance on CIFAR-10 and ImageNet-127 (Tables 1, 7), with DR+SimPro frequently best or competitive, especially in consistent and head-tail settings (Table 1; Figures 1–2), demonstrating empirical rigor and impact.\n  - Figure 1 highlights specific failure modes of SimPro (overshoot of head classes) and the improved estimates using DR (Section “Preliminaries”), which aids clarity and motivation.\n  - The exploration across multiple estimators (OR, IPW, DR) and model choices (Supervised, MLE, EM, SimPro) (Section 4.1; Table 1) provides comprehensive evidence.\n- **Thoughtful discussion of related DR-risk approaches and instability**\n  - The paper contrasts DR estimation of P(Y) vs DR-risk training losses (Section 2; Appendix B), identifying inverse-weighting instability (Appendix B; Section 4.3) and empirically supporting this with lower accuracy in DR-risk (Table 8), adding technical insight and practical guidance.\n  - The stability issues in reversed settings due to small P(A|Y) are explicitly discussed (Section 4.3), which matters for technical soundness and reproducibility.\n- **Practical design choices and efficiency considerations**\n  - Using a smaller Stage-1 model (Wide ResNet-28-2) and a larger Stage-2 model (ResNet-50) on ImageNet-127 (Section 4, Training paragraph) demonstrates that finite-dimensional distribution estimation can be done with lighter models, which affects efficiency and impact.\n  - The paper retains running estimates in SimPro’s logit-adjustment loss while plugging DR estimates into the E-step (Appendix C), balancing theoretical and empirical considerations; although ad hoc, it contributes to practical performance (Tables 3, 6).Weaknesses\n- **Mismatch between theoretical assumptions and experimental protocol**\n  - Theorem 3.2 requires sample splitting (Appendix A, Assumption A.1) and fourth-root-n convergence (Assumption 3.1; Equation 14), but Appendix C does not report use of auxiliary samples or explicit sample splitting for estimating P(Y|X) and P(A|Y), creating a gap between theory and practice (Appendix C).\n  - Bounded-away weights (ε > 0) are assumed (Appendix A, Assumption A.1), but the training section (Appendix C) lacks explicit clipping or diagnostics for P(A=1|Y), potentially undermining the claimed stability in estimation.\n  - The theoretical result targets P(Y), while the downstream use is P(Y|A=0) via decomposition P(Y)=∑_a P(A=a)P(Y|A=a) (Section 3.3); the paper does not provide an explicit error propagation bound from P(Y) to P(Y|A=0) or to final accuracy, beyond citing external Theorem 3.1 of (Wei et al., 2024) without details (Section 3.3).\n- **Clarity and notation concerns**\n  - In Section 3.2, “Equation (1) without the 3 operators is just the unlabeled term in Equation (5) and P(Y|A=1)=P(Y|A=0)=P(Y|uniform)” is confusing; FixMatch does not typically assume uniform class distribution for labeled and unlabeled sets, and the equality may be an artifact of parameterization rather than an assumption (Section 3.2).\n  - Equation (9) uses “log{P(Y=c|X, uniform, θ) + P(Y=c)}”; this additive form inside a log is unusual for logit adjustment and is insufficiently explained compared to Equation (3) (Section 3.2), risking misinterpretation.\n  - The narrative toggles between non-ignorable missingness (A depends on X and Y, Section “Non-ignorable missingness”) and label shift (P(X|Y,A)=P(X|Y), Section 2), yet the model decomposition assumes A depends only on Y (Section 3.1). The scope of dependence and identifiability is not consistently clarified.\n  - The unlabeled likelihood term is written as an equality “P(A=0, X) = ∑_c P(Y=c|X) P(A=0|Y=c)” (Section 4.1), but strictly it should be proportional to P(X)×∑_c P(Y=c|X) P(A=0|Y=c); dropping P(X) without indicating proportionality can confuse readers about the decomposition.\n  - In Section 3.2, the statement “we can recover the class distribution P(Y|A) from the missingness mechanism P(A|Y) and, because P(A) is known, they are learned equivalently” omits that recovering P(Y|A=a) requires P(Y) via P(Y|A=a)=P(A=a|Y)P(Y)/P(A=a); this dependency is only later addressed via estimators of P(Y) (Section 3.3), creating a notational and conceptual gap.\n- **Limited baselines and breadth of empirical comparison**\n  - While FixMatch, CReST+, DASO, SimPro, and BOAT are included (Table 3), several recent RTSSL methods cited (e.g., DC-SSL (Zhao et al., 2022), RDA (Duan et al., 2022), and other methods (Hu et al., 2022; Duan et al., 2023; Wei & Gan, 2023; Ma et al., 2024; Gan et al., 2024)) are not comprehensively benchmarked in stage-2 integration, limiting claims of broad applicability.\n  - Stage-1 distribution estimation on CIFAR-100 appears weak (Section 4.2 notes “Despite poor estimation in stage 1”), and Table 2 is truncated in the manuscript view, making it hard to assess TVD improvements across all settings and estimators (Table 2).\n  - ImageNet-127 stage-2 experiments are run once (Appendix C), which limits statistical confidence relative to CIFAR runs (which are run three times).\n- **Strong reliance on uniform test distribution and label-shift assumptions**\n  - The paper assumes a uniform test distribution throughout (Section 2, Notation), and uses logit adjustment toward uniform (Equation 3; Section 3.2; Appendix C), which narrows applicability in scenarios where test distributions are non-uniform or unknown.\n  - The label-shift assumption P(X|Y,A)=P(X|Y) (Equation 2) is central, yet the experimental section does not test robustness when this assumption is violated (e.g., domain shifts in X), limiting practical generality.\n- **Incomplete analysis of estimation-to-accuracy linkage**\n  - Although Section 3.3 cites a bound in (Wei et al., 2024) relating distribution estimation error to pseudo-label accuracy, there is no direct empirical analysis linking TVD improvements to accuracy gains across settings (e.g., correlation analysis between Table 1/7 and Table 3/6), weakening the causal narrative.\n  - Some settings show relatively modest TVD differences across estimators (e.g., SimPro OR vs DR in head-tail settings; Table 1), yet stage-2 accuracy improvements are small; without analysis, it is unclear when distribution estimation quality matters most.\n  - The conclusion claims “even inaccurate class-label distributions do not lead to degraded accuracy in CIFAR-10” (Section 5), but Table 3 shows at least one degradation (e.g., SimPro vs SimPro+ in the reversed setting, γ_l=150, γ_u=1/150), indicating a mismatch between summary statements and reported results.\n- **Practical training details and ablations could be expanded**\n  - The treatment of weight clipping, ε selection, and sensitivity to class ratios in reversed settings is only qualitatively discussed (Section 4.3; Appendix B), lacking quantitative ablations on ε and robustness to extreme imbalance.\n  - The stage-1 vs stage-2 architecture choice is demonstrated only on ImageNet-127 (Section 4, Training paragraph) without broader runtime or resource comparisons, leaving open questions about the computational overhead of two-stage training.\n- **Reproducibility gaps**\n  - Appendix C outlines high-level hyperparameters and hardware but does not specify code availability, random seeds, or detailed training schedules, particularly for EM updates of P(A|Y) and moving averages (Appendix C).\n  - Some tables lack standard deviations (e.g., Table 6, stage-2 ImageNet-127 runs once), which reduces reproducibility and confidence in gains.\n  - Table 5 is referenced for STL-10 improvements (Section 4.2) but is not included in the manuscript text provided; No direct evidence found in the manuscript to verify the STL-10 claim.Suggestions for Improvement\n- **Align theoretical assumptions with experimental protocol**\n  - Explicitly implement and document sample splitting for estimating P(Y|X) and P(A|Y) versus the sample averages used in Equations 10–13 (Appendix A, Assumption A.1), and rerun key experiments to quantify any changes in TVD (Tables 1, 7) and accuracy (Tables 3, 6, 8).\n  - Add diagnostics for bounded-away weights (ε clipping) during estimation (Appendix A) and report the ε used, its sensitivity, and its impact on reversed settings stability (Section 4.3; Table 8).\n  - Provide a derivation or bound showing how error in P(Y) estimation propagates to P(Y|A=0) and then to classification error, ideally with finite-sample terms; complement this with empirical correlation plots between TVD and accuracy across settings (linking Tables 1/7→Tables 3/6).\n- **Improve clarity and notation**\n  - Clarify the statement in Section 3.2 regarding “P(Y|A=1)=P(Y|A=0)=P(Y|uniform)” and whether this is a parameterization choice in SimPro’s training loop rather than an assumption about the datasets.\n  - Revisit Equation (9) and provide a derivation from Equation (3) or SimPro’s parameterization (Equation 8) to justify the additive term inside the log; include a short explanatory subsection or appendix snippet to avoid confusion.\n  - Harmonize the discussion of non-ignorable missingness (Section “Non-ignorable missingness”) with the label-shift and decomposition assumptions used (Section 3.1), clearly stating when A may depend on X and how identifiability is guaranteed in the proposed model.\n  - Correct the unlabeled likelihood decomposition in Section 4.1 to explicitly note proportionality to P(X), or explicitly state that P(X) is a nuisance constant with respect to θ in optimization, to prevent confusion about equality vs proportionality.\n  - Amend the discussion in Section 3.2 to state that recovering P(Y|A=a) from P(A|Y) requires knowledge or estimation of P(Y) (as done in Section 3.3), and make the dependency explicit in the main text.\n- **Broaden empirical baselines and comparisons**\n  - Include stage-2 integration results for a broader set of RTSSL methods cited (e.g., DC-SSL (Zhao et al., 2022), RDA (Duan et al., 2022), Hu et al., 2022; Duan et al., 2023; Wei & Gan, 2023; Ma et al., 2024), reporting top-1 accuracy with and without plugging DR-estimated P(Y|A=0) (extend Table 3/4/6).\n  - Ensure full reporting of CIFAR-100 distribution estimation (complete Table 2) and analyze why stage-1 estimation is weak; consider larger Stage-1 models or alternative training for P(A|Y), and report corresponding TVD improvements.\n  - For ImageNet-127 stage-2, run multiple seeds and report mean ± std (extend Table 6) to improve statistical confidence.\n- **Relax assumptions and test robustness**\n  - Add experiments where the test distribution is non-uniform or unknown, and investigate whether plugging DR estimates into methods that target non-uniform test distributions (or post-hoc label-shift adaptation) yields similar gains (extend Section 4.2; Table 3/6).\n  - Introduce settings with mild covariate shift (violating P(X|Y,A)=P(X|Y), Equation 2) to test robustness of the approach; discuss any observed degradation and potential remedies (e.g., hybrid decompositions that partially model P(X|Y)).\n- **Link estimation quality to downstream accuracy**\n  - Provide per-setting analyses that quantify how much improvement in TVD (Tables 1, 7) is needed to achieve measurable accuracy gains (Tables 3, 6), including scatter plots or per-class calibration analyses; this would substantiate the role of distribution estimation in pseudo-label quality (Section 3.3).\n  - Identify settings (e.g., middle vs reversed) where accurate P(Y|A=0) matters most and discuss how pseudo-label thresholds/augmentations (FixMatch-style) interact with distribution estimates (Section 4.1–4.2).\n  - Revise the conclusion (Section 5) to reflect cases where accuracy degrades (e.g., Table 3 reversed setting), and add a brief discussion on conditions under which plug-in estimates may harm accuracy.\n- **Expand practical ablations and efficiency reporting**\n  - Add ablations on ε clipping and class ratio sensitivity, including quantitative comparisons across reversed and head-tail settings, to complement the qualitative discussion of instability in DR-risk (Section 4.3; Appendix B; Table 8).\n  - Report training time and GPU hours for Stage-1 and Stage-2 per dataset, and compare to single-stage baselines to quantify overhead; include results with varying Stage-1 model sizes beyond ImageNet-127 (Appendix C).\n- **Enhance reproducibility**\n  - Provide code availability (or a statement of intent), random seeds, and detailed hyperparameters per dataset; describe the exact implementation of EM updates for P(A|Y) and moving averages, and list any data augmentations applied (Appendix C).\n  - For all tables, include standard deviations (especially Table 6) and specify the number of runs, improving confidence and enabling independent verification.\n  - Include the missing STL-10 table (Table 5, Section 4.2) with full numerical results and seeds, to substantiate claims of improvements on STL-10.Score\n- Overall (10): 7 — Strong theoretical grounding for DR estimation (Section 3.3.1; Appendix A) and consistent empirical gains in multiple settings (Tables 3, 6, 8), with some gaps in aligning assumptions to practice (Appendix C) and breadth of baselines (Table 3/4).\n- Novelty (10): 6 — Applies doubly robust estimation to unlabeled class distribution within a two-stage RTSSL pipeline and unifies EM/SimPro (Sections 3.1–3.2), but DR itself is established (Section 2; References) and related DR-risk approaches exist (Appendix B).\n- Technical Quality (10): 7 — Clear EM derivation (Equations 4–7), rigorous DR theory (Equation 15; Appendix A), and comprehensive estimators/ablations (Tables 1, 7, 8), with limitations in assumption verification (Appendix A vs Appendix C) and error propagation to P(Y|A=0).\n- Clarity (10): 7 — Generally well-written with diagrams and tables (Figure 2; Tables 1–8), but some notation/derivation ambiguities (Section 3.2; Equation 9) and mixed missingness narratives (Section 2) reduce clarity.\n- Confidence (5): 4 — High confidence in understanding and assessment based on detailed equations, proofs, and extensive experiments (Sections 3–4; Appendix A–C), with slight uncertainty due to truncated Table 2 and lack of multiple seeds in some results (Table 6).",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 7,
        "novelty": 6,
        "technical_quality": 7,
        "clarity": 7,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 7,
        "novelty": 6,
        "technical_quality": 7,
        "clarity": 7,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "Summary\n- The paper addresses realistic long-tailed semi-supervised learning where the unlabeled class distribution P(Y|A=0) differs from the labeled distribution and is unknown. It proposes a two-stage approach: Stage 1 uses a label-shift EM formulation to estimate the classifier P(Y|X) and the missingness mechanism P(A|Y), and then computes the unlabeled class distribution via a doubly robust estimator of P(Y) (Equations 10–13, Section 3.1–3.3). Under Assumption 3.1 with fourth-root-n rates and sample splitting (Appendix A), Theorem 3.2 shows asymptotic normality and efficiency of the DR estimator (Section 3.3.1, Equation 15, Appendix A). Stage 2 plugs the estimated P(Y|A=0) into existing pseudo-labeling methods (SimPro, BOAT), yielding accuracy gains across CIFAR-10, STL-10, and ImageNet-127 (Tables 3, 6, 8), and improved estimation quality in total variation (Tables 1, 7; Figures 1–2).Strengths\n- **Clear formulation of label-shift EM and its link to pseudo-labeling**\n  - The derivation of the observed-data likelihood and EM lower bound is explicit (Equation 4 to Equation 5, Section 3.1), showing how unlabeled terms induce pseudo-labels via ω^t(x,c) (Equation 7). This clarifies the connection to FixMatch and SimPro (Section 3.2), improving clarity and conceptual grounding.\n  - The decomposition P(A|Y)P(Y|X)P(X) avoids generative modeling of P(X|Y) while respecting label-shift invariance (Section 3.1), which is technically sound and practical.\n  - Demonstrating that SimPro’s parameterization is a reparameterization of the same EM objective (Equation 8, Section 3.2) helps unify prior methods under a common framework, aiding impact and clarity.\n- **Principled doubly robust estimator with strong theoretical guarantees**\n  - The DR estimator of P(Y) (Equation 13) combines outcome regression and inverse-probability weighting, with efficiency and asymptotic normality proved under Assumption 3.1 (Section 3.3.1, Equation 15; Appendix A).\n  - The proof details assumptions like sample splitting and bounded weights (Appendix A, Assumption A.1), and derives the efficient influence function and asymptotic linearity (Appendix A, Equations 16–27), demonstrating technical soundness.\n  - Positioning this in the context of semi-parametric efficiency and double machine learning (Section 2; References) supports novelty of application to RTSSL distribution estimation and impact on downstream pseudo-labeling.\n- **Effective two-stage design that improves existing methods**\n  - The pipeline in Figure 2 (Section 3.3) operationalizes Stage 1 EM estimates into a DR distribution estimate, then plugs into Stage 2 (SimPro, BOAT), balancing estimation quality and training stability—clear algorithmic contribution.\n  - CIFAR-10 gains are consistent: SimPro+ improves SimPro in 9/10 settings, BOAT+ improves BOAT in 8/10 settings (Table 3), indicating practical impact.\n  - ImageNet-127 shows improvements for both resolutions (Table 6), and the ablation (Table 8) demonstrates the benefit of the 2-stage approach over batch-update DR and DR-risk training, reinforcing robustness.\n- **Improved estimation of unlabeled class distribution**\n  - The paper quantifies P(Y|A=0) estimation using total variation distance on CIFAR-10 and ImageNet-127 (Tables 1, 7), with DR+SimPro frequently best or competitive, especially in consistent and head-tail settings (Table 1; Figures 1–2), demonstrating empirical rigor and impact.\n  - Figure 1 highlights specific failure modes of SimPro (overshoot of head classes) and the improved estimates using DR (Section “Preliminaries”), which aids clarity and motivation.\n  - The exploration across multiple estimators (OR, IPW, DR) and model choices (Supervised, MLE, EM, SimPro) (Section 4.1; Table 1) provides comprehensive evidence.\n- **Thoughtful discussion of related DR-risk approaches and instability**\n  - The paper contrasts DR estimation of P(Y) vs DR-risk training losses (Section 2; Appendix B), identifying inverse-weighting instability (Appendix B; Section 4.3) and empirically supporting this with lower accuracy in DR-risk (Table 8), adding technical insight and practical guidance.\n  - The stability issues in reversed settings due to small P(A|Y) are explicitly discussed (Section 4.3), which matters for technical soundness and reproducibility.\n- **Practical design choices and efficiency considerations**\n  - Using a smaller Stage-1 model (Wide ResNet-28-2) and a larger Stage-2 model (ResNet-50) on ImageNet-127 (Section 4, Training paragraph) demonstrates that finite-dimensional distribution estimation can be done with lighter models, which affects efficiency and impact.\n  - The paper retains running estimates in SimPro’s logit-adjustment loss while plugging DR estimates into the E-step (Appendix C), balancing theoretical and empirical considerations; although ad hoc, it contributes to practical performance (Tables 3, 6).Weaknesses\n- **Mismatch between theoretical assumptions and experimental protocol**\n  - Theorem 3.2 requires sample splitting (Appendix A, Assumption A.1) and fourth-root-n convergence (Assumption 3.1; Equation 14), but Appendix C does not report use of auxiliary samples or explicit sample splitting for estimating P(Y|X) and P(A|Y), creating a gap between theory and practice (Appendix C).\n  - Bounded-away weights (ε > 0) are assumed (Appendix A, Assumption A.1), but the training section (Appendix C) lacks explicit clipping or diagnostics for P(A=1|Y), potentially undermining the claimed stability in estimation.\n  - The theoretical result targets P(Y), while the downstream use is P(Y|A=0) via decomposition P(Y)=∑_a P(A=a)P(Y|A=a) (Section 3.3); the paper does not provide an explicit error propagation bound from P(Y) to P(Y|A=0) or to final accuracy, beyond citing external Theorem 3.1 of (Wei et al., 2024) without details (Section 3.3).\n- **Clarity and notation concerns**\n  - In Section 3.2, “Equation (1) without the 3 operators is just the unlabeled term in Equation (5) and P(Y|A=1)=P(Y|A=0)=P(Y|uniform)” is confusing; FixMatch does not typically assume uniform class distribution for labeled and unlabeled sets, and the equality may be an artifact of parameterization rather than an assumption (Section 3.2).\n  - Equation (9) uses “log{P(Y=c|X, uniform, θ) + P(Y=c)}”; this additive form inside a log is unusual for logit adjustment and is insufficiently explained compared to Equation (3) (Section 3.2), risking misinterpretation.\n  - The narrative toggles between non-ignorable missingness (A depends on X and Y, Section “Non-ignorable missingness”) and label shift (P(X|Y,A)=P(X|Y), Section 2), yet the model decomposition assumes A depends only on Y (Section 3.1). The scope of dependence and identifiability is not consistently clarified.\n  - The unlabeled likelihood term is written as an equality “P(A=0, X) = ∑_c P(Y=c|X) P(A=0|Y=c)” (Section 4.1), but strictly it should be proportional to P(X)×∑_c P(Y=c|X) P(A=0|Y=c); dropping P(X) without indicating proportionality can confuse readers about the decomposition.\n  - In Section 3.2, the statement “we can recover the class distribution P(Y|A) from the missingness mechanism P(A|Y) and, because P(A) is known, they are learned equivalently” omits that recovering P(Y|A=a) requires P(Y) via P(Y|A=a)=P(A=a|Y)P(Y)/P(A=a); this dependency is only later addressed via estimators of P(Y) (Section 3.3), creating a notational and conceptual gap.\n- **Limited baselines and breadth of empirical comparison**\n  - While FixMatch, CReST+, DASO, SimPro, and BOAT are included (Table 3), several recent RTSSL methods cited (e.g., DC-SSL (Zhao et al., 2022), RDA (Duan et al., 2022), and other methods (Hu et al., 2022; Duan et al., 2023; Wei & Gan, 2023; Ma et al., 2024; Gan et al., 2024)) are not comprehensively benchmarked in stage-2 integration, limiting claims of broad applicability.\n  - Stage-1 distribution estimation on CIFAR-100 appears weak (Section 4.2 notes “Despite poor estimation in stage 1”), and Table 2 is truncated in the manuscript view, making it hard to assess TVD improvements across all settings and estimators (Table 2).\n  - ImageNet-127 stage-2 experiments are run once (Appendix C), which limits statistical confidence relative to CIFAR runs (which are run three times).\n- **Strong reliance on uniform test distribution and label-shift assumptions**\n  - The paper assumes a uniform test distribution throughout (Section 2, Notation), and uses logit adjustment toward uniform (Equation 3; Section 3.2; Appendix C), which narrows applicability in scenarios where test distributions are non-uniform or unknown.\n  - The label-shift assumption P(X|Y,A)=P(X|Y) (Equation 2) is central, yet the experimental section does not test robustness when this assumption is violated (e.g., domain shifts in X), limiting practical generality.\n- **Incomplete analysis of estimation-to-accuracy linkage**\n  - Although Section 3.3 cites a bound in (Wei et al., 2024) relating distribution estimation error to pseudo-label accuracy, there is no direct empirical analysis linking TVD improvements to accuracy gains across settings (e.g., correlation analysis between Table 1/7 and Table 3/6), weakening the causal narrative.\n  - Some settings show relatively modest TVD differences across estimators (e.g., SimPro OR vs DR in head-tail settings; Table 1), yet stage-2 accuracy improvements are small; without analysis, it is unclear when distribution estimation quality matters most.\n  - The conclusion claims “even inaccurate class-label distributions do not lead to degraded accuracy in CIFAR-10” (Section 5), but Table 3 shows at least one degradation (e.g., SimPro vs SimPro+ in the reversed setting, γ_l=150, γ_u=1/150), indicating a mismatch between summary statements and reported results.\n- **Practical training details and ablations could be expanded**\n  - The treatment of weight clipping, ε selection, and sensitivity to class ratios in reversed settings is only qualitatively discussed (Section 4.3; Appendix B), lacking quantitative ablations on ε and robustness to extreme imbalance.\n  - The stage-1 vs stage-2 architecture choice is demonstrated only on ImageNet-127 (Section 4, Training paragraph) without broader runtime or resource comparisons, leaving open questions about the computational overhead of two-stage training.\n- **Reproducibility gaps**\n  - Appendix C outlines high-level hyperparameters and hardware but does not specify code availability, random seeds, or detailed training schedules, particularly for EM updates of P(A|Y) and moving averages (Appendix C).\n  - Some tables lack standard deviations (e.g., Table 6, stage-2 ImageNet-127 runs once), which reduces reproducibility and confidence in gains.\n  - Table 5 is referenced for STL-10 improvements (Section 4.2) but is not included in the manuscript text provided; No direct evidence found in the manuscript to verify the STL-10 claim.Suggestions for Improvement\n- **Align theoretical assumptions with experimental protocol**\n  - Explicitly implement and document sample splitting for estimating P(Y|X) and P(A|Y) versus the sample averages used in Equations 10–13 (Appendix A, Assumption A.1), and rerun key experiments to quantify any changes in TVD (Tables 1, 7) and accuracy (Tables 3, 6, 8).\n  - Add diagnostics for bounded-away weights (ε clipping) during estimation (Appendix A) and report the ε used, its sensitivity, and its impact on reversed settings stability (Section 4.3; Table 8).\n  - Provide a derivation or bound showing how error in P(Y) estimation propagates to P(Y|A=0) and then to classification error, ideally with finite-sample terms; complement this with empirical correlation plots between TVD and accuracy across settings (linking Tables 1/7→Tables 3/6).\n- **Improve clarity and notation**\n  - Clarify the statement in Section 3.2 regarding “P(Y|A=1)=P(Y|A=0)=P(Y|uniform)” and whether this is a parameterization choice in SimPro’s training loop rather than an assumption about the datasets.\n  - Revisit Equation (9) and provide a derivation from Equation (3) or SimPro’s parameterization (Equation 8) to justify the additive term inside the log; include a short explanatory subsection or appendix snippet to avoid confusion.\n  - Harmonize the discussion of non-ignorable missingness (Section “Non-ignorable missingness”) with the label-shift and decomposition assumptions used (Section 3.1), clearly stating when A may depend on X and how identifiability is guaranteed in the proposed model.\n  - Correct the unlabeled likelihood decomposition in Section 4.1 to explicitly note proportionality to P(X), or explicitly state that P(X) is a nuisance constant with respect to θ in optimization, to prevent confusion about equality vs proportionality.\n  - Amend the discussion in Section 3.2 to state that recovering P(Y|A=a) from P(A|Y) requires knowledge or estimation of P(Y) (as done in Section 3.3), and make the dependency explicit in the main text.\n- **Broaden empirical baselines and comparisons**\n  - Include stage-2 integration results for a broader set of RTSSL methods cited (e.g., DC-SSL (Zhao et al., 2022), RDA (Duan et al., 2022), Hu et al., 2022; Duan et al., 2023; Wei & Gan, 2023; Ma et al., 2024), reporting top-1 accuracy with and without plugging DR-estimated P(Y|A=0) (extend Table 3/4/6).\n  - Ensure full reporting of CIFAR-100 distribution estimation (complete Table 2) and analyze why stage-1 estimation is weak; consider larger Stage-1 models or alternative training for P(A|Y), and report corresponding TVD improvements.\n  - For ImageNet-127 stage-2, run multiple seeds and report mean ± std (extend Table 6) to improve statistical confidence.\n- **Relax assumptions and test robustness**\n  - Add experiments where the test distribution is non-uniform or unknown, and investigate whether plugging DR estimates into methods that target non-uniform test distributions (or post-hoc label-shift adaptation) yields similar gains (extend Section 4.2; Table 3/6).\n  - Introduce settings with mild covariate shift (violating P(X|Y,A)=P(X|Y), Equation 2) to test robustness of the approach; discuss any observed degradation and potential remedies (e.g., hybrid decompositions that partially model P(X|Y)).\n- **Link estimation quality to downstream accuracy**\n  - Provide per-setting analyses that quantify how much improvement in TVD (Tables 1, 7) is needed to achieve measurable accuracy gains (Tables 3, 6), including scatter plots or per-class calibration analyses; this would substantiate the role of distribution estimation in pseudo-label quality (Section 3.3).\n  - Identify settings (e.g., middle vs reversed) where accurate P(Y|A=0) matters most and discuss how pseudo-label thresholds/augmentations (FixMatch-style) interact with distribution estimates (Section 4.1–4.2).\n  - Revise the conclusion (Section 5) to reflect cases where accuracy degrades (e.g., Table 3 reversed setting), and add a brief discussion on conditions under which plug-in estimates may harm accuracy.\n- **Expand practical ablations and efficiency reporting**\n  - Add ablations on ε clipping and class ratio sensitivity, including quantitative comparisons across reversed and head-tail settings, to complement the qualitative discussion of instability in DR-risk (Section 4.3; Appendix B; Table 8).\n  - Report training time and GPU hours for Stage-1 and Stage-2 per dataset, and compare to single-stage baselines to quantify overhead; include results with varying Stage-1 model sizes beyond ImageNet-127 (Appendix C).\n- **Enhance reproducibility**\n  - Provide code availability (or a statement of intent), random seeds, and detailed hyperparameters per dataset; describe the exact implementation of EM updates for P(A|Y) and moving averages, and list any data augmentations applied (Appendix C).\n  - For all tables, include standard deviations (especially Table 6) and specify the number of runs, improving confidence and enabling independent verification.\n  - Include the missing STL-10 table (Table 5, Section 4.2) with full numerical results and seeds, to substantiate claims of improvements on STL-10.Score\n- Overall (10): 7 — Strong theoretical grounding for DR estimation (Section 3.3.1; Appendix A) and consistent empirical gains in multiple settings (Tables 3, 6, 8), with some gaps in aligning assumptions to practice (Appendix C) and breadth of baselines (Table 3/4).\n- Novelty (10): 6 — Applies doubly robust estimation to unlabeled class distribution within a two-stage RTSSL pipeline and unifies EM/SimPro (Sections 3.1–3.2), but DR itself is established (Section 2; References) and related DR-risk approaches exist (Appendix B).\n- Technical Quality (10): 7 — Clear EM derivation (Equations 4–7), rigorous DR theory (Equation 15; Appendix A), and comprehensive estimators/ablations (Tables 1, 7, 8), with limitations in assumption verification (Appendix A vs Appendix C) and error propagation to P(Y|A=0).\n- Clarity (10): 7 — Generally well-written with diagrams and tables (Figure 2; Tables 1–8), but some notation/derivation ambiguities (Section 3.2; Equation 9) and mixed missingness narratives (Section 2) reduce clarity.\n- Confidence (5): 4 — High confidence in understanding and assessment based on detailed equations, proofs, and extensive experiments (Sections 3–4; Appendix A–C), with slight uncertainty due to truncated Table 2 and lack of multiple seeds in some results (Table 6)."
}