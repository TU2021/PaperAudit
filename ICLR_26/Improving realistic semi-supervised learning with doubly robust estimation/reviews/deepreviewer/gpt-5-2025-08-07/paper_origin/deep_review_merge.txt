Summary
The paper addresses realistic long‑tailed semi‑supervised learning where the unlabeled class prior differs from the labeled one and is unknown. It proposes a two‑stage pipeline. Stage‑1 jointly estimates a discriminative classifier P(Y|X) and the missingness mechanism P(A|Y) via an EM procedure under label shift, yielding interpretable E‑step weights that depend on P(Y|X) and P(A=0|Y), and an M‑step that decouples into two cross‑entropy problems. The resulting estimates are fed into a doubly robust (DR) estimator to recover the overall class prior P(Y) and thereby the unlabeled prior P(Y|A=0). Stage‑2 plugs the estimated unlabeled class prior into existing pseudo‑labeling methods (e.g., SimPro, BOAT) to adjust training. The authors prove that their DR estimator is asymptotically efficient under standard double machine learning conditions, including sample splitting and positivity, and present experiments on CIFAR‑10/100, STL‑10, and ImageNet‑127 showing improved estimation accuracy (lower total variation distance) and consistent downstream top‑1 accuracy gains. Ablations indicate that the two‑stage design is more stable and effective than batch‑updated DR and direct DR‑risk training.

Strengths
- Clear and principled problem formulation for RTSSL with mismatched unlabeled priors, avoiding generative modeling through the factorization P(A|Y)P(Y|X)P(X).
- Sound EM derivation with practical E‑step weights and an M‑step that decouples into standard cross‑entropy objectives; helpful conceptual connection drawn between the EM view and SimPro.
- Doubly robust estimator for P(Y) with rigorous semiparametric efficiency guarantees under modern double ML conditions (rate assumptions, sample splitting, positivity), including an explicit efficient influence function and asymptotic normality.
- Consistent empirical gains in estimating the unlabeled class prior across diverse distributions and datasets, and corresponding improvements in downstream accuracy when plugging the estimate into SimPro and BOAT.
- Thoughtful ablations highlighting instability of inverse‑weighted DR‑risk and the advantage of separating estimation (stage‑1) from classifier training (stage‑2).
- Practical design choices: stage‑1 uses smaller backbones yet suffices for estimating finite‑dimensional priors; the pipeline integrates as a drop‑in module for existing SSL methods without heavy redesign.
- Generally clear organization and helpful figures illustrating bias issues and the overall pipeline.

Weaknesses
- Theory–practice gap: the efficiency proof relies on sample splitting/cross‑fitting of nuisance models, but the experimental protocol does not document its use, weakening the applicability of asymptotic guarantees to reported results.
- Positivity and weight stability: extreme long‑tail/reversed settings can induce large inverse weights; although clipping is mentioned, implementation details and sensitivity analyses are not provided, leaving finite‑sample robustness unclear.
- Unclear or nonstandard formulation of the logit adjustment loss (Eq. (9)); it deviates from the usual additive‑logit prior correction and the link to Eq. (3) is not formally reconciled, hindering reproducibility and obscuring the claimed equivalence to SimPro.
- Limited breadth of stage‑2 integrations: improvements are demonstrated for SimPro and BOAT, but not for other strong RTSSL methods (e.g., DASO, CReST+), and DR‑risk baselines are restricted in scope, limiting evidence for generality.
- Presentation and consistency issues: truncated or split materials (e.g., parts of Assumption 3.1 and a table), and a slightly overstated conclusion about “no degradation” despite small observed drops, impede clarity and careful assessment.
- Assumption coverage: the approach assumes label shift (P(X|Y,A)=P(X|Y)) and often a uniform test distribution; there are no stress tests under violations (e.g., conditional shift or non‑uniform test priors), which may limit applicability in real‑world scenarios.
- The discussion connecting EM and SimPro is plausible but remains informal; a more rigorous derivation would strengthen the conceptual contribution.
