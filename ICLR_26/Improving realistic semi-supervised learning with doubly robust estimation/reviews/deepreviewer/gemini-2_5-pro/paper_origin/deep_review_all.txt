Here are four distinct reviews of the research paper.

***

### **Review 1**

**Summary**
This paper addresses the problem of realistic long-tailed semi-supervised learning (RTSSL), where the class distribution of the unlabeled data is unknown and differs from the labeled data. The authors propose a two-stage approach. In the first stage, they frame the problem as one of non-ignorable missing data and use a doubly robust (DR) estimator, borrowed from causal inference and semi-parametric theory, to accurately estimate the unlabeled class distribution. In the second stage, this pre-computed distribution estimate is plugged into existing state-of-the-art SSL methods (like SimPro and BOAT) to improve their final classification performance. The paper provides theoretical guarantees for the DR estimator and demonstrates empirical improvements across several benchmark datasets.

**Soundness**
The methodology is sound and well-motivated. The core idea of separating the estimation of the finite-dimensional class distribution from the training of the high-dimensional classifier is elegant and powerful. The application of doubly robust estimation is appropriate for this problem, as it combines the strengths of a classifier-based outcome regression (OR) model and a missingness-based inverse probability weighting (IPW) model. The theoretical analysis, culminating in Theorem 3.2, provides strong justification for the choice of the DR estimator, proving its asymptotic normality and efficiency under standard assumptions. The experimental design is thorough, comparing the proposed method against multiple baselines and performing key ablations (Table 8) that validate the two-stage design over alternatives like batch-updating or using a DR-risk objective.

**Presentation**
The paper is well-written and organized. The introduction clearly motivates the problem and outlines the proposed solution. Figure 2 provides an excellent, high-level overview of the two-stage algorithm, which greatly aids in understanding the overall pipeline. The connection between the authors' EM framework (Section 3.1) and the SimPro algorithm (Section 3.2) is explained, clarifying the implementation choices. The experimental results are presented clearly in tables, with helpful annotations (e.g., color-coding in Tables 3 & 4) to highlight performance changes. The appendix provides necessary details for the theoretical proof and further background.

**Contribution**
The paper makes a significant and novel contribution to the field of semi-supervised learning, particularly for the realistic long-tailed setting. The primary novelty lies in the successful adaptation and application of doubly robust estimation for the specific sub-problem of estimating the unlabeled class distribution. While prior works have noted connections to missing data, this paper is the first to propose this clean, two-stage "estimate-then-plug-in" approach and demonstrate its effectiveness. By improving upon strong SOTA baselines like SimPro and BOAT, the work shows practical value. The bridging of concepts from semi-parametric efficiency theory and deep SSL is a valuable contribution in itself.

**Strengths**
1.  **Strong Theoretical Grounding:** The use of a doubly robust estimator is well-justified by semi-parametric theory, and the inclusion of Theorem 3.2 provides formal guarantees on the quality of the distribution estimate.
2.  **Novel and Effective Approach:** The two-stage pipeline is a simple but powerful idea. Separating distribution estimation from classifier training is a clean solution to the biased estimation problem highlighted in Figure 1.
3.  **Strong Empirical Results:** The method shows consistent improvements over strong baselines (SimPro, BOAT) across multiple datasets and settings (Tables 3, 5, 6). The superior performance of the DR estimator in distribution estimation is clearly demonstrated (Table 1).
4.  **Modularity:** The proposed distribution estimate can be used as a "plug-in" for various existing SSL methods, making the contribution broadly applicable.

**Weaknesses**
1.  **Increased Complexity:** The two-stage approach inherently adds complexity and computational overhead compared to end-to-end methods. A brief discussion on the practical trade-offs (e.g., wall-clock time) would be beneficial.
2.  **Marginal Gains on CIFAR-100:** While the method shows strong gains on CIFAR-10, the improvements on CIFAR-100 are quite small (Table 4), and even slightly negative in some cases for BOAT+. The paper notes this but could discuss potential reasons more deeply (e.g., is the stage-1 estimation harder with more classes and fewer labels?).

**Questions**
1.  In Section 4.1, you demonstrate that a smaller network (Wide ResNet-28-2) is sufficient for stage 1 on ImageNet-127. Have you explored how the performance of the stage-1 distribution estimation and the final stage-2 accuracy vary with the capacity of the stage-1 model? Is there a point of diminishing returns or even overfitting?
2.  The paper notes that for SimPro+, using the stage-1 estimate for logit adjustment in Eq. (9) was worse than using the running average. This is an interesting finding. Could you elaborate on why you believe this is the case? Does it suggest the role of the distribution in the E-step (Eq. 7) is different from its role in the logit adjustment loss (Eq. 9)?
3.  The performance of the EM model with the DR estimator (Table 1, "EM | DR") is significantly worse than the "SimPro | DR" model. Given that SimPro is presented as a variant of EM, what specific components of SimPro (e.g., confidence thresholding, logit adjustment loss) are critical for enabling the DR estimator to work so well?

**Rating**
- Overall (10): 9 — The paper presents a novel, theoretically sound, and empirically effective two-stage method that significantly advances realistic SSL.
- Novelty (10): 9 — The application of doubly robust estimation as a distinct first stage for class distribution estimation in SSL is highly novel.
- Technical Quality (10): 9 — The theoretical analysis is rigorous and the experiments are comprehensive, including important ablations and comparisons.
- Clarity (10): 9 — The paper is very well-written, with clear explanations and helpful figures (Fig. 2) that make a complex topic accessible.
- Confidence (5): 5 — I am confident in my assessment as I am familiar with the literature on SSL, long-tailed learning, and causal inference.

***

### **Review 2**

**Summary**
The paper proposes a two-stage algorithm for realistic long-tailed semi-supervised learning (RTSSL). The first stage is dedicated to estimating the unknown class distribution of the unlabeled data, `P(Y|A=0)`. To do this, the authors employ a doubly robust (DR) estimator that requires nuisance function estimates for a classifier `P(Y|X)` and a missingness mechanism `P(A|Y)`, which are obtained from an initial EM-based model (SimPro). The second stage then uses this fixed distribution estimate within an existing SSL algorithm to train the final classifier. The authors claim their DR estimator is more accurate and that this leads to better downstream classification accuracy.

**Soundness**
The technical soundness of the paper has some significant question marks, particularly regarding the link between the theory and the practical implementation.

1.  **Theoretical Assumptions:** The proof of Theorem 3.2 relies on Assumption 3.1, which states that the nuisance models converge at an `o_p(N^{-1/4})` rate. The justification provided is a brief reference to other works, noting that NNs are consistent but biased. This is a very strong assumption for complex, potentially mis-specified neural networks trained on finite, long-tailed data, and its validity is not established for the context of this paper.
2.  **Sample Splitting:** The proof in the appendix (Assumption A.1) explicitly requires sample splitting to ensure the nuisance models are estimated on data independent from the data used for the final averaging. This is a critical condition for the theoretical guarantees to hold. However, the experimental section (Section 4) does not mention whether sample splitting was actually used. If it was not, the theoretical claims (Theorem 3.2) do not apply to the reported experiments, and the estimator may suffer from bias due to overfitting.
3.  **Estimator Logic:** The best-performing stage-1 method is "SimPro | DR" (Table 1). This means the SimPro model is used to generate the nuisance functions `P(Y|X)` and `P(A|Y)`. The DR estimator then corrects the distribution estimate derived from these same functions. While DR estimation is designed for this, the massive improvement shown in Figure 1 and Table 1 (e.g., SimPro's OR estimate vs. SimPro's DR estimate) seems almost circular. It suggests the initial SimPro model is simultaneously good enough to provide useful nuisance functions but bad enough to produce a terrible direct estimate. This dynamic needs more explanation.

**Presentation**
The paper is dense and can be difficult to follow. The flow from the general EM framework (Section 3.1) to the specific SimPro implementation (Section 3.2) and then to the DR estimator (Section 3.3) feels disjointed. A clearer, more integrated explanation of how these pieces fit together would be beneficial. For instance, it's stated that SimPro is a "reparameterization" of the EM algorithm, but the practical differences (logit adjustment loss, confidence thresholding) seem more substantial than a simple reparameterization and are key to its performance, which complicates the narrative.

**Contribution**
The core idea is to apply a known statistical tool (DR estimation) to the problem of distribution estimation in SSL. While the application is novel, the tool itself is not. The contribution's significance hinges on the claimed empirical improvements. While the gains on CIFAR-10 are notable (Table 3), the results on CIFAR-100 are marginal at best (Table 4), which questions the general applicability and robustness of the proposed method. The paper's contribution would be stronger if the gap between theory and practice were closed and the benefits were more consistent across challenging datasets.

**Strengths**
1.  **Interesting Problem Framing:** Connecting RTSSL to non-ignorable missingness and semi-parametric theory is an insightful perspective.
2.  **Ablation Study:** The ablation in Table 8, which compares the two-stage approach to a batch-update version and a DR-risk method, is a strong point of the paper, as it provides clear evidence for the chosen design.
3.  **Identifies a Key Problem:** The paper correctly identifies and illustrates (Figure 1) that naive estimation of the unlabeled distribution in RTSSL is a significant problem.

**Weaknesses**
1.  **Mismatch between Theory and Practice:** The theoretical guarantees rely on assumptions (sample splitting, `N^{-1/4}` convergence) that are not confirmed to hold in the experimental setup, weakening the main theoretical claims.
2.  **Inconsistent Performance Gains:** The method provides large gains on CIFAR-10 but very minor gains on the more challenging CIFAR-100 dataset, raising questions about its effectiveness when the number of classes is large and labeled data is extremely scarce.
3.  **Clarity of Implementation Details:** Key details, such as how `P(A|Y)` is estimated from the SimPro model (which estimates `P(Y|A=0)`), are not explicitly detailed and seem to be inferred via Bayes' rule, which could be stated more clearly.

**Questions**
1.  Did you use sample splitting in your experiments to satisfy Assumption A.1 in the appendix? If not, how does this affect the validity of applying Theorem 3.2 to your results, and have you analyzed the potential bias from not using it?
2.  Assumption 3.1 requires an `o_p(N^{-1/4})` convergence rate for the nuisance neural networks. Can you provide stronger justification for why this assumption should hold in your experimental setting, especially given the long-tailed data and potential for model mis-specification?
3.  Why does the DR estimator provide such a dramatic improvement over the OR estimator when both are derived from the same SimPro model (Table 1)? Does this imply that the `P(A|Y)` estimate is significantly more accurate than the `P(Y|X)` estimate from SimPro, or is there another dynamic at play?

**Rating**
- Overall (10): 5 — The idea is interesting, but the significant gap between the theoretical requirements and the practical implementation, along with inconsistent empirical gains, raises concerns about the method's soundness.
- Novelty (10): 7 — The application of DR estimation in this specific two-stage manner is novel, though the underlying statistical tools are not.
- Technical Quality (10): 4 — The theoretical claims are not well-supported by the experimental methodology (re: sample splitting), and the justification for key assumptions is weak.
- Clarity (10): 6 — The paper is dense and the connections between the different components of the method could be explained more clearly.
- Confidence (5): 5 — I am highly confident in my assessment, having expertise in both machine learning methodology and statistical theory.

***

### **Review 3**

**Summary**
This paper presents a practical, two-stage method to tackle semi-supervised learning on long-tailed data where the unlabeled data has a different class distribution. The core idea is to first run a preliminary SSL model (SimPro) to get initial estimates, use these estimates in a "doubly robust" formula to get a better estimate of the unlabeled class distribution, and then freeze this distribution estimate and use it to improve a second, main SSL model (e.g., SimPro or BOAT). Experiments show this "plug-in" approach improves classification accuracy on several datasets.

**Soundness**
The overall logic of the method is sound from a practical standpoint. The problem of biased pseudo-labels in RTSSL is well-known, and trying to get a better estimate of the unlabeled distribution is a direct way to address it. The paper's key design choice—a two-stage process—is empirically validated in the ablation study (Table 8), where it is shown to outperform a more integrated "batch-update" approach. This suggests that stabilizing the distribution estimate before the final training phase is beneficial. The use of a smaller model for the first stage (as mentioned for ImageNet-127) is also a sensible, practical choice.

**Presentation**
The paper is structured logically, but the heavy reliance on statistical terminology and equations makes it less accessible for a practitioner-focused audience. Figure 2 is very helpful as it provides a clear, visual map of the entire process. The tables are well-formatted and easy to read. The use of "+" in method names (e.g., SimPro+) is a clear way to denote the proposed modification. However, the paper could benefit from a more explicit algorithm box summarizing the practical steps a researcher would need to take to implement the method.

**Contribution**
The main contribution is a modular and practical enhancement for existing RTSSL algorithms. Rather than proposing an entirely new, complex model, the authors offer a "booster" that can be applied to SOTA methods like SimPro and BOAT. The contribution is less about groundbreaking theory and more about a clever and effective engineering of existing ideas. The demonstrated accuracy improvements (e.g., +3.4% for SimPro on CIFAR-10 consistent, Table 3) are practically significant in some cases, validating the utility of the approach.

**Strengths**
1.  **Practical and Modular:** The method is designed as a plug-in that improves existing algorithms, which is a very useful contribution for the research community.
2.  **Strong Empirical Validation:** The method is tested on four different datasets, and the core claims are backed by extensive experiments, including important ablations in Section 4.3.
3.  **Cost-conscious Design:** The authors show awareness of practical constraints by using a smaller model for the first stage on ImageNet-127 (Section 4.1), which is a thoughtful detail.
4.  **Clear Problem Motivation:** The paper does an excellent job of showing *why* this work is needed with the visualizations in Figure 1, which clearly depicts the poor distribution estimates of a baseline method.

**Weaknesses**
1.  **Increased Complexity and Cost:** The most significant drawback is the added complexity. The two-stage process effectively requires training a model twice. The paper lacks any discussion of the computational overhead in terms of training time or resource usage. For large datasets, this could be a major barrier to adoption.
2.  **Hyperparameter Sensitivity:** The new first stage introduces its own set of hyperparameters (e.g., learning rate, number of epochs, model architecture). The paper states it follows the settings of (Du et al., 2024), but those settings are for the full training run. There is no guidance on how to set hyperparameters for the stage-1 estimation task, which could be critical for its success.
3.  **Modest Gains in Some Scenarios:** The performance lift on CIFAR-100 is very small (Table 4), often within the margin of error. This suggests the method's utility might be limited in settings with many classes and very few labeled examples per class.

**Questions**
1.  What is the practical cost of the two-stage approach? For example, on CIFAR-10, how much longer does it take to train "SimPro+" compared to the original "SimPro"?
2.  How did you determine the hyperparameters for the stage-1 training (e.g., how many epochs did you run it for)? Is the quality of the final distribution estimate sensitive to this choice?
3.  Could the proposed method be simplified to a single stage? For example, could one use the DR estimate from early in training to guide the rest of training, perhaps with a warm-up period, to avoid a full two-stage process? Your "batch-update" ablation touches on this, but perhaps a less aggressive update schedule would work better.

**Rating**
- Overall (10): 7 — A practical and well-engineered method with solid empirical results, but its added complexity and modest gains in some settings limit its impact.
- Novelty (10): 6 — The combination of existing ideas is clever, but it's more of an incremental engineering contribution than a fundamentally new concept.
- Technical Quality (10): 8 — The experiments are well-designed and thorough, especially the ablations, which strongly support the design choices.
- Clarity (10): 7 — The paper is generally clear but could be made more accessible to practitioners by reducing jargon and adding a concrete algorithm summary.
- Confidence (5): 4 — I am confident in my assessment from a practitioner's perspective, though less so on the deep theoretical nuances.

***

### **Review 4**

**Summary**
This paper tackles the challenging problem of semi-supervised learning under long-tailed distributions with label shift (RTSSL). The authors propose a principled two-stage method that first estimates the unlabeled class distribution and then uses this estimate to improve a downstream classifier. The novelty lies in framing the distribution estimation task as a problem of non-ignorable missing data and leveraging a doubly robust (DR) estimator from the semi-parametric statistics literature. This approach is shown to produce more accurate distribution estimates and, consequently, better classification accuracy when integrated with modern SSL algorithms like SimPro and BOAT.

**Soundness**
The methodological approach is sound and well-contextualized within the relevant literature. The choice to frame the problem under the non-ignorable missingness framework (Section 2) is apt and provides a solid theoretical foundation. The label shift assumption (`P(X|Y,A) = P(X|Y)`) is standard for this line of work and is what makes the problem identifiable. The derivation of the DR estimator (Eq. 13) and its theoretical properties (Theorem 3.2) are standard results from semi-parametric theory, but their application in this deep learning context is non-trivial and well-executed. The empirical work is strong, particularly the ablation study in Section 4.3, which convincingly argues against alternative approaches like using a DR-risk objective or on-the-fly updates.

**Presentation**
The paper is well-structured and clearly written. It successfully synthesizes concepts from three different fields: long-tailed SSL, missing data theory, and causal inference. The background section (Section 2) does a good job of situating the work with respect to recent, related papers like (Sportisse et al., 2023) and (Du et al., 2024). Figure 2 provides a very clear conceptual diagram of the proposed pipeline. The argument flows logically from problem definition to the proposed solution and empirical validation.

**Contribution**
The paper's primary contribution is in bridging the gap between the theory of semi-parametric efficient estimation and the practice of deep semi-supervised learning. While others (e.g., Sportisse et al., 2023; Hu et al., 2022) have also explored DR methods for SSL, this paper's distinct focus on *first estimating the distribution parameter* `P(Y|A=0)` as a separate stage is a key differentiator. This two-stage approach is shown to be more stable and effective than the DR-risk minimization approach proposed by others (Table 8). This work not only provides a better-performing algorithm but also offers a new and effective paradigm for tackling parameter estimation within larger deep learning pipelines. It refines the conversation around how to best leverage tools from statistics in modern machine learning.

**Strengths**
1.  **Excellent Conceptual Framing:** The paper's strength lies in its clear and correct framing of RTSSL as a non-ignorable missing data problem, which motivates the use of powerful statistical tools.
2.  **Principled Integration of Theory and Practice:** The work successfully adapts a theoretically optimal estimator (DR) for a practical deep learning problem and shows it works. The comparison to SimPro (Section 3.2) and DR-risk (Section 4.3) shows a deep understanding of the related literature.
3.  **Demonstrably Superior Design:** The ablation study (Table 8) provides compelling evidence that the proposed two-stage design is superior to more integrated alternatives, particularly the DR-risk approach, which the authors correctly identify as potentially unstable.
4.  **Clear Placement in Literature:** The paper does a commendable job of discussing and differentiating itself from very recent and highly related work, which is crucial in a fast-moving field.

**Weaknesses**
1.  **Stability of IPW:** The paper argues that the DR-risk approach is unstable due to the inverse weighting term (Section 4.3, Appendix B). However, the proposed DR estimator (Eq. 13) also contains the exact same inverse weighting term `1/P(A=1|Y)`. The paper claims it's "easier to control for estimation than for training," but this critical point is not elaborated upon. A more detailed explanation of why instability is a problem for one but not the other is needed.
2.  **Limitations of Label Shift:** The entire framework rests on the label shift assumption (Eq. 2). While standard, this is a strong assumption that may not hold in all real-world scenarios (e.g., if the nature of images for a class changes between labeled and unlabeled sets). A brief discussion of this limitation and potential future work to relax it would strengthen the paper.

**Questions**
1.  Could you please expand on the argument that the instability from the IPW term is more manageable in your DR estimator (Eq. 13) than in the DR-risk objective (Eq. 28)? Is it because the estimator is a single computation post-training, whereas the risk is used iteratively in the optimization loop, amplifying noise from small probabilities?
2.  The method relies on the label shift assumption. How do you think the proposed two-stage approach would perform if this assumption were violated, for example, in a more general dataset shift scenario where `P(X|Y)` also changes? Would the DR estimator still provide some benefit?
3.  Your work shows that separating the estimation of a low-dimensional parameter (`P(Y|A=0)`) from the main classifier training is beneficial. Do you see this as a more general principle that could be applied to other problems in machine learning where one needs to estimate and use key nuisance parameters?

**Rating**
- Overall (10): 9 — An excellent paper that provides a novel, principled, and effective solution to an important problem by thoughtfully bridging statistical theory and deep learning practice.
- Novelty (10): 8 — The core contribution is a novel and insightful reframing of how to apply DR methods in SSL, differentiating it clearly from prior art.
- Technical Quality (10): 9 — The work is technically deep, with sound theory and strong, well-designed experiments that support the main claims.
- Clarity (10): 9 — The paper is exceptionally clear and well-organized, successfully communicating complex ideas from multiple domains.
- Confidence (5): 5 — I am very confident in my evaluation, as my expertise aligns well with the topics covered in this paper.