Summary
The paper proposes DeepCompress, a reinforcement learning framework that adaptively modulates chain-of-thought length to balance accuracy and inference cost. The key idea is a dual, capability-aware length reward whose sign and magnitude are determined online by a difficulty signal β computed per question as the difference between the within-question group pass ratio and an EMA-smoothed batch pass ratio. Lengths are standardized within each sampled group and passed through a sigmoid-based reward that is scaled by a small factor and added to a correctness outcome reward. Two stabilizers are introduced: the length reward is applied only to correct samples to avoid reward hacking, and the batch pass ratio is smoothed via EMA with optimistic initialization to mitigate early-training instability. The approach is motivated by observed trends that shorter responses can improve pass@1 while longer responses help pass@k, and aims to allocate reasoning budget accordingly—compressing on easy items and encouraging exploration on hard ones. Experiments across seven mathematical reasoning benchmarks report consistent pass@1 improvements over strong RL baselines alongside substantial reductions in generated tokens. Diagnostics suggest the method increases policy entropy, adapts length in response to β, and promotes effective reflection on challenging problems. The paper is generally clear and well-illustrated, though there is a minor inconsistency regarding whether GRPO or DAPO is the underlying RL algorithm and some experimental details are terse.

Strengths
- Well-motivated formulation that reconciles the empirical tension between pass@1 vs pass@k length effects, and translates it into a capability-aware, online difficulty signal.
- Simple, practical mechanism with bounded reward shaping and two sensible robustness measures (correctness conditioning and EMA smoothing) that are easy to integrate into existing RL recipes.
- Strong empirical results on seven math datasets, including challenging benchmarks (e.g., AIME24/25), showing both accuracy gains and substantial token savings relative to competitive RL baselines.
- Informative diagnostics linking the method to exploration and reflection dynamics (e.g., increased policy entropy, adaptive length modulation, and more effective reflection on hard cases).
- Clear exposition with concrete equations and visualizations that make the mechanism interpretable; implementation appears straightforward.

Weaknesses
- Methodological clarity and consistency: the paper cites GRPO in the introduction but uses DAPO in the methods/experiments; the precise optimizer/objective and their interaction with the shaped reward are not fully clarified.
- Limited sensitivity analysis: key hyperparameters and edge cases (e.g., the shaping scale α, EMA coefficient λ and initialization, the standardization epsilon, group size G, and behavior when within-group variance σ is small) are not systematically studied. The dependence on within-group variance could destabilize the standardized length signal, and the effect of coarse β resolution with small groups is not analyzed.
- Missing component ablations: the individual contributions of correctness conditioning and EMA smoothing are not isolated, making it hard to assess their necessity and impact.
- Compute fairness and reporting: training-time costs (tokens seen, GPU type/hours, throughput) and matched-budget comparisons to baselines are absent; baselines are primarily re-evaluated under shared inference settings.
- Evaluation scope: all results are on math datasets; generalization to other domains with noisier verification remains speculative. The approach relies on reliable final-answer verification, which may limit applicability outside math.
- Analysis gaps: little discussion of how length shaping interacts with the underlying RL objective (e.g., relative ranking and gradient variance in DAPO), the early-training impact of optimistic EMA initialization on exploration vs efficiency, and potential distributional shifts between training group size and evaluation settings. Pass@k evaluations at test time and the effect of sampling temperature are not explored.
- Baseline coverage and alternatives: direct comparisons against closely related adaptive length methods are limited, and there is no study using oracle/static difficulty labels or learned difficulty predictors to isolate the benefit of the model-aware difficulty signal. Experimental details about training data composition are also sparse.
