Review 1

Summary
- The paper proposes DeepCompress, a reinforcement-learning framework that modulates Chain-of-Thought length via a dual length reward conditioned on model-aware difficulty.
- Difficulty is computed online per question as β = P_g(x_i) − P_b, where P_g is the within-question group pass ratio and P_b is the batch-average pass ratio (Eq. 8). β > 0 marks “Simple” (shorter preferred), β < 0 marks “Hard” (longer encouraged).
- A sigmoid function over standardized relative length (z) forms the length reward, scaled by α and added to an outcome reward R_o (Eqs. 3–5, 9).
- Two robustness refinements are introduced: apply length reward only to correct samples (Eq. 10), and smooth P_b via EMA with optimistic initialization (Eq. 11).
- On seven math benchmarks, DeepCompress improves pass@1 over RL baselines while reducing tokens substantially (Table 1; Fig. 3). Diagnostics show increased policy entropy and adaptive length dynamics (Fig. 4) and more effective reflection on hard problems (Table 2).

Soundness
- The core mechanism is logically consistent: using in-batch performance to infer relative difficulty and switch the sign of β is a principled way to reconcile the contradictory evidence that short responses improve pass@1 while long responses improve pass@k (Fig. 1; Sec. 3.2).
- Reward shaping is bounded and mild (R_l ∈ [0, α] with α=0.2; Appendix B), which reduces risk of destabilizing RL updates dominated by length rather than correctness (Eqs. 5, 9).
- Correctness-conditioned length reward (Eq. 10) directly mitigates reward hacking where incorrect but short/long outputs would be over-rewarded. The EMA-smoothed P_b (Eq. 11) addresses non-stationarity early in training.
- Potential edge cases are acknowledged but not fully analyzed: small within-group variance σ_i in Eq. 3 can inflate z; the authors add ε but do not study sensitivity. β is coarse when G=32 (P_g steps of 1/32); the paper does not analyze effect of G on stability, although the empirical curves in Fig. 4 suggest learning is stable.

Presentation
- The paper is clearly structured, with the motivation grounded in Fig. 1 and a clean formulation of R_z, β, and the final reward (Eqs. 3–9).
- Figures 2, 17–18 effectively visualize how β flips the slope of the reward-length relationship. The training/evaluation dynamics in Fig. 4 are informative.
- Minor inconsistencies: the Introduction cites GRPO as the base RL, but Sec. 4.1 uses DAPO (possible slip; Sec. 4.1 and Appendix B imply DAPO is used). Also, Sec. 3.1 says “refer to Section 4.2” for standardization details—this is correct but slightly disruptive.
- The experimental setup is mostly reproducible (Appendix B hyperparameters; code link in Abstract), though some dataset and compute details could be expanded (e.g., precise training corpus for RL, wall-clock/compute budgets).

Contribution
- The main novelty is the dual reward that adaptively flips preference for length using a model-aware signal tied to current capability (Eqs. 6–8), moving beyond static length penalties/bonuses and prompt-only adaptations.
- The paper offers a practical recipe to improve both accuracy and efficiency in math reasoning models, with strong gains on AIME and consistent token reductions (Table 1; Fig. 3).
- The diagnostic narrative (policy entropy, reflection frequency) ties the method to exploration/reflection dynamics (Fig. 4; Table 2), which is valuable for understanding behavior.

Strengths
- Clear motivation that reconciles pass@1 vs pass@k length trends (Fig. 1).
- Simple, well-scoped method with bounded shaping and two robustness fixes (Eqs. 10–11).
- Strong empirical results across seven benchmarks with consistent efficiency gains (Table 1; Fig. 3).
- Insightful training curves and behavior analysis (Fig. 4; Table 2).

Weaknesses
- Methodological clarity gaps: RL base algorithm naming inconsistency (GRPO vs DAPO; Intro vs Sec. 4.1), and lack of analysis on sensitivity to σ_i, ε, α, and G (Eqs. 3–5).
- Limited ablations: “length penalty/bonus” are included (Sec. 5.3), but no ablation isolating the two robustness components (Eq. 10 and Eq. 11), nor a comparison to using oracle/static difficulty labels.
- Fairness and compute reporting: no training cost comparison vs baselines; baselines are re-evaluated but it’s unclear if training budgets are matched (Sec. 5.1, Table 1).
- Generality beyond math remains untested (all results are mathematical reasoning benchmarks).

Questions
- Can the authors clarify the RL algorithm discrepancy (GRPO in Introduction vs DAPO in Sec. 4.1)? Which optimizer and objective are actually used?
- How sensitive is performance to α, λ, ε, and G? Could the authors provide a small sensitivity sweep or at least report stability ranges (Eqs. 3–5, 11)?
- What happens when σ_i is very small (narrow length variation within a group) despite the added ε? Do you clip z or standardize across a larger pool?
- Could you report training compute (GPU-days) and token throughput vs baselines to substantiate efficiency claims at training time, not just inference (Sec. 5.1)?
- How would DeepCompress fare with oracle/static difficulty labels, or with difficulty predicted by a separate classifier? This would help isolate the contribution of model-aware difficulty (Sec. 4.3).
- Did you examine failure cases where pass@1 degrades on specific categories due to over-compression? Any per-topic breakdowns?

Rating
- Overall (10): 8 — Strong, well-motivated method with consistent accuracy and length gains across seven math benchmarks (Table 1, Fig. 3), though some ablations and clarity gaps remain (Sec. 4.1; Eqs. 10–11).
- Novelty (10): 8 — The β-controlled dual reward with model-aware difficulty (Eqs. 6–8; Fig. 2) is a meaningful advance over static length penalties/bonuses.
- Technical Quality (10): 8 — Solid formulation and robustness fixes (Eqs. 10–11) with convincing diagnostics (Fig. 4; Table 2), but missing sensitivity analyses for key hyperparameters (Sec. 4.2–4.4).
- Clarity (10): 8 — Generally clear and well-illustrated (Figs. 1–4), with minor inconsistencies and some missing experimental details (Intro vs Sec. 4.1; Sec. 5.1; Appendix B).
- Confidence (5): 4 — High confidence based on explicit equations, figures, and comprehensive results, tempered by the limited ablations and minor inconsistencies checked across Secs. 3–5 and Appendix B.


Review 2

Summary
- DeepCompress aims to simultaneously improve accuracy and reduce inference cost by dynamically modulating reasoning length using a dual reward.
- The approach standardizes per-question lengths in a sampled group, uses a sigmoid-based length reward R_z with sign determined by β = P_g − P_b (Eqs. 3–5, 8), and adds this to outcome reward R_o (Eq. 9).
- Two stabilizers are introduced: correctness-conditioned application of the length reward (Eq. 10) and EMA smoothing of P_b with optimistic initialization (Eq. 11).
- Experiments across seven math datasets show consistent accuracy gains and shorter outputs vs strong RL baselines (Table 1; Fig. 3), with diagnostic analyses indicating improved exploration and reflection (Fig. 4; Table 2).

Soundness
- The method leverages a relative difficulty signal rooted in group sampling—a sound choice given that group-based RL (DAPO/GRPO) already compares solutions within a batch.
- Conditioning R_l on correctness is prudent; otherwise short but wrong solutions would be over-rewarded. The EMA-smoothed P_b helps control early-training instability when pass rates are low.
- However, the dependence on within-group standardization (Eq. 3) makes the reward sensitive to the variance of generated lengths; the paper does not report safeguards beyond ε nor sensitivity studies.
- The choice to evaluate primarily with pass@1 at 16 samples while training with G=32 introduces a distributional shift; the method still improves pass@1, but the paper does not analyze the effect of k or sampling temperature on the gains (Sec. 5.1).

Presentation
- The exposition is accessible; Equations 3–11 and Fig. 2/17/18 make the mechanism concrete.
- Empirical results are clearly tabulated, and the response-length plots are helpful (Table 1; Fig. 3). Training dynamics (Fig. 4) are well presented.
- Some experimental details are terse: training dataset composition for RL is unclear in Sec. 5.1 (the paper cites DeepMath-103K in Sec. 3.1 but not explicitly as RL data), and compute budgets are not reported. The GRPO vs DAPO inconsistency adds confusion (Intro vs Sec. 4.1).

Contribution
- The main contribution is an online, capability-aware difficulty signal that flips the sign of the length reward, allowing exploration on hard cases without sacrificing efficiency on easy ones.
- The paper ties this to an empirical observation that longer chains increase pass@k, helping RL identify positives (Fig. 1).
- The approach is straightforward to integrate into existing RL recipes and yields state-of-the-art results on multiple math benchmarks (Table 1).

Strengths
- Clear motivation anchored in pass@1 vs pass@k trends (Fig. 1).
- Practical mechanism with bounded shaping and stability-oriented refinements (Eqs. 10–11).
- Strong empirical improvements and substantive token savings (Table 1; Fig. 3).
- Useful behavioral diagnostics (Fig. 4; Table 2).

Weaknesses
- Fairness and compute: no training-time cost comparisons or matched-budget re-trainings of baselines are reported; only re-evaluation under shared inference settings (Sec. 5.1). I double-checked Table 1 and Sec. 5.1–5.2; compute details are absent.
- Limited ablations: no isolation of the impact of correctness-conditioning (Eq. 10) and EMA smoothing (Eq. 11). I searched Sec. 5.3–5.4 and Appendix B; only fixed-β variants are presented.
- Generalization: all evaluation is in math; claims about general reasoning remain untested. I verified Sec. 5.2 table entries and datasets; all math-focused.
- Minor inconsistency about RL algorithm choice (Intro vs Sec. 4.1).

Questions
- Please report the RL training dataset(s), the number of tokens seen, GPU type/hours, and batch-level throughput, and compare to baselines to support efficiency and fairness claims.
- Can you provide an ablation turning off Eq. 10 and Eq. 11 separately to quantify their contributions?
- How sensitive are results to G and to the variance of length within a group? What happens if σ_i is very small in Eq. 3—for instance, do you clip z?
- Could you evaluate pass@k for several k at test time to validate the hypothesized mechanism (longer reasoning helps pass@k) on the trained models, not just in the preliminary study?

Rating
- Overall (10): 7 — Method is sound and effective with clear empirical gains (Table 1; Fig. 3) but lacks compute/fairness reporting and full ablations (Sec. 5.1, 5.3).
- Novelty (10): 7 — Adaptive β-based dual reward tied to online difficulty (Eqs. 6–8; Fig. 2) is a meaningful extension over static penalties/bonuses.
- Technical Quality (10): 7 — Robustness fixes (Eqs. 10–11) and diagnostics (Fig. 4) are solid, but sensitivity analyses and component ablations are missing.
- Clarity (10): 8 — Generally clear with informative figures; minor inconsistencies and missing experimental details (Intro vs Sec. 4.1; Sec. 5.1).
- Confidence (5): 4 — High confidence in reading and cross-checking equations, figures, and results; reservations due to missing ablations and compute details.


Review 3

Summary
- The paper targets the “overthinking/underthinking” dilemma by shaping length adaptively: shorter chains for simple cases and longer chains for hard cases.
- It estimates difficulty online using β = P_g − P_b (Eq. 8) and computes a sigmoid-based length reward over standardized relative length (Eq. 3–5), added to the correctness outcome reward (Eq. 9).
- Correctness-conditioning (Eq. 10) and EMA smoothing (Eq. 11) aim to stabilize training and reduce reward hacking.
- Experiments on seven math benchmarks show accuracy improvements and fewer tokens compared to RL baselines (Table 1; Fig. 3).

Soundness
- The β mechanism aligns with an intuitive curriculum: easy items get compressed, hard items get exploratory budget. The use of bounded shaping and correctness conditioning is justified.
- However, the paper does not analyze the interaction between the shaped reward and the underlying RL objective (DAPO). For instance, with α=0.2 (Appendix B), the shaped reward can skew relative preferences among correct samples of different lengths; there is little discussion of how this interacts with the relative ranking in DAPO or gradient variance.
- The early-training initialization P_{b,0}=1.0 (Eq. 11) ensures β<0 and thus exploration, but the paper does not analyze how quickly this decays with λ=0.99 or whether too much initial exploration harms efficiency. There is no explicit schedule or sensitivity study.
- The approach depends on within-group variance; if σ_i≈0 (Eq. 3), z can be unstable even with ε. No clipping/normalization strategy is reported.

Presentation
- The paper is readable, with comprehensive figures and equations. The narrative from Fig. 1 to Eq. 8 is coherent.
- There is a small but noticeable inconsistency (Intro cites GRPO; Sec. 4.1 uses DAPO). Also, some crucial choices (α=0.2; λ=0.99) are only in Appendix B with no rationale.
- Ablation coverage is limited to fixed-β variants; component-wise ablations for Eq. 10 and Eq. 11 are missing.

Contribution
- The dynamic, capability-sensitive length shaping is a useful addition to the RL toolbox for reasoning. It operationalizes a widely observed phenomenon (longer responses help pass@k) within a principled online scheme.
- The empirical improvements are consistent and sizable in some hard settings (e.g., AIME24/25; Table 1) while reducing tokens, which is practically valuable.

Strengths
- Clear and motivated formulation (Eqs. 3–11) with interpretable β (Fig. 2).
- Strong empirical results and efficiency gains (Table 1; Fig. 3).
- Training dynamics and behavioral analysis offer insight (Fig. 4; Table 2).

Weaknesses
- Missing sensitivity and stability analyses for key hyperparameters and edge cases (α, λ, ε, G, σ_i→0). I checked Sec. 5.3–5.4 and Appendix B; no such study is provided.
- No comparisons to oracle/static difficulty labels or to learned difficulty predictors to validate the necessity of model-aware difficulty specifically (Sec. 4.3).
- Limited scope (math only) and missing compute fairness reporting. I searched Sec. 5.1–5.2; compute details are absent.

Questions
- Could you provide a brief theoretical or empirical analysis of how the shaped reward interacts with DAPO’s objective and variance reduction? For example, how does α affect the gradient magnitude relative to R_o?
- What is the effect of different initializations of P_b and λ on early training exploration vs eventual compression (Eq. 11)?
- Do you clip z or cap |z| to handle narrow σ_i in Eq. 3? What ε value is used?
- Would using oracle difficulty labels change results, and if so by how much?

Rating
- Overall (10): 6 — Promising idea with good results (Table 1; Fig. 3), but lacking crucial sensitivity analyses and component ablations (Secs. 4.2–4.4; 5.3).
- Novelty (10): 7 — Online, model-aware difficulty-driven dual reward (Eqs. 6–8) is a neat extension of length shaping.
- Technical Quality (10): 6 — Correctness conditioning and EMA are sensible (Eqs. 10–11), yet stability, sensitivity, and interaction with DAPO are under-explored.
- Clarity (10): 7 — Generally clear with minor inconsistencies and sparse justification for hyperparameters (Intro vs Sec. 4.1; Appendix B).
- Confidence (5): 4 — I carefully checked equations, figures, and experimental sections; conclusions seem reliable but hinge on unreported sensitivities.


Review 4

Summary
- DeepCompress introduces a dual reward scheme to adapt reasoning length based on online difficulty signals computed from group/batch pass ratios (Eq. 8).
- The length component uses a sigmoid over standardized group-relative length (Eq. 3–5) and is added to a binary outcome reward (Eq. 9).
- To avoid reward hacking and instability, the length reward is applied only to correct samples (Eq. 10) and the batch pass ratio is smoothed via EMA initialized at 1.0 (Eq. 11).
- Experiments on seven math benchmarks show new SOTA among open RL recipes with shorter outputs, plus diagnostic evidence of adaptive exploration and effective reflection (Table 1; Figs. 3–4; Table 2).

Soundness
- The design is internally consistent: β’s sign controls the direction of length preference and its magnitude modulates pressure (Sec. 4.3–4.4). This addresses the observed discrepancy between pass@1 and pass@k vs length (Fig. 1).
- Conditioning on correctness and the mild scale α=0.2 (Appendix B) keep shaping secondary to correctness, which is good RL hygiene.
- A possible concern is the reliance on accurate verification; if the rule-based verifier is noisy, β and the selective application of R_l could misguide the policy. The paper uses exact final-answer matching (Sec. 4.1), which is reliable in math but it’s a limitation for other domains.

Presentation
- The paper is well structured with clear equations and helpful visualizations (Figs. 1–4; Eqs. 3–11).
- Minor editorial issues: RL algorithm naming inconsistency (GRPO vs DAPO) and limited discussion of hyperparameter choices.
- Results are easy to follow; response length statistics are comprehensive (Fig. 3).

Contribution
- The work provides a practical and generalizable mechanism to marry efficiency and accuracy in RL for reasoning models via on-policy, capability-aware difficulty estimation.
- It demonstrates that dynamic length shaping can push pass@1 up while shrinking token budgets—contrary to many compression-only methods that hurt accuracy (Table 1).

Strengths
- Methodologically simple, easy to implement, and demonstrably effective.
- Strong improvements on hard math sets (AIME24/25) with substantial length reductions (Table 1; Fig. 3).
- Good training diagnostics and behavior analysis linking policy entropy and reflection to performance (Fig. 4; Table 2).

Weaknesses
- Evaluation breadth: only math is tested; claims of general reasoning improvements remain speculative. I checked Sec. 5.2 and Table 1 entries—no non-math results.
- Ablations: the contribution of Eq. 10 and Eq. 11 is not isolated. I verified Sec. 5.3–5.4; only fixed-β ablations are provided.
- Baseline coverage: while several RL baselines are included, direct comparisons against closely related adaptive length RL methods (e.g., L1, ALRS) under matched training are not reported beyond citations (Sec. 2). I double-checked Table 1; those methods are not in the table.

Questions
- How robust is DeepCompress if the verifier has false negatives/positives? Could you report performance with small synthetic noise injected into R_o to emulate imperfect verifiers?
- Please report training compute and budgets; were DeepCompress and DeepMath-Zero trained under the same number of steps and tokens (Appendix B lists 500 steps for DeepCompress)?
- Can you provide ablations turning off correctness conditioning and EMA smoothing to quantify their impact?
- Could you run a small-scale non-math evaluation (e.g., GSM8K, strategy QA) to substantiate generality?

Rating
- Overall (10): 8 — Practical, well-motivated method with strong empirical results and substantial efficiency gains (Table 1; Fig. 3), though ablations and breadth could be improved.
- Novelty (10): 8 — Model-aware, sign-flipping length reward controlled by β (Eqs. 6–8; Fig. 2) is a meaningful innovation over static penalties.
- Technical Quality (10): 8 — Sound reward shaping with robustness measures (Eqs. 10–11) and informative diagnostics (Fig. 4), but missing component ablations and compute reporting.
- Clarity (10): 8 — Clear presentation with minor inconsistencies (Intro vs Sec. 4.1) and limited hyperparameter discussion.
- Confidence (5): 4 — High confidence after checking equations, figures, and experiments; some uncertainty due to missing ablations and compute details.