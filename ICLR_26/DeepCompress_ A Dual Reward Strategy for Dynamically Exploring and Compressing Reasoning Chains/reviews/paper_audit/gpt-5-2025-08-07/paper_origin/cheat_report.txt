Academic integrity and consistency assessment report

Summary
The manuscript presents a clear approach (DeepCompress) but contains several high-impact internal inconsistencies and numerical mismatches that materially affect replicability and trust in the reported results. Below are evidence-backed findings with precise anchors.

Major internal inconsistencies

1) Core RL algorithm inconsistency (GRPO vs DAPO)
- Introduction states GRPO is the basic RL algorithm: “With Group Relative Policy Optimization (GRPO… as our basic RL algorithm” (Section: Introduction, Block #4).
- Method and Experiments consistently state the training uses DAPO: “utilize DAPO as our RL algorithm” (Section: Method, 4.1 Zero RL, Block #12); “applied dynamic sampling policy optimization (DAPO)” (Section: Experiments, 5.1, Block #26).
- Impact: This contradiction makes the training recipe unclear and hampers reproducibility. Readers cannot tell whether GRPO or DAPO underpins the reported results.

2) β parameter definition and role conflict (hyperparameter vs dynamic difficulty signal)
- In 4.2, β is introduced as a hyperparameter controlling sigmoid steepness: “β is a hyperparameter controlling the steepness of the sigmoid function” (Section: Method, 4.2, Block #15).
- In 4.3, β is redefined as a dynamic per-question difficulty signal: “β_i = P_g(x_i) − P_b” (Equation 8; Section: Method, 4.3, Block #20) and used to set both mode and intensity (Section: Method, Mechanism of β, Block #21).
- Impact: The dual and conflicting roles create ambiguity in the actual reward shape. If β_i is bounded in (−1,1), steepness is constrained; if β is a free hyperparameter, steepness can be tuned independently. The paper does not resolve which formulation is used in experiments.

3) Final reward definition changed mid-paper without clarity on which variant is used in main results
- Unconditional reward: “Finally, the reward for RL optimization integrates both outcome reward and length reward: R = R_o + R_l.” (Equation 9; Section: Method, 4.3, Block #20).
- Later refinement: “Correctness-conditioned length reward… R = R_o + R_l if correct; R = R_o otherwise.” (Equation 10; Section: Method, 4.4, Block #23).
- No explicit statement clarifies whether Table 1 and Figure 3 results use Equation 9 or Equation 10. Appendix B (training details, Block #44) lists α and λ but not whether correctness-conditioning was enabled.
- Impact: This materially affects interpretation of training signals and could change conclusions on efficiency/accuracy trade-offs.

Numerical/factual mismatches

4) Claimed 7B average compression (16.6%) conflicts with provided per-benchmark lengths
- Claim: “On average, DeepCompress compresses the response length by … 16.6% with the 7B model.” (Section: Experiments, 5.2, Block #29).
- Evidence from provided lengths (Section: Experiments, 5.2, Block #29):
  - MATH500: 4,829 → 2,234 (≈53.7% reduction)
  - AMC23: 4,267 → 2,124 (≈50.2%)
  - OlympiadBench: 4,781 → 2,640 (≈44.8%)
  - MinervaMath: 4,481 → 2,744 (≈38.7%)
  - AIME24: 11,091 → 7,181 (≈35.2%)
  - AIME25: 6,397 → 5,816 (≈9.1%)
  - PolyMath: 5,334 → 4,476 (≈16.1%)
- The majority of datasets show reductions far larger than 16.6%. Without a stated weighting scheme, the 16.6% average appears inconsistent with the listed numbers.

5) AIME24 (3B) token reduction claim (37.6%) contradicts the listed lengths
- Claim: “DeepCompress-Zero-3B uses 37.6% less tokens [on AIME24]” (Section: Experiments, 5.2, Block #29).
- Evidence: Listed lengths are 10,922 → 4,130, which is ≈62.2% reduction (Section: Experiments, 5.2, Block #29).
- Impact: This is a clear numerical mismatch in a key efficiency narrative.

6) Training steps mismatch: configuration vs figures
- Configuration: “total_training_steps = 500” (Appendix B, Table 3; Section: Appendix, Block #44).
- Figures show training curves up to 600 steps (Figure 4 subplots: Section: Experiments, Blocks #34, #36–#39; the x-axis is labeled to 600).
- Impact: This discrepancy creates uncertainty about the actual training duration used to produce reported results.

Additional clarity issues that affect interpretability

7) Ambiguity in evaluation sampling for pass@1
- In 5.1, the paper states: “we sample 16 responses for each question and report the pass@1 accuracy” (Section: Experiments, Block #26). Given pass@1 reduces to c/n for n samples (Equation 1, Section: Preliminaries, Block #8), the phrase can be misinterpreted as single-sample accuracy. While the authors say they “re-evaluate all baseline models under our precise evaluation settings” (Block #26), the evaluation protocol for pass@1 should be clarified to avoid misinterpretation.
- Impact: Moderate; affects comparability understanding but not core claims.

8) Cross-reference ambiguity on standardized length
- Preliminaries state: “We standardized the lengths of all sampled 8,192 responses for a given problem (refer to Section 4.2)” (Section: Preliminaries, Block #8), but Section 4.2 defines standardization per question using G responses (typically 32; Section: Appendix, Table 3, Block #44).
- Impact: Minor; does not invalidate results, but cross-referencing suggests different scale of standardization (G vs 8,192) and could confuse replication of Figure 1.

Recommendations
- Resolve RL algorithm inconsistency: Explicitly state whether GRPO or DAPO is used throughout and correct the Introduction (Block #4) or Methods (Block #12) accordingly.
- Unify the definition of β: Clarify whether β is a fixed hyperparameter (steepness) or the dynamic difficulty term β_i = P_g − P_b, and update Equation 4 and surrounding text to reflect the implemented approach used in experiments.
- Specify the reward variant used in Table 1 and Figure 3: Indicate whether Equation 9 or Equation 10 is applied in all reported results; if mixed, provide ablation and label figures accordingly.
- Correct efficiency claims: Update 7B average compression and the AIME24 (3B) percentage to match the provided per-benchmark lengths, or explain the weighting scheme used to derive averages.
- Align training steps in figures with the configuration or explain why figures extend to 600 steps (e.g., additional evaluation-only steps).
- Clarify pass@1 evaluation protocol when n > 1 to avoid confusion.

Conclusion
The manuscript contains multiple substantive internal inconsistencies (RL algorithm, β semantics, reward definition) and clear numerical mismatches (efficiency percentages and training steps) that materially affect the paper’s correctness and replicability. These issues should be corrected or clarified. If addressed, the core idea and reported gains could be more confidently evaluated; as it stands, the inconsistencies undermine the integrity of the claims.