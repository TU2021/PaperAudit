# Global Summary
DeepCompress targets inefficiencies in Large Reasoning Models (LRMs), specifically “overthinking” simple problems and “underthinking” complex ones. The core approach is a dual length reward combined with a model-aware difficulty mechanism that classifies questions as “Simple” or “Hard” in real time using the group pass ratio relative to the batch pass ratio. For “Simple” questions, shorter chains are encouraged; for “Hard” questions, longer chains are incentivized to expand solution coverage. Training builds on Zero RL with DAPO and a rule-based outcome reward, and adds adaptive length shaping, correctness-conditioned reward gating, and EMA smoothing of batch performance. Evaluation spans seven mathematical benchmarks with fixed decoding and pass@1 computed from 16 samples per question.

Key findings:
- Preliminaries show pass@1 decreases with longer responses, while pass@k (e.g., pass@32) increases, indicating longer chains improve solution coverage within sampled groups.
- DeepCompress sets new SOTA among Zero RL baselines across seven math benchmarks. Average accuracy improves by “+2.0” (3B) and “+2.7” (7B) over DeepMath-Zero. On AIME 2024, DeepCompress-Zero-7B gains “+4.1” absolute points; on AIME 2025, “+6.5”.
- Efficiency: average response length compressed by “57.9%” (3B) and “16.6%” (7B), with notable token reductions (e.g., AIME 24: 3B uses “37.6%” fewer tokens with “+5.2” pass@1 improvement; 7B uses “35.2%” fewer tokens with “+4.1” improvement).
- Emergent behavior analysis (hard set) shows higher reflection frequency with shorter average length and better pass@1 for DeepCompress (e.g., DeepCompress-Zero-7B Reflect “2.64”, Length “5,942”, Pass@1 “13.81”).

Explicit caveats:
- Effectiveness depends on sufficient length variation among sampled responses within RL groups.
- Maximum generation length capped at “10k” tokens during training may limit exploration of very long solutions.

# Abstract
- Problem: LRMs exhibit “overthinking” on simple tasks and “underthinking” on complex tasks; prior SFT/RL length-compression improves efficiency but often reduces accuracy.
- Proposal: DeepCompress framework with an adaptive length reward that classifies problems online as “Simple” or “Hard” and applies dual rewards—shorter reasoning for “Simple”, longer exploratory chains for “Hard”.
- Claim: Dynamically adjusts CoT length; compresses for well-mastered problems and extends for challenging ones.
- Results: On challenging mathematical benchmarks, DeepCompress outperforms baselines, achieving superior accuracy with significant token efficiency gains.
- Quantitative details in abstract: Not specified beyond general superiority; specifics provided in later sections.

# Introduction
- Context: LRMs (e.g., o1 series, DeepSeek R1, Gemini 2.5, Claude 3.7) demonstrate strong complex reasoning but have cognitive inefficiencies: overthinking (excessive intermediate steps for simple problems) and underthinking (frequent, unstable thought shifts for complex problems).
- Prior work: SFT on shortened CoT datasets and RL with token-length rewards improve efficiency but typically with accuracy drops.
- Core challenge: Achieve both superior accuracy and efficiency simultaneously.
- Approach: DeepCompress with adaptive length reward based on real-time model-aware difficulty.
  - Observation: Longer responses cover broader ranges of correct solutions for difficult problems; always optimizing shorter responses may restrict reasoning capacity.
  - Mechanism: Use GRPO as the basic RL paradigm (later, DAPO in Method) and define “Simple” if group pass ratio exceeds batch pass ratio; otherwise “Hard”.
  - Policy: Encourage shorter responses for “Simple” and longer for “Hard”.
- Contributions:
  - Model-aware difficulty classification (“Simple”/“Hard”) and dual length reward aligning shorter length for easy and longer exploration for hard.
  - Experiments show consistent superiority over baselines with significant token efficiency gains.
  - Analysis: DeepCompress encourages high policy entropy for exploration and reflection, improving performance on challenging problems.

# Related Work
- Prompt engineering to manipulate reasoning length:
  - Longer reasoning paths can improve performance (Jin et al., 2024); conciseness for efficiency with CCoT (Nayab et al., 2024).
  - Strategies: Thinking-Optimal Scaling (Yang et al., 2025) filters for shortest correct paths; ADOT (Xu et al., 2024) adapts prompting to difficulty; TALE (Han et al., 2024) tunes reasoning tokens via prompt.
- Post-training for reasoning efficiency:
  - SFT: Curated concise exemplars (Chen et al., 2024; Kang et al., 2025); weighted objectives adapting budget to difficulty (Yu et al., 2025b).
  - RL: Direct length-based rewards (Team et al., 2025; Luo et al., 2025; Arora & Zanette, 2025); dynamic reward shaping (Liu et al., 2025).
  - Architectural innovations: auxiliary reflection models (Deng et al., 2025), iterative pruning (Hou et al., 2025).
- Limitation noted: Efficiency gains often with limited accuracy improvements or minor performance losses.

# Preliminaries
- Aim: Analyze relationship between response length and performance; existing length reduction often degrades performance.
- Data: MATH-500, Olympiad-Bench, Minerva Math, AIME 2025.
- Models: DeepMath-Zero-3B and DeepMath-Zero-7B (Qwen finetuned on DeepMath-103K via Zero RL); these exhibit “aha moment” behaviors and longer responses.
- Metric: Rule-based outcome verifier; pass@k with formula
  - pass@k = 1 − ((n − c choose k) / (n choose k)); max generation length “32,768” tokens; temperature “0.6”, top-p “0.95”.
- Evaluation strategy:
  - For a given problem: sampled “8,192” responses; standardized lengths and sorted; uniformly divided into “16” bins; report average length and pass@1 and pass@32.
- Results:
  - Figure 1: pass@1 decreases with increasing standardized length (z); pass@32 generally increases with length.
  - Exception: DeepMath-Zero-7B on AIME 2025; trend holds with larger k (e.g., “k = 64”).
  - Interpretation: Longer responses increase coverage of potentially correct solutions within sampled groups, aiding RL with relative comparisons; constant length reduction may constrain capacity on complex problems; need adaptive strategy.

# Method
- Overview: DeepCompress enhances Zero RL with (1) Dual Length Reward and (2) Model-Aware Difficulty.
- 4.1 Zero RL:
  - Training set D = {(x_i, y_i)}; model policy π_θ samples G outputs {ŷ_i^1,…, ŷ_i^G} per question; rule-based verifier V.
  - Outcome reward R_o(ŷ, y) = +1 if final answer exactly correct, else −1.
- 4.2 Dual Length Reward:
  - Goal: Correct solutions with minimal tokens; maintain deep exploration for complex problems via distinct reward modes.
  - Compute response length mean μ_i and std σ_i over the G responses; standardized length:
    - z_i = (|ŷ_i| − μ_i) / (σ_i + ε).
  - Length reward via sigmoid:
    - R_z(ŷ, β) = sigmoid(−β z_i) = 1/(1 + e^{β z_i}).
  - Modes via β sign:
    - β > 0 (Simple): reward shorter.
    - β < 0 (Hard): reward longer.
  - Scale with α: R_l = α × R_z(ŷ, β).
- 4.3 Model-Aware Difficulty:
  - Define group pass ratio P_g(x_i) = (number correct among G)/G.
  - Batch pass ratio P_b = (∑_i P_g(x_i))/B.
  - β_i = P_g(x_i) − P_b ∈ (−1, 1); positive => Simple; negative => Hard.
  - Final reward: R = R_o + R_l.
  - Mechanism of β:
    - Sign controls mode; magnitude |β| scales preference intensity.
- 4.4 Enhancing Robustness:
  - Correctness-Conditioned Length Reward:
    - Apply R_l only if R_o = 1; otherwise reward is R_o.
    - R = R_o + R_l if correct; else R = R_o.
  - Smoothed Batch Pass Ratio:
    - EMA smoothing P_{b,t} = λ · P_{b,t−1} + (1 − λ) · P_{b,t}^{true}; λ ∈ [0,1], used in β computation.
    - Initialize P_{b,t} with “1.0” to avoid premature penalization.
- Key hyperparameters and training details (Appendix B):
  - reward_weight α = “0.2”; EMA parameter λ = “0.99”; rollout.n (group size G) = “32”.
  - max_prompt_length “2K”; max_response_length “10K”; train_batch_size “512”; ppo_mini_batch_size “32”; total_training_steps “500”.
  - lr “1e-6”; kl_coef “0.0”; clip_ratio_low “0.20”; clip_ratio_high “0.28”; temperature “1.0”; overlong_buffer.len “2K”.

# Experiments
- 5.1 Experimental Setup:
  - RL training: Follow He et al. (2025) Zero RL; use DAPO (Yu et al., 2025a); trained Qwen2.5-3B and Qwen2.5-7B with rule-based R_o; adjusted Qwen chat template (Hu et al., 2025). Further settings in Appendix B.
  - Evaluation: Seven benchmarks—MATH-500, AMC 2023, OlympiadBench, Minerva Math, AIME 2024–2025, English subset of PolyMath.
    - Primary metrics: pass@1 with “16” samples per question.
    - Validation set: “60” MATH + “60” AIME 2022–2023; choose checkpoint with highest pass@1.
    - Inference: vLLM; decoding parameters temperature “0.6”, top_p “0.95”, max_tokens “32,768”.
    - Baseline re-evaluated under same settings.
- 5.2 Main Results:
  - Performance (Table 1; pass@1, %):
    - DeepCompress-Zero-3B: MATH500 “75.3”, AMC23 “49.4”, OlympiadBench “39.3”, MinervaMath “32.7”, AIME24 “16.7”, AIME25 “7.1”, PolyMath “35.8”, AvgAcc “36.6”.
    - DeepMath-Zero-3B: MATH500 “72.8”, AMC23 “48.0”, OlympiadBench “38.0”, MinervaMath “30.8”, AIME24 “11.5”, AIME25 “6.9”, PolyMath “34.1”, AvgAcc “34.6”.
    - DeepCompress-Zero-7B: MATH500 “85.6”, AMC23 “67.8”, OlympiadBench “53.3”, MinervaMath “47.4”, AIME24 “23.5”, AIME25 “19.6”, PolyMath “44.0”, AvgAcc “48.7”.
    - DeepMath-Zero-7B: MATH500 “85.6”, AMC23 “64.7”, OlympiadBench “51.3”, MinervaMath “45.4”, AIME24 “19.4”, AIME25 “13.1”, PolyMath “42.6”, AvgAcc “46.0”.
    - Additional baselines (AvgAcc): Qwen-2.5-3B “20.5”; Qwen-2.5-3B-Instruct “28.9”; Qwen-2.5-7B “25.0”; Open-Reasoner-Zero-7B “42.5”; Qwen-2.5-7B-SRL-Zoo “38.9”.
  - Reported average accuracy gains vs DeepMath-Zero: “+2.0” (3B) and “+2.7” (7B). Noted gains on hard tasks: “+4.1” (AIME 24, 7B) and “+6.5” (AIME 25, 7B).
  - Efficiency (average response length; tokens):
    - 3B:
      - MATH500: DeepMath-Zero “5,441”; DeepCompress “2,073”.
      - AMC23: “6,097” vs “3,986”.
      - OlympiadBench: “8,496” vs “4,138”.
      - MinervaMath: “6,808” vs “2,370”.
      - AIME24: “10,922” vs “4,130”.
      - AIME25: “10,524” vs “3,382”.
      - PolyMath: “14,128” vs “8,385”.
    - 7B:
      - MATH500: “4,829” vs “2,234”.
      - AMC23: “4,267” vs “2,124”.
      - OlympiadBench: “4,781” vs “2,640”.
      - MinervaMath: “4,481” vs “2,744”.
      - AIME24: “11,091” vs “7,181”.
      - AIME25: “6,397” vs “5,816”.
      - PolyMath: “5,334” vs “4,476”.
    - Average compression: “57.9%” (3B) and “16.6%” (7B).
    - Specific: AIME 24—3B uses “37.6%” fewer tokens with “+5.2” improvement; 7B uses “35.2%” fewer tokens with “+4.1” improvement.
- 5.3 Impact of Length Reward:
  - Ablations: Fixed β variants—Length Penalty (β = “1”), Length Bonus (β = “−1”)—compared to DeepCompress and DeepMath-Zero-7B.
  - Observations:
    - Policy entropy: Length bonus higher entropy; length penalty low and stable entropy (Figure 4a).
    - Response length: Length bonus produces longer outputs; length penalty shorter (Figure 4d).
    - Performance: Length bonus yields higher pass@1 than penalty; DeepCompress shows adaptive entropy—initial rise then stabilization—with corresponding length dynamics (early increase then reduction) and continuous test performance growth.
- 5.4 Quantifying Emergence of Reasoning Behaviors:
  - Hard set construction: Problems unsolved by baseline Qwen2.5-3B/7B; behaviors tracked per Gandhi et al. (2025) following Zeng et al. (2025).
  - Table 2 (Reflection Frequency on hard questions; “Reflect”, “Length”, “Pass@1”):
    - DeepMath-Zero-3B: “2.45”, “11,222”, “7.21”.
    - DeepCompress-Zero-3B: “2.73”, “4,853”, “8.72”.
    - DeepMath-Zero-7B: “2.59”, “7,180”, “11.35”.
    - DeepCompress-Zero-7B: “2.64”, “5,942”, “13.81”.
    - w/ Length Penalty: “2.20”, “2,520”, “9.94”.
    - w/ Length Bonus: “2.87”, “13,575”, “11.89”.
  - Claim: DeepCompress reflects more often with shorter length and higher pass@1, indicating more efficient reflection and “aha moment” behaviors.

# Conclusion
- Summary: DeepCompress integrates model-aware difficulty and dual length reward to allocate reasoning effort—concise for simple, deeper for hard—achieving SOTA accuracy with significant token efficiency across challenging math benchmarks.
- Training dynamics: High policy entropy fosters exploration and effective reflection; performance improves continuously while response length becomes more efficient over time.
- Limitations:
  - Requires sufficient length variation among sampled responses within RL groups.
  - Max generation length capped at “10k” tokens during training, potentially restricting exploration of longer-form solutions.

# Appendix
- A Reasoning Behavior Prompt:
  - Describes a GPT-4o-based prompt to identify beneficial reasoning behaviors (Backtracking, Verification, Subgoal Setting, Enumeration, and others) from a chain-of-reasoning; specifies output format and examples. Used to extract “aha moment” behaviors (Table 2).
- B Training Details (verl framework):
  - Configurations:
    - DeepCompress-Zero-3B and -7B share: lr “1e-6”; kl_coef “0.0”; max_prompt_length “2K”; max_response_length “10K”; train_batch_size “512”; ppo_mini_batch_size “32”; clip_ratio_low “0.20”; clip_ratio_high “0.28”; temperature “1.0”; rollout.n “32”; overlong_buffer.len “2K”; total_training_steps “500”; reward_weight α “0.2”; EMA_parameter λ “0.99”.

# References
- Citations cover prior work on reasoning efficiency via SFT/RL, prompt-based length control, dynamic reward shaping, architectural enhancements, datasets/benchmarks (MATH, OlympiadBench, Minerva Math, AIME, PolyMath), and systems/libraries (vLLM, verl). Numerical details beyond citation metadata are not specified in this section.