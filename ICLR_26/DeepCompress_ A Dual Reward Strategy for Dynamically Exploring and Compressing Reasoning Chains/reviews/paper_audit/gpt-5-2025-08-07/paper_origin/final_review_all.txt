Summary
- The paper addresses the inefficiency–accuracy trade-off in Large Reasoning Models (LRMs), where shorter chains often improve test-time pass@1 while longer chains can improve group pass@k. It proposes DeepCompress, augmenting Zero RL/DAPO with a dual length reward and a model-aware difficulty mechanism. The length reward uses standardized response length (Equation 3) and a sigmoid transformation controlled by β (Equations 4–5), with β derived from the difference between per-question group pass ratio and batch pass ratio (Equations 6–8), and combined with outcome reward (Equation 9). Robustness additions include correctness-conditioned length reward (Equation 10) and an EMA-smoothed batch pass ratio (Equation 11). On seven math benchmarks, DeepCompress improves accuracy (Table 1) while reducing average response length (Figure 3). Ablations and training dynamics suggest increased policy entropy and balanced exploration (Figure 4). An analysis of reflection behaviors on hard problems shows higher reflection frequency with shorter outputs and better pass@1 (Table 2).Strengths
- Bold, evidence-backed motivation for adaptive length
  • The preliminary analysis shows pass@1 decreases with standardized length, while pass@32 generally increases (Figure 1; Section 3.2), clarifying the need for optimization that is sensitive to problem difficulty (technical soundness; impact on RL design).
  • The evaluation protocol in Section 3.1 describes standardized-length binning over 8,192 samples per problem and pass@k computation (Equation 1), grounding claims with a reproducible process (experimental rigor; clarity).
  • The paper explicitly connects pass@k trends to GRPO/DAPO-style group-sampling optimization (Section 3.2), arguing why blanket length-reduction may hinder exploration on hard questions (novelty of insight; impact).
- Clear, formalized dual-reward mechanism
  • Response length standardization z and its use in the reward are precisely defined (Equations 3–5), including a sigmoid transformation controlled by β and scaled by α (technical clarity; replicability).
  • The model-aware difficulty mechanism sets β from P_g − P_b (Equations 6–8), making the length shaping conditional on the model’s real-time capability (novelty; practicality for evolving policies).
  • Visualizations in Figure 2 (Simple vs. Hard) show how the combined reward R = R_o + R_l changes with z and β, aiding understanding of how the reward behaves for correct/incorrect responses (clarity; technical transparency).
- Robustness-oriented design improvements
  • Correctness-conditioned length reward (Equation 10) mitigates reward hacking by applying length shaping only when the answer is correct (technical soundness; alignment with the primary objective).
  • EMA-smoothed batch pass ratio (Equation 11) stabilizes β estimation and avoids early over-penalization (training stability; design robustness).
  • The “Mechanism of β” description (Section 4.3) clarifies mode control and intensity scaling, connecting qualitative behavior to quantitative settings (clarity; replicability).
- Comprehensive empirical evaluation with consistent gains
  • Accuracy improvements across seven benchmarks for both 3B and 7B models (Table 1), with notable gains on AIME 24/25 (+4.1/+6.5 absolute for 7B), demonstrating effectiveness on difficult problems (experimental rigor; impact).
  • Significant token savings: average compression of 57.9% (3B) and reported 16.6% (7B), with per-benchmark reductions provided (Figure 3; Section 5.2), showing efficiency improvements without sacrificing accuracy (impact; practical relevance).
  • Uniform evaluation settings and re-evaluation of baselines with fixed decoding parameters (Section 5.1), including vLLM, temperature=0.6, top_p=0.95, and max_tokens=32,768, improve comparability (experimental rigor; fairness).
- Insightful training-dynamics and ablation analyses
  • Figures 4a–4d compare policy entropy, training/test response lengths, and pass@1 across variants (length penalty, length bonus, DeepCompress, DeepMath-Zero-7B), providing diagnostic evidence that DeepCompress balances exploration and efficiency (technical soundness; clarity).
  • The ablation setup in Section 5.3 fixes β at ±1 to isolate the role of the length reward, supporting causal interpretation of observed differences (experimental rigor; clarity).
  • Reflection behavior analysis on hard sets (Table 2; Section 5.4) shows higher reflection frequency with shorter average length and higher pass@1, suggesting more efficient reflective reasoning (impact; useful behavioral metrics).
- Reproducibility aids and transparency
  • Training hyperparameters and settings are documented (Appendix B; Table 3), including α=0.2, λ=0.99, rollout.n=32, max_response_length=10k (clarity; replicability).
  • The rule-based outcome reward is explicitly defined (Equation 2) and tied to pass@k verifier practices (Section 3.1; Section 4.1) (technical transparency).
  • A public GitHub link is provided in the abstract for code/resources (Abstract), facilitating future adoption and verification (impact; openness).Weaknesses
- Limited evaluation scope and statistical reporting
  • Main results emphasize pass@1 at test-time with 16 samples (Section 5.1), whereas the central motivation hinges on pass@k behavior and group exploration (Section 3.2; Figure 1), creating a partial mismatch between training rationale and evaluation focus (experimental rigor; alignment).
  • Claims of “SOTA” are restricted to Zero RL baselines (Table 1), without broader comparisons to non-Zero RL or larger contemporary LRMs under similar settings (clarity of positioning; impact). No direct evidence found in the manuscript for broader SOTA comparisons beyond the listed models.
  • No statistical significance testing or confidence intervals are reported for improvements (Table 1; Section 5.2), making robustness of gains harder to assess (experimental rigor). No direct evidence found in the manuscript.
- Insufficient sensitivity analyses for key hyperparameters and components
  • No ablation over α (Equation 5) or λ (Equation 11) to quantify sensitivity of length reward scaling and EMA smoothing (Appendix B lists fixed values; Table 3), limiting understanding of robustness (technical soundness).
  • Group size G=32 and training temperature settings are fixed (Appendix B; Table 3), but effects of varying G or sampling parameters on β distribution and training dynamics are not studied (technical completeness).
  • The distribution and dynamics of β (P_g − P_b) during training are not reported (Section 4.3; Equations 6–8), leaving unclear how often the system switches between Simple/Hard modes (clarity; diagnostic insight). No direct evidence found in the manuscript.
- Verifier dependence and potential sparse reward early in training
  • The outcome reward depends on a rule-based verifier (Equation 2; Section 4.1), but its accuracy, failure modes, and extraction details are not documented, risking label noise (technical soundness; reproducibility). No direct evidence found in the manuscript beyond the binary rule.
  • Correctness-conditioned length reward (Equation 10) applies shaping only to correct samples; when early accuracy is low, reward shaping might become sparse, possibly slowing learning (technical soundness; training efficiency). No direct evidence found in the manuscript quantifying this effect.
  • Residual risk of reward hacking or pathological length modulation remains unquantified even after conditioning (Section 4.4 acknowledges reward hacking risk), with no audits of incorrect responses’ length behavior (experimental rigor). No direct evidence found in the manuscript.
- Interpretation and generality of the preliminary pass@k–length analysis
  • Figure 1 establishes correlation between longer responses and higher pass@32, but causal mechanisms are not empirically verified (Section 3.2), and confounders (e.g., diversity induced by longer generations) are not analyzed (novelty claims scope).
  • For AIME 2025’s exception, the text asserts the trend holds with k=64 (Section 3.2) without showing corresponding plots or data (clarity). No direct evidence found in the manuscript for k=64 results.
  • The standardized-length evaluation uses 8,192 samples per problem and 16 bins (Section 3.1), which may reflect high compute but unclear realism; the method for standardization across problems is not fully detailed beyond Equation 3 and binning (clarity). No direct evidence found in the manuscript about per-problem variability handling beyond z and uniform binning.
- Generalization beyond mathematics and behavior annotation validity
  • Experiments are limited to math benchmarks (Section 5.2; Table 1), leaving generality to other reasoning domains untested (impact). No direct evidence found in the manuscript.
  • Reflection/“aha moment” behaviors are extracted using GPT-4o (Table 2; Appendix A), with no inter-annotator agreement or validation of extraction reliability (experimental rigor).
  • The claim that DeepCompress leads to “smarter thinking” (Section 5.4 narrative) is supported by proxy indicators (reflection count, length, pass@1) but not triangulated with additional behavioral measures or human evaluation (clarity; impact). No direct evidence found in the manuscript beyond Table 2 metrics.
- Reproducibility and reporting gaps
  • Compute budget, hardware, and training time are not reported (Appendix B provides steps but not resource specifics), limiting reproducibility and cost assessment (clarity; practicality). No direct evidence found in the manuscript.
  • While a GitHub link is provided (Abstract), the paper does not specify whether trained checkpoints and exact training/evaluation scripts are released (reproducibility). No direct evidence found in the manuscript.
  • Data contamination safeguards for evaluation sets are not discussed (Section 5.2 lists datasets; Reference He et al., 2025 discusses decontamination for training data), leaving evaluation integrity unverified (experimental rigor). No direct evidence found in the manuscript.
- Internal inconsistencies and numerical mismatches affecting reproducibility
  • RL algorithm inconsistency: Introduction names GRPO as the basic RL algorithm (Section 1), while Method and Experiments specify DAPO (Section 4.1; Section 5.1), creating ambiguity about the actual training recipe (clarity; reproducibility).
  • β semantics conflict: β is introduced as a hyperparameter controlling sigmoid steepness (Section 4.2; Equation 4), but later defined as a dynamic per-question difficulty term β_i = P_g − P_b ∈ (−1,1) (Section 4.3; Equation 8), leaving unclear which formulation was used in the main experiments (technical clarity; replicability).
  • Reward variant ambiguity: Equation 9 defines R = R_o + R_l (unconditional), while Equation 10 gates R_l by correctness; the paper does not explicitly state which variant underlies Table 1 and Figure 3 (Section 4.4; Section 5.2). No direct evidence found in the manuscript clarifying this (technical transparency; interpretability).
  • Efficiency percentage mismatches: The reported 7B average compression “16.6%” (Section 5.2) appears inconsistent with the per-benchmark reductions listed (e.g., MATH500 4,829→2,234 ≈53.7%; AMC23 4,267→2,124 ≈50.2%; OlympiadBench 4,781→2,640 ≈44.8%; MinervaMath 4,481→2,744 ≈38.7%; AIME24 11,091→7,181 ≈35.2%; AIME25 6,397→5,816 ≈9.1%; PolyMath 5,334→4,476 ≈16.1%) (Section 5.2; Figure 3), without a stated weighting scheme (technical accuracy; reporting).
  • Conflicting AIME24 (3B) token reduction: The text claims “37.6% less tokens” (Section 5.2), whereas listed lengths 10,922→4,130 indicate ≈62.2% reduction (Section 5.2), undermining trust in efficiency reporting (technical accuracy; reproducibility).
  • Training steps mismatch: Appendix B reports total_training_steps=500 (Table 3), while training curves in Figure 4 extend to 600 steps (Figures 4a–4d; Section 5.3), making the actual training duration unclear (clarity; reproducibility).Suggestions for Improvement
- Broaden evaluation scope and add statistical reporting
  • Report pass@k alongside pass@1 in main results (Table 1; Section 5.2) to better align evaluation with the group-sampling rationale established in Figure 1 and Section 3.2; include curves or tables at k values relevant to training (e.g., 32).
  • Situate SOTA claims by adding comparisons to non-Zero RL baselines or stronger open models under identical evaluation settings (Section 5.1) and clarify the scope of “SOTA” (e.g., “among Zero RL baselines”).
  • Provide statistical significance testing (e.g., bootstrap CIs) or variance across multiple seeds for Table 1 gains (Section 5.2) to substantiate robustness.
- Conduct sensitivity analyses of key hyperparameters and components
  • Ablate α (Equation 5) and λ (Equation 11) to quantify sensitivity of length reward magnitude and EMA smoothing, including their effects on policy entropy and accuracy (Figure 4).
  • Examine the impact of varying G (Appendix B; Table 3) and sampling temperature/top-p on β distributions and training stability; add plots of β histograms over steps (Section 4.3; Equations 6–8).
  • Track and visualize β dynamics per batch (frequency of Simple vs. Hard classification, average |β|) to validate that the mechanism toggles modes meaningfully during training (Section 4.3).
- Strengthen verifier documentation and early-training reward density
  • Document the rule-based verifier details: extraction heuristics, edge cases, and estimated precision/recall on a labeled subset (Equation 2; Section 4.1), and include a small audit.
  • Quantify the effect of correctness-conditioned length reward (Equation 10) on reward density early in training (e.g., fraction of samples receiving R_l over steps) and, if sparse, consider a warmup phase where R_l applies to near-correct solutions verified by relaxed checks.
  • Add analysis of incorrect-response length behavior under DeepCompress vs. baselines to ensure no residual reward hacking (Section 4.4), e.g., distributions of z for incorrect samples.
- Deepen the preliminary analysis and clarify generality
  • Provide causal probes or controlled tests (e.g., matched-length interventions or diversity controls) to test whether longer responses directly cause higher pass@k (Figure 1; Section 3.2).
  • Include the asserted AIME 2025 k=64 plots and numbers (Section 3.2) to fully substantiate the exception handling.
  • Elaborate on standardized-length binning methodology (Section 3.1): per-problem z distribution, bin boundaries, and cross-problem aggregation details; discuss the computational feasibility and representativeness of 8,192 samples per problem.
- Expand beyond math and validate behavior extraction
  • Evaluate DeepCompress on at least one non-math reasoning benchmark (Section 5.2) to test generality of the dual reward approach.
  • Validate GPT-4o-based behavior extraction (Table 2; Appendix A) with a human-rated subset and report inter-rater agreement or consistency metrics.
  • Introduce additional behavioral measures (e.g., verification frequency, subgoal structuring; Appendix A behaviors) and link them to performance trends to support the “smarter thinking” claim.
- Improve reproducibility and reporting transparency
  • Report compute/hardware details (GPU type/count, training hours), and cost estimates for training/evaluation (Appendix B).
  • Specify which artifacts are released (code, training logs, evaluation scripts, trained checkpoints) and provide instructions matching Appendix B settings (Abstract; Appendix B; Table 3).
  • Discuss data contamination checks for evaluation datasets and any filtering employed, referencing decontamination practices (Section 5.2; References He et al., 2025), to strengthen evaluation integrity.
- Resolve internal inconsistencies and correct numerical claims
  • Unify and explicitly state the RL algorithm used (GRPO vs DAPO) across Introduction, Method, and Experiments (Section 1; Section 4.1; Section 5.1), and update text to reflect the actual training framework.
  • Clarify β’s role: specify whether β is a fixed steepness hyperparameter (Section 4.2) or a dynamic per-question difficulty signal β_i = P_g − P_b (Section 4.3), and revise Equations/figures accordingly; if both are used, describe their interaction and ranges.
  • State which reward variant (Equation 9 vs Equation 10) is used for the main results (Table 1; Figure 3) and, if gated, indicate this in captions and Appendix B.
  • Recompute and report the 7B average compression with the exact weighting scheme (Section 5.2; Figure 3), and correct the AIME24 (3B) percentage to match the listed lengths (Section 5.2).
  • Align training steps in figures with Appendix B (Table 3) or explain any additional steps; annotate Figure 4 axes or text to clarify the discrepancy (Section 5.3; Appendix B).Score
- Overall (10): 7 — Consistent accuracy and efficiency gains across seven math benchmarks (Table 1; Figure 3) with a clear, formalized method (Equations 3–11; Figure 2), though several reporting inconsistencies warrant clarification (Sections 1, 4.1, 5.2; Figures 4a–4d).
- Novelty (10): 7 — Dual length reward conditioned on model-aware difficulty via β=P_g−P_b (Equations 6–8; Figure 2) and robustness additions (Equations 10–11) present a distinctive approach over length-only shaping, supported by motivation in Figure 1.
- Technical Quality (10): 5 — Method is well-defined (Equations 3–11) and empirically validated (Table 1; Figure 4), but inconsistencies (GRPO vs DAPO; β semantics; reward variant) and numerical mismatches (Section 5.2; Appendix B vs Figure 4) reduce confidence in reproducibility.
- Clarity (10): 7 — Clear mathematical formulation (Equations 2–11), helpful visualizations (Figures 1–4), and training configs (Appendix B; Table 3), with notable ambiguities about algorithm choice, reward gating, and efficiency percentages (Sections 1, 4.4, 5.2).
- Confidence (5): 4 — High confidence based on detailed method specification and extensive experiments (Table 1; Figure 3; Figure 4; Appendix B), tempered by missing statistical tests, sensitivity analyses, and several internal inconsistencies (Sections 5.2–5.3; Sections 1, 4.1).