{
  "baseline_review": "1) Summary\nThis paper introduces DeepCompress, a framework designed to improve both the accuracy and efficiency of Large Reasoning Models (LRMs). The core problem addressed is the tendency of LRMs to \"overthink\" simple problems and \"underthink\" complex ones. The proposed method uses reinforcement learning with a novel dual-reward strategy. It dynamically classifies problems as \"Simple\" or \"Hard\" based on the model's real-time performance (group pass ratio vs. an exponential moving average of the batch pass ratio). For \"Simple\" problems, DeepCompress rewards shorter reasoning chains to improve efficiency. For \"Hard\" problems, it rewards longer, more exploratory chains to improve accuracy. Experiments on several mathematical reasoning benchmarks show that DeepCompress outperforms strong baselines in both pass@1 accuracy and token efficiency, achieving new state-of-the-art results.2) Strengths\n*   **Novel and Intuitive Method**: The paper proposes a novel dual-reward mechanism that challenges the prevailing assumption that shorter reasoning paths are always better for efficiency. The core idea of dynamically adapting the reward based on a model-aware difficulty metric is both innovative and well-motivated.\n    *   The preliminary analysis in Section 3 (Figure 1) provides a strong motivation, showing that while pass@1 scores favor shorter responses, pass@32 scores (relevant for RL with group sampling) often favor longer responses, suggesting longer chains contain a wider variety of correct solutions.\n    *   The model-aware difficulty mechanism (Section 4.3, Equations 6-8) is a clever way to assess problem difficulty without relying on static, pre-annotated labels, allowing the system to adapt to the model's evolving capabilities during training.\n    *   The dual-reward formulation itself, using the sign of a hyperparameter `β` to switch between length penalty and length bonus modes (Section 4.2, Figure 2), is an elegant implementation of the core concept.*   **Strong Empirical Performance**: The proposed method achieves state-of-the-art results on a wide range of challenging mathematical reasoning benchmarks, demonstrating significant improvements over strong baselines in both accuracy and efficiency.\n    *   Table 1 shows that DeepCompress-Zero models consistently outperform all baselines, including the previous state-of-the-art DeepMath-Zero models, across seven different benchmarks at both 3B and 7B scales.\n    *   The performance gains are particularly notable on the most difficult datasets, such as AIME 24 and AIME 25, where DeepCompress-Zero-7B achieves absolute improvements of +4.1 and +6.5 points, respectively (Table 1). This supports the claim that encouraging exploration on hard problems is effective.\n    *   Simultaneously, the method achieves significant token efficiency gains. Figure 3 shows that DeepCompress models produce substantially shorter responses on average compared to the DeepMath-Zero baseline, with reductions of up to 57.9% for the 3B model.*   **Thorough Experimental Analysis and Ablations**: The paper provides a comprehensive set of analyses that go beyond simple performance tables, offering valuable insights into the behavior and effectiveness of the proposed method.\n    *   The ablation study in Section 5.3 (Figure 4) effectively contrasts DeepCompress with fixed-strategy baselines (Length Penalty and Length Bonus). It clearly illustrates how DeepCompress adaptively balances exploration (higher initial policy entropy) and exploitation (stabilizing entropy and reducing length over time) to achieve superior final performance.\n    *   The analysis of training dynamics (Figure 4a, 4b) is particularly insightful, showing how policy entropy and response length evolve differently for each strategy, corroborating the method's intended behavior.\n    *   The investigation into emergent reasoning behaviors (Section 5.4, Table 2) provides evidence that DeepCompress encourages more frequent and efficient \"reflection,\" linking the method's higher policy entropy to tangible improvements in problem-solving on hard questions.3) Weaknesses\n*   **Ambiguity in Key Evaluation Metrics**: The paper's claims about efficiency gains are based on \"Response Length,\" but this metric is not precisely defined. This ambiguity makes it difficult to fully interpret the reported efficiency improvements.\n    *   In Figure 3 and the text below Table 1, \"Response Length\" is reported as a single number for each model-dataset pair. It is unclear if this is the average length of all 16 sampled responses, the average length of only correct responses, or the length of the single response used for pass@1 evaluation.\n    *   This lack of clarity is critical because different definitions have different implications. For example, if the model generates one very short correct answer and 15 very long incorrect ones, the average length might be high, but the inference cost for the pass@1 solution is low. The current presentation does not distinguish between these scenarios.\n    *   No direct evidence found in the manuscript defining how \"Response Length\" is calculated for the evaluation reported in Figure 3 and Table 1.*   **Lack of Sensitivity Analysis for Key Hyperparameters**: The proposed method introduces several new hyperparameters, but the paper does not provide an analysis of how sensitive the model's performance is to their specific values.\n    *   The reward weight `α` (Equation 5) and the EMA parameter `λ` (Equation 11) are critical to balancing the length reward and stabilizing the difficulty metric, respectively. Their values are fixed at 0.2 and 0.99 (Table 3) without justification or ablation.\n    *   The group size `G` (referred to as `rollout.n` in Table 3, set to 32) directly influences the calculation of the group pass ratio `P_g` (Equation 6), which is central to the difficulty classification. The impact of this choice is not explored.\n    *   The initialization of the smoothed batch pass ratio `P_{b,t}` to 1.0 (Section 4.4) is described as an important choice to prevent premature penalization, but the effect of this specific initialization value is not empirically validated against other potential choices (e.g., 0.5).*   **Unclear Application of Robustness Enhancements**: The paper introduces two enhancements for robustness in Section 4.4 (\"Correctness-Conditioned Length Reward\" and \"Smoothed Batch Pass Ratio\"), but it is not explicitly stated whether these were used to generate the main results.\n    *   Section 4.4 presents these as improvements upon the core DeepCompress framework. However, Section 5.2 (\"Main Results\") refers simply to \"DeepCompress\" without specifying which version of the algorithm was used to produce the results in Table 1 and Figure 3.\n    *   This is problematic because the \"Correctness-Conditioned Length Reward\" (Equation 10) is a significant modification designed to prevent reward hacking. If this was used for the main results, the ablation study in Section 5.3 (which appears to use the basic, unconditional reward from Equation 9) may not be a fair comparison.\n    *   The paper should clarify if the main results were generated using the basic DeepCompress (Section 4.1-4.3) or the enhanced version (Section 4.4), as this affects the interpretation of both the main claims and the ablation studies.4) Suggestions for Improvement\n*   **Clarify Evaluation Metrics**: To improve the clarity and reproducibility of the efficiency claims, the definition of \"Response Length\" used in the evaluation should be explicitly stated.\n    *   In Section 5.1 (\"Evaluation\") or in the captions for Table 1 and Figure 3, please specify precisely how the reported \"Response Length\" is calculated (e.g., \"average token count of the first generated response for each problem,\" or \"average token count across all 16 sampled responses per problem\").\n    *   If multiple length metrics were tracked (e.g., length of pass@1 response vs. average length of all samples), reporting both would provide a more nuanced and complete picture of the model's efficiency.\n    *   This clarification would strengthen the paper's claims by allowing readers to understand the exact nature of the efficiency gains.*   **Provide Hyperparameter Sensitivity Analysis**: To demonstrate the robustness of the method, please include an analysis of the key hyperparameters introduced.\n    *   An ablation study, perhaps in the Appendix, showing how performance (both accuracy and length) changes with different values of the reward weight `α` and the EMA parameter `λ` would be highly valuable.\n    *   It would also be beneficial to discuss the rationale behind the choice of the group size `G=32` and potentially show results for at least one other value to understand its impact on the difficulty estimation.\n    *   A brief experiment or discussion on the initialization of `P_{b,t}` would further strengthen the claims made in Section 4.4 about training stability.*   **Be Explicit About Experimental Configurations**: To avoid ambiguity and ensure fair comparisons, please clarify which version of the DeepCompress algorithm was used for each experiment.\n    *   In Section 5.2, explicitly state whether the results in Table 1 and Figure 3 were generated using the basic DeepCompress framework or the enhanced version described in Section 4.4.\n    *   Ensure that the model configurations are consistent across all experiments or that any differences are clearly stated and justified. For instance, if the main results use the correctness-conditioned reward, the ablation study in Section 5.3 should either use it as well (and ablate other components) or explicitly state that it is comparing the enhanced model to a more basic one.\n    *   This clarification will improve the internal consistency of the paper and allow for a more accurate interpretation of the experimental results.5) Score\n*   Overall (10): 8 — The paper presents a novel, well-motivated method with strong empirical results (Table 1, Figure 3) and insightful analysis (Figure 4).\n*   Novelty (10): 9 — The core idea of a model-aware, dual-reward strategy that adaptively encourages shorter or longer reasoning is highly original (Section 4.2, 4.3).\n*   Technical Quality (10): 8 — The methodology is sound and the experiments are extensive, though the lack of hyperparameter sensitivity analysis is a minor weakness (Section 4.4, Table 3).\n*   Clarity (10): 8 — The paper is very well-written and easy to follow, but key metric definitions are ambiguous (Figure 3).\n*   Confidence (5): 5 — I am highly confident in my assessment, as I have a strong background in reinforcement learning and large language models.",
  "final_review": "1) Summary\nThis paper introduces DeepCompress, a framework designed to improve both the accuracy and efficiency of Large Reasoning Models (LRMs). The core problem addressed is the tendency of LRMs to \"overthink\" simple problems and \"underthink\" complex ones. The proposed method uses reinforcement learning with a novel dual-reward strategy. It dynamically classifies problems as \"Simple\" or \"Hard\" based on the model's real-time performance (group pass ratio vs. an exponential moving average of the batch pass ratio). For \"Simple\" problems, DeepCompress rewards shorter reasoning chains to improve efficiency. For \"Hard\" problems, it rewards longer, more exploratory chains to improve accuracy. Experiments on several mathematical reasoning benchmarks show that DeepCompress outperforms strong baselines in both pass@1 accuracy and token efficiency, achieving new state-of-the-art results.2) Strengths\n*   **Novel and Intuitive Method**: The paper proposes a novel dual-reward mechanism that challenges the prevailing assumption that shorter reasoning paths are always better for efficiency. The core idea of dynamically adapting the reward based on a model-aware difficulty metric is both innovative and well-motivated.\n    *   The preliminary analysis in Section 3 (Figure 1) provides a strong motivation, showing that while pass@1 scores favor shorter responses, pass@32 scores (relevant for RL with group sampling) often favor longer responses, suggesting longer chains contain a wider variety of correct solutions.\n    *   The model-aware difficulty mechanism (Section 4.3, Equations 6-8) is a clever way to assess problem difficulty without relying on static, pre-annotated labels, allowing the system to adapt to the model's evolving capabilities during training.\n    *   The dual-reward formulation itself, using the sign of a hyperparameter `β` to switch between length penalty and length bonus modes (Section 4.2, Figure 2), is an elegant implementation of the core concept.*   **Strong Empirical Performance**: The proposed method achieves state-of-the-art results on a wide range of challenging mathematical reasoning benchmarks, demonstrating significant improvements over strong baselines in both accuracy and efficiency.\n    *   Table 1 shows that DeepCompress-Zero models consistently outperform all baselines, including the previous state-of-the-art DeepMath-Zero models, across seven different benchmarks at both 3B and 7B scales.\n    *   The performance gains are particularly notable on the most difficult datasets, such as AIME 24 and AIME 25, where DeepCompress-Zero-7B achieves absolute improvements of +4.1 and +6.5 points, respectively (Table 1). This supports the claim that encouraging exploration on hard problems is effective.\n    *   Simultaneously, the method achieves significant token efficiency gains. Figure 3 and the data below Table 1 show that DeepCompress models produce substantially shorter responses on average compared to the DeepMath-Zero baseline.*   **Thorough Experimental Analysis and Ablations**: The paper provides a comprehensive set of analyses that go beyond simple performance tables, offering valuable insights into the behavior and effectiveness of the proposed method.\n    *   The ablation study in Section 5.3 (Figure 4) effectively contrasts DeepCompress with fixed-strategy baselines (Length Penalty and Length Bonus). It clearly illustrates how DeepCompress adaptively balances exploration (higher initial policy entropy) and exploitation (stabilizing entropy and reducing length over time) to achieve superior final performance.\n    *   The analysis of training dynamics (Figure 4a, 4b) is particularly insightful, showing how policy entropy and response length evolve differently for each strategy, corroborating the method's intended behavior.\n    *   The investigation into emergent reasoning behaviors (Section 5.4, Table 2) provides evidence that DeepCompress encourages more frequent and efficient \"reflection,\" linking the method's higher policy entropy to tangible improvements in problem-solving on hard questions.3) Weaknesses\n*   **Inconsistencies in Reported Results and Methodology**: The manuscript contains numerical inconsistencies that undermine confidence in the technical reporting.\n    *   The average efficiency gain for the 7B model is misreported. Section 5.2 claims a 16.6% reduction in response length, but a manual calculation based on the seven data points provided in the text below Table 1 and visualized in Figure 3 yields an average reduction of approximately 35.4%. This is a significant reporting error.\n    *   There is a direct contradiction regarding the reward weight hyperparameter `α`. Appendix B (Table 3) states `α=0.2`, but the visualization of the reward function in Figure 2 shows a reward range that implies `α` is approximately 0.1 (e.g., the reward for correct responses ranges from 1.0 to 1.1, not 1.0 to 1.2). This discrepancy raises questions about the actual implementation.*   **Ambiguity and Errors in Metrics and Figures**: The paper's claims about efficiency are based on \"Response Length,\" but this metric is not precisely defined. This, combined with other clarity issues, makes it difficult to fully interpret the results.\n    *   In Figure 3 and the text below Table 1, \"Response Length\" is reported as a single number. It is unclear if this is the average length of all sampled responses, only correct responses, or the single response used for pass@1 evaluation. This is critical, as the inference cost for a pass@1 solution could be low even if other sampled responses are long.\n    *   The caption for Figure 2 is incorrect. It states that \"Blue indicates correct responses and Red indicates incorrect responses,\" but the figure legend and visual structure show that correctness is separated by vertical position (top vs. bottom clusters), while color encodes the value of the hyperparameter `β`.*   **Claims of Superiority Complicated by Ablation Results**: The paper's narrative of achieving superior accuracy is challenged by its own ablation study.\n    *   The ablation results in Figure 4c show that the \"w/ Length Bonus\" variant consistently achieves a higher pass@1 score than the proposed DeepCompress method in the later stages of training.\n    *   The paper acknowledges this finding in the text (Section 5.3), but the main claims in the introduction and conclusion emphasize achieving \"superior accuracy\" without this important nuance. This suggests that for maximizing accuracy alone, a simpler strategy might be more effective, and the primary benefit of DeepCompress is in finding a better balance between accuracy and efficiency.*   **Lack of Sensitivity Analysis for Key Hyperparameters**: The proposed method introduces several new hyperparameters, but the paper does not provide an analysis of how sensitive the model's performance is to their specific values.\n    *   The reward weight `α` (Equation 5) and the EMA parameter `λ` (Equation 11) are critical to balancing the length reward and stabilizing the difficulty metric, respectively. Their values are fixed at 0.2 and 0.99 (Table 3) without justification or ablation.\n    *   The group size `G` (referred to as `rollout.n` in Table 3, set to 32) directly influences the calculation of the group pass ratio `P_g` (Equation 6), which is central to the difficulty classification. The impact of this choice is not explored.\n    *   The initialization of the smoothed batch pass ratio `P_{b,t}` to 1.0 (Section 4.4) is described as an important choice, but the effect of this specific value is not empirically validated.*   **Unclear Application of Robustness Enhancements**: The paper introduces two enhancements for robustness in Section 4.4 (\"Correctness-Conditioned Length Reward\" and \"Smoothed Batch Pass Ratio\"), but it is not explicitly stated whether these were used to generate the main results.\n    *   Section 4.4 presents these as improvements, but Section 5.2 (\"Main Results\") refers simply to \"DeepCompress\" without specifying which version of the algorithm was used for Table 1 and Figure 3.\n    *   This is problematic because the \"Correctness-Conditioned Length Reward\" (Equation 10) is a significant modification. If this was used for the main results, the ablation study in Section 5.3 (which appears to use the basic reward from Equation 9) may not be a fair comparison.4) Suggestions for Improvement\n*   **Correct and Verify Numerical Claims and Figures**: To improve the technical quality and trustworthiness of the paper, please perform a thorough check of all reported numbers and figures.\n    *   Re-calculate and report the correct average efficiency gain for the 7B model in Section 5.2. If the 16.6% value is correct, please provide the data and calculation method that leads to it.\n    *   Ensure that the hyperparameters reported in the appendix (Table 3) match the values used to generate all figures and results. Specifically, resolve the discrepancy for `α` between Table 3 and Figure 2.*   **Clarify Evaluation Metrics and Figures**: To improve clarity and reproducibility, please provide precise definitions for metrics and correct figure descriptions.\n    *   In Section 5.1 or in the captions for Table 1 and Figure 3, please specify exactly how \"Response Length\" is calculated (e.g., \"average token count of the first generated response,\" or \"average token count across all 16 sampled responses\").\n    *   Correct the caption for Figure 2 to accurately describe that color represents the value of `β`, while the vertical grouping of curves represents response correctness.*   **Refine and Nuance Performance Claims**: To present a more accurate picture of the method's contribution, please nuance the claims about performance superiority.\n    *   In the main results and discussion, explicitly acknowledge that the \"Length Bonus\" ablation achieves higher accuracy (as shown in Figure 4c).\n    *   Frame the primary contribution of DeepCompress more precisely as achieving a superior *balance* between accuracy and efficiency compared to baselines and fixed-strategy ablations, rather than claiming unqualified superiority on both axes.*   **Provide Hyperparameter Sensitivity Analysis**: To demonstrate the robustness of the method, please include an analysis of the key hyperparameters introduced.\n    *   An ablation study, perhaps in the Appendix, showing how performance changes with different values of `α` and `λ` would be highly valuable.\n    *   It would also be beneficial to discuss the rationale behind the choice of `G=32` and potentially show results for at least one other value to understand its impact.\n    *   A brief experiment or discussion on the initialization of `P_{b,t}` would further strengthen the claims made in Section 4.4.*   **Be Explicit About Experimental Configurations**: To avoid ambiguity, please clarify which version of the DeepCompress algorithm was used for each experiment.\n    *   In Section 5.2, explicitly state whether the results in Table 1 and Figure 3 were generated using the basic DeepCompress framework or the enhanced version from Section 4.4.\n    *   Ensure that model configurations are consistent across experiments or that any differences are clearly stated. For instance, if the main results use the correctness-conditioned reward, the ablation study in Section 5.3 should either use it as well or explicitly state the difference.5) Score\n*   Overall (10): 7 — The paper presents a novel, well-motivated method with strong results against baselines (Table 1), but significant technical inconsistencies (Section 5.2, Figure 2) weaken its contribution.\n*   Novelty (10): 9 — The core idea of a model-aware, dual-reward strategy that adaptively encourages shorter or longer reasoning is highly original (Section 4.2, 4.3).\n*   Technical Quality (10): 6 — The methodology is sound, but major reporting errors, such as a miscalculated key efficiency statistic (Section 5.2) and a contradiction in a core hyperparameter (Figure 2 vs. Table 3), are present.\n*   Clarity (10): 7 — The paper is generally well-written, but key metric definitions are ambiguous and a core figure caption is incorrect (Figure 2, Figure 3).\n*   Confidence (5): 5 — I am highly confident in my assessment, as the identified weaknesses are based on direct, verifiable evidence within the manuscript.",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 8,
        "novelty": 9,
        "technical_quality": 8,
        "clarity": 8,
        "confidence": 5
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 7,
        "novelty": 9,
        "technical_quality": 6,
        "clarity": 7,
        "confidence": 5
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "1) Summary\nThis paper introduces DeepCompress, a framework designed to improve both the accuracy and efficiency of Large Reasoning Models (LRMs). The core problem addressed is the tendency of LRMs to \"overthink\" simple problems and \"underthink\" complex ones. The proposed method uses reinforcement learning with a novel dual-reward strategy. It dynamically classifies problems as \"Simple\" or \"Hard\" based on the model's real-time performance (group pass ratio vs. an exponential moving average of the batch pass ratio). For \"Simple\" problems, DeepCompress rewards shorter reasoning chains to improve efficiency. For \"Hard\" problems, it rewards longer, more exploratory chains to improve accuracy. Experiments on several mathematical reasoning benchmarks show that DeepCompress outperforms strong baselines in both pass@1 accuracy and token efficiency, achieving new state-of-the-art results.2) Strengths\n*   **Novel and Intuitive Method**: The paper proposes a novel dual-reward mechanism that challenges the prevailing assumption that shorter reasoning paths are always better for efficiency. The core idea of dynamically adapting the reward based on a model-aware difficulty metric is both innovative and well-motivated.\n    *   The preliminary analysis in Section 3 (Figure 1) provides a strong motivation, showing that while pass@1 scores favor shorter responses, pass@32 scores (relevant for RL with group sampling) often favor longer responses, suggesting longer chains contain a wider variety of correct solutions.\n    *   The model-aware difficulty mechanism (Section 4.3, Equations 6-8) is a clever way to assess problem difficulty without relying on static, pre-annotated labels, allowing the system to adapt to the model's evolving capabilities during training.\n    *   The dual-reward formulation itself, using the sign of a hyperparameter `β` to switch between length penalty and length bonus modes (Section 4.2, Figure 2), is an elegant implementation of the core concept.*   **Strong Empirical Performance**: The proposed method achieves state-of-the-art results on a wide range of challenging mathematical reasoning benchmarks, demonstrating significant improvements over strong baselines in both accuracy and efficiency.\n    *   Table 1 shows that DeepCompress-Zero models consistently outperform all baselines, including the previous state-of-the-art DeepMath-Zero models, across seven different benchmarks at both 3B and 7B scales.\n    *   The performance gains are particularly notable on the most difficult datasets, such as AIME 24 and AIME 25, where DeepCompress-Zero-7B achieves absolute improvements of +4.1 and +6.5 points, respectively (Table 1). This supports the claim that encouraging exploration on hard problems is effective.\n    *   Simultaneously, the method achieves significant token efficiency gains. Figure 3 and the data below Table 1 show that DeepCompress models produce substantially shorter responses on average compared to the DeepMath-Zero baseline.*   **Thorough Experimental Analysis and Ablations**: The paper provides a comprehensive set of analyses that go beyond simple performance tables, offering valuable insights into the behavior and effectiveness of the proposed method.\n    *   The ablation study in Section 5.3 (Figure 4) effectively contrasts DeepCompress with fixed-strategy baselines (Length Penalty and Length Bonus). It clearly illustrates how DeepCompress adaptively balances exploration (higher initial policy entropy) and exploitation (stabilizing entropy and reducing length over time) to achieve superior final performance.\n    *   The analysis of training dynamics (Figure 4a, 4b) is particularly insightful, showing how policy entropy and response length evolve differently for each strategy, corroborating the method's intended behavior.\n    *   The investigation into emergent reasoning behaviors (Section 5.4, Table 2) provides evidence that DeepCompress encourages more frequent and efficient \"reflection,\" linking the method's higher policy entropy to tangible improvements in problem-solving on hard questions.3) Weaknesses\n*   **Inconsistencies in Reported Results and Methodology**: The manuscript contains numerical inconsistencies that undermine confidence in the technical reporting.\n    *   The average efficiency gain for the 7B model is misreported. Section 5.2 claims a 16.6% reduction in response length, but a manual calculation based on the seven data points provided in the text below Table 1 and visualized in Figure 3 yields an average reduction of approximately 35.4%. This is a significant reporting error.\n    *   There is a direct contradiction regarding the reward weight hyperparameter `α`. Appendix B (Table 3) states `α=0.2`, but the visualization of the reward function in Figure 2 shows a reward range that implies `α` is approximately 0.1 (e.g., the reward for correct responses ranges from 1.0 to 1.1, not 1.0 to 1.2). This discrepancy raises questions about the actual implementation.*   **Ambiguity and Errors in Metrics and Figures**: The paper's claims about efficiency are based on \"Response Length,\" but this metric is not precisely defined. This, combined with other clarity issues, makes it difficult to fully interpret the results.\n    *   In Figure 3 and the text below Table 1, \"Response Length\" is reported as a single number. It is unclear if this is the average length of all sampled responses, only correct responses, or the single response used for pass@1 evaluation. This is critical, as the inference cost for a pass@1 solution could be low even if other sampled responses are long.\n    *   The caption for Figure 2 is incorrect. It states that \"Blue indicates correct responses and Red indicates incorrect responses,\" but the figure legend and visual structure show that correctness is separated by vertical position (top vs. bottom clusters), while color encodes the value of the hyperparameter `β`.*   **Claims of Superiority Complicated by Ablation Results**: The paper's narrative of achieving superior accuracy is challenged by its own ablation study.\n    *   The ablation results in Figure 4c show that the \"w/ Length Bonus\" variant consistently achieves a higher pass@1 score than the proposed DeepCompress method in the later stages of training.\n    *   The paper acknowledges this finding in the text (Section 5.3), but the main claims in the introduction and conclusion emphasize achieving \"superior accuracy\" without this important nuance. This suggests that for maximizing accuracy alone, a simpler strategy might be more effective, and the primary benefit of DeepCompress is in finding a better balance between accuracy and efficiency.*   **Lack of Sensitivity Analysis for Key Hyperparameters**: The proposed method introduces several new hyperparameters, but the paper does not provide an analysis of how sensitive the model's performance is to their specific values.\n    *   The reward weight `α` (Equation 5) and the EMA parameter `λ` (Equation 11) are critical to balancing the length reward and stabilizing the difficulty metric, respectively. Their values are fixed at 0.2 and 0.99 (Table 3) without justification or ablation.\n    *   The group size `G` (referred to as `rollout.n` in Table 3, set to 32) directly influences the calculation of the group pass ratio `P_g` (Equation 6), which is central to the difficulty classification. The impact of this choice is not explored.\n    *   The initialization of the smoothed batch pass ratio `P_{b,t}` to 1.0 (Section 4.4) is described as an important choice, but the effect of this specific value is not empirically validated.*   **Unclear Application of Robustness Enhancements**: The paper introduces two enhancements for robustness in Section 4.4 (\"Correctness-Conditioned Length Reward\" and \"Smoothed Batch Pass Ratio\"), but it is not explicitly stated whether these were used to generate the main results.\n    *   Section 4.4 presents these as improvements, but Section 5.2 (\"Main Results\") refers simply to \"DeepCompress\" without specifying which version of the algorithm was used for Table 1 and Figure 3.\n    *   This is problematic because the \"Correctness-Conditioned Length Reward\" (Equation 10) is a significant modification. If this was used for the main results, the ablation study in Section 5.3 (which appears to use the basic reward from Equation 9) may not be a fair comparison.4) Suggestions for Improvement\n*   **Correct and Verify Numerical Claims and Figures**: To improve the technical quality and trustworthiness of the paper, please perform a thorough check of all reported numbers and figures.\n    *   Re-calculate and report the correct average efficiency gain for the 7B model in Section 5.2. If the 16.6% value is correct, please provide the data and calculation method that leads to it.\n    *   Ensure that the hyperparameters reported in the appendix (Table 3) match the values used to generate all figures and results. Specifically, resolve the discrepancy for `α` between Table 3 and Figure 2.*   **Clarify Evaluation Metrics and Figures**: To improve clarity and reproducibility, please provide precise definitions for metrics and correct figure descriptions.\n    *   In Section 5.1 or in the captions for Table 1 and Figure 3, please specify exactly how \"Response Length\" is calculated (e.g., \"average token count of the first generated response,\" or \"average token count across all 16 sampled responses\").\n    *   Correct the caption for Figure 2 to accurately describe that color represents the value of `β`, while the vertical grouping of curves represents response correctness.*   **Refine and Nuance Performance Claims**: To present a more accurate picture of the method's contribution, please nuance the claims about performance superiority.\n    *   In the main results and discussion, explicitly acknowledge that the \"Length Bonus\" ablation achieves higher accuracy (as shown in Figure 4c).\n    *   Frame the primary contribution of DeepCompress more precisely as achieving a superior *balance* between accuracy and efficiency compared to baselines and fixed-strategy ablations, rather than claiming unqualified superiority on both axes.*   **Provide Hyperparameter Sensitivity Analysis**: To demonstrate the robustness of the method, please include an analysis of the key hyperparameters introduced.\n    *   An ablation study, perhaps in the Appendix, showing how performance changes with different values of `α` and `λ` would be highly valuable.\n    *   It would also be beneficial to discuss the rationale behind the choice of `G=32` and potentially show results for at least one other value to understand its impact.\n    *   A brief experiment or discussion on the initialization of `P_{b,t}` would further strengthen the claims made in Section 4.4.*   **Be Explicit About Experimental Configurations**: To avoid ambiguity, please clarify which version of the DeepCompress algorithm was used for each experiment.\n    *   In Section 5.2, explicitly state whether the results in Table 1 and Figure 3 were generated using the basic DeepCompress framework or the enhanced version from Section 4.4.\n    *   Ensure that model configurations are consistent across experiments or that any differences are clearly stated. For instance, if the main results use the correctness-conditioned reward, the ablation study in Section 5.3 should either use it as well or explicitly state the difference.5) Score\n*   Overall (10): 7 — The paper presents a novel, well-motivated method with strong results against baselines (Table 1), but significant technical inconsistencies (Section 5.2, Figure 2) weaken its contribution.\n*   Novelty (10): 9 — The core idea of a model-aware, dual-reward strategy that adaptively encourages shorter or longer reasoning is highly original (Section 4.2, 4.3).\n*   Technical Quality (10): 6 — The methodology is sound, but major reporting errors, such as a miscalculated key efficiency statistic (Section 5.2) and a contradiction in a core hyperparameter (Figure 2 vs. Table 3), are present.\n*   Clarity (10): 7 — The paper is generally well-written, but key metric definitions are ambiguous and a core figure caption is incorrect (Figure 2, Figure 3).\n*   Confidence (5): 5 — I am highly confident in my assessment, as the identified weaknesses are based on direct, verifiable evidence within the manuscript."
}