# Global Summary
This paper introduces DeepCompress, a framework designed to improve both the accuracy and efficiency of Large Reasoning Models (LRMs) by addressing their tendency to "overthink" simple problems and "underthink" complex ones. The core idea is that while shorter reasoning paths are efficient for simple problems, longer, more exploratory paths can contain a wider range of correct solutions for difficult problems. DeepCompress uses a dual-reward strategy within a reinforcement learning (RL) framework. It dynamically classifies problems as "Simple" or "Hard" based on the model's real-time performance, comparing a question's group pass ratio to the batch pass ratio. For "Simple" problems, it rewards shorter responses to compress reasoning. For "Hard" problems, it rewards longer responses to encourage exploration. The method is built on the Zero RL recipe using the DAPO algorithm. Experiments on 3B and 7B models across seven mathematical benchmarks show that DeepCompress consistently outperforms baselines like DeepMath-Zero. For instance, DeepCompress-Zero-7B achieves a +2.7 point average accuracy improvement over DeepMath-Zero-7B while reducing response length by 16.6% on average. The authors state a limitation is the method's reliance on sufficient length variation in sampled responses and a 10k token generation cap during training.

# Abstract
Large Reasoning Models (LRMs) exhibit cognitive inefficiencies, such as overthinking simple problems and underthinking complex ones. Existing methods using Supervised Fine-Tuning (SFT) or Reinforcement Learning (RL) with token-length rewards often improve efficiency at the cost of accuracy. This paper introduces DeepCompress, a framework to enhance both accuracy and efficiency. The authors challenge the idea of always favoring shorter reasoning paths, showing that longer responses can offer more correct solutions for difficult problems. DeepCompress uses an adaptive length reward that classifies problems as "Simple" or "Hard" in real-time based on the model's capability. It rewards shorter reasoning for "Simple" problems and encourages longer, exploratory chains for "Hard" ones. This dual-reward strategy allows the model to autonomously adjust its Chain-of-Thought (CoT) length. Experiments on mathematical benchmarks show DeepCompress outperforms baselines in both accuracy and token efficiency.

# Introduction
- LRMs like OpenAI's o1, DeepSeek's R1, Google's Gemini 2.5, and Anthropic's Claude 3.7 show strong reasoning but suffer from inefficiencies like "overthinking" (excessive steps for simple problems) and "underthinking" (unstable thought shifts for complex ones).
- Prior work to improve efficiency includes SFT on shortened CoT datasets and RL with token-length rewards. These methods often achieve efficiency gains but with slight accuracy drops.
- The paper proposes DeepCompress, a framework with an adaptive length reward mechanism. It is based on the finding that longer responses can contain a wider coverage of correct solutions for difficult problems.
- DeepCompress classifies problems as "Simple" or "Hard" during RL training. A problem is "Simple" if its group pass ratio (correct samples in G generations) is above the batch pass ratio (average group pass ratio in the batch), and "Hard" otherwise.
- The framework encourages shorter responses for "Simple" problems and longer responses for "Hard" ones, using Group Relative Policy Optimization (GRPO) as the base RL algorithm.
- Contributions are: (1) DeepCompress with a model-aware difficulty mechanism and dual length reward, (2) experimental results showing superior performance and efficiency on math benchmarks, and (3) analysis showing DeepCompress fosters high policy entropy for better exploration and reflection.

# Related Work
- **Manipulating Reasoning Length through Prompt Engineering:** Some studies show longer reasoning improves performance, while others advocate for conciseness (e.g., Constrained-CoT). Methods like Thinking-Optimal Scaling find an ideal length, while others like Adaption-of-Thought (ADOT) and TALE dynamically adapt reasoning length based on the problem.
- **Post-Training for Reasoning Efficiency:** This is a major area of work.
    - SFT-based methods train models on curated, concise reasoning examples.
    - RL-based methods are more common and typically penalize excessive length. Simple approaches use a length-based reward, while more advanced methods use dynamic reward-shaping.
    - Other architectural innovations include auxiliary reflection models and iterative pruning.
- The paper claims these existing post-training methods achieve notable efficiency but offer limited accuracy improvements and sometimes cause performance loss.

# Preliminaries
This section analyzes the relationship between response length and performance to motivate the proposed method.
- **Experimental Setup:**
    - **Data:** Four math benchmarks: MATH-500, Olympiad-Bench, Minerva Math, and AIME 2025.
    - **Model:** DeepMath-Zero-3B and DeepMath-Zero-7B, which are fine-tuned Qwen models.
    - **Metric:** pass@k, with a focus on pass@1 and pass@32. For each problem, n=8,192 samples are generated.
    - **Evaluation:** Responses are sorted by standardized length and divided into 16 bins. Average length and pass@k are calculated for each bin.
- **Results (Figure 1):**
    - For pass@1, shorter responses perform better.
    - For pass@32, longer responses generally perform better and surpass shorter ones. This suggests longer responses contain a wider variety of correct solutions.
    - This finding is crucial for RL algorithms like GRPO that sample multiple solutions, as it indicates that constantly optimizing for shorter responses may limit the model's problem-solving capacity.

# Method
The paper proposes DeepCompress, which enhances the Zero RL framework with a dual length reward and a model-aware difficulty mechanism.
- **Zero RL:** The method builds on the Zero RL training recipe using DAPO as the RL algorithm. The base outcome reward `R_o` is binary: +1 for a correct answer, -1 otherwise.
- **Dual Length Reward:**
    - For a group of G generated responses, the standardized length `z_i` is calculated for each response.
    - A length reward `R_z` is computed using a sigmoid function: `R_z = 1 / (1 + e^(β * z_i))`.
    - The sign of `β` controls the reward mode: `β > 0` (for simple questions) rewards shorter responses, while `β < 0` (for hard questions) rewards longer ones.
    - The final length reward is `R_l = α * R_z`, where `α` is a scaling hyperparameter.
- **Model-Aware Difficulty:**
    - Question difficulty is assessed dynamically during training.
    - **Group pass ratio `P_g(x_i)`:** The proportion of correct responses among G generated outputs for a question `x_i`.
    - **Batch pass ratio `P_b`:** The average of `P_g(x_i)` over a batch.
    - The parameter `β_i` is set to `P_g(x_i) - P_b`. A positive `β_i` means the question is "Simple" for the model, while a negative `β_i` means it is "Hard".
    - The final reward for RL is `R = R_o + R_l`.
- **Enhancing Robustness:**
    - **Correctness-Conditioned Length Reward:** The length reward `R_l` is only applied to responses that are correct (`R_o = 1`) to prevent reward hacking.
    - **Smoothed Batch Pass Ratio:** The batch pass ratio `P_b` is smoothed using an exponential moving average (EMA) with parameter `λ` to improve training stability. The EMA is initialized optimistically at 1.0.

# Experiments
- **Experimental Setup:**
    - **RL Training:** Models are fine-tuned from Qwen2.5-3B and Qwen2.5-7B using the DAPO algorithm.
    - **Evaluation:** Performed on seven math benchmarks: MATH-500, AMC 2023, OlympiadBench, Minerva Math, AIME 2024-2025, and PolyMath (English subset). The primary metric is pass@1 with 16 samples per question. Inference uses vLLM with temperature=0.6, top_p=0.95, and max_tokens=32,768.
- **Main Results:**
    - **Accuracy (Table 1):** DeepCompress models consistently outperform Zero RL baselines. DeepCompress-Zero-3B achieves a +2.0 point average absolute improvement over DeepMath-Zero-3B. DeepCompress-Zero-7B achieves a +2.7 point average improvement over DeepMath-Zero-7B. Gains are particularly large on hard datasets like AIME 24 (+4.1) and AIME 25 (+6.5) for the 7B model.
    - **Efficiency (Figure 3):** DeepCompress models generate more concise responses. On average, response length is compressed by 57.9% for the 3B model and 16.6% for the 7B model compared to DeepMath-Zero. For example, on AIME 24, DeepCompress-Zero-7B uses 35.2% fewer tokens for a +4.1 point accuracy gain.
- **Impact of Length Reward (Ablation Study):**
    - Compared DeepCompress with fixed-beta variants: Length Penalty (`β=1`) and Length Bonus (`β=-1`).
    - **Policy Entropy (Figure 4a):** Length Bonus has high entropy, Length Penalty has low, stable entropy. DeepCompress starts high (exploration) and then stabilizes.
    - **Response Length (Figure 4d):** Length Bonus produces the longest responses, Penalty the shortest. DeepCompress is adaptive.
    - **Performance (Figure 4c):** DeepCompress shows continuous performance growth, eventually matching or exceeding the Length Bonus variant, demonstrating a balance between exploration and efficiency.
- **Quantifying Emergence of Reasoning Behaviors:**
    - Analysis on a set of hard problems where baseline models failed. Cognitive behaviors ("aha moments") were tracked using GPT-4o.
    - **Results (Table 2):** DeepCompress models show a higher frequency of reflection compared to baselines (e.g., 2.64 vs 2.59 for 7B) but with significantly shorter average response lengths (5,942 vs 7,180) and higher pass@1 scores (13.81 vs 11.35). This suggests more efficient and effective reflection ("smarter thinking").

# Conclusion
- The paper introduces DeepCompress, a framework with a model-aware difficulty mechanism and a dual length reward to improve LRM accuracy and efficiency.
- It dynamically encourages short solutions for simple problems and deep exploration for hard ones.
- Experiments show DeepCompress achieves new SOTA performance on math benchmarks while improving token efficiency.
- Analysis indicates the method promotes effective learning through high policy entropy, leading to more frequent and efficient reflection.
- **Limitations:** The method's effectiveness depends on having sufficient length variation among sampled responses. Training was also constrained by a maximum generation length of 10k tokens, which might limit exploration on very complex problems.

# Appendix
- **Appendix A:** Contains the prompt used with GPT-4o to identify and analyze reasoning behaviors like Backtracking, Verification, Subgoal Setting, and Enumeration.
- **Appendix B:** Provides training configurations in Table 3.
    - Key hyperparameters for both 3B and 7B models include: lr=1e-6, max_response_length=10K, train_batch_size=512, rollout.n=32 (group size G), reward_weight α=0.2, and EMA_parameter λ=0.99.
    - The training framework used is `verl`.

# References
This section lists the references cited in the paper.