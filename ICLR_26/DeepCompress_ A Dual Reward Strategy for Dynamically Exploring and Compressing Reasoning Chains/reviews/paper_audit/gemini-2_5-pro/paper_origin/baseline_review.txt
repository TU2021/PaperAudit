1) Summary
This paper introduces DeepCompress, a framework designed to improve both the accuracy and efficiency of Large Reasoning Models (LRMs). The core problem addressed is the tendency of LRMs to "overthink" simple problems and "underthink" complex ones. The proposed method uses reinforcement learning with a novel dual-reward strategy. It dynamically classifies problems as "Simple" or "Hard" based on the model's real-time performance (group pass ratio vs. an exponential moving average of the batch pass ratio). For "Simple" problems, DeepCompress rewards shorter reasoning chains to improve efficiency. For "Hard" problems, it rewards longer, more exploratory chains to improve accuracy. Experiments on several mathematical reasoning benchmarks show that DeepCompress outperforms strong baselines in both pass@1 accuracy and token efficiency, achieving new state-of-the-art results.2) Strengths
*   **Novel and Intuitive Method**: The paper proposes a novel dual-reward mechanism that challenges the prevailing assumption that shorter reasoning paths are always better for efficiency. The core idea of dynamically adapting the reward based on a model-aware difficulty metric is both innovative and well-motivated.
    *   The preliminary analysis in Section 3 (Figure 1) provides a strong motivation, showing that while pass@1 scores favor shorter responses, pass@32 scores (relevant for RL with group sampling) often favor longer responses, suggesting longer chains contain a wider variety of correct solutions.
    *   The model-aware difficulty mechanism (Section 4.3, Equations 6-8) is a clever way to assess problem difficulty without relying on static, pre-annotated labels, allowing the system to adapt to the model's evolving capabilities during training.
    *   The dual-reward formulation itself, using the sign of a hyperparameter `β` to switch between length penalty and length bonus modes (Section 4.2, Figure 2), is an elegant implementation of the core concept.*   **Strong Empirical Performance**: The proposed method achieves state-of-the-art results on a wide range of challenging mathematical reasoning benchmarks, demonstrating significant improvements over strong baselines in both accuracy and efficiency.
    *   Table 1 shows that DeepCompress-Zero models consistently outperform all baselines, including the previous state-of-the-art DeepMath-Zero models, across seven different benchmarks at both 3B and 7B scales.
    *   The performance gains are particularly notable on the most difficult datasets, such as AIME 24 and AIME 25, where DeepCompress-Zero-7B achieves absolute improvements of +4.1 and +6.5 points, respectively (Table 1). This supports the claim that encouraging exploration on hard problems is effective.
    *   Simultaneously, the method achieves significant token efficiency gains. Figure 3 shows that DeepCompress models produce substantially shorter responses on average compared to the DeepMath-Zero baseline, with reductions of up to 57.9% for the 3B model.*   **Thorough Experimental Analysis and Ablations**: The paper provides a comprehensive set of analyses that go beyond simple performance tables, offering valuable insights into the behavior and effectiveness of the proposed method.
    *   The ablation study in Section 5.3 (Figure 4) effectively contrasts DeepCompress with fixed-strategy baselines (Length Penalty and Length Bonus). It clearly illustrates how DeepCompress adaptively balances exploration (higher initial policy entropy) and exploitation (stabilizing entropy and reducing length over time) to achieve superior final performance.
    *   The analysis of training dynamics (Figure 4a, 4b) is particularly insightful, showing how policy entropy and response length evolve differently for each strategy, corroborating the method's intended behavior.
    *   The investigation into emergent reasoning behaviors (Section 5.4, Table 2) provides evidence that DeepCompress encourages more frequent and efficient "reflection," linking the method's higher policy entropy to tangible improvements in problem-solving on hard questions.3) Weaknesses
*   **Ambiguity in Key Evaluation Metrics**: The paper's claims about efficiency gains are based on "Response Length," but this metric is not precisely defined. This ambiguity makes it difficult to fully interpret the reported efficiency improvements.
    *   In Figure 3 and the text below Table 1, "Response Length" is reported as a single number for each model-dataset pair. It is unclear if this is the average length of all 16 sampled responses, the average length of only correct responses, or the length of the single response used for pass@1 evaluation.
    *   This lack of clarity is critical because different definitions have different implications. For example, if the model generates one very short correct answer and 15 very long incorrect ones, the average length might be high, but the inference cost for the pass@1 solution is low. The current presentation does not distinguish between these scenarios.
    *   No direct evidence found in the manuscript defining how "Response Length" is calculated for the evaluation reported in Figure 3 and Table 1.*   **Lack of Sensitivity Analysis for Key Hyperparameters**: The proposed method introduces several new hyperparameters, but the paper does not provide an analysis of how sensitive the model's performance is to their specific values.
    *   The reward weight `α` (Equation 5) and the EMA parameter `λ` (Equation 11) are critical to balancing the length reward and stabilizing the difficulty metric, respectively. Their values are fixed at 0.2 and 0.99 (Table 3) without justification or ablation.
    *   The group size `G` (referred to as `rollout.n` in Table 3, set to 32) directly influences the calculation of the group pass ratio `P_g` (Equation 6), which is central to the difficulty classification. The impact of this choice is not explored.
    *   The initialization of the smoothed batch pass ratio `P_{b,t}` to 1.0 (Section 4.4) is described as an important choice to prevent premature penalization, but the effect of this specific initialization value is not empirically validated against other potential choices (e.g., 0.5).*   **Unclear Application of Robustness Enhancements**: The paper introduces two enhancements for robustness in Section 4.4 ("Correctness-Conditioned Length Reward" and "Smoothed Batch Pass Ratio"), but it is not explicitly stated whether these were used to generate the main results.
    *   Section 4.4 presents these as improvements upon the core DeepCompress framework. However, Section 5.2 ("Main Results") refers simply to "DeepCompress" without specifying which version of the algorithm was used to produce the results in Table 1 and Figure 3.
    *   This is problematic because the "Correctness-Conditioned Length Reward" (Equation 10) is a significant modification designed to prevent reward hacking. If this was used for the main results, the ablation study in Section 5.3 (which appears to use the basic, unconditional reward from Equation 9) may not be a fair comparison.
    *   The paper should clarify if the main results were generated using the basic DeepCompress (Section 4.1-4.3) or the enhanced version (Section 4.4), as this affects the interpretation of both the main claims and the ablation studies.4) Suggestions for Improvement
*   **Clarify Evaluation Metrics**: To improve the clarity and reproducibility of the efficiency claims, the definition of "Response Length" used in the evaluation should be explicitly stated.
    *   In Section 5.1 ("Evaluation") or in the captions for Table 1 and Figure 3, please specify precisely how the reported "Response Length" is calculated (e.g., "average token count of the first generated response for each problem," or "average token count across all 16 sampled responses per problem").
    *   If multiple length metrics were tracked (e.g., length of pass@1 response vs. average length of all samples), reporting both would provide a more nuanced and complete picture of the model's efficiency.
    *   This clarification would strengthen the paper's claims by allowing readers to understand the exact nature of the efficiency gains.*   **Provide Hyperparameter Sensitivity Analysis**: To demonstrate the robustness of the method, please include an analysis of the key hyperparameters introduced.
    *   An ablation study, perhaps in the Appendix, showing how performance (both accuracy and length) changes with different values of the reward weight `α` and the EMA parameter `λ` would be highly valuable.
    *   It would also be beneficial to discuss the rationale behind the choice of the group size `G=32` and potentially show results for at least one other value to understand its impact on the difficulty estimation.
    *   A brief experiment or discussion on the initialization of `P_{b,t}` would further strengthen the claims made in Section 4.4 about training stability.*   **Be Explicit About Experimental Configurations**: To avoid ambiguity and ensure fair comparisons, please clarify which version of the DeepCompress algorithm was used for each experiment.
    *   In Section 5.2, explicitly state whether the results in Table 1 and Figure 3 were generated using the basic DeepCompress framework or the enhanced version described in Section 4.4.
    *   Ensure that the model configurations are consistent across all experiments or that any differences are clearly stated and justified. For instance, if the main results use the correctness-conditioned reward, the ablation study in Section 5.3 should either use it as well (and ablate other components) or explicitly state that it is comparing the enhanced model to a more basic one.
    *   This clarification will improve the internal consistency of the paper and allow for a more accurate interpretation of the experimental results.5) Score
*   Overall (10): 8 — The paper presents a novel, well-motivated method with strong empirical results (Table 1, Figure 3) and insightful analysis (Figure 4).
*   Novelty (10): 9 — The core idea of a model-aware, dual-reward strategy that adaptively encourages shorter or longer reasoning is highly original (Section 4.2, 4.3).
*   Technical Quality (10): 8 — The methodology is sound and the experiments are extensive, though the lack of hyperparameter sensitivity analysis is a minor weakness (Section 4.4, Table 3).
*   Clarity (10): 8 — The paper is very well-written and easy to follow, but key metric definitions are ambiguous (Figure 3).
*   Confidence (5): 5 — I am highly confident in my assessment, as I have a strong background in reinforcement learning and large language models.