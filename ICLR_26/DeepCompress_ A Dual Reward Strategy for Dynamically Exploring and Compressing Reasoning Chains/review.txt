### Summary

This submission proposes **DeepCompress**, an RL-based framework for **reasoning-chain length control** in Large Reasoning Models (LRMs). The key idea is **not** to always penalize long CoTs: instead, DeepCompress uses a **dual reward** that **encourages shorter reasoning on “Simple” problems** and **longer, more exploratory reasoning on “Hard” problems**, where “difficulty” is determined **dynamically during training** (via batch/group pass-rate statistics). The authors argue this resolves “overthinking simple problems / underthinking hard problems,” improving **both pass@1 accuracy and token efficiency**.

In the rebuttal, they emphasize:

* **Training stability** via large sampling batches (e.g., 2048) and steadily increasing batch pass ratio.
* **Stronger baselines** added (e.g., Kimi-k1.5, Short-RL) showing that naive length compression often hurts accuracy, while DeepCompress reduces length vs DeepMath-Zero without collapsing performance.
* **Hyperparameter ablations** (e.g., α, λ) and a comparison of **model-aware difficulty vs. data-labeled difficulty** (their dynamic scheme wins).
* **Generalization beyond math** via added GPQA / BBH / MMLU-STEM results (DeepCompress improves over relevant baselines).

### Strengths

* **Addresses a real pain point**: length-control RL often trades accuracy for concision; the “adaptive” framing is timely for LRMs.
* **Conceptually intuitive dual regime**: compress when solved, explore when not—matches practical expectations of compute allocation.
* **Strong empirical story (post-rebuttal)**: added comparisons suggest DeepCompress can **cut average length substantially** relative to “always think long” RL (DeepMath-Zero) while improving accuracy on hard sets (notably AIME-style).
* **Expanded evaluation**: math benchmarks are broad; rebuttal adds non-math reasoning benchmarks, which helps the claim that this is not a math-only trick.
* **Evidence for stability**: the reported batch pass ratio curves and multi-step training behavior aim to reduce concerns about noisy difficulty signals.

### Weaknesses

These are the main “still slightly shaky” points that reviewers repeatedly circled:

* **Difficulty definition is relative + batch-dependent**
  “Hard vs Simple” is derived from **batch/group pass ratios**, so difficulty is **not absolute** and can fluctuate with batch composition. Large batch size helps, but the conceptual critique remains: it’s a moving target that may bias learning dynamics.

* **The “no-correct-rollout” paradox** (important)
  If a prompt yields **Pg = 0** (no correct samples), then correctness-conditioned length rewards may not activate—so the mechanism meant to push exploration for hard problems may provide **no exploratory signal exactly when needed**. Authors argue relaxing this causes reward hacking; fair, but this still exposes a limitation of the design.

* **Dependence on length variance / decoding diversity**
  The reward uses standardized length (σ issues). If rollouts have similar length, the length signal weakens or becomes unstable, making the method sensitive to **sampling temperature/top-p** and the model’s initial behavioral diversity.

* **Training objective vs evaluation objective tension**
  If training benefits from “finding at least one correct in a group” (pass@k-ish dynamics) but evaluation is pass@1, there’s a subtle risk that improvements come from exploration-friendly training dynamics rather than genuinely better single-shot reasoning. They added pass@32 results, but the conceptual tension still exists.

* **Mechanistic clarity**
  The method is largely **reward shaping**. The rebuttal gives a clearer policy-gradient formulation (“not a heuristic add-on”), but some readers may still feel it’s an empirical trick unless there’s deeper analysis on why the dynamic rule is the *right* shaping beyond “it works.”
