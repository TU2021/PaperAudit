Here are four distinct reviews of the research paper.

***

### **Review 1**

**Summary**
This paper introduces two main contributions to the field of Test-Time Adaptation (TTA). First, it proposes a new benchmark dataset called Inherent Temporal Dependencies (ITD), which is constructed from object tracklets to better simulate realistic deployment scenarios with temporal dependencies, a feature missing in current TTA benchmarks. Second, the authors identify a weakness in memory-based TTA methods, particularly their poor performance when initialized with an empty memory bank in non-i.i.d. settings. To address this, they propose ADVMEM, a novel adversarial memory initialization strategy that populates the memory bank with synthetic, class-representative samples before adaptation begins. The paper demonstrates through extensive experiments on ITD that ADVMEM significantly improves the performance of several memory-based TTA methods.

**Soundness**
The paper's methodology is generally sound. The motivation for the ITD benchmark is well-argued and supported by a preliminary experiment (Table 1) showing the performance degradation of existing TTA methods when temporal correlation is mimicked. The construction of ITD from the TrackingNet dataset (Section 3.1) is a logical and effective way to introduce natural temporal dependencies. The experimental design is comprehensive, evaluating methods across three increasingly challenging scenarios: frame-wise i.i.d., tracklet-wise i.i.d., and tracklet-wise non-i.i.d. (Section 4). The proposed ADVMEM method is a creative approach to the identified problem of memory initialization, and its formulation (Section 5.1) is clear. The ablation studies on the label distribution (Section 6.1) and batch size (Section 6.2) further strengthen the empirical validation of the method.

**Presentation**
The paper is well-written and clearly structured. The introduction effectively motivates the problem and outlines the contributions. The figures, particularly Figure 1 and Figure 2, are excellent at visually explaining the difference between existing benchmarks and the proposed ITD benchmark. The experimental sections are logically organized, building from simpler to more complex scenarios.

However, there is a significant issue with the numbering and referencing of tables in the main text. For instance, Section 5 ("Performance with ADVMEM") refers to "Table 2" and "Table 3" for results, but Table 2 in the paper is an overview of methods and Table 3 describes corruptions. The actual results tables for these experiments appear to be in the appendix (e.g., Table 7, Table 8), which makes the main text very difficult to follow and verify. This is a major organizational flaw that needs to be corrected for the final version.

**Contribution**
The paper makes two significant contributions.
1.  **ITD Benchmark:** The lack of realistic benchmarks that incorporate temporal dependencies is a known gap in TTA research. ITD is a novel and valuable resource that directly addresses this limitation. By using tracklets, it provides a more faithful simulation of real-world data streams than existing benchmarks based on shuffling corrupted images.
2.  **ADVMEM:** The proposed adversarial memory initialization is a novel and clever idea. It identifies a key weakness in memory-based TTA—the "cold start" problem in non-i.i.d. streams—and offers an elegant, data-free solution. The demonstration that this plug-and-play module can substantially boost the performance of methods like SHOT-IM and TENT (as shown in the appendix, Table 4a) is impressive.

**Strengths**
- **Novel and Realistic Benchmark:** The ITD benchmark is the paper's strongest point. It pushes the TTA community towards evaluating methods in more realistic and challenging conditions, which is crucial for real-world deployment.
- **Clear Problem Identification:** The paper clearly identifies and demonstrates the failure modes of existing TTA methods under temporal correlation (Table 1) and the specific issue of empty memory initialization in non-i.i.d. streams (Section 4.7).
- **Effective Proposed Method:** ADVMEM is a simple yet powerful idea. Its "plug-and-play" nature and data-free initialization (not requiring access to the original training set) make it a very practical contribution.
- **Thorough Experimentation:** The paper includes a comprehensive set of experiments, including multiple evaluation scenarios, different model architectures (ResNet, ViT), and insightful ablation studies.

**Weaknesses**
- **Presentation of Results:** The primary weakness is the confusing and incorrect table referencing in the main body of the paper, as mentioned under "Presentation". This severely hinders readability and makes it hard to connect claims to evidence without hunting through the appendix.
- **Explanation of TENT/SHOT-IM Failure:** The paper shows that simply adding a memory bank to TENT and SHOT-IM results in catastrophic failure (Table 6). While it hypothesizes this is due to "biased statistics," a more in-depth analysis would be beneficial. Why does the memory mechanism work so well for RoTTA's distillation objective but fail so completely for entropy-based objectives?

**Questions**
1.  Could the authors please fix the table numbering and references throughout the manuscript? For example, in Section 5, what tables should the text be pointing to for the tracklet-wise i.i.d. and non-i.i.d. results with ADVMEM?
2.  The performance of TENT and SHOT-IM with a memory bank is surprisingly poor (Table 6). Have you investigated the dynamics of the memory bank for these methods? For example, is it being filled with low-quality or misclassified samples, leading to the negative result?
3.  The ADVMEM initialization generates synthetic samples by optimizing from noise. What is the computational cost of this initialization step compared to, for example, a single epoch of training? Is this a one-time cost per pre-trained model?

**Rating**
- Overall (10): 8 — The paper introduces a valuable benchmark and a novel method, but the presentation issues with table references are significant.
- Novelty (10): 9 — The tracklet-based benchmark is highly novel, and the adversarial memory initialization is a creative and original idea.
- Technical Quality (10): 8 — The experiments are extensive and well-designed, but the confusing results presentation and a few gaps in analysis detract from the quality.
- Clarity (10): 6 — The core ideas are explained well, but the incorrect table references (e.g., Section 5) make the results section very difficult to follow.
- Confidence (5): 5 — I am confident in my assessment, having expertise in domain adaptation and model robustness.

***

### **Review 2**

**Summary**
The authors propose a new benchmark for Test-Time Adaptation (TTA), named Inherent Temporal Dependencies (ITD), which is based on object tracklets from videos to better model real-world data streams with temporal correlations. They use this benchmark to show that existing TTA methods, especially those with memory banks, struggle in non-i.i.d. scenarios. To remedy this, they introduce ADVMEM, an adversarial initialization technique for the memory bank, which populates it with synthetic samples to ensure class diversity from the start. They claim this method significantly improves the performance of memory-based TTA approaches on their new benchmark.

**Soundness**
The paper's soundness is questionable in several key areas.

First, a central claim of the paper is the superiority of ADVMEM. However, the experimental evidence for this is contradictory. In the appendix (Section 11, Table 5), the authors compare ADVMEM to "TrainMem," an alternative that initializes the memory with real training samples. The text claims ADVMEM is superior, stating "it significantly underperforms our novel ADVMEM." However, the results in Table 5 for RoTTA show an average error of 75.5% for TrainMem (labeled ✓✗) and 78.2% for ADVMEM (labeled ✓). This suggests TrainMem is actually *better* than ADVMEM for RoTTA, directly contradicting the paper's claims. This is a major flaw that undermines the core contribution.

Second, the experiments where TENT and SHOT-IM are equipped with a memory bank (Section 4.6, Table 6) show their error rates jumping to ~94%. This is an extreme failure. The paper offers a brief explanation of "biased statistics," but this is insufficient. Such a catastrophic result could indicate a flaw in the experimental setup or implementation rather than a fundamental weakness of the methods themselves. Without a deeper investigation, it is difficult to accept the conclusion that these methods are incompatible with memory banks.

Third, the motivation experiment in Table 1, which uses duplicated images to "mimic a video clip," is a crude approximation. While it serves to motivate the work, the drastic performance drop of RoTTA (from 27.6% to 66.8%) seems disproportionate and requires more explanation than is provided.

**Presentation**
The paper is poorly organized, particularly concerning the presentation of results. The main text repeatedly refers to incorrect table numbers. For example, Section 5 discusses results for ADVMEM and cites "Table 2" and "Table 3," which contain no such results. The reader is forced to search the appendix for the relevant data, which is itself confusingly organized with its own set of tables (e.g., Table 4, 5, 6, etc.). This makes the paper extremely difficult to read and verify. The symbol "✓✗" used for TrainMem in the appendix's Table 5 is not explained. This level of sloppiness is not acceptable for a scientific publication.

**Contribution**
The contribution of this paper is mixed. The ITD benchmark is a good idea in principle, as the field needs more realistic evaluation settings. However, the execution and analysis presented here are not entirely convincing. The second contribution, ADVMEM, is undermined by the contradictory evidence presented in the paper's own appendix. If a simpler baseline like using real training data (TrainMem) performs better, the novelty and utility of the more complex adversarial approach are questionable. The claim of "equipping existing TTA methods with memory" (Contribution 3, Section 9) is also an overstatement, given that the attempt to do so for TENT and SHOT-IM failed catastrophically.

**Strengths**
- **Benchmark Motivation:** The paper correctly identifies a key limitation of existing TTA benchmarks—the lack of temporal dependencies—and proposes a solution. The motivation for a more realistic benchmark is strong.
- **Problem Formulation:** The identification of the "cold start" problem for memory banks in non-i.i.d. streams is an interesting and relevant insight.

**Weaknesses**
- **Contradictory Results:** The results in the appendix (Table 5) contradict the main claim about ADVMEM's superiority over the TrainMem baseline, which is a critical flaw.
- **Insufficient Analysis:** The paper fails to adequately explain key results, such as the catastrophic failure of TENT/SHOT-IM with memory (Table 6) and the massive performance drop for RoTTA in the motivation experiment (Table 1).
- **Poor Presentation:** The paper is riddled with incorrect table references, making it very difficult to follow the empirical evidence. The organization of the main text and appendix is confusing.
- **Oversold Contributions:** The contributions are presented as more significant than the evidence supports, especially regarding ADVMEM and the modification of non-memory methods.

**Questions**
1.  Can you please clarify the results in Appendix Table 5? The table shows RoTTA with TrainMem (75.5% error) outperforming RoTTA with ADVMEM (78.2% error). This contradicts your claim that TrainMem "significantly underperforms" ADVMEM. Is there an error in the table or the text?
2.  Why do TENT and SHOT-IM fail so completely when a memory bank is added (Table 6)? Could this be an implementation issue? A more thorough diagnosis is needed before concluding that these methods are fundamentally incompatible with memory.
3.  What does the symbol "✓✗" mean in Appendix Table 5?
4.  Given the TrainMem results, what is the justification for using a more complex adversarial generation process for initialization, especially if privacy is not a concern and training data is available (as is the case for the TrainMem experiment)?

**Rating**
- Overall (10): 3 — The paper has a good idea for a benchmark, but the core methodological contribution (ADVMEM) is not supported by the paper's own results, and the presentation is deeply flawed.
- Novelty (10): 6 — The benchmark idea is novel, but the ADVMEM method's novelty is diminished if it doesn't outperform simpler baselines.
- Technical Quality (10): 2 — The technical quality is low due to contradictory results, insufficient analysis of key findings, and a flawed experimental presentation.
- Clarity (10): 3 — The writing is mostly understandable, but the incorrect table references and confusing structure make the paper very difficult to parse and verify.
- Confidence (5): 5 — I am very confident in my assessment, as the contradictions in the results are clear and directly undermine the paper's central claims.

***

### **Review 3**

**Summary**
This paper presents ITD, a new benchmark for test-time adaptation (TTA) that uses sequences of object images (tracklets) to better represent real-world scenarios involving temporal correlation. The authors show that existing TTA methods perform poorly on this benchmark, particularly in non-i.i.d. settings. They diagnose a key issue in memory-based methods: starting with an empty memory bank hurts adaptation. To solve this, they propose ADVMEM, a "plug-and-play" module that initializes the memory bank with adversarially generated synthetic samples. Experiments show that ADVMEM improves the performance of several TTA methods on the ITD benchmark.

**Soundness**
The overall logic of the paper is sound. The motivation for a tracklet-based benchmark is practical and well-founded. The experimental protocol, which progressively increases difficulty from frame-wise i.i.d. to tracklet-wise non-i.i.d., is a good way to dissect model performance. The ADVMEM method seems like a practical solution to the identified problem, as it does not require access to the original training data, which is often a constraint in real-world deployment.

However, the paper could be strengthened by providing more practical details. The ADVMEM initialization process (Algorithm 1) involves an iterative optimization to generate each sample. This seems computationally expensive. The paper should include an analysis of this cost (e.g., wall-clock time, number of gradient steps per sample) to assess its practicality. Is this a one-time, offline cost for a given pre-trained model, or does it need to be run frequently?

Furthermore, the comparison with the "TrainMem" baseline in the appendix is concerning. Appendix Table 5 shows TrainMem achieving a 75.5% error rate for RoTTA, while ADVMEM achieves 78.2%. This suggests that if one *does* have access to the training data, using it directly is better. The paper should discuss this trade-off more explicitly: ADVMEM is valuable when training data is unavailable, but may be suboptimal otherwise. The claim that TrainMem "significantly underperforms" ADVMEM appears to be incorrect based on this table.

**Presentation**
The paper is generally well-written, and the high-level concepts are communicated effectively. Figures 1, 2, and 3 are helpful for understanding the benchmark's design. The structure of the paper flows logically from motivation to benchmark creation, evaluation, and the proposed solution.

The biggest issue is the disorganization of the results. As other reviewers will likely note, the main text (Section 5) refers to tables that do not contain the claimed results. This forces the reader to hunt through the appendix, which is inconvenient and confusing. For example, the claim that ADVMEM reduces SHOT-IM's error by 44% in the abstract is very impressive, but finding the corresponding numbers (presumably in Appendix Table 4a, comparing 91.0% to 40.1% for SHOT-IM) requires significant effort from the reader. This must be fixed.

**Contribution**
The paper's primary contribution is the ITD benchmark, which is a welcome addition to the field. It provides a more realistic testbed for TTA methods and will likely spur further research into handling temporal dependencies. The second contribution, ADVMEM, is a practical tool for improving memory-based TTA, especially in scenarios where the original training data is inaccessible due to privacy or storage constraints. Its "plug-and-play" design is appealing from an engineering perspective. While its absolute performance might be surpassed by methods with access to training data (like TrainMem), its data-free nature is a key advantage.

**Strengths**
- **Practical Motivation:** The work is grounded in a real-world problem: the failure of models on continuous data streams like video.
- **Realistic Benchmark:** ITD is a well-designed and much-needed benchmark that captures temporal aspects of data.
- **Data-Free Solution:** ADVMEM does not require access to the original training set, which is a significant practical advantage for deployment.
- **Plug-and-Play:** The modular nature of ADVMEM makes it easy to integrate with existing memory-based methods, lowering the barrier to adoption.

**Weaknesses**
- **Lack of Practical Analysis:** The paper lacks discussion on the computational overhead and runtime of the ADVMEM initialization procedure.
- **Confusing Results Presentation:** The incorrect table references in the main text are a major flaw that needs to be addressed.
- **Potentially Misleading Claims:** The claim of ADVMEM's superiority over the TrainMem baseline is contradicted by the data in Appendix Table 5, which requires clarification or correction.

**Questions**
1.  What is the computational cost of the ADVMEM initialization? How long does it take to populate the memory bank for the ResNet-18 model used in the experiments?
2.  The results in Appendix Table 5 suggest that initializing with real training data (TrainMem) is more effective for RoTTA than ADVMEM. Could you comment on this? Does this imply that ADVMEM's primary advantage is its data-free nature, rather than absolute performance?
3.  In Section 4.6, you equip TENT and SHOT-IM with RoTTA's memory bank. Was this a straightforward integration? The catastrophic failure suggests there might be subtleties in how the memory bank should interact with different adaptation objectives. Could you elaborate on the implementation?

**Rating**
- Overall (10): 7 — A strong paper with a valuable benchmark and a practical method, held back by significant presentation flaws and a key contradictory result.
- Novelty (10): 8 — The benchmark is highly novel, and the data-free adversarial initialization is a creative approach.
- Technical Quality (10): 6 — The experiments are mostly solid, but the contradictory result with the TrainMem baseline and the lack of computational analysis are notable weaknesses.
- Clarity (10): 5 — While the prose is clear, the disorganized results section with incorrect table references severely impacts the overall clarity.
- Confidence (5): 4 — I am confident in my review, focusing on the practical aspects and the clarity of the evidence provided.

***

### **Review 4**

**Summary**
This paper introduces a new benchmark, Inherent Temporal Dependencies (ITD), for evaluating Test-Time Adaptation (TTA) methods. ITD is derived from an object tracking dataset to ensure that test data streams exhibit natural temporal correlations, a property absent in standard TTA benchmarks. The authors demonstrate that existing methods struggle on ITD, and they identify a particular vulnerability in memory-based methods related to empty memory initialization. They propose ADVMEM, a method to initialize the memory bank with adversarially generated synthetic samples to provide class-balanced priors. The paper claims that ADVMEM significantly improves the stability and performance of memory-based TTA methods in challenging non-i.i.d. scenarios.

**Soundness**
The methodological soundness of this paper has some serious issues.

The core claim revolves around the effectiveness of ADVMEM. The authors propose generating samples via `x* = argmin_x L_ce(f_θ(x), y)` starting from noise. This is a standard technique for feature visualization or generating "fooling" images. The novelty here is using them to initialize a memory buffer. The critical comparison should be against reasonable baselines. The paper provides one such baseline in the appendix: TrainMem, which uses samples from the training set. The data in Appendix Table 5 shows RoTTA+TrainMem (75.5% error) outperforming RoTTA+ADVMEM (78.2% error). This is a fatal flaw. The central technical contribution of the paper is shown to be inferior to a simple, intuitive baseline. The authors' textual claim that TrainMem "significantly underperforms" ADVMEM is factually incorrect based on their own table and must be addressed.

Furthermore, the analysis of why memory-augmented TENT and SHOT-IM fail so dramatically (Table 6) is superficial. RoTTA uses the memory bank for knowledge distillation from a teacher model, which helps prevent drift. TENT and SHOT-IM use entropy minimization and information maximization, respectively. When fed a stream of identical-label images from a tracklet, their batch statistics become extremely biased. A memory bank filled with diverse samples *should* theoretically help stabilize these statistics. The fact that it makes performance *worse* (e.g., 94.7% error for SHOT-IM without memory vs. 93.4% with memory is a negligible change, both are failing) suggests a fundamental incompatibility or a flawed implementation. A proper methodological contribution would require a deeper dive into why this is the case. Is the memory update rule, borrowed from RoTTA, simply wrong for these other objectives?

The construction of the ITD benchmark itself is a good idea. However, the experimental setup could be more rigorous. For example, the paper fine-tunes models on the ITD training set (Section 4.4). This is reasonable, but it complicates the "pre-trained model" narrative common in TTA. The standard TTA setup assumes a model pre-trained on a large source dataset (like ImageNet) which is then adapted to a target domain. Here, the "source" is a model already fine-tuned on the target dataset's training split. This is more of a continual learning setup on a held-out test set than a classic domain adaptation problem.

**Presentation**
The paper's presentation is poor. The most glaring issue is the systematic misuse of table references in the main text, particularly in Section 5, which makes the experimental results section nearly impossible to follow without extensive cross-referencing with the appendix. This suggests a lack of care in the preparation of the manuscript. Figures are generally good, but Figure 8, which visualizes the adversarial examples, is not very informative. Showing pure noise images is expected, but the paper could add a sentence to manage reader expectations, explaining that these are not meant to be visually interpretable but are effective in the feature space.

**Contribution**
The paper's intended contributions are significant, but the execution falls short.
1.  **ITD Benchmark:** This is the most solid contribution. The field needs benchmarks that model temporal dependencies, and ITD is a well-motivated step in that direction.
2.  **Comprehensive Evaluation:** The evaluation reveals interesting failure modes of existing methods, which is valuable. However, the analysis of these failures is not deep enough.
3.  **ADVMEM:** This contribution is severely weakened by the paper's own results. The method is more complex than a simple baseline (TrainMem) and, according to Appendix Table 5, less effective. The novelty is therefore questionable, as its utility is not clearly demonstrated over simpler alternatives.

**Strengths**
- The paper addresses a clear and important gap in TTA evaluation by proposing the ITD benchmark.
- The experimental design, with its three scenarios of increasing difficulty, is logical and allows for a nuanced analysis of model capabilities.
- The identification of memory initialization as a potential failure point is a good insight.

**Weaknesses**
- **The main technical contribution (ADVMEM) is undermined by a baseline comparison in the appendix (Table 5), where it performs worse than the baseline.**
- The analysis of why certain methods fail (e.g., TENT/SHOT-IM with memory) is shallow and inconclusive.
- The experimental setup blurs the line between test-time adaptation and continual learning by fine-tuning on an in-domain training set.
- The presentation is sloppy, with numerous incorrect table references that severely hinder readability and verification of the results.

**Questions**
1.  Please address the discrepancy in Appendix Table 5, where TrainMem appears to outperform ADVMEM for the RoTTA method, contradicting the text. Does this not invalidate the primary claim about ADVMEM's superiority?
2.  Why was the specific adversarial formulation `argmin L_ce` chosen for ADVMEM? Have you considered other adversarial generation techniques, or even generative models (e.g., a GAN or diffusion model trained on the source data) to create more "natural" looking initial samples?
3.  Could you provide a more detailed hypothesis for why adding a memory bank to TENT and SHOT-IM leads to such catastrophic failure? Does the memory update rule need to be co-designed with the adaptation objective, rather than simply borrowed from RoTTA?
4.  How do you position your work in relation to continual test-time adaptation (e.g., CoTTA)? Your tracklet-based non-i.i.d. setting seems very similar to the scenarios studied in that subfield.

**Rating**
- Overall (10): 4 — The benchmark idea is strong, but the paper is critically flawed by contradictory evidence for its main method and poor presentation.
- Novelty (10): 5 — The benchmark is novel, but the ADVMEM method's novelty is highly questionable given it is outperformed by a simple baseline.
- Technical Quality (10): 3 — The technical analysis is shallow, and the contradictory results in the appendix represent a major failure of scientific rigor.
- Clarity (10): 4 — The core ideas are understandable, but the execution of the writing, especially the incorrect references, makes the paper confusing and untrustworthy.
- Confidence (5): 5 — I am an expert in this area and am highly confident that the identified flaws are critical.