# Global Summary
This paper introduces a new benchmark for Test-Time Adaptation (TTA) called Inherent Temporal Dependencies (ITD), designed to be more realistic than existing benchmarks. The core problem identified is that current TTA evaluations often use i.i.d. image streams with random corruptions, failing to capture the temporal dependencies inherent in real-world data like video streams. The ITD benchmark is constructed from object tracklets (sequences of images of the same object) from the TrackingNet dataset, thus naturally incorporating temporal dependencies and non-i.i.d. data streams. The authors evaluate existing TTA methods on ITD and find that most, especially those without memory mechanisms, suffer severe performance degradation. To address the challenges observed even in memory-based methods, the paper proposes ADVMEM, an adversarial memory initialization strategy. ADVMEM populates the memory bank with synthetic, class-diverse samples at the start of adaptation to prevent catastrophic forgetting and stabilize performance in non-i.i.d. scenarios. Experiments show that ADVMEM significantly improves the performance of memory-based TTA methods on the ITD benchmark, with a key result being a 44% error rate reduction for SHOT-IM in a tracklet-wise i.i.d. setting.

# Introduction
- Deep neural networks suffer performance degradation from distribution shifts and corruptions in real-world scenarios.
- Test-Time Adaptation (TTA) aims to adapt pre-trained models on-the-fly to new data streams.
- Current TTA benchmarks are criticized for oversimplifying the problem by simulating only covariate shifts (e.g., CIFAR10-C, ImageNet-C) and neglecting temporal dependencies that violate the i.i.d. assumption.
- The paper introduces a new benchmark, Inherent Temporal Dependencies (ITD), built from object tracklets from the TrackingNet dataset to provide a more realistic evaluation setting with natural temporal dependencies.
- Evaluation on ITD reveals that existing TTA methods, particularly memory-bank-based ones, struggle due to the compounded effects of distribution shifts and temporal dependencies, often suffering from poor memory bank initialization.
- To solve this, the paper proposes ADVMEM, an adversarial memory initialization strategy that acts as a plug-and-play enhancement for memory-based TTA methods.
- A key claim is that equipping SHOT-IM with ADVMEM reduces its error rate by 44% in the Tracklet-wise i.i.d. setting.
- Contributions are summarized as: (1) The ITD benchmark, (2) A comprehensive evaluation of TTA methods on ITD, and (3) The ADVMEM method.

# Abstract
- The paper introduces a novel tracklet-based dataset, Inherent Temporal Dependencies (ITD), for benchmarking TTA methods to mimic real-world challenges like video streams.
- Current benchmarks are criticized for failing to represent temporal dependencies where consecutive frames often show the same object.
- ITD is built from tracklets (sequences of object-centric images) compiled from an object-tracking dataset.
- Experimental analysis on ITD shows limitations of current TTA methods when faced with temporal dependencies.
- The paper proposes a novel adversarial memory initialization strategy that substantially boosts the performance of memory-based TTA methods on the ITD benchmark.

# Related Work
- TTA methods adapt pre-trained models at test time using unlabeled data.
- Many methods focus on covariate shifts, using techniques like updating Batch Normalization statistics (e.g., AdaBN) or entropy minimization (e.g., TENT).
- EATA extends TENT by selecting reliable and non-redundant samples for updates.
- RoTTA is a more recent method that uses a memory bank to adapt to non-i.i.d. streams.

# Method
- The paper introduces the Inherent Temporal Dependencies (ITD) dataset and the ADVMEM initialization strategy.
- **ITD Dataset Construction**:
    - Built from the TrackingNet dataset, which is derived from YouTube Bounding-Boxes.
    - Tracklets are sequences of cropped bounding boxes of the same object across video frames.
    - Preprocessing steps: frames are selected at a 5-frame interval, crops are resized to be 10% larger than the bounding box, and all crops are standardized to 224x224.
    - ITD contains over 23K objects across 21 classes, totaling over 220K images.
    - The dataset is split into training (50%), validation (30%), and test (20%) sets.
- **ADVMEM: Adversarial Memory Initialization**:
    - Aims to populate a memory bank with class-representative synthetic samples to prevent forgetting and balance statistics.
    - The process involves initializing each memory entry as Gaussian noise, assigning it a random label `y`, and then using gradient descent to solve `x* = argmin_x L_ce(f_θ(x), y)`. This is a targeted adversarial attack to make the network classify the noise as the desired class.
    - This initialization is independent of the memory update mechanism and can be plugged into methods like RoTTA.
    - An alternative of using real training samples (TrainMem) is explored but found to be less effective than ADVMEM.

# Motivation
- The paper argues that recent TTA research has moved towards non-i.i.d. setups (e.g., RoTTA), but still uses artificially correlated labels rather than naturally occurring temporal dependencies.
- A motivating experiment on CIFAR-10-C shows that mimicking tracklets by duplicating images in a batch causes significant performance drops for TTA methods.
- For example, RoTTA's error rate increased from 27.6% (non-i.i.d.) to 66.8% (Tracklet Mimic), a 39.2% increase. CoTTA's error increased by 13.6%.
- This motivates the need for the ITD benchmark, which is built from real object tracklets, providing a natural source of visual non-i.i.d. data.

# Experiments
- **Evaluation Setups**:
    - Three scenarios of increasing complexity are used:
        1.  **Frame-wise i.i.d.**: Frames sampled independently, ignoring temporal links.
        2.  **Tracklet-wise i.i.d.**: Entire tracklets are sampled i.i.d., preserving intra-tracklet dependencies.
        3.  **Tracklet-wise non-i.i.d.**: Tracklets are sampled following a Dirichlet distribution `Dir(γ)` to create strong label correlations over time (γ → 0).
- **Experimental Details**:
    - Base model: ResNet-18 (ViT-B-16 also used in ablations).
    - Corruptions from ImageNet-C are applied at severity level 5.
    - Batch size is 64.
    - Baselines: Source, AdaBN, SHOT-IM, TENT, SAR, EATA, CoTTA, RoTTA.
    - Models are fine-tuned on the ITD training set. ResNet-18 achieves 9.4% test error; ViT achieves 4.0% test error.
- **Key Results**:
    - **Frame-wise i.i.d. (Table 5)**: Source model error is 62.4%. SHOT-IM performs best with 39.3% error. RoTTA performs poorly with 53.4% error.
    - **Tracklet-wise i.i.d. (Table 6)**: Methods without memory like TENT and SHOT-IM fail completely, with error rates of 94.0% and 94.7% respectively. RoTTA, with its memory bank, is much more stable with a 51.3% error rate.
    - **Tracklet-wise non-i.i.d. (Table 6, γ=10⁻⁴)**: This is the most challenging setting. RoTTA's performance degrades significantly to 79.3% error, a 28% drop compared to the i.i.d. tracklet case. This is attributed to empty memory initialization.
- **Performance with ADVMEM**:
    - **Tracklet-wise i.i.d.**: ADVMEM significantly improves performance. The text claims it reduces TENT's error by ~15% and SHOT-IM's by over 40%.
    - **Tracklet-wise non-i.i.d. (γ → 0)**: ADVMEM improves RoTTA's performance by an average of 4%, with specific improvements of 16% on pixelate and 10% on zoom corruptions. TENT improves by over 10% on JPEG corruption.
- **Ablation Studies**:
    - **Label Distribution (γ)**: ADVMEM is most effective in highly non-i.i.d. scenarios (low γ). Its benefit diminishes as the data stream becomes more i.i.d. (high γ).
    - **Batch Size**: Performance improves with larger batch sizes (tested with 8, 16, 32). ADVMEM provides consistent benefits across batch sizes, especially in non-i.i.d. settings.

# Conclusion
- The paper introduces ITD, a novel benchmark with inherent temporal dependencies that poses a significant challenge to existing TTA methods.
- It proposes ADVMEM, an adversarial memory initialization strategy that improves the adaptability of TTA methods by mitigating catastrophic forgetting, especially in non-i.i.d. scenarios.
- The work advocates for more realistic benchmarks and robust adaptation strategies to handle dynamic real-world data distributions.

# Appendix
- **ITD Dataset Details**: Provides a class distribution chart for the 21 classes in the test set. The distribution is non-uniform, with 'Person' being the most frequent class and 'Zebra' being one of the least frequent.
- **Dynamic Corruptions**: An experiment where corruption severity varies within a tracklet according to `S(t) = s · |sin(t)|`. Results in Appendix Table 4 show ADVMEM is also effective in this setting. For example, in the tracklet-wise i.i.d. dynamic setting, SHOT-IM with ADVMEM achieves 40.1% error vs. 91.0% without.
- **Vision Transformer (ViT) Experiments**: The main findings are replicated using a ViT-B-16 architecture, showing that ADVMEM's benefits are not specific to ResNet.
- **TrainMem Baseline**: A memory initialization strategy using real training samples is proposed (Algorithm 2) and evaluated. In the tracklet-wise non-i.i.d. setting for RoTTA (Appendix Table 5), TrainMem achieves a 78.2% error rate, which is better than no initialization (79.3%) but worse than ADVMEM (75.5%).
- **Visualizations**: Includes images of the adversarial noise samples used to initialize the memory bank for different classes.
- **Detailed Tables**: Provides full per-corruption results for all experimental settings discussed in the main paper.

# References
- This section lists the references cited in the manuscript. No summary is required.