{
  "baseline_review": "1) Summary\nThis paper introduces the Two-Way Garment Transfer Model (TWGTM), a unified diffusion-based framework for performing both virtual try-on (VTON) and its inverse, virtual try-off (VTOFF). The core idea is to treat these as complementary tasks solvable within a single model by reversing input feature concatenation. The method employs a dual-conditioned guidance mechanism, using features from both the latent space for structural consistency and the pixel space for semantic and spatial details. To address the asymmetry in mask availability between the mask-guided VTON and mask-free VTOFF tasks, a two-stage training strategy is proposed. The model's effectiveness is demonstrated through quantitative and qualitative experiments on the VITON-HD and DressCode datasets, where it shows competitive or state-of-the-art performance for both VTON and VTOFF tasks.2) Strengths\n*   **Novel and Well-Motivated Unified Framework**\n    *   The central contribution of unifying VTON and VTOFF into a single bidirectional framework is novel and compelling. The paper argues that these are dual objectives of a single deformation modeling problem (Section: Introduction).\n    *   The motivation for a unified model is empirically grounded. The analysis in Figure 1, showing significant parameter and feature similarity between separate VTON and VTOFF models, provides a strong rationale for the proposed approach.\n    *   The mechanism for switching between tasks by simply reversing the spatial order of concatenated inputs in the latent space is an elegant and efficient design choice (Section: Processing in Latent Space, Figure 2(a)). This highlights the inherent symmetry the authors aim to exploit.*   **Sophisticated and Coherent Model Architecture**\n    *   The proposed dual-space guidance mechanism is a technically sound approach to inject rich conditioning information. It combines latent-space concatenation with detailed pixel-space features (Section: Method).\n    *   The pixel-space feature extraction is well-designed, disentangling semantic and spatial information. The Semantic Abstraction Module (SAM) uses a frozen CLIP and QFormer for high-level understanding (Section: Semantic Abstraction Module), while the Spatial Refinement Module (SRM) uses a Swin Transformer and a novel \"TaskFormer\" for fine-grained details (Section: Spatial Refinement Module).\n    *   The Extended Attention Block (Figure 2(c)) provides a principled way to fuse the extracted spatial and semantic features into the U-Net's generative process, going beyond simple concatenation or standard cross-attention.*   **Extensive and Rigorous Experimental Validation**\n    *   The paper provides a thorough evaluation on two standard, high-resolution benchmarks (VITON-HD and DressCode), using a comprehensive set of metrics to assess different aspects of generation quality (Section: Datasets and Metrics).\n    *   The quantitative results are strong, demonstrating state-of-the-art performance on the VTOFF task (Table 3) and highly competitive performance on the VTON task against numerous recent baselines (Tables 1, 2).\n    *   The ablation study is particularly comprehensive and convincing (Table 4, Figures 5, 6). It systematically validates key design choices, such as the necessity of the SRM (Variant 1), the benefit of spatial concatenation over warping (Variant 2), and the effectiveness of the proposed feature fusion method (Variants 6, 7).\n    *   The analysis of joint training (Figures 7, 8, Variants 4, 5) provides direct evidence for the core hypothesis that VTON and VTOFF are complementary tasks that can mutually benefit from being trained together.3) Weaknesses\n*   **Lack of Clarity in Methodological Description**\n    *   The overall architecture diagram (Figure 2(b)) is highly complex, and the data flow is difficult to trace. For instance, it is not immediately clear how the outputs of the SAM and SRM are integrated into the main U-Net decoder at multiple levels.\n    *   The description of the \"TaskFormer\" module is dense and hard to fully grasp (Section: Spatial Refinement Module, Equations 5-11). The text mentions masked attention, hierarchical masks, and dual projection branches with query-specific processing, but these concepts are introduced rapidly without sufficient explanation or visual aid.\n    *   The two-stage training strategy lacks detail (Section: Training Strategy). The paper states that Stage 1 uses \"UNet's native cross-attention blocks\" while Stage 2 activates the \"Extended Attention Block.\" This implies a significant change in the model's architecture between stages, but it is unclear how this transition is managed (e.g., are weights from Stage 1 used to initialize Stage 2? Are some parts frozen?).*   **Inconsistent Quantitative Performance and Superficial Analysis**\n    *   For VTON on the VITON-HD dataset (Table 1), the proposed method is outperformed on the FID metric by two baselines (IDM-VTON and CatVTON). The text describes the performance as \"competitive\" but does not offer any analysis or hypothesis for why it might lag on this key perceptual metric while excelling on others like SSIM.\n    *   The explanation for weaker performance on the DressCode dataset (Table 2) is attributed to \"inherent color and texture inconsistencies\" in the dataset (Section: Quantitative Comparison). This is a speculative claim without supporting evidence, such as qualitative examples of these inconsistencies and their effect on the model's output.\n    *   The VTOFF comparison (Table 3) is conducted against only two other methods, both of which are very recent preprints (Velioglu et al. 2024; Xarchakos and Koukopoulos 2025). While understandable for a new task, this narrow comparison limits the conclusiveness of the state-of-the-art claim.*   **Omission of Limitations and Practical Considerations**\n    *   The paper does not include a discussion of limitations or an analysis of failure cases. The qualitative results (Figures 3, 4) exclusively showcase successful examples. It would be beneficial to understand scenarios where the model struggles, such as with highly complex garment textures, severe occlusions, or unusual human poses.\n    *   The claim of \"mask-free VTOFF\" could be more nuanced. The model internally predicts a mask for the garment in VTOFF using TaskFormer (Equation 10) and uses this for training in Stage 1. While no user-provided mask is needed at inference, the model is not entirely independent of mask-like spatial guidance.\n    *   Practical performance metrics such as computational cost (e.g., model parameters, training time, inference latency) are not reported. Given the complexity of the architecture involving multiple transformers (Swin, QFormer, TaskFormer) and a large diffusion model, this information is crucial for assessing the method's practicality.4) Suggestions for Improvement\n*   **Enhance Methodological Clarity**\n    *   Please consider revising Figure 2(b) to improve its readability. This could involve using color-coding, numbered steps, or a simplified block diagram that clearly illustrates the flow of features from the SAM and SRM into the different layers of the U-Net decoder.\n    *   Provide a more detailed explanation of the TaskFormer module. An additional diagram illustrating its internal components (e.g., the cascaded blocks and dual projection branches) or a more step-by-step description of its operation would greatly aid reader understanding and reproducibility.\n    *   Elaborate on the two-stage training process. Please clarify if Stage 2 is a fine-tuning of the model from Stage 1. Specify which weights are carried over, which are re-initialized or unfrozen, and how the architectural shift from native cross-attention to the Extended Attention Block is implemented.*   **Strengthen Analysis of Quantitative Results**\n    *   In the discussion of Table 1, please provide a more balanced analysis. Acknowledge the lower FID score and offer a potential explanation. For example, discuss if there is an inherent trade-off in the model's design that prioritizes structural fidelity (higher SSIM) over distributional realism (FID).\n    *   To substantiate the claim about dataset issues in DressCode (Table 2), please include a few examples in the appendix that visualize the \"color and texture inconsistencies\" and demonstrate how they pose a challenge for the model.\n    *   When presenting the VTOFF results (Table 3), please explicitly acknowledge the limited number of available baselines and frame the contribution as a strong new benchmark in an emerging research area.*   **Address Limitations and Practicality**\n    *   Please add a \"Limitations\" section or paragraph, ideally in the conclusion or as a separate section. Including a few qualitative examples of failure cases (e.g., for challenging poses or garment types) would provide a more complete and honest assessment of the method's capabilities.\n    *   Refine the phrasing around \"mask-free VTOFF.\" We suggest clarifying that the method is \"user-mask-free\" at inference time but internally relies on a predicted spatial guidance map, and explain why this is a practical advantage.\n    *   Please report key computational statistics. A table in the appendix detailing the number of trainable parameters, the total training time on the hardware used, and the average inference time per image pair would significantly increase the practical value of the paper.5) Score\n*   Overall (10): 7 — The paper presents a novel and well-motivated unified framework with strong empirical results, but its clarity is hindered by a dense method description and a superficial analysis of some quantitative outcomes.\n*   Novelty (10): 8 — The core idea of a unified, bidirectional framework for VTON and VTOFF is highly novel and elegantly implemented (Figure 2(a), Section: Introduction).\n*   Technical Quality (10): 7 — The model architecture is sophisticated and the experimental validation, particularly the ablation studies (Table 4), is rigorous; however, the lack of clarity in the training strategy and key modules slightly weakens the technical presentation.\n*   Clarity (10): 6 — While the high-level motivation is clear, the detailed methodology is difficult to follow due to a complex main diagram (Figure 2(b)) and condensed descriptions of crucial components like the TaskFormer.\n*   Confidence (5): 4 — I am confident in my assessment, having experience with diffusion models and generative image synthesis, though I am not an expert specifically in virtual try-on.",
  "final_review": "1) Summary\nThis paper introduces the Two-Way Garment Transfer Model (TWGTM), a unified diffusion-based framework for performing both virtual try-on (VTON) and its inverse, virtual try-off (VTOFF). The core idea is to treat these as complementary tasks solvable within a single model by reversing input feature concatenation. The method employs a dual-conditioned guidance mechanism, using features from both the latent space for structural consistency and the pixel space for semantic and spatial details. To address the asymmetry in mask availability between the mask-guided VTON and mask-free VTOFF tasks, a two-stage training strategy is proposed. The model's effectiveness is demonstrated through quantitative and qualitative experiments on the VITON-HD and DressCode datasets, where it shows competitive or state-of-the-art performance for both VTON and VTOFF tasks.2) Strengths\n*   **Novel and Well-Motivated Unified Framework**\n    *   The central contribution of unifying VTON and VTOFF into a single bidirectional framework is novel and compelling. The paper argues that these are dual objectives of a single deformation modeling problem (Section: Introduction).\n    *   The motivation for a unified model is empirically grounded. The analysis in Figure 1, showing significant parameter and feature similarity between separate VTON and VTOFF models, provides a strong rationale for the proposed approach.\n    *   The mechanism for switching between tasks by simply reversing the spatial order of concatenated inputs in the latent space is an elegant and efficient design choice (Section: Processing in Latent Space). This highlights the inherent symmetry the authors aim to exploit.*   **Sophisticated and Coherent Model Architecture**\n    *   The proposed dual-space guidance mechanism is a technically sound approach to inject rich conditioning information. It combines latent-space concatenation with detailed pixel-space features (Section: Method, Figure 2(b)).\n    *   The pixel-space feature extraction is well-designed, disentangling semantic and spatial information. The Semantic Abstraction Module (SAM) uses a frozen CLIP and QFormer for high-level understanding (Section: Semantic Abstraction Module), while the Spatial Refinement Module (SRM) uses a Swin Transformer and a novel \"TaskFormer\" for fine-grained details (Section: Spatial Refinement Module).\n    *   The Extended Attention Block (Figure 2(c)) provides a principled way to fuse the extracted spatial and semantic features into the U-Net's generative process, going beyond simple concatenation or standard cross-attention.*   **Extensive Experimental Validation**\n    *   The paper provides a thorough evaluation on two standard, high-resolution benchmarks (VITON-HD and DressCode), using a comprehensive set of metrics to assess different aspects of generation quality (Section: Datasets and Metrics).\n    *   The quantitative results are strong, demonstrating state-of-the-art performance on the VTOFF task (Table 3) and highly competitive performance on the VTON task against numerous recent baselines (Tables 1, 2).\n    *   The ablation study is particularly comprehensive (Table 4, Figures 5, 6). It systematically validates key design choices, such as the necessity of the SRM (Variant 1), the benefit of spatial concatenation over warping (Variant 2), and the effectiveness of the proposed feature fusion method (Variants 6, 7).3) Weaknesses\n*   **Lack of Clarity in Methodological Description**\n    *   The overall architecture diagram (Figure 2(b)) is highly complex, and the data flow is difficult to trace. For instance, it is not immediately clear how the outputs of the SAM and SRM are integrated into the main U-Net decoder at multiple levels.\n    *   The description of the \"TaskFormer\" module is dense and hard to fully grasp (Section: Spatial Refinement Module, Equations 5-11). The text mentions masked attention, hierarchical masks, and dual projection branches with query-specific processing, but these concepts are introduced rapidly without sufficient explanation or visual aid.\n    *   The two-stage training strategy lacks detail (Section: Training Strategy). The paper states that Stage 1 uses \"UNet's native cross-attention blocks\" while Stage 2 activates the \"Extended Attention Block.\" This implies a significant change in the model's architecture between stages, but it is unclear how this transition is managed (e.g., are weights from Stage 1 used to initialize Stage 2?).\n    *   A key explanatory figure is missing. The \"Processing in Latent Space\" section and the caption for Figure 2 both refer to a \"Figure 2(a)\" to illustrate the feature concatenation process, but this sub-figure is not present in the manuscript, hindering a full understanding of the core task-switching mechanism.*   **Contradictory or Poorly Supported Quantitative Analysis**\n    *   For VTON on the VITON-HD dataset (Table 1), the proposed method is outperformed on the FID metric by two baselines (IDM-VTON and CatVTON). The text describes the performance as \"competitive\" but does not offer any analysis for why it might lag on this key perceptual metric.\n    *   The explanation for weaker performance on the DressCode dataset (Table 2) is attributed to \"inherent color and texture inconsistencies\" in the dataset (Section: Quantitative Comparison). This is a speculative claim without supporting evidence.\n    *   A core component of the proposed model is significantly outperformed by a simpler variant in the ablation study. For the VTOFF task, \"Variant 3 (w/ Mask2BBox)\" achieves substantially better scores across all metrics compared to the main \"TWGTM (Ours)\" model (Table 4). The paper notes this but fails to justify why the inferior model is presented as the main contribution.\n    *   The claim of mutual performance enhancement between VTON and VTOFF is not well-supported by the provided evidence. The text claims Figures 7 and 8 \"substantiate our hypothesis,\" but the plots show marginal, fluctuating, or even negative effects (e.g., DISTS for VTOFF worsens in Figure 7), which does not constitute strong evidence for mutual enhancement.*   **Omissions and Reporting Inaccuracies**\n    *   The paper does not include a discussion of limitations or an analysis of failure cases. The qualitative results (Figures 3, 4) exclusively showcase successful examples.\n    *   The claim of \"mask-free VTOFF\" could be more nuanced. The model internally predicts a mask for the garment in VTOFF using TaskFormer (Equation 10) and uses this for training in Stage 1. While no user-provided mask is needed at inference, the model is not entirely independent of mask-like spatial guidance.\n    *   Practical performance metrics such as computational cost (e.g., model parameters, training time, inference latency) are not reported.\n    *   There are several reporting inaccuracies. The caption for Figure 4 incorrectly labels the qualitative results as \"VTON\" when the content clearly shows the VTOFF task. In Table 2, the LPIPS score for \"Dresses\" is bolded for \"Ours,\" incorrectly suggesting it is the best result when it is tied for the worst. The text also claims DISTS is the \"primary metric for VTOFF\" without justification (Section: Quantitative Comparison).4) Suggestions for Improvement\n*   **Enhance Methodological Clarity**\n    *   Please consider revising Figure 2(b) to improve its readability. This could involve using color-coding, numbered steps, or a simplified block diagram.\n    *   Provide a more detailed explanation of the TaskFormer module. An additional diagram illustrating its internal components or a more step-by-step description would greatly aid reader understanding.\n    *   Elaborate on the two-stage training process. Please clarify if Stage 2 is a fine-tuning of the model from Stage 1 and specify which weights are carried over.\n    *   Please provide the missing Figure 2(a) that is referenced in the text to illustrate the latent space processing, as this is crucial for understanding the method's core mechanism.*   **Strengthen Quantitative Analysis and Justify Claims**\n    *   In the discussion of Table 1, please provide a more balanced analysis that acknowledges the lower FID score and offers a potential explanation.\n    *   To substantiate the claim about dataset issues in DressCode (Table 2), please include a few examples in the appendix that visualize the alleged \"color and texture inconsistencies.\"\n    *   Please address the significant contradiction in the ablation study (Table 4). Justify why TWGTM is the preferred model for VTOFF despite being clearly outperformed by Variant 3, or consider reframing Variant 3 as the recommended approach for that task.\n    *   Please temper the claims regarding mutual enhancement from joint training. The analysis should more accurately reflect the mixed results shown in Figures 7 and 8, or stronger evidence should be provided.*   **Address Omissions and Correct Reporting**\n    *   Please add a \"Limitations\" section or paragraph, ideally including a few qualitative examples of failure cases to provide a more complete assessment.\n    *   Refine the phrasing around \"mask-free VTOFF.\" We suggest clarifying that the method is \"user-mask-free\" at inference time but internally relies on a predicted spatial guidance map.\n    *   Please report key computational statistics, such as the number of parameters, training time, and inference latency, in the main text or an appendix.\n    *   Please correct the identified reporting errors: update the caption for Figure 4 to refer to VTOFF, fix the incorrect bolding in Table 2, and either provide a citation/justification for DISTS as the \"primary metric\" for VTOFF or remove this assertion.5) Score\n*   Overall (10): 6 — The paper proposes an interesting unified framework, but the contribution is undermined by significant clarity issues, a major contradiction in the ablation results, and several reporting inaccuracies.\n*   Novelty (10): 8 — The core idea of a unified, bidirectional framework for VTON and VTOFF is presented as a novel contribution with an elegant implementation concept (Section: Introduction, Figure 1).\n*   Technical Quality (10): 5 — The technical contribution is weakened by the fact that a simpler variant significantly outperforms the proposed model on a key task (Table 4) and by weakly supported claims about joint training benefits (Figures 7, 8).\n*   Clarity (10): 4 — The methodology is difficult to follow due to a complex diagram (Figure 2(b)), dense descriptions, and a missing key illustration (Figure 2(a)), compounded by an incorrect figure caption (Figure 4).\n*   Confidence (5): 4 — I am confident in my assessment, having experience with diffusion models and generative image synthesis, though I am not an expert specifically in virtual try-on.",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 7,
        "novelty": 8,
        "technical_quality": 7,
        "clarity": 6,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 6,
        "novelty": 8,
        "technical_quality": 5,
        "clarity": 4,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "1) Summary\nThis paper introduces the Two-Way Garment Transfer Model (TWGTM), a unified diffusion-based framework for performing both virtual try-on (VTON) and its inverse, virtual try-off (VTOFF). The core idea is to treat these as complementary tasks solvable within a single model by reversing input feature concatenation. The method employs a dual-conditioned guidance mechanism, using features from both the latent space for structural consistency and the pixel space for semantic and spatial details. To address the asymmetry in mask availability between the mask-guided VTON and mask-free VTOFF tasks, a two-stage training strategy is proposed. The model's effectiveness is demonstrated through quantitative and qualitative experiments on the VITON-HD and DressCode datasets, where it shows competitive or state-of-the-art performance for both VTON and VTOFF tasks.2) Strengths\n*   **Novel and Well-Motivated Unified Framework**\n    *   The central contribution of unifying VTON and VTOFF into a single bidirectional framework is novel and compelling. The paper argues that these are dual objectives of a single deformation modeling problem (Section: Introduction).\n    *   The motivation for a unified model is empirically grounded. The analysis in Figure 1, showing significant parameter and feature similarity between separate VTON and VTOFF models, provides a strong rationale for the proposed approach.\n    *   The mechanism for switching between tasks by simply reversing the spatial order of concatenated inputs in the latent space is an elegant and efficient design choice (Section: Processing in Latent Space). This highlights the inherent symmetry the authors aim to exploit.*   **Sophisticated and Coherent Model Architecture**\n    *   The proposed dual-space guidance mechanism is a technically sound approach to inject rich conditioning information. It combines latent-space concatenation with detailed pixel-space features (Section: Method, Figure 2(b)).\n    *   The pixel-space feature extraction is well-designed, disentangling semantic and spatial information. The Semantic Abstraction Module (SAM) uses a frozen CLIP and QFormer for high-level understanding (Section: Semantic Abstraction Module), while the Spatial Refinement Module (SRM) uses a Swin Transformer and a novel \"TaskFormer\" for fine-grained details (Section: Spatial Refinement Module).\n    *   The Extended Attention Block (Figure 2(c)) provides a principled way to fuse the extracted spatial and semantic features into the U-Net's generative process, going beyond simple concatenation or standard cross-attention.*   **Extensive Experimental Validation**\n    *   The paper provides a thorough evaluation on two standard, high-resolution benchmarks (VITON-HD and DressCode), using a comprehensive set of metrics to assess different aspects of generation quality (Section: Datasets and Metrics).\n    *   The quantitative results are strong, demonstrating state-of-the-art performance on the VTOFF task (Table 3) and highly competitive performance on the VTON task against numerous recent baselines (Tables 1, 2).\n    *   The ablation study is particularly comprehensive (Table 4, Figures 5, 6). It systematically validates key design choices, such as the necessity of the SRM (Variant 1), the benefit of spatial concatenation over warping (Variant 2), and the effectiveness of the proposed feature fusion method (Variants 6, 7).3) Weaknesses\n*   **Lack of Clarity in Methodological Description**\n    *   The overall architecture diagram (Figure 2(b)) is highly complex, and the data flow is difficult to trace. For instance, it is not immediately clear how the outputs of the SAM and SRM are integrated into the main U-Net decoder at multiple levels.\n    *   The description of the \"TaskFormer\" module is dense and hard to fully grasp (Section: Spatial Refinement Module, Equations 5-11). The text mentions masked attention, hierarchical masks, and dual projection branches with query-specific processing, but these concepts are introduced rapidly without sufficient explanation or visual aid.\n    *   The two-stage training strategy lacks detail (Section: Training Strategy). The paper states that Stage 1 uses \"UNet's native cross-attention blocks\" while Stage 2 activates the \"Extended Attention Block.\" This implies a significant change in the model's architecture between stages, but it is unclear how this transition is managed (e.g., are weights from Stage 1 used to initialize Stage 2?).\n    *   A key explanatory figure is missing. The \"Processing in Latent Space\" section and the caption for Figure 2 both refer to a \"Figure 2(a)\" to illustrate the feature concatenation process, but this sub-figure is not present in the manuscript, hindering a full understanding of the core task-switching mechanism.*   **Contradictory or Poorly Supported Quantitative Analysis**\n    *   For VTON on the VITON-HD dataset (Table 1), the proposed method is outperformed on the FID metric by two baselines (IDM-VTON and CatVTON). The text describes the performance as \"competitive\" but does not offer any analysis for why it might lag on this key perceptual metric.\n    *   The explanation for weaker performance on the DressCode dataset (Table 2) is attributed to \"inherent color and texture inconsistencies\" in the dataset (Section: Quantitative Comparison). This is a speculative claim without supporting evidence.\n    *   A core component of the proposed model is significantly outperformed by a simpler variant in the ablation study. For the VTOFF task, \"Variant 3 (w/ Mask2BBox)\" achieves substantially better scores across all metrics compared to the main \"TWGTM (Ours)\" model (Table 4). The paper notes this but fails to justify why the inferior model is presented as the main contribution.\n    *   The claim of mutual performance enhancement between VTON and VTOFF is not well-supported by the provided evidence. The text claims Figures 7 and 8 \"substantiate our hypothesis,\" but the plots show marginal, fluctuating, or even negative effects (e.g., DISTS for VTOFF worsens in Figure 7), which does not constitute strong evidence for mutual enhancement.*   **Omissions and Reporting Inaccuracies**\n    *   The paper does not include a discussion of limitations or an analysis of failure cases. The qualitative results (Figures 3, 4) exclusively showcase successful examples.\n    *   The claim of \"mask-free VTOFF\" could be more nuanced. The model internally predicts a mask for the garment in VTOFF using TaskFormer (Equation 10) and uses this for training in Stage 1. While no user-provided mask is needed at inference, the model is not entirely independent of mask-like spatial guidance.\n    *   Practical performance metrics such as computational cost (e.g., model parameters, training time, inference latency) are not reported.\n    *   There are several reporting inaccuracies. The caption for Figure 4 incorrectly labels the qualitative results as \"VTON\" when the content clearly shows the VTOFF task. In Table 2, the LPIPS score for \"Dresses\" is bolded for \"Ours,\" incorrectly suggesting it is the best result when it is tied for the worst. The text also claims DISTS is the \"primary metric for VTOFF\" without justification (Section: Quantitative Comparison).4) Suggestions for Improvement\n*   **Enhance Methodological Clarity**\n    *   Please consider revising Figure 2(b) to improve its readability. This could involve using color-coding, numbered steps, or a simplified block diagram.\n    *   Provide a more detailed explanation of the TaskFormer module. An additional diagram illustrating its internal components or a more step-by-step description would greatly aid reader understanding.\n    *   Elaborate on the two-stage training process. Please clarify if Stage 2 is a fine-tuning of the model from Stage 1 and specify which weights are carried over.\n    *   Please provide the missing Figure 2(a) that is referenced in the text to illustrate the latent space processing, as this is crucial for understanding the method's core mechanism.*   **Strengthen Quantitative Analysis and Justify Claims**\n    *   In the discussion of Table 1, please provide a more balanced analysis that acknowledges the lower FID score and offers a potential explanation.\n    *   To substantiate the claim about dataset issues in DressCode (Table 2), please include a few examples in the appendix that visualize the alleged \"color and texture inconsistencies.\"\n    *   Please address the significant contradiction in the ablation study (Table 4). Justify why TWGTM is the preferred model for VTOFF despite being clearly outperformed by Variant 3, or consider reframing Variant 3 as the recommended approach for that task.\n    *   Please temper the claims regarding mutual enhancement from joint training. The analysis should more accurately reflect the mixed results shown in Figures 7 and 8, or stronger evidence should be provided.*   **Address Omissions and Correct Reporting**\n    *   Please add a \"Limitations\" section or paragraph, ideally including a few qualitative examples of failure cases to provide a more complete assessment.\n    *   Refine the phrasing around \"mask-free VTOFF.\" We suggest clarifying that the method is \"user-mask-free\" at inference time but internally relies on a predicted spatial guidance map.\n    *   Please report key computational statistics, such as the number of parameters, training time, and inference latency, in the main text or an appendix.\n    *   Please correct the identified reporting errors: update the caption for Figure 4 to refer to VTOFF, fix the incorrect bolding in Table 2, and either provide a citation/justification for DISTS as the \"primary metric\" for VTOFF or remove this assertion.5) Score\n*   Overall (10): 6 — The paper proposes an interesting unified framework, but the contribution is undermined by significant clarity issues, a major contradiction in the ablation results, and several reporting inaccuracies.\n*   Novelty (10): 8 — The core idea of a unified, bidirectional framework for VTON and VTOFF is presented as a novel contribution with an elegant implementation concept (Section: Introduction, Figure 1).\n*   Technical Quality (10): 5 — The technical contribution is weakened by the fact that a simpler variant significantly outperforms the proposed model on a key task (Table 4) and by weakly supported claims about joint training benefits (Figures 7, 8).\n*   Clarity (10): 4 — The methodology is difficult to follow due to a complex diagram (Figure 2(b)), dense descriptions, and a missing key illustration (Figure 2(a)), compounded by an incorrect figure caption (Figure 4).\n*   Confidence (5): 4 — I am confident in my assessment, having experience with diffusion models and generative image synthesis, though I am not an expert specifically in virtual try-on."
}