# Global Summary
This paper introduces the Two-Way Garment Transfer Model (TWGTM), a unified diffusion-based framework for both virtual try-on (VTON) and its inverse, virtual try-off (VTOFF). The core problem addressed is that these complementary tasks are typically handled by separate, specialized models, despite their inherent symmetry as forward and inverse deformation problems. TWGTM unifies them by leveraging dual-conditioned guidance from both latent space (via spatial concatenation for structural integrity) and pixel space (via semantic and spatial feature modules for detail). A key challenge is the mask asymmetry between mask-guided VTON and mask-free VTOFF, which the authors address with a two-stage training strategy. In Stage 1, the model learns inpainting and predicts a garment mask for VTOFF. In Stage 2, it is challenged with generic square masks to improve shape reasoning, and all feature fusion modules are activated. The model is evaluated on the VITON-HD and DressCode datasets. For VTOFF on VITON-HD, it achieves state-of-the-art performance on most metrics, including a DISTS score of 0.195. For VTON, it shows competitive performance, achieving an LPIPS of 0.055 and SSIM of 0.905 on VITON-HD. Ablation studies validate the design choices, and experiments suggest that joint training allows the two tasks to mutually enhance each other's performance.

# Abstract
The paper addresses the gap in research where virtual try-on (VTON) and virtual try-off (VTOFF) are treated as isolated tasks. It proposes the Two-Way Garment Transfer Model (TWGTM), described as the first unified framework for both dressing (VTON) and undressing (VTOFF) synthesis. The model uses a bidirectional feature disentanglement approach with dual-conditioned guidance from latent and pixel spaces. To handle the asymmetry where VTON is mask-guided and VTOFF is mask-free, a phased training paradigm is introduced. The method's efficacy is validated through extensive experiments on the DressCode and VITON-HD datasets, where it shows a competitive edge.

# Introduction
- VTON and VTOFF are complementary tasks in fashion computer vision, but are typically researched in isolation.
- Traditional VTON methods used a two-stage warping and synthesis pipeline (often with GANs), which suffered from error propagation and distortion.
- Diffusion models have improved VTON, leading to two main approaches: warping-enhanced diffusion models and warping-free models that learn transformations implicitly.
- VTOFF is an emerging field, with current methods also using diffusion models to learn inverse deformations.
- The paper's key insight is that VTON and VTOFF are dual objectives of a single deformation modeling problem (forward vs. inverse deformation).
- An analysis of existing models, CatVTON (VTON) and TryoffAnyone (VTOFF), reveals significant parameter similarity (6.4% at 0.0005 relative error threshold, 65.6% at 0.05) and high cosine similarity of intermediate features, suggesting a unified model is feasible.
- The proposed TWGTM uses a dual-space guidance mechanism: latent-space concatenation for structure and pixel-space features for detail. Pixel-space features are disentangled into a semantic abstraction module and a spatial refinement module, fused via an extended attention mechanism.
- To address the mask dependency difference, a phased training protocol is used. Stage 1 co-optimizes feature extraction and a mask predictor for VTOFF. Stage 2 enables cross-task knowledge transfer with task-specific attention gating.
- Main contributions are: (1) the first unified diffusion framework for bidirectional garment manipulation, (2) a dual-phase training strategy to handle mask discrepancy, and (3) experimental validation of the joint modeling approach.

# Related Work
- **Virtual Try-On (VTON):**
    - Early methods were two-stage (geometric transformation + GAN synthesis), using techniques like Thin Plate Spline (TPS) warping or flow estimation. These were prone to artifacts and error propagation.
    - Diffusion models led to two new research lines:
        1.  Warping-enhanced frameworks that use diffusion to refine initial warping predictions.
        2.  Warping-free frameworks that implicitly learn spatial transformations via feature extraction and attention mechanisms.
    - Recent models like MMTryOn reduce reliance on auxiliary inputs by using multi-modal conditioning.
- **Virtual Try-Off (VTOFF):**
    - VTOFF is a newer research area focused on reconstructing canonical garments from images of dressed people.
    - Two pioneering works are mentioned:
        1.  TryOffDiff: Uses a SigLIP-conditioned latent diffusion model but has limitations in reconstructing fine details and color fidelity.
        2.  TryOffAnyone: A computationally efficient Stable Diffusion variant that can suffer from spatial inaccuracies like over-inference or distortions.

# Method
The method is built upon a Latent Diffusion Model (LDM) and introduces several new components for unified VTON/VTOFF.

- **Processing in Latent Space:**
    - The model input is a 9-channel tensor created by concatenating latent representations. For VTON, this is `[noisy_target(4), guidance(4), mask(1)]`, where the target is `[person_image, garment_image]` and guidance is `[masked_person, garment_image]`.
    - For VTOFF, the spatial order of the person and garment images is reversed in the concatenated inputs.
    - During inference, the target channels are replaced with pure noise. For VTOFF, the mask input is an all-zeros mask.

- **Processing in Pixel Space:**
    - **Semantic Abstraction Module (SAM):** Uses a frozen CLIP image encoder and a QFormer. The QFormer filters CLIP features using text prompts (e.g., "upper garment") to extract context-aware semantics.
    - **Spatial Refinement Module (SRM):**
        - A Swin Transformer extracts multi-scale features from the reference image.
        - A `TaskFormer` module, with a transformer-decoder-like architecture, processes these spatial features alongside semantic features from the QFormer.
        - `TaskFormer` has dual projection branches. One query in the task-space projection explicitly predicts a flattened garment mask for VTOFF.
        - A lightweight Decoder makes the output features compatible with the U-Net.
    - **Extended Attention Block:**
        - This module integrates the spatial features (`F_SRM`) and semantic features (`F_SAM`) into the U-Net's attention layers.
        - It combines standard self-attention with two cross-attention operations: one with `F_SRM` (modulated by a Zero Linear Layer) and another with `F_SAM`.

- **Training Strategy:**
    - A two-stage training strategy is employed to handle the different difficulties of VTON and VTOFF.
    - **Stage 1:** Focuses on inpainting. The model is trained to predict a flattened garment mask for VTOFF, and an auxiliary mask loss (`L_mask = λ'L_dice + λ''L_bce`) is added to the diffusion loss. Reference features are injected only via standard cross-attention.
    - **Stage 2:** Focuses on shape awareness. VTOFF uses morphologically generated square inpainting masks. The Extended Attention Block is activated to fuse spatial features from the SRM, reinforcing shape constraints. The loss is the standard diffusion objective.

# Preliminaries
- The framework is based on the Latent Diffusion Model (LDM).
- Core components are:
    1.  A CLIP text encoder for conditioning.
    2.  A Variational Autoencoder (VAE) to encode images into a latent space (typically with a downsampling factor of 8, e.g., HxW -> H/8 x W/8) and decode them back.
    3.  A time-conditional U-Net that predicts noise in the latent space.
- The model is trained to optimize the noise prediction objective: `L_DM = E[||ε - ε_θ(z_t, t, y)||_2^2]`.

# Experiments
- **Datasets:** VITON-HD (>10,000 upper-body pairs) and DressCode (>40,000 pairs across upper garments, lower garments, and dresses).
- **Metrics:** SSIM, LPIPS, DISTS, FID, KID, DINO similarity, and CLIP-FID.

- **Quantitative Comparison (VTON):**
    - **VITON-HD (Table 1):** Compared to 6 baselines (e.g., CatVTON, IDM-VTON), TWGTM achieves the best SSIM (0.905) and LPIPS (0.055). Its FID is 6.107.
    - **DressCode (Table 2):** TWGTM achieves the best or second-best scores on most metrics across upper body, lower body, and dresses categories. For example, it gets the best SSIM (0.941) for upper body and best FID for lower body (8.298) and dresses (8.412).

- **Quantitative Comparison (VTOFF):**
    - **VITON-HD (Table 3):** Compared to TryOffDiff and TryOffAnyone, TWGTM is state-of-the-art on most metrics. It achieves the best DISTS (0.195), FID (10.393), LPIPS (0.332), KID (1.5), MS-SSIM (0.590), and CW-SSIM (0.524). TryOffAnyone is better on CLIP-FID (5.131 vs 5.651).

- **Qualitative Comparison:**
    - **VTON (Figure 3):** The model shows superior preservation of fine-grained details (e.g., text on clothing) and robustness to complex textures.
    - **VTOFF (Figure 4):** The model better preserves garment color, texture, and shape compared to baselines, which suffer from color distortion (TryOffDiff) or feature redundancy (TryOffAnyone).

- **Ablation Studies (Table 4):**
    - Removing the Spatial Refinement Module (Variant 1) or the QFormer (Variant 8) degrades performance for both tasks.
    - Replacing latent-space spatial concatenation with warped garment fusion (Variant 2) leads to artifacts and alignment issues, confirming the importance of the concatenation strategy.
    - Using a bounding box mask for VTOFF inference (Variant 3) significantly improves VTOFF metrics (e.g., FID from 10.393 to 9.201, DISTS from 0.195 to 0.174), showing the benefit of explicit geometric guidance.
    - Training VTON (Variant 4) and VTOFF (Variant 5) from scratch as single tasks shows that VTON performs better due to mask guidance.
    - Fine-tuning on one task shows performance improvements on the other (Figures 7 & 8), supporting the hypothesis that the tasks mutually enhance each other.

# Conclusion
The paper introduces TWGTM, a unified diffusion framework for both VTON and VTOFF. The model bridges these complementary tasks using dual-conditioned guidance from latent and pixel spaces, with specialized modules for semantic and spatial feature extraction. An Extended Attention Block integrates these features. A phased training strategy is proposed to address the mask dependency asymmetry between the two tasks. Experimental results demonstrate the model's superior performance, particularly in the mask-free VTOFF task.

# References
This section lists the 55 academic papers and preprints cited in the manuscript, covering topics like GANs, diffusion models, transformers, virtual try-on, and image quality metrics.