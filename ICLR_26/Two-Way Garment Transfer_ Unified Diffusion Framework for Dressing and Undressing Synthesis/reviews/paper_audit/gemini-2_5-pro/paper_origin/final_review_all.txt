1) Summary
This paper introduces the Two-Way Garment Transfer Model (TWGTM), a unified diffusion-based framework for performing both virtual try-on (VTON) and its inverse, virtual try-off (VTOFF). The core idea is to treat these as complementary tasks solvable within a single model by reversing input feature concatenation. The method employs a dual-conditioned guidance mechanism, using features from both the latent space for structural consistency and the pixel space for semantic and spatial details. To address the asymmetry in mask availability between the mask-guided VTON and mask-free VTOFF tasks, a two-stage training strategy is proposed. The model's effectiveness is demonstrated through quantitative and qualitative experiments on the VITON-HD and DressCode datasets, where it shows competitive or state-of-the-art performance for both VTON and VTOFF tasks.2) Strengths
*   **Novel and Well-Motivated Unified Framework**
    *   The central contribution of unifying VTON and VTOFF into a single bidirectional framework is novel and compelling. The paper argues that these are dual objectives of a single deformation modeling problem (Section: Introduction).
    *   The motivation for a unified model is empirically grounded. The analysis in Figure 1, showing significant parameter and feature similarity between separate VTON and VTOFF models, provides a strong rationale for the proposed approach.
    *   The mechanism for switching between tasks by simply reversing the spatial order of concatenated inputs in the latent space is an elegant and efficient design choice (Section: Processing in Latent Space). This highlights the inherent symmetry the authors aim to exploit.*   **Sophisticated and Coherent Model Architecture**
    *   The proposed dual-space guidance mechanism is a technically sound approach to inject rich conditioning information. It combines latent-space concatenation with detailed pixel-space features (Section: Method, Figure 2(b)).
    *   The pixel-space feature extraction is well-designed, disentangling semantic and spatial information. The Semantic Abstraction Module (SAM) uses a frozen CLIP and QFormer for high-level understanding (Section: Semantic Abstraction Module), while the Spatial Refinement Module (SRM) uses a Swin Transformer and a novel "TaskFormer" for fine-grained details (Section: Spatial Refinement Module).
    *   The Extended Attention Block (Figure 2(c)) provides a principled way to fuse the extracted spatial and semantic features into the U-Net's generative process, going beyond simple concatenation or standard cross-attention.*   **Extensive Experimental Validation**
    *   The paper provides a thorough evaluation on two standard, high-resolution benchmarks (VITON-HD and DressCode), using a comprehensive set of metrics to assess different aspects of generation quality (Section: Datasets and Metrics).
    *   The quantitative results are strong, demonstrating state-of-the-art performance on the VTOFF task (Table 3) and highly competitive performance on the VTON task against numerous recent baselines (Tables 1, 2).
    *   The ablation study is particularly comprehensive (Table 4, Figures 5, 6). It systematically validates key design choices, such as the necessity of the SRM (Variant 1), the benefit of spatial concatenation over warping (Variant 2), and the effectiveness of the proposed feature fusion method (Variants 6, 7).3) Weaknesses
*   **Lack of Clarity in Methodological Description**
    *   The overall architecture diagram (Figure 2(b)) is highly complex, and the data flow is difficult to trace. For instance, it is not immediately clear how the outputs of the SAM and SRM are integrated into the main U-Net decoder at multiple levels.
    *   The description of the "TaskFormer" module is dense and hard to fully grasp (Section: Spatial Refinement Module, Equations 5-11). The text mentions masked attention, hierarchical masks, and dual projection branches with query-specific processing, but these concepts are introduced rapidly without sufficient explanation or visual aid.
    *   The two-stage training strategy lacks detail (Section: Training Strategy). The paper states that Stage 1 uses "UNet's native cross-attention blocks" while Stage 2 activates the "Extended Attention Block." This implies a significant change in the model's architecture between stages, but it is unclear how this transition is managed (e.g., are weights from Stage 1 used to initialize Stage 2?).
    *   A key explanatory figure is missing. The "Processing in Latent Space" section and the caption for Figure 2 both refer to a "Figure 2(a)" to illustrate the feature concatenation process, but this sub-figure is not present in the manuscript, hindering a full understanding of the core task-switching mechanism.*   **Contradictory or Poorly Supported Quantitative Analysis**
    *   For VTON on the VITON-HD dataset (Table 1), the proposed method is outperformed on the FID metric by two baselines (IDM-VTON and CatVTON). The text describes the performance as "competitive" but does not offer any analysis for why it might lag on this key perceptual metric.
    *   The explanation for weaker performance on the DressCode dataset (Table 2) is attributed to "inherent color and texture inconsistencies" in the dataset (Section: Quantitative Comparison). This is a speculative claim without supporting evidence.
    *   A core component of the proposed model is significantly outperformed by a simpler variant in the ablation study. For the VTOFF task, "Variant 3 (w/ Mask2BBox)" achieves substantially better scores across all metrics compared to the main "TWGTM (Ours)" model (Table 4). The paper notes this but fails to justify why the inferior model is presented as the main contribution.
    *   The claim of mutual performance enhancement between VTON and VTOFF is not well-supported by the provided evidence. The text claims Figures 7 and 8 "substantiate our hypothesis," but the plots show marginal, fluctuating, or even negative effects (e.g., DISTS for VTOFF worsens in Figure 7), which does not constitute strong evidence for mutual enhancement.*   **Omissions and Reporting Inaccuracies**
    *   The paper does not include a discussion of limitations or an analysis of failure cases. The qualitative results (Figures 3, 4) exclusively showcase successful examples.
    *   The claim of "mask-free VTOFF" could be more nuanced. The model internally predicts a mask for the garment in VTOFF using TaskFormer (Equation 10) and uses this for training in Stage 1. While no user-provided mask is needed at inference, the model is not entirely independent of mask-like spatial guidance.
    *   Practical performance metrics such as computational cost (e.g., model parameters, training time, inference latency) are not reported.
    *   There are several reporting inaccuracies. The caption for Figure 4 incorrectly labels the qualitative results as "VTON" when the content clearly shows the VTOFF task. In Table 2, the LPIPS score for "Dresses" is bolded for "Ours," incorrectly suggesting it is the best result when it is tied for the worst. The text also claims DISTS is the "primary metric for VTOFF" without justification (Section: Quantitative Comparison).4) Suggestions for Improvement
*   **Enhance Methodological Clarity**
    *   Please consider revising Figure 2(b) to improve its readability. This could involve using color-coding, numbered steps, or a simplified block diagram.
    *   Provide a more detailed explanation of the TaskFormer module. An additional diagram illustrating its internal components or a more step-by-step description would greatly aid reader understanding.
    *   Elaborate on the two-stage training process. Please clarify if Stage 2 is a fine-tuning of the model from Stage 1 and specify which weights are carried over.
    *   Please provide the missing Figure 2(a) that is referenced in the text to illustrate the latent space processing, as this is crucial for understanding the method's core mechanism.*   **Strengthen Quantitative Analysis and Justify Claims**
    *   In the discussion of Table 1, please provide a more balanced analysis that acknowledges the lower FID score and offers a potential explanation.
    *   To substantiate the claim about dataset issues in DressCode (Table 2), please include a few examples in the appendix that visualize the alleged "color and texture inconsistencies."
    *   Please address the significant contradiction in the ablation study (Table 4). Justify why TWGTM is the preferred model for VTOFF despite being clearly outperformed by Variant 3, or consider reframing Variant 3 as the recommended approach for that task.
    *   Please temper the claims regarding mutual enhancement from joint training. The analysis should more accurately reflect the mixed results shown in Figures 7 and 8, or stronger evidence should be provided.*   **Address Omissions and Correct Reporting**
    *   Please add a "Limitations" section or paragraph, ideally including a few qualitative examples of failure cases to provide a more complete assessment.
    *   Refine the phrasing around "mask-free VTOFF." We suggest clarifying that the method is "user-mask-free" at inference time but internally relies on a predicted spatial guidance map.
    *   Please report key computational statistics, such as the number of parameters, training time, and inference latency, in the main text or an appendix.
    *   Please correct the identified reporting errors: update the caption for Figure 4 to refer to VTOFF, fix the incorrect bolding in Table 2, and either provide a citation/justification for DISTS as the "primary metric" for VTOFF or remove this assertion.5) Score
*   Overall (10): 6 — The paper proposes an interesting unified framework, but the contribution is undermined by significant clarity issues, a major contradiction in the ablation results, and several reporting inaccuracies.
*   Novelty (10): 8 — The core idea of a unified, bidirectional framework for VTON and VTOFF is presented as a novel contribution with an elegant implementation concept (Section: Introduction, Figure 1).
*   Technical Quality (10): 5 — The technical contribution is weakened by the fact that a simpler variant significantly outperforms the proposed model on a key task (Table 4) and by weakly supported claims about joint training benefits (Figures 7, 8).
*   Clarity (10): 4 — The methodology is difficult to follow due to a complex diagram (Figure 2(b)), dense descriptions, and a missing key illustration (Figure 2(a)), compounded by an incorrect figure caption (Figure 4).
*   Confidence (5): 4 — I am confident in my assessment, having experience with diffusion models and generative image synthesis, though I am not an expert specifically in virtual try-on.