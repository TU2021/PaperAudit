{
  "paper": "LoCoT2V-Bench_ A Benchmark for Long-Form and Complex Text-to-Video Generation",
  "runs": [
    {
      "config": {
        "judge_model": "gpt-5-2025-08-07",
        "review_agent": "paper_audit",
        "ai_model": "gpt-5-2025-08-07",
        "baseline_file": "baseline_review.txt",
        "final_file": "final_review_all.txt",
        "temperature": 0.0,
        "max_tokens": 20000,
        "timeout_s": 600,
        "metric": "regression_v2_minimal",
        "prompt_hash": "2142c04332"
      },
      "config_key": "1a46f14f5902ebb0fc2c14773cec6644d2fa5338",
      "inputs": {
        "baseline_review": "reviews/paper_audit/gpt-5-2025-08-07/paper_origin/baseline_review.txt",
        "final_review": "reviews/paper_audit/gpt-5-2025-08-07/paper_origin/final_review_all.txt"
      },
      "regression": {
        "score_delta": {
          "baseline_score": 7.0,
          "final_score": 7.0,
          "delta": 0.0,
          "scale_hint": "1-10"
        },
        "differences": [
          {
            "diff_type": "new_critique",
            "summary": "Conflicting evaluator sources for high-level assessments (Seed1.5-VL vs Qwen2.5-VL-72B)",
            "paperaudit_types": [
              "EVIDENCE_DATA_INTEGRITY",
              "METHOD_LOGIC_CONSISTENCY",
              "CONTEXT_MISALIGNMENT_INCOHERENCE"
            ],
            "why_impacts_score": "Undermines validity and consistency of HERD/expectations used in evaluation",
            "evidence": {
              "baseline_quote": "Integration of Seed1.5-VL assessments into prompts (Section 3.1: “employ Seed1.5-VL… evaluations… integrated”).",
              "final_quote": "Section 3.1 integrates Seed1.5-VL evaluations into prompts, whereas Appendix B.8 states Qwen2.5-VL-72B was used to generate dimension-wise assessments."
            }
          },
          {
            "diff_type": "intensified_critique",
            "summary": "Content Clarity details: R unspecified and evaluator model not named",
            "paperaudit_types": [
              "EXPERIMENTAL_DESIGN_PROTOCOL"
            ],
            "why_impacts_score": "Missing protocol/model details hinder reproducibility and variance analysis",
            "evidence": {
              "baseline_quote": "R in Eq. (2) is undefined in practice (Section 3.2.4).",
              "final_quote": "R in Eq. (2) is not specified; the evaluator model for CC is also not named."
            }
          },
          {
            "diff_type": "new_critique",
            "summary": "RR-UB scaling value not reported for Aesthetic Quality",
            "paperaudit_types": [
              "EXPERIMENTAL_DESIGN_PROTOCOL",
              "EVIDENCE_DATA_INTEGRITY"
            ],
            "why_impacts_score": "Prevents independent verification of scaling; risks miscalibration",
            "evidence": {
              "baseline_quote": "Aesthetic RR-UB derived from Text-to-Image-2M data_1024_10K (Appendix B.5).",
              "final_quote": "RR-UB derived from Text-to-Image-2M, but the final numeric RR-UB value used for scaling is not reported."
            }
          },
          {
            "diff_type": "intensified_critique",
            "summary": "Normalization/orientation ambiguity (e.g., error metrics) could distort interpretation",
            "paperaudit_types": [
              "CLAIM_RESULT_DISTORTION",
              "EXPERIMENTAL_DESIGN_PROTOCOL"
            ],
            "why_impacts_score": "Ambiguous ‘higher/lower is better’ and percent conversion can mislead comparisons",
            "evidence": {
              "baseline_quote": "“All values are expressed as percentages” (Table 2–4), but normalization schemes for many sub-metrics are not detailed.",
              "final_quote": "Orientation for metrics like Warping Error not detailed; ambiguous “higher/lower is better” risks misinterpretation and uneven weighting."
            }
          },
          {
            "diff_type": "new_critique",
            "summary": "Additional clarity issues: CC prompt mislabeling and unnamed methods in Table 5",
            "paperaudit_types": [
              "CONTEXT_MISALIGNMENT_INCOHERENCE",
              "RHETORICAL_PRESENTATION_MANIPULATION"
            ],
            "why_impacts_score": "Mislabeling and omissions impede transparency and replication",
            "evidence": {
              "baseline_quote": "Case study metrics include “HPSV2” without a definition in the main text.",
              "final_quote": "CC prompt header labels “HERD Evaluation Prompt” though it defines CC; Table 5 names “three representative methods” without naming them."
            }
          }
        ]
      },
      "generated_at": "2026-01-05T20:43:35"
    }
  ]
}