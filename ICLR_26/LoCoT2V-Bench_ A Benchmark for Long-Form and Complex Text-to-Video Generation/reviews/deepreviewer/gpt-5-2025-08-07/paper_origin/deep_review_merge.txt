Summary
The paper introduces LoCoT2V-Bench, a benchmark designed to assess long-form, complex text-to-video (T2V) generation. It constructs a suite of 240 multi-event prompts spanning 18 themes, grounded in real-world YouTube videos and refined via an LLM self-refinement loop with manual edits. The evaluation framework covers five dimensions: static quality (aesthetic and technical), text–video alignment (overall and event-level), temporal quality (motion, flicker, transition smoothness, intra-/inter-event consistency), content clarity (theme, logic, completeness, consistency), and a new Human Expectation Realization Degree (HERD) that targets high-level, abstract attributes (e.g., emotional response, narrative flow, character development) using polarity-aware binary questions. Event-level alignment is formalized by extracting structured events from prompts and video descriptions, matching via the Hungarian algorithm, and penalizing sequence inversions to account for temporal order. Transition smoothness is defined using scene detection and multi-cue similarity/variance. The benchmark evaluates nine long video generation (LVG) systems, reporting strong frame-level quality but notable deficiencies in fine-grained event alignment, long-term temporal coherence, and adherence to high-level narrative expectations. The manuscript is generally well-structured with extensive tables, figures, and appendices; however, some exact parameter choices and evaluator configurations are not fully specified. Correlation analyses and entanglement studies are informative but are conducted on limited model subsets.

Strengths
- Realistic and diverse prompt suite grounded in real videos, with longer and more complex prompts than prior benchmarks.
- Principled event-level alignment metric that combines semantic fidelity with temporal order via Hungarian matching and inversion penalties.
- Comprehensive temporal quality evaluation, including novel transition smoothness and event-level temporal consistency (intra-/inter-event, subject/background), extending beyond standard clip-level metrics.
- Introduction of HERD, a polarity-aware binary evaluation targeting abstract, human-centric attributes (e.g., narrative flow, emotional tone), aiming to mitigate common yes/no scoring biases.
- Broad empirical coverage across nine LVG methods, yielding clear insights: strong static/frame-level quality contrasts with weaknesses in multi-event alignment, long-range temporal coherence, and higher-level storytelling.
- Clear organization, visualizations, and detailed appendices with methodological descriptions and prompt templates; inclusion of ethical and reproducibility statements and partial data/code availability.
- Case studies and correlation analyses that help interpret metric behavior and model weaknesses in long-form generation settings.

Weaknesses
- Extensive reliance on multimodal LLMs across both benchmark construction and multiple evaluation components (overall alignment description, event extraction, content clarity scoring, HERD Q&A) introduces risks of evaluator bias, circularity, version/seed sensitivity, and domain drift; robustness across different evaluators/models/seeds is not validated.
- Important implementation details are under-specified, reducing reproducibility: exact text embedding/encoder and similarity function for overall alignment; hyperparameters and weights for transition smoothness (e.g., α weights, window size, scaling constants); event segmentation thresholds; number and configuration of HERD questions; and some model versions used for evaluation.
- Aggregation and normalization of heterogeneous sub-metrics into unified dimension scores (often reported as percentages) lack transparent justification and sensitivity analyses; weighting schemes for combining sub-dimensions are unclear.
- Statistical and correlation analyses are conducted on limited subsets (e.g., three methods), constraining the generality of conclusions and potentially overstating broader trends.
- Moderate benchmark scale (240 prompts) and unclear control/normalization for generated video lengths across baselines may affect temporal metrics and comparability; length handling and normalization procedures are not fully detailed.
- Limited human validation: no inter-annotator agreement or calibration against human mean opinion scores for HERD and content clarity, leaving the reliability and external validity of LLM-driven judgments uncertain.
- Minor presentation inconsistencies and omissions (e.g., HERD/HERO naming, figure labels like “HPSV2,” missing precise hyperparameters in the main text) further hinder straightforward reproduction.
