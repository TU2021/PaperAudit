1) Summary
This paper introduces LoCoT2V-Bench, a new benchmark for evaluating long-form video generation (LVG) from complex text prompts. The authors identify that existing benchmarks focus on short clips with simple prompts and lack metrics for high-level attributes. To address this, they construct a prompt suite of 240 samples derived from real-world videos, featuring longer and more complex descriptions. They propose a multi-dimensional evaluation framework with five categories: Static Quality, Text-Video Alignment, Temporal Quality, Content Clarity, and a novel metric called Human Expectation Realization Degree (HERD) for assessing abstract qualities like narrative flow and emotional response. The authors evaluate nine representative LVG models, finding that while they perform well on static quality, they struggle with fine-grained event-level alignment, long-term temporal consistency, and high-level thematic adherence.2) Strengths
*   **Timely and Well-Motivated Problem**
    *   The paper addresses the critical and timely problem of evaluating long-form video generation, a rapidly advancing but difficult-to-assess area (Section 1, Paragraph 1).
    *   It correctly identifies the limitations of existing benchmarks, such as their focus on short videos, simplified prompts, and low-level metrics, thereby establishing a clear motivation for the work (Section 1, Paragraph 2; Section 2).
    *   The benchmark's focus on complex prompts containing elements like scene transitions and event dynamics directly targets the next frontier of challenges in text-to-video generation (Section 1, Bullet 1).*   **Comprehensive and Novel Evaluation Framework**
    *   The proposed framework is extensive, comprising five major dimensions and 26 sub-dimensions, offering a more holistic assessment than prior work (Section 3.2; Figure 1).
    *   The paper introduces several novel and well-conceived metrics. For instance, **Event-level Alignment (EA)** moves beyond global similarity scores by matching structured events and penalizing incorrect temporal ordering, which is crucial for complex narratives (Section 3.2.2, Equation 1).
    *   The **Human Expectation Realization Degree (HERD)** metric is an innovative attempt to quantify abstract, high-level attributes like emotional response, narrative flow, and character development using a polarity-aware VQA framework (Section 3.2.5; Section B.8).
    *   The framework also introduces more fine-grained temporal metrics, such as **Transition Smoothness** and the distinction between **Intra- and Inter-event Temporal Consistency**, which are highly relevant for multi-scene long videos (Section 3.2.3).*   **High-Quality and Challenging Prompt Suite**
    *   The prompts are derived from 240 real-world videos across 18 diverse themes, ensuring the benchmark's relevance and breadth (Section 3.1; Figure 2).
    *   The prompt generation process is systematic, leveraging MLLMs with self-refinement and manual review to ensure quality and complexity (Section 3.1).
    *   Quantitative analysis confirms that the resulting prompts are significantly longer and more complex (semantically, structurally, and in control elements) than those in several existing benchmarks, making LoCoT2V-Bench a more challenging testbed (Table 1).*   **Extensive Experiments and Insightful Analysis**
    *   The benchmark is used to evaluate a wide range of nine contemporary open-source LVG methods, providing a valuable snapshot of the current state-of-the-art (Section 4.1; Appendix B.1).
    *   The results are presented clearly and reveal important trends, such as the gap between models' performance on static quality versus more complex dimensions like event-level alignment and inter-event consistency (Table 2, Table 3, Figure 7).
    *   The paper includes several insightful analyses, such as the limited correlation between static quality and other metrics (Section 4.3, Figure 3), the entanglement between alignment and consistency (Section 4.4, Table 5), and the impact of prompt complexity on model performance (Section 4.5, Figure 9), which add depth to the findings.3) Weaknesses
*   **Heavy Reliance on LLMs/MLLMs as Evaluators**
    *   Many of the novel and central metrics, including Overall Alignment (OA), Event-level Alignment (EA), Content Clarity (CC), and Human Expectation Realization Degree (HERD), depend entirely on the outputs of other large models (e.g., Qwen2.5-VL, DeepSeek-V3.1) (Sections 3.2.2, 3.2.4, 3.2.5).
    *   This "model-as-a-judge" paradigm is susceptible to the inherent biases, failure modes, and reasoning limitations of the evaluator models. The benchmark may inadvertently measure how well generated videos align with the judge model's world knowledge and biases rather than true quality or human perception.
    *   The paper does not sufficiently discuss or analyze the potential limitations and failure cases of this approach. For example, the event extraction from generated video descriptions for the EA metric is a critical step whose robustness is not quantified (Section 3.2.2).*   **Lack of Human Correlation Study**
    *   The benchmark aims to measure alignment with "Human Expectation" (as named in the HERD metric), yet the paper presents no study correlating its automated metrics with actual human judgments. This is a significant omission for a benchmark paper.
    *   Without such validation, it is difficult to assess whether the proposed high-level metrics like HERD and Content Clarity are truly meaningful. For example, does a high "Narrative Flow" score from HERD (Table 4) actually correspond to a video that humans perceive as having a coherent story?
    *   This validation is crucial for establishing the credibility and utility of the benchmark, especially for the novel metrics that aim to capture subjective qualities.*   **Unclear Prompt Construction Process**
    *   The description of how the final test prompts are created is ambiguous. The paper states that evaluation information from Seed1.5-VL is "integrated into our previously generated raw prompts" (Section 3.1, Evaluation Information Integration).
    *   It is unclear what this integration entails. Does the final prompt explicitly instruct the model to generate a video with a certain emotional response or narrative flow? For example: "Generate a video of a cat chasing a mouse. The video should evoke a feeling of suspense and have a fast-paced narrative flow." This would constitute a different and potentially much harder task than standard text-to-video generation.
    *   The distinction between the "prompt base" and the "raw input prompt" (Section 3.2.2) further complicates this, but a concrete example of a final, complete prompt given to a model is never provided in the main text or appendix.*   **Insufficient Detail for Full Reproducibility of Some Metrics**
    *   The computation of **Transition Smoothness** involves several hyperparameters, including weights (α1-α4) and scaling factors (b, c), whose values and justification are not provided (Appendix B.6, Equations 3-4). This makes it difficult to reproduce the metric exactly.
    *   The process for generating and annotating the polarity of HERD questions is complex, relying on multiple LLM steps (Appendix B.8). While prompts are provided, the reliability and variance of this pipeline are not discussed. Given that there are potentially over 10,000 questions (240 videos * 7 dimensions * 6 questions/dim), a more thorough validation of this automated process is warranted.4) Suggestions for Improvement
*   **Acknowledge and Mitigate "Model-as-Judge" Limitations**
    *   In the paper, please add a dedicated discussion in the limitations section about the risks and potential biases of using MLLMs as evaluators.
    *   Consider performing a small-scale analysis to test the robustness of critical components, such as the event extraction from generated video descriptions (Section 3.2.2), to demonstrate its reliability.
    *   To mitigate potential bias, it would be beneficial to report results using multiple different MLLMs as judges for a key metric like Content Clarity or HERD to see how much the scores vary.*   **Conduct a Human Correlation Study**
    *   To substantially strengthen the paper's claims, please conduct a user study on a subset of the generated videos. Ask human raters to score videos on key dimensions like text-video alignment, narrative coherence, and emotional response.
    *   Report the correlation (e.g., Pearson's r, Spearman's ρ) between these human scores and the corresponding automated metrics (e.g., EA, Content Clarity, HERD). Strong positive correlations would provide crucial validation for the benchmark.
    *   Even a small-scale study would significantly increase confidence in the benchmark's alignment with human perception.*   **Clarify the Prompt Construction Process**
    *   Please provide a clear, end-to-end example of the prompt construction process in the main paper or a dedicated appendix section.
    *   This example should show: (1) the raw video description generated from the real-world video, (2) the "evaluation information" (e.g., desired emotional response) generated by Seed1.5-VL, and (3) the final, complete text prompt that is fed into the LVG models for generation.
    *   This clarification is essential for readers to understand the exact task the models are being evaluated on.*   **Provide More Details for Metric Reproducibility**
    *   For the **Transition Smoothness** metric, please specify the values used for all hyperparameters (α1-α4, b, c in Appendix B.6) and provide a brief justification for their selection (e.g., based on empirical tuning on a validation set).
    *   For the **HERD** question generation pipeline, please add a brief discussion on its quality and consistency. For instance, reporting the percentage of generated questions that required manual correction or the inter-annotator agreement if multiple LLM runs were used for polarity annotation would be helpful.5) Score
*   Overall (10): 8 — The paper introduces a comprehensive and much-needed benchmark for long-form video generation, with novel metrics and extensive experiments, though it lacks crucial human validation.
*   Novelty (10): 9 — The benchmark's focus on long, complex prompts and its introduction of novel high-level metrics like Event-level Alignment and HERD are highly original contributions.
*   Technical Quality (10): 7 — The framework is well-designed and the empirical evaluation is thorough, but the heavy reliance on unvalidated "model-as-a-judge" metrics without a human correlation study is a significant technical weakness.
*   Clarity (10): 7 — The paper is generally well-written, but the ambiguity surrounding the final prompt construction (Section 3.1) and lack of detail for some metrics (Appendix B.6) hinder full understanding and reproducibility.
*   Confidence (5): 5 — I am highly confident in my assessment, as I have expertise in generative models and their evaluation.