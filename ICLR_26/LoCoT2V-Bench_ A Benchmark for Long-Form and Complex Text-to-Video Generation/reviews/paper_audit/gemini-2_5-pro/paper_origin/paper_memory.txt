# Global Summary
This paper introduces LoCoT2V-Bench, a new benchmark for evaluating long-form and complex text-to-video (LVG) generation models. The authors argue that existing benchmarks are insufficient, as they rely on simple prompts and low-level metrics, failing to assess narrative coherence, thematic expression, and fine-grained alignment for longer videos. LoCoT2V-Bench is built upon a suite of 240 complex prompts derived from real-world videos (30-60 seconds) across 18 themes. Its evaluation framework consists of five dimensions with 26 sub-dimensions, including novel metrics like event-level alignment, inter-event temporal consistency, content clarity, and the Human Expectation Realization Degree (HERD) for high-level attributes. The authors evaluate nine open-source LVG models and find that while they perform well on static visual quality and short-term consistency, they struggle significantly with fine-grained prompt adherence, long-term temporal consistency (e.g., inter-event consistency), and high-level narrative aspects like character development and narrative flow. The benchmark is presented as a robust tool to guide future research in LVG.

# Abstract
The paper addresses the challenge of evaluating long-form video generation from complex text prompts. It notes that existing benchmarks use simplified prompts and focus on low-level metrics, neglecting fine-grained alignment, narrative coherence, and thematic expression. To fill this gap, the authors propose LoCoT2V-Bench, a benchmark for long video generation (LVG) with complex inputs. The benchmark includes a suite of realistic prompts derived from real-world videos, incorporating elements like scene transitions and event dynamics. It also introduces a multi-dimensional evaluation framework with new metrics such as event-level alignment, fine-grained temporal consistency, content clarity, and the Human Expectation Realization Degree (HERD), which assesses abstract attributes like narrative flow and character development. An evaluation of nine representative LVG models reveals that current methods perform well on basic visual and temporal aspects but struggle with inter-event consistency, fine-grained alignment, and high-level thematic adherence. LoCoT2V-Bench is positioned as a comprehensive platform to evaluate and guide future improvements in long-form complex text-to-video generation.

# Method
The LoCoT2V-Bench methodology is composed of two main parts: constructing a prompt suite and constructing an evaluation dimension suite.

### Prompt Suite Construction
The prompt suite is created in a three-stage process:
- **Video Collection:** Thousands of short-form videos (30-60 seconds) were collected from YouTube using `yt-dlp` based on 18 thematic keywords. After manual filtering for quality and relevance, a final dataset of 240 videos was established, evenly distributed across the 18 themes.
- **Prompt Generation:** Multimodal Large Language Models (MLLMs) were used to generate raw prompts from the collected videos, employing a self-refine paradigm for iterative optimization. Prompts were manually reviewed for factual accuracy.
- **Evaluation Information Integration:** Seven high-level dimensions (emotional response, narrative flow, etc.) were defined. The Seed1.5-VL model was used to evaluate the original videos along these dimensions. This evaluation information was then integrated into the raw prompts to create the final test prompts, which are typically 200-300 words long.

### Evaluation Dimension Suite Construction
The evaluation framework is divided into five main dimensions:
- **Static Quality (SQ):**
    - Aesthetic Quality (AQ): Assessed using Aesthetic Predictor V2.5 on frames sampled at one per second. The score is normalized against a more realistic upper bound derived from high-quality images.
    - Technical Quality (TQ): Assessed using DOVER++ for user-generated video quality. Long videos are segmented into clips shorter than 10s for reliable scoring.
- **Text-Video Alignment (TVA):**
    - Overall Alignment (OA): Qwen2.5-VL-7B generates a video description, and its semantic similarity to the prompt base is calculated.
    - Event-level Alignment (EA): Events (description, subject, setting, action, camera motion) are extracted from the prompt and the video description. Events are matched using the Hungarian algorithm. The score is a product of semantic and field-level similarities, penalized for incorrect temporal ordering of events.
- **Temporal Quality (TQ):**
    - Includes metrics adopted from VBench (Dynamic Degree, Motion Smoothness, Temporal Flickering) and EvalCrafter (Warping Error, Semantic Consistency).
    - Introduces new metrics:
        - Human Action: An MLLM evaluates the occurrence and smoothness of actions specified in the prompt.
        - Transition Smoothness: PySceneDetect locates transitions, and smoothness is quantified based on the variance of frame similarity around each transition point.
        - Intra- and Inter-event Temporal Consistency: Measures subject and background consistency within a single event (intra) and across different events (inter), using Time-R1-7B for event clipping and Grounded-SAM-2 for segmentation.
- **Content Clarity (CC):**
    - A new dimension assessing semantic coherence and narrative quality using MLLMs.
    - Comprises four sub-dimensions: Theme Clarity (TC), Logical Structure (LS), Information Completeness (ICP), and Information Consistency (ICS).
    - Scores (0-4) are averaged over multiple trials to mitigate randomness.
- **Human Expectation Realization Degree (HERD):**
    - A new framework to quantify how well videos meet high-level human expectations.
    - Based on the evaluation information embedded in the prompts, multiple binary (yes/no) questions are generated for seven dimensions (e.g., Emotional Response, Narrative Flow).
    - Each question is annotated with a polarity (positive/negative). An MLLM answers the questions, and the final score is the proportion of polarity-consistent responses.

# Introduction
The paper highlights that while text-to-video generation has advanced for short clips, generating long-form, complex videos remains a challenge. Existing evaluation benchmarks are inadequate for this task as they primarily focus on short videos, use simple prompts, and overlook higher-level aspects like thematic expression and event-level coherence. To address these gaps, the paper introduces LoCoT2V-Bench, a benchmark specifically for long-form video generation from complex prompts.

The key contributions are:
- A challenging prompt suite of 240 samples across 18 themes, derived from real-world videos, with prompts that are longer and more complex than those in existing benchmarks.
- A multi-dimensional evaluation framework that includes novel high-level metrics for thematic expression and event-level adherence.
- An evaluation of nine open-source long video generation (LVG) methods across 26 sub-dimensions, revealing that they excel in visual quality but struggle with inter-event coherence, fine-grained prompt adherence, and narrative flow.

# Related Work
- **Long Video Generation:** The paper categorizes LVG methods into diffusion-based models that extend short video generation, autoregressive models that support variable-length generation, and LLM-driven agent-based systems for multi-scene video creation. It notes that most methods are limited to single-scene generation.
- **Video Generation Evaluation:** Traditional metrics like FID, FVD, and CLIP-Score are mentioned as limited in scope. Recent multi-dimensional benchmarks like VBench and EvalCrafter are acknowledged but criticized for targeting short videos with simple prompts. The paper compares LoCoT2V-Bench with other benchmarks in Table 1, showing it has the highest average prompt length (236.66 words) and the highest average complexity score (8.75), surpassing benchmarks like VBench 2.0-Complex Plot (8.02) and EvalCrafter (3.73).

# Experiments
The paper evaluates nine open-source LVG methods on the LoCoT2V-Bench.

### Performance on LoCoT2V-Bench
- **Overall Findings:** Models show good performance in Static Quality but are limited in the other four dimensions. VGoT achieves the highest overall average score (72.17%), while MEVG has the lowest (49.89%).
- **Text-Video Alignment:** Scores for event-level alignment are much lower than for overall alignment, indicating difficulty in capturing fine-grained semantics. For example, CausVid scores 73.30% on OA but only 59.64% on EA.
- **Temporal Quality:** Models perform well on short-term consistency (e.g., Semantic Consistency, Intra-event Consistency scores are high, often >90%) but struggle with long-term consistency. Inter-event consistency scores are low (e.g., CausVid: 35.58% for subject, 43.26% for background). Transition Smoothness scores are also very low for most models (e.g., CausVid: 10.76%).
- **HERD:** Models struggle with high-level adherence. While scores for Visual Style and Emotional Response are relatively high (e.g., CausVid: 88.33% and 80.07%), they are very low for Narrative Flow (e.g., 31.81%) and Character Development (e.g., 39.44%).

### Does video content type impact evaluation?
- The 18 themes were grouped into three categories: Human Daily Life, Nature Exploration, and Virtual Entertainment.
- The performance differences across these categories were found to be minor, suggesting the evaluation framework is robust to content type.
- A slight increase in text-video alignment was observed for the "Nature Exploration" category, which the authors attribute to its prompts being shorter and of intermediate complexity.

### Are videos with higher static quality preferred?
- A correlation analysis was performed between Static Quality and the other four dimensions.
- The results show no strong linear correlation. The highest correlation was with Content Clarity (Pearson's r = 0.4977). The correlation with Text-Video Alignment was very low (r = 0.0841).
- This suggests that bias from static quality is limited, and the evaluation is robust.

### Entanglement between event-level alignment and temporal consistency
- The study investigated if poor alignment prevents the measurement of temporal consistency.
- The correlation between event-level alignment and event-level temporal consistency was found to be low, especially for inter-event consistency (Pearson's r for subject consistency was 0.0121).
- This indicates that even if a video is poorly aligned with the prompt, the generated content can still be internally consistent.

### How does complexity of prompts influence evaluation?
- Higher semantic and structural complexity in prompts generally led to worse model performance.
- The effect of control complexity was less pronounced, as models often fail to adhere to such constraints.
- Static Quality and Text-Video Alignment were found to be less influenced by prompt complexity, suggesting they better reflect the models' inherent capabilities.

# Conclusion
The paper introduces LoCoT2V-Bench, a benchmark for long-form and complex text-to-video generation. It features prompts derived from real-world videos and a multi-dimensional evaluation suite with five dimensions. Experiments on nine methods show that current models perform well on visual fidelity and short-term stability but struggle with fine-grained event alignment, long-range temporal coherence, and high-level narrative adherence. The authors conclude that LoCoT2V-Bench is a robust foundation for rigorous evaluation and can guide future research towards generating more coherent and controllable long-form videos.

# Appendix
- **Ethical Statement:** Video data was collected from YouTube via `yt-dlp` in compliance with its terms. Content was filtered to exclude harmful material. Prompts were generated with instructions to avoid PII and offensive content.
- **Reproducibility Statement:** The prompt data, evaluation code, and results will be released.
- **Baseline Methods:** Provides descriptions for the nine evaluated methods: FreeNoise, MEVG, FreeLong, FIFO-Diffusion, DiTCtrl, CausVid, SkyReels-V2, Vlogger, and VGoT.
- **Prompt Details:** Includes statistics on prompt length, a word cloud, and a breakdown of prompt complexity by theme category. Defines three complexity types: Semantic, Structural, and Control.
- **Aesthetic Quality Upper Bound:** Details the calculation of a Relative Reference Upper Bound (RR-UB) for aesthetic scores, derived from the top 10% of scores on the `data_1024_10K` subset of the Text-to-Image-2M dataset.
- **Metric Implementation Details:** Provides mathematical formulas and implementation details for Transition Smoothness and Intra-/Inter-event Temporal Consistency.
- **HERD Construction:** Defines the seven dimensions of HERD (e.g., Emotional Response, Narrative Flow). Explains that Qwen2.5-VL-72B was used to generate assessments from real videos, and DeepSeek-V3.1 was used to generate six questions per dimension and then annotate their polarity for scoring.
- **Additional Results and Visualizations:** Includes radar plots for main results, results broken down by theme category, and violin plots showing correlations between prompt complexity and evaluation scores.
- **Prompt Templates:** Provides the exact prompt templates used for complexity scoring, overall description generation, event extraction, human action evaluation, content clarity evaluation, and HERD evaluation.
- **LLM Usage:** States that LLMs were used to polish manually drafted text and to generate complex LaTeX code, with careful manual review and revision.

# References
This section contains the bibliography of the paper, listing all cited works.