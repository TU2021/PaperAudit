# Global Summary
The paper introduces LoCoT2V-Bench, a benchmark for evaluating long-form, complex text-to-video (T2V) generation. It targets gaps in existing evaluations that emphasize short clips, simple prompts, and low-level metrics. The benchmark is built from 240 real-world videos (30–60 s each) collected from YouTube across 18 themes, grouped into three categories shown in figures: Human Real Life (101), Nature Exploration (74), and Virtual Entertainment (65). Prompts are automatically generated by MLLMs, refined via self-refine, and manually checked; they average “236.66” words with high measured complexity (semantic 9.01, structural 8.98, control 8.25; average 8.75).

LoCoT2V-Bench evaluates five major dimensions via 26 sub-dimensions: static quality; text–video alignment (overall and event-level); temporal quality (including motion smoothness, transition smoothness, intra-/inter-event subject/background consistency, etc.); content clarity (Theme Clarity, Logical Structure, Information Completeness, Information Consistency; scored 0–4 then normalized); and Human Expectation Realization Degree (HERD) for seven high-level attributes (emotional response, narrative flow, character development, visual style, themes, interpretive depth, overall impression) using polarity-aware binary VQA.

Nine representative long video generation methods are evaluated. Key aggregate results (percentages): VGoT attains the highest overall average 72.17, with the highest static quality (91.15); SkyReels-V2 leads temporal quality (79.49); CausVid leads text–video alignment average (66.47); FIFO-Diffusion leads content clarity (80.82); HERD is highest for VGoT (63.74), with consistently low scores for Narrative Flow across models (e.g., 19.24–35.49). Event-level alignment is notably lower than overall alignment across methods (e.g., DiTCtrl OA 71.70 vs EA 54.42), and inter-event temporal consistency and transition smoothness are weak (e.g., inter-event BC often ~37–58; CausVid transition smoothness 10.76). Correlation analysis shows static quality has limited linear correlation with alignment (Pearson r = 0.0841) and HERD (0.2136) but a moderate correlation with content clarity (0.4977). Event-level alignment shows very low linear correlation with event-level temporal consistency (e.g., inter-event subject consistency r = 0.0121).

Caveats stated: prompts and many evaluations rely on MLLMs/VLMs; number of repeated trials R for content clarity scoring is not specified; details like exact weights for some composite metrics are deferred to the appendix; potential entanglement between event-level alignment and temporal consistency is analyzed and found to have low linear correlation in results.

# Abstract
- Problem: Existing T2V benchmarks focus on simple prompts and low-level metrics, making long-form, complex prompt evaluation difficult. Higher-level aspects like narrative coherence and thematic expression are overlooked.
- Proposal: LoCoT2V-Bench for long video generation under complex prompts, built from real-world videos. Includes realistic, complex prompts with scene transitions and event dynamics.
- Evaluation framework: Multi-dimensional metrics covering event-level alignment, fine-grained temporal consistency, content clarity, and HERD (emphasizing narrative flow, emotional response, character development).
- Findings: Across nine LVG models, current methods do well on basic visual/temporal aspects but struggle with inter-event consistency, fine-grained alignment, and high-level thematic adherence.
- Claim: Provides a comprehensive, reliable platform for long-form complex T2V evaluation and highlights directions for improvement.

# Method
- Benchmark overview: Five evaluation dimensions—static quality (SQ), text–video alignment (TVA), temporal quality (TQ), content clarity (CC), and HERD—supported by automated tools and MLLMs.
- Prompt suite construction:
  - Video collection: Thousands of 30–60 s YouTube videos gathered via yt-dlp, filtered manually; final set: 240 videos, evenly across 18 themes; higher-level grouping shown as Human Real Life (101), Nature Exploration (74), Virtual Entertainment (65).
  - Prompt generation: Use MLLMs with video understanding; instructions plus self-refine; manual correction. Prompts cover lighting, camera, spatial relations; typically 200–300 words.
  - Evaluation info integration: Define seven high-level dimensions (emotional response, narrative flow, character development, visual style, themes, interpretive depth, overall impression). Seed1.5-VL is used to evaluate collected videos on these; results are merged into prompts.
- Evaluation dimension suite (five categories):
  - Static Quality:
    - Aesthetic Quality (AQ): Aesthetic Predictor V2.5 (SigLIP-based) scoring 1–10 on frames sampled at 1 fps; normalized using a relative upper bound derived from high-quality images (Appendix B.5).
    - Technical Quality (TQ): DOVER++ (UGC-VQA). Long videos are split into <10 s clips for scoring, then averaged.
  - Text–Video Alignment:
    - Overall Alignment (OA): Qwen2.5-VL-7B generates a detailed video description; semantic similarity computed between this description and the “prompt base” (not the input prompt including eval info).
    - Event-level Alignment (EA): Events defined by event, subject, setting, action, camera motion. DeepSeek-V3.1 extracts ground-truth events from prompt base; events from the generated video description are matched via maximum-weight bipartite matching (Hungarian algorithm). Score is product of event description semantic similarity and average field-level similarity (subject, setting, action, camera motion), averaged across matched pairs, with a penalty for inversions in event order.
  - Temporal Quality:
    - Dynamic Degree and Motion Smoothness from VBench.
    - Human Action: Detect presence and smoothness of human actions per prompt via MLLMs (extraction + verification prompts in Appendix C.4).
    - Temporal Flickering: Mean absolute difference on static frames as in VBench.
    - Transition Smoothness: Scene cuts via PySceneDetect; within a temporal window around each transition, compute pixel-level, structural, feature, and motion similarities; abruptness quantified by variance; average across transitions (formulas in Appendix B.6).
    - Warping Error and Semantic Consistency: As in EvalCrafter.
    - Intra-/Inter-event Temporal Consistency: Subject Consistency (SC) and Background Consistency (BC). Time-R1-7B extracts event clips; Grounded-SAM-2 for masks/crops; compute intra-event stability and inter-event stability; average across subjects/backgrounds (Appendix B.7).
  - Content Clarity:
    - Dimensions: Theme Clarity (TC), Logical Structure (LS), Information Completeness (ICP), Information Consistency (ICS).
    - Scoring: An MLLM makes multiple (R) evaluations with controlled randomness; each dimension scored 0–4 with a brief rationale; scores normalized by 4 and averaged across R; S(v) is the mean across the four dimensions. Number of rounds R: Not specified.
  - HERD:
    - Objective: High-level human expectation adherence (emotional response, narrative flow, character development, visual style, themes, interpretive depth, overall impression).
    - Procedure: Generate multiple binary, dimension-specific questions from integrated assessments (Appendix B.8); annotate question polarity (positive/negative) with DeepSeek-V3.1; an MLLM answers yes/no/unclear; score = proportion of polarity-consistent yes/no answers; “unclear” ignored.
- Tools and models used: Aesthetic Predictor V2.5, DOVER++, Qwen2.5-VL-7B, DeepSeek-V3.1, Seed1.5-VL, Time-R1-7B, Grounded-SAM-2, PySceneDetect, RAFT.

# Introduction
- Motivation: Strong progress in T2V for short, high-quality clips, but long-form generation under complex prompts remains challenging. Existing evaluations emphasize short videos and low-level metrics and often rely on simplified prompts.
- Gaps: Lack of evaluation for fine-grained prompt adherence, event-level coherence, thematic expression, and narrative-level qualities in long-form outputs.
- Contributions:
  - Prompt suite from real-world videos: 240 samples across 18 themes; prompts are longer (see Table 1) and explicitly include scene transitions, camera motion, event dynamics.
  - Multi-dimensional evaluation with higher-level dimensions (e.g., thematic expression) and event-level adherence.
  - Benchmarking nine open-source LVG methods across five major dimensions and 26 sub-dimensions. Findings: models excel in visual quality and some consistency but struggle with inter-event coherence, fine-grained adherence, narrative flow.

# Related Work
- Long video generation methods include diffusion-based (e.g., LVDM; StreamingT2V; others) and autoregressive models (e.g., Time-Agnostic VQGAN + Time-Sensitive Transformer; ART-V; MAGI-1; Distilled autoregressive video diffusion). Many are limited to single-scene videos; multi-scene methods leverage LLM planning and multi-agent pipelines.
- Video generation evaluation: Traditional metrics (FID, FVD, IS, CLIPScore) are limited for multi-factor video evaluation. Recent benchmarks (VBench, EvalCrafter, VMBench, etc.) target comprehensive multi-metric evaluation but typically for short, simpler prompts. Efforts for long-form evaluation (VBench-Long; VBench 2.0 complex plot/landscape) either use simplified prompts or have limited prompt counts/complexity.
- Prompt complexity comparison (Table 1):
  - EvalCrafter: 700 samples; avg length 12.33; complexity avg 3.73 (semantic 3.88, structural 3.05, control 4.27).
  - VBench-Long: 946; 7.64; 2.54 (2.75, 2.11, 2.76).
  - VBench 2.0 – Complex Plot: 60; 117.15; 8.02 (8.80, 8.30, 6.95).
  - VBench 2.0 – Complex Landscape: 30; 142.10; 8.34 (7.73, 8.80, 8.50).
  - VMBench: 1050; 26.23; 5.24 (5.96, 5.36, 4.39).
  - FilMaster-Complex: 10; 95.70; 8.07 (9.00, 8.00, 7.20).
  - LoCoT2V-Bench (ours): 240; 236.66; 8.75 (9.01, 8.98, 8.25).

# Experiments
- Baselines (nine representative open-source methods; brief context in Appendix B.1):
  - FreeNoise; MEVG; FreeLong; FIFO-Diffusion; DiTCtrl; CausVid; SkyReels-V2 (540p model used); Vlogger; VGoT.
- Main results (Table 2; percentages):
  - Overall averages: VGoT 72.17; SkyReels-V2 69.64; CausVid 67.54; FIFO-Diffusion 64.47; FreeNoise 64.15; DiTCtrl 63.21; FreeLong 60.61; Vlogger 56.48; MEVG 49.89.
  - Static Quality (AQ, TQ; avg): VGoT 91.15 (AQ 85.50; TQ 96.79) highest; CausVid 75.99; SkyReels-V2 73.69; FIFO-Diffusion 63.91; FreeNoise 68.36; MEVG 30.04 (lowest).
  - Text–Video Alignment (OA, EA; avg): CausVid 66.47 (OA 73.30; EA 59.64) highest; DiTCtrl 63.06; FreeLong 61.04; SkyReels-V2 58.66; FreeNoise 55.23; MEVG 56.90; VGoT 54.95; FIFO-Diffusion 52.30; Vlogger 44.73. Event-level alignment is consistently lower than OA (e.g., DiTCtrl 71.70 vs 54.42).
  - Temporal Quality (aggregate of sub-dims in Table 3): SkyReels-V2 79.49 highest; FIFO-Diffusion 75.58; VGoT 71.21; DiTCtrl 70.77; CausVid 69.84; FreeNoise 73.26; FreeLong 66.57; MEVG 66.70; Vlogger 66.07.
  - Content Clarity (TC, LS, ICP, ICS; avg): FIFO-Diffusion 80.82 highest; VGoT 79.79; SkyReels-V2 73.62; FreeNoise 73.90; CausVid 61.83; DiTCtrl 63.28; FreeLong 60.63; Vlogger 49.00; MEVG 48.27.
  - HERD (avg across seven sub-dims; Table 4): VGoT 63.74 highest; CausVid 63.55; SkyReels-V2 62.74; DiTCtrl 60.72; FreeLong 57.65; Vlogger 58.59; FreeNoise 50.00; FIFO-Diffusion 49.76; MEVG 47.54.
- Temporal quality sub-dimensions (Table 3; selected ranges):
  - MotionSmoothness: 95.35–99.35 across methods (high).
  - TransitionSmoothness: ranges widely; e.g., CausVid 10.76; SkyReels-V2 79.28.
  - HumanAction: 23.30–48.15.
  - Intra-event SC/BC: typically >90 (e.g., FreeNoise ITAE SC 95.60; BC 97.82).
  - Inter-event SC/BC: much lower (e.g., FreeLong ITRE SC 37.41; BC 42.40).
- HERD sub-dimensions (Table 4; notable patterns):
  - Emotional Response: 58.82–82.22 (higher).
  - Narrative Flow: low across all, 19.24–35.49.
  - Character Development: low, 22.99–48.13.
  - Visual Style: high, 72.99–90.35.
  - Themes: mid-to-high, 45.62–72.57.
  - Interpretive Depth: low-to-mid, 27.99–42.57.
  - Overall Impression: high, 72.71–90.07.
- Content Clarity scoring equation: the per-video score equals the average over four dimensions, each being the average over R trials of scores normalized by 4 (R not specified).
- Analysis (Section 4.1): Models show strong frame-level static quality and short-term stability but underperform on fine-grained event alignment, long-term temporal coherence (inter-event consistency, transition smoothness), and high-level narrative adherence (HERD).
- Theme category analysis (Section 4.2; Table 6): Performance differences across the three content categories are minor overall. Example aggregated results (averages across five dimensions):
  - Human Real Life: e.g., VGoT 72.91; SkyReels-V2 71.55; CausVid 68.29.
  - Nature Exploration: e.g., VGoT 74.32; CausVid 71.71; DiTCtrl 68.62.
  - Virtual Entertainment: e.g., VGoT 70.39; SkyReels-V2 66.46; CausVid 63.61.
  - Observation noted: slightly higher text–video alignment on nature exploration; prompts there are shorter and of intermediate complexity.
- Static quality bias (Section 4.3; Table 5 correlations; representative methods):
  - SQ vs Temporal Quality: Pearson 0.2942; Spearman 0.2924; Kendall 0.1937.
  - SQ vs Text–Video Alignment: 0.0841; 0.1093; 0.0701.
  - SQ vs Content Clarity: 0.4977; 0.4853; 0.3401.
  - SQ vs HERD: 0.2136; 0.1958; 0.1328.
  - Event-level Alignment vs event-level consistency: low correlations (e.g., vs inter-event SC Pearson 0.0121; vs inter-event BC 0.0110).
- Entanglement analysis (Section 4.4): Despite potential entanglement (missing events/subjects), event-level alignment has low linear correlation with event-level temporal consistency, attributed to differences in evaluation model behaviors.
- Prompt complexity effects (Section 4.5): Higher semantic and structural complexity generally reduce performance; control complexity has weaker effect. Static quality and text–video alignment are less affected by prompt complexity than other dimensions.
- Case studies (Appendix E; sample-level numbers):
  - Food_3: FIFO-Diffusion—SQ 0.6526; TVA 0.6434; TQ 0.7667; CC 0.7500; HERD 0.5714. DiTCtrl—SQ 0.6296; TVA 0.5743; TQ 0.6773; CC 0.6250; HERD 0.5952.
  - Minivlog_9: VGoT—SQ 0.9051; TVA 0.5955; TQ 0.7019; CC 0.9167; HERD 0.5238. MEVG—SQ 0.2995; TVA 0.4600; TQ 0.6751; CC 0.5000; HERD 0.2619.
  - Pets_8: CausVid—SQ 0.7512; TQ 0.7064; TVA 0.8659; CC 0.6250. SkyReels-V2—SQ 0.7573; TQ 0.7971; TVA 0.5358; CC 0.5000.
- Implementation notes:
  - Aesthetic Quality normalization: Relative Reference Upper Bound (RR-UB) from Text-to-Image-2M data_1024_10K; the mean of the top 10% of Aesthetic Predictor V2.5 scores is used as reference.
  - Transition Smoothness: Features combine pixel MAE, SSIM, SigLIP features, and RAFT motion; exact weights and constants provided in Appendix B.6; numeric hyperparameters not specified in main text.

# Conclusion
- LoCoT2V-Bench benchmarks long-form, complex T2V with realistic prompts and a five-dimension, 26 sub-dimension evaluation suite, enabling fine-grained and high-level assessment. Experiments on nine methods show strong visual fidelity and short-term stability, but persistent deficiencies in event-level alignment, long-range temporal coherence, and narrative adherence. Additional analyses (content types, prompt complexity, metric entanglement) support robustness and clarify current challenges.

# Appendix
- Ethical statement: YouTube data collected under platform terms; automatic and manual filtering to remove invalid/harmful content; prompts generated/refined under strict instructions excluding PII/offensive/violent content with human verification; no private/sensitive data used.
- Reproducibility: Prompt data, evaluation code, and results to be released at https://anonymous.4open.science/r/LoCoT2V-Bench-1518/; an initial JSON prompt suite is provided; full release after code reorganization/verification.
- Baseline method notes (selected quantitative claims from authors’ descriptions):
  - FreeLong: training-free SpectralBlend Temporal Attention to extend 16-frame to 128-frame generation.
  - FIFO-Diffusion: infinite video via FIFO denoising; introduces latent partitioning and lookahead denoising.
  - DiTCtrl: tuning-free multi-prompt longer video generation with KV-sharing and latent blending.
  - CausVid: fast autoregressive video diffusion, distilling from a bidirectional teacher; reduces latency from “219 s” to “1.3 s” and enables streaming “9.4 fps” on one GPU while maintaining quality.
  - SkyReels-V2: infinite-length cinematic videos via MLLM captioning, multi-stage pretraining, motion-specific RL, and diffusion forcing; 540p model used here.
  - Vlogger: four-stage LLM-directed pipeline to produce 5-minute vlogs from open-world text without extra long-video training.
  - VGoT: modular, training-free multi-shot generation with script, keyframe, shot-level synthesis, and cross-shot smoothing.
- Prompt statistics: Word cloud and length distribution shown; prompts mostly 200–300 words (figure); theme-wise prompt lengths/complexities normalized (Appendix B.3, Fig. 5).
- HERD construction: Qwen2.5-VL-72B first produces dimension-wise assessments from real videos; DeepSeek-V3.1 generates six binary questions per dimension; polarity annotation corrects yes/no bias; evaluation counts only polarity-consistent yes/no; “unclear” ignored.
- Additional technical details:
  - Event-level consistency formulas for intra-/inter-event subject/background consistency (cosine similarity over encoded features).
  - Transition Smoothness score defined via normalized variance of similarity sequence within transition windows; overall score averaged over detected transitions.
  - Aesthetic RR-UB computed on Text-to-Image-2M data_1024_10K by averaging top 10% Aesthetic Predictor V2.5 scores.
- Detailed LLM usage: Human authors draft text; LLMs used for polishing and complex LaTeX/table generation with human verification.

# References
- Key models and tools cited for evaluation: Aesthetic Predictor V2.5 (SigLIP); DOVER++ (UGC-VQA); CLIP/CLIPScore; RAFT optical flow; PySceneDetect; VBench and VBench++; EvalCrafter; Seed1.5-VL; Qwen2.5-VL; DeepSeek-V3.1; Time-R1-7B; Grounded-SAM-2.
- Benchmarks and datasets referenced: VBench, VBench-Long, VBench 2.0 (Complex Plot/Landscape), EvalCrafter, VMBench, FilMaster-Complex, Video-Bench, MovieBench, T2VEval, VistoryBench; Text-to-Image-2M (for RR-UB).
- Baselines and related generation works: FreeNoise; MEVG; FreeLong; FIFO-Diffusion; DiTCtrl; CausVid; SkyReels-V2; Vlogger; VGoT; StreamingT2V; MAGI-1; History-Guided Video Diffusion; FlexiFilm; VideoStudio; DreamFactory; MovieFactory; VideoDirectorGPT; Videogen-of-Thought; ART-V.
- Foundational references: FID, FVD, IS; CLIP; SigLIP; Hungarian algorithm (Kuhn, 1955); GPT-4o system card.