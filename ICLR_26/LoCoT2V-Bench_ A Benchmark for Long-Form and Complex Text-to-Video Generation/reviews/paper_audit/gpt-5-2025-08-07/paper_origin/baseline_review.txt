Summary
- The paper introduces LoCoT2V-Bench, a benchmark for evaluating long-form text-to-video generation under complex prompts. It builds a 240-video prompt suite from real-world YouTube content (30–60s) and uses MLLMs to generate and refine long, multi-scene prompts (Section 3.1; Fig. 2; Table 1; Appendix B.2–B.4). The evaluation framework spans five dimensions—static quality, text–video alignment, temporal quality, content clarity, and Human Expectation Realization Degree (HERD)—with novel components such as event-level alignment with order penalties (Eq. 1), transition smoothness (Appendix B.6), intra/inter-event temporal consistency (Appendix B.7), content clarity via repeated MLLM MOS (Eq. 2; Section 3.2.4), and polarity-aware HERD (Section 3.2.5; Appendix B.8). Nine representative methods are benchmarked across 26 sub-dimensions (Table 2–4; Fig. 7), with analyses on theme categories, correlations, entanglement, and prompt complexity (Sections 4.2–4.5; Table 5; Fig. 3, 5, 8, 9). A reproducibility statement promises code and data release (Section 7).Strengths
- Bold focus on long-form, complex prompts
  • Evidence: The prompt suite averages 236.66 words and scores highest in complexity across benchmarks (Table 1), with distribution shown in Fig. 4 (Appendix B.2) and coverage of multi-scene elements (Section 3.1; “200–300 words” claim; Appendix B.2).
  • Why it matters: Addresses a notable gap in current evaluation (novelty/impact) by moving beyond short, single-scene prompts.- Multi-dimensional framework extends evaluation to high-level attributes
  • Evidence: Five major dimensions and 26 sub-dimensions are defined (Fig. 1; Section 3.2), introducing content clarity (Section 3.2.4) and HERD for emotional/narrative aspects (Section 3.2.5; Appendix B.8).
  • Why it matters: Captures abstract qualities (impact) often missed by visual/temporal metrics; improves holistic assessment.- Novel event-level alignment with temporal order penalty
  • Evidence: Event-level alignment formalization with Hungarian matching and inversion penalty in Eq. (1); event definition and extraction details (Section 3.2.2; Appendix C.2–C.3).
  • Why it matters: Fine-grained adherence and order sensitivity are critical for multi-scene long videos (technical soundness/novelty).- Temporal quality suite includes transition smoothness and intra/inter-event consistency
  • Evidence: Transition smoothness pipeline with multi-cue similarity and RAFT motion consistency (Appendix B.6); intra/inter-event subject/background consistency via Time-R1-7B and Grounded-SAM-2 (Section 3.2.3; Appendix B.7).
  • Why it matters: Moves beyond short-term smoothness into long-range coherence across events (technical depth/impact).- Polarity-aware HERD scoring to mitigate yes/no bias
  • Evidence: Polarity annotation and scoring scheme (Section 3.2.5; Appendix B.8, C.6).
  • Why it matters: Reduces systematic scoring bias in binary QA (technical soundness/rigor).- Comprehensive benchmarking across diverse methods with detailed results
  • Evidence: Nine baselines evaluated (Appendix B.1), aggregated dimension scores (Table 2), temporal sub-dimensions (Table 3), and HERD sub-dimensions (Table 4); visualizations (Fig. 7; Appendix B.9–B.10; Table 6).
  • Why it matters: Offers actionable insights into where current models fail (experimental rigor/impact).- Analytical studies on correlations, entanglement, and thematic effects
  • Evidence: Correlation analyses between static quality and other dimensions (Section 4.3; Fig. 3; Table 5); entanglement discussion (Section 4.4; Table 5); theme category comparison (Section 4.2; Fig. 5; Fig. 8; Table 6).
  • Why it matters: Diagnoses metric interactions and robustness, guiding future research (clarity/impact).- Ethical and reproducibility intentions
  • Evidence: Ethical statement (Section 6) and planned release of data/code/results (Section 7; link provided).
  • Why it matters: Supports responsible, repeatable science (rigor/community impact).Weaknesses
- Heavy reliance on MLLMs for both prompt construction and evaluation without human validation
  • Evidence: Prompts and refinements via MLLMs (Section 3.1: “employ them directly to generate raw prompts… self-refine”; Appendix B.4); HERD and content clarity are MLLM-based (Section 3.2.4–3.2.5; Appendix C.5–C.6).
  • Why it matters: Potential circularity/bias and lack of human-grounded calibration for abstract dimensions (experimental rigor/validity).
  • Evidence: Integration of Seed1.5-VL assessments into prompts (Section 3.1: “employ Seed1.5-VL… evaluations… integrated”).
  • Why it matters: Using model-derived “human expectation” proxies may embed model-specific biases (validity).
  • Evidence: No user study validation or correlation with human judgments is reported.
  • Why it matters: No direct evidence found in the manuscript of human validation reduces confidence in high-level metrics (technical quality/impact).- Ambiguities and incomplete specification of evaluation hyperparameters
  • Evidence: Content clarity repeats “prompted multiple times with controlled randomness” but R in Eq. (2) is undefined in practice (Section 3.2.4).
  • Why it matters: Missing R undermines reproducibility and variance analysis (technical rigor).
  • Evidence: Transition smoothness formulae given (Appendix B.6, Eq. 3–6), but numeric values for α1–α4, window size k, and b,c are not specified.
  • Why it matters: Without fixed parameters, comparisons may vary by implementation (reproducibility).
  • Evidence: Event-level alignment mentions “embedding models” for similarity and “field-level similarity” (Section 3.2.2) but omits the exact embedding model(s), similarity thresholds, and matching settings.
  • Why it matters: Unclear configuration could affect EA scores (technical clarity/reproducibility).- Potential metric entanglement and cascading errors across chained tools without ablations
  • Evidence: Entanglement acknowledged (Section 4.4), but no ablation to quantify impact across pipelines (e.g., replacing MLLM event extraction with human labels).
  • Why it matters: Without ablations, it’s hard to separate alignment vs. consistency effects (technical soundness).
  • Evidence: Event extraction depends on Qwen2.5-VL descriptions and DeepSeek-V3.1 (Section 3.2.2; Appendix C.2–C.3).
  • Why it matters: Cascading errors from LLM mis-descriptions can skew EA (experimental reliability).
  • Evidence: HERD polarity relies on DeepSeek-V3.1 (Appendix B.8; C.6).
  • Why it matters: Polarity misclassification can systematically bias HERD scores (metric validity).- Dataset scale and representativeness; unclear generated video lengths impacting comparability
  • Evidence: 240 prompts across 18 themes collected from 30–60s YouTube videos (Section 3.1; Fig. 2).
  • Why it matters: Smaller sample size than some benchmarks (Table 1: e.g., VMBench 1050, VBench-Long 946) could limit statistical power (experimental scope).
  • Evidence: No standardized output duration across baselines; DOVER++ evaluation requires segmentation <10s (Section 3.2.1).
  • Why it matters: No direct evidence found in the manuscript of fixed output lengths can affect metric comparability (fairness).
  • Evidence: Real-world short videos as prompt bases may bias content types (Section 3.1; Fig. 2).
  • Why it matters: May under-represent cinematic multi-minute narratives the benchmark claims to target (generalizability).- Normalization and scaling consistency may conflate absolute interpretability
  • Evidence: Aesthetic RR-UB derived from Text-to-Image-2M data_1024_10K (Appendix B.5).
  • Why it matters: Using T2I image distributions to set video frame upper bounds may miscalibrate AQ for videos (technical soundness).
  • Evidence: “All values are expressed as percentages” (Table 2–4), but normalization schemes for many sub-metrics (beyond Eq. 2 and B.6/B.7 descriptions) are not detailed.
  • Why it matters: No direct evidence found in the manuscript for consistent normalization across heterogeneous metrics risks uneven weighting (clarity/rigor).
  • Evidence: HERD counts only polarity-consistent yes/no answers; “unclear” excluded (Section 3.2.5).
  • Why it matters: Exclusion may inflate scores if ambiguity is frequent, affecting comparability across models (metric validity).- Clarity issues and minor inconsistencies in terminology/figures
  • Evidence: “HERO” appears in radar labels (Appendix B.9–B.10: “HERO Results”; and Fig. 7 description).
  • Why it matters: Terminology inconsistency can confuse readers (clarity).
  • Evidence: TQ is used for both Technical Quality (Section 3.2.1) and Temporal Quality heading (Section 3.2.3).
  • Why it matters: Ambiguous abbreviations reduce interpretability (clarity).
  • Evidence: Case study metrics include “HPSV2” (Appendix E; Fig. 10–11) without a definition in the main text.
  • Why it matters: No direct evidence found in the manuscript explaining HPSV2 hampers transparency (clarity).- Reproducibility and openness remain incomplete at submission time
  • Evidence: Section 7 states only an initial prompt suite JSON is provided; full code release will follow reorganization.
  • Why it matters: Review-time replication is limited (reproducibility).
  • Evidence: Heavy use of specific MLLMs (Qwen2.5-VL, DeepSeek-V3.1, Seed1.5-VL; Sections 3.1, 3.2.2, 3.2.5) may hinder exact reproduction if versions or access change.
  • Why it matters: Dependency on evolving proprietary/open models complicates stability (reproducibility).
  • Evidence: Baseline generation details lack seeds, hardware specs, and standardized output lengths (Appendix B.1).
  • Why it matters: No direct evidence found in the manuscript of standardized generation settings may affect fairness and repeatability (experimental rigor).Suggestions for Improvement
- Add human-grounded validation to calibrate MLLM-based metrics
  • Conduct a user study comparing HERD and content clarity scores against human judgments on a stratified subset of videos (Sections 3.2.4–3.2.5; Table 4) to quantify correlation and potential bias.
  • Validate the Seed1.5-VL-derived prompt-integrated “expectations” (Section 3.1) with human annotations for a sample of prompts and report agreement statistics.
  • Include inter-rater reliability (e.g., Cohen’s κ) and provide protocols to ensure replicability; if infeasible, report “No direct evidence found” limitations explicitly in the paper.- Specify evaluation hyperparameters and implementation details
  • Report the number of rounds R in Eq. (2) and the randomization controls used in content clarity (Section 3.2.4) to enable exact reproduction.
  • Provide numeric settings for transition smoothness (Appendix B.6: α1–α4, k, b, c) and justify choices via a sensitivity analysis.
  • Detail the embedding model(s), similarity metrics, and thresholds used in event-level alignment (Section 3.2.2; Appendix C.2–C.3), including ablation on alternatives.- Quantify and mitigate metric entanglement with ablations
  • Replace LLM-based event extraction with human-labeled events on a subset to measure EA sensitivity (Section 3.2.2; Section 4.4).
  • Swap specific tools in the pipeline (e.g., Time-R1 vs. alternative temporal grounding, Grounded-SAM-2 vs. SAM variants) and report variance in intra/inter-event consistency (Section 3.2.3; Appendix B.7).
  • Evaluate HERD with alternative polarity sources and cross-check consistency (Section 3.2.5; Appendix B.8), reporting error bars.- Strengthen dataset scope and standardize output durations
  • Expand beyond 240 prompts or provide statistical power analysis justifying sample size (Section 3.1; Fig. 2; Table 1) and include more multi-minute narrative bases when feasible.
  • Enforce standardized generated video lengths (e.g., fixed target durations per prompt) and report any exception handling (Section 3.2.1), ensuring comparable evaluation across baselines.
  • Diversify sources beyond YouTube 30–60s material and document coverage of cinematic structures to better match benchmark goals (Section 3.1).- Improve normalization and cross-metric comparability
  • Validate RR-UB for AQ against a curated high-quality video frame set and report alignment with human aesthetic judgments (Appendix B.5).
  • Document normalization schemes for all metrics and provide a unified scaling protocol (beyond Eq. 2 and Appendix B.6/B.7), including min–max or z-score strategies per metric with rationale (Tables 2–4).
  • In HERD, report the proportion of “unclear” answers per model and consider weighted penalties for high ambiguity to avoid inflating scores (Section 3.2.5).- Resolve terminology and figure inconsistencies
  • Harmonize “HERD” naming across all figures and text (Appendix B.9–B.10; Fig. 7) and avoid ambiguous abbreviations like dual-use “TQ” (Sections 3.2.1, 3.2.3).
  • Define “HPSV2” or remove it from case study figures (Appendix E; Fig. 10–11) to prevent confusion.
  • Add a concise glossary mapping all abbreviations to dimensions and sub-metrics at first use (Fig. 1; Sections 3.2.*).- Enhance reproducibility and fairness details
  • Release full code, prompts, and evaluation scripts at camera-ready; include versioned configs for all MLLMs and tools (Section 7).
  • Document seeds, hardware, inference parameters, and standardized output lengths for each baseline (Appendix B.1), and provide Docker/conda environments to stabilize dependencies.
  • Where access to specific MLLMs is limited, supply distilled or open alternatives and report cross-model robustness of scores (Sections 3.1, 3.2.2, 3.2.5).Score
- Overall (10): 7 — Strong benchmark contributions with novel event-level and high-level metrics (Section 3.2; Eq. 1; Tables 2–4; Fig. 1), tempered by reliance on MLLMs without human validation and incomplete reproducibility details (Sections 3.2.4–3.2.5; Section 7).
- Novelty (10): 8 — New event-level alignment with order penalty (Eq. 1), transition smoothness (Appendix B.6), intra/inter-event consistency (Appendix B.7), and polarity-aware HERD (Section 3.2.5) advance long-form evaluation beyond prior suites (Table 1; Section 1).
- Technical Quality (10): 6 — Solid formulations and analyses (Sections 3.2.2–3.2.5; Table 5) but missing hyperparameter specifics (Section 3.2.4; Appendix B.6), limited ablations (Section 4.4), and no human validation.
- Clarity (10): 7 — Generally clear structure and extensive appendices (Appendix B–C), with minor inconsistencies (HERO vs. HERD; dual “TQ”; undefined “HPSV2”) (Appendix B.9–B.10; E; Sections 3.2.1, 3.2.3).
- Confidence (5): 3 — Moderate confidence based on comprehensive manuscript detail and results (Tables 2–4; Fig. 7), but absent human studies and pending full code release (Section 7) limit verification.