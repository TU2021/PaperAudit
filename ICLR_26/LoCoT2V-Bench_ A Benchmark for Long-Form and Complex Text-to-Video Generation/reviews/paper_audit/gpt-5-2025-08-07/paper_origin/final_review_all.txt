Summary
- The paper introduces LoCoT2V-Bench, a benchmark for evaluating long-form text-to-video generation under complex prompts. It builds a 240-prompt suite derived from 30–60s real-world YouTube videos and uses MLLMs to generate and self-refine long, multi-scene prompts (Section 3.1; Fig. 2; Table 1; Appendix B.2–B.4). The evaluation spans five dimensions—static quality, text–video alignment (overall and event-level), temporal quality, content clarity, and Human Expectation Realization Degree (HERD)—with new components such as event-level alignment with order penalties (Eq. 1), transition smoothness (Appendix B.6), intra/inter-event temporal consistency (Appendix B.7), content clarity via repeated MLLM MOS (Eq. 2; Section 3.2.4), and polarity-aware HERD (Section 3.2.5; Appendix B.8). Nine methods are benchmarked across 26 sub-dimensions (Tables 2–4; Fig. 7), with analyses on themes, correlations, entanglement, and prompt complexity (Sections 4.2–4.5; Table 5; Fig. 3, 5, 8, 9). Section 7 states an initial JSON prompt release, with full code/data to follow.Strengths
- Bold focus on long-form, complex prompts
  • Evidence: Prompts average 236.66 words and rank highest in complexity among benchmarks (Table 1), with length distribution in Fig. 4 (Appendix B.2) and multi-scene coverage detailed in Section 3.1 and Appendix B.2.
  • Why it matters: Moves evaluation beyond short, single-scene prompts, addressing a recognized gap (novelty/impact).
  • Evidence: The suite spans 18 themes grouped into three categories (Section 3.1; Fig. 2).
  • Why it matters: Broad topical coverage improves generality (impact).
  • Evidence: Instructions and self-refine paradigm with manual review (Section 3.1).
  • Why it matters: Enhances prompt quality under complex, realistic settings (rigor).- Multi-dimensional framework extends evaluation to high-level attributes
  • Evidence: Five major dimensions and 26 sub-dimensions (Fig. 1; Section 3.2), including content clarity (Section 3.2.4) and HERD (Section 3.2.5; Appendix B.8).
  • Why it matters: Abstract qualities (clarity, narrative/thematic) are often overlooked; this framework adds them (impact).
  • Evidence: CC comprises TC, LS, ICP, ICS with a formal aggregation (Eq. 2; Section 3.2.4).
  • Why it matters: Structured scoring improves interpretability and replicability (technical soundness/clarity).
  • Evidence: HERD defines seven human-centric sub-dimensions (Section 3.2.5; Appendix B.8).
  • Why it matters: Captures narrative/emotional adherence beyond low-level metrics (impact).- Novel event-level alignment with temporal order penalty
  • Evidence: EA formulation with Hungarian matching and inversion penalty (Eq. 1); event schema and extraction (Section 3.2.2; Appendix C.2–C.3).
  • Why it matters: Order-aware, fine-grained adherence is crucial for multi-scene long videos (novelty/technical soundness).
  • Evidence: Ground-truth events extracted from prompt base via DeepSeek-V3.1 (Section 3.2.2; Appendix C.3).
  • Why it matters: Aligns evaluation to intended event structure (rigor).
  • Evidence: OA uses MLLMs to produce detailed video descriptions before similarity scoring (Section 3.2.2; Appendix C.2).
  • Why it matters: Improves semantic fidelity over CLIP-only baselines (impact).- Temporal quality suite includes transition smoothness and intra/inter-event consistency
  • Evidence: Transition smoothness combines pixel, structural, feature, and motion cues and averages across transitions (Appendix B.6); intra/inter-event SC/BC computed with Time-R1-7B and Grounded-SAM-2 (Section 3.2.3; Appendix B.7).
  • Why it matters: Evaluates long-range coherence across events and scene transitions (technical depth/impact).
  • Evidence: Warping error and semantic consistency follow EvalCrafter (Section 3.2.3).
  • Why it matters: Leverages established temporal consistency baselines (rigor).
  • Evidence: Human Action evaluation via MLLMs with extraction/verification prompts (Section 3.2.3; Appendix C.4).
  • Why it matters: Enables assessment of complex, prompt-conditioned actions (impact).- Polarity-aware HERD scoring to mitigate yes/no bias
  • Evidence: Polarity annotation and scoring scheme (Section 3.2.5; Appendix B.8; Appendix C.6).
  • Why it matters: Reduces bias from question framing in binary VQA (technical soundness/rigor).
  • Evidence: Multiple questions per dimension (Appendix B.8).
  • Why it matters: Improves robustness via redundancy (rigor).
  • Evidence: Integration of dimension definitions (Appendix B.8).
  • Why it matters: Clear criteria support consistent evaluation (clarity).- Comprehensive benchmarking with detailed results
  • Evidence: Nine baselines (Appendix B.1), dimension aggregates (Table 2), temporal sub-dimensions (Table 3), HERD sub-dimensions (Table 4), visualizations (Fig. 7; Appendix B.9–B.10; Table 6).
  • Why it matters: Offers granular insights into strengths/weaknesses of current models (experimental rigor/impact).
  • Evidence: Low event-level alignment vs overall alignment across methods (Tables 2–3).
  • Why it matters: Highlights where models fail under fine-grained adherence (impact).
  • Evidence: Consistently low narrative flow across HERD (Table 4).
  • Why it matters: Identifies persistent shortcomings in high-level narrative coherence (impact).- Analytical studies on correlations, entanglement, and thematic effects
  • Evidence: Correlations between SQ and other dimensions (Section 4.3; Fig. 3; Table 5); entanglement analysis (Section 4.4; Table 5); theme category comparison (Section 4.2; Fig. 5; Fig. 8; Table 6).
  • Why it matters: Diagnoses metric interactions and potential biases (clarity/impact).
  • Evidence: Prompt complexity effects (Section 4.5; Fig. 9).
  • Why it matters: Explores evaluation sensitivity to prompt semantics/structure/control (rigor).
  • Evidence: Low correlation between EA and inter-event consistency (Table 5).
  • Why it matters: Suggests separable failure modes (technical soundness).- Ethical and reproducibility intentions
  • Evidence: Ethical statement (Section 6) and planned release of data/code/results (Section 7; link).
  • Why it matters: Supports responsible, repeatable science (community impact).
  • Evidence: Initial prompt JSON provided at submission (Section 7).
  • Why it matters: Enables partial verification (reproducibility).
  • Evidence: Tool/model citations and appendix formulas (References; Appendices B.6–B.7).
  • Why it matters: Improves transparency of evaluation components (clarity/rigor).Weaknesses
- Heavy reliance on MLLMs for both prompt construction and evaluation without human validation
  • Evidence: Prompts and refinements via MLLMs with self-refine and manual review (Section 3.1: “employ them directly to generate raw prompts… self-refine”), and HERD/content clarity are MLLM-based (Section 3.2.4–3.2.5; Appendix C.5–C.6).
  • Why it matters: Risks circularity/bias and lacks human-grounded calibration for abstract dimensions (experimental rigor/validity).
  • Evidence: Source inconsistency in high-level assessments: Section 3.1 integrates Seed1.5-VL evaluations into prompts, whereas Appendix B.8 states Qwen2.5-VL-72B was used to generate dimension-wise assessments.
  • Why it matters: Conflicting evaluator sources undermine the correctness of HERD setup and prompt-integrated “expectations” (validity/clarity).
  • Evidence: No user study or correlation with human judgments is reported.
  • Why it matters: No direct evidence found in the manuscript of human validation reduces confidence in high-level metrics (technical quality/impact).- Ambiguities and incomplete specification of evaluation hyperparameters
  • Evidence: Content clarity repeats “prompted multiple times with controlled randomness,” but R in Eq. (2) is not specified; the evaluator model for CC is also not named (Section 3.2.4; Appendix C.5).
  • Why it matters: Missing R/model details undermine reproducibility and variance analysis (technical rigor).
  • Evidence: Transition smoothness equations provided (Appendix B.6, Eq. 3–6), but numeric values for α1–α4, window size k, and b,c are not specified.
  • Why it matters: Without fixed parameters, comparisons may vary by implementation (reproducibility).
  • Evidence: Event-level alignment mentions “embedding models” and field-level similarity (Section 3.2.2), but the exact text encoders, similarity thresholds, and matching settings are not specified.
  • Why it matters: Unclear configuration could affect EA/OA scores (technical clarity/reproducibility).- Potential metric entanglement and cascading errors across chained tools without ablations
  • Evidence: Entanglement acknowledged (Section 4.4), but no ablation quantifies pipeline impact (e.g., replacing MLLM event extraction with human labels).
  • Why it matters: Without ablations, it’s hard to separate alignment vs. consistency effects (technical soundness).
  • Evidence: Event extraction and description depend on Qwen2.5-VL and DeepSeek-V3.1 (Section 3.2.2; Appendix C.2–C.3).
  • Why it matters: Cascading errors from mis-descriptions can skew EA (experimental reliability).
  • Evidence: HERD polarity relies on DeepSeek-V3.1 (Appendix B.8; C.6).
  • Why it matters: Polarity misclassification can systematically bias HERD scores (metric validity).- Dataset scale and representativeness; unclear generated video lengths impacting comparability
  • Evidence: 240 prompts across 18 themes collected from 30–60s videos (Section 3.1; Fig. 2).
  • Why it matters: Smaller sample size than some benchmarks (Table 1: e.g., VMBench 1050; VBench-Long 946) may limit statistical power (experimental scope).
  • Evidence: DOVER++ evaluation requires <10s segmentation (Section 3.2.1; Page following Fig. 1), but standardized output durations across baselines are not stated.
  • Why it matters: No direct evidence found in the manuscript of fixed output lengths can affect comparability across models (fairness).
  • Evidence: Real-world short videos as prompt bases may bias content types (Section 3.1; Fig. 2).
  • Why it matters: May under-represent cinematic multi-minute narratives the benchmark targets (generalizability).- Normalization and scaling consistency may conflate absolute interpretability
  • Evidence: Aesthetic RR-UB derived from Text-to-Image-2M data_1024_10K (Appendix B.5), but the final numeric RR-UB value used for scaling is not reported.
  • Why it matters: Without the reference value, AQ scaling cannot be independently verified (technical soundness).
  • Evidence: “All values are expressed as percentages” (Table 2–4), but normalization schemes and orientation for several metrics—especially error-type metrics like Warping Error—are not detailed (Section 3.2.3; Table 3).
  • Why it matters: Ambiguous “higher/lower is better” and conversion to percentages risks misinterpretation and uneven weighting (clarity/rigor).
  • Evidence: HERD scores count only polarity-consistent yes/no answers and ignore “unclear” (Section 3.2.5).
  • Why it matters: Excluding ambiguity may inflate scores if “unclear” is frequent, affecting comparability across models (metric validity).- Clarity issues and minor inconsistencies in terminology/figures
  • Evidence: “HERO” appears in radar labels and sub-dimensions with mismatches (Appendix B.9–B.10; Fig. 7), while the text defines “HERD” and different sub-dimension names (Section 3.2.5; Appendix B.8).
  • Why it matters: Naming/sub-dimension inconsistencies can confuse readers and implementations (clarity).
  • Evidence: TQ abbreviation is used for both Technical Quality (Static Quality; Section 3.2.1) and Temporal Quality heading (Section 3.2.3).
  • Why it matters: Ambiguous abbreviations degrade interpretability (clarity).
  • Evidence: Case studies use “HPSV2” (Appendix E; Fig. 10–11) without definition; the CC prompt header labels “HERD Evaluation Prompt” though it defines CC dimensions (Appendix C.5); Table 5 refers to “three representative methods” without naming them (Section 4.3; Table 5).
  • Why it matters: Undefined terms, mislabeling, and unspecified sample selection hamper transparency and replication (clarity).- Reproducibility and openness remain incomplete at submission time
  • Evidence: Section 7 states only an initial prompt suite JSON is provided; full code release will follow reorganization.
  • Why it matters: Review-time replication is limited (reproducibility).
  • Evidence: Heavy use of specific MLLMs (Qwen2.5-VL, DeepSeek-V3.1, Seed1.5-VL; Sections 3.1, 3.2.2, 3.2.5; Appendix B.8) may hinder exact reproduction if versions/access change.
  • Why it matters: Dependency on evolving models complicates stability (reproducibility).
  • Evidence: Baseline generation details lack seeds, hardware specs, and standardized output lengths (Appendix B.1; Section 3.2.1 for segmentation protocol).
  • Why it matters: No direct evidence found in the manuscript of standardized generation settings may affect fairness and repeatability (experimental rigor).Suggestions for Improvement
- Add human-grounded validation to calibrate MLLM-based metrics
  • Conduct a user study comparing HERD and content clarity scores against human judgments on a stratified subset of videos (Sections 3.2.4–3.2.5; Table 4), reporting correlations and variance to quantify potential bias.
  • Clarify and validate the evaluator source for high-level assessments—specify whether Seed1.5-VL (Section 3.1) or Qwen2.5-VL-72B (Appendix B.8) produced the prompt-integrated evaluations—and report agreement statistics on a sample.
  • Include inter-rater reliability (e.g., Cohen’s κ) and provide detailed protocols; if infeasible, state this limitation explicitly.- Specify evaluation hyperparameters and implementation details
  • Report the number of rounds R in Eq. (2), the randomization controls, and the evaluator model used for content clarity (Section 3.2.4; Appendix C.5) to enable exact reproduction.
  • Provide numeric settings for transition smoothness (Appendix B.6: α1–α4, k, b, c) and justify choices via a sensitivity analysis.
  • Detail the embedding/encoder models, similarity metrics, thresholds, and matching settings used in OA/EA (Section 3.2.2; Appendix C.2–C.3), including ablation on alternatives.- Quantify and mitigate metric entanglement with ablations
  • Replace LLM-based event extraction with human-labeled events on a subset to measure EA sensitivity (Section 3.2.2; Section 4.4).
  • Swap specific tools in the pipeline (e.g., Time-R1 vs. alternatives for temporal grounding; Grounded-SAM-2 vs. SAM variants) and report variance in intra/inter-event consistency (Section 3.2.3; Appendix B.7).
  • Evaluate HERD with alternative polarity sources and cross-check consistency (Section 3.2.5; Appendix B.8), reporting error bars.- Strengthen dataset scope and standardize output durations
  • Expand beyond 240 prompts or provide a statistical power analysis justifying sample size (Section 3.1; Fig. 2; Table 1), and include more multi-minute narrative bases when feasible.
  • Enforce standardized generated video lengths per prompt and document exception handling (Section 3.2.1), ensuring comparable evaluation across baselines.
  • Diversify sources beyond YouTube 30–60s material and document coverage of cinematic structures to better match benchmark goals (Section 3.1).- Improve normalization and cross-metric comparability
  • Validate RR-UB for AQ against a curated high-quality video frame set and report the numeric RR-UB value used (Appendix B.5), including alignment with human aesthetic judgments.
  • Document normalization schemes and orientation (“higher/lower is better”) for all metrics—including error-type metrics like Warping Error—and provide a unified scaling protocol (beyond Eq. 2 and Appendix B.6/B.7) with rationale (Tables 2–4; Section 3.2.3; Table 3).
  • In HERD, report the proportion of “unclear” answers per model and consider weighted penalties for high ambiguity to avoid inflating scores (Section 3.2.5).- Resolve terminology and figure inconsistencies
  • Harmonize “HERD” naming and sub-dimensions across figures and text (Appendix B.9–B.10; Fig. 7; Section 3.2.5), and clarify which model produced the high-level evaluations integrated into prompts (Section 3.1 vs. Appendix B.8).
  • Avoid ambiguous abbreviations like dual-use “TQ” (Sections 3.2.1, 3.2.3), ensure undefined terms used in case studies (e.g., “HPSV2”) are defined or removed, and complete tables with missing values (Appendix B.3; Table 5 method identification).
  • Add a concise glossary mapping all abbreviations to dimensions and sub-metrics at first use (Fig. 1; Sections 3.2.*).- Enhance reproducibility and fairness details
  • Release full code, prompts, and evaluation scripts at camera-ready; include versioned configs for all MLLMs/tools (Section 7).
  • Document seeds, hardware, inference parameters, and standardized output lengths for each baseline (Appendix B.1; Section 3.2.1), and provide Docker/conda environments to stabilize dependencies.
  • Where access to specific MLLMs is limited, supply distilled or open alternatives and report cross-model robustness of scores (Sections 3.1, 3.2.2, 3.2.5; Appendix B.8).Score
- Overall (10): 7 — Substantive benchmark with novel event-level and high-level metrics (Section 3.2; Eq. 1; Tables 2–4; Fig. 1), offset by MLLM reliance without human validation and incomplete reproducibility/clarity details (Sections 3.2.4–3.2.5; Section 7; Appendix B.9–B.10).
- Novelty (10): 8 — Event-level alignment with order penalty (Eq. 1), transition smoothness (Appendix B.6), intra/inter-event consistency (Appendix B.7), and polarity-aware HERD (Section 3.2.5) extend long-form evaluation beyond prior suites (Table 1; Section 1).
- Technical Quality (10): 6 — Sound formulations and analyses (Sections 3.2.2–3.2.5; Table 5) but missing key specifics (Section 3.2.4; Appendix B.6), limited ablations (Section 4.4), and no human validation.
- Clarity (10): 7 — Generally clear structure and extensive appendices (Appendix B–C), with inconsistencies (HERO vs. HERD; dual “TQ”; undefined “HPSV2”; CC prompt mislabeling; unspecified methods in Table 5) (Appendix B.9–B.10; C.5; E; Sections 3.2.1, 3.2.3; Section 4.3).
- Confidence (5): 3 — Moderate confidence based on detailed manuscript and results (Tables 2–4; Fig. 7), but absent human studies and pending full code release (Section 7) limit verification.