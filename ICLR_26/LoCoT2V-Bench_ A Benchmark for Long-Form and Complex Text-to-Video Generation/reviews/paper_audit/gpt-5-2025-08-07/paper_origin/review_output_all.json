{
  "baseline_review": "Summary\n- The paper introduces LoCoT2V-Bench, a benchmark for evaluating long-form text-to-video generation under complex prompts. It builds a 240-video prompt suite from real-world YouTube content (30–60s) and uses MLLMs to generate and refine long, multi-scene prompts (Section 3.1; Fig. 2; Table 1; Appendix B.2–B.4). The evaluation framework spans five dimensions—static quality, text–video alignment, temporal quality, content clarity, and Human Expectation Realization Degree (HERD)—with novel components such as event-level alignment with order penalties (Eq. 1), transition smoothness (Appendix B.6), intra/inter-event temporal consistency (Appendix B.7), content clarity via repeated MLLM MOS (Eq. 2; Section 3.2.4), and polarity-aware HERD (Section 3.2.5; Appendix B.8). Nine representative methods are benchmarked across 26 sub-dimensions (Table 2–4; Fig. 7), with analyses on theme categories, correlations, entanglement, and prompt complexity (Sections 4.2–4.5; Table 5; Fig. 3, 5, 8, 9). A reproducibility statement promises code and data release (Section 7).Strengths\n- Bold focus on long-form, complex prompts\n  • Evidence: The prompt suite averages 236.66 words and scores highest in complexity across benchmarks (Table 1), with distribution shown in Fig. 4 (Appendix B.2) and coverage of multi-scene elements (Section 3.1; “200–300 words” claim; Appendix B.2).\n  • Why it matters: Addresses a notable gap in current evaluation (novelty/impact) by moving beyond short, single-scene prompts.- Multi-dimensional framework extends evaluation to high-level attributes\n  • Evidence: Five major dimensions and 26 sub-dimensions are defined (Fig. 1; Section 3.2), introducing content clarity (Section 3.2.4) and HERD for emotional/narrative aspects (Section 3.2.5; Appendix B.8).\n  • Why it matters: Captures abstract qualities (impact) often missed by visual/temporal metrics; improves holistic assessment.- Novel event-level alignment with temporal order penalty\n  • Evidence: Event-level alignment formalization with Hungarian matching and inversion penalty in Eq. (1); event definition and extraction details (Section 3.2.2; Appendix C.2–C.3).\n  • Why it matters: Fine-grained adherence and order sensitivity are critical for multi-scene long videos (technical soundness/novelty).- Temporal quality suite includes transition smoothness and intra/inter-event consistency\n  • Evidence: Transition smoothness pipeline with multi-cue similarity and RAFT motion consistency (Appendix B.6); intra/inter-event subject/background consistency via Time-R1-7B and Grounded-SAM-2 (Section 3.2.3; Appendix B.7).\n  • Why it matters: Moves beyond short-term smoothness into long-range coherence across events (technical depth/impact).- Polarity-aware HERD scoring to mitigate yes/no bias\n  • Evidence: Polarity annotation and scoring scheme (Section 3.2.5; Appendix B.8, C.6).\n  • Why it matters: Reduces systematic scoring bias in binary QA (technical soundness/rigor).- Comprehensive benchmarking across diverse methods with detailed results\n  • Evidence: Nine baselines evaluated (Appendix B.1), aggregated dimension scores (Table 2), temporal sub-dimensions (Table 3), and HERD sub-dimensions (Table 4); visualizations (Fig. 7; Appendix B.9–B.10; Table 6).\n  • Why it matters: Offers actionable insights into where current models fail (experimental rigor/impact).- Analytical studies on correlations, entanglement, and thematic effects\n  • Evidence: Correlation analyses between static quality and other dimensions (Section 4.3; Fig. 3; Table 5); entanglement discussion (Section 4.4; Table 5); theme category comparison (Section 4.2; Fig. 5; Fig. 8; Table 6).\n  • Why it matters: Diagnoses metric interactions and robustness, guiding future research (clarity/impact).- Ethical and reproducibility intentions\n  • Evidence: Ethical statement (Section 6) and planned release of data/code/results (Section 7; link provided).\n  • Why it matters: Supports responsible, repeatable science (rigor/community impact).Weaknesses\n- Heavy reliance on MLLMs for both prompt construction and evaluation without human validation\n  • Evidence: Prompts and refinements via MLLMs (Section 3.1: “employ them directly to generate raw prompts… self-refine”; Appendix B.4); HERD and content clarity are MLLM-based (Section 3.2.4–3.2.5; Appendix C.5–C.6).\n  • Why it matters: Potential circularity/bias and lack of human-grounded calibration for abstract dimensions (experimental rigor/validity).\n  • Evidence: Integration of Seed1.5-VL assessments into prompts (Section 3.1: “employ Seed1.5-VL… evaluations… integrated”).\n  • Why it matters: Using model-derived “human expectation” proxies may embed model-specific biases (validity).\n  • Evidence: No user study validation or correlation with human judgments is reported.\n  • Why it matters: No direct evidence found in the manuscript of human validation reduces confidence in high-level metrics (technical quality/impact).- Ambiguities and incomplete specification of evaluation hyperparameters\n  • Evidence: Content clarity repeats “prompted multiple times with controlled randomness” but R in Eq. (2) is undefined in practice (Section 3.2.4).\n  • Why it matters: Missing R undermines reproducibility and variance analysis (technical rigor).\n  • Evidence: Transition smoothness formulae given (Appendix B.6, Eq. 3–6), but numeric values for α1–α4, window size k, and b,c are not specified.\n  • Why it matters: Without fixed parameters, comparisons may vary by implementation (reproducibility).\n  • Evidence: Event-level alignment mentions “embedding models” for similarity and “field-level similarity” (Section 3.2.2) but omits the exact embedding model(s), similarity thresholds, and matching settings.\n  • Why it matters: Unclear configuration could affect EA scores (technical clarity/reproducibility).- Potential metric entanglement and cascading errors across chained tools without ablations\n  • Evidence: Entanglement acknowledged (Section 4.4), but no ablation to quantify impact across pipelines (e.g., replacing MLLM event extraction with human labels).\n  • Why it matters: Without ablations, it’s hard to separate alignment vs. consistency effects (technical soundness).\n  • Evidence: Event extraction depends on Qwen2.5-VL descriptions and DeepSeek-V3.1 (Section 3.2.2; Appendix C.2–C.3).\n  • Why it matters: Cascading errors from LLM mis-descriptions can skew EA (experimental reliability).\n  • Evidence: HERD polarity relies on DeepSeek-V3.1 (Appendix B.8; C.6).\n  • Why it matters: Polarity misclassification can systematically bias HERD scores (metric validity).- Dataset scale and representativeness; unclear generated video lengths impacting comparability\n  • Evidence: 240 prompts across 18 themes collected from 30–60s YouTube videos (Section 3.1; Fig. 2).\n  • Why it matters: Smaller sample size than some benchmarks (Table 1: e.g., VMBench 1050, VBench-Long 946) could limit statistical power (experimental scope).\n  • Evidence: No standardized output duration across baselines; DOVER++ evaluation requires segmentation <10s (Section 3.2.1).\n  • Why it matters: No direct evidence found in the manuscript of fixed output lengths can affect metric comparability (fairness).\n  • Evidence: Real-world short videos as prompt bases may bias content types (Section 3.1; Fig. 2).\n  • Why it matters: May under-represent cinematic multi-minute narratives the benchmark claims to target (generalizability).- Normalization and scaling consistency may conflate absolute interpretability\n  • Evidence: Aesthetic RR-UB derived from Text-to-Image-2M data_1024_10K (Appendix B.5).\n  • Why it matters: Using T2I image distributions to set video frame upper bounds may miscalibrate AQ for videos (technical soundness).\n  • Evidence: “All values are expressed as percentages” (Table 2–4), but normalization schemes for many sub-metrics (beyond Eq. 2 and B.6/B.7 descriptions) are not detailed.\n  • Why it matters: No direct evidence found in the manuscript for consistent normalization across heterogeneous metrics risks uneven weighting (clarity/rigor).\n  • Evidence: HERD counts only polarity-consistent yes/no answers; “unclear” excluded (Section 3.2.5).\n  • Why it matters: Exclusion may inflate scores if ambiguity is frequent, affecting comparability across models (metric validity).- Clarity issues and minor inconsistencies in terminology/figures\n  • Evidence: “HERO” appears in radar labels (Appendix B.9–B.10: “HERO Results”; and Fig. 7 description).\n  • Why it matters: Terminology inconsistency can confuse readers (clarity).\n  • Evidence: TQ is used for both Technical Quality (Section 3.2.1) and Temporal Quality heading (Section 3.2.3).\n  • Why it matters: Ambiguous abbreviations reduce interpretability (clarity).\n  • Evidence: Case study metrics include “HPSV2” (Appendix E; Fig. 10–11) without a definition in the main text.\n  • Why it matters: No direct evidence found in the manuscript explaining HPSV2 hampers transparency (clarity).- Reproducibility and openness remain incomplete at submission time\n  • Evidence: Section 7 states only an initial prompt suite JSON is provided; full code release will follow reorganization.\n  • Why it matters: Review-time replication is limited (reproducibility).\n  • Evidence: Heavy use of specific MLLMs (Qwen2.5-VL, DeepSeek-V3.1, Seed1.5-VL; Sections 3.1, 3.2.2, 3.2.5) may hinder exact reproduction if versions or access change.\n  • Why it matters: Dependency on evolving proprietary/open models complicates stability (reproducibility).\n  • Evidence: Baseline generation details lack seeds, hardware specs, and standardized output lengths (Appendix B.1).\n  • Why it matters: No direct evidence found in the manuscript of standardized generation settings may affect fairness and repeatability (experimental rigor).Suggestions for Improvement\n- Add human-grounded validation to calibrate MLLM-based metrics\n  • Conduct a user study comparing HERD and content clarity scores against human judgments on a stratified subset of videos (Sections 3.2.4–3.2.5; Table 4) to quantify correlation and potential bias.\n  • Validate the Seed1.5-VL-derived prompt-integrated “expectations” (Section 3.1) with human annotations for a sample of prompts and report agreement statistics.\n  • Include inter-rater reliability (e.g., Cohen’s κ) and provide protocols to ensure replicability; if infeasible, report “No direct evidence found” limitations explicitly in the paper.- Specify evaluation hyperparameters and implementation details\n  • Report the number of rounds R in Eq. (2) and the randomization controls used in content clarity (Section 3.2.4) to enable exact reproduction.\n  • Provide numeric settings for transition smoothness (Appendix B.6: α1–α4, k, b, c) and justify choices via a sensitivity analysis.\n  • Detail the embedding model(s), similarity metrics, and thresholds used in event-level alignment (Section 3.2.2; Appendix C.2–C.3), including ablation on alternatives.- Quantify and mitigate metric entanglement with ablations\n  • Replace LLM-based event extraction with human-labeled events on a subset to measure EA sensitivity (Section 3.2.2; Section 4.4).\n  • Swap specific tools in the pipeline (e.g., Time-R1 vs. alternative temporal grounding, Grounded-SAM-2 vs. SAM variants) and report variance in intra/inter-event consistency (Section 3.2.3; Appendix B.7).\n  • Evaluate HERD with alternative polarity sources and cross-check consistency (Section 3.2.5; Appendix B.8), reporting error bars.- Strengthen dataset scope and standardize output durations\n  • Expand beyond 240 prompts or provide statistical power analysis justifying sample size (Section 3.1; Fig. 2; Table 1) and include more multi-minute narrative bases when feasible.\n  • Enforce standardized generated video lengths (e.g., fixed target durations per prompt) and report any exception handling (Section 3.2.1), ensuring comparable evaluation across baselines.\n  • Diversify sources beyond YouTube 30–60s material and document coverage of cinematic structures to better match benchmark goals (Section 3.1).- Improve normalization and cross-metric comparability\n  • Validate RR-UB for AQ against a curated high-quality video frame set and report alignment with human aesthetic judgments (Appendix B.5).\n  • Document normalization schemes for all metrics and provide a unified scaling protocol (beyond Eq. 2 and Appendix B.6/B.7), including min–max or z-score strategies per metric with rationale (Tables 2–4).\n  • In HERD, report the proportion of “unclear” answers per model and consider weighted penalties for high ambiguity to avoid inflating scores (Section 3.2.5).- Resolve terminology and figure inconsistencies\n  • Harmonize “HERD” naming across all figures and text (Appendix B.9–B.10; Fig. 7) and avoid ambiguous abbreviations like dual-use “TQ” (Sections 3.2.1, 3.2.3).\n  • Define “HPSV2” or remove it from case study figures (Appendix E; Fig. 10–11) to prevent confusion.\n  • Add a concise glossary mapping all abbreviations to dimensions and sub-metrics at first use (Fig. 1; Sections 3.2.*).- Enhance reproducibility and fairness details\n  • Release full code, prompts, and evaluation scripts at camera-ready; include versioned configs for all MLLMs and tools (Section 7).\n  • Document seeds, hardware, inference parameters, and standardized output lengths for each baseline (Appendix B.1), and provide Docker/conda environments to stabilize dependencies.\n  • Where access to specific MLLMs is limited, supply distilled or open alternatives and report cross-model robustness of scores (Sections 3.1, 3.2.2, 3.2.5).Score\n- Overall (10): 7 — Strong benchmark contributions with novel event-level and high-level metrics (Section 3.2; Eq. 1; Tables 2–4; Fig. 1), tempered by reliance on MLLMs without human validation and incomplete reproducibility details (Sections 3.2.4–3.2.5; Section 7).\n- Novelty (10): 8 — New event-level alignment with order penalty (Eq. 1), transition smoothness (Appendix B.6), intra/inter-event consistency (Appendix B.7), and polarity-aware HERD (Section 3.2.5) advance long-form evaluation beyond prior suites (Table 1; Section 1).\n- Technical Quality (10): 6 — Solid formulations and analyses (Sections 3.2.2–3.2.5; Table 5) but missing hyperparameter specifics (Section 3.2.4; Appendix B.6), limited ablations (Section 4.4), and no human validation.\n- Clarity (10): 7 — Generally clear structure and extensive appendices (Appendix B–C), with minor inconsistencies (HERO vs. HERD; dual “TQ”; undefined “HPSV2”) (Appendix B.9–B.10; E; Sections 3.2.1, 3.2.3).\n- Confidence (5): 3 — Moderate confidence based on comprehensive manuscript detail and results (Tables 2–4; Fig. 7), but absent human studies and pending full code release (Section 7) limit verification.",
  "final_review": "Summary\n- The paper introduces LoCoT2V-Bench, a benchmark for evaluating long-form text-to-video generation under complex prompts. It builds a 240-prompt suite derived from 30–60s real-world YouTube videos and uses MLLMs to generate and self-refine long, multi-scene prompts (Section 3.1; Fig. 2; Table 1; Appendix B.2–B.4). The evaluation spans five dimensions—static quality, text–video alignment (overall and event-level), temporal quality, content clarity, and Human Expectation Realization Degree (HERD)—with new components such as event-level alignment with order penalties (Eq. 1), transition smoothness (Appendix B.6), intra/inter-event temporal consistency (Appendix B.7), content clarity via repeated MLLM MOS (Eq. 2; Section 3.2.4), and polarity-aware HERD (Section 3.2.5; Appendix B.8). Nine methods are benchmarked across 26 sub-dimensions (Tables 2–4; Fig. 7), with analyses on themes, correlations, entanglement, and prompt complexity (Sections 4.2–4.5; Table 5; Fig. 3, 5, 8, 9). Section 7 states an initial JSON prompt release, with full code/data to follow.Strengths\n- Bold focus on long-form, complex prompts\n  • Evidence: Prompts average 236.66 words and rank highest in complexity among benchmarks (Table 1), with length distribution in Fig. 4 (Appendix B.2) and multi-scene coverage detailed in Section 3.1 and Appendix B.2.\n  • Why it matters: Moves evaluation beyond short, single-scene prompts, addressing a recognized gap (novelty/impact).\n  • Evidence: The suite spans 18 themes grouped into three categories (Section 3.1; Fig. 2).\n  • Why it matters: Broad topical coverage improves generality (impact).\n  • Evidence: Instructions and self-refine paradigm with manual review (Section 3.1).\n  • Why it matters: Enhances prompt quality under complex, realistic settings (rigor).- Multi-dimensional framework extends evaluation to high-level attributes\n  • Evidence: Five major dimensions and 26 sub-dimensions (Fig. 1; Section 3.2), including content clarity (Section 3.2.4) and HERD (Section 3.2.5; Appendix B.8).\n  • Why it matters: Abstract qualities (clarity, narrative/thematic) are often overlooked; this framework adds them (impact).\n  • Evidence: CC comprises TC, LS, ICP, ICS with a formal aggregation (Eq. 2; Section 3.2.4).\n  • Why it matters: Structured scoring improves interpretability and replicability (technical soundness/clarity).\n  • Evidence: HERD defines seven human-centric sub-dimensions (Section 3.2.5; Appendix B.8).\n  • Why it matters: Captures narrative/emotional adherence beyond low-level metrics (impact).- Novel event-level alignment with temporal order penalty\n  • Evidence: EA formulation with Hungarian matching and inversion penalty (Eq. 1); event schema and extraction (Section 3.2.2; Appendix C.2–C.3).\n  • Why it matters: Order-aware, fine-grained adherence is crucial for multi-scene long videos (novelty/technical soundness).\n  • Evidence: Ground-truth events extracted from prompt base via DeepSeek-V3.1 (Section 3.2.2; Appendix C.3).\n  • Why it matters: Aligns evaluation to intended event structure (rigor).\n  • Evidence: OA uses MLLMs to produce detailed video descriptions before similarity scoring (Section 3.2.2; Appendix C.2).\n  • Why it matters: Improves semantic fidelity over CLIP-only baselines (impact).- Temporal quality suite includes transition smoothness and intra/inter-event consistency\n  • Evidence: Transition smoothness combines pixel, structural, feature, and motion cues and averages across transitions (Appendix B.6); intra/inter-event SC/BC computed with Time-R1-7B and Grounded-SAM-2 (Section 3.2.3; Appendix B.7).\n  • Why it matters: Evaluates long-range coherence across events and scene transitions (technical depth/impact).\n  • Evidence: Warping error and semantic consistency follow EvalCrafter (Section 3.2.3).\n  • Why it matters: Leverages established temporal consistency baselines (rigor).\n  • Evidence: Human Action evaluation via MLLMs with extraction/verification prompts (Section 3.2.3; Appendix C.4).\n  • Why it matters: Enables assessment of complex, prompt-conditioned actions (impact).- Polarity-aware HERD scoring to mitigate yes/no bias\n  • Evidence: Polarity annotation and scoring scheme (Section 3.2.5; Appendix B.8; Appendix C.6).\n  • Why it matters: Reduces bias from question framing in binary VQA (technical soundness/rigor).\n  • Evidence: Multiple questions per dimension (Appendix B.8).\n  • Why it matters: Improves robustness via redundancy (rigor).\n  • Evidence: Integration of dimension definitions (Appendix B.8).\n  • Why it matters: Clear criteria support consistent evaluation (clarity).- Comprehensive benchmarking with detailed results\n  • Evidence: Nine baselines (Appendix B.1), dimension aggregates (Table 2), temporal sub-dimensions (Table 3), HERD sub-dimensions (Table 4), visualizations (Fig. 7; Appendix B.9–B.10; Table 6).\n  • Why it matters: Offers granular insights into strengths/weaknesses of current models (experimental rigor/impact).\n  • Evidence: Low event-level alignment vs overall alignment across methods (Tables 2–3).\n  • Why it matters: Highlights where models fail under fine-grained adherence (impact).\n  • Evidence: Consistently low narrative flow across HERD (Table 4).\n  • Why it matters: Identifies persistent shortcomings in high-level narrative coherence (impact).- Analytical studies on correlations, entanglement, and thematic effects\n  • Evidence: Correlations between SQ and other dimensions (Section 4.3; Fig. 3; Table 5); entanglement analysis (Section 4.4; Table 5); theme category comparison (Section 4.2; Fig. 5; Fig. 8; Table 6).\n  • Why it matters: Diagnoses metric interactions and potential biases (clarity/impact).\n  • Evidence: Prompt complexity effects (Section 4.5; Fig. 9).\n  • Why it matters: Explores evaluation sensitivity to prompt semantics/structure/control (rigor).\n  • Evidence: Low correlation between EA and inter-event consistency (Table 5).\n  • Why it matters: Suggests separable failure modes (technical soundness).- Ethical and reproducibility intentions\n  • Evidence: Ethical statement (Section 6) and planned release of data/code/results (Section 7; link).\n  • Why it matters: Supports responsible, repeatable science (community impact).\n  • Evidence: Initial prompt JSON provided at submission (Section 7).\n  • Why it matters: Enables partial verification (reproducibility).\n  • Evidence: Tool/model citations and appendix formulas (References; Appendices B.6–B.7).\n  • Why it matters: Improves transparency of evaluation components (clarity/rigor).Weaknesses\n- Heavy reliance on MLLMs for both prompt construction and evaluation without human validation\n  • Evidence: Prompts and refinements via MLLMs with self-refine and manual review (Section 3.1: “employ them directly to generate raw prompts… self-refine”), and HERD/content clarity are MLLM-based (Section 3.2.4–3.2.5; Appendix C.5–C.6).\n  • Why it matters: Risks circularity/bias and lacks human-grounded calibration for abstract dimensions (experimental rigor/validity).\n  • Evidence: Source inconsistency in high-level assessments: Section 3.1 integrates Seed1.5-VL evaluations into prompts, whereas Appendix B.8 states Qwen2.5-VL-72B was used to generate dimension-wise assessments.\n  • Why it matters: Conflicting evaluator sources undermine the correctness of HERD setup and prompt-integrated “expectations” (validity/clarity).\n  • Evidence: No user study or correlation with human judgments is reported.\n  • Why it matters: No direct evidence found in the manuscript of human validation reduces confidence in high-level metrics (technical quality/impact).- Ambiguities and incomplete specification of evaluation hyperparameters\n  • Evidence: Content clarity repeats “prompted multiple times with controlled randomness,” but R in Eq. (2) is not specified; the evaluator model for CC is also not named (Section 3.2.4; Appendix C.5).\n  • Why it matters: Missing R/model details undermine reproducibility and variance analysis (technical rigor).\n  • Evidence: Transition smoothness equations provided (Appendix B.6, Eq. 3–6), but numeric values for α1–α4, window size k, and b,c are not specified.\n  • Why it matters: Without fixed parameters, comparisons may vary by implementation (reproducibility).\n  • Evidence: Event-level alignment mentions “embedding models” and field-level similarity (Section 3.2.2), but the exact text encoders, similarity thresholds, and matching settings are not specified.\n  • Why it matters: Unclear configuration could affect EA/OA scores (technical clarity/reproducibility).- Potential metric entanglement and cascading errors across chained tools without ablations\n  • Evidence: Entanglement acknowledged (Section 4.4), but no ablation quantifies pipeline impact (e.g., replacing MLLM event extraction with human labels).\n  • Why it matters: Without ablations, it’s hard to separate alignment vs. consistency effects (technical soundness).\n  • Evidence: Event extraction and description depend on Qwen2.5-VL and DeepSeek-V3.1 (Section 3.2.2; Appendix C.2–C.3).\n  • Why it matters: Cascading errors from mis-descriptions can skew EA (experimental reliability).\n  • Evidence: HERD polarity relies on DeepSeek-V3.1 (Appendix B.8; C.6).\n  • Why it matters: Polarity misclassification can systematically bias HERD scores (metric validity).- Dataset scale and representativeness; unclear generated video lengths impacting comparability\n  • Evidence: 240 prompts across 18 themes collected from 30–60s videos (Section 3.1; Fig. 2).\n  • Why it matters: Smaller sample size than some benchmarks (Table 1: e.g., VMBench 1050; VBench-Long 946) may limit statistical power (experimental scope).\n  • Evidence: DOVER++ evaluation requires <10s segmentation (Section 3.2.1; Page following Fig. 1), but standardized output durations across baselines are not stated.\n  • Why it matters: No direct evidence found in the manuscript of fixed output lengths can affect comparability across models (fairness).\n  • Evidence: Real-world short videos as prompt bases may bias content types (Section 3.1; Fig. 2).\n  • Why it matters: May under-represent cinematic multi-minute narratives the benchmark targets (generalizability).- Normalization and scaling consistency may conflate absolute interpretability\n  • Evidence: Aesthetic RR-UB derived from Text-to-Image-2M data_1024_10K (Appendix B.5), but the final numeric RR-UB value used for scaling is not reported.\n  • Why it matters: Without the reference value, AQ scaling cannot be independently verified (technical soundness).\n  • Evidence: “All values are expressed as percentages” (Table 2–4), but normalization schemes and orientation for several metrics—especially error-type metrics like Warping Error—are not detailed (Section 3.2.3; Table 3).\n  • Why it matters: Ambiguous “higher/lower is better” and conversion to percentages risks misinterpretation and uneven weighting (clarity/rigor).\n  • Evidence: HERD scores count only polarity-consistent yes/no answers and ignore “unclear” (Section 3.2.5).\n  • Why it matters: Excluding ambiguity may inflate scores if “unclear” is frequent, affecting comparability across models (metric validity).- Clarity issues and minor inconsistencies in terminology/figures\n  • Evidence: “HERO” appears in radar labels and sub-dimensions with mismatches (Appendix B.9–B.10; Fig. 7), while the text defines “HERD” and different sub-dimension names (Section 3.2.5; Appendix B.8).\n  • Why it matters: Naming/sub-dimension inconsistencies can confuse readers and implementations (clarity).\n  • Evidence: TQ abbreviation is used for both Technical Quality (Static Quality; Section 3.2.1) and Temporal Quality heading (Section 3.2.3).\n  • Why it matters: Ambiguous abbreviations degrade interpretability (clarity).\n  • Evidence: Case studies use “HPSV2” (Appendix E; Fig. 10–11) without definition; the CC prompt header labels “HERD Evaluation Prompt” though it defines CC dimensions (Appendix C.5); Table 5 refers to “three representative methods” without naming them (Section 4.3; Table 5).\n  • Why it matters: Undefined terms, mislabeling, and unspecified sample selection hamper transparency and replication (clarity).- Reproducibility and openness remain incomplete at submission time\n  • Evidence: Section 7 states only an initial prompt suite JSON is provided; full code release will follow reorganization.\n  • Why it matters: Review-time replication is limited (reproducibility).\n  • Evidence: Heavy use of specific MLLMs (Qwen2.5-VL, DeepSeek-V3.1, Seed1.5-VL; Sections 3.1, 3.2.2, 3.2.5; Appendix B.8) may hinder exact reproduction if versions/access change.\n  • Why it matters: Dependency on evolving models complicates stability (reproducibility).\n  • Evidence: Baseline generation details lack seeds, hardware specs, and standardized output lengths (Appendix B.1; Section 3.2.1 for segmentation protocol).\n  • Why it matters: No direct evidence found in the manuscript of standardized generation settings may affect fairness and repeatability (experimental rigor).Suggestions for Improvement\n- Add human-grounded validation to calibrate MLLM-based metrics\n  • Conduct a user study comparing HERD and content clarity scores against human judgments on a stratified subset of videos (Sections 3.2.4–3.2.5; Table 4), reporting correlations and variance to quantify potential bias.\n  • Clarify and validate the evaluator source for high-level assessments—specify whether Seed1.5-VL (Section 3.1) or Qwen2.5-VL-72B (Appendix B.8) produced the prompt-integrated evaluations—and report agreement statistics on a sample.\n  • Include inter-rater reliability (e.g., Cohen’s κ) and provide detailed protocols; if infeasible, state this limitation explicitly.- Specify evaluation hyperparameters and implementation details\n  • Report the number of rounds R in Eq. (2), the randomization controls, and the evaluator model used for content clarity (Section 3.2.4; Appendix C.5) to enable exact reproduction.\n  • Provide numeric settings for transition smoothness (Appendix B.6: α1–α4, k, b, c) and justify choices via a sensitivity analysis.\n  • Detail the embedding/encoder models, similarity metrics, thresholds, and matching settings used in OA/EA (Section 3.2.2; Appendix C.2–C.3), including ablation on alternatives.- Quantify and mitigate metric entanglement with ablations\n  • Replace LLM-based event extraction with human-labeled events on a subset to measure EA sensitivity (Section 3.2.2; Section 4.4).\n  • Swap specific tools in the pipeline (e.g., Time-R1 vs. alternatives for temporal grounding; Grounded-SAM-2 vs. SAM variants) and report variance in intra/inter-event consistency (Section 3.2.3; Appendix B.7).\n  • Evaluate HERD with alternative polarity sources and cross-check consistency (Section 3.2.5; Appendix B.8), reporting error bars.- Strengthen dataset scope and standardize output durations\n  • Expand beyond 240 prompts or provide a statistical power analysis justifying sample size (Section 3.1; Fig. 2; Table 1), and include more multi-minute narrative bases when feasible.\n  • Enforce standardized generated video lengths per prompt and document exception handling (Section 3.2.1), ensuring comparable evaluation across baselines.\n  • Diversify sources beyond YouTube 30–60s material and document coverage of cinematic structures to better match benchmark goals (Section 3.1).- Improve normalization and cross-metric comparability\n  • Validate RR-UB for AQ against a curated high-quality video frame set and report the numeric RR-UB value used (Appendix B.5), including alignment with human aesthetic judgments.\n  • Document normalization schemes and orientation (“higher/lower is better”) for all metrics—including error-type metrics like Warping Error—and provide a unified scaling protocol (beyond Eq. 2 and Appendix B.6/B.7) with rationale (Tables 2–4; Section 3.2.3; Table 3).\n  • In HERD, report the proportion of “unclear” answers per model and consider weighted penalties for high ambiguity to avoid inflating scores (Section 3.2.5).- Resolve terminology and figure inconsistencies\n  • Harmonize “HERD” naming and sub-dimensions across figures and text (Appendix B.9–B.10; Fig. 7; Section 3.2.5), and clarify which model produced the high-level evaluations integrated into prompts (Section 3.1 vs. Appendix B.8).\n  • Avoid ambiguous abbreviations like dual-use “TQ” (Sections 3.2.1, 3.2.3), ensure undefined terms used in case studies (e.g., “HPSV2”) are defined or removed, and complete tables with missing values (Appendix B.3; Table 5 method identification).\n  • Add a concise glossary mapping all abbreviations to dimensions and sub-metrics at first use (Fig. 1; Sections 3.2.*).- Enhance reproducibility and fairness details\n  • Release full code, prompts, and evaluation scripts at camera-ready; include versioned configs for all MLLMs/tools (Section 7).\n  • Document seeds, hardware, inference parameters, and standardized output lengths for each baseline (Appendix B.1; Section 3.2.1), and provide Docker/conda environments to stabilize dependencies.\n  • Where access to specific MLLMs is limited, supply distilled or open alternatives and report cross-model robustness of scores (Sections 3.1, 3.2.2, 3.2.5; Appendix B.8).Score\n- Overall (10): 7 — Substantive benchmark with novel event-level and high-level metrics (Section 3.2; Eq. 1; Tables 2–4; Fig. 1), offset by MLLM reliance without human validation and incomplete reproducibility/clarity details (Sections 3.2.4–3.2.5; Section 7; Appendix B.9–B.10).\n- Novelty (10): 8 — Event-level alignment with order penalty (Eq. 1), transition smoothness (Appendix B.6), intra/inter-event consistency (Appendix B.7), and polarity-aware HERD (Section 3.2.5) extend long-form evaluation beyond prior suites (Table 1; Section 1).\n- Technical Quality (10): 6 — Sound formulations and analyses (Sections 3.2.2–3.2.5; Table 5) but missing key specifics (Section 3.2.4; Appendix B.6), limited ablations (Section 4.4), and no human validation.\n- Clarity (10): 7 — Generally clear structure and extensive appendices (Appendix B–C), with inconsistencies (HERO vs. HERD; dual “TQ”; undefined “HPSV2”; CC prompt mislabeling; unspecified methods in Table 5) (Appendix B.9–B.10; C.5; E; Sections 3.2.1, 3.2.3; Section 4.3).\n- Confidence (5): 3 — Moderate confidence based on detailed manuscript and results (Tables 2–4; Fig. 7), but absent human studies and pending full code release (Section 7) limit verification.",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 7,
        "novelty": 8,
        "technical_quality": 6,
        "clarity": 7,
        "confidence": 3
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 7,
        "novelty": 8,
        "technical_quality": 6,
        "clarity": 7,
        "confidence": 3
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "Summary\n- The paper introduces LoCoT2V-Bench, a benchmark for evaluating long-form text-to-video generation under complex prompts. It builds a 240-prompt suite derived from 30–60s real-world YouTube videos and uses MLLMs to generate and self-refine long, multi-scene prompts (Section 3.1; Fig. 2; Table 1; Appendix B.2–B.4). The evaluation spans five dimensions—static quality, text–video alignment (overall and event-level), temporal quality, content clarity, and Human Expectation Realization Degree (HERD)—with new components such as event-level alignment with order penalties (Eq. 1), transition smoothness (Appendix B.6), intra/inter-event temporal consistency (Appendix B.7), content clarity via repeated MLLM MOS (Eq. 2; Section 3.2.4), and polarity-aware HERD (Section 3.2.5; Appendix B.8). Nine methods are benchmarked across 26 sub-dimensions (Tables 2–4; Fig. 7), with analyses on themes, correlations, entanglement, and prompt complexity (Sections 4.2–4.5; Table 5; Fig. 3, 5, 8, 9). Section 7 states an initial JSON prompt release, with full code/data to follow.Strengths\n- Bold focus on long-form, complex prompts\n  • Evidence: Prompts average 236.66 words and rank highest in complexity among benchmarks (Table 1), with length distribution in Fig. 4 (Appendix B.2) and multi-scene coverage detailed in Section 3.1 and Appendix B.2.\n  • Why it matters: Moves evaluation beyond short, single-scene prompts, addressing a recognized gap (novelty/impact).\n  • Evidence: The suite spans 18 themes grouped into three categories (Section 3.1; Fig. 2).\n  • Why it matters: Broad topical coverage improves generality (impact).\n  • Evidence: Instructions and self-refine paradigm with manual review (Section 3.1).\n  • Why it matters: Enhances prompt quality under complex, realistic settings (rigor).- Multi-dimensional framework extends evaluation to high-level attributes\n  • Evidence: Five major dimensions and 26 sub-dimensions (Fig. 1; Section 3.2), including content clarity (Section 3.2.4) and HERD (Section 3.2.5; Appendix B.8).\n  • Why it matters: Abstract qualities (clarity, narrative/thematic) are often overlooked; this framework adds them (impact).\n  • Evidence: CC comprises TC, LS, ICP, ICS with a formal aggregation (Eq. 2; Section 3.2.4).\n  • Why it matters: Structured scoring improves interpretability and replicability (technical soundness/clarity).\n  • Evidence: HERD defines seven human-centric sub-dimensions (Section 3.2.5; Appendix B.8).\n  • Why it matters: Captures narrative/emotional adherence beyond low-level metrics (impact).- Novel event-level alignment with temporal order penalty\n  • Evidence: EA formulation with Hungarian matching and inversion penalty (Eq. 1); event schema and extraction (Section 3.2.2; Appendix C.2–C.3).\n  • Why it matters: Order-aware, fine-grained adherence is crucial for multi-scene long videos (novelty/technical soundness).\n  • Evidence: Ground-truth events extracted from prompt base via DeepSeek-V3.1 (Section 3.2.2; Appendix C.3).\n  • Why it matters: Aligns evaluation to intended event structure (rigor).\n  • Evidence: OA uses MLLMs to produce detailed video descriptions before similarity scoring (Section 3.2.2; Appendix C.2).\n  • Why it matters: Improves semantic fidelity over CLIP-only baselines (impact).- Temporal quality suite includes transition smoothness and intra/inter-event consistency\n  • Evidence: Transition smoothness combines pixel, structural, feature, and motion cues and averages across transitions (Appendix B.6); intra/inter-event SC/BC computed with Time-R1-7B and Grounded-SAM-2 (Section 3.2.3; Appendix B.7).\n  • Why it matters: Evaluates long-range coherence across events and scene transitions (technical depth/impact).\n  • Evidence: Warping error and semantic consistency follow EvalCrafter (Section 3.2.3).\n  • Why it matters: Leverages established temporal consistency baselines (rigor).\n  • Evidence: Human Action evaluation via MLLMs with extraction/verification prompts (Section 3.2.3; Appendix C.4).\n  • Why it matters: Enables assessment of complex, prompt-conditioned actions (impact).- Polarity-aware HERD scoring to mitigate yes/no bias\n  • Evidence: Polarity annotation and scoring scheme (Section 3.2.5; Appendix B.8; Appendix C.6).\n  • Why it matters: Reduces bias from question framing in binary VQA (technical soundness/rigor).\n  • Evidence: Multiple questions per dimension (Appendix B.8).\n  • Why it matters: Improves robustness via redundancy (rigor).\n  • Evidence: Integration of dimension definitions (Appendix B.8).\n  • Why it matters: Clear criteria support consistent evaluation (clarity).- Comprehensive benchmarking with detailed results\n  • Evidence: Nine baselines (Appendix B.1), dimension aggregates (Table 2), temporal sub-dimensions (Table 3), HERD sub-dimensions (Table 4), visualizations (Fig. 7; Appendix B.9–B.10; Table 6).\n  • Why it matters: Offers granular insights into strengths/weaknesses of current models (experimental rigor/impact).\n  • Evidence: Low event-level alignment vs overall alignment across methods (Tables 2–3).\n  • Why it matters: Highlights where models fail under fine-grained adherence (impact).\n  • Evidence: Consistently low narrative flow across HERD (Table 4).\n  • Why it matters: Identifies persistent shortcomings in high-level narrative coherence (impact).- Analytical studies on correlations, entanglement, and thematic effects\n  • Evidence: Correlations between SQ and other dimensions (Section 4.3; Fig. 3; Table 5); entanglement analysis (Section 4.4; Table 5); theme category comparison (Section 4.2; Fig. 5; Fig. 8; Table 6).\n  • Why it matters: Diagnoses metric interactions and potential biases (clarity/impact).\n  • Evidence: Prompt complexity effects (Section 4.5; Fig. 9).\n  • Why it matters: Explores evaluation sensitivity to prompt semantics/structure/control (rigor).\n  • Evidence: Low correlation between EA and inter-event consistency (Table 5).\n  • Why it matters: Suggests separable failure modes (technical soundness).- Ethical and reproducibility intentions\n  • Evidence: Ethical statement (Section 6) and planned release of data/code/results (Section 7; link).\n  • Why it matters: Supports responsible, repeatable science (community impact).\n  • Evidence: Initial prompt JSON provided at submission (Section 7).\n  • Why it matters: Enables partial verification (reproducibility).\n  • Evidence: Tool/model citations and appendix formulas (References; Appendices B.6–B.7).\n  • Why it matters: Improves transparency of evaluation components (clarity/rigor).Weaknesses\n- Heavy reliance on MLLMs for both prompt construction and evaluation without human validation\n  • Evidence: Prompts and refinements via MLLMs with self-refine and manual review (Section 3.1: “employ them directly to generate raw prompts… self-refine”), and HERD/content clarity are MLLM-based (Section 3.2.4–3.2.5; Appendix C.5–C.6).\n  • Why it matters: Risks circularity/bias and lacks human-grounded calibration for abstract dimensions (experimental rigor/validity).\n  • Evidence: Source inconsistency in high-level assessments: Section 3.1 integrates Seed1.5-VL evaluations into prompts, whereas Appendix B.8 states Qwen2.5-VL-72B was used to generate dimension-wise assessments.\n  • Why it matters: Conflicting evaluator sources undermine the correctness of HERD setup and prompt-integrated “expectations” (validity/clarity).\n  • Evidence: No user study or correlation with human judgments is reported.\n  • Why it matters: No direct evidence found in the manuscript of human validation reduces confidence in high-level metrics (technical quality/impact).- Ambiguities and incomplete specification of evaluation hyperparameters\n  • Evidence: Content clarity repeats “prompted multiple times with controlled randomness,” but R in Eq. (2) is not specified; the evaluator model for CC is also not named (Section 3.2.4; Appendix C.5).\n  • Why it matters: Missing R/model details undermine reproducibility and variance analysis (technical rigor).\n  • Evidence: Transition smoothness equations provided (Appendix B.6, Eq. 3–6), but numeric values for α1–α4, window size k, and b,c are not specified.\n  • Why it matters: Without fixed parameters, comparisons may vary by implementation (reproducibility).\n  • Evidence: Event-level alignment mentions “embedding models” and field-level similarity (Section 3.2.2), but the exact text encoders, similarity thresholds, and matching settings are not specified.\n  • Why it matters: Unclear configuration could affect EA/OA scores (technical clarity/reproducibility).- Potential metric entanglement and cascading errors across chained tools without ablations\n  • Evidence: Entanglement acknowledged (Section 4.4), but no ablation quantifies pipeline impact (e.g., replacing MLLM event extraction with human labels).\n  • Why it matters: Without ablations, it’s hard to separate alignment vs. consistency effects (technical soundness).\n  • Evidence: Event extraction and description depend on Qwen2.5-VL and DeepSeek-V3.1 (Section 3.2.2; Appendix C.2–C.3).\n  • Why it matters: Cascading errors from mis-descriptions can skew EA (experimental reliability).\n  • Evidence: HERD polarity relies on DeepSeek-V3.1 (Appendix B.8; C.6).\n  • Why it matters: Polarity misclassification can systematically bias HERD scores (metric validity).- Dataset scale and representativeness; unclear generated video lengths impacting comparability\n  • Evidence: 240 prompts across 18 themes collected from 30–60s videos (Section 3.1; Fig. 2).\n  • Why it matters: Smaller sample size than some benchmarks (Table 1: e.g., VMBench 1050; VBench-Long 946) may limit statistical power (experimental scope).\n  • Evidence: DOVER++ evaluation requires <10s segmentation (Section 3.2.1; Page following Fig. 1), but standardized output durations across baselines are not stated.\n  • Why it matters: No direct evidence found in the manuscript of fixed output lengths can affect comparability across models (fairness).\n  • Evidence: Real-world short videos as prompt bases may bias content types (Section 3.1; Fig. 2).\n  • Why it matters: May under-represent cinematic multi-minute narratives the benchmark targets (generalizability).- Normalization and scaling consistency may conflate absolute interpretability\n  • Evidence: Aesthetic RR-UB derived from Text-to-Image-2M data_1024_10K (Appendix B.5), but the final numeric RR-UB value used for scaling is not reported.\n  • Why it matters: Without the reference value, AQ scaling cannot be independently verified (technical soundness).\n  • Evidence: “All values are expressed as percentages” (Table 2–4), but normalization schemes and orientation for several metrics—especially error-type metrics like Warping Error—are not detailed (Section 3.2.3; Table 3).\n  • Why it matters: Ambiguous “higher/lower is better” and conversion to percentages risks misinterpretation and uneven weighting (clarity/rigor).\n  • Evidence: HERD scores count only polarity-consistent yes/no answers and ignore “unclear” (Section 3.2.5).\n  • Why it matters: Excluding ambiguity may inflate scores if “unclear” is frequent, affecting comparability across models (metric validity).- Clarity issues and minor inconsistencies in terminology/figures\n  • Evidence: “HERO” appears in radar labels and sub-dimensions with mismatches (Appendix B.9–B.10; Fig. 7), while the text defines “HERD” and different sub-dimension names (Section 3.2.5; Appendix B.8).\n  • Why it matters: Naming/sub-dimension inconsistencies can confuse readers and implementations (clarity).\n  • Evidence: TQ abbreviation is used for both Technical Quality (Static Quality; Section 3.2.1) and Temporal Quality heading (Section 3.2.3).\n  • Why it matters: Ambiguous abbreviations degrade interpretability (clarity).\n  • Evidence: Case studies use “HPSV2” (Appendix E; Fig. 10–11) without definition; the CC prompt header labels “HERD Evaluation Prompt” though it defines CC dimensions (Appendix C.5); Table 5 refers to “three representative methods” without naming them (Section 4.3; Table 5).\n  • Why it matters: Undefined terms, mislabeling, and unspecified sample selection hamper transparency and replication (clarity).- Reproducibility and openness remain incomplete at submission time\n  • Evidence: Section 7 states only an initial prompt suite JSON is provided; full code release will follow reorganization.\n  • Why it matters: Review-time replication is limited (reproducibility).\n  • Evidence: Heavy use of specific MLLMs (Qwen2.5-VL, DeepSeek-V3.1, Seed1.5-VL; Sections 3.1, 3.2.2, 3.2.5; Appendix B.8) may hinder exact reproduction if versions/access change.\n  • Why it matters: Dependency on evolving models complicates stability (reproducibility).\n  • Evidence: Baseline generation details lack seeds, hardware specs, and standardized output lengths (Appendix B.1; Section 3.2.1 for segmentation protocol).\n  • Why it matters: No direct evidence found in the manuscript of standardized generation settings may affect fairness and repeatability (experimental rigor).Suggestions for Improvement\n- Add human-grounded validation to calibrate MLLM-based metrics\n  • Conduct a user study comparing HERD and content clarity scores against human judgments on a stratified subset of videos (Sections 3.2.4–3.2.5; Table 4), reporting correlations and variance to quantify potential bias.\n  • Clarify and validate the evaluator source for high-level assessments—specify whether Seed1.5-VL (Section 3.1) or Qwen2.5-VL-72B (Appendix B.8) produced the prompt-integrated evaluations—and report agreement statistics on a sample.\n  • Include inter-rater reliability (e.g., Cohen’s κ) and provide detailed protocols; if infeasible, state this limitation explicitly.- Specify evaluation hyperparameters and implementation details\n  • Report the number of rounds R in Eq. (2), the randomization controls, and the evaluator model used for content clarity (Section 3.2.4; Appendix C.5) to enable exact reproduction.\n  • Provide numeric settings for transition smoothness (Appendix B.6: α1–α4, k, b, c) and justify choices via a sensitivity analysis.\n  • Detail the embedding/encoder models, similarity metrics, thresholds, and matching settings used in OA/EA (Section 3.2.2; Appendix C.2–C.3), including ablation on alternatives.- Quantify and mitigate metric entanglement with ablations\n  • Replace LLM-based event extraction with human-labeled events on a subset to measure EA sensitivity (Section 3.2.2; Section 4.4).\n  • Swap specific tools in the pipeline (e.g., Time-R1 vs. alternatives for temporal grounding; Grounded-SAM-2 vs. SAM variants) and report variance in intra/inter-event consistency (Section 3.2.3; Appendix B.7).\n  • Evaluate HERD with alternative polarity sources and cross-check consistency (Section 3.2.5; Appendix B.8), reporting error bars.- Strengthen dataset scope and standardize output durations\n  • Expand beyond 240 prompts or provide a statistical power analysis justifying sample size (Section 3.1; Fig. 2; Table 1), and include more multi-minute narrative bases when feasible.\n  • Enforce standardized generated video lengths per prompt and document exception handling (Section 3.2.1), ensuring comparable evaluation across baselines.\n  • Diversify sources beyond YouTube 30–60s material and document coverage of cinematic structures to better match benchmark goals (Section 3.1).- Improve normalization and cross-metric comparability\n  • Validate RR-UB for AQ against a curated high-quality video frame set and report the numeric RR-UB value used (Appendix B.5), including alignment with human aesthetic judgments.\n  • Document normalization schemes and orientation (“higher/lower is better”) for all metrics—including error-type metrics like Warping Error—and provide a unified scaling protocol (beyond Eq. 2 and Appendix B.6/B.7) with rationale (Tables 2–4; Section 3.2.3; Table 3).\n  • In HERD, report the proportion of “unclear” answers per model and consider weighted penalties for high ambiguity to avoid inflating scores (Section 3.2.5).- Resolve terminology and figure inconsistencies\n  • Harmonize “HERD” naming and sub-dimensions across figures and text (Appendix B.9–B.10; Fig. 7; Section 3.2.5), and clarify which model produced the high-level evaluations integrated into prompts (Section 3.1 vs. Appendix B.8).\n  • Avoid ambiguous abbreviations like dual-use “TQ” (Sections 3.2.1, 3.2.3), ensure undefined terms used in case studies (e.g., “HPSV2”) are defined or removed, and complete tables with missing values (Appendix B.3; Table 5 method identification).\n  • Add a concise glossary mapping all abbreviations to dimensions and sub-metrics at first use (Fig. 1; Sections 3.2.*).- Enhance reproducibility and fairness details\n  • Release full code, prompts, and evaluation scripts at camera-ready; include versioned configs for all MLLMs/tools (Section 7).\n  • Document seeds, hardware, inference parameters, and standardized output lengths for each baseline (Appendix B.1; Section 3.2.1), and provide Docker/conda environments to stabilize dependencies.\n  • Where access to specific MLLMs is limited, supply distilled or open alternatives and report cross-model robustness of scores (Sections 3.1, 3.2.2, 3.2.5; Appendix B.8).Score\n- Overall (10): 7 — Substantive benchmark with novel event-level and high-level metrics (Section 3.2; Eq. 1; Tables 2–4; Fig. 1), offset by MLLM reliance without human validation and incomplete reproducibility/clarity details (Sections 3.2.4–3.2.5; Section 7; Appendix B.9–B.10).\n- Novelty (10): 8 — Event-level alignment with order penalty (Eq. 1), transition smoothness (Appendix B.6), intra/inter-event consistency (Appendix B.7), and polarity-aware HERD (Section 3.2.5) extend long-form evaluation beyond prior suites (Table 1; Section 1).\n- Technical Quality (10): 6 — Sound formulations and analyses (Sections 3.2.2–3.2.5; Table 5) but missing key specifics (Section 3.2.4; Appendix B.6), limited ablations (Section 4.4), and no human validation.\n- Clarity (10): 7 — Generally clear structure and extensive appendices (Appendix B–C), with inconsistencies (HERO vs. HERD; dual “TQ”; undefined “HPSV2”; CC prompt mislabeling; unspecified methods in Table 5) (Appendix B.9–B.10; C.5; E; Sections 3.2.1, 3.2.3; Section 4.3).\n- Confidence (5): 3 — Moderate confidence based on detailed manuscript and results (Tables 2–4; Fig. 7), but absent human studies and pending full code release (Section 7) limit verification."
}