# Global Summary
- Problem: The term â€œmemoryâ€ in RL is inconsistently defined and evaluated, leading to misleading claims about agentsâ€™ memory capabilities. The paper seeks to formalize memory types (short-term vs long-term; declarative vs procedural) and provide a standardized experimental methodology for Memory Decision-Making (Memory DM) tasks.
- Core approach: Formal quantitative definitions of agent memory using context length K and correlation horizon Î¾; a distinction between Memory DM (declarative memory in a single environment/episode) and Meta-RL (procedural memory across environments/episodes); a definition of memory-intensive environments; Theorem 1 introducing a context memory border KÌ„ = min Îž âˆ’ 1; Algorithm 1 to configure experiments validating STM (Î¾ â‰¤ K) and LTM (Î¾ > K) including memory mechanisms Î¼(K) that extend effective context K_eff â‰¥ K.
- Evaluation scope: Experiments in two memory-intensive environments (Passive T-Maze, MiniGrid-Memory) using agents DTQN, DQN-GPT-2, and SAC-GPT-2. Protocol: 3 runs per agent, validation during training using 100 random seeds (0â€“99), results reported as mean success rate or reward Â± SEM. Hyperparameters detailed.
- Key findings:
  - Misconfigured experiments can conflate LTM and STM, yielding incorrect conclusions. In MiniGrid-Memory with map size L = 21: variable mode with Î¾ âˆˆ [7, L + 1] produced success rate near 1.0 for both K = 22 (STM) and K = 14 (intended LTM), while fixed mode with Î¾ = L + 1 demonstrated STM success but LTM failure, revealing lack of LTM mechanisms in SAC-GPT-2 as claimed by authors.
  - In Passive T-Maze: STM with K = 15, Î¾ = 15 achieved return 1.0; reducing K to 5 with Î¾ = 15 (LTM test) reduced return to 0.5; further reducing Î¾ to 5 with K = 5 (STM again) restored return to 1.0.
- Explicitly stated caveats: The work focuses on declarative memory within Memory DM and does not attempt to replicate full human memory. Procedural memory is defined but not the primary experimental focus. Some environment parameters (e.g., specific success rate numeric values beyond â€œalmost 1.0â€), compute budgets, and exact SEM values are not specified.

# Abstract
- Motivation: Memory is vital for RL tasks requiring past information, adaptation, and sample efficiency, but â€œmemoryâ€ is ambiguous and lacks unified validation.
- Contributions:
  - Practical precise definitions of agent memory types inspired by cognitive science: LTM vs STM; declarative vs procedural.
  - Categorization of memory classes; robust experimental methodology to evaluate memory capabilities and standardize evaluations.
  - Empirical demonstration that adhering to methodology is crucial; violating it leads to erroneous judgments about agent memory.
- Scope: Focus on evaluating different RL agents across memory types; emphasis on Memory DM tasks.

# Introduction
- Context: RL solves MDP problems but struggles in partially observable settings (POMDPs), where history processing is needed. Sequence models (e.g., Transformers, RNNs) are applicable.
- Challenge: Observational complexity/noise, sparse events/rewards, long episodes make storing/retrieving important information hard; â€œmemoryâ€ is inconsistently defined across literature.
- Literature examples:
  - Memory as dependencies within fixed-size context (Esslinger et al., 2022; Ni et al., 2023; Grigsby et al., 2024).
  - Memory as out-of-context information via external mechanisms (Parisotto et al., 2020; Lampinen et al., 2021; Cherepanov et al., 2024).
  - Meta-RL uses â€œmemoryâ€ to adapt across tasks or episodes (Team et al., 2023; Kang et al., 2024a; Grigsby et al., 2024).
- Contributions summarized:
  1) Formalize RL agent memory types: LTM/STM and declarative/procedural (section 5).
  2) Decouple Memory DM vs Meta-RL tasks (section 5).
  3) Propose experimental methodology for testing LTM/STM in Memory DM (subsection 5.2).
  4) Show violations of methodology mislead memory capability assessments (section 6).

# Preliminaries
- POMDP definition: ð“œ_P = âŸ¨S, ð’œ, ð’ª, ð’«, ð’­, ð’µâŸ©, observation function ð’µ(o_{t+1}|s_{t+1}, a_t), agent acts using policy Ï€(a_t | o_t, h_{0:t-1}); state s_t is not observed; memory mechanisms required to use history h_{0:t-1}.
- Memory in cognitive science: Memory distinguished by temporal scale (STM: seconds; LTM: lifetime) and type (declarative/explicit vs procedural/implicit). Adopt â€œretain and recall laterâ€ as high-level definition.
- Mapping to RL: Two POMDP interpretationsâ€”dependencies within context (STM) and out-of-context (LTM), both aligning with declarative memory. Meta-RL aligns with procedural memory (skills transfer). Clarifies that focus is on declarative memory for current decisions in same environment/episode.
- Memory vs credit assignment: Uses Ni et al. (2023) distinction but treats both under a unified memory definition centered on temporal dependencies; definitions apply to both since they concern temporal correlation rather than essence.

# Related Work
- Varied definitions and mechanisms of memory in RL across architectures and benchmarks (e.g., PopGym, Memory Gym).
- Examples:
  - External buffers and retrieval (Oh et al., 2016).
  - Long-interval recall (Lampinen et al., 2021).
  - Working and episodic memory (Fortunato et al., 2020) enabling use of past events for current decisions.
  - Decoupling memory vs credit assignment (Ni et al., 2023).
- The paper notes prevalent ambiguity and misaligned benchmark selection; fuller details in Appendix C.

# Method
- Memory Decision-Making (Memory DM): Distinguishes Memory DM (declarative memory within one POMDP/episode) vs Meta-RL (procedural memory across tasks/episodes).
- Definition 1 (Agent context length): K âˆˆ â„• is maximum number of prior (o, a, r) triplets processed at time t; e.g., MLP: K = 1; Transformer: K = K_attn.
- Definition 2 (Memory DM): Ï€*(a_t | o_t, h_{0:t-1}) maximizes J^Ï€ = E_Ï€[âˆ‘_{t=0}^{Tâˆ’1} Î³^t r_t] within a single POMDP.
- Definition 3 (Declarative vs Procedural):
  - Declarative memory â‡” n_envs Ã— n_eps = 1 (within single environment and episode).
  - Procedural memory â‡” n_envs Ã— n_eps > 1 (across multiple environments or episodes).
- Definition 4 (STM vs LTM via correlation horizon Î¾): For event Î±_{t_e}^{Î”t} and recall Î²_{t_r}(Î±):
  - Î¾ = t_r âˆ’ t_e âˆ’ Î”t + 1.
  - STM â‡” Î¾ â‰¤ K.
  - LTM â‡” Î¾ > K.
- Classification schema: Distinguishes Memory DM (declarative STM/LTM) and Meta-RL (procedural), with inner-loop task possibly POMDP (green) or MDP (blue). Table 1 organizes task types by n_envs, n_eps, inner-loop MDP/POMDP, and whether Î¾ > K or Î¾ â‰¤ K.
- Definition 5 (Memory-intensive environments): ð“œÌƒ_P is memory-intensive â‡” min Îž > 1, where Îž is the set of all eventâ€“recall correlation horizons; corollary: max Îž = 1 â‡” MDP.
- Theorem 1 (Context memory border): âˆƒ KÌ„ â‰¥ 1 such that âˆ€ K âˆˆ [1, KÌ„]: K < min_n Îž; i.e., with K â‰¤ KÌ„ = min Îž âˆ’ 1, environment validates exclusively LTM.
  - Equation (4): KÌ„ = min Îž âˆ’ 1 = min_n{(t_r âˆ’ t_e âˆ’ Î”t + 1)_n} âˆ’ 1.
  - Validation conditions:
    - If K âˆˆ [1, KÌ„]: validate LTM only.
    - If K âˆˆ (KÌ„, max Îž): validate both STM and LTM (ambiguous for LTM estimation).
    - If K âˆˆ [max Îž, âˆž): validate STM only.
- Definition 6 (Memory mechanisms): Î¼(K) yields K_eff = Î¼(K) â‰¥ K, enabling processing of out-of-context information; example: RNN recurrent hidden state increases effective context beyond K = 1.
- Condition for LTM testing (Equation 6): ð“œÌƒ_P : K â‰¤ KÌ„ < Î¾ â‰¤ K_eff = Î¼(K).
- Algorithm 1 (experiment setup):
  1) Count eventâ€“recall pairs n; if n = 0, environment unsuitable; if n â‰¥ 1, suitable.
  2) Compute KÌ„ as min Î¾_n âˆ’ 1.
  3) Conduct experiment:
     - STM: set K > KÌ„.
     - LTM: set K â‰¤ KÌ„ â‰¤ K_eff = Î¼(K).
  4) Analyze results.
- Example setupâ€”Passive T-Maze:
  - One eventâ€“recall pair (clue at start; turn at junction): n = 1; Î”t = 0; Î¾ = T (with t_e = 0, t_r = T âˆ’ 1); hence KÌ„ = T âˆ’ 1.
  - STM test: fix T and set K > KÌ„ = T âˆ’ 1.
  - LTM test: set K â‰¤ KÌ„ = T âˆ’ 1 â‰¤ K_eff = Î¼(K); recommend choosing K near left boundary of [1, KÌ„] to more clearly expose Î¼(K).

# Experiments
- Environments: Passive T-Maze (Ni et al., 2023) and MiniGrid-Memory (Chevalier-Boisvert et al., 2023). See Appendix E.1 for detailed specs and reward structures.
- Agents/baselines: DTQN (Esslinger et al., 2022), DQN-GPT-2 (Ni et al., 2023), SAC-GPT-2 (Ni et al., 2023).
- Protocol: 3 runs per agent with different initializations; validation using 100 random seeds (0â€“99); results reported as mean success rate/reward Â± SEM.

6.1 Impact of experiment configuration on memory type tested
- MiniGrid-Memory with map size L = 21; two modes:
  - Fixed corridors: Î¾ = L + 1.
  - Variable corridors: Î¾ âˆˆ [7, L + 1].
- SAC-GPT-2 configurations:
  - STM intended: K = 22 (> L).
  - LTM intended: K = 14 (< L).
- Results (Figure 4):
  - Variable mode (green): success rate nearly 1.0 for both K = 22 and K = 14, which can misleadingly suggest both memory types.
  - Fixed mode (red): STM success with K = 22; LTM failure with K = 14.
- Interpretation stated by authors: SAC-GPT-2 lacks mechanisms to solve LTM; performance confusion arises when K is chosen naively relative to L without isolating STM vs LTM per Algorithm 1.

6.2 The relative nature of an agentâ€™s memory
- Passive T-Maze experiments varying K and Î¾ with DTQN and DQN-GPT-2:
  - STM test: K = Î¾ = 15 â†’ return 1.0 (both agents), confirming STM capability.
  - LTM test: K = 5, Î¾ = 15 â†’ return drops to 0.5 (both agents), indicating inability to recall clue when out of context (lack of LTM).
  - STM again: K = 5, Î¾ = 5 â†’ return returns to 1.0.
- Summary: Verifying LTM vs STM can be done by adjusting K or Î¾ while keeping the other fixed; Passive T-Maze enables parameterization via corridor length L with Î¾ = L + 1.

# Conclusion
- The paper formalizes RL memory types (LTM vs STM; declarative vs procedural) and separates Memory DM from Meta-RL, inspired by neuroscience but tailored to RL with quantitative definitions.
- Provides a methodology (Theorem 1, Algorithm 1) to design experiments that isolate STM and LTM and avoid conflation.
- Empirical demonstrations show that neglecting the methodology yields misleading conclusions (e.g., variable Î¾ mode suggesting LTM where fixed mode reveals none).
- The approach aims to standardize memory evaluations, enabling fair comparisons and identifying architectural limitations. Focus remains on declarative memory in Memory DM; full human memory replication is not attempted.

# References
- Cites foundational and recent works across RL memory mechanisms (RNNs, Transformers, SSMs), external memory architectures, Meta-RL surveys and algorithms, and memory benchmarks.
- Not specified in this section: explicit numeric comparative SOTA margins or compute budgets.

# Appendix
- Glossary (Appendix A): Defines key symbols and concepts:
  - ð“œ (MDP), ð“œ_P (POMDP), ð“œÌƒ_P (memory-intensive), h history, K base context length, KÌ„ context memory border, Î¼(K) memory mechanism, K_eff = Î¼(K), Î± event, Î² recall, Î¾ correlation horizon.
- Motivation notes (Appendix B): Uses neuroscience definitions already familiar in RL; aims for quantitative clarity without introducing new terminology; practical applications emphasize simple numeric parameters (n_envs, n_eps), and balancing K with Î¾ for unambiguous memory type determination.
- Memory mechanisms (Appendix C): Overview of architectures and strategies:
  - RNNs (hidden state), Transformers (self-attention in context), SSMs (state preservation), temporal convolutions, world models, graph-structured memory, external memory buffers (read-only and read/write), policy-encoded timing (no architectural memory).
- Meta-RL (Appendix D): Formal definition:
  - Meta-RL function f_Î¸ maps data ð’Ÿ (H episodes of length T in tasks sampled from p(ð’®ð‘€)) to policy Ï€_Ï• with objective J^Î¸ = E_{ð’®ð‘€_iâˆ¼p(ð’®ð‘€)}[E_ð’Ÿ[âˆ‘_{Ï„âˆˆð’Ÿ_{I:H}} G_i(Ï„) | f_Î¸, ð’®ð‘€_i]]. Differentiates outer-loop (Î¸) and inner-loop (Ï•). Tasks typically share S and ð’œ; differ in ð’­, ð’«, P_0.
- Environments (Appendix E.1):
  - Passive T-Maze:
    - Actions: {left, up, right, down}.
    - Observation vector o = [y, clue, flag, noise]; clue âˆˆ {1, âˆ’1} at t = 0; flag = 1 one step before junction; noise âˆˆ {âˆ’1, 0, +1}.
    - Episode duration T = L + 1; reward 1 for correct turn; 0 otherwise.
    - Dense reward variant: per-step penalty = âˆ’1/(T âˆ’ 1).
    - 1D observations, discrete actions; sparse/dense reward configurations.
  - MiniGrid-Memory:
    - 2D grid; agent observes 3 Ã— 3 frame; corridor with room containing object; turn toward matching object at junction.
    - Reward r = 1 âˆ’ 0.9 Ã— t/T for success; 0 otherwise; episode ends on turn or at 95 steps (time limit).
    - Discrete actions; sparse reward.
- Experimental protocol (Appendix E.2):
  - Runs: 3; validation: 100 random seeds (0â€“99); report mean SR/reward Â± SEM.
  - Hyperparameters:
    - SAC-GPT-2: layers = 2; attention heads = 2; hidden dim = 256; batch size = 64; optimizer = Adam; learning rate = 3eâˆ’4; dropout = 0.1; replay buffer size = 1e6; discount Î³ = 0.99; entropy temperature = 0.1.
    - DQN-GPT-2: layers = 2; attention heads = 2; hidden dim = 256; batch size = 64; optimizer = Adam; learning rate = 3eâˆ’4; dropout = 0.1; replay buffer size = 1e6; discount Î³ = 0.99.
    - DTQN: layers = 4; attention heads = 8; hidden dim = 128; batch size = 32; optimizer = Adam; learning rate = 3eâˆ’4; dropout = 0.1; replay buffer size = 5e5; discount Î³ = 0.99.
- Not specified in this section: exact compute resources, training wall-clock time, SEM values, or detailed network architectures beyond listed hyperparameters.