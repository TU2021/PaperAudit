{
  "baseline_review": "Summary\n- The paper formalizes the concept of memory in reinforcement learning by introducing precise definitions for declarative versus procedural memory and short-term versus long-term memory, grounded in cognitive science. It proposes the Memory Decision-Making (Memory DM) framework, defines agent context length K, correlation horizon ξ, and “memory-intensive” environments, and provides Theorem 1 and Algorithm 1 to design experiments that separate LTM from STM. The methodology is illustrated on Passive T-Maze and MiniGrid-Memory tasks using DTQN, DQN-GPT-2, and SAC-GPT-2, showing that misconfigured experiments (e.g., mixing ξ inside and outside K) can lead to incorrect claims about LTM. Results (Figure 4–5) demonstrate how varying K and ξ clarifies whether agents possess STM or LTM capabilities.Strengths\n- Bolded titles are required, including sub-point evidence and why it matters.- Strong formalization of RL memory types\n  - Evidence: Definitions 1–6 (Section 5), especially Definition 3 (Equations (1)–(2)) and Definition 4 (STM/LTM via ξ and K), Figure 2 (STM/LTM scheme), Figure 1 (declarative/procedural scheme).\n  - Why it matters: Provides quantitative anchors (K, ξ, n_envs, n_eps) for memory types, enabling standardized classification; enhances clarity and comparability across works, addressing confusion noted in Section 4.2.- Clear separation of task classes and memory manifestations\n  - Evidence: Definition 2 (Memory DM), Appendix Definition 7 (Meta-RL), Table 1 (task classification by n_envs and n_eps), Figure 19 (memory taxonomy diagram; Section 5).\n  - Why it matters: Distinguishing Memory DM (declarative) from Meta-RL (procedural) avoids conflating within-episode recall with across-task skill transfer, strengthening evaluation validity and aligning with cognitive science (Section 4.1).- Methodology for rigorous memory evaluation\n  - Evidence: Theorem 1 and Equation (3) (context memory border K̄), Equation (4) (computing K̄), strong/weak conditions for STM (Section 5.1), Algorithm 1 (Section 5.2), Equation (6) (condition for LTM testing with μ(K)).\n  - Why it matters: Offers a replicable experiment-design protocol that demarcates when an environment tests exclusively LTM or STM, preventing ambiguous setups that blur interpretations; improves experimental rigor.- Practical environment analysis and setup examples\n  - Evidence: Section 5.3 (Passive T-Maze) including ξ = T and K̄ = T − 1; Appendix E.1 environment descriptions for Passive T-Maze and MiniGrid-Memory (Figures 6, 49).\n  - Why it matters: Concrete guidance shows how to parameterize tasks (via L or corridor length) to probe memory types; enhances applicability and reproducibility for other researchers.- Empirical demonstration of evaluation pitfalls and corrective design\n  - Evidence: Section 6.1 (Impact of configuration), Figure 4 (MiniGrid-Memory: K=22 vs K=14; fixed vs variable ξ), Section 6.2 (relative nature via K and ξ), Figure 5 (Passive T-Maze transitions K=15, ξ=15 → K=5, ξ=15 → K=5, ξ=5).\n  - Why it matters: Shows that “variable mode” corridors can falsely suggest LTM when agents only have STM; validates the necessity of the proposed methodology; improves impact by exposing common error modes.- Notational clarity and reader support\n  - Evidence: Appendix A Glossary (items 1–11), consistent use of α, β, ξ, K, K̄, μ(K), K_eff across Sections 4–5.\n  - Why it matters: Clear notation enables exact reasoning and makes the framework accessible; strengthens clarity claims and reduces ambiguity.- Reproducibility details\n  - Evidence: Appendix E.2 (three runs, validation over 100 random seeds 0–99; Table 2 hyperparameters for SAC-GPT-2, DQN-GPT-2, DTQN).\n  - Why it matters: Reporting training protocol and hyperparameters supports replicability and increases confidence in empirical results.Weaknesses\n- Bolded titles are required, including sub-point evidence and why it matters.- Limited empirical scope and environment diversity\n  - Evidence: Section 6 uses only Passive T-Maze and MiniGrid-Memory; Appendix E.1 (two environments, both discrete control with single-clue recall).\n  - Why it matters: Conclusions about LTM/STM methodology are not stress-tested on diverse modalities (e.g., continuous control, multi-cue tasks), reducing generality.\n  - Evidence: Both tasks feature single-event recall pairs (Section 5.3, step 1 states n=1; Appendix E.1 Minigrid uses one object cue).\n  - Why it matters: Single event pairs simplify ξ estimation and may not capture compounded temporal dependencies.\n  - Evidence: No benchmarks beyond these two (Section 6); Related Work cites PopGym and Memory Gym (Section 3; Appendix C), but they are not used.\n  - Why it matters: Missing broader validation raises questions on robustness across standard memory benchmarks.- Underspecified operationalization of K and μ(K)\n  - Evidence: Definition 1 (K) defines maximal history tokens the base model can process; experiments set K=14/22 (Section 6.1) but do not explain how K was enforced or measured per architecture.\n  - Why it matters: Without a precise operational procedure, K may differ between implementations (e.g., GPT-2 attention window vs effective sequence length), undermining reproducibility.\n  - Evidence: Definition 6 (μ(K)) and Equation (5) introduce K_eff ≥ K, but experiments do not report measured K_eff or mechanisms used to increase it for tested agents.\n  - Why it matters: LTM validation hinges on μ(K) producing K_eff > K (Equation (6)), yet no direct evidence of μ(K) > K for any evaluated agent is provided.\n  - Evidence: Algorithm 1 (steps 2–3) requires estimating ξ_n and setting K relative to K̄; however, details on computing ξ in MiniGrid variable mode (range ξ ∈ [7, L+1]) lack sampling protocol specifics (Section 6.1).\n  - Why it matters: Ambiguity in ξ distribution affects whether the experiment sits in the transitional interval, impacting conclusions.- Transitional interval remains unquantified\n  - Evidence: Section 5.1 defines “weak condition” where K lies in (K̄, maxΞ), implying mixed STM/LTM; Section 6.1 notes this yields ambiguity.\n  - Why it matters: The paper does not provide quantitative criteria or diagnostic metrics to characterize ambiguity or to estimate the relative contributions of STM vs LTM.\n  - Evidence: Equation (4) gives K̄, but there is no analysis of sensitivity around K≈K̄ (e.g., performance gradients).\n  - Why it matters: Lack of sensitivity analyses leaves practitioners uncertain about “safe margins” for exclusive validation.\n  - Evidence: No statistical tests are reported to distinguish curves in Figure 4 between fixed vs variable mode beyond SEM shading (Section 6.1; Appendix E.2).\n  - Why it matters: Without formal quantification, claims about ambiguity are illustrative rather than rigorously substantiated.- No positive LTM baseline to validate the framework end-to-end\n  - Evidence: Evaluated agents (DTQN, DQN-GPT-2, SAC-GPT-2; Section 6) are primarily context-bounded Transformers; no external-memory or explicit LTM mechanism is included.\n  - Why it matters: The methodology shows failures (absence of LTM) but does not demonstrate success with an agent where μ(K) produces K_eff > K (Equation (6)).\n  - Evidence: Appendix C cites external-memory (Graves et al., 2016; Parisotto & Salakhutdinov, 2017), retrieval-augmented RL (Goyal et al., 2022), and world models (Hafner et al., 2019; Ha & Schmidhuber, 2018), but none are tested.\n  - Why it matters: Without positive LTM cases, the empirical validation remains partial and may be perceived as tailored to transformer limitations.\n  - Evidence: Section 5.2 asserts “agents with memory mechanisms … that can solve long-term memory tasks can also handle short-term” but provides no experiment with such an agent.\n  - Why it matters: This claim is plausible but unverified in the presented results.- Reporting gaps affecting reproducibility and interpretation\n  - Evidence: Appendix E.2 states “three runs” and “validation using 100 random seeds,” but it is unclear how validation seeds interact with training runs, and whether curves (Figures 4–5) aggregate across both.\n  - Why it matters: Ambiguity in aggregation protocols can impact error bars and claims of significance.\n  - Evidence: Section 6.1 “variable mode” ξ ∈ [7, L+1] lacks a sampling distribution (uniform? curriculum?), and Figure 4 does not specify how ξ varies over timesteps.\n  - Why it matters: Different ξ sampling schemes could produce the observed high SR for K=14, changing the interpretation.\n  - Evidence: Appendix E.1 modifies Passive T-Maze reward structure relative to Ni et al. (2023) (dense penalty r_step = −1/(T−1)), but does not describe how this affects training duration or returns scaling in Figure 5.\n  - Why it matters: Deviations from prior setups without ablation hinder comparability and may confound results.- Conceptual mapping of declarative/procedural memory may be too coarse\n  - Evidence: Definition 3 reduces declarative memory to n_envs × n_eps = 1 (Equation (1)) and procedural memory to n_envs × n_eps > 1 (Equation (2)); Table 1 maps MDP-based Meta-RL to “outer-loop memory only.”\n  - Why it matters: Tying memory type to training regime (number of envs/episodes) may conflate data organization with behavioral manifestation; edge cases (e.g., single-env multiple episodes with within-episode recall) may not cleanly fit.\n  - Evidence: Section 4.2 notes confusion in prior literature and introduces these definitions, but does not provide empirical tests that corroborate the declarative/procedural split beyond taxonomy.\n  - Why it matters: Without empirical validation of this mapping, the framework risks oversimplification, potentially misclassifying tasks where procedural elements arise in single environments.\n  - Evidence: Appendix B.1 states intent not to replicate human memory fully but “align with intuitive understanding”; still, the mapping to n_envs/n_eps is a design choice without formal derivation.\n  - Why it matters: Readers may require stronger justification to adopt these definitions across diverse RL tasks.Suggestions for Improvement\n- Bolded titles are required, including sub-point evidence and why it matters.- Broaden empirical coverage across environments and modalities\n  - Add evaluations on standardized memory benchmarks referenced in the paper (PopGym, Memory Gym; Section 3; Appendix C), including tasks with multiple event–recall pairs to stress ξ estimation beyond n=1.\n  - Include continuous-control POMDPs (e.g., partially observable MuJoCo; Section 5.1 mentions their limitations) with parameterizable ξ to test whether Algorithm 1 generalizes beyond discrete grid worlds.\n  - Incorporate tasks with overlapping, nested, or distractor cues (Appendix E.1 Minigrid uses a single cue) to probe interaction among multiple ξ_n and validate the framework in more complex settings.- Precisely operationalize and report K and μ(K)\n  - For each architecture, define how K is enforced (e.g., transformer attention window, sequence truncation) and measured during training and evaluation (Definition 1; Section 6.1) to ensure comparability across agents.\n  - When claiming or testing μ(K), report K_eff, the mechanism that increases it (Equation (5)–(6)), and empirical checks (e.g., synthetic tasks where ξ is systematically varied) to verify K_eff > K in practice.\n  - Document ξ sampling protocols explicitly in variable-mode experiments (Section 6.1) and provide code to compute ξ for each episode to prevent inadvertent placement in the transitional interval.- Quantify the transitional interval and provide diagnostics\n  - Around K≈K̄ (Equation (4)), perform sensitivity analyses by sweeping K and reporting performance gradients or slopes, identifying “safe margins” for exclusive LTM/STM validation (Section 5.1).\n  - Introduce a diagnostic metric estimating the proportion of event–recall pairs with ξ inside versus outside the current K during evaluation, to quantify STM/LTM mixture in the weak condition (Section 5.1).\n  - Apply statistical tests (e.g., bootstrap confidence intervals) to differences between fixed and variable modes in Figure 4, reporting p-values or effect sizes to strengthen claims about ambiguity.- Include a positive LTM baseline to validate the full methodology\n  - Evaluate at least one agent with explicit external memory (e.g., Neural Map or Neural Turing Machines; Appendix C, Graves et al., 2016; Parisotto & Salakhutdinov, 2017) to demonstrate μ(K) achieving K_eff > K and success under Equation (6).\n  - Test retrieval-augmented RL (Goyal et al., 2022; Appendix C) or world models (Hafner et al., 2019; Ha & Schmidhuber, 2018) on Passive T-Maze/MiniGrid-Memory to show positive LTM outcomes and contrast curves with transformers.\n  - Provide ablations where memory mechanisms are disabled to confirm causal contribution to LTM performance under the proposed conditions (Section 5.2 claim that LTM-capable agents handle STM).- Strengthen reporting for reproducibility and interpretation\n  - Clarify aggregation: specify whether training curves (Figures 4–5) aggregate across three runs and 100 validation seeds, and define SEM computation; include training budgets and wall-clock compute.\n  - Detail the ξ sampling distribution in variable mode (Section 6.1), whether uniform over [7, L+1] or curriculum-based, and show per-ξ performance to support the observed high SR for K=14.\n  - Provide ablation on reward shaping differences (Appendix E.1) versus the original Passive T-Maze, reporting how dense penalties affect learning curves and returns scaling.- Provide stronger justification and testing of declarative/procedural mapping\n  - Augment Section 5 and Appendix B.1–B.2 with empirical case studies illustrating edge cases (e.g., single environment with multiple episodes involving within-episode recall) to validate n_envs × n_eps rules (Equations (1)–(2); Table 1).\n  - Offer theoretical or behavioral arguments (e.g., task structures showing skill transfer vs information recall) that tie n_envs/n_eps directly to procedural vs declarative manifestations in RL, beyond taxonomy (Section 4.2; Table 1).\n  - Include a brief experimental probe where the same inner-loop POMDP is evaluated under different outer-loop regimes (n_envs/n_eps) to empirically test whether the proposed classification predicts observed behavior.Score\n- Overall (10): 6 — Formal definitions (Section 5; Definitions 1–6; Equations (1)–(6)) and a clear methodology (Algorithm 1; Theorem 1; Table 1) are strong, but empirical validation is limited to two simple environments with no positive LTM baseline (Section 6; Appendix E.1–E.2; Figure 4–5).\n- Novelty (10): 6 — The quantitative framing of K, ξ, K̄ and “memory-intensive environments” (Section 5; Theorem 1; Equation (4)) is a useful synthesis, though related ideas about memory vs credit assignment exist (Section 4.2; Ni et al., 2023).\n- Technical Quality (10): 5 — Theorem 1 is correct but straightforward (Section 5.1), and experiments illustrate pitfalls (Figure 4–5) without rigorous quantification of the transitional interval or inclusion of LTM-capable baselines (Section 6.1–6.2).\n- Clarity (10): 8 — The paper is clearly written with consistent notation and helpful diagrams/glossary (Figures 1–2, 19; Appendix A; Algorithm 1; Table 1), though some operational details for K, μ(K), and ξ sampling are missing (Section 6.1; Appendix E.2).\n- Confidence (5): 4 — The manuscript provides enough detail and anchors for a careful assessment, but the narrow empirical scope and reporting gaps (Appendix E.2; Section 6.1) limit certainty about generality.",
  "final_review": "Summary\n- The paper formalizes memory in reinforcement learning by defining declarative versus procedural memory and short-term versus long-term memory (STM/LTM) with quantitative constructs such as agent context length K and correlation horizon ξ. It introduces the Memory Decision-Making (Memory DM) framework, defines memory-intensive environments, and presents Theorem 1 (context memory border K̄ = min Ξ − 1) and Algorithm 1 for configuring experiments to isolate STM and LTM. The methodology is illustrated in Passive T-Maze and MiniGrid-Memory using DTQN, DQN-GPT-2, and SAC-GPT-2. Findings show that misconfigured setups (e.g., variable ξ mode) can conflate STM and LTM, whereas fixed ξ configurations reveal the absence of LTM mechanisms in a transformer agent (Figures 4–5; Sections 6.1–6.2; Section 5.3; Appendix E.1–E.2).Strengths\n- Strong formalization of RL memory types\n  - Evidence: Definitions 1–6 (Section 5), especially Definition 4 (STM/LTM via ξ and K) and quantitative instantiation of ξ = t_r − t_e − Δt + 1; Figures 1–2 visualize declarative/procedural (Figure 1) and STM/LTM (Figure 2).\n  - Why it matters: Establishes clear, quantitative grounding for memory types using K and ξ, reducing ambiguity and enabling standardized classification across works; this addresses definitional confusion raised in Section 4.2.- Clear separation of task classes and memory manifestations\n  - Evidence: Definition 2 (Memory DM) and Definition 3 (Declarative/Procedural; Equations (1)–(2)), Table 1 (classification by n_envs and n_eps; Section 5), and the taxonomy figure in Section 5 (image between Theorem 1 and Eq. (4)).\n  - Why it matters: Distinguishes Memory DM (declarative) from Meta-RL (procedural) and clarifies inner-loop versus outer-loop regimes, helping avoid conflation of within-episode recall with cross-task skill transfer (Section 4.2; Section 5).- Methodology for rigorous memory evaluation\n  - Evidence: Theorem 1 and Equation (4) (K̄ = min Ξ − 1), the weak/strong STM conditions (Section 5.1), Definition 6 (μ(K) and K_eff), and Algorithm 1 (Section 5.2), including the LTM testing condition (Equation (6)).\n  - Why it matters: Provides a replicable protocol to design experiments that isolate when an environment tests exclusively LTM or STM, preventing ambiguous configurations that confound interpretation; strengthens experimental rigor.- Practical environment analysis and setup examples\n  - Evidence: Section 5.3 (Passive T-Maze) with ξ = T and K̄ = T − 1; Appendix E.1 detailed specifications for Passive T-Maze and MiniGrid-Memory and visuals (Figure 6).\n  - Why it matters: Supplies concrete guidance for parameterizing tasks (e.g., corridor length L) to probe memory types, supporting applicability and reproducibility for practitioners.- Empirical demonstration of evaluation pitfalls and corrective design\n  - Evidence: Section 6.1 (impact of configuration), Figure 4 (MiniGrid-Memory: fixed ξ = L + 1 vs variable ξ ∈ [7, L + 1] for K = 22 and K = 14), Section 6.2 (relative nature via K and ξ), Figure 5 (Passive T-Maze transitions: K = ξ = 15 → K = 5, ξ = 15 → K = 5, ξ = 5).\n  - Why it matters: Demonstrates how variable ξ can falsely suggest LTM when agents only exhibit STM, validating the necessity of the proposed methodology and highlighting common error modes in memory evaluation.- Notational clarity and reader support\n  - Evidence: Appendix A Glossary (items 1–11) and consistent use of symbols K, K̄, μ(K), K_eff, ξ across Sections 4–5.\n  - Why it matters: Clear, consistent notation enables precise reasoning and makes the framework accessible, bolstering clarity and reducing ambiguity.- Reproducibility details\n  - Evidence: Appendix E.2 (three runs, validation using 100 random seeds; hyperparameters for SAC-GPT-2, DQN-GPT-2, DTQN), and environment descriptions (Appendix E.1).\n  - Why it matters: Reporting training protocol and hyperparameters supports replicability and increases confidence in empirical claims.Weaknesses\n- Limited empirical scope and environment diversity\n  - Evidence: Section 6 evaluates only Passive T-Maze and MiniGrid-Memory; Appendix E.1 (two environments, both discrete control with single-clue recall).\n  - Why it matters: The methodology is not stress-tested on diverse modalities (e.g., continuous control, multi-cue tasks), limiting generality.\n  - Evidence: Both tasks focus on single event–recall pairs (Section 5.3, n = 1; Appendix E.1 MiniGrid-Memory uses one object cue).\n  - Why it matters: Single pairs simplify ξ estimation and may not capture compounded or overlapping temporal dependencies.\n  - Evidence: No benchmarks beyond these two environments are used (Section 6), despite citing PopGym and Memory Gym (Related Work; Section 3).\n  - Why it matters: Missing broader validation raises questions about robustness across standard memory benchmarks.- Underspecified operationalization of K and μ(K)\n  - Evidence: Definition 1 (K) defines maximal history tokens processed; Section 6.1 changes K to 14/22 but does not explain enforcement or measurement per architecture (e.g., attention window vs effective sequence length).\n  - Why it matters: Without precise procedures, K may differ across implementations, undermining reproducibility and comparability.\n  - Evidence: Definition 6 and Equation (5) introduce K_eff ≥ K, but experiments do not report measured K_eff or mechanisms used to increase it for tested agents (Section 6).\n  - Why it matters: LTM validation hinges on μ(K) producing K_eff > K (Equation (6)); no direct evidence of μ(K) > K is presented.\n  - Evidence: Algorithm 1 requires estimating ξ_n and setting K relative to K̄; Section 6.1 specifies ξ ∈ [7, L + 1] for variable mode but omits sampling protocol specifics and ξ computation per episode.\n  - Why it matters: Ambiguity in ξ distribution affects whether experiments fall into the transitional interval, impacting conclusions.\n  - Evidence: Section 6.1 labels LTM/STM using K relative to L (e.g., “K = 14 representing LTM, since K < L”), rather than ξ as defined in Definition 4.\n  - Why it matters: Using L as a proxy for ξ contradicts the formal criteria (Definition 4), risking misclassification of memory regimes.- Transitional interval remains unquantified\n  - Evidence: Section 5.1 defines the “weak condition” when K ∈ (K̄, max Ξ), implying mixed STM/LTM; Section 6.1 notes ambiguity under this setup.\n  - Why it matters: The paper lacks quantitative diagnostics for characterizing ambiguity or estimating STM vs LTM contributions.\n  - Evidence: Equation (4) gives K̄, but there is no sensitivity analysis around K ≈ K̄ (e.g., performance gradients).\n  - Why it matters: Without sensitivity analyses, practitioners cannot determine “safe margins” for exclusive validation.\n  - Evidence: No statistical tests are reported to distinguish curves in Figure 4 between fixed vs variable mode beyond SEM shading (Section 6.1; Appendix E.2).\n  - Why it matters: Absent formal quantification, claims about ambiguity remain illustrative.\n  - Evidence: Section 5.1 uses inconsistent phrasing by stating the weak condition “validates both short-term and long-term memory,” yet also that “it is not possible to estimate long-term memory explicitly.”\n  - Why it matters: Terminological inconsistency may confuse interpretation in mixed regimes and weakens methodological guidance.- No positive LTM baseline to validate the framework end-to-end\n  - Evidence: Evaluated agents (DTQN, DQN-GPT-2, SAC-GPT-2; Section 6) are context-bounded Transformers; no external-memory or explicit LTM-capable mechanism is tested.\n  - Why it matters: The methodology shows failures (absence of LTM) but does not demonstrate success with μ(K) producing K_eff > K (Equation (6)).\n  - Evidence: Appendix C cites external-memory and retrieval-augmented mechanisms (Graves et al., 2016; Parisotto & Salakhutdinov, 2017; Goyal et al., 2022) and world models (Hafner et al., 2019; Ha & Schmidhuber, 2018), but none are empirically evaluated.\n  - Why it matters: Without positive LTM cases, validation remains partial and may be perceived as tailored to transformer limitations.\n  - Evidence: Section 5.2 suggests agents capable of LTM should also handle STM, but provides no experiment with such an agent.\n  - Why it matters: This plausible claim remains unverified, limiting end-to-end validation of the framework.- Reporting gaps affecting reproducibility and interpretation\n  - Evidence: Appendix E.2 states “three runs” and “validation using 100 random seeds,” but it is unclear how validation seeds interact with training runs and whether curves (Figures 4–5) aggregate across both.\n  - Why it matters: Aggregation ambiguity affects error bars and significance claims.\n  - Evidence: Section 6.1 “variable mode” ξ ∈ [7, L + 1] lacks a sampling distribution (uniform vs curriculum), and Figure 4 does not specify per-episode ξ variation.\n  - Why it matters: Different ξ sampling schemes could produce the observed high SR for K = 14, altering interpretation.\n  - Evidence: Appendix E.1 introduces a dense penalty variant for Passive T-Maze (penalty = −1/(T − 1)) but does not specify which reward configuration is used in Figure 5.\n  - Why it matters: Returns scale differs between sparse and dense rewards; lack of specification complicates interpreting reported return values.\n  - Evidence: Section 5 references “see Figure 3” for classifying inner-loop tasks, but no Figure 3 appears in the manuscript’s figures list (Figures 1–6 are present).\n  - Why it matters: Missing or misnumbered figures impede verification of the taxonomy and reduce clarity.\n  - Evidence: Related Work (Section 3) says “see Appendix C for details” regarding benchmarks, but Appendix C covers memory mechanisms rather than benchmark details.\n  - Why it matters: Misreferences hinder readers seeking methodological context and reproducibility.\n  - Evidence: Section 4.2 cites “Bartlett & Kintsch (1995),” while References list Bartlett (1995) without Kintsch.\n  - Why it matters: Citation mismatch reduces trust in source attribution for reconstructive memory claims.- Conceptual mapping of declarative/procedural memory may be too coarse\n  - Evidence: Definition 3 reduces declarative memory to n_envs × n_eps = 1 and procedural memory to n_envs × n_eps > 1; Table 1 maps Meta-RL to outer-loop memory only (Section 5).\n  - Why it matters: Tying memory type to training regime may conflate data organization with behavioral manifestation; single-environment, multi-episode setups with within-episode recall may not cleanly fit.\n  - Evidence: Section 4.2 highlights confusion and introduces these definitions but provides no empirical tests corroborating the declarative/procedural split beyond taxonomy.\n  - Why it matters: Without empirical validation, the framework risks oversimplification and misclassification.\n  - Evidence: Appendix B.1 states intent not to replicate human memory fully but to align with intuitive understanding; nevertheless, mapping to n_envs/n_eps is a design choice without formal derivation.\n  - Why it matters: Stronger justification may be required for adoption.\n  - Evidence: Section 6 asserts experiments are within the Memory DM framework (Section 6), yet the training protocol implies multiple episodes per environment (Appendix E.1–E.2; time limit 95 steps, replay buffers up to 1e6, and long training curves), contradicting the declarative condition n_envs × n_eps = 1 (Definition 3).\n  - Why it matters: This taxonomy–experiment mismatch undermines the claim that empirical validation targets declarative memory.Suggestions for Improvement\n- Broaden empirical coverage across environments and modalities\n  - Add evaluations on standardized memory benchmarks referenced in the paper (PopGym, Memory Gym; Section 3), including tasks with multiple event–recall pairs to stress ξ estimation beyond n = 1.\n  - Include continuous-control POMDPs (e.g., partially observable MuJoCo) with parameterizable ξ to test whether Algorithm 1 generalizes beyond discrete grid worlds (Section 5.1).\n  - Incorporate tasks with overlapping, nested, or distractor cues (Appendix E.1 MiniGrid-Memory uses a single cue) to probe interactions among multiple ξ_n and validate the framework in more complex settings.- Precisely operationalize and report K and μ(K)\n  - For each architecture, define how K is enforced (e.g., attention window, sequence truncation) and measured during training/evaluation (Definition 1; Section 6.1) to ensure comparability.\n  - When claiming or testing μ(K), report K_eff, the mechanism that increases it (Equation (5)–(6)), and empirical checks (e.g., synthetic tasks sweeping ξ) to verify K_eff > K in practice.\n  - Document ξ sampling protocols explicitly in variable-mode experiments (Section 6.1) and provide code to compute ξ per episode to avoid inadvertent placement in the transitional interval.\n  - In Section 6.1, relabel STM/LTM conditions using ξ rather than L (Definition 4), and explicitly map corridor length L to ξ to maintain consistency with the formal criteria.- Quantify the transitional interval and provide diagnostics\n  - Around K ≈ K̄ (Equation (4)), perform sensitivity analyses by sweeping K and reporting performance gradients, identifying “safe margins” for exclusive LTM/STM validation (Section 5.1).\n  - Introduce a diagnostic metric estimating the proportion of event–recall pairs with ξ inside versus outside the current K during evaluation to quantify STM/LTM mixture in the weak condition (Section 5.1).\n  - Apply statistical tests (e.g., bootstrap confidence intervals) to differences between fixed and variable modes in Figure 4, reporting p-values or effect sizes to strengthen claims about ambiguity.\n  - Clarify terminology for the weak condition (Section 5.1) to avoid inconsistency: explicitly state that mixed regimes cannot isolate LTM and only yield partial validation, and revise phrasing accordingly.- Include a positive LTM baseline to validate the full methodology\n  - Evaluate at least one agent with explicit external memory (e.g., Neural Map or Neural Turing Machines; Appendix C) to demonstrate μ(K) achieving K_eff > K and success under Equation (6).\n  - Test retrieval-augmented RL (Goyal et al., 2022; Appendix C) or world models (Hafner et al., 2019; Ha & Schmidhuber, 2018) on Passive T-Maze/MiniGrid-Memory to show positive LTM outcomes and contrast curves with transformers.\n  - Provide ablations where memory mechanisms are disabled to confirm causal contributions to LTM performance (Section 5.2 claim that LTM-capable agents handle STM).- Strengthen reporting for reproducibility and interpretation\n  - Clarify aggregation: specify whether training curves (Figures 4–5) aggregate across three runs and 100 validation seeds, and define SEM computation; include training budgets and wall-clock compute (Appendix E.2).\n  - Detail the ξ sampling distribution in variable mode (Section 6.1), whether uniform over [7, L + 1] or curriculum-based, and show per-ξ performance to support the observed high SR for K = 14.\n  - Provide ablation on reward shaping differences (Appendix E.1) versus the original Passive T-Maze, reporting how dense penalties affect learning curves and returns scaling.\n  - Supply or correct the missing figure reference for inner-loop task classification (Section 5; “see Figure 3”) or remap to existing Figures 1–2/Table 1 to ensure verifiability.\n  - Correct the Appendix pointer in Related Work (Section 3) to the appropriate section containing benchmark details or add those details to Appendix C.\n  - Fix citation mismatches (e.g., Section 4.2 “Bartlett & Kintsch (1995)” vs References listing Bartlett, 1995) to ensure accurate attribution.- Provide stronger justification and testing of declarative/procedural mapping\n  - Augment Section 5 and Appendix B.1–B.2 with empirical case studies illustrating edge cases (e.g., single environment with multiple episodes involving within-episode recall) to validate n_envs × n_eps rules (Equations (1)–(2); Table 1).\n  - Offer theoretical or behavioral arguments tying n_envs/n_eps directly to procedural vs declarative manifestations in RL, beyond taxonomy (Section 4.2; Table 1).\n  - Include an experimental probe where the same inner-loop POMDP is evaluated under different outer-loop regimes (n_envs/n_eps) to test whether the proposed classification predicts observed behavior.\n  - Align empirical setups and claims: if experiments necessarily use multiple episodes (Appendix E.2; Section 6), either reclassify them under procedural memory per Definition 3 or adjust the declarative definition to accommodate Memory DM evaluated across episodes, and explicitly report n_eps.Score\n- Overall (10): 6 — Strong formalization (Section 5; Definitions 1–6; Equations (1)–(6); Algorithm 1; Table 1) with clear demonstrations (Figures 4–5), but limited empirical scope and reporting inconsistencies (Section 6; Appendix E.1–E.2; missing/misnumbered figure in Section 5).\n- Novelty (10): 6 — The quantitative framing with K, ξ, and K̄ (Section 5; Theorem 1; Equation (4)) is a useful synthesis, though related notions exist (Section 4.2); the taxonomy adds clarity but needs stronger empirical corroboration (Table 1; Section 6).\n- Technical Quality (10): 5 — The core theorem is correct but straightforward (Section 5.1); experiments illustrate pitfalls (Figures 4–5) without quantifying the transitional interval (Section 5.1; Section 6.1) or including LTM-capable baselines (Section 6).\n- Clarity (10): 7 — Generally clear with consistent notation and diagrams (Figures 1–2; Appendix A; Algorithm 1; Table 1), but misreferences/missing figure (Section 5 “Figure 3”; Related Work to Appendix C) and ξ sampling/reporting gaps (Section 6.1; Appendix E.2) reduce clarity.\n- Confidence (5): 4 — Sufficient anchors for assessment, but narrow empirical scope and reporting inconsistencies (Appendix E.2; Section 6.1; Section 5 figure reference) limit certainty about generality.",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 6,
        "novelty": 6,
        "technical_quality": 5,
        "clarity": 8,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 6,
        "novelty": 6,
        "technical_quality": 5,
        "clarity": 7,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "Summary\n- The paper formalizes memory in reinforcement learning by defining declarative versus procedural memory and short-term versus long-term memory (STM/LTM) with quantitative constructs such as agent context length K and correlation horizon ξ. It introduces the Memory Decision-Making (Memory DM) framework, defines memory-intensive environments, and presents Theorem 1 (context memory border K̄ = min Ξ − 1) and Algorithm 1 for configuring experiments to isolate STM and LTM. The methodology is illustrated in Passive T-Maze and MiniGrid-Memory using DTQN, DQN-GPT-2, and SAC-GPT-2. Findings show that misconfigured setups (e.g., variable ξ mode) can conflate STM and LTM, whereas fixed ξ configurations reveal the absence of LTM mechanisms in a transformer agent (Figures 4–5; Sections 6.1–6.2; Section 5.3; Appendix E.1–E.2).Strengths\n- Strong formalization of RL memory types\n  - Evidence: Definitions 1–6 (Section 5), especially Definition 4 (STM/LTM via ξ and K) and quantitative instantiation of ξ = t_r − t_e − Δt + 1; Figures 1–2 visualize declarative/procedural (Figure 1) and STM/LTM (Figure 2).\n  - Why it matters: Establishes clear, quantitative grounding for memory types using K and ξ, reducing ambiguity and enabling standardized classification across works; this addresses definitional confusion raised in Section 4.2.- Clear separation of task classes and memory manifestations\n  - Evidence: Definition 2 (Memory DM) and Definition 3 (Declarative/Procedural; Equations (1)–(2)), Table 1 (classification by n_envs and n_eps; Section 5), and the taxonomy figure in Section 5 (image between Theorem 1 and Eq. (4)).\n  - Why it matters: Distinguishes Memory DM (declarative) from Meta-RL (procedural) and clarifies inner-loop versus outer-loop regimes, helping avoid conflation of within-episode recall with cross-task skill transfer (Section 4.2; Section 5).- Methodology for rigorous memory evaluation\n  - Evidence: Theorem 1 and Equation (4) (K̄ = min Ξ − 1), the weak/strong STM conditions (Section 5.1), Definition 6 (μ(K) and K_eff), and Algorithm 1 (Section 5.2), including the LTM testing condition (Equation (6)).\n  - Why it matters: Provides a replicable protocol to design experiments that isolate when an environment tests exclusively LTM or STM, preventing ambiguous configurations that confound interpretation; strengthens experimental rigor.- Practical environment analysis and setup examples\n  - Evidence: Section 5.3 (Passive T-Maze) with ξ = T and K̄ = T − 1; Appendix E.1 detailed specifications for Passive T-Maze and MiniGrid-Memory and visuals (Figure 6).\n  - Why it matters: Supplies concrete guidance for parameterizing tasks (e.g., corridor length L) to probe memory types, supporting applicability and reproducibility for practitioners.- Empirical demonstration of evaluation pitfalls and corrective design\n  - Evidence: Section 6.1 (impact of configuration), Figure 4 (MiniGrid-Memory: fixed ξ = L + 1 vs variable ξ ∈ [7, L + 1] for K = 22 and K = 14), Section 6.2 (relative nature via K and ξ), Figure 5 (Passive T-Maze transitions: K = ξ = 15 → K = 5, ξ = 15 → K = 5, ξ = 5).\n  - Why it matters: Demonstrates how variable ξ can falsely suggest LTM when agents only exhibit STM, validating the necessity of the proposed methodology and highlighting common error modes in memory evaluation.- Notational clarity and reader support\n  - Evidence: Appendix A Glossary (items 1–11) and consistent use of symbols K, K̄, μ(K), K_eff, ξ across Sections 4–5.\n  - Why it matters: Clear, consistent notation enables precise reasoning and makes the framework accessible, bolstering clarity and reducing ambiguity.- Reproducibility details\n  - Evidence: Appendix E.2 (three runs, validation using 100 random seeds; hyperparameters for SAC-GPT-2, DQN-GPT-2, DTQN), and environment descriptions (Appendix E.1).\n  - Why it matters: Reporting training protocol and hyperparameters supports replicability and increases confidence in empirical claims.Weaknesses\n- Limited empirical scope and environment diversity\n  - Evidence: Section 6 evaluates only Passive T-Maze and MiniGrid-Memory; Appendix E.1 (two environments, both discrete control with single-clue recall).\n  - Why it matters: The methodology is not stress-tested on diverse modalities (e.g., continuous control, multi-cue tasks), limiting generality.\n  - Evidence: Both tasks focus on single event–recall pairs (Section 5.3, n = 1; Appendix E.1 MiniGrid-Memory uses one object cue).\n  - Why it matters: Single pairs simplify ξ estimation and may not capture compounded or overlapping temporal dependencies.\n  - Evidence: No benchmarks beyond these two environments are used (Section 6), despite citing PopGym and Memory Gym (Related Work; Section 3).\n  - Why it matters: Missing broader validation raises questions about robustness across standard memory benchmarks.- Underspecified operationalization of K and μ(K)\n  - Evidence: Definition 1 (K) defines maximal history tokens processed; Section 6.1 changes K to 14/22 but does not explain enforcement or measurement per architecture (e.g., attention window vs effective sequence length).\n  - Why it matters: Without precise procedures, K may differ across implementations, undermining reproducibility and comparability.\n  - Evidence: Definition 6 and Equation (5) introduce K_eff ≥ K, but experiments do not report measured K_eff or mechanisms used to increase it for tested agents (Section 6).\n  - Why it matters: LTM validation hinges on μ(K) producing K_eff > K (Equation (6)); no direct evidence of μ(K) > K is presented.\n  - Evidence: Algorithm 1 requires estimating ξ_n and setting K relative to K̄; Section 6.1 specifies ξ ∈ [7, L + 1] for variable mode but omits sampling protocol specifics and ξ computation per episode.\n  - Why it matters: Ambiguity in ξ distribution affects whether experiments fall into the transitional interval, impacting conclusions.\n  - Evidence: Section 6.1 labels LTM/STM using K relative to L (e.g., “K = 14 representing LTM, since K < L”), rather than ξ as defined in Definition 4.\n  - Why it matters: Using L as a proxy for ξ contradicts the formal criteria (Definition 4), risking misclassification of memory regimes.- Transitional interval remains unquantified\n  - Evidence: Section 5.1 defines the “weak condition” when K ∈ (K̄, max Ξ), implying mixed STM/LTM; Section 6.1 notes ambiguity under this setup.\n  - Why it matters: The paper lacks quantitative diagnostics for characterizing ambiguity or estimating STM vs LTM contributions.\n  - Evidence: Equation (4) gives K̄, but there is no sensitivity analysis around K ≈ K̄ (e.g., performance gradients).\n  - Why it matters: Without sensitivity analyses, practitioners cannot determine “safe margins” for exclusive validation.\n  - Evidence: No statistical tests are reported to distinguish curves in Figure 4 between fixed vs variable mode beyond SEM shading (Section 6.1; Appendix E.2).\n  - Why it matters: Absent formal quantification, claims about ambiguity remain illustrative.\n  - Evidence: Section 5.1 uses inconsistent phrasing by stating the weak condition “validates both short-term and long-term memory,” yet also that “it is not possible to estimate long-term memory explicitly.”\n  - Why it matters: Terminological inconsistency may confuse interpretation in mixed regimes and weakens methodological guidance.- No positive LTM baseline to validate the framework end-to-end\n  - Evidence: Evaluated agents (DTQN, DQN-GPT-2, SAC-GPT-2; Section 6) are context-bounded Transformers; no external-memory or explicit LTM-capable mechanism is tested.\n  - Why it matters: The methodology shows failures (absence of LTM) but does not demonstrate success with μ(K) producing K_eff > K (Equation (6)).\n  - Evidence: Appendix C cites external-memory and retrieval-augmented mechanisms (Graves et al., 2016; Parisotto & Salakhutdinov, 2017; Goyal et al., 2022) and world models (Hafner et al., 2019; Ha & Schmidhuber, 2018), but none are empirically evaluated.\n  - Why it matters: Without positive LTM cases, validation remains partial and may be perceived as tailored to transformer limitations.\n  - Evidence: Section 5.2 suggests agents capable of LTM should also handle STM, but provides no experiment with such an agent.\n  - Why it matters: This plausible claim remains unverified, limiting end-to-end validation of the framework.- Reporting gaps affecting reproducibility and interpretation\n  - Evidence: Appendix E.2 states “three runs” and “validation using 100 random seeds,” but it is unclear how validation seeds interact with training runs and whether curves (Figures 4–5) aggregate across both.\n  - Why it matters: Aggregation ambiguity affects error bars and significance claims.\n  - Evidence: Section 6.1 “variable mode” ξ ∈ [7, L + 1] lacks a sampling distribution (uniform vs curriculum), and Figure 4 does not specify per-episode ξ variation.\n  - Why it matters: Different ξ sampling schemes could produce the observed high SR for K = 14, altering interpretation.\n  - Evidence: Appendix E.1 introduces a dense penalty variant for Passive T-Maze (penalty = −1/(T − 1)) but does not specify which reward configuration is used in Figure 5.\n  - Why it matters: Returns scale differs between sparse and dense rewards; lack of specification complicates interpreting reported return values.\n  - Evidence: Section 5 references “see Figure 3” for classifying inner-loop tasks, but no Figure 3 appears in the manuscript’s figures list (Figures 1–6 are present).\n  - Why it matters: Missing or misnumbered figures impede verification of the taxonomy and reduce clarity.\n  - Evidence: Related Work (Section 3) says “see Appendix C for details” regarding benchmarks, but Appendix C covers memory mechanisms rather than benchmark details.\n  - Why it matters: Misreferences hinder readers seeking methodological context and reproducibility.\n  - Evidence: Section 4.2 cites “Bartlett & Kintsch (1995),” while References list Bartlett (1995) without Kintsch.\n  - Why it matters: Citation mismatch reduces trust in source attribution for reconstructive memory claims.- Conceptual mapping of declarative/procedural memory may be too coarse\n  - Evidence: Definition 3 reduces declarative memory to n_envs × n_eps = 1 and procedural memory to n_envs × n_eps > 1; Table 1 maps Meta-RL to outer-loop memory only (Section 5).\n  - Why it matters: Tying memory type to training regime may conflate data organization with behavioral manifestation; single-environment, multi-episode setups with within-episode recall may not cleanly fit.\n  - Evidence: Section 4.2 highlights confusion and introduces these definitions but provides no empirical tests corroborating the declarative/procedural split beyond taxonomy.\n  - Why it matters: Without empirical validation, the framework risks oversimplification and misclassification.\n  - Evidence: Appendix B.1 states intent not to replicate human memory fully but to align with intuitive understanding; nevertheless, mapping to n_envs/n_eps is a design choice without formal derivation.\n  - Why it matters: Stronger justification may be required for adoption.\n  - Evidence: Section 6 asserts experiments are within the Memory DM framework (Section 6), yet the training protocol implies multiple episodes per environment (Appendix E.1–E.2; time limit 95 steps, replay buffers up to 1e6, and long training curves), contradicting the declarative condition n_envs × n_eps = 1 (Definition 3).\n  - Why it matters: This taxonomy–experiment mismatch undermines the claim that empirical validation targets declarative memory.Suggestions for Improvement\n- Broaden empirical coverage across environments and modalities\n  - Add evaluations on standardized memory benchmarks referenced in the paper (PopGym, Memory Gym; Section 3), including tasks with multiple event–recall pairs to stress ξ estimation beyond n = 1.\n  - Include continuous-control POMDPs (e.g., partially observable MuJoCo) with parameterizable ξ to test whether Algorithm 1 generalizes beyond discrete grid worlds (Section 5.1).\n  - Incorporate tasks with overlapping, nested, or distractor cues (Appendix E.1 MiniGrid-Memory uses a single cue) to probe interactions among multiple ξ_n and validate the framework in more complex settings.- Precisely operationalize and report K and μ(K)\n  - For each architecture, define how K is enforced (e.g., attention window, sequence truncation) and measured during training/evaluation (Definition 1; Section 6.1) to ensure comparability.\n  - When claiming or testing μ(K), report K_eff, the mechanism that increases it (Equation (5)–(6)), and empirical checks (e.g., synthetic tasks sweeping ξ) to verify K_eff > K in practice.\n  - Document ξ sampling protocols explicitly in variable-mode experiments (Section 6.1) and provide code to compute ξ per episode to avoid inadvertent placement in the transitional interval.\n  - In Section 6.1, relabel STM/LTM conditions using ξ rather than L (Definition 4), and explicitly map corridor length L to ξ to maintain consistency with the formal criteria.- Quantify the transitional interval and provide diagnostics\n  - Around K ≈ K̄ (Equation (4)), perform sensitivity analyses by sweeping K and reporting performance gradients, identifying “safe margins” for exclusive LTM/STM validation (Section 5.1).\n  - Introduce a diagnostic metric estimating the proportion of event–recall pairs with ξ inside versus outside the current K during evaluation to quantify STM/LTM mixture in the weak condition (Section 5.1).\n  - Apply statistical tests (e.g., bootstrap confidence intervals) to differences between fixed and variable modes in Figure 4, reporting p-values or effect sizes to strengthen claims about ambiguity.\n  - Clarify terminology for the weak condition (Section 5.1) to avoid inconsistency: explicitly state that mixed regimes cannot isolate LTM and only yield partial validation, and revise phrasing accordingly.- Include a positive LTM baseline to validate the full methodology\n  - Evaluate at least one agent with explicit external memory (e.g., Neural Map or Neural Turing Machines; Appendix C) to demonstrate μ(K) achieving K_eff > K and success under Equation (6).\n  - Test retrieval-augmented RL (Goyal et al., 2022; Appendix C) or world models (Hafner et al., 2019; Ha & Schmidhuber, 2018) on Passive T-Maze/MiniGrid-Memory to show positive LTM outcomes and contrast curves with transformers.\n  - Provide ablations where memory mechanisms are disabled to confirm causal contributions to LTM performance (Section 5.2 claim that LTM-capable agents handle STM).- Strengthen reporting for reproducibility and interpretation\n  - Clarify aggregation: specify whether training curves (Figures 4–5) aggregate across three runs and 100 validation seeds, and define SEM computation; include training budgets and wall-clock compute (Appendix E.2).\n  - Detail the ξ sampling distribution in variable mode (Section 6.1), whether uniform over [7, L + 1] or curriculum-based, and show per-ξ performance to support the observed high SR for K = 14.\n  - Provide ablation on reward shaping differences (Appendix E.1) versus the original Passive T-Maze, reporting how dense penalties affect learning curves and returns scaling.\n  - Supply or correct the missing figure reference for inner-loop task classification (Section 5; “see Figure 3”) or remap to existing Figures 1–2/Table 1 to ensure verifiability.\n  - Correct the Appendix pointer in Related Work (Section 3) to the appropriate section containing benchmark details or add those details to Appendix C.\n  - Fix citation mismatches (e.g., Section 4.2 “Bartlett & Kintsch (1995)” vs References listing Bartlett, 1995) to ensure accurate attribution.- Provide stronger justification and testing of declarative/procedural mapping\n  - Augment Section 5 and Appendix B.1–B.2 with empirical case studies illustrating edge cases (e.g., single environment with multiple episodes involving within-episode recall) to validate n_envs × n_eps rules (Equations (1)–(2); Table 1).\n  - Offer theoretical or behavioral arguments tying n_envs/n_eps directly to procedural vs declarative manifestations in RL, beyond taxonomy (Section 4.2; Table 1).\n  - Include an experimental probe where the same inner-loop POMDP is evaluated under different outer-loop regimes (n_envs/n_eps) to test whether the proposed classification predicts observed behavior.\n  - Align empirical setups and claims: if experiments necessarily use multiple episodes (Appendix E.2; Section 6), either reclassify them under procedural memory per Definition 3 or adjust the declarative definition to accommodate Memory DM evaluated across episodes, and explicitly report n_eps.Score\n- Overall (10): 6 — Strong formalization (Section 5; Definitions 1–6; Equations (1)–(6); Algorithm 1; Table 1) with clear demonstrations (Figures 4–5), but limited empirical scope and reporting inconsistencies (Section 6; Appendix E.1–E.2; missing/misnumbered figure in Section 5).\n- Novelty (10): 6 — The quantitative framing with K, ξ, and K̄ (Section 5; Theorem 1; Equation (4)) is a useful synthesis, though related notions exist (Section 4.2); the taxonomy adds clarity but needs stronger empirical corroboration (Table 1; Section 6).\n- Technical Quality (10): 5 — The core theorem is correct but straightforward (Section 5.1); experiments illustrate pitfalls (Figures 4–5) without quantifying the transitional interval (Section 5.1; Section 6.1) or including LTM-capable baselines (Section 6).\n- Clarity (10): 7 — Generally clear with consistent notation and diagrams (Figures 1–2; Appendix A; Algorithm 1; Table 1), but misreferences/missing figure (Section 5 “Figure 3”; Related Work to Appendix C) and ξ sampling/reporting gaps (Section 6.1; Appendix E.2) reduce clarity.\n- Confidence (5): 4 — Sufficient anchors for assessment, but narrow empirical scope and reporting inconsistencies (Appendix E.2; Section 6.1; Section 5 figure reference) limit certainty about generality."
}