Summary
- The paper formalizes memory in reinforcement learning by defining declarative versus procedural memory and short-term versus long-term memory (STM/LTM) with quantitative constructs such as agent context length K and correlation horizon ξ. It introduces the Memory Decision-Making (Memory DM) framework, defines memory-intensive environments, and presents Theorem 1 (context memory border K̄ = min Ξ − 1) and Algorithm 1 for configuring experiments to isolate STM and LTM. The methodology is illustrated in Passive T-Maze and MiniGrid-Memory using DTQN, DQN-GPT-2, and SAC-GPT-2. Findings show that misconfigured setups (e.g., variable ξ mode) can conflate STM and LTM, whereas fixed ξ configurations reveal the absence of LTM mechanisms in a transformer agent (Figures 4–5; Sections 6.1–6.2; Section 5.3; Appendix E.1–E.2).Strengths
- Strong formalization of RL memory types
  - Evidence: Definitions 1–6 (Section 5), especially Definition 4 (STM/LTM via ξ and K) and quantitative instantiation of ξ = t_r − t_e − Δt + 1; Figures 1–2 visualize declarative/procedural (Figure 1) and STM/LTM (Figure 2).
  - Why it matters: Establishes clear, quantitative grounding for memory types using K and ξ, reducing ambiguity and enabling standardized classification across works; this addresses definitional confusion raised in Section 4.2.- Clear separation of task classes and memory manifestations
  - Evidence: Definition 2 (Memory DM) and Definition 3 (Declarative/Procedural; Equations (1)–(2)), Table 1 (classification by n_envs and n_eps; Section 5), and the taxonomy figure in Section 5 (image between Theorem 1 and Eq. (4)).
  - Why it matters: Distinguishes Memory DM (declarative) from Meta-RL (procedural) and clarifies inner-loop versus outer-loop regimes, helping avoid conflation of within-episode recall with cross-task skill transfer (Section 4.2; Section 5).- Methodology for rigorous memory evaluation
  - Evidence: Theorem 1 and Equation (4) (K̄ = min Ξ − 1), the weak/strong STM conditions (Section 5.1), Definition 6 (μ(K) and K_eff), and Algorithm 1 (Section 5.2), including the LTM testing condition (Equation (6)).
  - Why it matters: Provides a replicable protocol to design experiments that isolate when an environment tests exclusively LTM or STM, preventing ambiguous configurations that confound interpretation; strengthens experimental rigor.- Practical environment analysis and setup examples
  - Evidence: Section 5.3 (Passive T-Maze) with ξ = T and K̄ = T − 1; Appendix E.1 detailed specifications for Passive T-Maze and MiniGrid-Memory and visuals (Figure 6).
  - Why it matters: Supplies concrete guidance for parameterizing tasks (e.g., corridor length L) to probe memory types, supporting applicability and reproducibility for practitioners.- Empirical demonstration of evaluation pitfalls and corrective design
  - Evidence: Section 6.1 (impact of configuration), Figure 4 (MiniGrid-Memory: fixed ξ = L + 1 vs variable ξ ∈ [7, L + 1] for K = 22 and K = 14), Section 6.2 (relative nature via K and ξ), Figure 5 (Passive T-Maze transitions: K = ξ = 15 → K = 5, ξ = 15 → K = 5, ξ = 5).
  - Why it matters: Demonstrates how variable ξ can falsely suggest LTM when agents only exhibit STM, validating the necessity of the proposed methodology and highlighting common error modes in memory evaluation.- Notational clarity and reader support
  - Evidence: Appendix A Glossary (items 1–11) and consistent use of symbols K, K̄, μ(K), K_eff, ξ across Sections 4–5.
  - Why it matters: Clear, consistent notation enables precise reasoning and makes the framework accessible, bolstering clarity and reducing ambiguity.- Reproducibility details
  - Evidence: Appendix E.2 (three runs, validation using 100 random seeds; hyperparameters for SAC-GPT-2, DQN-GPT-2, DTQN), and environment descriptions (Appendix E.1).
  - Why it matters: Reporting training protocol and hyperparameters supports replicability and increases confidence in empirical claims.Weaknesses
- Limited empirical scope and environment diversity
  - Evidence: Section 6 evaluates only Passive T-Maze and MiniGrid-Memory; Appendix E.1 (two environments, both discrete control with single-clue recall).
  - Why it matters: The methodology is not stress-tested on diverse modalities (e.g., continuous control, multi-cue tasks), limiting generality.
  - Evidence: Both tasks focus on single event–recall pairs (Section 5.3, n = 1; Appendix E.1 MiniGrid-Memory uses one object cue).
  - Why it matters: Single pairs simplify ξ estimation and may not capture compounded or overlapping temporal dependencies.
  - Evidence: No benchmarks beyond these two environments are used (Section 6), despite citing PopGym and Memory Gym (Related Work; Section 3).
  - Why it matters: Missing broader validation raises questions about robustness across standard memory benchmarks.- Underspecified operationalization of K and μ(K)
  - Evidence: Definition 1 (K) defines maximal history tokens processed; Section 6.1 changes K to 14/22 but does not explain enforcement or measurement per architecture (e.g., attention window vs effective sequence length).
  - Why it matters: Without precise procedures, K may differ across implementations, undermining reproducibility and comparability.
  - Evidence: Definition 6 and Equation (5) introduce K_eff ≥ K, but experiments do not report measured K_eff or mechanisms used to increase it for tested agents (Section 6).
  - Why it matters: LTM validation hinges on μ(K) producing K_eff > K (Equation (6)); no direct evidence of μ(K) > K is presented.
  - Evidence: Algorithm 1 requires estimating ξ_n and setting K relative to K̄; Section 6.1 specifies ξ ∈ [7, L + 1] for variable mode but omits sampling protocol specifics and ξ computation per episode.
  - Why it matters: Ambiguity in ξ distribution affects whether experiments fall into the transitional interval, impacting conclusions.
  - Evidence: Section 6.1 labels LTM/STM using K relative to L (e.g., “K = 14 representing LTM, since K < L”), rather than ξ as defined in Definition 4.
  - Why it matters: Using L as a proxy for ξ contradicts the formal criteria (Definition 4), risking misclassification of memory regimes.- Transitional interval remains unquantified
  - Evidence: Section 5.1 defines the “weak condition” when K ∈ (K̄, max Ξ), implying mixed STM/LTM; Section 6.1 notes ambiguity under this setup.
  - Why it matters: The paper lacks quantitative diagnostics for characterizing ambiguity or estimating STM vs LTM contributions.
  - Evidence: Equation (4) gives K̄, but there is no sensitivity analysis around K ≈ K̄ (e.g., performance gradients).
  - Why it matters: Without sensitivity analyses, practitioners cannot determine “safe margins” for exclusive validation.
  - Evidence: No statistical tests are reported to distinguish curves in Figure 4 between fixed vs variable mode beyond SEM shading (Section 6.1; Appendix E.2).
  - Why it matters: Absent formal quantification, claims about ambiguity remain illustrative.
  - Evidence: Section 5.1 uses inconsistent phrasing by stating the weak condition “validates both short-term and long-term memory,” yet also that “it is not possible to estimate long-term memory explicitly.”
  - Why it matters: Terminological inconsistency may confuse interpretation in mixed regimes and weakens methodological guidance.- No positive LTM baseline to validate the framework end-to-end
  - Evidence: Evaluated agents (DTQN, DQN-GPT-2, SAC-GPT-2; Section 6) are context-bounded Transformers; no external-memory or explicit LTM-capable mechanism is tested.
  - Why it matters: The methodology shows failures (absence of LTM) but does not demonstrate success with μ(K) producing K_eff > K (Equation (6)).
  - Evidence: Appendix C cites external-memory and retrieval-augmented mechanisms (Graves et al., 2016; Parisotto & Salakhutdinov, 2017; Goyal et al., 2022) and world models (Hafner et al., 2019; Ha & Schmidhuber, 2018), but none are empirically evaluated.
  - Why it matters: Without positive LTM cases, validation remains partial and may be perceived as tailored to transformer limitations.
  - Evidence: Section 5.2 suggests agents capable of LTM should also handle STM, but provides no experiment with such an agent.
  - Why it matters: This plausible claim remains unverified, limiting end-to-end validation of the framework.- Reporting gaps affecting reproducibility and interpretation
  - Evidence: Appendix E.2 states “three runs” and “validation using 100 random seeds,” but it is unclear how validation seeds interact with training runs and whether curves (Figures 4–5) aggregate across both.
  - Why it matters: Aggregation ambiguity affects error bars and significance claims.
  - Evidence: Section 6.1 “variable mode” ξ ∈ [7, L + 1] lacks a sampling distribution (uniform vs curriculum), and Figure 4 does not specify per-episode ξ variation.
  - Why it matters: Different ξ sampling schemes could produce the observed high SR for K = 14, altering interpretation.
  - Evidence: Appendix E.1 introduces a dense penalty variant for Passive T-Maze (penalty = −1/(T − 1)) but does not specify which reward configuration is used in Figure 5.
  - Why it matters: Returns scale differs between sparse and dense rewards; lack of specification complicates interpreting reported return values.
  - Evidence: Section 5 references “see Figure 3” for classifying inner-loop tasks, but no Figure 3 appears in the manuscript’s figures list (Figures 1–6 are present).
  - Why it matters: Missing or misnumbered figures impede verification of the taxonomy and reduce clarity.
  - Evidence: Related Work (Section 3) says “see Appendix C for details” regarding benchmarks, but Appendix C covers memory mechanisms rather than benchmark details.
  - Why it matters: Misreferences hinder readers seeking methodological context and reproducibility.
  - Evidence: Section 4.2 cites “Bartlett & Kintsch (1995),” while References list Bartlett (1995) without Kintsch.
  - Why it matters: Citation mismatch reduces trust in source attribution for reconstructive memory claims.- Conceptual mapping of declarative/procedural memory may be too coarse
  - Evidence: Definition 3 reduces declarative memory to n_envs × n_eps = 1 and procedural memory to n_envs × n_eps > 1; Table 1 maps Meta-RL to outer-loop memory only (Section 5).
  - Why it matters: Tying memory type to training regime may conflate data organization with behavioral manifestation; single-environment, multi-episode setups with within-episode recall may not cleanly fit.
  - Evidence: Section 4.2 highlights confusion and introduces these definitions but provides no empirical tests corroborating the declarative/procedural split beyond taxonomy.
  - Why it matters: Without empirical validation, the framework risks oversimplification and misclassification.
  - Evidence: Appendix B.1 states intent not to replicate human memory fully but to align with intuitive understanding; nevertheless, mapping to n_envs/n_eps is a design choice without formal derivation.
  - Why it matters: Stronger justification may be required for adoption.
  - Evidence: Section 6 asserts experiments are within the Memory DM framework (Section 6), yet the training protocol implies multiple episodes per environment (Appendix E.1–E.2; time limit 95 steps, replay buffers up to 1e6, and long training curves), contradicting the declarative condition n_envs × n_eps = 1 (Definition 3).
  - Why it matters: This taxonomy–experiment mismatch undermines the claim that empirical validation targets declarative memory.Suggestions for Improvement
- Broaden empirical coverage across environments and modalities
  - Add evaluations on standardized memory benchmarks referenced in the paper (PopGym, Memory Gym; Section 3), including tasks with multiple event–recall pairs to stress ξ estimation beyond n = 1.
  - Include continuous-control POMDPs (e.g., partially observable MuJoCo) with parameterizable ξ to test whether Algorithm 1 generalizes beyond discrete grid worlds (Section 5.1).
  - Incorporate tasks with overlapping, nested, or distractor cues (Appendix E.1 MiniGrid-Memory uses a single cue) to probe interactions among multiple ξ_n and validate the framework in more complex settings.- Precisely operationalize and report K and μ(K)
  - For each architecture, define how K is enforced (e.g., attention window, sequence truncation) and measured during training/evaluation (Definition 1; Section 6.1) to ensure comparability.
  - When claiming or testing μ(K), report K_eff, the mechanism that increases it (Equation (5)–(6)), and empirical checks (e.g., synthetic tasks sweeping ξ) to verify K_eff > K in practice.
  - Document ξ sampling protocols explicitly in variable-mode experiments (Section 6.1) and provide code to compute ξ per episode to avoid inadvertent placement in the transitional interval.
  - In Section 6.1, relabel STM/LTM conditions using ξ rather than L (Definition 4), and explicitly map corridor length L to ξ to maintain consistency with the formal criteria.- Quantify the transitional interval and provide diagnostics
  - Around K ≈ K̄ (Equation (4)), perform sensitivity analyses by sweeping K and reporting performance gradients, identifying “safe margins” for exclusive LTM/STM validation (Section 5.1).
  - Introduce a diagnostic metric estimating the proportion of event–recall pairs with ξ inside versus outside the current K during evaluation to quantify STM/LTM mixture in the weak condition (Section 5.1).
  - Apply statistical tests (e.g., bootstrap confidence intervals) to differences between fixed and variable modes in Figure 4, reporting p-values or effect sizes to strengthen claims about ambiguity.
  - Clarify terminology for the weak condition (Section 5.1) to avoid inconsistency: explicitly state that mixed regimes cannot isolate LTM and only yield partial validation, and revise phrasing accordingly.- Include a positive LTM baseline to validate the full methodology
  - Evaluate at least one agent with explicit external memory (e.g., Neural Map or Neural Turing Machines; Appendix C) to demonstrate μ(K) achieving K_eff > K and success under Equation (6).
  - Test retrieval-augmented RL (Goyal et al., 2022; Appendix C) or world models (Hafner et al., 2019; Ha & Schmidhuber, 2018) on Passive T-Maze/MiniGrid-Memory to show positive LTM outcomes and contrast curves with transformers.
  - Provide ablations where memory mechanisms are disabled to confirm causal contributions to LTM performance (Section 5.2 claim that LTM-capable agents handle STM).- Strengthen reporting for reproducibility and interpretation
  - Clarify aggregation: specify whether training curves (Figures 4–5) aggregate across three runs and 100 validation seeds, and define SEM computation; include training budgets and wall-clock compute (Appendix E.2).
  - Detail the ξ sampling distribution in variable mode (Section 6.1), whether uniform over [7, L + 1] or curriculum-based, and show per-ξ performance to support the observed high SR for K = 14.
  - Provide ablation on reward shaping differences (Appendix E.1) versus the original Passive T-Maze, reporting how dense penalties affect learning curves and returns scaling.
  - Supply or correct the missing figure reference for inner-loop task classification (Section 5; “see Figure 3”) or remap to existing Figures 1–2/Table 1 to ensure verifiability.
  - Correct the Appendix pointer in Related Work (Section 3) to the appropriate section containing benchmark details or add those details to Appendix C.
  - Fix citation mismatches (e.g., Section 4.2 “Bartlett & Kintsch (1995)” vs References listing Bartlett, 1995) to ensure accurate attribution.- Provide stronger justification and testing of declarative/procedural mapping
  - Augment Section 5 and Appendix B.1–B.2 with empirical case studies illustrating edge cases (e.g., single environment with multiple episodes involving within-episode recall) to validate n_envs × n_eps rules (Equations (1)–(2); Table 1).
  - Offer theoretical or behavioral arguments tying n_envs/n_eps directly to procedural vs declarative manifestations in RL, beyond taxonomy (Section 4.2; Table 1).
  - Include an experimental probe where the same inner-loop POMDP is evaluated under different outer-loop regimes (n_envs/n_eps) to test whether the proposed classification predicts observed behavior.
  - Align empirical setups and claims: if experiments necessarily use multiple episodes (Appendix E.2; Section 6), either reclassify them under procedural memory per Definition 3 or adjust the declarative definition to accommodate Memory DM evaluated across episodes, and explicitly report n_eps.Score
- Overall (10): 6 — Strong formalization (Section 5; Definitions 1–6; Equations (1)–(6); Algorithm 1; Table 1) with clear demonstrations (Figures 4–5), but limited empirical scope and reporting inconsistencies (Section 6; Appendix E.1–E.2; missing/misnumbered figure in Section 5).
- Novelty (10): 6 — The quantitative framing with K, ξ, and K̄ (Section 5; Theorem 1; Equation (4)) is a useful synthesis, though related notions exist (Section 4.2); the taxonomy adds clarity but needs stronger empirical corroboration (Table 1; Section 6).
- Technical Quality (10): 5 — The core theorem is correct but straightforward (Section 5.1); experiments illustrate pitfalls (Figures 4–5) without quantifying the transitional interval (Section 5.1; Section 6.1) or including LTM-capable baselines (Section 6).
- Clarity (10): 7 — Generally clear with consistent notation and diagrams (Figures 1–2; Appendix A; Algorithm 1; Table 1), but misreferences/missing figure (Section 5 “Figure 3”; Related Work to Appendix C) and ξ sampling/reporting gaps (Section 6.1; Appendix E.2) reduce clarity.
- Confidence (5): 4 — Sufficient anchors for assessment, but narrow empirical scope and reporting inconsistencies (Appendix E.2; Section 6.1; Section 5 figure reference) limit certainty about generality.