{
  "paper": "SuRe_ Surprise-Driven Prioritised Replay for Continual LLM Learning",
  "runs": [
    {
      "config": {
        "judge_model": "gpt-5-2025-08-07",
        "review_agent": "paper_audit",
        "ai_model": "gpt-5-2025-08-07",
        "baseline_file": "baseline_review.txt",
        "final_file": "final_review_all.txt",
        "temperature": 0.0,
        "max_tokens": 20000,
        "timeout_s": 600,
        "metric": "regression_v2_minimal",
        "prompt_hash": "2142c04332"
      },
      "config_key": "1a46f14f5902ebb0fc2c14773cec6644d2fa5338",
      "inputs": {
        "baseline_review": "reviews/paper_audit/gpt-5-2025-08-07/paper_origin/baseline_review.txt",
        "final_review": "reviews/paper_audit/gpt-5-2025-08-07/paper_origin/final_review_all.txt"
      },
      "regression": {
        "score_delta": {
          "baseline_score": 7.0,
          "final_score": 7.0,
          "delta": 0.0,
          "scale_hint": "1-10"
        },
        "differences": [
          {
            "diff_type": "new_critique",
            "summary": "EMA variance scaling contradiction added",
            "paperaudit_types": [
              "METHOD_LOGIC_CONSISTENCY"
            ],
            "why_impacts_score": "Undermines theoretical coherence; weakens integration argument",
            "evidence": {
              "baseline_quote": "The fast–slow LoRA design… motivated by variance reduction B(β)=1/(1−β), including an intuitive lag–variance trade-off.",
              "final_quote": "Eq. (2) variance scales as 1/(1−β); Remark claims scaling by (1−β), a direct contradiction."
            }
          },
          {
            "diff_type": "intensified_critique",
            "summary": "Stronger SOTA/comparability critique with bolding/marker issues and PerTaskFT",
            "paperaudit_types": [
              "CLAIM_RESULT_DISTORTION",
              "RHETORICAL_PRESENTATION_MANIPULATION",
              "CONTEXT_MISALIGNMENT_INCOHERENCE"
            ],
            "why_impacts_score": "Overclaims SOTA and confuses rankings; reduces credibility",
            "evidence": {
              "baseline_quote": "Abstract states ‘state-of-the-art’ in LNT, but ProgPrompt 78.4 > Slow Surprise Replay 75.1.",
              "final_quote": "ProgPrompt 78.4 and PerTaskFT 78.1 exceed 75.1; bold/‘second best’ markers reduce interpretability."
            }
          },
          {
            "diff_type": "intensified_critique",
            "summary": "Expanded reporting issues: misreferences and mislabeled benchmarks",
            "paperaudit_types": [
              "EVIDENCE_DATA_INTEGRITY",
              "CONTEXT_MISALIGNMENT_INCOHERENCE"
            ],
            "why_impacts_score": "Inconsistent reporting harms reproducibility and reader trust",
            "evidence": {
              "baseline_quote": "Appendix tables include WSC, RC, SQuAD, DROP and omit QQP, MultiRC, IMDB.",
              "final_quote": "Section 4.2 misreferences ‘Table 11’ vs Table 2; Table 7 column mislabeled for LNT orders."
            }
          },
          {
            "diff_type": "new_critique",
            "summary": "Misattribution of negative forgetting corrected",
            "paperaudit_types": [
              "CLAIM_RESULT_DISTORTION",
              "EVIDENCE_DATA_INTEGRITY"
            ],
            "why_impacts_score": "Misstates results; inflates method’s forgetting performance",
            "evidence": {
              "baseline_quote": "Forgetting analyses show Slow Replay can yield negative forgetting (Table 9).",
              "final_quote": "Negative forgetting appears for Slow Surprise variants; Replay shows positive forgetting (Table 9)."
            }
          },
          {
            "diff_type": "intensified_critique",
            "summary": "Stronger critique of statistical markers and filtered comparisons",
            "paperaudit_types": [
              "RHETORICAL_PRESENTATION_MANIPULATION",
              "CLAIM_RESULT_DISTORTION"
            ],
            "why_impacts_score": "Ambiguous markers and comparisons mislead readers about rankings",
            "evidence": {
              "baseline_quote": "‘Second best’ markers are used without significance testing.",
              "final_quote": "Star/marker conventions and filtered comparisons are not clearly defined (Table 1; Table 7)."
            }
          },
          {
            "diff_type": "score_rationale_shift",
            "summary": "Technical Quality score lowered due to EMA inconsistency",
            "paperaudit_types": [
              "METHOD_LOGIC_CONSISTENCY"
            ],
            "why_impacts_score": "Internal inconsistency lowers technical confidence and quality score",
            "evidence": {
              "baseline_quote": "Technical Quality (10): 7 — Theory articulated; selection-term validation and efficiency metrics are missing.",
              "final_quote": "Technical Quality (10): 6 — Missing selection-term validation and an internal EMA scaling inconsistency reduces confidence."
            }
          },
          {
            "diff_type": "score_rationale_shift",
            "summary": "Clarity score lowered due to mislabeling/misreferences",
            "paperaudit_types": [
              "CONTEXT_MISALIGNMENT_INCOHERENCE",
              "RHETORICAL_PRESENTATION_MANIPULATION",
              "EVIDENCE_DATA_INTEGRITY"
            ],
            "why_impacts_score": "Additional labeling and cross-reference errors reduce clarity score",
            "evidence": {
              "baseline_quote": "Clarity (10): 6 — dataset/task inconsistencies and truncated Algorithm 1 reduce readability.",
              "final_quote": "Clarity (10): 5 — mislabeling/misreferences and presentation inconsistencies hinder readability and reproducibility."
            }
          }
        ]
      },
      "generated_at": "2026-01-05T20:43:10"
    }
  ]
}