# Innovation & Motivation Report

## 1. The Authors' Claimed Contribution
- Core Problem: Continual learning for large language models in class-incremental settings, especially when scaling to a large number of tasks (LNT), suffers from catastrophic forgetting and replay/regularization methods often trail multi-task learning.
- Claimed Gap: â€œFormalise two failure modes: selection (what to rehearse) and integration (how to consolidate updates), and argue they are additive and complementary.â€ The paper also claims replay has been undervalued in LLM CL when scaling to many tasks: â€œCL requires balancing plasticityâ€¦ and stabilityâ€¦ prior LLM CL often mixes online replay with methods using known task boundaries, leading to unfair comparisons.â€ The authors propose to â€œfocus on replay under fair assumptions (known task boundaries)â€¦ show surprise-based selection outperforms random replay, achieving state-of-the-art in LNT and best overall average across Standard CL and LNT.â€
- Proposed Solution: Surprise-prioritised Replay (SuRe), which â€œreplace[s] uniform buffer updates with sequence-level surprise computed as average NLL per token,â€ and a dual-learner design that â€œattach[es] two LoRA adapters: fast and slow headsâ€¦ update slow adapters via EMA: Î¸_t^{slow} â† Î² Î¸_{tâˆ’1}^{slow} + (1 âˆ’ Î²) Î¸_t^{fast}.â€ These are tied by a formal decomposition and bound: â€œExpected forgetting satisfies E ğ”½ â‰¤ A Â· D_{F_loc}(P_{1:Tâˆ’1}, q) + B(Ïˆ) Â· (Ïƒ^2/(Î¼ N)) + C Â· Î”_drift,â€ implying replay selection and EMA consolidation are complementary and neither can be driven to zero by adjusting only the other.

## 2. Comparative Scrutiny (The "Trial")
Here, we analyze how the paper stands against the identified similar works.

### vs. Adaptive Aggregation Networks for Class-Incremental Learning
- Identified Overlap: Both introduce dual pathways per layer to balance plasticity and stability: AANetsâ€™ â€œstableâ€ and â€œplasticâ€ residual blocks with learned aggregation; this paperâ€™s fast and slow LoRA adapters merged via EMA (Î²).
- Manuscript's Defense:
  - The paper does not cite AANets, but it explicitly frames and supports its dual-head consolidation with theory: â€œIntroduce dual learners with fast and slow LoRA adapters merged via EMA to enable rapid adaptation and stable consolidation.â€ In Method 3.3, the consolidation rule is specified: â€œÎ¸_t^{slow} â† Î² Î¸_{tâˆ’1}^{slow} + (1 âˆ’ Î²) Î¸_t^{fast}â€¦ equivalent geometric ensembleâ€¦ effective window â‰ˆ 1/(1âˆ’Î²).â€
  - It extends this with Lemma 2 and Theorem 1: EMA reduces integration variance, and forgetting decomposes additively into selection and integration terms, arguing complementarity rather than a single architectural trick.
- Reviewer's Assessment: The architectural motif of dual stable/plastic branches is well-established in CIL; the specific EMA-based dual LoRA instantiation for LLM CL is a practical, parameter-efficient variant. The theoretical decomposition (Eq. 3) helps motivate the design, but the EMA variance reduction is rooted in classic Polyak averaging. This is a meaningful application-level adaptation rather than a fundamentally new architectural principle.

### vs. Curious Replay for Model-based Adaptation
- Identified Overlap: Both prioritize replay based on prediction error/surprise to improve sample efficiency over uniform/Reservoir sampling.
- Manuscript's Defense:
  - The authors position their selection against replay baselines and prior â€œsurpriseâ€ work in vision/LLM CL: â€œInfoRS (Sun et al., 2022) combines surprise (Bayesian posterior) with learnabilityâ€¦ Maximally Interfered Retrieval (Aljundi et al., 2019)â€¦ prior LLM CL (Araujo et al., 2022) reported reservoir best and surprise selection poor.â€ They differentiate by â€œsequence-level surprise computed as average NLL per tokenâ€ (Eq. 4) and by task-balanced buffer allocation, and empirically show reversal of that prior trend in LNT.
  - They provide a theory link: surprise approximates gradient geometry (â€œ||âˆ‡â„“|| largeâ€) and reduces the IPM mismatch term, aligning selection with the formal bound (Eq. 3).
- Reviewer's Assessment: Prioritized replay by prediction error/curiosity is a known strategy; using sequence-level NLL within LLMs and tying it to IPM-based selection error is a reasonable, domain-appropriate instantiation with stronger empirical support than earlier â€œlabel surpriseâ€ variants. The novelty is incremental but well-motivated and empirically significant in LNT.

### vs. Generative Feature Replay For Class-Incremental Learning
- Identified Overlap: Combining replay to approximate the past distribution with a stabilization mechanism (distillation in GFR; EMA dual learners here) to mitigate forgetting.
- Manuscript's Defense:
  - The paper cites generative replay work: â€œGenerative Replay (Shin et al., 2017) and LAMOL (Sun et al., 2019): generate past samples; LAMOL requires task identity during training,â€ and positions SuRe as â€œarchitecture-agnosticâ€ exemplar-based replay that is robust under small buffers and low replay ratios.
  - It argues complementarity via Theorem 1: replay reduces selection error D_{F_loc}(P, q); EMA reduces integration variance.
- Reviewer's Assessment: The replay-plus-consolidation paradigm is standard; this manuscriptâ€™s contribution is not in avoiding exemplars but in improving exemplar selection and providing a LoRA-EMA consolidation tailored to LLMs. Relative to generative feature replay, the distinction is practical (LLM token-level NLL selection; PEFT) rather than conceptual.

### vs. Dual-Teacher Class-Incremental Learning With Data-Free Generative Replay
- Identified Overlap: Two-pronged strategyâ€”replay plus dual-path integration (EMA dual learners in this paper vs. dual-teacher distillation).
- Manuscript's Defense:
  - The paper does not cite dual-teacher distillation but justifies its dual learners through Lemma 2 and the EMA-based variance reduction, with explicit sensitivity analysis over Î² (Table 10).
  - It reports empirical gains and negative forgetting in LNT (Table 9), supporting that combining SuRe with EMA dual learners yields â€œadditional gains, including â€˜up to +5 accuracy pointsâ€™ on LNT over prior SOTA.â€
- Reviewer's Assessment: The dual-path consolidation idea is broadly parallel; the use of EMA instead of distillation and a PEFT LoRA instantiation fits LLM fine-tuning constraints. The novelty is primarily in adapting these ideas to LLM CL with a formal selectionâ€“integration framing; conceptually, this is an incremental variation.

### vs. Supervised Contrastive Replay: Revisiting the NCM Classifier in Online CIL
- Identified Overlap: Replay-centric mitigation of recency bias coupled with an averaging/low-variance stabilizer (EMA in this paper; NCM prototypes in SCR).
- Manuscript's Defense:
  - The manuscript emphasizes that selection mismatch and integration variance are complementary levers (Theorem 1) and that sequence-level NLL selection plus EMA consolidation yields â€œnegative forgettingâ€ (Table 9), addressing recency-like biases in LNT.
  - It contrasts settings: â€œfocus on replay under fair assumptions (known task boundaries),â€ while SCR targets online streams. It evaluates against strong LLM baselines (Reservoir, O-LoRA, LB-CL, MoRA).
- Reviewer's Assessment: The stabilizing-by-averaging principle is shared. This paperâ€™s contribution lies in the LLM-specific selection signal and EMA consolidation under task-structured CL, not in introducing a new stabilizer class. The motivation is defensible given the different modality and training protocol, but closely aligned in spirit.

### vs. PLoP: Precise LoRA Placement for Efficient Finetuning of Large Models
- Identified Overlap: Both operate within the LoRA PEFT paradigm and place adapters in attention (W_Q, W_V).
- Manuscript's Defense:
  - The paper cites LoRA and PEFT works (O-LoRA, LB-CL, MoRA) and fixes adapter placement: â€œLoRA rank 8 and Î± = 32; adapters on Q and V in all attention layers.â€ It does not study or cite placement optimization like PLoP.
- Reviewer's Assessment: The manuscript leverages standard adapter placement without investigating placement sensitivity or automation. This is a gap in motivation relative to PLoPâ€™s focus. However, the paperâ€™s core novelty is in replay selection and consolidation, not adapter placement; the omission weakens generality claims (â€œarchitecture-agnosticâ€) but does not negate the main contribution.

### vs. Dual-LoRA and Quality-Enhanced Pseudo Replay for Multimodal Continual Food Learning
- Identified Overlap: Dual LoRA modules combined with replay quality control.
- Manuscript's Defense:
  - The paper situates itself among LoRA-based CL baselines (O-LoRA, LB-CL, MoRA) and explicitly articulates a complementary selectionâ€“integration framework. It does not cite multimodal dual-LoRA works, but differentiates via EMA consolidation and NLL-based sample selection, plus formal bounds and LNT evaluation.
- Reviewer's Assessment: The dual-LoRA theme exists; this paperâ€™s EMA-driven consolidation and sequence-level surprise selection are plausible LLM-specific refinements. Overlap suggests the conceptual space is crowded; empirical SOTA in LNT and theoretical framing bolster motivation.

### vs. LoRA-FAIR: Federated LoRA Fine-Tuning with Aggregation and Initialization Refinement
- Identified Overlap: Careful aggregation of LoRA updates under data heterogeneity; EMA as a variance-reducing consolidator.
- Manuscript's Defense:
  - The paper references â€œModel merging with EMA (Hickok, 2025) and related sequential merging strategies,â€ and provides Lemma 2 quantifying variance reduction with Î² (and ablation in Table 10), but does not cite LoRA-FAIR or discuss aggregation bias specific to LoRA merging.
- Reviewer's Assessment: The consolidation principle is shared, but the problem settings differ (continual tasks vs federated clients). The EMA choice and its analysis are appropriate; novelty is limited relative to the broader literature on EMA/Polyak averaging.

## 3. Novelty Verdict
- Innovation Type: Application-Oriented with Incremental algorithmic novelty.
- Assessment:
  The manuscriptâ€™s core ideasâ€”prioritized replay using prediction error (sequence-level NLL), dual-path consolidation via EMA, and a formal additive decompositionâ€”have precedents across CIL, RL, and PEFT literatures. The contribution lies in adapting and unifying these ideas for LLM continual fine-tuning under LNT, grounding them with a clean theoretical bound and demonstrating strong empirical gains, including negative forgetting and robustness under tight replay budgets. The paper largely survives the comparison in its target domain (LLMs), but the conceptual overlap with established dual-path and prioritized replay paradigms means the novelty is more in integration and careful engineering than in introducing fundamentally new theory or mechanisms.
  - Strength:
    - Clear, theory-backed motivation: â€œwe formalise two failure modesâ€¦ additive and complementaryâ€ with Eq. 3 linking selection mismatch and integration variance.
    - Practical, LLM-specific instantiation: sequence-level NLL surprise (Eq. 4) and EMA dual LoRA (Eq. 5) with solid ablations (Î² sensitivity; buffer sizes; replay ratios).
    - Strong evidence: SOTA in LNT and best overall averages under controlled hyperparameters (Table 1â€“2); negative forgetting (Table 9); robustness under small buffers and low replay frequency.
    - Fair protocol emphasis: explicit assumption of known task boundaries and balanced buffer quotas, distinguishing from task-ID-at-inference methods and online-only baselines.
  - Weakness:
    - Conceptual overlap with prior dual-path stabilityâ€“plasticity architectures (AANets; dual-teacher; NCM/averaging) and prioritized replay (curiosity/MIR/InfoRS) reduces originality.
    - Limited engagement with adapter-placement optimization (e.g., PLoP), despite claiming â€œarchitecture-agnosticâ€ design; adapters are fixed to Q/V.
    - Theoretical components (IPM bound; EMA variance reduction) are adaptations of known results rather than new theory; Theorem 1 is a useful synthesis but not a deep mathematical advance.
    - Assumption of known task boundaries and per-task quotas narrows applicability to fully online settings; authors acknowledge this limitation.

## 4. Key Evidence Anchors
- Introduction/Abstract: â€œFormalise two failure modes: selectionâ€¦ and integrationâ€¦, and argue they are additive and complementary.â€ â€œPropose Surprise-prioritised Replay (SuRe)â€¦ architecture-agnostic.â€ â€œIntroduce dual learners with fast and slow LoRA adapters merged via EMAâ€¦â€
- Method 3.1 (Selectionâ€“Integration Decomposition): Theorem 1 â€œE ğ”½ â‰¤ A Â· D_{F_loc}(P_{1:Tâˆ’1}, q) + B(Ïˆ) Â· (Ïƒ^2/(Î¼ N)) + C Â· Î”_drift.â€ Lemma 1 (IPM selection mismatch). Lemma 2 (EMA variance reduction).
- Method 3.2 (Surprise Replay): Eq. 4 â€œsequence-level surprise computed as average NLL per token.â€
- Method 3.3 (Dual Learners): Eq. 5 â€œÎ¸_t^{slow} â† Î² Î¸_{tâˆ’1}^{slow} + (1 âˆ’ Î²) Î¸_t^{fast}â€¦ geometric ensembleâ€¦ effective window â‰ˆ 1/(1âˆ’Î²).â€
- Results: Table 1 (Slow Surprise Replay â€œ78.1â€ Standard avg; â€œ75.1â€ LNT avg; â€œup to +5 accuracy pointsâ€ over prior SOTA in LNT); Progressive Prompts â€œ78.4â€ but requires task identity, situating fairness.
- Hyperparameter-controlled comparison: Table 2 (Slow Surprise Replay â€œAll avg 78.0â€ best; LNT avg â€œ77.9â€ best).
- Forgetting: Table 9 (Negative forgetting: Standard â€œâˆ’2.30â€; LNT â€œâˆ’4.80â€).
- Ablations: Table 4â€“5 (robustness under small buffers and low replay ratios); Table 10 (Î² sensitivity: Î²=0.995 best; Î²=0.999 degrades); Table 11 (average surprise > sum surprise).
- Discussion (Limitations): â€œRequires known task boundaries during trainingâ€¦ Surprise computation requires extra forward passesâ€¦ extending to fully online settings is future work.â€
- Related Work positioning: â€œReservoir best and surprise selection poor in those specific setupsâ€ (contrast to current findings); cites MIR, generative replay, LoRA-based CL baselines, and EMA model merging.