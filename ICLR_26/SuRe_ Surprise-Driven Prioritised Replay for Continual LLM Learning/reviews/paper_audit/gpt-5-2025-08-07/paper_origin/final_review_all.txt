Summary
- The paper addresses catastrophic forgetting in continual learning for LLMs by decomposing error into two complementary sources: selection mismatch (what to replay) and integration variance (how updates are consolidated) (Section 3.1; Eq. (1)–(3); Appendix H). It introduces SuRe, a surprise-prioritized replay policy that selects sequences with high per-token NLL (Eq. (4); Section 3.2; Figure 1), and a dual-learner setup with fast and slow LoRA adapters merged via EMA (Eq. (5)–(6); Section 3.3; Algorithm 1). Experiments on Standard CL (DBpedia, AG News, Amazon, Yahoo; Section 4.1) and Large Number of Tasks (LNT; Section 4.1) show improved final accuracy over random/Reservoir replay and several CL baselines (Tables 1–2), with robustness to smaller buffers and lower replay ratios (Tables 4–5) and ablations on timing and surprise variants (Table 3; Table 11). The paper also reports forgetting metrics (Table 9) and preliminary results on Llama 3.1 8B (Table 7) and CPT perplexity (Table 8).Strengths
- Bold selection–integration framework with theory and practical design
  - The decomposition into selection mismatch and integration variance is formalized with an IPM bound (Lemma 1; Eq. (1)) and an EMA variance reduction analysis (Lemma 2; Eq. (2)), culminating in an additive forgetting bound (Theorem 1; Eq. (3); Section 3.1; Appendix H.1–H.3). This is technically sound and clarifies why replay choice and consolidation jointly matter.
  - The mapping to design knobs is explicit: lowering D_Floc(P,q) via buffer policies and lowering variance via EMA (Section 3.1, “Design implication”), improving clarity and potential impact on method selection.
  - The discussion connects to multi-timescale consolidation and variance control, providing interpretability for the EMA component (Section 3.1 Remarks; Section 3.3; Appendix H.2) which supports novelty in framing even if components are known.
- Simple, architecture-agnostic surprise-based replay
  - SuRe defines sequence surprise as average per-token NLL (Eq. (4); Section 3.2), an easily implementable criterion that aligns with importance sampling intuitions (Section 3.2 last paragraph; Appendix F).
  - The buffer policy is clearly specified: 2% total memory, equal per-task quotas regardless of dataset size (Section 3.2), facilitating reproducibility and fair comparisons across settings.
  - The method is presented as architecture-agnostic and compatible with PEFT/LoRA (Section 3.2), which matters for practical adoption and potential impact.
- Dual learner with EMA for consolidation
  - The fast–slow LoRA design, with EMA merging, is formally described (Eq. (5)–(6); Section 3.3) and motivated by variance reduction B(β)=1/(1−β), including an intuitive lag–variance trade-off (Section 3.3).
  - The implementation focuses LoRA on Q,V projections with rank/alpha specified (Appendix G), showing a practical and compute-aware architecture that is technically sound and comparable to common PEFT baselines.
  - Ablations on β show performance sensitivity and appropriate ranges (Table 10; Appendix D), evidencing experimental rigor on a key hyperparameter.
- Strong empirical performance and broad baseline coverage
  - On T5, Slow Surprise Replay achieves top or second-best averages across Standard CL orders and LNT orders among methods that do not require task identity at inference (Table 1; Section 4.2), reducing the gap to MTL (Table 1 Standard avg: 78.1 vs. 80.0).
  - Under Cao & Wu (2025) hyperparams, Slow Surprise Replay yields the best “All avg” (78.0) against O-LoRA/N-LoRA/AimMerging (Table 2; Appendix Table 6), supporting robustness to hyperparameter choices.
  - The claimed improvements are consistent across replay ratios and buffer sizes (Tables 4–5), indicating sample efficiency and stability, which enhances impact.
- Thorough ablations and forgetting analysis
  - Ablations on where/when to compute surprise and buffer updates, and dynamic surprise, are detailed (Table 3; Appendix F.2–F.3), strengthening the causal story behind performance gains (experimental rigor).
  - Forgetting/AP/FP analyses show Slow Replay can yield negative forgetting (Table 9), directly addressing the CL objective (technical soundness and impact).
  - Surprise variants (sum vs. average) are compared (Table 11), improving clarity on the chosen criterion and supporting method selection.
- Extensions beyond classification
  - Preliminary Llama 3.1 8B experiments suggest method transferability (Table 7; Appendix B.2), increasing potential impact.
  - CPT perplexity results show Slow Surprise outperforming alternatives on average (Table 8; Appendix B.3), indicating modality/task generality (impact), even if downstream accuracy is not evaluated (clearly stated).Weaknesses
- Ambiguous “state-of-the-art” framing and comparability
  - The Abstract states “SuRe achieves state-of-the-art performance in the LNT setting” (Abstract), but Table 1 reports ProgPrompt at 78.4 LNT avg and PerTaskFT at 78.1, both higher than Slow Surprise Replay at 75.1, albeit ProgPrompt requires task identity during training and inference and PerTaskFT trains separate models (Baselines; Table 1 caption). This risks overclaiming and affects clarity/credibility.
  - The Abstract further claims “best overall average across both Standard CL and LNT benchmarks” (Abstract), but Table 1 Standard avg shows MTL at 80.0 versus Slow Surprise Replay at 78.1; while MTL is an upper bound, this should be explicitly excluded in the claim to avoid confusion (Section 4.2; Table 1).
  - Bold and “second best” markers in Table 1 appear to treat some methods as out of scope without a clear rule (e.g., Slow Surprise Replay bolded while ProgPrompt/PerTaskFT/Mtl have higher numbers; MoRA marked “∗” second best at 72.0 in LNT Order-6 despite higher entries; Table 1 caption), which reduces interpretability.
- Dataset/task list inconsistencies and reporting gaps
  - LNT composition in Section 4.1 lists MNLI, QQP, RTE, SST-2, WiC, CB, COPA, BoolQ, MultiRC, IMDB (Section 4.1), but Appendix figures include WSC, RC, SQuAD, DROP and omit some listed ones in certain runs (Appendix Figures 4–5; Figure 4/5 tables), creating confusion (clarity, reproducibility).
  - Standard CL setup describes DBpedia, AG News, Amazon, Yahoo (Section 4.1), yet Appendix figures include Yelp (Figure 3; Figure 4–5 heatmaps), which is not listed for Standard CL or LNT in the main text (clarity).
  - Orders are referenced (Orders 1–6; Tables 1–2), but full task orderings and run counts are not consistently specified for main tables (Section 4.2; Table 1 captions), impacting reproducibility; additionally, Section 4.2 misreferences the hyperparameter-controlled comparison as “Table 11” whereas the correct table is Table 2 (Section 4.2; Table 2; Appendix E Table 11 covers sum vs average surprise).
  - The narrative in Section 4.2 attributes “Slow Replay maintains strong stability with negative forgetting” (Section 4.2), but Table 9 shows negative forgetting specifically for Slow Surprise variants (Slow Surprise and Slow SB-UA) while Replay shows positive forgetting (Table 9), leading to misattribution.
  - In Table 7 (Appendix B.2), the column “Standard CL Benchmark (Order-4)” is mislabeled given the paper’s own convention where Orders 4–6 correspond to LNT (Table 1; Section 4.2), causing confusion on which benchmark is being reported.
- Limited compute/efficiency reporting
  - Surprise computation requires an extra forward pass (Discussion; Section 5), but there are no runtime, FLOPs, or wall-clock comparisons vs Reservoir or other baselines (No direct evidence found in the manuscript), limiting technical completeness.
  - Buffer size and replay ratio for main results are mentioned in prose (e.g., “1:2 replay ratio” in Section 4.2; “2% buffer” in Section 3.2), but not annotated per row in Tables 1–2; this hinders precise reproducibility (clarity).
  - Replay frequency (“every other gradient step”) is set in Appendix G, but its effect is not reported in main results (Appendix G; No direct evidence found in the manuscript for runtime or memory), weakening the efficiency story.
- Theory-to-practice validation gap
  - While the selection mismatch term uses IPMs (Eq. (1); Section 3.1), there is no empirical measurement of D_Floc(P,q) or MMD under SuRe vs Reservoir (No direct evidence found in the manuscript), leaving the theoretical control unvalidated.
  - Appendix H.62 acknowledges no general proof that sequence-level surprise minimizes D_Floc without extra assumptions (Appendix H.62), so the link from surprise to selection-term tightening remains heuristic (technical rigor).
  - Constants A,B,C and σ^2, μ, N terms are not instantiated or estimated in experiments (Eq. (2) and (3); Appendix H), which limits practical interpretability of the bounds (clarity/technical quality).
- Assumption of known task boundaries and per-task quotas
  - The method requires known task boundaries and balanced per-task buffer allocation (Section 3.2; Discussion Section 5), limiting applicability to fully online or task-agnostic settings (impact).
  - The choice to allocate equal per-task quotas irrespective of dataset size (Section 3.2) may bias replay towards small tasks or under-represent large tasks, potentially affecting fairness and performance (technical soundness).
  - Extensions to online settings are discussed but not implemented (Discussion Section 5), leaving a key practical scenario unsupported (impact).
- Incomplete algorithmic description and code availability
  - Algorithm 1 is truncated after line 6 (Algorithm 1; Section 3.3), omitting replay sampling details, replacement policy specifics, and full training loop integration (clarity/reproducibility).
  - Buffer update schedules (Before/After/Online variants) are defined across text/appendix (Section 4.3; Appendix F.2), but Algorithm 1 does not reflect these branches (clarity).
  - No code release or repository link is provided (No direct evidence found in the manuscript), which affects reproducibility (impact).
- Statistical reporting and significance
  - Main tables (Table 1; Table 2) do not report standard deviations or confidence intervals; while Table 4–5 specify means over 3 runs × 3 orders, this is not consistent across all results (Section 4.2; Appendix Tables), limiting statistical confidence (experimental rigor).
  - “Second best” markers (∗) are used (Table 1; Table 3–5) without significance testing (No direct evidence found in the manuscript), which affects the reliability of ranking claims (technical quality).
  - Llama results are labeled “preliminary” and cover only two orders (Table 7; Appendix B.2), reducing confidence in cross-model generalization (impact); additionally, star/marker conventions and filtered comparisons are not clearly defined (Table 1; Table 7).
- Inconsistency in EMA variance scaling and interpretation
  - Eq. (2) shows the variance term scales as C_v·(1/(1−β))·(σ^2/(μ n)) (Section 3.1; Appendix H.2), which increases as β→1; however, the Remark below Eq. (2) states “the Slow Learner scales the variance term by (1−β)” and claims a ~200× reduction for β=0.995 (Section 3.1 Remark), a direct contradiction that impacts the integration control argument.
  - Section 3.3 says “In our bound, B(β) = 1/(1 − β)… larger β (e.g., 0.995) means stronger averaging, reducing the effective noise…” (Section 3.3), which conflicts with Eq. (2)’s scaling unless clarified (technical consistency).
  - The β ablation shows performance collapse at β=0.999 (Table 10), consistent with large β being harmful; reconciling this empirical trend with the bound and textual claims is necessary for interpretability (Table 10; Appendix D).Suggestions for Improvement
- Clarify SOTA scope and comparability
  - Explicitly restrict SOTA claims to methods not requiring task identity at inference, and reflect this in the Abstract and Section 4.2 (Abstract; Table 1 caption), avoiding overclaiming relative to ProgPrompt and PerTaskFT.
  - State in Table 1/2 captions the precise assumption set (known task boundaries, single shared LoRA vs per-task modules) and whether MTL/prompt-based methods are treated as upper bounds or out of scope (Tables 1–2), and align bolding rules accordingly.
  - When importing results (⋄/♦), summarize any differences in evaluation protocols (e.g., batch sizes, buffer ratios) to improve fairness transparency (Table 2; Appendix Table 6).
- Align dataset/task lists and reporting
  - Reconcile the LNT task set throughout: ensure tables include the tasks listed in Section 4.1 (MNLI, QQP, RTE, SST-2, WiC, CB, COPA, BoolQ, MultiRC, IMDB) or update Section 4.1 to match the actual used set (Appendix Figures 4–5).
  - Remove or justify additional tasks (WSC, RC, SQuAD, DROP, Yelp) and provide a consolidated table of datasets, sizes, and orders used in each experiment (Section 4.1; Appendix B), improving clarity and reproducibility.
  - Provide complete task orderings and run counts for Table 1–2, matching the detailed reporting style of Tables 4–5 (Section 4.2; Table 1–2 captions); correct cross-references (e.g., Section 4.2’s “Table 11” should point to Table 2) to aid verification.
  - Align narrative statements about forgetting with Table 9 (e.g., attribute negative forgetting to the Slow Surprise variants rather than vanilla Replay) (Section 4.2; Table 9).
  - Correct benchmark labels in Table 7 to reflect that Orders 4–6 correspond to LNT (Appendix B.2; Table 7).
- Report efficiency metrics and settings per result
  - Include runtime (training/inference), memory, and additional forward-pass overhead for SuRe vs Reservoir and other baselines, ideally on the same hardware (Discussion Section 5; No direct evidence found).
  - Annotate buffer size, replay ratio, and replay frequency in each main table row (Table 1–2), or add a global settings footnote to remove ambiguity (Section 3.2; Appendix G).
  - Provide sensitivity analysis for replay frequency (beyond buffer size/ratio) and its impact on accuracy and time (Appendix G), supporting the efficiency narrative.
- Empirically validate the theory’s selection term
  - Measure D_Floc(P,q) (e.g., MMD with an appropriate kernel) for SuRe vs Reservoir to empirically test Eq. (1) and Theorem 1’s selection addend (Section 3.1; Appendix H.1–H.3).
  - Compare gradient norm distributions and effective gradient alignment between SuRe buffers and past-task gradients to support the importance-sampling intuition (Section 3.2 last paragraph; Appendix F).
  - If feasible, instantiate constants or report proxies (variance estimates, σ^2/μN) to connect bounds (Eq. (2)–(3)) to observed forgetting (Table 9), improving practical interpretability.
- Address the known-boundaries and per-task quota limitations
  - Implement and evaluate an online variant with task-agnostic buffer rebalancing or shift detection (Discussion Section 5), and compare against online Reservoir/GSS baselines (No direct evidence found).
  - Explore proportional per-task quotas (scaled by dataset size or class distribution) and report their effect on LNT performance (Section 3.2), improving fairness and realism.
  - Include experiments without known task boundaries (e.g., streaming with unknown shifts) to demonstrate robustness beyond the current assumption set (Discussion Section 5).
- Complete algorithmic description and share code
  - Expand Algorithm 1 to show the full training loop: replay sampling strategy, replacement policy, buffer maintenance, and the Before/After/Online update branches (Algorithm 1; Appendix F.2).
  - Integrate the ablation variants into pseudocode or provide separate algorithms for SB-UB, SB-UA, SA-UA, and Surprise-with-Updates (Section 4.3; Appendix F.2–F.3), improving clarity.
  - Release an anonymized code repository with configuration files matching Tables 1–5 and Appendix settings (No direct evidence found), enabling verification and reuse.
- Strengthen statistical reporting
  - Report means ± standard deviations (or confidence intervals) for Table 1–2 across multiple seeds and orders, matching the style in Tables 4–5 (“Means over 3 runs × 3 task orders”) (Section 4.2; Table 1–2).
  - Conduct significance tests (e.g., paired t-tests across seeds/orders) to support “best/second-best” claims (Tables 1–3), increasing rigor.
  - Audit “∗ second best” markers for consistency and define any filtering criteria used (e.g., excluding task-identity-requiring methods) directly in captions (Table 1; Table 7), to prevent misinterpretation.
- Resolve EMA variance scaling and reconcile theory/text
  - Correct the Remark under Eq. (2) to align with the stated bound (Section 3.1; Appendix H.2), or revise Eq. (2) and its derivation if the intended scaling is (1−β); ensure consistency across Section 3.1, Section 3.3, and Appendix H.
  - Clarify in Section 3.3 how B(β)=1/(1−β) interacts with variance reduction, explicitly explaining why larger β may reduce high-frequency noise while potentially increasing the bound’s variance coefficient (Section 3.3; Eq. (2)).
  - Bridge the theoretical discussion with the empirical β ablation (Table 10), explaining why very large β (e.g., 0.999) degrades performance and specifying recommended ranges with rationale (Table 10; Appendix D).Score
- Overall (10): 7 — Strong framing and empirical gains across CL benchmarks (Section 3.1; Tables 1–2; Tables 4–5), with several clear reporting and comparability issues (Abstract; Table 1 bolding; Section 4.2 misreference) and an EMA scaling inconsistency (Eq. (2) vs Remark) tempering impact.
- Novelty (10): 7 — The selection–integration decomposition and SuRe+EMA combination provide a coherent, moderately novel perspective (Section 3.1; Eq. (1)–(3)), though components (prioritized replay; EMA) are known and adapted.
- Technical Quality (10): 6 — Broad experiments and ablations (Tables 3–5, 9–11) support claims, but theory-to-practice validation of the selection term is missing (No direct evidence found), and an internal inconsistency in EMA variance scaling (Eq. (2); Section 3.1 Remark; Section 3.3) reduces confidence.
- Clarity (10): 5 — Method and equations are generally clear (Eq. (4)–(6); Figure 1), but dataset/task inconsistencies (Section 4.1 vs Appendix), mislabeling/misreferences (Section 4.2 Table 11 vs Table 2; Table 7 labeling), and truncated Algorithm 1 (Section 3.3) hinder readability and reproducibility.
- Confidence (5): 4 — Conclusions are supported by multiple tables and ablations (Tables 1–5; 9–11), yet missing code, limited statistical reporting in main results, presentation inconsistencies, and the EMA scaling contradiction warrant moderate caution.