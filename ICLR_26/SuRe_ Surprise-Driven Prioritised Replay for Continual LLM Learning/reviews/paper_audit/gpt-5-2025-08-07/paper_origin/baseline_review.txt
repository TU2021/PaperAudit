Summary
- The paper addresses catastrophic forgetting in continual learning for LLMs by decomposing error into two complementary sources: selection mismatch (what to replay) and integration variance (how updates are consolidated) (Section 3.1; Eq. (1)–(3); Appendix H). It introduces SuRe, a surprise-prioritized replay policy that selects sequences with high per-token NLL (Eq. (4); Section 3.2; Figure 1), and a dual-learner setup with fast and slow LoRA adapters merged via EMA (Eq. (5)–(6); Section 3.3; Algorithm 1). Experiments on Standard CL (DBpedia, AG News, Amazon, Yahoo; Section 4.1) and Large Number of Tasks (LNT; Section 4.1) show improved final accuracy over random/Reservoir replay and several CL baselines (Tables 1–2), with robustness to smaller buffers and lower replay ratios (Tables 4–5) and ablations on timing and surprise variants (Table 3; Table 11). The paper also reports forgetting metrics (Table 9) and preliminary results on Llama 3.1 8B (Table 7) and CPT perplexity (Table 8).Strengths
- Bold selection–integration framework with theory and practical design
  - The decomposition into selection mismatch and integration variance is formalized with an IPM bound (Lemma 1; Eq. (1)) and an EMA variance reduction analysis (Lemma 2; Eq. (2)), culminating in an additive forgetting bound (Theorem 1; Eq. (3); Section 3.1; Appendix H.1–H.3). This is technically sound and clarifies why replay choice and consolidation jointly matter.
  - The mapping to design knobs is explicit: lowering D_Floc(P,q) via buffer policies and lowering variance via EMA (Section 3.1, “Design implication”), improving clarity and potential impact on method selection.
  - The discussion connects to multi-timescale consolidation and variance control, providing interpretability for the EMA component (Section 3.1 Remarks; Section 3.3; Appendix H.2) which supports novelty in framing even if components are known.- Simple, architecture-agnostic surprise-based replay
  - SuRe defines sequence surprise as average per-token NLL (Eq. (4); Section 3.2), an easily implementable criterion that aligns with importance sampling intuitions (Section 3.2 last paragraph; Appendix F).
  - The buffer policy is clearly specified: 2% total memory, equal per-task quotas regardless of dataset size (Section 3.2), facilitating reproducibility and fair comparisons across settings.
  - The method is presented as architecture-agnostic and compatible with PEFT/LoRA (Section 3.2), which matters for practical adoption and potential impact.- Dual learner with EMA for consolidation
  - The fast–slow LoRA design, with EMA merging, is formally described (Eq. (5)–(6); Section 3.3) and motivated by variance reduction B(β)=1/(1−β), including an intuitive lag–variance trade-off (Section 3.3).
  - The implementation focuses LoRA on Q,V projections with rank/alpha specified (Appendix G), showing a practical and compute-aware architecture that is technically sound and comparable to common PEFT baselines.
  - Ablations on β show performance sensitivity and appropriate ranges (Table 10; Appendix D), evidencing experimental rigor on a key hyperparameter.- Strong empirical performance and broad baseline coverage
  - On T5, Slow Surprise Replay achieves top or second-best averages across Standard CL orders and LNT orders among methods that do not require task identity at inference (Table 1; Section 4.2), reducing the gap to MTL (Table 1 Standard avg: 78.1 vs. 80.0).
  - Under Cao & Wu (2025) hyperparams, Slow Surprise Replay yields the best “All avg” (78.0) against O-LoRA/N-LoRA/AimMerging (Table 2; Appendix Table 6), supporting robustness to hyperparameter choices.
  - The claimed improvements are consistent across replay ratios and buffer sizes (Tables 4–5), indicating sample efficiency and stability, which enhances impact.- Thorough ablations and forgetting analysis
  - Ablations on where/when to compute surprise and buffer updates, and dynamic surprise, are detailed (Table 3; Appendix F.2–F.3), strengthening the causal story behind performance gains (experimental rigor).
  - Forgetting/AP/FP analyses show Slow Replay can yield negative forgetting (Table 9), directly addressing the CL objective (technical soundness and impact).
  - Surprise variants (sum vs. average) are compared (Table 11), improving clarity on the chosen criterion and supporting method selection.- Extensions beyond classification
  - Preliminary Llama 3.1 8B experiments suggest method transferability (Table 7; Appendix B.2), increasing potential impact.
  - CPT perplexity results show Slow Surprise outperforming alternatives on average (Table 8; Appendix B.3), indicating modality/task generality (impact), even if downstream accuracy is not evaluated (clearly stated).Weaknesses
- Ambiguous “state-of-the-art” framing and comparability
  - The Abstract states “SuRe achieves state-of-the-art performance in the LNT setting” (Abstract), but Table 1 reports ProgPrompt at 78.4 LNT avg, higher than Slow Surprise Replay at 75.1, albeit ProgPrompt requires task identity during training and inference (Baselines; Table 1 caption). This risks overclaiming and affects clarity/credibility.
  - The Abstract further claims “best overall average across both Standard CL and LNT benchmarks” (Abstract), but Table 1 Standard avg shows MTL at 80.0 versus Slow Surprise Replay at 78.1; while MTL is an upper bound, this should be explicitly excluded in the claim to avoid confusion (Section 4.2; Table 1).
  - Some results are taken from other sources (Qiao & Mahdavi, 2024; Lu et al., 2025; Cao & Wu, 2025) with potential differences in assumptions; although marks ⋄/♦ indicate sourcing (Table 1; Table 2), the SOTA framing does not consistently state the assumption set (why it matters: fairness and interpretability).- Dataset/task list inconsistencies and reporting gaps
  - LNT composition in Section 4.1 lists MNLI, QQP, RTE, SST-2, WiC, CB, COPA, BoolQ, MultiRC, IMDB (Section 4.1), but Appendix tables include WSC, RC, SQuAD, DROP and omit QQP, MultiRC, IMDB (Appendix Figures 4–5; Block #41–#42), creating confusion (clarity, reproducibility).
  - Standard CL setup describes DBpedia, AG News, Amazon, Yahoo (Section 4.1), yet Appendix tables include Yelp (Figure 4; Block #41), which is not listed for Standard CL or LNT in the main text (clarity).
  - Orders are referenced (Orders 1–6; Tables 1–2), but full task orderings and run counts are not consistently specified for main tables (Section 4.2; Table 1 captions), impacting reproducibility.- Limited compute/efficiency reporting
  - Surprise computation requires an extra forward pass (Discussion; Section 5), but there are no runtime, FLOPs, or wall-clock comparisons vs Reservoir or other baselines (No direct evidence found in the manuscript), limiting technical completeness.
  - Buffer size and replay ratio for main results are mentioned in prose (e.g., “1:2 replay ratio” in Section 4.2; “2% buffer” in Section 3.2), but not annotated per row in Tables 1–2; this hinders precise reproducibility (clarity).
  - Replay frequency (“every other gradient step”) is set in Appendix G, but its effect is not reported in main results (Appendix G; No direct evidence found in the manuscript for runtime or memory), weakening the efficiency story.- Theory-to-practice validation gap
  - While the selection mismatch term uses IPMs (Eq. (1); Section 3.1), there is no empirical measurement of D_Floc(P,q) or MMD under SuRe vs Reservoir (No direct evidence found in the manuscript), leaving the theoretical control unvalidated.
  - Appendix H.62 acknowledges no general proof that sequence-level surprise minimizes D_Floc without extra assumptions (Appendix H.62), so the link from surprise to selection-term tightening remains heuristic (technical rigor).
  - Constants A,B,C and σ^2, μ, N terms are not instantiated or estimated in experiments (Eq. (2) and (3); Appendix H), which limits practical interpretability of the bounds (clarity/technical quality).- Assumption of known task boundaries and per-task quotas
  - The method requires known task boundaries and balanced per-task buffer allocation (Section 3.2; Discussion Section 5), limiting applicability to fully online or task-agnostic settings (impact).
  - The choice to allocate equal per-task quotas irrespective of dataset size (Section 3.2) may bias replay towards small tasks or under-represent large tasks, potentially affecting fairness and performance (technical soundness).
  - Extensions to online settings are discussed but not implemented (Discussion Section 5), leaving a key practical scenario unsupported (impact).- Incomplete algorithmic description and code availability
  - Algorithm 1 is truncated after line 6 (Algorithm 1; Section 3.3), omitting replay sampling details, replacement policy specifics, and full training loop integration (clarity/reproducibility).
  - Buffer update schedules (Before/After/Online variants) are defined across text/appendix (Section 4.3; Appendix F.2), but Algorithm 1 does not reflect these branches (clarity).
  - No code release or repository link is provided (No direct evidence found in the manuscript), which affects reproducibility (impact).- Statistical reporting and significance
  - Main tables (Table 1; Table 2) do not report standard deviations or confidence intervals; while Table 4–5 specify means over 3 runs × 3 orders, this is not consistent across all results (Section 4.2; Appendix Tables), limiting statistical confidence (experimental rigor).
  - “Second best” markers (∗) are used (Table 1; Table 3–5) without significance testing (No direct evidence found in the manuscript), which affects the reliability of ranking claims (technical quality).
  - Llama results are labeled “preliminary” and cover only two orders (Table 7; Appendix B.2), reducing confidence in cross-model generalization (impact).Suggestions for Improvement
- Clarify SOTA scope and comparability
  - Explicitly restrict SOTA claims to methods not requiring task identity at inference, and reflect this in the Abstract and Section 4.2 (Abstract; Table 1 caption), avoiding overclaiming relative to ProgPrompt.
  - State in Table 1/2 captions the precise assumption set (known task boundaries, single shared LoRA vs per-task modules) and whether MTL/propt-based methods are treated as upper bounds or out of scope (Tables 1–2).
  - When importing results (⋄/♦), summarize any differences in evaluation protocols (e.g., batch sizes, buffer ratios) to improve fairness transparency (Table 2; Appendix Table 6).- Align dataset/task lists and reporting
  - Reconcile the LNT task set throughout: ensure tables include the tasks listed in Section 4.1 (MNLI, QQP, RTE, SST-2, WiC, CB, COPA, BoolQ, MultiRC, IMDB) or update Section 4.1 to match the actual used set (Appendix Figures 4–5; Block #41–#42).
  - Remove or justify additional tasks (WSC, RC, SQuAD, DROP, Yelp) and provide a consolidated table of datasets, sizes, and orders used in each experiment (Section 4.1; Appendix B), improving clarity and reproducibility.
  - Provide complete task orderings and run counts for Table 1–2, matching the detailed reporting style of Tables 4–5 (Section 4.2; Table 1–2 captions).- Report efficiency metrics and settings per result
  - Include runtime (training/inference), memory, and additional forward-pass overhead for SuRe vs Reservoir and other baselines, ideally on the same hardware (Discussion Section 5; No direct evidence found).
  - Annotate buffer size, replay ratio, and replay frequency in each main table row (Table 1–2), or add a global settings footnote to remove ambiguity (Section 3.2; Appendix G).
  - Provide sensitivity analysis for replay frequency (beyond buffer size/ratio) and its impact on accuracy and time (Appendix G), supporting the efficiency narrative.- Empirically validate the theory’s selection term
  - Measure D_Floc(P,q) (e.g., MMD with an appropriate kernel) for SuRe vs Reservoir to empirically test Eq. (1) and Theorem 1’s selection addend (Section 3.1; Appendix H.1–H.3).
  - Compare gradient norm distributions and effective gradient alignment between SuRe buffers and past-task gradients to support the importance-sampling intuition (Section 3.2 last paragraph; Appendix F).
  - If feasible, instantiate constants or report proxies (variance estimates, σ^2/μN) to connect bounds (Eq. (2)–(3)) to observed forgetting (Table 9), improving practical interpretability.- Address the known-boundaries and per-task quota limitations
  - Implement and evaluate an online variant with task-agnostic buffer rebalancing or shift detection (Discussion Section 5), and compare against online Reservoir/GSS baselines (No direct evidence found).
  - Explore proportional per-task quotas (scaled by dataset size or class distribution) and report their effect on LNT performance (Section 3.2), improving fairness and realism.
  - Include experiments without known task boundaries (e.g., streaming with unknown shifts) to demonstrate robustness beyond the current assumption set (Discussion Section 5).- Complete algorithmic description and share code
  - Expand Algorithm 1 to show the full training loop: replay sampling strategy, replacement policy, buffer maintenance, and the Before/After/Online update branches (Algorithm 1; Appendix F.2).
  - Integrate the ablation variants into pseudocode or provide separate algorithms for SB-UB, SB-UA, SA-UA, and Surprise-with-Updates (Section 4.3; Appendix F.2–F.3), improving clarity.
  - Release an anonymized code repository with configuration files matching Tables 1–5 and Appendix settings (No direct evidence found), enabling verification and reuse.- Strengthen statistical reporting
  - Report means ± standard deviations (or confidence intervals) for Table 1–2 across multiple seeds and orders, matching the style in Tables 4–5 (“Means over 3 runs × 3 task orders”) (Section 4.2; Table 1–2).
  - Conduct significance tests (e.g., paired t-tests across seeds/orders) to support “best/second-best” claims (Tables 1–3), increasing rigor.
  - For Llama (Table 7), expand to all orders and more seeds, or label these as exploratory and defer SOTA claims until coverage matches T5 experiments (Appendix B.2).Score
- Overall (10): 7 — Strong framing and empirical gains across CL benchmarks (Section 3.1; Tables 1–2; Tables 4–5), but clarity/comparability issues (Abstract; Table 1 vs ProgPrompt) and limited efficiency/statistical reporting (Discussion; Table 1–2) temper impact.
- Novelty (10): 7 — The selection–integration decomposition and SuRe+EMA combination provide a coherent, moderately novel perspective (Section 3.1; Eq. (1)–(3)), though components (prioritized replay; EMA) are known and adapted.
- Technical Quality (10): 7 — Theory is well-articulated under local assumptions (Appendix H), and experiments are broad with ablations (Tables 3–5, 9–11), but the selection-term validation and efficiency metrics are missing (No direct evidence found).
- Clarity (10): 6 — Method and equations are clear (Eq. (4)–(6); Figure 1), but dataset/task inconsistencies (Section 4.1 vs Appendix) and truncated Algorithm 1 (Section 3.3) reduce readability and reproducibility.
- Confidence (5): 4 — Conclusions are supported by multiple tables and ablations (Tables 1–5; 9–11), yet missing code, limited statistical reporting in main results, and dataset list mismatches warrant moderate caution.