# Global Summary
The paper addresses catastrophic forgetting in continual learning for large language models (LLMs), focusing on Class-Incremental scenarios and especially the Large Number of Tasks (LNT) setting. It decomposes forgetting into two additive sources: selection error (what to replay) and integration error (how to consolidate updates). The core method, Surprise-prioritised Replay (SuRe), ranks and stores the most surprising sequences using sequence-level negative log-likelihood (NLL), combined with a dual-learner design that uses fast and slow LoRA adapters merged via exponential moving average (EMA). Evaluations on T5 across Standard CL (three task orders) and LNT (three task orders) show SuRe and its Slow Surprise variant achieve state-of-the-art results claimed by the authors, including improvements of up to ‚Äú+5 accuracy points‚Äù in LNT over prior SOTA. Key numbers: Slow Surprise Replay achieves ‚Äú78.1‚Äù average accuracy on Standard CL and ‚Äú75.1‚Äù average accuracy on LNT (Table 1); under hyperparameters from Cao & Wu (2025), Slow Surprise Replay achieves an ‚ÄúAll avg‚Äù of ‚Äú78.0‚Äù (Table 2), reportedly best across both benchmarks. Ablations demonstrate robustness under reduced replay ratios (down to 1:16) and small memory buffers (150‚Äì500 samples), and show negative forgetting for slow variants (e.g., ‚Äú‚àí4.80‚Äù on LNT; Table 9). Caveats: the approach assumes known task boundaries during training, and computing surprise incurs extra forward passes; extending to fully online settings and broader model families is outlined as future work.

# Abstract
- Problem: Continual learning (CL) for LLMs suffers from catastrophic forgetting; replay and regularisation trail multi-task learning when scaling to many tasks.
- Contributions:
  - Formalise two failure modes: selection (what to rehearse) and integration (how to consolidate updates), and argue they are additive and complementary.
  - Propose Surprise-prioritised Replay (SuRe): rank/store most surprising sequences by high sequence NLL; architecture-agnostic.
  - Introduce dual learners with fast and slow LoRA adapters merged via EMA to enable rapid adaptation and stable consolidation.
- Claims:
  - SuRe achieves state-of-the-art in LNT and best overall average across Standard CL and LNT benchmarks.
  - Combining SuRe with dual learners yields additional gains, including ‚Äúup to +5 accuracy points‚Äù on LNT over prior SOTA.
  - Robustness under reduced replay frequency and small buffer size; effectiveness and sample efficiency.
- Overall conclusion: Surprise-based selection and slow-weight consolidation are complementary components to mitigate catastrophic forgetting; replay is a strong baseline for continual LLM fine-tuning.

# Introduction
- Context: CL requires balancing plasticity (adaptation to new tasks) and stability (retention), with catastrophic forgetting especially challenging in Class-Incremental settings and when dealing with many tasks; prior LLM CL often mixes online replay with methods using known task boundaries, leading to unfair comparisons.
- Proposal:
  - Formalise catastrophic forgetting as the sum of selection error (mismatch between replayed and true past distributions) and integration error (instability from consolidating updates), asserting additivity and complementarity.
  - Focus on replay under fair assumptions (known task boundaries) and show surprise-based selection outperforms random replay, achieving state-of-the-art in LNT and best overall average across Standard CL and LNT.
  - For integration error, add EMA-based consolidation in a dual-learner design (fast vs slow LoRA).
- Summary of contributions:
  1) Formal decomposition motivating complementary mechanisms.
  2) Surprise-prioritised Replay (SuRe) for efficient sample selection.
  3) Combining SuRe with EMA dual learners yields strong performance, SOTA in LNT, and best average across both benchmarks, with gains ‚Äúup to +5 points‚Äù over prior methods.

# Related Work
- Replay methods:
  - Experience Replay (Reservoir sampling) (Rolnick et al., 2019; Chaudhry et al., 2019; Vitter, 1985) often strongest baseline; prior RL comparisons (Isele & Cosgun, 2018) and LLM CL (Araujo et al., 2022) reported reservoir best and surprise selection poor in those specific setups.
  - InfoRS (Sun et al., 2022) combines surprise (Bayesian posterior) with learnability to update memory.
  - Maximally Interfered Retrieval (Aljundi et al., 2019): selects memory samples expected to incur maximal loss increase.
  - Generative Replay (Shin et al., 2017) and LAMOL (Sun et al., 2019): generate past samples; LAMOL requires task identity during training.
- Parameter-efficient continual learning (PEFT):
  - LoRA adapters (Hu et al., 2021) widely used.
  - O-LoRA (Wang et al., 2023): per-dataset LoRA with orthogonality.
  - Learn More, Bother Less (Qiao & Mahdavi, 2024): SVD init and gradient projection into orthogonal subspaces.
  - Progressive Prompts (Razdaibiedina et al., 2023): per-task soft prompts; effective in LNT but needs task identity during training/inference.
  - Model merging with EMA (Hickok, 2025) and related sequential merging strategies investigated.

# Method
- 3.1 Selection‚ÄìIntegration Decomposition:
  - Setup: Tasks 1,‚Ä¶,t with per-task risk R_k(Œ∏) = E_{z‚àºP_k} ‚Ñì(Œ∏; z). Past mixture P_{1:t-1} = (1/(t‚àí1)) ‚àë_{k<t} P_k. Replay buffer induces q with replay risk \tilde{R}_{1:t-1}(Œ∏) = E_{z‚àºq} ‚Ñì(Œ∏; z). Consider local function class F_loc and IPM D_{F_loc}(P,Q).
  - Lemma 1: | \tilde{R}_{1:t-1}(Œ∏) ‚àí R_{1:t-1}(Œ∏) | ‚â§ D_{F_loc}(P_{1:t-1}, q). (Eq. 1)
  - Lemma 2 (EMA reduces integration variance): For slow EMA parameters Œ∏_s with rate Œ≤,
    E[ R_k(Œ∏_s^{(n)}) ‚àí R_k(Œ∏_k^*) ] ‚â§ C_b(1‚àíŒ≤) + C_v (1/(1‚àíŒ≤)) (œÉ^2/(Œº n)) + C_d Œ¥. (Eq. 2)
    Remark: For Œ≤ = 0.995, effective variance reduced by ~200√ó compared to single learner (Œ≤=0).
  - Theorem 1 (Additive bound): Expected forgetting satisfies
    E ùîΩ ‚â§ A ¬∑ D_{F_loc}(P_{1:T‚àí1}, q) + B(œà) ¬∑ (œÉ^2/(Œº N)) + C ¬∑ Œî_drift. (Eq. 3)
    With finite memory and steps, neither term can be driven to zero by tuning the other; replay (selection) and EMA (integration) are complementary.
- 3.2 Surprise Replay:
  - Replace uniform buffer updates with sequence-level surprise computed as average NLL per token:
    s_Œ∏(z_i) = ‚àí(1/T_i) ‚àë_{t=1}^{T_i} log p_Œ∏(z_{i,t} | z_{i,<t}, x_i). (Eq. 4)
  - Buffer size S set to ‚Äú2%‚Äù of the overall dataset (Rolnick et al., 2019), with equal per-task quota m_i^{(d)} = [S/d] after training on d datasets (task-identity-aware allocation).
  - Hypothesis: Prioritising high-NLL sequences focuses computation on underrepresented/high-error regions, approximates gradient geometry (||‚àá‚Ñì|| large), and reduces the IPM mismatch term.
- 3.3 Dual Learners:
  - Base model frozen; for each attention layer‚Äôs W_Q and W_V, attach two LoRA adapters: fast and slow heads (A randomly initialised; B = 0).
  - Before training on dataset D_i, compute surprise (Eq. 4) and insert m_i most surprising sequences into buffer M.
  - Train fast adapters on union batch ‚Ñ¨_t ‚äÇ D_i ‚à™ M (cross-entropy); update slow adapters via EMA:
    Œ∏_t^{slow} ‚Üê Œ≤ Œ∏_{t‚àí1}^{slow} + (1 ‚àí Œ≤) Œ∏_t^{fast}. (Eq. 5)
    Equivalent geometric ensemble Œ∏_t^{slow} = (1 ‚àí Œ≤) ‚àë_{k=0}^t Œ≤^k Œ∏_{t‚àík}^{fast} (effective window ‚âà 1/(1‚àíŒ≤)); minimises exponentially weighted least squares to fast history (Eq. 6).
  - Design intuition: Fast learner adapts quickly; slow learner low-pass filters updates, aggregating stable changes and mitigating forgetting. In the bound, B(Œ≤) = 1/(1 ‚àí Œ≤).
- Implementation notes (Appendix H/G):
  - Assumptions: local L-smoothness and Œº-PL, bounded per-example gradients, bounded task drift Œ¥.
  - Hyperparameters (Appendix G): learning rate 1e‚àí3 (T5-Large), 1e‚àí4 (Llama 3.1 8B); batch size 64; replay frequency 1/2 (every other gradient step); one epoch; dropout 0.1; LoRA rank 8 and Œ± = 32; adapters on Q and V in all attention layers.

# Experiments
- Datasets:
  - Standard CL: includes DBpedia and Yahoo Answers (Zhang et al., 2016). Following Wang et al. (2023), ‚Äú5,000‚Äù training samples and ‚Äú500‚Äù test sequences per dataset. Other datasets in this benchmark are not fully specified in this section.
  - LNT: 11 additional datasets: MNLI, QQP, RTE, SST-2 (Wang et al., 2019), WiC, CB, COPA, BoolQ, MultiRC, IMDB (Wang et al., 2020). ‚Äú1,000‚Äù training and ‚Äú500‚Äù test samples per dataset.
- Metrics:
  - Final Performance (FP): average accuracy across all tasks after training on the final task: FP = (1/N) ‚àë a_{T_i}^N.
  - Average Performance (AP) and Forgetting also reported in Appendix C/Table 9.
- Baselines:
  - Multi-Task Learning (MTL) (upper bound); Sequential Fine-Tuning (SeqFT); Elastic Weight Consolidation (EWC); Reservoir Replay (ER); O-LoRA (Wang et al., 2023); LB-CL (Qiao & Mahdavi, 2024); MoRA (Lu et al., 2025); Progressive Prompts (Razdaibiedina et al., 2023); AimMerging (Feng et al., 2025); PerTaskFT; SeqLoRA; N-LoRA (Cao & Wu, 2025).
- Main results (Table 1; T5):
  - Standard CL avg:
    - Slow Surprise Replay: ‚Äú78.1‚Äù (best).
    - MoRA: ‚Äú77.6‚Äù; Surprise Replay: ‚Äú77.2‚Äù; Reservoir Replay: ‚Äú76.9‚Äù; O-LoRA: ‚Äú75.4‚Äù; LB-CL: ‚Äú76.7‚Äù.
    - MTL: ‚Äú80.0‚Äù (upper bound).
  - LNT avg:
    - Slow Surprise Replay: ‚Äú75.1‚Äù (best in table among replay/PEFT methods noted by authors).
    - Surprise Replay: ‚Äú72.1‚Äù; Slow Reservoir Replay: ‚Äú72.5‚Äù; Reservoir Replay: ‚Äú69.1‚Äù; O-LoRA: ‚Äú68.8‚Äù; LB-CL: ‚Äú69.2‚Äù; MoRA: ‚Äú69.7‚Äù.
    - Progressive Prompts: ‚Äú78.4‚Äù (requires task identity during training and inference).
    - PerTaskFT: ‚Äú78.1‚Äù; MTL: ‚Äú76.3‚Äù.
  - Authors state Surprise Dual Learner improves by ‚Äúover 5 percentage points‚Äù compared to previous state-of-the-art in LNT.
- Hyperparameter-controlled comparison (Table 2; using Cao & Wu (2025) settings):
  - Standard CL avg: O-LoRA ‚Äú79.6‚Äù (best among listed baselines); Slow Surprise Replay ‚Äú78.1‚Äù.
  - LNT avg: Slow Surprise Replay ‚Äú77.9‚Äù (best); Surprise Replay ‚Äú77.3‚Äù.
  - All avg: Slow Surprise Replay ‚Äú78.0‚Äù (best); O-LoRA ‚Äú76.1‚Äù; N-LoRA ‚Äú75.6‚Äù.
- Ablations (Table 3‚Äì5):
  - Label vs full sequence surprise:
    - Label Surprise: ‚Äú64.9‚Äù (Standard avg), ‚Äú61.2‚Äù (LNT avg) ‚Äî poor performance.
  - Timing of computing surprise and buffer update:
    - Surprise Before Update Before (SB-UB): ‚Äú77.2‚Äù (Standard avg), ‚Äú72.1‚Äù (LNT avg).
    - Surprise Before Update After (SB-UA): ‚Äú75.5‚Äù (Standard avg), ‚Äú73.0‚Äù (LNT avg; ‚Äú73.8‚Äù and ‚Äú74.3‚Äù for two orders reported as best/second-best in table row).
    - Surprise After (SA-UA): ‚Äú76.5‚Äù (Standard avg), ‚Äú73.1‚Äù (LNT avg).
    - Surprise with updates (dynamic): ‚Äú74.7‚Äù (Standard avg), ‚Äú71.4‚Äù (LNT avg).
  - Buffer size (Table 4; LNT; means over ‚Äú3 runs √ó 3 task orders‚Äù; replay ratio ‚Äú1:4‚Äù):
    - 150 samples: Slow-RA ‚Äú72.50‚Äù, Slow-SA ‚Äú72.13‚Äù.
    - 300 samples: Slow-RA ‚Äú73.76‚Äù, Slow-SA ‚Äú74.56‚Äù.
    - 500 samples: Slow-RA ‚Äú73.89‚Äù, Slow-SA ‚Äú75.01‚Äù.
    - 1500 samples: Slow-SA ‚Äú75.99‚Äù (best); Surprise-A ‚Äú74.58‚Äù; Surprise-B ‚Äú73.66‚Äù.
  - Replay ratio (Table 5; buffer size ‚Äú300‚Äù):
    - 1:2: Slow-SA ‚Äú76.12‚Äù; Surprise-A ‚Äú74.22‚Äù.
    - 1:4: Slow-SA ‚Äú75.01‚Äù; Surprise-B ‚Äú73.23‚Äù.
    - 1:8: Slow-SA ‚Äú72.69‚Äù; Surprise-A ‚Äú70.70‚Äù.
    - 1:16: Slow-SA ‚Äú69.42‚Äù; Surprise-B ‚Äú68.38‚Äù.
    - Gains vs random increase as replay ratio decreases (e.g., at 1:16 Slow-SA +‚Äú2.48‚Äù over Slow-Random).
- Forgetting metrics (Table 9):
  - Standard CL: Slow Surprise FP ‚Äú78.10‚Äù, AP ‚Äú75.80‚Äù, Forget ‚Äú‚àí2.30‚Äù; Surprise Before: Forget ‚Äú1.00‚Äù; Replay: Forget ‚Äú3.91‚Äù.
  - LNT: Slow Surprise FP ‚Äú75.10‚Äù, AP ‚Äú70.30‚Äù, Forget ‚Äú‚àí4.80‚Äù; Replay: Forget ‚Äú3.83‚Äù; Surprise Before: Forget ‚Äú2.70‚Äù.
- Œ≤ ablation (EMA rate; Table 10; LNT; ‚Äúmeans over 3 runs √ó 3 task orders‚Äù, replay ratio ‚Äú1:2‚Äù):
  - Œ≤=0.985: Slow SB-UA ‚Äú75.36‚Äù; Œ≤=0.99: ‚Äú75.45‚Äù; Œ≤=0.995: ‚Äú75.68‚Äù; Œ≤=0.999: ‚Äú57.74‚Äù.
  - Very high Œ≤ (0.999) degrades performance markedly.
- Sum vs average surprise (Table 11; LNT):
  - Average Surprise vs Sum Surprise:
    - Average Surprise: ‚Äú74.22‚Äù (avg over orders); Sum Surprise: ‚Äú72.84‚Äù.
    - Slow Avg Surprise: ‚Äú76.12‚Äù; Slow Sum Surprise: ‚Äú75.66‚Äù.
- Extended results:
  - Llama 3.1 8B (Table 7; orders 1 and 4; ‚Äú3 runs each‚Äù):
    - Order-1: Slow Surprise ‚Äú76.00‚Äù; MTL ‚Äú76.00‚Äù; Individual FT ‚Äú75.25‚Äù; Surprise Replay ‚Äú72.25‚Äù.
    - Order-4: Surprise Replay ‚Äú68.53‚Äù; Slow Surprise ‚Äú67.00‚Äù; Individual FT ‚Äú72.56‚Äù; MTL ‚Äú69.06‚Äù.
  - Continual Pre-Training (CPT) on M2D2 domains (Table 8; perplexity, lower better):
    - Slow Surprise avg ‚Äú11.63‚Äù (best); MTL ‚Äú12.01‚Äù; Surprise Replay ‚Äú15.72‚Äù; Random Replay ‚Äú15.66‚Äù.
  - Additional hyperparameter batch-size comparisons (Table 6):
    - Batch 64: Slow Surprise Replay ‚ÄúAll avg 78.0‚Äù (best); Surprise Replay ‚Äú77.2‚Äù; O-LoRA ‚Äú76.1‚Äù.
    - Batch 8: Slow Surprise Replay ‚Äú77.7‚Äù (All avg; best marked), Surprise Replay ‚Äú76.5‚Äù.
- Visualisations: Heatmaps of accuracy trajectories across tasks for SeqFT (score ‚Äú5.60‚Äù), Reservoir Replay (score ‚Äú69.27‚Äù), Surprise Buffer (score ‚Äú72.13‚Äù), and Surprise Dual Learners (score ‚Äú74.6‚Äù) on LNT (Appendix Figures).

# Discussion
- Limitations:
  - Requires known task boundaries during training to maintain balanced buffer allocation; extending to fully online settings to prevent early-task dominance is future work.
  - Surprise computation requires extra forward passes across datasets.
- Potential extensions:
  - Online variants using distribution shift detection or adaptive buffer rebalancing; adaptation to fully online replay akin to GSS/Reservoir variants.
  - Broader evaluation across foundation models (e.g., Llama, Qwen), other modalities (vision, VLMs), and continual pre-training; preliminary CPT shows Slow Surprise best average perplexity ‚Äú11.63‚Äù (Table 8).
  - Further exploration of dual-learner designs and objectives.
- Neuroscience alignment:
  - Surprise-driven selection aligns with event boundaries and prediction error in episodic encoding; prioritised replay and EMA-style consolidation map to complementary learning systems; predicts selective protection of boundary-adjacent knowledge under tight replay budgets, and increased interference without EMA.

# Conclusion
- Replay performance in LLM CL is revisited and found stronger than often reported. SuRe, a surprise-based buffer update (sequence-level NLL), achieves state-of-the-art in LNT and best overall averages across Standard CL and LNT, with robustness under reduced buffer sizes and replay ratios. Coupling SuRe with a dual fast‚Äìslow LoRA architecture and EMA further improves performance, including gains ‚Äúup to +5 percentage points‚Äù on LNT over prior work. The selection‚Äìintegration framework supports these gains as complementary controls; results position replay as a competitive, scalable baseline for continual LLM fine-tuning.

# References
- Cites core CL and replay literature (ER, MIR, generative replay, LAMOL), PEFT (LoRA; O-LoRA; LB-CL; MoRA), prompt-based CL (Progressive Prompts), model merging and EMA, importance sampling, and neuroscience of memory and consolidation.
- Benchmarks and datasets: GLUE, SuperGLUE, multi-domain LM datasets (M2D2), text classification datasets from Zhang et al. (2015/2016).
- Foundational optimisation and averaging theory used in proofs (Polyak‚ÄìRuppert averaging; SGD stability; IPMs/MMD).

# Appendix
- Additional Figures:
  - Heatmap trajectories for SeqFT (score ‚Äú5.60‚Äù), Reservoir Replay (score ‚Äú69.27‚Äù), Surprise Buffer (score ‚Äú72.13‚Äù), Surprise Dual Learners (score ‚Äú74.6‚Äù) on LNT.
- Extended Experiments:
  - Batch size comparison (Table 6): Slow Surprise Replay best overall (‚Äú78.0‚Äù All avg, batch 64).
  - Llama 3.1 8B preliminary results (Table 7): Slow Surprise ‚Äú76.00‚Äù on Standard CL order-1; on order-4 ‚Äú67.00‚Äù.
  - Continual Pre-Training (CPT) on M2D2 (Table 8): Slow Surprise avg perplexity ‚Äú11.63‚Äù (best), MTL ‚Äú12.01‚Äù.
- Measuring Forgetting (Table 9):
  - Reports FP/AP/Forgetting for Replay variants on both benchmarks; slow variants show negative forgetting (e.g., ‚Äú‚àí4.80‚Äù on LNT).
- Ablations:
  - EMA rate Œ≤ (Table 10): performance sensitive; Œ≤=‚Äú0.995‚Äù yields strong results (e.g., Slow SB-UA ‚Äú75.68‚Äù), Œ≤=‚Äú0.999‚Äù degrades (e.g., ‚Äú57.74‚Äù).
  - Sum vs Average Surprise (Table 11): average surprise outperforms sum (e.g., Slow Avg Surprise ‚Äú76.12‚Äù vs Slow Sum Surprise ‚Äú75.66‚Äù).
  - Surprise variants and timing (Appendix F): label-level surprise (Eq. 7) underperforms; before/after timing (Eq. 8); dynamic updates during replay (Eq. 9).
- Implementation Details (Appendix G):
  - Learning rate: ‚Äú1e‚àí3‚Äù (T5-Large), ‚Äú1e‚àí4‚Äù (Llama 3.1 8B); batch size ‚Äú64‚Äù; replay frequency ‚Äú1/2‚Äù; one epoch; dropout ‚Äú0.1‚Äù; LoRA rank ‚Äú8‚Äù, Œ± ‚Äú32‚Äù; LoRA on Q,V in all attention layers.
- Proofs (Appendix H):
  - Lemma 1 (IPM selection mismatch).
  - Lemma 2 (EMA variance reduction; bias/variance decomposition).
  - Theorem 1 (Additive bound: selection via IPM mismatch + integration via variance + drift).
  - Remarks on importance sampling motivations for surprise-based selection.