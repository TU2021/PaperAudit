# Global Summary
This paper addresses catastrophic forgetting in continual learning for Large Language Models (LLMs). The authors formalize forgetting as the sum of two complementary errors: selection error (what to rehearse) and integration error (how to consolidate new knowledge). To address selection, they propose Surprise-prioritised Replay (SuRe), a method that stores and replays the most surprising samples, measured by their negative log-likelihood (NLL). To address integration, they use a dual-learner architecture with fast and slow LoRA adapters, where the slow adapter is updated via an exponential moving average (EMA) of the fast one, stabilizing knowledge consolidation. The approach is evaluated on Standard Continual Learning (CL) and Large Number of Tasks (LNT) benchmarks using a T5 model. The combination of SuRe and the dual learner achieves state-of-the-art performance, particularly in the LNT setting, with an average accuracy of 75.1%, an improvement of over 5 percentage points compared to previous methods that do not require task identity at inference. Ablation studies confirm the method's robustness to smaller buffer sizes and lower replay frequencies. A key limitation is the reliance on known task boundaries during training.

# Abstract
The paper tackles the challenge of continual learning (CL) for LLMs, where existing methods like regularization and replay underperform multi-task learning (MTL), especially with many tasks. The authors attribute this gap to two failure modes: selection (what to replay) and integration (how to consolidate knowledge). They propose Surprise-prioritised Replay (SuRe), which ranks and stores sequences with high Negative Log-Likelihood (NLL), to address selection. SuRe achieves state-of-the-art (SOTA) performance in the Large Number of Tasks (LNT) setting and the best overall average across Standard CL and LNT benchmarks. For integration, they introduce a dual-learner design with fast and slow LoRA adapters merged via an exponential moving average (EMA). Combining SuRe with the dual learner yields further gains, improving accuracy by up to +5 points on LNT over prior SOTA. Ablations show the method is effective and sample-efficient, remaining robust with reduced replay frequency and small buffer sizes. The results establish replay as a strong baseline and show that surprise-based selection and slow-weight consolidation are complementary for mitigating catastrophic forgetting.

# Introduction
The paper frames continual learning (CL) as the challenge of adapting to new data (plasticity) while retaining old knowledge (stability), a core issue being catastrophic forgetting. The Class-Incremental setting, where the model must distinguish between old and new classes without task identity at test time, is highlighted as particularly difficult, especially with a large number of tasks (LNT). The authors formalize catastrophic forgetting as the sum of two additive and complementary errors: (i) selection error from imperfect replay distributions and (ii) integration error from unstable knowledge consolidation. To address these, the paper proposes Surprise-prioritised Replay (SuRe) for the selection problem and an EMA-based dual-learner for the integration problem. The contributions are: (1) formalizing forgetting as selection and integration errors, (2) proposing SuRe for efficient sample selection, and (3) showing that combining SuRe with EMA achieves SOTA in the LNT setting and the best average performance across benchmarks.

# Related Work
This section reviews three main approaches to catastrophic forgetting: replay, regularization, and architecture-based methods, focusing on replay and parameter-efficient techniques for LLMs.
- **Replay Based Methods**: Experience Replay (ER) with reservoir sampling is described as a simple and effective baseline. The paper notes that prior work by Isele & Cosgun (2018) and Araujo et al. (2022) found surprise-based selection to perform poorly. Other methods mentioned include InfoRS (information-theory-based updates), Maximally Interfered Retrieval (replaying samples with the highest potential loss increase), and Generative Replay (e.g., LAMOL).
- **Parameter-Efficient Continual Learning**: Many NLP CL methods use PEFT techniques like LoRA. Baselines discussed include O-LoRA (orthogonal LoRA heads per task), Learn More but Bother Less (SVD-based adapters with gradient projection), Progressive Prompts (learning a new prompt per task), and model merging approaches like Hickok (2025) which uses EMA for regularization.

# Method
The method is based on a theoretical decomposition of forgetting into selection and integration errors.
- **Selection–Integration Decomposition**: The paper presents a theorem stating that expected forgetting is bounded by the sum of a selection term and an integration term: `E[F] <= A * D(P, q) + B(psi) * (sigma^2 / (mu*N)) + C * Delta_drift`. The selection term `D(P, q)` measures the mismatch between the replay buffer distribution `q` and the true past data distribution `P`. The integration term captures variance from SGD noise, which is reduced by a consolidation operator `A_psi` (e.g., EMA) with a variance-reduction factor `B(psi)`. The paper argues that these terms are complementary and require separate mechanisms to address.
- **Surprise Replay (SuRe)**: To reduce selection error, SuRe replaces uniform reservoir sampling with a surprise-based update rule. Surprise is measured as the sequence's negative log-likelihood (NLL) under the current model. The buffer stores the most surprising samples, which is hypothesized to focus learning on high-error, infrequent, or boundary examples, and to perform implicit importance sampling. The buffer size is set to 2% of the total dataset size, with an equal quota per task.
- **Dual Learners**: To reduce integration error, a dual-learner architecture is used. The base model is frozen, and two LoRA adapters (fast and slow) are attached to the query and value projection matrices. The fast adapters are trained on a mix of current task data and replayed samples. The slow adapters are not trained directly but are updated via an EMA of the fast adapters: `θ_t^{slow} ← β θ_{t-1}^{slow} + (1 − β) θ_t^{fast}`. At inference, only the slow adapters are used. This EMA mechanism acts as a low-pass filter, stabilizing the learned representations.

# Experiments
The experiments evaluate SuRe and the dual learner on a T5 model.
- **Setup**: Two benchmarks are used: Standard CL (4 datasets: AG News, DBPedia, Amazon, Yahoo Answers; 5k train/500 test samples each) and Large Number of Tasks (LNT) (Standard CL + 11 GLUE/SuperGLUE datasets; 1k train/500 test samples each).
- **Metrics**: The primary metric is Final Performance (FP), the average accuracy across all tasks after training is complete.
- **Baselines**: Methods include Multi-Task Learning (MTL), Sequential Fine-Tuning (SeqFT), EWC, Reservoir Replay, O-LoRA, Learn More but Bother Less (LB-CL), MoRA, Progressive Prompts, and AimMerging.
- **Main Results**:
    - In Table 1, Surprise Replay outperforms Reservoir Replay on both benchmarks (e.g., 72.1% vs. 69.1% on LNT avg).
    - The combined "Slow Surprise Replay" (SuRe + Dual Learner) achieves the best results among methods not requiring task ID at inference. It scores 78.1% on Standard CL (avg) and 75.1% on LNT (avg).
    - The 75.1% LNT score is noted as a >5 percentage point improvement over prior SOTA like MoRA (69.7%).
    - Progressive Prompts achieves a higher score on LNT (78.4%), but it requires task identity during both training and inference.
    - In Table 2 (using different hyperparameters), Slow Surprise Replay again achieves the best overall average accuracy (78.0%) and the best LNT average (77.9%).
- **Ablation Studies**:
    - **Surprise Signal**: Computing surprise on full sequences is superior to labels only (e.g., 77.2% vs. 64.9% avg on Standard CL).
    - **Timing**: The timing of surprise computation and buffer updates affects the plasticity-stability trade-off. "Surprise Before Update Before" was best on Standard CL (77.2%), while "Surprise After" was best on LNT (73.1%).
    - **Buffer Size**: Performance generally increases with buffer size (Table 4/10). SuRe outperforms random selection, and moderate buffer sizes (300-500 samples) provide most of the benefit. The best result was 75.99% with a 1500-sample buffer.
    - **Replay Ratio**: Performance degrades as fewer past samples are replayed (Table 5). Surprise-based methods are more robust to lower replay ratios. At a 1:16 ratio, Slow-SA outperforms its random counterpart by +2.48%.

# Discussion
- **Limitations**: The proposed method requires known task boundaries to maintain a balanced buffer, making it unsuitable for fully online settings without modification. It also requires an extra forward pass to compute surprise, adding computational overhead.
- **Future Work**: The authors suggest extending the method to online settings (e.g., using distribution shift detection), evaluating on other model families (Llama, Qwen) and modalities (vision), and exploring continual pre-training.
- **Neuroscience and Consolidation**: The paper draws parallels between its selection-integration framework and concepts in memory neuroscience. Surprise-driven selection is linked to episodic encoding and hippocampal replay, while EMA-based consolidation is compared to complementary learning systems and multi-timescale synaptic consolidation.

# Conclusion
The paper argues that the performance of replay-based methods in LLM continual learning has been underestimated. It introduces SuRe, a surprise-based buffer update rule that achieves SOTA on the LNT benchmark and the best overall average performance. The paper's selection-integration framework is supported by the complementary benefits of SuRe (for selection) and a dual-learner architecture with EMA (for integration). The combined approach improves LNT performance by up to +5 percentage points over prior work, establishing replay as a strong and scalable baseline for continual LLM fine-tuning.

# References
This section contains a list of all academic papers and works cited throughout the manuscript.

# Appendix
The appendix provides additional experimental details, figures, and theoretical proofs.
- **Additional Figures**: Heatmaps visualize the performance matrix for different methods on the LNT benchmark. Final scores shown are: SeqFT (5.60), Reservoir Buffer (69.27), Surprise Buffer (72.13), and Surprise Dual Learners (74.6).
- **Extended Experiments**:
    - **Llama 3.1 8B**: Preliminary results show Slow Surprise also performs strongly on Llama 3.1 8B, achieving 76.00% on the Standard CL benchmark (Order-1), matching MTL.
    - **Continual Pre-Training (CPT)**: On the M2D2 dataset, Slow Surprise achieves the best average perplexity (11.63), outperforming MTL (12.01).
- **Measuring Forgetting**: Table 9 shows that Slow Surprise methods achieve negative forgetting (-2.30 on Standard CL, -4.80 on LNT), indicating they improve performance on previously learned tasks.
- **Ablation on β**: Table 10 shows that the EMA rate `β` is critical. A value of 0.995 performs well, while a value of 0.999 (too slow integration) leads to a significant performance drop.
- **Sum vs. Average Surprise**: Table 11 shows that using the average surprise per sequence is more effective than the sum of surprise for selection (e.g., 76.12% vs. 75.66% for the slow variants on LNT).
- **Surprise Variants**: Provides formal definitions for the different surprise computation methods and timings ablated in the main experiments.
- **Implementation Details**: Key hyperparameters are listed: learning rate 1e-3 (T5) / 1e-4 (Llama 3.1 8B), batch size 64, replay ratio 1:2, LoRA rank 8, and α=32 on Q and V projections.
- **Proofs and Technical Details**: This section contains the mathematical derivations and proofs for Lemma 1, Lemma 2, and Theorem 1, which formalize the selection-integration decomposition of forgetting. It states the underlying assumptions, such as local smoothness and PL-conditions in the LoRA parameter subspace.