1) Summary
This paper addresses catastrophic forgetting in continual learning for large language models (LLMs). The authors propose a framework that decomposes forgetting into two components: selection error and integration error. To address selection error, they introduce Surprise-prioritised Replay (SuRe), a method that stores and rehearses samples with the highest negative log-likelihood. To address integration error, they employ a dual-learner architecture with fast and slow LoRA adapters, where the slow adapter consolidates knowledge from the fast one via an exponential moving average (EMA). Experiments on standard and large-scale continual learning benchmarks show that SuRe alone outperforms reservoir replay, and the combined SuRe with the dual-learner architecture achieves state-of-the-art performance, particularly in the challenging Large Number of Tasks (LNT) setting.2) Strengths
*   **Strong Empirical Performance and State-of-the-Art Results**
    *   The proposed method, Slow Surprise Replay, achieves the highest average performance across all benchmarks and task orders (Table 1, "All avg" in Table 2).
    *   It demonstrates particularly strong performance in the more challenging Large Number of Tasks (LNT) setting, outperforming the previous state-of-the-art by a significant margin (e.g., 75.1% vs. 69.7% for MoRA on LNT avg in Table 1).
    *   The gains are shown to be additive; both Surprise Replay alone and the Slow Learner (Slow Reservoir Replay) individually outperform the baseline Reservoir Replay, and their combination yields the best results (Table 1). This provides strong empirical support for the paper's core hypothesis.*   **Thorough and Insightful Ablation Studies**
    *   The paper systematically investigates key design choices, providing a clear understanding of what drives the method's success. This includes comparing different surprise signals (full sequence vs. label-only, Table 3), the timing of surprise computation and buffer updates (Table 3), and the effect of dynamic surprise updates (Table 3).
    *   The robustness of the method is demonstrated under varying resource constraints, showing that SuRe variants consistently outperform random baselines across different buffer sizes (Table 4) and replay ratios (Table 5).
    *   The performance gap between surprise-based and random replay widens as the replay ratio decreases (Table 5), highlighting the sample efficiency of the proposed selection mechanism.*   **Novel Conceptual Framing with Theoretical Motivation**
    *   The decomposition of catastrophic forgetting into "selection error" and "integration error" (Section 3.1) provides an intuitive and powerful conceptual lens for analyzing the problem.
    *   This framing is supported by a formal analysis that bounds forgetting with an additive expression containing a selection term (related to the replay buffer) and an integration term (related to update consolidation) (Theorem 1, Equation 3).
    *   This theory motivates the use of two complementary mechanisms—a better replay strategy to reduce selection error (Lemma 1) and a variance-reducing consolidation method to reduce integration error (Lemma 2)—which is then validated empirically.*   **Simplicity and Generality of the Proposed Method**
    *   The core components of the method are conceptually simple and widely applicable. Surprise is measured using negative log-likelihood (Equation 4), a standard metric available in any probabilistic model. The dual-learner uses a standard EMA update (Equation 5).
    *   The approach is architecture-agnostic and can be combined with any parameter-efficient fine-tuning method (Section 3.2).
    *   Preliminary experiments in the appendix show the method's promise on a decoder-only model (Llama 3.1 8B, Table 7) and in a continual pre-training setting (Table 8), suggesting its potential for broad impact.3) Weaknesses
*   **Reliance on Known Task Boundaries**
    *   The buffer management strategy relies on knowing when tasks begin and end to allocate an equal per-task memory quota (Section 3.2: "we allocate an equal per-task quota").
    *   This assumption is common in the literature but limits the method's applicability in fully online or task-agnostic settings where such boundaries are not available.
    *   The authors acknowledge this as a primary limitation and an area for future work (Section 5).*   **Computational Overhead of Surprise Calculation**
    *   Calculating surprise requires a full forward pass over all samples in a new task's dataset to rank them by NLL before training on that task can begin (Algorithm 1, line 4; Section 5).
    *   This introduces a non-trivial computational cost that is not quantified in the paper. While feasible for the dataset sizes used in the experiments (1k-5k samples per task, Section 4.1), this overhead could become a bottleneck in scenarios with much larger datasets or in a truly online stream.*   **Clarity of the Theoretical Contribution**
    *   The theoretical exposition in Section 3.1 is dense and assumes familiarity with concepts like Integral Probability Metrics (IPMs) and the Polyak-Lojasiewicz (PL) condition, which may make it inaccessible to a broader audience.
    *   The connection between the proposed heuristic (selecting high-NLL samples) and the theoretical goal (minimizing the IPM distance `D_F_loc(P, q)`) is stated as a motivation rather than a formal result (Section 3.2, Section H.3: "On surprise-based selection"). This leaves a gap between the theory and the practical algorithm.*   **Limited Evaluation on Diverse Model Architectures in Main Paper**
    *   The main experimental results are exclusively based on T5-Large, an encoder-decoder model (Section 4).
    *   While promising preliminary results on Llama 3.1 8B (a decoder-only model) are included in the appendix (Table 7), this evaluation is limited (only two task orders) and not as comprehensive as the T5 experiments. A more thorough evaluation on a decoder-only model in the main paper would strengthen the claims of architectural generality.4) Suggestions for Improvement
*   **Investigate Task-Agnostic Buffer Management**
    *   To address the reliance on task boundaries, the authors could discuss or experiment with adaptations for a task-agnostic setting. For instance, they could replace the per-task quota system with a single global reservoir buffer and evaluate whether SuRe's prioritization of high-NLL samples is sufficient to maintain a diverse and effective memory without explicit balancing. This would significantly increase the practical relevance of the method.*   **Quantify and Discuss Computational Costs**
    *   The authors should provide an analysis of the computational overhead introduced by the surprise calculation step. This could be in the form of wall-clock time comparisons or FLOPs analysis. Additionally, discussing potential mitigation strategies, such as computing surprise on a random subset of the data or using a less expensive proxy for surprise, would be valuable for practitioners considering the method for large-scale applications.*   **Enhance the Accessibility of the Theoretical Section**
    *   To improve clarity, the authors could add a few sentences in Section 3.1 to provide high-level intuitions for concepts like IPMs (e.g., "a way to measure the distance between probability distributions based on how differently they affect a class of functions") and the PL condition. Clarifying in the main text that using NLL to reduce the IPM distance is a well-motivated heuristic, as noted in the appendix (Section H.3), would also help manage reader expectations regarding the theoretical guarantees.*   **Expand Evaluation on Decoder-Only Models**
    *   To strengthen the paper's claims of generality, the authors should consider elevating the Llama 3.1 8B experiments from the appendix (Table 7) to the main paper. Expanding this evaluation to include more task orders and a direct comparison with key baselines would provide more conclusive evidence that the benefits of SuRe and the dual-learner architecture transfer effectively to popular decoder-only LLMs.5) Score
*   Overall (10): 8 — The paper presents a well-motivated and simple method with strong empirical results, thorough ablations, and a novel conceptual framing.
*   Novelty (10): 7 — While the components (surprise replay, EMA) are not new, their combination, theoretical framing for LLM CL, and strong empirical validation are novel.
*   Technical Quality (10): 9 — The experimental methodology is rigorous, with extensive ablations and fair comparisons that strongly support the paper's claims.
*   Clarity (10): 8 — The paper is generally well-written and easy to follow, though the theoretical section (Section 3.1) is quite dense.
*   Confidence (5): 5 — I have carefully reviewed the manuscript, including the appendix, and am confident in my assessment.