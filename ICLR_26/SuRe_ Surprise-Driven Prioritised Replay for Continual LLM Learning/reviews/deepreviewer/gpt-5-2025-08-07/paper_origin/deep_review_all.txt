Summary
The paper proposes SuRe, a surprise-prioritised replay strategy for continual fine-tuning of LLMs that selects and stores sequences with high average NLL (Eq. 4). It pairs SuRe with a dual-learner scheme using fast and slow LoRA adapters consolidated via exponential moving average (EMA) (Eq. 5). The authors formalise catastrophic forgetting as the sum of selection error (distribution mismatch of the replay buffer, bounded via an IPM; Lemma 1, Eq. 1) and integration error (variance from stochastic updates, mitigated by EMA; Lemma 2, Eq. 2; Theorem 1, Eq. 3). Empirically, on T5-Large across Standard CL and Large Number of Tasks (LNT) settings, SuRe and Slow SuRe outperform strong baselines (Tables 1–2) and remain robust under reduced buffers and replay ratios (Tables 4–5). Ablations examine surprise computation timing, label vs sequence-level surprise, buffer sizes, replay ratios, and EMA β (Tables 3–5, 10–11). Preliminary results on Llama 3.1 8B and a CPT scenario further suggest generality (Tables 7–8).

Soundness
- Theory: The IPM-based selection mismatch bound (Lemma 1, Eq. 1) is standard and sound, but the claim that SuRe reduces D_{F_loc}(P,q) is only motivated (importance sampling) and not proven (Appendix §H.62 explicitly notes lack of a general proof). The dual-learner variance claim is conceptually grounded in averaging literature, yet there is a notable inconsistency: the “Remark” under Lemma 2 states variance scales by (1−β) (variance reduction), while Eq. (2) and §3.3 state B(β)=1/(1−β), which increases with β—these are contradictory and need reconciliation. The additive bound (Theorem 1, Eq. 3) correctly expresses complementary terms, but constants and locality assumptions (PL/smoothness in LoRA subspace; Appendix §H) limit generality.
- Methodology: Surprise selection via mean per-token NLL (Eq. 4) is reasonable and architecture-agnostic; the dual-learner EMA (Eq. 5–6) is standard. Buffer quotas equal per task (Section 3.2) can mitigate early-task dominance but may bias class distributions when datasets differ in size/class skew. Experimental controls (multi-order averages, multiple runs, replay frequency/ratio variations) are adequate, though statistical tests are absent.

Presentation
- Clarity: The selection–integration framing is clear (Section 3.1; Eq. 1–3). The method’s narrative (Sections 3.2–3.3; Figure 1) is accessible. However, Algorithm 1 is truncated (Section 3.3: lines 3–6 only) and lacks full steps for replay and EMA updates; this harms reproducibility. Results are well-organised (Tables 1–6), and extended visuals aid interpretation (Appendix Figures 3–5).
- Claims: The Abstract claims “SOTA in LNT and best overall average across both benchmarks.” In Table 1, Progressive Prompts achieve higher LNT avg (78.4) than Slow Surprise Replay (75.1), albeit requiring task identity at inference; the SOTA claim should be qualified to “among methods without task-identity at inference.”

Contribution
- Conceptual: A useful formal decomposition of forgetting into selection and integration terms that motivates pairing replay and consolidation (Theorem 1, Eq. 3). While not fundamentally new theoretically, the articulation for LLM continual learning is timely.
- Practical: A simple, effective, and architecture-agnostic replay rule (SuRe) plus dual LoRA EMA that improves over widely used baselines, especially in LNT (Tables 1–2, 4–5). The identification of timing effects (Table 3) and robustness under small buffers and replay budgets is a valuable practical insight. Neuroscience cross-references (Section 5–6) contextualise the design but remain descriptive.

Strengths
- Strong empirical performance and breadth of evaluation: multiple task orders, buffer sizes, replay ratios, and baselines (Tables 1–6; Appendix Tables 10–11).
- Simple, compute-light modifications (surprise scoring; dual EMA) with clear ablations demonstrating complementary gains (Theorem 1; Tables 3–5, 9–11).
- Good reproducibility details (Appendix §G hyperparameters, replay frequency; multiple seeds/orders).
- Careful discussion of assumptions and limitations (Section 5), including known task boundaries and overhead of surprise computation.

Weaknesses
- Theoretical inconsistency: variance scaling with β is contradictory (Lemma 2 “Remark” vs Eq. 2 and §3.3’s B(β)=1/(1−β)); this undermines the integration-term interpretation and should be corrected.
- SOTA claim needs qualification vis-à-vis methods requiring task identity (Table 1: Progressive Prompts > Slow Surprise in LNT).
- Algorithm 1 is incomplete; the pseudo-code does not fully specify replay scheduling, buffer maintenance, and EMA updates.
- Surprise computation overhead is acknowledged but not quantified; scaling to large datasets or longer sequences may be nontrivial (Section 5).
- Reliance on known task boundaries at training time (Section 5) limits applicability to fully online scenarios; though common, it should be explicitly reflected in headline claims.
- No statistical significance testing or confidence intervals on reported improvements; some margins are modest and could be within variance.

Questions
1. Can the authors reconcile the variance-scaling inconsistency for EMA (Lemma 2 remark vs Eq. 2 and §3.3)? Which expression is correct, and how does β influence the integration term in practice?
2. How much extra compute (time/TPU/GPU hours) does surprise scoring (Eq. 4) add per dataset/task? Is there a batched or cached implementation to amortise cost?
3. In Table 1, could the SOTA claim be explicitly constrained to methods not requiring task identity at inference? If so, please add that qualifier to the Abstract.
4. Algorithm 1 is truncated—can the authors provide full pseudocode covering buffer updates (Before/After/Online), top-k eviction/replacement, replay ratio scheduling, and EMA updates?
5. How sensitive are results to using LoRA on only W_Q and W_V? Have you tried including W_K/W_O or FFN projections, and does SuRe’s benefit persist?
6. In uneven class distributions (e.g., AG News vs Amazon/Yelp), does equal per-task buffer quota lead to class imbalance upon replay? Could adaptive per-class quotas further improve D_{F_loc}(P,q)?
7. Could the importance-sampling perspective be empirically validated further with gradient-norm diagnostics to show SuRe better approximates E_P[∇ℓ] (Section 3.2 rationale)?

Rating
- Overall (10): 7 — Strong empirical gains and a useful selection–integration framing, but theoretical inconsistency around EMA variance (Section 3.1 “Remark”; Eq. 2; §3.3) and overbroad SOTA phrasing (Table 1) weaken the case.
- Novelty (10): 7 — Surprise-based replay is known in RL, and EMA dual learners exist; the combination and CL-LLM-specific framing (Theorem 1; Sections 3.2–3.3) are a practical, timely synthesis.
- Technical Quality (10): 6 — Solid experiments and ablations (Tables 3–5, 10–11), but the EMA variance contradiction and lack of proof that SuRe tightens the IPM term (Appendix §H.62) reduce rigor.
- Clarity (10): 7 — Clear motivation and results, but Algorithm 1 is incomplete; claims need qualification (Abstract vs Table 1); some notation and constants left abstract (Appendix §H).
- Confidence (5): 4 — Based on detailed reading of theory and experiments (Sections 3–4; Tables 1–6; Appendix §H), with moderate uncertainty due to the variance-scaling inconsistency.


Summary
This paper introduces SuRe, a surprise-prioritised replay policy (sequence-level average NLL; Eq. 4) and a dual fast–slow LoRA adapter setup with EMA consolidation (Eq. 5–6) to mitigate catastrophic forgetting in continual LLM learning. It formalises forgetting as an additive combination of selection mismatch (IPM gap; Lemma 1, Eq. 1) and integration variance (Lemma 2, Eq. 2), culminating in Theorem 1 (Eq. 3) that motivates complementary controls via replay and consolidation. Extensive experiments on T5-Large (Standard CL and LNT) show SuRe and Slow SuRe surpass baselines (Tables 1–2), with robustness under buffer/replay reductions and multiple ablations (Tables 3–5, 10–11). Preliminary results on Llama 3.1 8B and continual pre-training suggest generality (Tables 7–8).

Soundness
- Empirical: The evaluation protocol tracks final performance across multiple orders, uses means over 3 runs, and includes strong baselines (Tables 1–2, 6). Ablations systematically vary buffer size and replay ratio (Tables 4–5) and confirm the hypothesised additive benefits (Theorem 1). Forgetting analyses (Table 9) show negative forgetting with Slow variants, supporting stability claims.
- Theory: The IPM bound (Eq. 1) is correct, but the core claim that SuRe reduces D_{F_loc}(P,q) is argued heuristically; more formal grounding is deferred (Appendix §H.62). The EMA-based variance reduction is standard, yet the text contains conflicting dependence on β (variance scales with (1−β) vs 1/(1−β)), which needs correction. Local PL/smoothness assumptions in the LoRA subspace (Appendix §H) are reasonable but restrict generality.

Presentation
- The paper is well-structured (Sections 1–6) with clear figures/tables (Figure 1; Tables 1–6). The surprise and EMA components are explained with equations and intuitions (Eq. 4–6).
- Minor issues: Algorithm 1 is incomplete (Section 3.3), impeding reproducibility; some dataset/task names in Appendix figures extend beyond the main benchmark description (Appendix Figures 3–5 list “rc/squad/drop”), which should be cross-referenced to Extended Experiments (§B).

Contribution
- Advances the replay baseline for LLM CL by selective surprise-based retention and a slow-weight consolidation mechanism. The selection–integration decomposition offers a useful lens to organise design choices.
- Empirical significance is high in LNT (Tables 1–2, 4–5), where many methods struggle; SuRe narrows the gap to MTL and outperforms regularisation-heavy baselines.

Strengths
- Strong empirical evidence across settings and hyperparameters (Tables 1–6; Appendix Tables 10–11).
- Simple, architecture-agnostic approach amenable to broad adoption (Section 3.2–3.3; minimal code changes around LoRA).
- Thoughtful discussion of limitations and mapping to neuroscience concepts (Sections 5–6).
- Robust to smaller replay budgets (Table 5) and buffer sizes (Table 4), useful for practical deployments.

Weaknesses
- The EMA variance term inconsistency (Lemma 2 remark vs Eq. 2/§3.3) is a technical flaw that should be fixed for theoretical coherence.
- SOTA claim in Abstract is overbroad given Progressive Prompts (Table 1); qualifiers about task-identity assumptions are needed.
- Surprise computation overhead is not quantified; the cost might be substantial for long sequences/large datasets.
- Algorithm 1 lacks full procedural detail (e.g., eviction policy, replay scheduling), and no statistical significance tests are reported.

Questions
1. Please clarify whether the variance term scales with (1−β) or 1/(1−β), and update Eq. (2) and §3.3 accordingly.
2. Could you provide wall-clock/compute overhead for surprise scoring on T5-Large in Standard CL and LNT?
3. How does SuRe perform when task boundaries are unknown (fully online)? Can SA-UA (After/After) be adapted with drift detection?
4. Does computing surprise under θ_pre vs θ_fast change performance (Appendix §F.2 suggests timing matters); could you add results for θ_slow-based scoring?
5. Would adding LoRA to FFN layers alter the selection or integration dynamics? Any ablation?
6. Can you provide confidence intervals or statistical tests for Table 1–2 results across runs?

Rating
- Overall (10): 8 — A strong, practical contribution with extensive evidence (Tables 1–6), marred by a fixable theoretical inconsistency around EMA variance and an overbroad SOTA claim (Abstract).
- Novelty (10): 7 — Replay prioritisation and EMA are known, but the specific LLM-CL coupling and selection–integration framing (Theorem 1; Sections 3.1–3.3) are valuable and timely.
- Technical Quality (10): 7 — Good ablations and breadth; theory is mostly motivational with one contradiction (Lemma 2/§3.3); reproducibility could be improved (Algorithm 1).
- Clarity (10): 8 — Clear exposition, equations, and tables; needs minor corrections (Algorithm 1 completeness; Abstract qualifiers).
- Confidence (5): 4 — High confidence based on thorough reading and cross-checking tables/equations; moderate uncertainty due to the β-scaling inconsistency.


Summary
The paper revisits replay for continual LLM fine-tuning, introducing Surprise Replay (SuRe) that retains high-NLL sequences (Eq. 4) and a dual-learner approach that consolidates fast LoRA updates into slow LoRA via EMA (Eq. 5–6). A selection–integration decomposition (Section 3.1; Theorem 1, Eq. 3) motivates complementary controls: selection via buffer policy (Lemma 1, Eq. 1) and integration via consolidation (Lemma 2, Eq. 2). Experiments on T5-Large across Standard CL and LNT show SuRe and Slow SuRe outperform reservoir replay and several PEFT baselines (Tables 1–2), with ablations demonstrating robustness to buffer size and replay ratio (Tables 4–5) and sensitivity to update timing (Table 3). Forgetting analyses indicate improved stability for slow variants (Table 9).

Soundness
- The empirical design is fair and thorough: per-task quotas (Section 3.2) avoid early-task dominance; multiple orders/runs; buffer size and replay ratio sweeps; reference baselines with cited results (Tables 1–2; footnotes). However, mixing baselines that require task identity at inference (e.g., Progressive Prompts) with methods that do not can complicate headline claims; the paper notes PP’s requirement but should condition SOTA claims more explicitly.
- The theoretical components are sensible but modest: Lemma 1 is a straightforward IPM bound; Lemma 2’s EMA variance reduction intuition is correct in spirit but contains a contradictory dependence on β across sections. The lack of a formal result that surprise selection minimises the selection mismatch is acknowledged (Appendix §H.62).

Presentation
- Clear structure and strong visuals (Figure 1; Tables 1–6; Appendix Figures). The ablation tables are informative and easy to parse (Tables 3–5, 10–11).
- Weaknesses: The pseudocode (Algorithm 1) is incomplete; it should include buffer eviction, replay scheduling, and EMA application per step. Claims in Abstract should reflect task-identity assumptions (Table 1).

Contribution
- Practically significant: elevates replay with a simple selection heuristic and demonstrates complementary gains from EMA consolidation in LLM CL, especially in LNT where catastrophic forgetting is severe (Tables 1–2).
- Conceptually useful framing: selection vs integration errors provides a design map for future work (Section 3.1; Theorem 1).

Strengths
- Robust across hyperparameters and budgets (Tables 4–5).
- Comprehensive ablations including surprise timing and label vs sequence surprise (Table 3), and EMA β (Table 10).
- Negative forgetting for slow variants (Table 9), indicating improved stability.
- Generality indications via Llama 3.1 and CPT (Tables 7–8).

Weaknesses
- The β-scaling inconsistency in the integration term (Lemma 2 remark vs Eq. 2 and §3.3) needs correction.
- Surprise scoring overhead unquantified; reader cannot assess practical cost-benefit.
- Algorithm 1 incomplete; impedes exact reproduction.
- SOTA claim overstates relative to Progressive Prompts in LNT (Table 1) unless qualified by inference-time task identity.

Questions
1. Please reconcile and correct the dependence of the variance term on β (is it (1−β) or 1/(1−β)?).
2. What is the measured cost of computing sequence surprise (Eq. 4) per dataset, and does it scale linearly with sequence length?
3. How would SuRe behave without known task boundaries? Could adaptive buffer rebalancing mitigate early-task dominance?
4. Did you evaluate surprise scoring under θ_slow vs θ_fast, and does one correlate better with future forgetting?
5. Can the authors provide full pseudocode for Algorithm 1 including eviction, quotas, and replay-ratio scheduling?
6. Are results sensitive to the LoRA placement (Q,V only) and rank (Appendix §G: rank=8, α=32)? Any ablation?

Rating
- Overall (10): 7 — Effective and practical method with strong empirical evidence (Tables 1–5), but a theoretical inconsistency and incomplete pseudocode reduce confidence.
- Novelty (10): 6 — Combines known ideas (prioritised replay; EMA dual-learner) in an LLM CL context; useful synthesis more than a fundamentally new technique.
- Technical Quality (10): 7 — Solid experiments and ablations; theory provides a helpful lens but lacks a key proof and contains a contradiction.
- Clarity (10): 7 — Clear exposition and tables; needs corrections to Algorithm 1 and Abstract claims relative to Table 1.
- Confidence (5): 4 — High confidence in empirical findings based on multiple tables and runs; moderate uncertainty due to β-scaling inconsistency.


Summary
The authors propose Surprise-prioritised Replay (SuRe) for continual LLM learning, selecting sequences with high average NLL (Eq. 4) and combining this with a dual fast/slow LoRA adapter scheme merged via EMA (Eq. 5–6). They frame catastrophic forgetting as additive selection and integration errors (Section 3.1; Theorem 1, Eq. 3). Experiments on T5-Large across Standard CL and LNT benchmarks show improvements over reservoir replay, EWC, O-LoRA, LB-CL, and MoRA, with Slow Surprise Replay achieving the best averages among non-task-identity baselines (Tables 1–2). Ablations highlight when to compute surprise and update the buffer (Table 3), buffer size effects (Table 4), replay ratios (Table 5), and β (Table 10). Appendix analyses report negative forgetting for slow variants (Table 9) and preliminary transfer to Llama 3.1 and CPT (Tables 7–8).

Soundness
- The empirical methodology is carefully controlled: fixed buffer quotas per task (Section 3.2), multiple orders and runs, and ablations. However, the reservoir baseline is sometimes evaluated with online updates vs boundary-aware variants (Section 4.3: “Random Buffer Update After”), making cross-method fairness dependent on assumptions; the paper calls this out and attempts to control it (Section 1; Section 4.3).
- The formal decomposition (Eq. 1–3) is plausible but largely descriptive. Lemma 1 is a direct IPM inequality; Lemma 2 and Theorem 1 borrow from averaging/stability theory (Appendix §H), but the β dependence is inconsistent across text, which needs clarification. The claim that surprise reduces D_{F_loc}(P,q) is empirically supported (Tables 3–5, 11) but not proved.

Presentation
- Strengths: Clear figures, structured tables, and detailed appendices. The ablation suite is informative and connects well to theory (e.g., timing trade-offs relating to plasticity–stability; Table 3; Section 1).
- Issues: Algorithm 1 is incomplete; Abstract’s SOTA claim should specify “among methods without task-identity at inference” (Table 1 indicates Progressive Prompts has higher LNT avg but requires task identity).

Contribution
- Practical: Establishes replay (with a principled selection heuristic) as a strong baseline for LLM CL and demonstrates complementary gains with slow-weight consolidation. Results in the LNT setting are particularly compelling (Tables 1–2, 4–5).
- Conceptual: Provides a clear lens (selection vs integration) for analysing CL methods and motivates targeted mechanisms.

Strengths
- Robust gains across buffer sizes and replay ratios (Tables 4–5), suggesting sample efficiency under constrained budgets.
- Negative forgetting for slow variants (Table 9), aligning with the integration-error mitigation hypothesis.
- Generality hints via Llama 3.1 and CPT settings (Tables 7–8).
- Thorough related work and limitations discussion (Sections 2 and 5).

Weaknesses
- EMA variance contradiction (Lemma 2 remark vs Eq. 2; §3.3) undermines the theoretical narrative.
- Surprise computation overhead and memory footprint for surprise scoring (extra forward pass across datasets) are not quantified.
- Algorithm 1 lacks full procedural detail, reducing reproducibility.
- Claims in Abstract need qualification regarding task identity at inference (Table 1).

Questions
1. Please correct and unify the dependence on β for the integration term; if B(β)=1/(1−β), how do you reconcile the “variance reduction” claim?
2. What are the compute and memory overheads for surprise scoring (Eq. 4) and buffer maintenance across the LNT datasets?
3. Could adaptive per-class quotas inside tasks reduce class-imbalance effects noted for reservoir replay (Section 4.3), and would SuRe benefit?
4. Have you assessed whether high-NLL sequences are outliers/noisy labels? Is there a safeguard (like InfoRS’s learnability filter) to avoid storing harmful samples?
5. Can you provide full pseudocode and release code for Algorithm 1, including eviction and replay scheduling?
6. Are the gains sensitive to LoRA hyperparameters (rank, α) or adapter placements beyond Q,V (Appendix §G)?

Rating
- Overall (10): 7 — Convincing empirical advances and a useful framework, tempered by theoretical inconsistency and incomplete algorithmic details.
- Novelty (10): 6 — Builds on existing ideas (prioritised replay; EMA) but offers a neat synthesis and strong LLM CL evidence (Tables 1–2).
- Technical Quality (10): 7 — Good experimental rigor and ablations; theoretical support is partial and has a contradiction to fix.
- Clarity (10): 7 — Generally clear; Abstract claims and Algorithm 1 need updates; appendices are helpful.
- Confidence (5): 4 — Based on close reading of main text and appendices (Sections 3–4; Tables 1–6; Appendix §H), with moderate uncertainty due to the β-scaling issue.