Summary
The paper proposes SuRe, a surprise-prioritized replay strategy for continual fine-tuning of large language models that selects and stores sequences with high average negative log-likelihood. SuRe is paired with a dual-learner setup using fast and slow LoRA adapters, where the slow adapter is updated via an exponential moving average of the fast adapter to stabilize integration of new knowledge. The authors present a decomposition of catastrophic forgetting into selection error (distribution mismatch between the replay buffer and the true past-task distribution, bounded via an IPM) and integration error (variance from stochastic updates), arguing that surprise-based selection reduces the former and EMA consolidation reduces the latter. Experiments on T5-Large in both Standard CL and Large Number of Tasks (LNT) settings show that SuRe and its slow variant outperform strong baselines across multiple task orders and runs, and remain robust under reduced buffer sizes and replay ratios. Ablations study when to compute surprise and update the buffer, sequence- vs label-level surprise, buffer and replay budgets, and the EMA coefficient. Additional preliminary results on Llama 3.1 8B and a continual pretraining scenario suggest generality. The paper is generally clear and thorough, but has an inconsistency in the theoretical treatment of EMA variance reduction, incomplete pseudocode for the algorithm, overbroad headline claims relative to methods that require task identity at inference, and unquantified overhead for computing surprise.

Strengths
- Clear conceptual framing: The selection–integration decomposition of forgetting provides a useful lens to motivate combining selective replay with consolidation and organizes design choices throughout the method and ablations.
- Simple, architecture-agnostic method: Surprise scoring via mean per-token NLL and a dual fast–slow LoRA EMA are straightforward to implement and compatible with standard LLM fine-tuning pipelines.
- Strong empirical performance: Across Standard CL and LNT benchmarks on T5-Large, SuRe and Slow SuRe outperform reservoir replay and several PEFT and regularization baselines, narrowing the gap to multitask training where reported. Gains persist across multiple task orders and seeds.
- Robustness under constrained budgets: Performance degrades gracefully with smaller buffers and lower replay ratios; ablations are systematic and corroborate the complementary benefits of selection and integration controls.
- Informative ablations: The paper analyzes when to compute surprise (before/after/online), sequence- vs label-level surprise, EMA β, and replay/buffer budget sensitivities; forgetting metrics indicate improved stability (including negative forgetting for slow variants).
- Generality indications: Preliminary transfer to Llama 3.1 8B and a continual pretraining scenario suggests applicability beyond the main T5-Large setup.
- Reproducibility practices: Multiple task orders, several runs, and detailed hyperparameters are provided; assumptions and limitations are discussed candidly.

Weaknesses
- Theoretical inconsistency in EMA variance: The dependence of the integration variance term on the EMA factor β is contradictory across sections (variance scaling with (1−β) versus 1/(1−β)). This undermines the clarity of the theoretical motivation for the dual-learner consolidation and needs correction and reconciliation.
- Limited theoretical guarantees for selection: The IPM-based bound is standard and sound, but the central claim that surprise-based selection reduces the selection mismatch is argued heuristically (e.g., via an importance sampling perspective) rather than proven; locality assumptions (PL/smoothness in the LoRA subspace) and abstract constants further limit generality.
- Incomplete algorithmic specification: The provided pseudocode omits key steps such as buffer maintenance (eviction and replacement), replay scheduling and ratios, and EMA update details, impeding full reproducibility.
- Overstated headline claim: The abstract’s SOTA statement is not qualified with respect to methods that require task identity at inference; Progressive Prompts achieve higher LNT averages but are not directly comparable due to stronger assumptions. Claims should be conditioned on the inference-time task-identity requirement.
- Unquantified overheads: The compute and memory costs of computing sequence-level surprise and maintaining the prioritized buffer are not measured or reported. This is important for assessing scalability to larger datasets, longer sequences, and larger models.
- Assumption of known task boundaries: The approach and several buffer-update policies rely on known task boundaries during training. This limits applicability to fully online or task-agnostic scenarios and should be reflected more prominently in claims and positioning.
- Statistical reporting: Results are averaged over multiple runs/orders, but confidence intervals or statistical significance tests are absent. Some improvements are modest and could fall within variance without further statistical support.
- Evaluation comparability and potential biases: Mixing baselines that require task identity with those that do not complicates headline comparisons. Fixed per-task buffer quotas mitigate early-task dominance but may introduce class-imbalance biases when task datasets differ in size or skew; the impact of such imbalance and potential remedies (e.g., per-class quotas) are not explored.
- Sensitivity and safeguards not fully studied: The method’s sensitivity to LoRA placement and hyperparameters (e.g., including K/O or FFN layers, rank/α variations) is not reported. High-NLL “surprising” samples could include outliers or noisy labels; mechanisms to filter harmful samples are not discussed.
