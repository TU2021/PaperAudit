Summary
The paper investigates post-training “superficiality” by analyzing effective rank (eRank) of linear weights in paired Base/Instruct LLMs (Llama, Qwen3). It finds near-identical eRanks across pairs (Figure 1), arguing post-training preserves representational dimensionality. Building on this, the authors propose Timber, a training-free refinement that computes the weight delta WΔ = WI − WB per layer, performs SVD on WΔ, sets a threshold K = ⌈eRank(WΔ)⌉, and either zeroes tail singular values (Timber-L) or attenuates them by λ ∈ (0,1) (Timber). Extensive experiments show consistent but modest gains over Instruct across multiple benchmarks (Table 2), with larger improvements in Pass@k (Figure 5), supporting the claim that Timber improves exploration while preserving exploitation.

Soundness
- The overall approach—using eRank of deltas to gate SVD refinement—is reasonable and anchored in the entropy-based measure of singular-value flatness (Equation 1). SVD-based modifications to weight deltas are mathematically coherent (Equations 2–7).
- However, there are technical inconsistencies:
  - The paper states singular values are in “non-decreasing order” (Equation 3) but then “Top-K preserve” the first K values in Equation 6, which would keep the smallest singular values, contrary to typical practice of preserving the largest. This ordering ambiguity materially affects Timber’s intended effect.
  - Figure 1 and the accompanying tables list eRank values for q_proj and up/down_proj as multi-million numbers (Blocks 6, 8–10), which is impossible because eRank ≤ min(m,n) (Section 2.1; Equation 1). This casts doubt on measurement or labeling accuracy and undermines the weight-level superficiality claim unless corrected.
- The inference that similar eRank implies preserved singular subspaces (Block 11) is suggestive but not proven: identical eRank does not guarantee subspace alignment; additional analyses (e.g., principal angle distributions or subspace overlap metrics) would be needed.
- The claim that Timber enhances exploration is supported empirically by Pass@k curves (Figure 5) and average benchmark gains (Table 2), though a causal mechanism is not established beyond the heuristic that attenuating tail components broadens sampling space. The Truncated SVD baselines (Table 3) help situate Timber but do not exhaust alternatives (e.g., layer-wise adaptive μ for model merge, spectrum-shaping of WI directly).
- Hyperparameter search for λ on AIME’24 (Block 19) is modest, but using a single task to select λ may introduce selection bias; robustness plots (Figure 4) partly mitigate this.

Presentation
- The paper is clearly structured with a strong motivation and explicit formulations (Equations 2–7). Figures 4–5 and Table 2 convey the main empirical story.
- That said, Figure 1 (Blocks 6, 8–10) contains implausible “eRank” numbers for several layers (tens of millions), inconsistent with the formal definition. This appears to be a labeling or unit error and needs correction. The eRank-to-rank ratio plot (Figure 2; Block 11) is helpful but would benefit from explicitly showing Base vs Instruct distributions and per-layer paired differences.
- Ambiguity in the singular-value ordering (Equation 3 vs Equation 6) reduces clarity of the refinement procedure.
- Some notation choices (e.g., σ0 vs σ1 indexing) and whether “Top-K” refers to largest or earliest indices should be clarified.

Contribution
- Novelty lies in revisiting post-training superficiality via the eRank of weights and using eRank of WΔ to set an adaptive SVD threshold for training-free refinement. The “attenuate-tail” idea (Timber) is a practical variant that differs from pure truncation and simple merge.
- The empirical contribution is a systematic evaluation across multiple model families and sizes with consistent, modest gains without additional training, especially in exploration metrics (Pass@k).
- While related to model merging and SVD-based compression, Timber’s use of eRank on deltas and attenuation rather than truncation is a meaningful, lightweight addition to the toolbox.

Strengths
- Clear problem framing: exploitation–exploration trade-off after post-training (Section 3.1).
- Simple, training-free method with broad applicability across families/sizes, including MoE (Table 2).
- Robustness analyses: λ sweep (Figure 4; Table 5), Pass@k improvements (Figure 5), comparison to truncation baselines (Table 3), and module ablation (Figure 7).
- Formalization of eRank and thresholding provides an adaptive criterion rather than arbitrary ranks or energies.

Weaknesses
- Apparent errors in Figure 1 eRank values (Blocks 6, 8–10) and the mismatch between eRank ≤ rank vs reported multi-million values; this undermines the central evidentiary claim about superficiality at the weight level.
- Ambiguous singular-value ordering (Equation 3 vs Equation 6): preserving “Top-K” is unclear and may invert the intended operation.
- Limited direct evidence that eRank similarity implies preserved subspaces; no subspace overlap metrics or layer-wise paired eRank deltas distribution are shown.
- Compute cost/scalability of full SVD on all linear layers of large models is not quantified; practical deployment guidance is limited.
- Safety/alignment impacts are not evaluated (e.g., helpfulness, toxicity), despite modifying weights; IFEval is included but broader alignment metrics are absent.

Questions
- Can you clarify Figure 1’s eRank magnitudes for q_proj/up_proj/down_proj and reconcile them with Equation 1’s upper bound? Are these actually parameter counts or ranks?
- In Equations 3 and 6, are singular values sorted descending? If not, why is preserving the first K entries considered “Top-K”? Please specify the ordering and indexing.
- Did you measure subspace similarity (e.g., principal angles) of Base vs Instruct weights to substantiate “preserved singular subspaces” beyond equal eRank?
- What is the computational cost (wall-clock, memory) of running full SVD on all linear layers per model, and how does it scale with model size?
- How stable is Timber across different random seeds and sampling configurations? Are improvements still significant under deterministic decoding or lower n?
- Do Timber or Timber-L affect safety/harmlessness metrics or calibration? Can you report alignment-oriented evaluations?
- How does Timber compare to a layer-wise adaptive μ in model merge (Equation 9) or spectrum-shaping applied directly to WI (not WΔ)?

Rating
- Overall (10): 7 — Practical, training-free refinement with consistent gains, but central eRank evidence (Figure 1; Equations 1, 3, 6) has labeling/ordering issues that need correction.
- Novelty (10): 7 — eRank-based thresholding of deltas and attenuation is a meaningful twist beyond truncation and simple merge (Sections 2.2, 3.2; Table 3).
- Technical Quality (10): 6 — Sound core idea, but eRank reporting inconsistency (Figure 1) and singular-value ordering ambiguity (Equations 3–6) weaken rigor; limited subspace analysis.
- Clarity (10): 7 — Generally well-written with useful figures/tables, but Figure 1 magnitudes and Top-K definition are confusing (Blocks 6, 8–10; Equation 6).
- Confidence (5): 4 — High confidence in empirical improvements; medium confidence in the weight-level superficiality claim due to figure inconsistencies and ordering ambiguity.



Summary
The manuscript argues that post-training is superficial at the weight level by showing effective ranks of Base vs Instruct are nearly identical (Section 2; Figure 1). It then introduces Timber, a training-free refinement that SVD-decomposes the weight delta WΔ, uses K = ⌈eRank(WΔ)⌉ as a spectrum threshold, and attenuates (or zeroes) tail singular values to partially revert Instruct toward Base (Equations 2–7). Experiments across Llama/Qwen models report consistent improvements on standard benchmarks (Table 2) and stronger gains in Pass@k (Figure 5), with robustness across λ (Figure 4) and comparisons to truncated SVD and model merge (Table 3, Section 5.2).

Soundness
- The proposed method is conceptually consistent: eRank provides an adaptive measure of spectral complexity, and attenuating small singular values in WΔ reduces over-specialized directions likely introduced by post-training, potentially widening sampling diversity.
- The empirical evidence supports improved exploration: Pass@k curves on AIME’24 and GPQA-Diamond show Timber > Instruct, with the gap growing in k (Figure 5).
- However, there are methodological gaps:
  - The inference “identical eRank ⇒ preserved subspaces” is not directly demonstrated; eRank equality alone is insufficient to claim subspace preservation.
  - The λ selection is tuned on AIME’24 (Block 19). While robustness is shown (Figure 4; Table 5), cross-task hyperparameter sensitivity could be further examined (e.g., per-model cross-validation across tasks).
  - Singular value ordering ambiguity (Equation 3 vs Equation 6) risks mis-implementation; “Top-K” should correspond to largest singular values.
  - The figures of eRank (Figure 1) contain implausible magnitudes for several layers, conflicting with eRank ≤ r.

Presentation
- The manuscript is well-motivated with clear equations and algorithmic steps (Equations 2–7), and comprehensive experimental tables and plots (Table 2; Figures 4–5).
- Figure 1 needs correction: reported “eRank” values for q_proj and FFN projections in the tens of millions are physically inconsistent with the definition (Equation 1). This likely reflects mislabeling (parameter counts or energy) and should be rectified.
- The paper would benefit from a procedural box or pseudo-code for Timber and a precise statement of singular-value sorting.
- Terminology and notation are otherwise consistent; the exploitation vs exploration framing is clear (Section 3.1).

Contribution
- The paper offers a simple, broadly applicable, training-free refinement strategy that consistently improves Instruct models across families and sizes and strengthens exploration as measured by Pass@k.
- The use of eRank of WΔ to adaptively partition the spectrum and attenuate the tail is a practical insight that outperforms naive truncation and uniform merging (Table 3; Section 5.2).
- The weight-level analysis using eRank is a fresh angle on post-training superficiality, although the current evidence requires correction and deeper validation.

Strengths
- Training-free, low-overhead approach; no extra data or RL needed (Section 3.2).
- Broad empirical coverage and consistent gains (Table 2), robust to λ choices (Figure 4; Table 5).
- Clear improvements in exploration (Pass@k), aligning with the stated goal (Figure 5).
- Ablations and baselines (Truncate-R/E, model merge) contextualize Timber’s advantages (Table 3; Section 5.2).

Weaknesses
- Key figure inconsistency in eRank magnitudes (Figure 1; Blocks 6, 8–10) undermines the central superficiality claim.
- Missing direct measures of subspace similarity and layer-wise paired differences; reliance on eRank alone may be insufficient.
- Singular-value ordering ambiguity (Equations 3–6) and lack of pseudo-code could confuse reproduction.
- No report of computational cost (SVD per layer) and deployment-time guidance; scalability not quantified.
- Limited coverage of alignment/safety metrics; only IFEval for instruction following.

Questions
- Please correct Figure 1’s eRank values and clarify whether they represent eRank, rank, parameter count, or energy; reconcile with Equation 1’s bounds.
- Are singular values sorted descending? If so, update Equations 3 and 6 to make “Top-K” unambiguous.
- Can you include subspace overlap metrics (e.g., principal angles, projection metrics) to support the claim that post-training preserves singular subspaces?
- What is the runtime and memory footprint of Timber on 8B/14B/30B models? Any approximation (e.g., randomized SVD) explored?
- How does Timber affect safety/harmlessness and calibration metrics? Any trade-offs observed?
- Could Timber benefit further from per-layer λ or per-module λ (attention vs FFN), given Figure 7’s module sensitivity?

Rating
- Overall (10): 7 — Strong practical method with consistent gains, but central eRank evidence and singular-value ordering need correction (Figure 1; Equations 3–6).
- Novelty (10): 7 — eRank-thresholded attenuation of WΔ is a useful refinement beyond truncation/merge (Sections 3.2, 5.1–5.2).
- Technical Quality (10): 6 — Good empirical support; theoretical claims on subspace preservation incomplete; figure inconsistency reduces rigor (Figure 1).
- Clarity (10): 7 — Generally clear, but crucial ambiguities in Figure 1 and Top-K ordering should be fixed (Blocks 6–10; Equation 6).
- Confidence (5): 4 — Confident in empirical findings; medium confidence in weight-level analysis pending figure corrections and subspace metrics.



Summary
The work diagnoses a trade-off induced by post-training—better Pass@1, weaker Pass@k—and proposes Timber, a training-free refinement of Instruct weights toward Base by SVD of WΔ and eRank-based thresholding with tail attenuation (Equations 2–7). Across Llama/Qwen models, Timber improves average benchmark scores (Table 2) and notably boosts Pass@k (Figure 5). The paper also contrasts Timber with truncated SVD (Table 3) and simple model merge (Section 5.2), and shows module-specific ablations (Figure 7).

Soundness
- The exploitation/exploration diagnosis is consistent with prior literature and the paper’s empirical Pass@k gains (Section 4.2; Figure 5). The unbiased estimator for Pass@k (Equation 8) is appropriate.
- Timber’s mechanism is plausible: attenuating lower-energy directions in WΔ can reduce overfit/alignment-induced narrowness while retaining high-energy (likely robust) modifications. The attenuation variant generally beats pure truncation (Table 2; Table 3).
- Important technical issues:
  - eRank plots in Figure 1 contradict the formal bound; very large “eRank” values suggest a plotting/labeling error that must be fixed to validate the superficiality claim.
  - The ordering of singular values in Equations 3–6 is unclear; “non-decreasing” together with “Top-K preserve” implies keeping smallest singular values. The implementation intent likely needs to be “non-increasing” or to reverse indices.
  - The claim “post-training preserves singular subspaces” is not directly measured. Without subspace overlap metrics or deltas of eRank distributions per layer, the conclusion remains a hypothesis.

Presentation
- The narrative is logically structured; method equations are explicit; benchmarks and settings are summarized (Section 4.1; Appendix A.3–A.4).
- Figures 4–5 communicate the key improvements. Table 2 is comprehensive and easy to read.
- Presentation issues: Figure 1 numeric anomalies and ambiguous Top-K definition; some figure captions could better state what is plotted (e.g., Base vs Instruct per-layer paired differences, ranks vs eRank vs energy).

Contribution
- Practical significance: a low-cost, training-free technique with consistent gains and improved exploration, useful for practitioners wanting better sampling diversity without retraining.
- Conceptual contribution: using eRank on WΔ as an adaptive spectral threshold and preferring attenuation over truncation; bridges model merging and SVD compression literatures with a new lens.
- Weight-level eRank analysis is a novel angle on superficiality, pending corrections.

Strengths
- Training-free method with broad applicability and meaningful, consistent gains (Table 2).
- Robustness to λ and stronger improvements in Pass@k support the intended exploration enhancement (Figure 4; Figure 5).
- Comparisons to reasonable baselines and informative ablations (Table 3; Section 5.2; Figure 7).

Weaknesses
- Central figure inconsistencies undermine the superficiality evidence; eRank magnitudes must be corrected.
- Missing subspace analyses; reliance on eRank alone is insufficient for the strongest claims.
- Compute/resource considerations (SVD cost) and deployment guidance are missing.
- Potential side effects on alignment/safety are not evaluated; only instruction following (IFEval) is reported.

Questions
- Please correct and clarify Figure 1: what is plotted and why are values exceeding min(m,n)? Can you add per-layer paired differences and correlations between Base/Instruct eRanks?
- Is “Top-K” intended to keep the largest singular values? If so, please revise Equations 3–6 to reflect descending order and correct indexing.
- Could you quantify Timber’s offline compute cost and suggest practical approximations (e.g., randomized SVD)?
- Do results hold under deterministic decoding or lower sampling budgets? Is Mean@k sensitive to temperature/top-p (Appendix A.3.2)?
- Any evaluation of safety/harmlessness or long-form instruction-following quality beyond IFEval?
- Would per-layer or per-module λ further improve performance given Figure 7’s module-specific behavior?

Rating
- Overall (10): 7 — A useful, well-motivated training-free method with consistent empirical gains, but key analytical figures and ordering details need correction (Figure 1; Equations 3–6).
- Novelty (10): 7 — eRank-based attenuation of WΔ is a neat and practical twist over truncation/merge (Sections 3.2, 5.1–5.2).
- Technical Quality (10): 6 — Solid empirical work; analytical support for weight-level claims incomplete; figure/ordering issues reduce rigor.
- Clarity (10): 7 — Clear structure and results, marred by Figure 1 inconsistencies and Top-K ambiguity (Blocks 6–10; Equation 6).
- Confidence (5): 4 — Confident in empirical improvements; moderate caution on analysis due to figure/ordering issues.



Summary
The paper studies Base vs Instruct through effective rank of linear weights, claiming negligible changes post-training (Figure 1), and introduces Timber: compute WΔ, SVD it, choose K via eRank(WΔ), and attenuate or remove tail singular values to partially revert Instruct to Base (Equations 2–7). Evaluations over Llama/Qwen families show consistent average gains and improved Pass@k (Table 2; Figures 4–5). Comparisons with truncated SVD (Table 3) and simple model merge (Section 5.2) indicate Timber’s robustness and superiority.

Soundness
- The Pass@k estimator (Equation 8) and experimental design (multiple rollouts; thinking/non-thinking modes; recommended decoding hyperparameters) are appropriate and strengthen the exploration claim.
- The method’s rationale is credible: attenuating lower-energy directions in alignment deltas may relax over-constrained behaviors, increasing diversity without discarding useful high-energy adjustments.
- Critical deficiencies:
  - Figure 1’s reported eRank values for several layers are implausible (multi-million), inconsistent with Equation 1; likely mislabeling undermines the weight-level superficiality evidence.
  - Equations 3–6 have a sorting ambiguity; preserving “Top-K” with “non-decreasing” σ implies retaining smallest σ. This should be corrected to “non-increasing” or indexing reversed.
  - The paper assumes eRank invariance implies superficiality but does not test subspace overlap or show Base vs Instruct eRank deltas across all layers; only three layers are “randomly selected,” yet Figure 2’s ratio plot does not explicitly compare Base vs Instruct.

Presentation
- Strong organization and comprehensive experiments; ablations and baselines are well-chosen.
- Clarity suffers in two places: Figure 1 numeric anomalies and the Top-K ambiguity. A concise algorithm box specifying SVD ordering and exact per-layer operations would improve reproducibility.
- Minor: specify whether Equation 7’s λ is global or per-layer/per-module; currently appears global (Block 19), but module-sensitive gains (Figure 7) suggest potential benefits of finer granularity.

Contribution
- Practical impact: A simple, training-free plug-in that reliably improves exploration and mean accuracy across diverse models.
- Conceptual impact: Using eRank as an adaptive threshold on WΔ and preferring attenuation over truncation is a useful insight not common in model merge/compression literature.
- Analytical impact: Fresh perspective on post-training via eRank of weights, though more rigorous validation is needed.

Strengths
- Consistent empirical gains, including on MoE Qwen3-30B-A3B (Table 2).
- Meaningful improvements in Pass@k and robustness to λ (Figures 4–5; Table 5).
- Comparisons to truncation and merge baselines, showing Timber’s stability and effectiveness (Table 3; Section 5.2).
- Ablations (Figure 7) illuminate where benefits arise (attention vs FFN).

Weaknesses
- Central figure inconsistency and ordering ambiguity are non-trivial and must be corrected.
- No runtime/memory analysis; feasibility for very large models and practical deployment considerations are missing.
- Lack of deeper analysis on why attenuating tail components translates to exploration gains; e.g., diversity metrics over generations are not reported.
- Safety/alignment impacts remain unassessed; IFEval is limited.

Questions
- Please fix Figure 1 and clarify whether values are eRank, rank, or parameter counts; include per-layer Base vs Instruct paired differences and correlation.
- Are singular values sorted descending? If so, update Equations 3–6 accordingly and add pseudo-code for Timber.
- What is the end-to-end cost of Timber (per model, per GPU), and how does randomized SVD or blockwise SVD affect results?
- Could per-layer or per-module λ improve performance? Any evidence from preliminary sweeps beyond Figure 7?
- Do improvements persist at lower sampling budgets (smaller n) or under deterministic decoding?
- Can you add diversity measures (e.g., self-BLEU across samples) to support the exploration interpretation?

Rating
- Overall (10): 7 — Useful training-free method with consistent gains; figure/ordering issues weaken analytical claims but not the empirical contribution (Figure 1; Equations 3–6; Table 2).
- Novelty (10): 7 — eRank-based attenuation of WΔ is a practical, novel twist over truncation/merge (Sections 3.2, 5.1–5.2).
- Technical Quality (10): 6 — Solid experiments; analytical rigor is limited by figure inconsistencies and lack of subspace metrics.
- Clarity (10): 7 — Clear narrative overall; needs corrections in key figures and method specification (Blocks 6–10; Equation 6).
- Confidence (5): 4 — High confidence in empirical improvements; moderate confidence in the weight-level superficiality analysis pending corrections.