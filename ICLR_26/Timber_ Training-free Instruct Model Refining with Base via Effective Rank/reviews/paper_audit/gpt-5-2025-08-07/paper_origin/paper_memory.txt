# Global Summary
The paper investigates the weight-level “superficiality” of LLM post-training (Base → Instruct) using effective rank (eRank) and introduces Timber, a training-free method to refine Instruct models by partially reverting them toward their paired Base models via targeted modification of weight deltas. Timber decomposes the Instruct–Base weight difference with SVD, uses eRank to define a head/tail split of singular values, and either discards the tail (Timber-L) or attenuates it by a factor λ ∈ (0,1) (Timber). The approach aims to improve exploration (Pass@k) without harming exploitation (Pass@1/Mean@1).

Evaluation spans Llama 3 and Qwen3 series (0.6B–30B, including MoE Qwen3-30B-A3B), on IFEval, MATH, MATH-500, GPQA-Diamond, HellaSwag, plus AIME’24 and HumanEval for Qwen “Thinking” mode. Across 7 models, Timber improves average accuracy over Instruct (e.g., Llama-3.1-8B: 55.23 vs. 53.52, +1.71; Qwen3-8B: 74.84 vs. 74.09, +0.75; Qwen3-14B: 77.66 vs. 76.86, +0.80). Timber also boosts AIME’24/HumanEval in Thinking mode (e.g., Qwen3-30B-A3B HumanEval: 96.14 vs. 93.7). Pass@k curves on AIME’24 and GPQA-D show widening gains over vanilla Instruct as k increases. The authors report eRank of paired Base/Instruct weight matrices is nearly identical (eRank-to-rank ratios centered around ≈0.85), reinforcing the superficiality hypothesis.

Caveats and constraints: Timber is training-free and modifies only linear-layer weights (biases and normalization untouched). λ is selected from {0.2, 0.5, 0.8} via AIME’24 validation. Runtime cost and memory footprint are not specified. Numerical Pass@k values from curves are not explicitly tabulated; authors state gains increase with k.

# Abstract
- Problem: Post-training alignment creates Instruct models from Base models but is hypothesized to be superficial. Authors provide weight-level evidence via effective rank (eRank) showing negligible changes after post-training and identify a trade-off: improved exploitation but limited exploration.
- Method: Timber, training-free, refines Instruct by partially reverting toward Base using SVD of weight deltas; tail singular values are removed (Timber-L) or attenuated (Timber).
- Findings: On Llama and Qwen series, Timber consistently improves vanilla Instruct models, especially Pass@k.
- Claims: eRank of corresponding linear layers in Base/Instruct is almost identical; Timber enhances exploration while preserving exploitation.

# Introduction
- Context: LLMs are pretrained Base models followed by post-training (SFT/RL) to become Instruct models (e.g., Qwen3, Llama 3, Deepseek R1). Prior works suggest post-training is superficial and alignment-resistant.
- New perspective: Effective rank (eRank) quantifies effective dimensionality via singular value uniformity; authors show Base vs. Instruct have almost identical eRanks across linear layers (Figure 1).
- Identified trade-off: Instruct improves Pass@1 but underperforms on Pass@k for large k; post-training suppresses sampling space (exploration).
- Proposal: Timber refines weight deltas to partly revert Instruct toward Base; SVD-based decomposition with eRank-derived head/tail, attenuating or removing the tail to balance exploitation/exploration.
- Contributions:
  - Revisit Base/Instruct via eRank, giving weight-level evidence of superficiality.
  - Introduce training-free Timber to enhance exploration while preserving exploitation.
  - Demonstrate effectiveness/robustness across Llama and Qwen models.

# Preliminaries
- Paired models (Table 1):
  - Llama-3.1-8B → Llama-3.1-8B-Instruct (Thinking: ✗)
  - Llama-3.2-1B → Llama-3.2-1B-Instruct (Thinking: ✗)
  - Llama-3.2-3B → Llama-3.2-3B-Instruct (Thinking: ✗)
  - Qwen3-0.6B-Base → Qwen3-0.6B (Thinking: ✓)
  - Qwen3-8B-Base → Qwen3-8B (Thinking: ✓)
  - Qwen3-14B-Base → Qwen3-14B (Thinking: ✓)
  - Qwen3-30B-A3B-Base → Qwen3-30B-A3B (Thinking: ✓)
- Effective rank definition (γ=1): eRank(W) = exp(−∑ p_i log p_i) with p_i = σ_i / ∑ σ_i; equivalently base-2 entropy/log₂ and 2^x yields same eRank.
- eRank observations (Figures 1–3):
  - eRank values for corresponding linear layers are nearly identical between Base and Instruct (e.g., Qwen3-14B Layer 4 k_proj: 935/935).
  - eRank-to-rank ratio distribution (Figure 2): medians around 0.842 (Llama-3.2-3B), 0.851 (Llama-3.1-8B), 0.861 (Qwen3-8B), 0.864 (Qwen3-14B); mean ≈0.85, IQR ≈0.75–0.95 across models.
  - Interpretation: Post-training preserves singular subspaces, applying linear transformations; effective dimensionality remains unchanged, supporting superficiality.
- Prior uses of eRank: hidden-state efficiency (Diff-eRank), gradient eRank for data quality; authors state they are first to directly analyze eRank of weights.

# Method
- Goal: Refine Instruct by modifying the weight delta W_Δ = W_I − W_B of linear layers via SVD, guided by eRank.
- Steps:
  - Compute SVD(W_Δ) → U Σ V^T with Σ sorted.
  - Set threshold K = ⌈eRank(W_Δ)⌉.
  - Timber-L (lower-rank): zero tail singular values beyond K (Equation 6).
  - Timber (full-rank): attenuate tail singular values with factor λ (0 < λ < 1), preserving top-K (Equation 7); Timber-L is λ=0.
  - Refined Instruct: W_I^+ = W_B + U refine(Σ) V^T.
- Design notes:
  - eRank acts as adaptive threshold to preserve principal components.
  - Only linear-layer weights are modified; biases and normalization layers remain unchanged.
- Parameter search: λ ∈ {0.2, 0.5, 0.8}, selected by AIME’24 performance (minimal cost, training-free).

# Motivation
- Challenge: Post-training optimizes exploitative reasoning paths, diminishing exploration; empirically Instruct > Base on Pass@1 but < Base on large-k Pass@k.
- Strategy: Use training-free refinement leveraging Base weights; model merge successes motivate weight-delta approach.
- Insight: Since post-training is superficial at weight level, partially revert Instruct toward Base by refining W_Δ using eRank-guided SVD.

# Experiments
- Setup:
  - Models: Llama 3.2 (1B, 3B), Llama 3.1 (8B), Qwen3 (0.6B, 8B, 14B), Qwen3-30B-A3B (MoE).
  - Checkpoints: Official HuggingFace.
  - λ search: {0.2, 0.5, 0.8} on AIME’24 (Appendix A.4).
- Benchmarks:
  - IFEval (instruction-following).
  - MATH (7,500 train; 5,000 test; 4-shot).
  - MATH-500 (500 selected test, 4-shot).
  - GPQA-Diamond (198 questions, 0-shot).
  - HellaSwag (0-shot).
  - Thinking-mode tasks: AIME’24 (0-shot) and HumanEval (164 problems).
- Metrics:
  - Mean@k (average accuracy across k trials) on 6 benchmarks.
  - Exploration: Pass@k via unbiased estimator (Equation 8), rolling out n ≥ k times; repeats: 4 for Llama-3.2-1B, 3 for larger models.
  - Qwen3-0.6B Thinking mode rollouts: 320 (AIME’24) and 256 (GPQA-D).
- Main results (Table 2, averages across 5 or 6 benchmarks):
  - Llama-3.2-1B: Instruct 27.60; Timber-L 28.56 (+0.96); Timber 28.32 (+0.72).
  - Llama-3.2-3B: Instruct 46.19; Timber-L 46.41 (+0.22); Timber 46.57 (+0.38).
  - Llama-3.1-8B: Instruct 53.52; Timber-L 54.89 (+1.37); Timber 55.23 (+1.71).
  - Qwen3-0.6B: Instruct 43.57; Timber-L 43.74 (+0.17); Timber 44.83 (+1.26).
  - Qwen3-8B: Instruct 74.09; Timber-L 74.39 (+0.30); Timber 74.84 (+0.75).
  - Qwen3-14B: Instruct 76.86; Timber-L 77.60 (+0.74); Timber 77.66 (+0.80).
  - Qwen3-30B-A3B: Instruct 76.67; Timber-L 77.12 (+0.45); Timber 77.19 (+0.52).
- Thinking-mode robustness vs. λ (Figure 4; vanilla scores listed):
  - Qwen3-0.6B AIME’24: Instruct 6.67; Timber λ=0.0: 8.89; λ=0.2: 10.00; λ=0.5: 13.33; λ=0.8: 10.00.
  - Qwen3-0.6B HumanEval: Instruct 53.05; Timber λ=0.0: 53.66; λ=0.2: 53.46; λ=0.5: 55.49; λ=0.8: 56.91.
  - Qwen3-30B-A3B AIME’24: Instruct 80.0; Timber λ=0.0: 83.33; λ=0.2: 82.22; λ=0.5: 82.22; λ=0.8: 81.11.
  - Qwen3-30B-A3B HumanEval: Instruct 93.7; Timber λ=0.0: 94.92; λ=0.2: 94.11; λ=0.5: 95.33; λ=0.8: 96.14.
- Pass@k trends (Figure 5, Qwen3-0.6B, Thinking mode):
  - AIME’24 and GPQA-D: Timber and Timber-L achieve higher Pass@k than Instruct, with gaps widening as k increases. Exact Pass@k values not tabulated.
- Claims:
  - Timber-L vs. Timber: Attenuation (Timber) yields larger gains in 6/7 cases.
  - Timber is robust across model families and sizes.

# Discussion
- Truncated SVD comparison (Table 3, Qwen3-8B):
  - Timber energy preservation: 98.82% total energy; Avg 74.84.
  - Truncate-R (rank ratios 0.95, 0.9, 0.85, 0.8; energies 99.60%, 99.04%, 98.30%, 97.39%):
    - Avgs: 74.25, 74.46, 73.84, 74.58.
  - Truncate-E (energy thresholds 99.50%, 99.00%, 98.00%, 95.00%):
    - Avgs: 73.91, 74.07, 74.32, 74.08.
  - Claim: Timber achieves highest average and more robust performance; truncation baselines are sensitive to thresholds.
- Model merge (weighted averaging W_merge = μ W_I + (1−μ) W_B):
  - On Llama-3.2-1B, simple merging slightly improves at μ=0.95 but degrades sharply for smaller μ; Timber shows stronger, more robust gains. Exact numbers for μ sweep not specified; Instruct baseline is 27.60.
- Modules ablation (Llama-3.1-8B, Timber-L):
  - Applying Timber to both attention and FFN yields best overall performance.
  - Attention-only (wo/ FFN) favors IFEval; FFN-only (wo/ Attn) benefits math tasks; normalized scores shown but exact values not specified.
- Case studies (Appendix A.5):
  - GPQA-D examples show Timber producing more comprehensive reasoning (e.g., NMR/DoU analysis selecting C11H12O2; fixation chemistry reasoning identifying active promoters/enhancers).
  - A relativistic travel problem: Timber computes γ≈1961 and time ≈81–83 years, selecting option A; vanilla Instruct selects incorrect option and lacks formula-based reasoning.

# Conclusion
- The paper provides eRank-based weight-level evidence that post-training is superficial: Base and Instruct have nearly identical effective ranks.
- Timber refines weight deltas with eRank-guided SVD, partially reverting Instruct toward Base to enhance exploration while keeping exploitation.
- Two variants: Timber-L (tail removal) and Timber (tail attenuation with λ), applied only to linear weights.
- Extensive experiments across Llama and Qwen models show consistent gains (e.g., up to +1.71 average points on Llama-3.1-8B; improved AIME’24/HumanEval; Pass@k gains increasing with k).
- Future work: Explore additional strategies for weight-delta enhancement.

# References
- Citations include datasets, evaluation platforms, and related analyses: HumanEval (Chen et al., 2021), MATH (Hendrycks et al., 2021), GPQA-Diamond (Rein et al., 2024), HellaSwag (Zellers et al., 2019), OpenCompass (2023), LLaMA 3 (Grattafiori et al., 2024), Qwen3 (Yang et al., 2025), Diff-eRank (Wei et al., 2024), Lima (Zhou et al., 2023a), model merging surveys (Yang et al., 2024), alignment/superficiality analyses (Ye et al., 2025; Zhou et al., 2023a; Ji et al., 2024; Wu et al., 2025), and SVD compression baselines (Wang et al., 2024b; 2025).

# Appendix
- LLM usage (A.1): A language model was used for proofreading; authors are responsible for scientific claims.
- Related work (A.2): eRank applications (Diff-eRank; gradient eRank; KRAdapter; Stiefel optimizer); Base–Instruct weight similarity (RL changes small subnetworks; differences <5% reported by Wu et al., 2025; similar training dynamics and attention sink).
- Evaluation details (A.3):
  - Benchmarks and settings summarized: IFEval (≈500 prompts, prompt_level_strict, 0-shot), MATH (7,500 train, 5,000 test, 4-shot), MATH-500 (500 test, 4-shot), GPQA-D (198 questions, 0-shot), HellaSwag (0-shot), AIME’24 (0-shot), HumanEval (164 problems).
  - Generation hyperparameters (Table 4):
    - Llama-3.2-1B: Temp 0.6, Top_p 0.9, Max_token 4096.
    - Llama-3.2-3B: Temp 0.6, Top_p 0.9, Max_token 8192.
    - Llama-3.1-8B: Temp 0.6, Top_p 0.9, Max_token 8192.
    - Qwen3 Non-Thinking: Temp 0.7, Top_p 0.8, Top_k 20, Max_token 8192.
    - Qwen3 Thinking: Temp 0.6, Top_p 0.95, Top_k 20, Max_token 38912.
- Performance vs. λ (A.4, Table 5; average of 5 benchmarks):
  - Llama-3.2-1B: Vanilla 27.60; λ=0.0: 28.56; 0.2: 28.32; 0.5: 27.91; 0.8: 27.69.
  - Llama-3.2-3B: 46.19; 46.41; 46.23; 46.57; 46.49.
  - Llama-3.1-8B: 53.52; 54.89; 55.23; 53.84; 53.16.
  - Qwen3-0.6B: 43.57; 43.74; 43.86; 43.92; 44.83.
  - Qwen3-8B: 74.09; 74.39; 74.84; 74.46; 74.12.
  - Qwen3-14B: 76.86; 77.60; 77.66; 77.31; 77.43.
  - Qwen3-30B-A3B: 76.67; 77.12; 76.82; 77.19; 77.19.
  - Authors note λ=0.2 is often a sweet point; recommended for recent models.
- Case studies (A.5): Prompts and responses demonstrating Timber’s more thorough reasoning on GPQA-D (chemistry, genomics) and physics/relativity; specific answers highlighted: chemical formula D (C11H12O2), fixation sites C (active promoters/enhancers), relativistic travel time A (~81–83 years).
- Additional details: Timber is training-free; search over λ incurs minimal compute. Exact runtime budgets, memory overhead, or inference latency impacts are not specified in this section.