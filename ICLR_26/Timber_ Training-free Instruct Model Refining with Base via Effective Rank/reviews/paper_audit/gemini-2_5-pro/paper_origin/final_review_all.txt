1) Summary
This paper investigates the "superficial" nature of post-training in Large Language Models (LLMs). The authors first provide novel quantitative evidence for this hypothesis by demonstrating that the effective rank (eRank) of weight matrices remains nearly unchanged between paired Base and Instruct models. Arguing that this superficiality leads to a trade-off that favors exploitation over exploration, the paper proposes Timber, a training-free method to refine an Instruct model. Timber computes the weight delta between the Instruct and Base models, performs SVD on this delta, and then uses the delta's eRank as a threshold to either truncate (Timber-L) or attenuate (Timber) the tail singular values. This process partially reverts the Instruct model towards its Base state to enhance exploration. Experiments on Llama and Qwen models show that Timber consistently improves performance on various benchmarks, particularly by increasing Pass@k scores.2) Strengths
*   **Novel Weight-Level Analysis of Post-Training**
    *   The paper introduces a novel perspective on the superficial alignment hypothesis by analyzing the effective rank (eRank) of weight matrices (Section 2.2). This provides a new form of quantitative evidence for the effects of post-training.
    *   The core finding that eRank values are nearly identical between corresponding layers of Base and Instruct models is compelling and demonstrated across multiple model families and sizes (Figure 1). This is a valuable analytical contribution to understanding instruction tuning.
    *   The analysis of the eRank-to-Rank ratio further strengthens this point by showing a consistent distribution across different models, suggesting a stable property of LLM weight matrices (Figure 2, Section 2.2).*   **Simple and Computationally Inexpensive Method**
    *   The proposed method, Timber, is training-free, which makes it computationally inexpensive and easy to apply to off-the-shelf models (Section 3.2). This is a significant practical advantage over methods requiring additional training.
    *   The core mechanism, which involves SVD on the weight delta and modification of singular values based on the eRank, is conceptually simple and straightforward to implement (Equations 2-7).
    *   The method demonstrates consistent performance improvements in the main results table across a wide range of models and benchmarks, highlighting its potential utility (Table 2). For instance, Timber improves the Llama-3.1-8B average score by +1.71 and the Qwen3-14B score by +0.80.*   **Broad Evaluation Across Diverse Models and Benchmarks**
    *   The experiments cover a diverse set of recent models, including the Llama 3 and Qwen3 series, with sizes ranging from 0.6B to a 30B Mixture-of-Experts model (Table 1, Section 4.1). This demonstrates the broad applicability of the method.
    *   The evaluation is conducted on a suite of well-established benchmarks testing various capabilities, such as instruction following (IFEval), mathematical reasoning (MATH), and scientific question answering (GPQA-Diamond) (Section 4.1).
    *   The paper provides qualitative evidence via case studies that Timber-enhanced models can produce more comprehensive reasoning trajectories compared to the vanilla Instruct model (Appendix A.5, Tables 6-8).3) Weaknesses
*   **Unsubstantiated Claims Due to Missing Figures**
    *   The discussion section makes several key claims that are intended to be supported by figures that are not present in the manuscript. This makes it impossible to verify the conclusions of these analyses.
    *   The comparison against a simple model merging baseline in Section 5.2 relies entirely on a missing "Figure 6". Without this figure, the claim that Timber is more robust and superior is unsubstantiated.
    *   The ablation study on applying Timber to different module types (Attention vs. FFN) in Section 5.3 is based on a missing "Figure 7". The conclusions about the distinct roles of these modules are therefore unsupported by any presented evidence.*   **Serious Concerns Regarding Scholarly Referencing**
    *   The manuscript contains a large number of citations to papers with a publication year of 2025 (e.g., Yang et al., 2025; Guo et al., 2025; Wu et al., 2025; Chen et al., 2025; Yue et al., 2025).
    *   Many of these references include future-dated arXiv identifiers (e.g., `arXiv:2505.XXXXX`, `arXiv:2508.XXXXX`), which correspond to dates that have not yet occurred.
    *   This pattern of referencing non-existent or future-dated work is a critical issue that undermines the paper's credibility and its positioning within the existing scientific literature.*   **Inconsistent Reporting and Limited Scope of Key Experiments**
    *   The paper's central claim that Timber enhances exploration is supported primarily by Pass@k analysis (Figure 5), but this analysis contains inconsistencies. The text claims "The attenuation strategy of Timber is generally superior to directly dropping in Timber-L" (Section 4.2), but the plot for GPQA-Diamond (Figure 5, right) shows Timber-L performing substantially better than Timber.
    *   The crucial Pass@k analysis is only presented for a single small model, Qwen3-0.6B, on two benchmarks (AIME'24 and GPQA-Diamond). The lack of results for larger models like Llama-3.1-8B makes it difficult to assess if the exploration benefits generalize.
    *   The paper claims to enhance exploration "without compromising exploitation" (Section 4.2), but does not report Pass@1 scores, which are a direct measure of exploitation. This omission makes the trade-off difficult to evaluate.*   **Insufficient Justification for eRank as the SVD Threshold**
    *   The paper's initial analysis shows that `eRank(W_Base) ≈ eRank(W_Instruct)` (Figure 1), but the method uses `K = ceil(eRank(W_Delta))` as the threshold (Equation 5). The conceptual link between the eRank of the model weights and the eRank of the delta matrix is not clearly established.
    *   The motivation for using the eRank of the delta as the cutoff point relies on a toy example (Figure 3) and is described as an "adaptive threshold," but this justification lacks a deeper theoretical or empirical grounding for why this specific value is a principled choice.
    *   The paper does not analyze the properties of the singular components below the eRank threshold. Understanding whether these components correspond to noise or overfitting would strengthen the rationale for attenuating them.4) Suggestions for Improvement
*   **Provide Missing Figures to Substantiate Claims**
    *   To allow for proper evaluation of the work, it is essential to include all figures referenced in the text.
    *   Please provide the missing Figure 6 to support the comparison against the model merging baseline discussed in Section 5.2.
    *   Please provide the missing Figure 7 to support the claims made in the module ablation study in Section 5.3.*   **Thoroughly Revise and Verify All Citations**
    *   Please conduct a complete review of the bibliography to correct all references.
    *   Ensure that all cited works are existing publications and that their publication dates and identifiers (e.g., arXiv numbers) are accurate.
    *   Properly situating the work within the established literature is critical for assessing its contribution.*   **Clarify Inconsistencies and Expand Core Analysis**
    *   Please address the direct contradiction between the claim in Section 4.2 that Timber is superior to Timber-L and the results presented in the GPQA-Diamond plot of Figure 5, where Timber-L is clearly better.
    *   To make the central claim about enhanced exploration more convincing, please provide Pass@k analysis for at least one of the larger models evaluated in the paper, such as Llama-3.1-8B, on a key reasoning benchmark.
    *   To substantiate the claim of preserving exploitation, please report Pass@1 scores for the main experiments, for example by adding them to Table 2.*   **Strengthen the Justification for the eRank Threshold**
    *   Please elaborate on the connection between the observation that `eRank(W_Base) ≈ eRank(W_Instruct)` and the methodological choice to use `eRank(W_Delta)` as the threshold. A more principled explanation is needed.
    *   Consider providing a more formal justification for why eRank is a suitable threshold. For instance, does this threshold correspond to a particular property of the singular value distribution of the delta matrix?
    *   An analysis of the function of the singular components being attenuated would be highly valuable. For example, does removing them disproportionately affect performance on the instruction-tuning dataset?5) Score
*   Overall (10): 3 — The paper presents an interesting idea, but is severely undermined by critical flaws including missing figures, unsubstantiated claims, inconsistent results, and highly problematic referencing practices.
*   Novelty (10): 7 — The use of effective rank to analyze post-training and to define a refinement strategy for the weight delta is a novel contribution, though the paper fails to properly situate itself in the literature.
*   Technical Quality (10): 2 — The technical quality is very low due to missing experimental evidence for key claims (Figures 6, 7), contradictions in reported results (Figure 5), and a lack of theoretical grounding.
*   Clarity (10): 4 — While the prose is generally readable, the missing figures, inconsistent plots, and poorly formatted data tables (Figure 1) significantly impede understanding and verification of the paper's claims.
*   Confidence (5): 5 — I am highly confident in my assessment as the topic aligns directly with my area of expertise and I have carefully checked the manuscript's claims against the provided evidence.