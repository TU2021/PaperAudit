1) Summary
This paper investigates the "superficial" nature of post-training in Large Language Models (LLMs). The authors first provide novel quantitative evidence for this hypothesis by demonstrating that the effective rank (eRank) of weight matrices remains nearly unchanged between paired Base and Instruct models. Arguing that this superficiality leads to a trade-off that favors exploitation over exploration, the paper proposes Timber, a training-free method to refine an Instruct model. Timber computes the weight delta between the Instruct and Base models, performs SVD on this delta, and then uses the delta's eRank as a threshold to either truncate (Timber-L) or attenuate (Timber) the tail singular values. This process partially reverts the Instruct model towards its Base state to enhance exploration. Experiments on Llama and Qwen models show that Timber consistently improves performance on various benchmarks, particularly by increasing Pass@k scores.2) Strengths
*   **Novel Weight-Level Analysis of Post-Training**
    *   The paper introduces a novel perspective on the superficial alignment hypothesis by analyzing the effective rank (eRank) of weight matrices (Section 2.2). This provides a new form of quantitative evidence beyond existing work.
    *   The core finding that eRank values are nearly identical between corresponding layers of Base and Instruct models is compelling and demonstrated across multiple model families and sizes (Figure 1). This is a valuable analytical contribution to understanding the effects of instruction tuning.
    *   The analysis of the eRank-to-Rank ratio further strengthens this point by showing a consistent distribution across different models, suggesting a stable property of LLM weight matrices (Figure 2, Section 2.2).*   **Simple, Effective, and Practical Method**
    *   The proposed method, Timber, is training-free, which makes it computationally inexpensive and easy to apply to off-the-shelf models (Section 3.2).
    *   The core mechanism, which involves SVD on the weight delta and modification of singular values, is conceptually simple and straightforward to implement (Equations 2-7).
    *   The method demonstrates consistent performance improvements across a wide range of models and benchmarks, highlighting its effectiveness and practical utility (Table 2). For instance, Timber improves the Llama-3.1-8B average score by +1.71 and the Qwen3-14B score by +0.80.*   **Comprehensive and Rigorous Empirical Evaluation**
    *   The experiments cover a diverse set of recent models, including the Llama 3 and Qwen3 series, with sizes ranging from 0.6B to a 30B Mixture-of-Experts model (Table 1, Section 4.1). This demonstrates the broad applicability of the method.
    *   The evaluation is conducted on a suite of well-established benchmarks testing various capabilities, such as instruction following (IFEval), mathematical reasoning (MATH), and coding (HumanEval) (Section 4.1).
    *   The paper includes strong comparisons and ablations that validate the design choices. This includes comparisons against standard truncated SVD (Table 3) and simple model merging (Figure 6), as well as an analysis of applying Timber to different module types (Figure 7).*   **Clear Motivation and Targeted Validation**
    *   The work is well-motivated by the exploration-exploitation trade-off in post-trained models, a known issue in the field (Section 3.1).
    *   The authors provide direct evidence supporting their central hypothesis that Timber enhances exploration. The Pass@k analysis clearly shows that Timber and Timber-L significantly outperform the baseline Instruct model, with the performance gap widening as `k` increases (Figure 5). This directly validates the method's intended effect.
    *   The case studies provide qualitative evidence that Timber-enhanced models produce more comprehensive reasoning trajectories compared to the vanilla Instruct model (Appendix A.5, Tables 6-8).3) Weaknesses
*   **Insufficient Justification for eRank as the SVD Threshold**
    *   The paper's initial analysis shows that `eRank(W_Base) ≈ eRank(W_Instruct)` (Figure 1), which is used to argue for the superficiality of post-training. However, the method uses `K = ceil(eRank(W_Delta))` as the threshold for refining the delta's singular values (Equation 5). The conceptual link between these two distinct uses of eRank is not well-established.
    *   The motivation for using the eRank of the delta as the cutoff point relies on a toy example (Figure 3) and the claim that it serves as an "adaptive threshold." This justification is somewhat hand-wavy and lacks a deeper theoretical or empirical grounding for why this specific value is a principled choice for separating "head" and "tail" components of the weight delta.
    *   It is unclear what properties the singular components below the eRank threshold possess. The paper does not analyze whether these components correspond to noise, overfitting on instruction data, or some other specific function, which would strengthen the rationale for attenuating them.*   **Limited Scope of Pass@k Analysis**
    *   The central claim of the paper is that Timber enhances the exploration capability of Instruct models. The primary evidence for this is the Pass@k performance (Figure 5).
    *   However, this crucial analysis is only presented for a single small model, Qwen3-0.6B, on two benchmarks (AIME'24 and GPQA-Diamond).
    *   The lack of Pass@k results for larger and more capable models (e.g., Llama-3.1-8B, Qwen3-14B) makes it difficult to assess whether the observed exploration benefits generalize and scale with model size.*   **Incomplete Picture of the Exploration-Exploitation Trade-off**
    *   The paper claims that Timber enhances exploration "without compromising exploitation" (Section 4.2, Conclusion). Exploitation is often associated with Pass@1 performance.
    *   The main results are reported in terms of Mean@k (Table 2) or overall accuracy (Figure 4), which are aggregate metrics that combine the effects of both exploration and exploitation.
    *   The paper does not explicitly report Pass@1 scores. Without this metric, it is difficult to verify that exploitation capabilities are fully preserved or to understand the nuanced trade-offs. It is possible that Pass@1 performance sees a slight degradation that is masked by the gains in Pass@k for larger `k`.4) Suggestions for Improvement
*   **Strengthen the Justification for the eRank Threshold**
    *   Please elaborate on the connection between the observation that `eRank(W_Base) ≈ eRank(W_Instruct)` and the methodological choice to use `eRank(W_Delta)` as the threshold. A more principled explanation would significantly strengthen the paper.
    *   Consider providing a more formal justification for why eRank is a suitable threshold. For instance, does this threshold correspond to a particular property of the singular value distribution of the delta matrix across different layers or models?
    *   An analysis of the function of the singular components being attenuated would be highly valuable. For example, does removing them disproportionately affect performance on the instruction-tuning dataset while preserving performance on general benchmarks?*   **Expand the Pass@k Analysis**
    *   To make the central claim about enhanced exploration more convincing, please provide Pass@k analysis for at least one of the larger models evaluated in the paper, such as Llama-3.1-8B or Qwen3-8B.
    *   Presenting these results on a key reasoning benchmark like MATH or HumanEval, where diverse solution paths are critical, would substantially bolster the paper's conclusions about the method's scalability and generalizability.*   **Provide a More Complete View of the Trade-off**
    *   To substantiate the claim of preserving exploitation, please report Pass@1 scores for the main experiments. This could be added to Table 2 or presented alongside the Pass@k plots in Figure 5.
    *   Reporting Pass@1 would allow for a direct and transparent assessment of the method's impact on the model's primary, most confident predictions.
    *   A brief discussion of any observed changes in Pass@1, even if minor, would provide a more complete and nuanced picture of how Timber navigates the exploration-exploitation trade-off.5) Score
*   Overall (10): 8 — The paper presents a novel analysis, a simple and effective method, and strong empirical results, though the justification for the threshold and the scope of the core Pass@k analysis could be expanded.
*   Novelty (10): 8 — The use of effective rank to analyze post-training and to define a refinement strategy for the weight delta is a novel contribution to the field of training-free model editing.
*   Technical Quality (10): 7 — The method is technically sound and the experiments are extensive, but the central claim about exploration is supported by a limited set of results (Figure 5) and the theoretical justification for the method's core parameter is underdeveloped.
*   Clarity (10): 9 — The paper is very well-written, clearly structured, and easy to follow, with effective figures and tables supporting the main claims.
*   Confidence (5): 5 — I am highly confident in my assessment as the topic aligns directly with my area of expertise.