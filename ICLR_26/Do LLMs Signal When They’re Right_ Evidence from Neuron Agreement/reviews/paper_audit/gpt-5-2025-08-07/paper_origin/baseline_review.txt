Summary
The paper studies whether internal signals in LLMs—specifically neuron activation patterns—can better indicate response correctness than external outputs (log-probs, entropy, self-evaluation). It reports that (i) external confidence metrics correlate with and are low-dimensional projections of internal activation states (Figure 2; Section 3.2), (ii) correct responses activate fewer unique neurons and show stronger cross-sample agreement (Figure 4(b,c); Section 3.3), and (iii) leveraging these properties enables a new unsupervised ensemble method, Neuron-Agreement Decoding (NAD), based on Jaccard similarity over activated-neuron sets, with early stopping at 32 tokens (Sections 4.1–4.2). Across multiple reasoning and coding benchmarks, NAD variants match or approach majority voting and outperform Avg@64 under fixed sampling (Table 1), while early stopping reduces tokens by ~98–99% with minimal accuracy loss (Table 2; Appendix Table 3; Figure 8).Strengths
- Bold internal-signal-based ensemble formulation
  • NAD builds selection on pairwise Jaccard similarities of activated-neuron sets and medoid/kNN/DBSCAN variants (Section 4.1; Figure 5), proposing a clear alternative to output-based confidence methods; this is novel and potentially impactful for open-ended tasks where textual voting is hard (Section 2; Section 4.1).
  • The MinAct strategy operationalizes Insight 2 directly by selecting the sample with fewest activated neurons (Section 4.1), offering a lightweight, unsupervised selector; this increases methodological diversity (novelty/efficiency).
  • The framework relies solely on internal signals and does not require comparable textual outputs (Figure 1; Section 4.1), important for settings where majority voting is inapplicable (impact).- Empirical evidence linking internal activations to external confidence
  • Neuron count correlates negatively with self-certainty and positively with entropy (Pearson r ≈ −0.605/+0.633; Spearman r ≈ −0.631/+0.660) in Qwen3-4B-Think on AIME24 (Figure 2; Section 3.2), supporting the claim that external metrics are low-dimensional projections (technical soundness).
  • t-SNE over Jaccard similarities reveals clusters of activation patterns that do not collapse onto a single entropy value (Figure 3; Section 3.2), indicating richer internal structure than scalar metrics capture (novelty/clarity).
  • Visualizations place incorrect responses at cluster margins (Figure 4(a); Section 3.3), consistent with the consensus hypothesis (technical plausibility).- Discoveries about correctness and neuron usage
  • Correct samples activate fewer unique neurons than incorrect ones (Figure 4(b); Section 3.3), a core empirical finding that directly motivates selectors and early pruning (novelty/impact).
  • Token-wise trajectories show incorrect responses engage more neurons and shift strategies repeatedly (Figure 4(c); Section 3.3), supporting early-stage discrimination (technical soundness).
  • Density plots further illustrate separability between correct/incorrect activation counts across tasks (Conclusion Figures (a–c)), strengthening generality claims within the evaluated scope (experimental rigor).- Early stopping yields large efficiency gains with competitive accuracy
  • Early pruning at B=32 reduces token usage by ~97–99% across scientific benchmarks (Table 2; Section 5.2) and code benchmarks (Appendix Table 3), demonstrating practical efficiency (impact).
  • Accuracy remains competitive or improved versus Avg@64 under early stopping (Table 2; Appendix Table 3), validating usefulness of internal signals for early correctness prediction (experimental rigor).
  • Accuracy vs. stopping position analysis shows non-monotonic behavior with strong performance at small B (Figure 8; Appendix Figures 9–10), informing practical truncation choices (clarity/engineering guidance).- Broad empirical evaluation across tasks and models
  • Tests on math/science (AIME24/25, GPQA) and code (HumanEval, MBPP, LiveCodeBench v5) with three models (Qwen3-4B-Think, Qwen3-4B-Instruct, R1-Qwen3-8B) (Section 5.1; Table 1) show NAD variants often match or exceed baselines (experimental breadth).
  • NAD-kNN often provides the strongest performance, e.g., Qwen3-4B-Think: 79.9 avg vs. 76.7 Avg@64 (Table 1), evidencing practical selection effectiveness (impact).
  • Results on code, where majority voting is inapplicable, show consistent improvements over Avg@64 on most tasks (Table 1; Section 5.2), highlighting applicability (experimental rigor).- Clear methodological exposition and anchoring
  • The activated-neuron contribution is defined precisely via SwiGLU FFN and unembedding (Equation (1); Section 3.1), aiding reproducibility (clarity/technical soundness).
  • Activated-neuron set over chunks is defined (Equations (2–3); Section 3.1), explaining local aggregation (clarity).
  • Threshold implementation with token-level top-k aggregation is given (Appendix B; Equations (5–6)), enabling replication of activation extraction (experimental rigor).- Ablations and analysis strengthen understanding
  • Top-k aggregation ablation shows best separation with no global top-k across tokens (Figure 7; Section 5.3), clarifying design trade-offs (technical soundness).
  • Early stopping position sweeps (Figure 8; Appendix Figures 9–10) detail the robustness and trade-off between signal quality and noise accumulation (experimental rigor).
  • Comparison of MinAct vs MaxActivation across domains (Figure 6; Section 5.3) probes domain-specific behavior of activation heuristics (insight/clarity).- Practicality considerations discussed
  • Limitations note selector sensitivity and storage overhead; propose bitset/bitmap and parallel Jaccard under early stopping to manage memory (Appendix A), reflecting engineering awareness (clarity/practicality).Weaknesses
- Limited theoretical grounding and causal claims
  • The “low-dimensional projection” assertion is presented as a hypothesis (Section 3.2: “We hypothesize...”), with correlational evidence (Figure 2) but no formal derivation mapping activations to entropy/self-certainty (technical rigor).
  • The t-SNE clustering in Figure 3 is qualitative and can be sensitive to perplexity/initialization; no robustness checks or statistical cluster validity tests are reported (experimental rigor).
  • Footnote in Section 3.3 acknowledges inability to exhaust mappings and defers exploration (“We cannot exhaust...”), leaving the central premise under-theorized (novelty grounding).- Incomplete experimental detail and potential reproducibility gaps
  • The kNN, DBSCAN, and medoid selectors lack specified hyperparameters (e.g., k for kNN; eps/minPts for DBSCAN) and tuning procedures (Section 4.1; Appendix A acknowledges “did not determine which is most effective...”), impeding replication (clarity).
  • Early stopping is fixed at B=32 in main experiments without reporting how B was chosen beyond Figure 8’s sweep; many positions show non-monotonic behavior (Section 4.2; Section 5.3; Figure 8), suggesting sensitivity (experimental rigor).
  • The threshold top-k value is fixed at 500 (Appendix B), but the impact of varying this key hyperparameter on correlations (Figure 2) and accuracy (Table 1–3) is not reported (technical completeness).- Potential confounds and measurement choices
  • Although Figure 4(c) aligns trajectories by token to argue non-length effects, broader controls (e.g., normalizing by decoded steps, sampling temperature, top-p across tasks/models) are not provided (Section 3.3), leaving residual confounds (technical rigor).
  • Activation counting depends on chunking and union over chunks (Equation (3); Section 3.1); alternative aggregation schemes (e.g., weighted by token position or per-layer normalization) are not explored beyond top-k ablation (Figure 7), possibly biasing counts (experimental completeness).
  • The choice of using Jaccard over binary activated sets ignores contribution magnitudes; similarity may change if weighted Jaccard or continuous overlap is used (Section 4.1), affecting NAD’s decisions (methodological robustness).- Scope limitations across models and tasks
  • Only three models are evaluated (Qwen3-4B-Think/-Instruct, R1-Qwen3-8B) (Section 5.1); larger or structurally different architectures are absent, limiting generality (experimental breadth).
  • The science/math datasets emphasize extractable answers; while coding is included, other open-ended domains (e.g., long-form reasoning, multi-hop QA) are not evaluated (Section 5.1), narrowing applicability (impact).
  • The activated-neuron computation relies on SwiGLU-based FFN formulation (Equation (1); Section 3.1); generalization to other activation and FFN variants is not demonstrated (technical generality).- Baseline coverage and comparability
  • Baselines are Avg@64 and Cons@64; contemporary probability/entropy-based pruning/selection are discussed in Related Work (Section 2) but not included in empirical comparisons, leaving unanswered whether NAD is better than outputs-based signals (experimental rigor).
  • For code, the “curated baseline” and single-execution protocol (Section 5.1) may understate what stronger multi-execution or validator-based settings achieve; comparability to practical deployment regimes is unclear (clarity/impact).
  • Claims that NAD “matches majority voting” are task-dependent; in Table 1, NAD-kNN is competitive but not consistently superior to Cons@64 (e.g., AIME24+25: 85.0 vs. 86.7 for Qwen3-4B-Think), so phrasing should be carefully bounded (clarity).- Practical overhead and scalability not fully quantified
  • Pairwise Jaccard requires O(n^2) similarity computations; the paper does not report wall-clock, memory footprint, or throughput impact under n=64 (Section 4.1; Appendix A), leaving efficiency claims incomplete (engineering rigor).
  • Storage overhead for activation bitsets is acknowledged as non-trivial (Appendix A) but not quantified across models/datasets (memory per sample or per chunk), making operational feasibility uncertain (practicality).
  • Early stopping reduces tokens dramatically (Table 2; Appendix Table 3), yet total end-to-end latency including activation extraction and selection is not reported, which matters in real-time systems (impact/clarity).Suggestions for Improvement
- Strengthen theoretical grounding and validate causality
  • Provide a formal derivation or approximation linking activation statistics to entropy/self-certainty (e.g., via linearization of the unembedding layer), and test predictive models that reconstruct scalar metrics from activation features, reporting reconstruction error (Section 3.2; Figure 2).
  • Include robustness analyses of clustering (Figure 3): vary t-SNE parameters, compare with UMAP/PCA, and compute quantitative cluster validity indices (e.g., silhouette) over the Jaccard matrix to reduce reliance on qualitative plots (Section 3.2).
  • Expand the analysis referenced in the footnote (Section 3.3) by enumerating additional activation mappings, e.g., per-layer/position histograms or feature importance, and test whether these improve selection beyond Jaccard or MinAct.- Provide fuller experimental detail to enhance reproducibility
  • Report selector hyperparameters and tuning: k values for kNN; eps/minPts for DBSCAN; tie-breaking rules; and sensitivity curves showing accuracy vs. these parameters (Section 4.1; Appendix A).
  • Justify the choice of B=32 with cross-dataset sweeps and selection of a principled operating point; where Figure 8 shows non-monotonicity, include a selection policy (e.g., choose B that optimizes validation accuracy/efficiency trade-off) and report it consistently (Sections 4.2, 5.3).
  • Vary the threshold top-k=500 (Appendix B) and report its effect on correlations (Figure 2) and accuracy (Tables 1–3), to confirm that key findings are robust to this parameter.- Address potential confounds and refine measurement choices
  • Control for output length and decoding settings: report correlations conditioned on fixed sequence lengths, and include ablations over temperature/top-p to rule out sampling-induced activation inflation (Section 3.3; Figure 4(c)).
  • Explore alternative aggregation schemes beyond union (Equation (3)), such as per-layer weighting, recency-weighted counts, or activation magnitude thresholds, and compare their separability (extension of Figure 7 analysis).
  • Evaluate weighted similarity metrics (e.g., weighted Jaccard or cosine over contribution vectors) versus binary Jaccard, and report whether selectors improve with magnitude-aware measures (Section 4.1).- Broaden scope across models and tasks
  • Add evaluations on larger and architecturally different models (e.g., varying FFN architectures or activation functions) to test generality of Equation (1)-based definitions and NAD selectors (Section 5.1).
  • Include additional open-ended tasks (e.g., multi-hop QA, long-form reasoning) to assess whether neuron agreement holds beyond math/science and coding (Section 5.1).
  • Where FFN variants differ, adapt the activation contribution computation and report whether the fewer-neurons-for-correctness pattern persists (Section 3.1).- Expand baseline coverage and ensure fair comparability
  • Implement outputs-based baselines from Related Work (entropy-based pruning, self-certainty scoring) and compare head-to-head with NAD under the same n=64, B, and single-execution protocols (Section 2; Tables 1–3).
  • For code, complement single-execution with a limited multi-execution regime and clear validator constraints, to situate NAD’s gains relative to realistic deployment policies (Section 5.1; Appendix Table 3).
  • Temper claims by highlighting where NAD is competitive but not superior to majority voting (e.g., AIME24+25 in Table 1) and articulate conditions where NAD is preferred (clarifying scope of “matches”).- Quantify practical overhead and end-to-end efficiency
  • Report runtime profiles: activation extraction time per token/chunk, Jaccard matrix build cost (O(n^2)), selector time, and total latency, alongside token savings (Section 4.1; Table 2; Appendix Table 3).
  • Provide memory usage for activation storage per sample and total, and evaluate compression (bitsets/bitmaps) empirically to substantiate Appendix A (practical feasibility).
  • Compare end-to-end throughput under early stopping vs. parallel sampling to show real-world efficiency beyond token counts (Section 5.2; Figure 8), ensuring operational relevance.Score
- Overall (10): 7 — Novel internal-signal decoding and strong efficiency gains with early stopping (Figure 1; Sections 4.1–4.2; Table 2) balanced by limited baselines and theory (Section 3.2; Table 1).
- Novelty (10): 7 — Leveraging neuron agreement for ensemble selection and early correctness prediction is a distinct approach (Section 4.1; Figure 5; Figure 3).
- Technical Quality (10): 6 — Solid correlational and empirical evidence (Figure 2; Figure 4; Tables 1–3) but lacks formal analysis and broader baselines; some hyperparameter details are missing (Section 3.2; Appendix B; Appendix A).
- Clarity (10): 7 — Clear definitions and diagrams (Equations (1–3); Figure 5; Appendix B) with comprehensive tables, though some selector/parameter specifics are underreported (Section 4.1; Section 5.1).
- Confidence (5): 4 — Review based on the full manuscript’s methods/experiments; conclusions rely on provided figures/tables, with some missing implementation details acknowledged (Appendix A; Section 4.1).