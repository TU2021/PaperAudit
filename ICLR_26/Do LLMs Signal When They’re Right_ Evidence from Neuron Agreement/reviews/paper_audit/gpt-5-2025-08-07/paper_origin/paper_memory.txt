# Global Summary
- Problem: Ensemble decoders for LLMs typically judge candidate responses using external signals (e.g., token probabilities, entropy, or self-evaluations), which can be miscalibrated after post-training. The paper asks whether internal signals—neuron activation patterns—better indicate correctness, and whether they can guide label-free ensemble selection early in generation.
- Core approach: Analyze internal neuron activations during generation; find that correct responses activate fewer unique neurons and exhibit stronger cross-sample agreement. Propose Neuron-Agreement Decoding (NAD), an unsupervised best-of-N selector that uses activation sparsity (MinAct) and cross-sample neuron agreement (kNN, medoid, DBSCAN) via Jaccard similarity of activated-neuron sets. Enable early correctness prediction within the first 32 tokens (B=32) and early stopping.
- Evaluation scope: Three models (Qwen3-4B-Think-0527, Qwen3-4B-Instruct-0527, DeepSeek-R1-0528-Qwen3-8B), n=64 samples per input, temperature=0.6, top-p=0.9. Benchmarks include math/science with canonical answers (AIME24, AIME25, GPQA) and open-ended code (LiveCodeBench v5, HumanEval, MBPP). Baselines: Avg@64 (mean accuracy over all samples) and Cons@64 (majority vote; ties count as failure) where applicable. Single-execution protocol for code.
- Key findings:
  - External confidence metrics are low-dimensional projections of richer internal dynamics. Activated-neuron counts correlate negatively with self-certainty (Pearson r = −0.605; Spearman r = −0.631) and positively with entropy (Pearson r = 0.633; Spearman r = 0.660); p < 0.05.
  - Correct responses activate substantially fewer unique neurons and cluster by activation patterns; incorrect ones diverge and lie at margins.
  - NAD matches or approaches majority voting on math/science tasks and consistently outperforms Avg@64, including open-ended coding where voting is inapplicable. Example overall averages: Qwen3-4B-Think NAD-kNN 79.9 vs Avg@64 76.7; R1-Qwen3-8B NAD-kNN 74.5 vs Avg@64 72.6; Qwen3-4B-Instruct NAD-DBSCAN 65.1 vs Avg@64 62.2.
  - Early stopping at B=32 reduces token consumption by ~97–99% while maintaining or improving accuracy versus random sampling averages (e.g., Qwen3-4B-Think AIME24+25 tokens: 55.2M Avg@64 vs 1.2–1.3M with NAD; GPQA tokens: 102.2M vs 1.8–2.0M).
- Caveats explicitly stated: Best selector choice under different sampling patterns not fully determined; storage overhead for neuron activations; experiments fixed at n=64 and single-execution for code; engineering trade-offs discussed.

# Abstract
- Motivation: External ensemble signals (token probabilities, entropies, self-evals) can be poorly calibrated post-training.
- Observations:
  1) External signals are low-dimensional projections of internal neuron dynamics.
  2) Correct responses activate “substantially fewer unique neurons” than incorrect ones.
  3) Correct-response activations show stronger cross-sample agreement; incorrect ones diverge.
- Method: Neuron Agreement Decoding (NAD), an unsupervised best-of-N selection using activation sparsity and cross-sample neuron agreement; relies solely on internal signals and does not require comparable textual outputs.
- Capabilities: Early correctness prediction within the first 32 tokens; aggressive early stopping.
- Results:
  - Across math/science with verifiable answers, NAD matches majority voting.
  - On open-ended coding where voting is inapplicable, NAD consistently outperforms Avg@64.
  - Early pruning reduces token usage by “99%” with minimal quality loss.
- Claim: Internal signals provide reliable, scalable, efficient guidance for label-free ensemble decoding.

# Introduction
- Context: Sample–evaluate–ensemble methods (e.g., majority voting) improve LLM reasoning without ground truth and are used both at inference and in unsupervised RL training. More sophisticated ensembles leverage self-evaluation or output confidence (token probabilities, entropy), but these are external behaviors and can be miscalibrated (GPT-4 reports: token probability not linearly related to correctness after post-training).
- Proposal: Investigate internal neuron activation vectors and their relation to external signals and correctness. Findings:
  1) External metrics (entropy/confidence) are low-dimensional projections of internal activations.
  2) Correct responses activate fewer neurons than incorrect ones.
  3) Correct responses activate similar unique neurons across samples (agreement).
- Method overview: Neuron-Agreement Decoding (NAD) selects responses via minimal neuron activations or maximal agreement (similar activated-neuron sets) and supports early prediction (e.g., within 32 tokens). Does not require text-level comparability.
- Contributions:
  - Analyze relationships among neuron activation, external behavior, and correctness.
  - Design NAD for scalable best-of-N sampling.
  - Validate effectiveness and efficiency via early stopping, with substantial token savings.

# Related Work
- Outputs-based voting:
  - Self-consistency (Wang et al., 2022) selects by majority vote over CoT samples; improved arithmetic/commonsense QA; variants include Soft Self-Consistency (Wang et al., 2024) with partial credit for similar answers. Effective for constrained outputs but less applicable to open-ended generation.
- Confidence-based selection:
  - Self-certainty (Kang et al., 2025) uses token-level probabilities to estimate chain confidence for best-of-N selection.
  - DeepConf (Fu et al., 2025) prunes low-confidence paths by monitoring entropy during generation to save compute while maintaining accuracy.
  - These require probability distributions and comparable answers, limiting applicability.
- Distinction: NAD operates on internal neuron activations and is designed to function without comparable textual outputs.

# Preliminaries
- Activated neuron set definition:
  - For SwiGLU-based FFN, per-neuron contribution to token y_j: f_neuron(i, l, y_j | y_<j) = (W_u W_out^l ∘ SiLU(x_j^l W_g^l))_{y_j, i}.
  - Activated set N_activated(x, y) includes (i,l) with any token’s contribution exceeding threshold η.
  - Chunked reasoning: divide sequence into B-sized chunks, compute activation sets per chunk, then take union: N_activated(x, y) = ⋃ N_activated(x, y_i).
  - Threshold function details in Appendix B (token-level top-k aggregation).
- Neuron activations as approximate preimages of confidence metrics:
  - Setup: Qwen3-4B-Think; 64 responses per instance on AIME24; compute activated-neuron counts per chunk; compute mean self-certainty and entropy per chunk.
  - Correlations (Fig. 2; p < 0.05):
    - Activated Neuron Count vs Average Self-Certainty: Pearson r = −0.605, Spearman r = −0.631 (negative correlation).
    - Activated Neuron Count vs Average Entropy: Pearson r = 0.633, Spearman r = 0.660 (positive correlation).
  - t-SNE embedding using Jaccard similarity S_{ij} = |N_activated(x, y_i) ∩ N_activated(x, y_j)| / |N_activated(x, y_i) ∪ N_activated(x, y_j)| shows clustering by activation patterns; clusters do not align with entropy values.
  - Hypothesis: Scalar confidence metrics are low-dimensional projections of high-dimensional activated-neuron states; richer structure can guide selection more effectively.

# Experiments
- 3.3 Preliminary experiments:
  - Visualization (t-SNE) shows clusters; correct responses (green) cluster centrally; incorrect (red) lie at margins or diverge.
  - Insight 1: Cross-sample neuron activation patterns define consensus; trajectories aligned with consensus are more likely correct without text-level matching.
  - Insight 2: Correct samples activate fewer neurons throughout generation; the number of activated neurons early can distinguish quality; token-aligned plots show fewer unique neurons for correct chains at matched lengths; motivates early stopping based on chunks.
- 5.1 Setup:
  - Models: Qwen3-4B-thinking-0527, Qwen3-4B-Instruct-0527, DeepSeek-R1-0528-Qwen3-8B.
  - Sampling: n = 64; temperature = 0.6; top-p = 0.9.
  - Datasets:
    - Scientific reasoning with canonical answers: AIME24, AIME25, GPQA.
    - Open-ended code generation: LiveCodeBench v5, HumanEval, MBPP.
  - Protocol & baselines:
    - Avg@64: mean accuracy over 64 samples.
    - Cons@64: majority vote on tasks with extractable ground truth; ties count as failure.
    - Fixed sampling budget n=64; low-interaction regime; code evaluated with single execution of selected candidate.
- 5.2 Main results (Table 1):
  - Qwen3-4B-Think:
    - Avg@64: AIME24+25 74.6; GPQA 66.3; HumanEval 96.0; LCBv5 61.9; MBPP 84.6; Avg 76.7.
    - Cons@64: AIME24+25 86.7; GPQA 68.2.
    - NAD-kNN: 85.0; 68.7; 98.2; 61.7; 86.0; Avg 79.9.
    - NAD-Medoid: 81.7; 66.2; 97.0; 59.3; 85.2; Avg 77.9.
    - NAD-DBSCAN: 83.4; 66.2; 97.6; 59.9; 85.0; Avg 78.4.
    - NAD-MinAct: 85.0; 66.7; 92.7; 58.1; 83.8; Avg 77.3.
  - R1-Qwen3-8B:
    - Avg@64: 70.6; 58.1; 92.1; 58.5; 83.7; Avg 72.6.
    - Cons@64: 78.3; 62.6.
    - NAD-kNN: 78.3; 62.6; 89.6; 57.5; 84.4; Avg 74.5.
    - NAD-Medoid: 75.0; 61.1; 91.5; 55.1; 85.4; Avg 73.6.
    - NAD-DBSCAN: 73.3; 60.1; 90.9; 55.7; 85.4; Avg 73.1.
    - NAD-MinAct: 75.0; 61.1; 90.2; 59.3; 79.0; Avg 72.9.
  - Qwen3-4B-Instruct:
    - Avg@64: 51.7; 59.2; 90.8; 34.1; 75.4; Avg 62.2.
    - Cons@64: 66.7; 61.1.
    - NAD-kNN: 55.0; 61.1; 91.5; 37.1; 74.2; Avg 63.8.
    - NAD-Medoid: 55.0; 61.1; 92.1; 36.5; 75.2; Avg 64.0.
    - NAD-DBSCAN: 60.0; 61.1; 92.1; 36.5; 75.8; Avg 65.1.
    - NAD-MinAct: 61.7; 60.6; 91.5; 31.1; 75.0; Avg 64.0.
  - Claims: NAD substantially outperforms Avg@64 overall; kNN variant is consistently strong; on math/science, NAD is competitive with majority voting; on coding, NAD yields gains where voting is inapplicable; MinAct, while simpler, still beats baselines but lags global-structure methods.
- 5.2 Early stopping results (Table 2; B=32):
  - Qwen3-4B-Think (tokens in millions):
    - Avg@64: AIME 74.6, 55.2M; GPQA 66.3, 102.2M; Avg 70.4.
    - NAD-MinAct: AIME 80.0, 1.2M (−97.8%); GPQA 67.7, 1.8M (−98.2%); Avg 73.9.
    - Similar reductions for kNN/Medoid/DBSCAN: 1.3–2.0M tokens (−97.6% to −98.0%).
  - R1-Qwen3-8B:
    - Avg@64: AIME 70.6, 48.1M; GPQA 58.1, 99.6M; Avg 64.4.
    - NAD-MinAct: AIME 76.7, 0.9M (−98.1%); GPQA 58.1, 1.7M (−98.3%); Avg 67.4.
    - Other NAD: 1.0–1.9M tokens (−97.7% to −98.2%).
  - Qwen3-4B-Instruct:
    - Avg@64: AIME 51.7, 32.0M; GPQA 59.2, 41.6M; Avg 55.5.
    - NAD-MinAct: AIME 61.7, 0.4M (−98.8%); GPQA 63.1, 0.9M (−97.8%); Avg 62.4.
    - Other NAD: 0.4–1.0M tokens (−97.6% to −98.8%).
  - Conclusion: Early stopping enables two-order-of-magnitude token savings while surpassing random sampling in accuracy.
- 5.3 Analysis:
  - Activated neuron aggregation across tokens:
    - Ablation with sequence-level top-k merging (k from 2K to 200K, and no top-k). As k increases, correct-sample distributions shift left (fewer neurons) and incorrect-sample distributions shift right; no-top-k yields best separation (Figure 7).
  - Early stopping position:
    - Positions explored: 2^6 (=64) up to 2^14 (=16384). Later stopping does not monotonically improve selection; potential noise accumulation.
    - Example: AIME24+25, kNN accuracy at stop=4096 is “86.7 vs. 85.0” with full response; stopping at 32 tokens achieves relatively good performance.
  - Minimal vs maximal activations:
    - On math/science, selecting minimal activations yields significantly higher accuracy than maximal; on coding, the gap narrows or reverses (Figure 6). Interpreted as code being more open-ended and knowledge-intensive, weakening activation–performance linkage.

# Method
- Framework (Figure 5): Generate n paths in parallel; stop at k tokens (B=32); compute activated neuron sets; build Jaccard similarity matrix; select by consensus (kNN, medoid, DBSCAN) or by minimal activation (MinAct); resume generation on selected path.
- NAD algorithms (Insight 1):
  - kNN-Agreement: For each sample i, sum top-k similarity scores to others; select highest s_i.
  - Global Medoid: Select sample maximizing sum of similarities ∑_j S_{ij}.
  - DBSCAN: Cluster on D = 1 − S; select largest cluster C; choose medoid within C.
- MinAct (Insight 2): Select trajectory with minimal |N_activated(x, y_i)|.
- Activated neuron computation (Section 3.1):
  - Per-token, per-layer contributions via SwiGLU FFN (SiLU), unembedding W_u, gate W_g^l, output W_out^l; thresholding yields activated set; chunked union (B-sized chunks).
- Early stopping strategy (Section 4.2):
  - For partial output y_≤j (j=B=32), compute N_activated(x, y_≤j), apply selectors, then continue generation only for the chosen trace.

# Conclusion
- Summary: Internal neuron activation patterns signal correctness — correct outputs activate fewer neurons and exhibit stronger agreement across samples. NAD uses these signals to select high-quality responses without requiring comparable text outputs.
- Results: On math/science, NAD matches majority voting; on coding, NAD beats Avg@64. Early pruning reduces tokens up to “99%” with minimal quality loss.
- Implication: Neuron-level internal signals can guide label-free ensemble decoding efficiently and reliably.

# References
- Cited datasets and methods: AIME 2024/2025 (Art of Problem Solving, 2024a; 2024b; 2025a; 2025b), GPQA (Rein et al., 2024), LiveCodeBench v5 (Jain et al., 2024), HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021).
- Prior work: Self-consistency (Wang et al., 2022; ICLR 2023), Soft Self-Consistency (Wang et al., 2024), self-certainty (Kang et al., 2025), DeepConf (Fu et al., 2025), PaLM architecture (Chowdhery et al., 2023), GLU variants (Shazeer, 2020), monosemanticity and neuron analyses (Templeton et al., 2024; Pan et al., 2024), Qwen3 technical report (Qwen Team, 2025), DeepSeek-R1 (DeepSeek-AI, 2025), Model Utility Law (Cao et al., 2025), EffiEval (Wang et al., 2025), Chain-of-thought (Wei et al., 2022).
- Not specified in this section: Additional implementation details beyond those in Appendix B.

# Appendix
- Limitations:
  - Selector impact: Best selector across sampling regimes not fully determined.
  - Storage overhead: Neuron activation storage can be substantial; bitset/bitmap encodings and parallel Jaccard under early stopping (32 tokens) help; compute cost is small.
  - Fixed budget n=64 and single-execution protocol for code; other regimes (more samples or multiple executions) are out of scope.
- Threshold function (Appendix B):
  - Token-level per-layer top-k on j-th token: F_{jl} = topk(A(y_j, l), 64).
  - Sequence-level threshold aggregation: η(j, k) = min{ topk([F_{j1}; …; F_{jl}], k) }.
  - Experiments set k = 500; 64 in Eq. (5) used for efficiency; sequence-level global top-k across tokens not applied unless ablation (Sec. 5.3).
- Detailed code results (Appendix Table 3; tokens in millions):
  - Qwen3-4B-Think:
    - Avg@64: HumanEval 97.0, 52.2M; LCBv5 58.7, 191.9M; MBPP 85.6, 169.8M; Avg 80.4.
    - NAD-kNN: 97.6, 1.1M (−97.9%); 63.5, 3.2M (−98.3%); 85.2, 3.5M (−98.0%); Avg 82.1.
    - NAD-MinAct: 96.3, 1.0M (−98.1%); 65.9, 3.2M (−98.3%); 85.2, 3.1M (−98.2%); Avg 82.4.
  - R1-Qwen3-8B:
    - Avg@64: HumanEval 92.1, 49.3M; LCBv5 61.1, 190.4M; MBPP 84.6, 171.6M; Avg 79.3.
    - NAD-kNN: 94.5, 0.9M (−98.2%); 56.9, 3.2M (−98.3%); 83.2, 3.3M (−98.1%); Avg 78.2.
    - NAD-MinAct: 93.9, 0.9M (−98.2%); 58.7, 2.9M (−98.4%); 82.4, 3.0M (−98.2%); Avg 78.3.
  - Qwen3-4B-Instruct:
    - Avg@64: HumanEval 89.6, 6.0M; LCBv5 31.1, 26.6M; MBPP 74.9, 37.1M; Avg 65.2.
    - NAD-kNN: 90.2, 0.4M (−93.3%); 35.9, 0.6M (−97.7%); 73.9, 1.4M (−96.2%); Avg 66.7.
    - NAD-DBSCAN: 90.9, 0.4M (−93.3%); 35.9, 0.7M (−97.4%); 76.2, 1.5M (−96.0%); Avg 67.7.
- Additional analyses:
  - Early stopping position curves provided for R1-Qwen3-8B on AIME24 and AIME25 (Figures 9–10), showing accuracy and token consumption as functions of stop position (2^6 to 2^14). Performance is non-monotonic with longer generation; early stops (e.g., 32 tokens) achieve strong accuracy with minimal tokens.
- Not specified in this section: Training budgets, hardware, wall-clock runtimes, or error bars beyond correlation p-values.