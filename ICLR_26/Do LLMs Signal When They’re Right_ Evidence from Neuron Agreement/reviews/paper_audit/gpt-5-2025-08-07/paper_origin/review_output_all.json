{
  "baseline_review": "Summary\nThe paper studies whether internal signals in LLMs—specifically neuron activation patterns—can better indicate response correctness than external outputs (log-probs, entropy, self-evaluation). It reports that (i) external confidence metrics correlate with and are low-dimensional projections of internal activation states (Figure 2; Section 3.2), (ii) correct responses activate fewer unique neurons and show stronger cross-sample agreement (Figure 4(b,c); Section 3.3), and (iii) leveraging these properties enables a new unsupervised ensemble method, Neuron-Agreement Decoding (NAD), based on Jaccard similarity over activated-neuron sets, with early stopping at 32 tokens (Sections 4.1–4.2). Across multiple reasoning and coding benchmarks, NAD variants match or approach majority voting and outperform Avg@64 under fixed sampling (Table 1), while early stopping reduces tokens by ~98–99% with minimal accuracy loss (Table 2; Appendix Table 3; Figure 8).Strengths\n- Bold internal-signal-based ensemble formulation\n  • NAD builds selection on pairwise Jaccard similarities of activated-neuron sets and medoid/kNN/DBSCAN variants (Section 4.1; Figure 5), proposing a clear alternative to output-based confidence methods; this is novel and potentially impactful for open-ended tasks where textual voting is hard (Section 2; Section 4.1).\n  • The MinAct strategy operationalizes Insight 2 directly by selecting the sample with fewest activated neurons (Section 4.1), offering a lightweight, unsupervised selector; this increases methodological diversity (novelty/efficiency).\n  • The framework relies solely on internal signals and does not require comparable textual outputs (Figure 1; Section 4.1), important for settings where majority voting is inapplicable (impact).- Empirical evidence linking internal activations to external confidence\n  • Neuron count correlates negatively with self-certainty and positively with entropy (Pearson r ≈ −0.605/+0.633; Spearman r ≈ −0.631/+0.660) in Qwen3-4B-Think on AIME24 (Figure 2; Section 3.2), supporting the claim that external metrics are low-dimensional projections (technical soundness).\n  • t-SNE over Jaccard similarities reveals clusters of activation patterns that do not collapse onto a single entropy value (Figure 3; Section 3.2), indicating richer internal structure than scalar metrics capture (novelty/clarity).\n  • Visualizations place incorrect responses at cluster margins (Figure 4(a); Section 3.3), consistent with the consensus hypothesis (technical plausibility).- Discoveries about correctness and neuron usage\n  • Correct samples activate fewer unique neurons than incorrect ones (Figure 4(b); Section 3.3), a core empirical finding that directly motivates selectors and early pruning (novelty/impact).\n  • Token-wise trajectories show incorrect responses engage more neurons and shift strategies repeatedly (Figure 4(c); Section 3.3), supporting early-stage discrimination (technical soundness).\n  • Density plots further illustrate separability between correct/incorrect activation counts across tasks (Conclusion Figures (a–c)), strengthening generality claims within the evaluated scope (experimental rigor).- Early stopping yields large efficiency gains with competitive accuracy\n  • Early pruning at B=32 reduces token usage by ~97–99% across scientific benchmarks (Table 2; Section 5.2) and code benchmarks (Appendix Table 3), demonstrating practical efficiency (impact).\n  • Accuracy remains competitive or improved versus Avg@64 under early stopping (Table 2; Appendix Table 3), validating usefulness of internal signals for early correctness prediction (experimental rigor).\n  • Accuracy vs. stopping position analysis shows non-monotonic behavior with strong performance at small B (Figure 8; Appendix Figures 9–10), informing practical truncation choices (clarity/engineering guidance).- Broad empirical evaluation across tasks and models\n  • Tests on math/science (AIME24/25, GPQA) and code (HumanEval, MBPP, LiveCodeBench v5) with three models (Qwen3-4B-Think, Qwen3-4B-Instruct, R1-Qwen3-8B) (Section 5.1; Table 1) show NAD variants often match or exceed baselines (experimental breadth).\n  • NAD-kNN often provides the strongest performance, e.g., Qwen3-4B-Think: 79.9 avg vs. 76.7 Avg@64 (Table 1), evidencing practical selection effectiveness (impact).\n  • Results on code, where majority voting is inapplicable, show consistent improvements over Avg@64 on most tasks (Table 1; Section 5.2), highlighting applicability (experimental rigor).- Clear methodological exposition and anchoring\n  • The activated-neuron contribution is defined precisely via SwiGLU FFN and unembedding (Equation (1); Section 3.1), aiding reproducibility (clarity/technical soundness).\n  • Activated-neuron set over chunks is defined (Equations (2–3); Section 3.1), explaining local aggregation (clarity).\n  • Threshold implementation with token-level top-k aggregation is given (Appendix B; Equations (5–6)), enabling replication of activation extraction (experimental rigor).- Ablations and analysis strengthen understanding\n  • Top-k aggregation ablation shows best separation with no global top-k across tokens (Figure 7; Section 5.3), clarifying design trade-offs (technical soundness).\n  • Early stopping position sweeps (Figure 8; Appendix Figures 9–10) detail the robustness and trade-off between signal quality and noise accumulation (experimental rigor).\n  • Comparison of MinAct vs MaxActivation across domains (Figure 6; Section 5.3) probes domain-specific behavior of activation heuristics (insight/clarity).- Practicality considerations discussed\n  • Limitations note selector sensitivity and storage overhead; propose bitset/bitmap and parallel Jaccard under early stopping to manage memory (Appendix A), reflecting engineering awareness (clarity/practicality).Weaknesses\n- Limited theoretical grounding and causal claims\n  • The “low-dimensional projection” assertion is presented as a hypothesis (Section 3.2: “We hypothesize...”), with correlational evidence (Figure 2) but no formal derivation mapping activations to entropy/self-certainty (technical rigor).\n  • The t-SNE clustering in Figure 3 is qualitative and can be sensitive to perplexity/initialization; no robustness checks or statistical cluster validity tests are reported (experimental rigor).\n  • Footnote in Section 3.3 acknowledges inability to exhaust mappings and defers exploration (“We cannot exhaust...”), leaving the central premise under-theorized (novelty grounding).- Incomplete experimental detail and potential reproducibility gaps\n  • The kNN, DBSCAN, and medoid selectors lack specified hyperparameters (e.g., k for kNN; eps/minPts for DBSCAN) and tuning procedures (Section 4.1; Appendix A acknowledges “did not determine which is most effective...”), impeding replication (clarity).\n  • Early stopping is fixed at B=32 in main experiments without reporting how B was chosen beyond Figure 8’s sweep; many positions show non-monotonic behavior (Section 4.2; Section 5.3; Figure 8), suggesting sensitivity (experimental rigor).\n  • The threshold top-k value is fixed at 500 (Appendix B), but the impact of varying this key hyperparameter on correlations (Figure 2) and accuracy (Table 1–3) is not reported (technical completeness).- Potential confounds and measurement choices\n  • Although Figure 4(c) aligns trajectories by token to argue non-length effects, broader controls (e.g., normalizing by decoded steps, sampling temperature, top-p across tasks/models) are not provided (Section 3.3), leaving residual confounds (technical rigor).\n  • Activation counting depends on chunking and union over chunks (Equation (3); Section 3.1); alternative aggregation schemes (e.g., weighted by token position or per-layer normalization) are not explored beyond top-k ablation (Figure 7), possibly biasing counts (experimental completeness).\n  • The choice of using Jaccard over binary activated sets ignores contribution magnitudes; similarity may change if weighted Jaccard or continuous overlap is used (Section 4.1), affecting NAD’s decisions (methodological robustness).- Scope limitations across models and tasks\n  • Only three models are evaluated (Qwen3-4B-Think/-Instruct, R1-Qwen3-8B) (Section 5.1); larger or structurally different architectures are absent, limiting generality (experimental breadth).\n  • The science/math datasets emphasize extractable answers; while coding is included, other open-ended domains (e.g., long-form reasoning, multi-hop QA) are not evaluated (Section 5.1), narrowing applicability (impact).\n  • The activated-neuron computation relies on SwiGLU-based FFN formulation (Equation (1); Section 3.1); generalization to other activation and FFN variants is not demonstrated (technical generality).- Baseline coverage and comparability\n  • Baselines are Avg@64 and Cons@64; contemporary probability/entropy-based pruning/selection are discussed in Related Work (Section 2) but not included in empirical comparisons, leaving unanswered whether NAD is better than outputs-based signals (experimental rigor).\n  • For code, the “curated baseline” and single-execution protocol (Section 5.1) may understate what stronger multi-execution or validator-based settings achieve; comparability to practical deployment regimes is unclear (clarity/impact).\n  • Claims that NAD “matches majority voting” are task-dependent; in Table 1, NAD-kNN is competitive but not consistently superior to Cons@64 (e.g., AIME24+25: 85.0 vs. 86.7 for Qwen3-4B-Think), so phrasing should be carefully bounded (clarity).- Practical overhead and scalability not fully quantified\n  • Pairwise Jaccard requires O(n^2) similarity computations; the paper does not report wall-clock, memory footprint, or throughput impact under n=64 (Section 4.1; Appendix A), leaving efficiency claims incomplete (engineering rigor).\n  • Storage overhead for activation bitsets is acknowledged as non-trivial (Appendix A) but not quantified across models/datasets (memory per sample or per chunk), making operational feasibility uncertain (practicality).\n  • Early stopping reduces tokens dramatically (Table 2; Appendix Table 3), yet total end-to-end latency including activation extraction and selection is not reported, which matters in real-time systems (impact/clarity).Suggestions for Improvement\n- Strengthen theoretical grounding and validate causality\n  • Provide a formal derivation or approximation linking activation statistics to entropy/self-certainty (e.g., via linearization of the unembedding layer), and test predictive models that reconstruct scalar metrics from activation features, reporting reconstruction error (Section 3.2; Figure 2).\n  • Include robustness analyses of clustering (Figure 3): vary t-SNE parameters, compare with UMAP/PCA, and compute quantitative cluster validity indices (e.g., silhouette) over the Jaccard matrix to reduce reliance on qualitative plots (Section 3.2).\n  • Expand the analysis referenced in the footnote (Section 3.3) by enumerating additional activation mappings, e.g., per-layer/position histograms or feature importance, and test whether these improve selection beyond Jaccard or MinAct.- Provide fuller experimental detail to enhance reproducibility\n  • Report selector hyperparameters and tuning: k values for kNN; eps/minPts for DBSCAN; tie-breaking rules; and sensitivity curves showing accuracy vs. these parameters (Section 4.1; Appendix A).\n  • Justify the choice of B=32 with cross-dataset sweeps and selection of a principled operating point; where Figure 8 shows non-monotonicity, include a selection policy (e.g., choose B that optimizes validation accuracy/efficiency trade-off) and report it consistently (Sections 4.2, 5.3).\n  • Vary the threshold top-k=500 (Appendix B) and report its effect on correlations (Figure 2) and accuracy (Tables 1–3), to confirm that key findings are robust to this parameter.- Address potential confounds and refine measurement choices\n  • Control for output length and decoding settings: report correlations conditioned on fixed sequence lengths, and include ablations over temperature/top-p to rule out sampling-induced activation inflation (Section 3.3; Figure 4(c)).\n  • Explore alternative aggregation schemes beyond union (Equation (3)), such as per-layer weighting, recency-weighted counts, or activation magnitude thresholds, and compare their separability (extension of Figure 7 analysis).\n  • Evaluate weighted similarity metrics (e.g., weighted Jaccard or cosine over contribution vectors) versus binary Jaccard, and report whether selectors improve with magnitude-aware measures (Section 4.1).- Broaden scope across models and tasks\n  • Add evaluations on larger and architecturally different models (e.g., varying FFN architectures or activation functions) to test generality of Equation (1)-based definitions and NAD selectors (Section 5.1).\n  • Include additional open-ended tasks (e.g., multi-hop QA, long-form reasoning) to assess whether neuron agreement holds beyond math/science and coding (Section 5.1).\n  • Where FFN variants differ, adapt the activation contribution computation and report whether the fewer-neurons-for-correctness pattern persists (Section 3.1).- Expand baseline coverage and ensure fair comparability\n  • Implement outputs-based baselines from Related Work (entropy-based pruning, self-certainty scoring) and compare head-to-head with NAD under the same n=64, B, and single-execution protocols (Section 2; Tables 1–3).\n  • For code, complement single-execution with a limited multi-execution regime and clear validator constraints, to situate NAD’s gains relative to realistic deployment policies (Section 5.1; Appendix Table 3).\n  • Temper claims by highlighting where NAD is competitive but not superior to majority voting (e.g., AIME24+25 in Table 1) and articulate conditions where NAD is preferred (clarifying scope of “matches”).- Quantify practical overhead and end-to-end efficiency\n  • Report runtime profiles: activation extraction time per token/chunk, Jaccard matrix build cost (O(n^2)), selector time, and total latency, alongside token savings (Section 4.1; Table 2; Appendix Table 3).\n  • Provide memory usage for activation storage per sample and total, and evaluate compression (bitsets/bitmaps) empirically to substantiate Appendix A (practical feasibility).\n  • Compare end-to-end throughput under early stopping vs. parallel sampling to show real-world efficiency beyond token counts (Section 5.2; Figure 8), ensuring operational relevance.Score\n- Overall (10): 7 — Novel internal-signal decoding and strong efficiency gains with early stopping (Figure 1; Sections 4.1–4.2; Table 2) balanced by limited baselines and theory (Section 3.2; Table 1).\n- Novelty (10): 7 — Leveraging neuron agreement for ensemble selection and early correctness prediction is a distinct approach (Section 4.1; Figure 5; Figure 3).\n- Technical Quality (10): 6 — Solid correlational and empirical evidence (Figure 2; Figure 4; Tables 1–3) but lacks formal analysis and broader baselines; some hyperparameter details are missing (Section 3.2; Appendix B; Appendix A).\n- Clarity (10): 7 — Clear definitions and diagrams (Equations (1–3); Figure 5; Appendix B) with comprehensive tables, though some selector/parameter specifics are underreported (Section 4.1; Section 5.1).\n- Confidence (5): 4 — Review based on the full manuscript’s methods/experiments; conclusions rely on provided figures/tables, with some missing implementation details acknowledged (Appendix A; Section 4.1).",
  "final_review": "Summary\nThe paper studies whether internal signals in LLMs—specifically neuron activation patterns—can better indicate response correctness than external outputs (log-probs, entropy, self-evaluation). It reports that (i) external confidence metrics correlate with and are low-dimensional projections of internal activation states (Figure 2; Section 3.2), (ii) correct responses activate fewer unique neurons and show stronger cross-sample agreement (Figure 4(b,c); Section 3.3), and (iii) leveraging these properties enables a new unsupervised ensemble method, Neuron-Agreement Decoding (NAD), based on Jaccard similarity over activated-neuron sets, with early stopping at 32 tokens (Sections 4.1–4.2). Across multiple reasoning and coding benchmarks, NAD variants match or approach majority voting and outperform Avg@64 under fixed sampling (Table 1), while early stopping reduces tokens by up to ~98.8% with minimal accuracy loss (Table 2; Appendix Table 3; Figure 8).Strengths\n- Bold internal-signal-based ensemble formulation\n  • NAD builds selection on pairwise Jaccard similarities of activated-neuron sets and medoid/kNN/DBSCAN variants (Section 4.1; Figure 5), proposing a clear alternative to output-based confidence methods; this is novel and potentially impactful for open-ended tasks where textual voting is hard (Section 2; Section 4.1).\n  • The MinAct strategy operationalizes Insight 2 directly by selecting the sample with fewest activated neurons (Section 4.1), offering a lightweight, unsupervised selector; this increases methodological diversity (novelty/efficiency).\n  • The framework relies solely on internal signals and does not require comparable textual outputs (Figure 1; Section 4.1), important for settings where majority voting is inapplicable (impact).\n- Empirical evidence linking internal activations to external confidence\n  • Neuron count correlates negatively with self-certainty and positively with entropy (Pearson r ≈ −0.605/+0.633; Spearman r ≈ −0.631/+0.660) in Qwen3-4B-Think on AIME24 (Figure 2; Section 3.2), supporting the claim that external metrics are low-dimensional projections (technical soundness).\n  • t-SNE over Jaccard similarities reveals clusters of activation patterns that do not collapse onto a single entropy value (Figure 3; Section 3.2), indicating richer internal structure than scalar metrics capture (novelty/clarity).\n  • Visualizations place incorrect responses at cluster margins (Figure 4(a); Section 3.3), consistent with the consensus hypothesis (technical plausibility).\n- Discoveries about correctness and neuron usage\n  • Correct samples activate fewer unique neurons than incorrect ones (Figure 4(b); Section 3.3), a core empirical finding that directly motivates selectors and early pruning (novelty/impact).\n  • Token-wise trajectories show incorrect responses engage more neurons and shift strategies repeatedly (Figure 4(c); Section 3.3), supporting early-stage discrimination (technical soundness).\n  • Density plots further illustrate separability between correct/incorrect activation counts across tasks (Conclusion Figures (a–c)), strengthening generality claims within the evaluated scope (experimental rigor).\n- Early stopping yields large efficiency gains with competitive accuracy\n  • Early pruning at B=32 reduces token usage by ~97.6–98.8% across scientific benchmarks (Table 2; Section 5.2) and by ~93.3–98.4% across code benchmarks (Appendix Table 3), demonstrating practical efficiency (impact).\n  • Accuracy remains competitive or improved versus Avg@64 under early stopping (Table 2; Appendix Table 3), validating usefulness of internal signals for early correctness prediction (experimental rigor).\n  • Accuracy vs. stopping position analysis shows non-monotonic behavior with strong performance at small B (Figure 8; Appendix Figures 9–10), informing practical truncation choices (clarity/engineering guidance).\n- Broad empirical evaluation across tasks and models\n  • Tests on math/science (AIME24/25, GPQA) and code (HumanEval, MBPP, LiveCodeBench v5) with three models (Qwen3-4B-Think, Qwen3-4B-Instruct, R1-Qwen3-8B) (Section 5.1; Table 1) show NAD variants often match or exceed baselines (experimental breadth).\n  • NAD-kNN often provides the strongest performance, e.g., Qwen3-4B-Think: 79.9 avg vs. 76.7 Avg@64 (Table 1), evidencing practical selection effectiveness (impact).\n  • Results on code, where majority voting is inapplicable, show consistent improvements over Avg@64 on most tasks (Table 1; Section 5.2), highlighting applicability (experimental rigor).\n- Clear methodological exposition and anchoring\n  • The activated-neuron contribution is defined precisely via SwiGLU FFN and unembedding (Equation (1); Section 3.1), aiding reproducibility (clarity/technical soundness).\n  • Activated-neuron set over chunks is defined (Equations (2–3); Section 3.1), explaining local aggregation (clarity).\n  • Threshold implementation with token-level top-k aggregation is given (Appendix B; Equations (5–6)), enabling replication of activation extraction (experimental rigor).\n- Ablations and analysis strengthen understanding\n  • Top-k aggregation ablation shows best separation with no global top-k across tokens (Figure 7; Section 5.3), clarifying design trade-offs (technical soundness).\n  • Early stopping position sweeps (Figure 8; Appendix Figures 9–10) detail the robustness and trade-off between signal quality and noise accumulation (experimental rigor).\n  • Comparison of MinAct vs MaxActivation across domains (Figure 6; Section 5.3) probes domain-specific behavior of activation heuristics (insight/clarity).\n- Practicality considerations discussed\n  • Limitations note selector sensitivity and storage overhead; propose bitset/bitmap and parallel Jaccard under early stopping to manage memory (Appendix A), reflecting engineering awareness (clarity/practicality).Weaknesses\n- Limited theoretical grounding and causal claims\n  • The “low-dimensional projection” assertion is presented as a hypothesis (Section 3.2: “We hypothesize...”), with correlational evidence (Figure 2) but no formal derivation mapping activations to entropy/self-certainty (technical rigor).\n  • The t-SNE clustering in Figure 3 is qualitative and can be sensitive to perplexity/initialization; no robustness checks or statistical cluster validity tests are reported (experimental rigor).\n  • Footnote in Section 3.3 acknowledges inability to exhaust mappings and defers exploration (“We cannot exhaust...”), leaving the central premise under-theorized (novelty grounding).\n  • The claim of “p-value less than 0.05” for correlations (Section 3.2; Figure 2) lacks reporting of sample sizes, exact p-values, or test procedures; No direct evidence found in the manuscript (verifiability).\n  • The Introduction references GPT-4 calibration issues post-training (Section 1) without a corresponding citation in the references; No direct evidence found in the manuscript (clarity/grounding).\n- Incomplete experimental detail and potential reproducibility gaps\n  • The kNN, DBSCAN, and medoid selectors lack specified hyperparameters (e.g., k for kNN; eps/minPts for DBSCAN) and tuning procedures (Section 4.1; Appendix A acknowledges “did not determine which is most effective...”), impeding replication (clarity).\n  • Early stopping is fixed at B=32 in main experiments without reporting how B was chosen beyond Figure 8’s sweep; many positions show non-monotonic behavior (Section 4.2; Section 5.3; Figure 8), suggesting sensitivity (experimental rigor).\n  • The threshold top-k value is fixed at 500 (Appendix B), but the impact of varying this key hyperparameter on correlations (Figure 2) and accuracy (Table 1–3) is not reported (technical completeness).\n- Potential confounds and measurement choices\n  • Although Figure 4(c) aligns trajectories by token to argue non-length effects, broader controls (e.g., normalizing by decoded steps, sampling temperature, top-p across tasks/models) are not provided (Section 3.3), leaving residual confounds (technical rigor).\n  • Activation counting depends on chunking and union over chunks (Equation (3); Section 3.1); alternative aggregation schemes (e.g., weighted by token position or per-layer normalization) are not explored beyond top-k ablation (Figure 7), possibly biasing counts (experimental completeness).\n  • The choice of using Jaccard over binary activated sets ignores contribution magnitudes; similarity may change if weighted Jaccard or continuous overlap is used (Section 4.1), affecting NAD’s decisions (methodological robustness).\n- Scope limitations across models and tasks\n  • Only three models are evaluated (Qwen3-4B-Think/-Instruct, R1-Qwen3-8B) (Section 5.1); larger or structurally different architectures are absent, limiting generality (experimental breadth).\n  • The science/math datasets emphasize extractable answers; while coding is included, other open-ended domains (e.g., long-form reasoning, multi-hop QA) are not evaluated (Section 5.1), narrowing applicability (impact).\n  • The activated-neuron computation relies on SwiGLU-based FFN formulation (Equation (1); Section 3.1); generalization to other activation and FFN variants is not demonstrated (technical generality).\n- Baseline coverage and comparability\n  • Baselines are Avg@64 and Cons@64; contemporary probability/entropy-based pruning/selection are discussed in Related Work (Section 2) but not included in empirical comparisons, leaving unanswered whether NAD is better than outputs-based signals (experimental rigor).\n  • For code, the “curated baseline” and single-execution protocol (Section 5.1) may understate what stronger multi-execution or validator-based settings achieve; comparability to practical deployment regimes is unclear (clarity/impact).\n  • Claims that NAD “matches majority voting” are task-dependent; in Table 1, NAD-kNN is competitive but not consistently superior to Cons@64 (e.g., AIME24+25: 85.0 vs. 86.7 for Qwen3-4B-Think), so phrasing should be carefully bounded (clarity).\n- Practical overhead and scalability not fully quantified\n  • Pairwise Jaccard requires O(n^2) similarity computations; the paper does not report wall-clock, memory footprint, or throughput impact under n=64 (Section 4.1; Appendix A), leaving efficiency claims incomplete (engineering rigor).\n  • Storage overhead for activation bitsets is acknowledged as non-trivial (Appendix A) but not quantified across models/datasets (memory per sample or per chunk), making operational feasibility uncertain (practicality).\n  • Early stopping reduces tokens dramatically (Table 2; Appendix Table 3), yet total end-to-end latency including activation extraction and selection is not reported, which matters in real-time systems (impact/clarity).\n- Cross-table numerical inconsistencies\n  • Several accuracies differ between Table 1 and Appendix Table 3 for the same settings (e.g., Qwen3‑4B‑Think HumanEval Avg@64: 96.0 vs. 97.0; Table 1 vs. Appendix Table 3), undermining consistency (reproducibility).\n  • R1‑Qwen3‑8B NAD‑kNN HumanEval is reported as 89.6 in Table 1 but 94.5 in Appendix Table 3 (Section 5.1; Appendix C), affecting comparative claims (validity).\n  • Qwen3‑4B‑Instruct LCBv5 Avg@64 appears as 34.1 in Table 1 and 31.1 in Appendix Table 3 (Section 5.1; Appendix C), suggesting reporting mismatches (clarity).Suggestions for Improvement\n- Strengthen theoretical grounding and validate causality\n  • Provide a formal derivation or approximation linking activation statistics to entropy/self-certainty (e.g., via linearization of the unembedding layer), and test predictive models that reconstruct scalar metrics from activation features, reporting reconstruction error (Section 3.2; Figure 2).\n  • Include robustness analyses of clustering (Figure 3): vary t-SNE parameters, compare with UMAP/PCA, and compute quantitative cluster validity indices (e.g., silhouette) over the Jaccard matrix to reduce reliance on qualitative plots (Section 3.2).\n  • Expand the analysis referenced in the footnote (Section 3.3) by enumerating additional activation mappings, e.g., per-layer/position histograms or feature importance, and test whether these improve selection beyond Jaccard or MinAct.\n  • Report correlation-testing details: sample sizes, exact p-values, hypothesis tests, and confidence intervals for the Pearson/Spearman statistics in Figure 2 and Section 3.2 to substantiate the “p < 0.05” claim (verifiability).\n  • Provide a proper citation for the GPT-4 calibration assertion in Section 1 or remove/soften the claim to align strictly with referenced evidence (clarity).\n- Provide fuller experimental detail to enhance reproducibility\n  • Report selector hyperparameters and tuning: k values for kNN; eps/minPts for DBSCAN; tie-breaking rules; and sensitivity curves showing accuracy vs. these parameters (Section 4.1; Appendix A).\n  • Justify the choice of B=32 with cross-dataset sweeps and selection of a principled operating point; where Figure 8 shows non-monotonicity, include a selection policy (e.g., choose B that optimizes validation accuracy/efficiency trade-off) and report it consistently (Sections 4.2, 5.3).\n  • Vary the threshold top-k=500 (Appendix B) and report its effect on correlations (Figure 2) and accuracy (Tables 1–3), to confirm that key findings are robust to this parameter.\n- Address potential confounds and refine measurement choices\n  • Control for output length and decoding settings: report correlations conditioned on fixed sequence lengths, and include ablations over temperature/top-p to rule out sampling-induced activation inflation (Section 3.3; Figure 4(c)).\n  • Explore alternative aggregation schemes beyond union (Equation (3)), such as per-layer weighting, recency-weighted counts, or activation magnitude thresholds, and compare their separability (extension of Figure 7 analysis).\n  • Evaluate weighted similarity metrics (e.g., weighted Jaccard or cosine over contribution vectors) versus binary Jaccard, and report whether selectors improve with magnitude-aware measures (Section 4.1).\n- Broaden scope across models and tasks\n  • Add evaluations on larger and architecturally different models (e.g., varying FFN architectures or activation functions) to test generality of Equation (1)-based definitions and NAD selectors (Section 5.1).\n  • Include additional open-ended tasks (e.g., multi-hop QA, long-form reasoning) to assess whether neuron agreement holds beyond math/science and coding (Section 5.1).\n  • Where FFN variants differ, adapt the activation contribution computation and report whether the fewer-neurons-for-correctness pattern persists (Section 3.1).\n- Expand baseline coverage and ensure fair comparability\n  • Implement outputs-based baselines from Related Work (entropy-based pruning, self-certainty scoring) and compare head-to-head with NAD under the same n=64, B, and single-execution protocols (Section 2; Tables 1–3).\n  • For code, complement single-execution with a limited multi-execution regime and clear validator constraints, to situate NAD’s gains relative to realistic deployment policies (Section 5.1; Appendix Table 3).\n  • Temper claims by highlighting where NAD is competitive but not superior to majority voting (e.g., AIME24+25 in Table 1) and articulate conditions where NAD is preferred (clarifying scope of “matches”).\n- Quantify practical overhead and end-to-end efficiency\n  • Report runtime profiles: activation extraction time per token/chunk, Jaccard matrix build cost (O(n^2)), selector time, and total latency, alongside token savings (Section 4.1; Table 2; Appendix Table 3).\n  • Provide memory usage for activation storage per sample and total, and evaluate compression (bitsets/bitmaps) empirically to substantiate Appendix A (practical feasibility).\n  • Compare end-to-end throughput under early stopping vs. parallel sampling to show real-world efficiency beyond token counts (Section 5.2; Figure 8), ensuring operational relevance.\n- Audit and reconcile cross-table results\n  • Reconcile Qwen3‑4B‑Think HumanEval Avg@64 (Table 1: 96.0 vs. Appendix Table 3: 97.0) by clarifying protocols and correcting one source; include seeds and confidence intervals (Section 5.1; Appendix C).\n  • Reconcile R1‑Qwen3‑8B NAD‑kNN HumanEval (Table 1: 89.6 vs. Appendix Table 3: 94.5) and explain any differences (e.g., stop positions or evaluation scripts) with unified reporting (Section 5.1; Appendix C).\n  • Reconcile Qwen3‑4B‑Instruct LCBv5 Avg@64 (Table 1: 34.1 vs. Appendix Table 3: 31.1) and ensure all main-text claims reflect the corrected numbers (Section 5.1; Appendix C).Score\n- Overall (10): 7 — Novel internal-signal decoding and strong efficiency gains with early stopping (Figure 1; Sections 4.1–4.2; Table 2) balanced by limited baselines and theory, plus some cross-table inconsistencies (Table 1 vs. Appendix Table 3).\n- Novelty (10): 7 — Leveraging neuron agreement for ensemble selection and early correctness prediction is a distinct approach (Section 4.1; Figure 5; Figure 3).\n- Technical Quality (10): 5 — Solid correlational and empirical evidence (Figure 2; Figure 4; Tables 1–3) but lacks formal analysis and broader baselines; missing statistical test details and inconsistencies between tables reduce rigor (Section 3.2; Appendix B; Table 1 vs. Appendix Table 3).\n- Clarity (10): 6 — Clear definitions and diagrams (Equations (1–3); Figure 5; Appendix B) with comprehensive tables, though some selector/parameter specifics are underreported and numbers conflict across tables (Section 4.1; Section 5.1; Appendix C).\n- Confidence (5): 4 — Review based on the full manuscript’s methods/experiments; conclusions rely on provided figures/tables, with some missing implementation and reporting details acknowledged (Appendix A; Section 4.1; Table 1 vs. Appendix Table 3).",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 7,
        "novelty": 7,
        "technical_quality": 6,
        "clarity": 7,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 7,
        "novelty": 7,
        "technical_quality": 5,
        "clarity": 6,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "Summary\nThe paper studies whether internal signals in LLMs—specifically neuron activation patterns—can better indicate response correctness than external outputs (log-probs, entropy, self-evaluation). It reports that (i) external confidence metrics correlate with and are low-dimensional projections of internal activation states (Figure 2; Section 3.2), (ii) correct responses activate fewer unique neurons and show stronger cross-sample agreement (Figure 4(b,c); Section 3.3), and (iii) leveraging these properties enables a new unsupervised ensemble method, Neuron-Agreement Decoding (NAD), based on Jaccard similarity over activated-neuron sets, with early stopping at 32 tokens (Sections 4.1–4.2). Across multiple reasoning and coding benchmarks, NAD variants match or approach majority voting and outperform Avg@64 under fixed sampling (Table 1), while early stopping reduces tokens by up to ~98.8% with minimal accuracy loss (Table 2; Appendix Table 3; Figure 8).Strengths\n- Bold internal-signal-based ensemble formulation\n  • NAD builds selection on pairwise Jaccard similarities of activated-neuron sets and medoid/kNN/DBSCAN variants (Section 4.1; Figure 5), proposing a clear alternative to output-based confidence methods; this is novel and potentially impactful for open-ended tasks where textual voting is hard (Section 2; Section 4.1).\n  • The MinAct strategy operationalizes Insight 2 directly by selecting the sample with fewest activated neurons (Section 4.1), offering a lightweight, unsupervised selector; this increases methodological diversity (novelty/efficiency).\n  • The framework relies solely on internal signals and does not require comparable textual outputs (Figure 1; Section 4.1), important for settings where majority voting is inapplicable (impact).\n- Empirical evidence linking internal activations to external confidence\n  • Neuron count correlates negatively with self-certainty and positively with entropy (Pearson r ≈ −0.605/+0.633; Spearman r ≈ −0.631/+0.660) in Qwen3-4B-Think on AIME24 (Figure 2; Section 3.2), supporting the claim that external metrics are low-dimensional projections (technical soundness).\n  • t-SNE over Jaccard similarities reveals clusters of activation patterns that do not collapse onto a single entropy value (Figure 3; Section 3.2), indicating richer internal structure than scalar metrics capture (novelty/clarity).\n  • Visualizations place incorrect responses at cluster margins (Figure 4(a); Section 3.3), consistent with the consensus hypothesis (technical plausibility).\n- Discoveries about correctness and neuron usage\n  • Correct samples activate fewer unique neurons than incorrect ones (Figure 4(b); Section 3.3), a core empirical finding that directly motivates selectors and early pruning (novelty/impact).\n  • Token-wise trajectories show incorrect responses engage more neurons and shift strategies repeatedly (Figure 4(c); Section 3.3), supporting early-stage discrimination (technical soundness).\n  • Density plots further illustrate separability between correct/incorrect activation counts across tasks (Conclusion Figures (a–c)), strengthening generality claims within the evaluated scope (experimental rigor).\n- Early stopping yields large efficiency gains with competitive accuracy\n  • Early pruning at B=32 reduces token usage by ~97.6–98.8% across scientific benchmarks (Table 2; Section 5.2) and by ~93.3–98.4% across code benchmarks (Appendix Table 3), demonstrating practical efficiency (impact).\n  • Accuracy remains competitive or improved versus Avg@64 under early stopping (Table 2; Appendix Table 3), validating usefulness of internal signals for early correctness prediction (experimental rigor).\n  • Accuracy vs. stopping position analysis shows non-monotonic behavior with strong performance at small B (Figure 8; Appendix Figures 9–10), informing practical truncation choices (clarity/engineering guidance).\n- Broad empirical evaluation across tasks and models\n  • Tests on math/science (AIME24/25, GPQA) and code (HumanEval, MBPP, LiveCodeBench v5) with three models (Qwen3-4B-Think, Qwen3-4B-Instruct, R1-Qwen3-8B) (Section 5.1; Table 1) show NAD variants often match or exceed baselines (experimental breadth).\n  • NAD-kNN often provides the strongest performance, e.g., Qwen3-4B-Think: 79.9 avg vs. 76.7 Avg@64 (Table 1), evidencing practical selection effectiveness (impact).\n  • Results on code, where majority voting is inapplicable, show consistent improvements over Avg@64 on most tasks (Table 1; Section 5.2), highlighting applicability (experimental rigor).\n- Clear methodological exposition and anchoring\n  • The activated-neuron contribution is defined precisely via SwiGLU FFN and unembedding (Equation (1); Section 3.1), aiding reproducibility (clarity/technical soundness).\n  • Activated-neuron set over chunks is defined (Equations (2–3); Section 3.1), explaining local aggregation (clarity).\n  • Threshold implementation with token-level top-k aggregation is given (Appendix B; Equations (5–6)), enabling replication of activation extraction (experimental rigor).\n- Ablations and analysis strengthen understanding\n  • Top-k aggregation ablation shows best separation with no global top-k across tokens (Figure 7; Section 5.3), clarifying design trade-offs (technical soundness).\n  • Early stopping position sweeps (Figure 8; Appendix Figures 9–10) detail the robustness and trade-off between signal quality and noise accumulation (experimental rigor).\n  • Comparison of MinAct vs MaxActivation across domains (Figure 6; Section 5.3) probes domain-specific behavior of activation heuristics (insight/clarity).\n- Practicality considerations discussed\n  • Limitations note selector sensitivity and storage overhead; propose bitset/bitmap and parallel Jaccard under early stopping to manage memory (Appendix A), reflecting engineering awareness (clarity/practicality).Weaknesses\n- Limited theoretical grounding and causal claims\n  • The “low-dimensional projection” assertion is presented as a hypothesis (Section 3.2: “We hypothesize...”), with correlational evidence (Figure 2) but no formal derivation mapping activations to entropy/self-certainty (technical rigor).\n  • The t-SNE clustering in Figure 3 is qualitative and can be sensitive to perplexity/initialization; no robustness checks or statistical cluster validity tests are reported (experimental rigor).\n  • Footnote in Section 3.3 acknowledges inability to exhaust mappings and defers exploration (“We cannot exhaust...”), leaving the central premise under-theorized (novelty grounding).\n  • The claim of “p-value less than 0.05” for correlations (Section 3.2; Figure 2) lacks reporting of sample sizes, exact p-values, or test procedures; No direct evidence found in the manuscript (verifiability).\n  • The Introduction references GPT-4 calibration issues post-training (Section 1) without a corresponding citation in the references; No direct evidence found in the manuscript (clarity/grounding).\n- Incomplete experimental detail and potential reproducibility gaps\n  • The kNN, DBSCAN, and medoid selectors lack specified hyperparameters (e.g., k for kNN; eps/minPts for DBSCAN) and tuning procedures (Section 4.1; Appendix A acknowledges “did not determine which is most effective...”), impeding replication (clarity).\n  • Early stopping is fixed at B=32 in main experiments without reporting how B was chosen beyond Figure 8’s sweep; many positions show non-monotonic behavior (Section 4.2; Section 5.3; Figure 8), suggesting sensitivity (experimental rigor).\n  • The threshold top-k value is fixed at 500 (Appendix B), but the impact of varying this key hyperparameter on correlations (Figure 2) and accuracy (Table 1–3) is not reported (technical completeness).\n- Potential confounds and measurement choices\n  • Although Figure 4(c) aligns trajectories by token to argue non-length effects, broader controls (e.g., normalizing by decoded steps, sampling temperature, top-p across tasks/models) are not provided (Section 3.3), leaving residual confounds (technical rigor).\n  • Activation counting depends on chunking and union over chunks (Equation (3); Section 3.1); alternative aggregation schemes (e.g., weighted by token position or per-layer normalization) are not explored beyond top-k ablation (Figure 7), possibly biasing counts (experimental completeness).\n  • The choice of using Jaccard over binary activated sets ignores contribution magnitudes; similarity may change if weighted Jaccard or continuous overlap is used (Section 4.1), affecting NAD’s decisions (methodological robustness).\n- Scope limitations across models and tasks\n  • Only three models are evaluated (Qwen3-4B-Think/-Instruct, R1-Qwen3-8B) (Section 5.1); larger or structurally different architectures are absent, limiting generality (experimental breadth).\n  • The science/math datasets emphasize extractable answers; while coding is included, other open-ended domains (e.g., long-form reasoning, multi-hop QA) are not evaluated (Section 5.1), narrowing applicability (impact).\n  • The activated-neuron computation relies on SwiGLU-based FFN formulation (Equation (1); Section 3.1); generalization to other activation and FFN variants is not demonstrated (technical generality).\n- Baseline coverage and comparability\n  • Baselines are Avg@64 and Cons@64; contemporary probability/entropy-based pruning/selection are discussed in Related Work (Section 2) but not included in empirical comparisons, leaving unanswered whether NAD is better than outputs-based signals (experimental rigor).\n  • For code, the “curated baseline” and single-execution protocol (Section 5.1) may understate what stronger multi-execution or validator-based settings achieve; comparability to practical deployment regimes is unclear (clarity/impact).\n  • Claims that NAD “matches majority voting” are task-dependent; in Table 1, NAD-kNN is competitive but not consistently superior to Cons@64 (e.g., AIME24+25: 85.0 vs. 86.7 for Qwen3-4B-Think), so phrasing should be carefully bounded (clarity).\n- Practical overhead and scalability not fully quantified\n  • Pairwise Jaccard requires O(n^2) similarity computations; the paper does not report wall-clock, memory footprint, or throughput impact under n=64 (Section 4.1; Appendix A), leaving efficiency claims incomplete (engineering rigor).\n  • Storage overhead for activation bitsets is acknowledged as non-trivial (Appendix A) but not quantified across models/datasets (memory per sample or per chunk), making operational feasibility uncertain (practicality).\n  • Early stopping reduces tokens dramatically (Table 2; Appendix Table 3), yet total end-to-end latency including activation extraction and selection is not reported, which matters in real-time systems (impact/clarity).\n- Cross-table numerical inconsistencies\n  • Several accuracies differ between Table 1 and Appendix Table 3 for the same settings (e.g., Qwen3‑4B‑Think HumanEval Avg@64: 96.0 vs. 97.0; Table 1 vs. Appendix Table 3), undermining consistency (reproducibility).\n  • R1‑Qwen3‑8B NAD‑kNN HumanEval is reported as 89.6 in Table 1 but 94.5 in Appendix Table 3 (Section 5.1; Appendix C), affecting comparative claims (validity).\n  • Qwen3‑4B‑Instruct LCBv5 Avg@64 appears as 34.1 in Table 1 and 31.1 in Appendix Table 3 (Section 5.1; Appendix C), suggesting reporting mismatches (clarity).Suggestions for Improvement\n- Strengthen theoretical grounding and validate causality\n  • Provide a formal derivation or approximation linking activation statistics to entropy/self-certainty (e.g., via linearization of the unembedding layer), and test predictive models that reconstruct scalar metrics from activation features, reporting reconstruction error (Section 3.2; Figure 2).\n  • Include robustness analyses of clustering (Figure 3): vary t-SNE parameters, compare with UMAP/PCA, and compute quantitative cluster validity indices (e.g., silhouette) over the Jaccard matrix to reduce reliance on qualitative plots (Section 3.2).\n  • Expand the analysis referenced in the footnote (Section 3.3) by enumerating additional activation mappings, e.g., per-layer/position histograms or feature importance, and test whether these improve selection beyond Jaccard or MinAct.\n  • Report correlation-testing details: sample sizes, exact p-values, hypothesis tests, and confidence intervals for the Pearson/Spearman statistics in Figure 2 and Section 3.2 to substantiate the “p < 0.05” claim (verifiability).\n  • Provide a proper citation for the GPT-4 calibration assertion in Section 1 or remove/soften the claim to align strictly with referenced evidence (clarity).\n- Provide fuller experimental detail to enhance reproducibility\n  • Report selector hyperparameters and tuning: k values for kNN; eps/minPts for DBSCAN; tie-breaking rules; and sensitivity curves showing accuracy vs. these parameters (Section 4.1; Appendix A).\n  • Justify the choice of B=32 with cross-dataset sweeps and selection of a principled operating point; where Figure 8 shows non-monotonicity, include a selection policy (e.g., choose B that optimizes validation accuracy/efficiency trade-off) and report it consistently (Sections 4.2, 5.3).\n  • Vary the threshold top-k=500 (Appendix B) and report its effect on correlations (Figure 2) and accuracy (Tables 1–3), to confirm that key findings are robust to this parameter.\n- Address potential confounds and refine measurement choices\n  • Control for output length and decoding settings: report correlations conditioned on fixed sequence lengths, and include ablations over temperature/top-p to rule out sampling-induced activation inflation (Section 3.3; Figure 4(c)).\n  • Explore alternative aggregation schemes beyond union (Equation (3)), such as per-layer weighting, recency-weighted counts, or activation magnitude thresholds, and compare their separability (extension of Figure 7 analysis).\n  • Evaluate weighted similarity metrics (e.g., weighted Jaccard or cosine over contribution vectors) versus binary Jaccard, and report whether selectors improve with magnitude-aware measures (Section 4.1).\n- Broaden scope across models and tasks\n  • Add evaluations on larger and architecturally different models (e.g., varying FFN architectures or activation functions) to test generality of Equation (1)-based definitions and NAD selectors (Section 5.1).\n  • Include additional open-ended tasks (e.g., multi-hop QA, long-form reasoning) to assess whether neuron agreement holds beyond math/science and coding (Section 5.1).\n  • Where FFN variants differ, adapt the activation contribution computation and report whether the fewer-neurons-for-correctness pattern persists (Section 3.1).\n- Expand baseline coverage and ensure fair comparability\n  • Implement outputs-based baselines from Related Work (entropy-based pruning, self-certainty scoring) and compare head-to-head with NAD under the same n=64, B, and single-execution protocols (Section 2; Tables 1–3).\n  • For code, complement single-execution with a limited multi-execution regime and clear validator constraints, to situate NAD’s gains relative to realistic deployment policies (Section 5.1; Appendix Table 3).\n  • Temper claims by highlighting where NAD is competitive but not superior to majority voting (e.g., AIME24+25 in Table 1) and articulate conditions where NAD is preferred (clarifying scope of “matches”).\n- Quantify practical overhead and end-to-end efficiency\n  • Report runtime profiles: activation extraction time per token/chunk, Jaccard matrix build cost (O(n^2)), selector time, and total latency, alongside token savings (Section 4.1; Table 2; Appendix Table 3).\n  • Provide memory usage for activation storage per sample and total, and evaluate compression (bitsets/bitmaps) empirically to substantiate Appendix A (practical feasibility).\n  • Compare end-to-end throughput under early stopping vs. parallel sampling to show real-world efficiency beyond token counts (Section 5.2; Figure 8), ensuring operational relevance.\n- Audit and reconcile cross-table results\n  • Reconcile Qwen3‑4B‑Think HumanEval Avg@64 (Table 1: 96.0 vs. Appendix Table 3: 97.0) by clarifying protocols and correcting one source; include seeds and confidence intervals (Section 5.1; Appendix C).\n  • Reconcile R1‑Qwen3‑8B NAD‑kNN HumanEval (Table 1: 89.6 vs. Appendix Table 3: 94.5) and explain any differences (e.g., stop positions or evaluation scripts) with unified reporting (Section 5.1; Appendix C).\n  • Reconcile Qwen3‑4B‑Instruct LCBv5 Avg@64 (Table 1: 34.1 vs. Appendix Table 3: 31.1) and ensure all main-text claims reflect the corrected numbers (Section 5.1; Appendix C).Score\n- Overall (10): 7 — Novel internal-signal decoding and strong efficiency gains with early stopping (Figure 1; Sections 4.1–4.2; Table 2) balanced by limited baselines and theory, plus some cross-table inconsistencies (Table 1 vs. Appendix Table 3).\n- Novelty (10): 7 — Leveraging neuron agreement for ensemble selection and early correctness prediction is a distinct approach (Section 4.1; Figure 5; Figure 3).\n- Technical Quality (10): 5 — Solid correlational and empirical evidence (Figure 2; Figure 4; Tables 1–3) but lacks formal analysis and broader baselines; missing statistical test details and inconsistencies between tables reduce rigor (Section 3.2; Appendix B; Table 1 vs. Appendix Table 3).\n- Clarity (10): 6 — Clear definitions and diagrams (Equations (1–3); Figure 5; Appendix B) with comprehensive tables, though some selector/parameter specifics are underreported and numbers conflict across tables (Section 4.1; Section 5.1; Appendix C).\n- Confidence (5): 4 — Review based on the full manuscript’s methods/experiments; conclusions rely on provided figures/tables, with some missing implementation and reporting details acknowledged (Appendix A; Section 4.1; Table 1 vs. Appendix Table 3)."
}