1) Summary
The paper investigates whether internal neuron activations in Large Language Models (LLMs) can serve as reliable signals for response quality, as an alternative to external signals like token probabilities which can be poorly calibrated. The authors find that correct responses tend to activate fewer unique neurons and exhibit stronger cross-sample agreement in their activation patterns compared to incorrect responses. Based on these findings, they propose Neuron Agreement Decoding (NAD), an unsupervised best-of-N selection method that scores candidates based on activation sparsity and cross-sample neuron agreement. Experiments on math, science, and code generation benchmarks show that NAD is competitive with majority voting on tasks with canonical answers and outperforms average sampling on open-ended tasks. Furthermore, NAD enables an early stopping strategy that reduces token consumption by up to 99% with minimal impact on performance.2) Strengths
*   **Novel and Well-Motivated Approach:** The core idea of using internal neuron activations for ensemble decoding, rather than relying on potentially miscalibrated external signals, is a significant and novel contribution.
    *   The motivation is clearly established by questioning the reliability of output-based signals like token probabilities after post-training (Section 1, Paragraph 2).
    *   The pilot study in Section 3 provides a strong empirical foundation for the method. It demonstrates a correlation between neuron activation counts and traditional confidence metrics (Figure 2), suggesting internal states are a richer source of information.
    *   The key insights are visually and quantitatively supported by preliminary experiments showing that correct answers form clusters in the activation space and activate fewer neurons (Figure 4, Section 3.3). This provides a solid rationale for the proposed NAD methods.*   **Comprehensive and Rigorous Experimental Evaluation:** The paper validates the proposed method across a diverse set of models, tasks, and datasets, demonstrating the robustness and generalizability of the findings.
    *   The evaluation includes three different models (Qwen3-4B-Think, R1-Qwen3-8B, Qwen3-4B-Instruct), showing the approach is not model-specific (Table 1).
    *   It covers both scientific reasoning tasks with verifiable answers (AIME24+25, GPQA) and open-ended code generation tasks (HumanEval, LCBv5, MBPP), where majority voting is inapplicable. This highlights a key advantage of NAD (Section 5.1, Table 1).
    *   The results consistently show that NAD variants outperform the Avg@64 baseline and perform competitively with or match the strong Cons@64 (majority voting) baseline where applicable (Table 1). For instance, NAD-kNN matches Cons@64 on R1-Qwen3-8B for both math reasoning datasets.*   **Significant Practical Impact via Efficiency Gains:** The proposed early stopping strategy offers a compelling practical benefit, making best-of-N sampling substantially more efficient and affordable.
    *   The experiments demonstrate a massive reduction in token consumption, often around 98-99%, by making a selection after only the first 32 tokens (Table 2, Table 3).
    *   Crucially, this efficiency gain is achieved with minimal loss in generation quality, and in some cases, NAD with early stopping still outperforms the Avg@64 baseline that uses full-length samples (Table 2).
    *   The analysis of performance versus early stopping position (Figure 8, Section 5.3) provides further insight, showing that performance does not always improve with more tokens, suggesting early signals are strong and can be less noisy.*   **Insightful Analyses and Ablations:** The paper goes beyond main results to provide several analyses that deepen the understanding of the method and its underlying principles.
    *   The comparison between selecting for minimal versus maximal neuron activations (Figure 6, Section 5.3) reveals an interesting task-dependent phenomenon, where sparsity is a good heuristic for reasoning but less so for coding. This adds nuance to the paper's claims.
    *   The analysis of the top-k setting for neuron identification (Figure 7, Section 5.3) shows how different filtering levels affect the separability of correct and incorrect responses, justifying the chosen approach.
    *   The paper explores multiple distinct methods for operationalizing neuron agreement (kNN, Medoid, DBSCAN) in addition to the sparsity-based MinAct, providing a thorough exploration of the design space (Section 4.1, Table 1).3) Weaknesses
*   **Ambiguity in the Core Definition of Neuron Activation:** The technical definition of an "activated neuron," which is central to the entire method, is not sufficiently self-contained or intuitively explained in the main paper.
    *   The method relies on a specific formulation for neuron contribution (Equation 1) from a cited work (Cao et al., 2025), which involves complex components like the unembedding matrix (`W_u`). The paper does not provide an intuition for why this specific formulation is appropriate.
    *   The thresholding function `η` is critical for defining the activated set `N_activated` (Equation 2), but its definition and the motivation for its hyperparameters (e.g., `k=500`) are deferred to Appendix B. This makes it difficult for a reader to fully grasp the core mechanism without flipping to the appendix.
    *   The chunking-and-union strategy (Equation 3) is introduced to capture localized information, but its impact compared to a simpler, non-chunked global activation set is not analyzed or ablated.*   **Limited Comparison to Relevant Baselines:** The paper compares NAD primarily against average sampling (Avg@64) and majority voting (Cons@64), but omits direct comparisons to other confidence-based selection methods mentioned in the related work.
    *   The related work section discusses methods like self-certainty (Kang et al., 2025) and DeepConf (Fu et al., 2025), which also perform best-of-N selection using model-derived signals (token probabilities, entropy). A direct comparison, where these methods are used to select the best of the same 64 samples, is missing.
    *   Without this comparison, the central claim that internal signals are superior to external ones is not fully substantiated. It is unclear if NAD's gains over Avg@64 are greater than what could be achieved with these simpler, external-signal-based selectors.
    *   The paper claims that confidence-based approaches "still rely on the availability of comparable answers" (Section 2), which seems to conflate them with voting methods and may not be an accurate limitation for all such techniques.*   **Insufficient Analysis of Computational and Memory Overhead:** The practical implementation of NAD involves non-trivial computational and storage costs, which are only briefly mentioned as a limitation in the appendix.
    *   Storing the activated neuron sets for `n=64` samples, even for a short prefix, can require significant memory, especially for large models. The paper does not quantify this overhead (Appendix A).
    *   The proposed agreement-based methods require computing an `n x n` similarity matrix, which is an O(n^2) operation. The cost of this step, especially as `n` scales, is not discussed or measured against the baselines.
    *   A more complete picture of the method's practicality would include measurements of wall-clock time and peak memory usage required for the selection process, in addition to the reported token savings.*   **Inconsistent Generalizability of the Activation Sparsity Heuristic:** One of the two core insights motivating the method—that correct answers activate fewer neurons (Insight 2)—is shown to not hold universally across tasks.
    *   The analysis in Section 5.3 and Figure 6 explicitly shows that for code generation tasks, selecting for maximal activations can perform as well as or better than selecting for minimal activations. This contradicts the initial premise.
    *   This finding weakens the foundation of the NAD-MinAct variant and suggests that activation sparsity is a domain-specific heuristic rather than a general principle of correctness.
    *   While the authors offer a plausible explanation, the paper's main narrative and contributions section could more clearly frame neuron *agreement* as the primary, more robust signal and sparsity as a secondary, task-dependent one. The results in Table 1, where MinAct is often the weakest NAD variant, support this view.4) Suggestions for Improvement
*   **Clarify and Motivate the Neuron Activation Definition:**
    *   In Section 3.1, add a brief, intuitive explanation for Equation 1, clarifying why the contribution score is defined in this way and the role of the unembedding matrix.
    *   Summarize the thresholding mechanism from Appendix B in the main text of Section 3.1 and provide a brief justification for the choice of `k=500`, perhaps through a sensitivity analysis or by linking it to prior work.
    *   Include a short discussion or ablation on the effect of the chunking strategy (Equation 3) to justify its inclusion over a simpler global approach.*   **Strengthen Baseline Comparisons:**
    *   To directly validate the superiority of internal over external signals, implement and report results for at least one confidence-based selection baseline (e.g., select the sample with the highest average self-certainty or lowest average entropy) on the same set of 64 generated samples used for NAD.
    *   Re-evaluate and clarify the stated limitations of confidence-based methods in Section 2 to ensure the claims are precise and well-supported.*   **Quantify and Discuss Practical Overhead:**
    *   In the experimental section or an appendix, add a table or paragraph quantifying the practical overhead of NAD. This should include metrics like the additional wall-clock time and peak memory usage required for the selection process compared to the baselines.
    *   Discuss the scalability of the O(n^2) similarity matrix computation and provide guidance on its feasibility for `n` values larger than 64.*   **Refine the Narrative Around Activation Sparsity:**
    *   In the introduction and conclusion, more explicitly frame neuron *agreement* as the more generalizable signal for correctness, while positioning activation *sparsity* as a strong but more domain-specific heuristic, citing the evidence from Figure 6.
    *   In the discussion of the main results (Section 5.2), connect the relatively weaker performance of NAD-MinAct in Table 1 to the findings in the analysis section (Section 5.3), providing a more cohesive narrative about the relative strengths of the different NAD variants.5) Score
*   Overall (10): 8 — The paper introduces a highly novel and impactful method for ensemble decoding with strong empirical results, particularly regarding efficiency, though it could be strengthened with clearer technical definitions and more direct baseline comparisons.
*   Novelty (10): 9 — The core idea of using internal neuron activation patterns for best-of-N selection is a fresh and significant departure from existing methods that rely on external signals.
*   Technical Quality (10): 7 — The experimental design is thorough and rigorous, but the technical exposition of the core mechanism is somewhat opaque, and the set of baselines is not comprehensive enough to fully support all claims of superiority.
*   Clarity (10): 7 — The paper is generally well-written and structured, but the central technical concept of "activated neuron" is not explained with sufficient clarity and intuition in the main text, requiring reference to an appendix and prior work.
*   Confidence (5): 5 — I have carefully reviewed the manuscript and am highly confident in my assessment of its strengths and weaknesses.