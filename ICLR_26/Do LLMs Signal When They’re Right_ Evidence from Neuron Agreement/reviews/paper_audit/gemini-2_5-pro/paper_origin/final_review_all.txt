1) Summary
The paper investigates whether internal neuron activations in Large Language Models (LLMs) can serve as reliable signals for response quality, as an alternative to external signals like token probabilities which can be poorly calibrated. The authors find that correct responses tend to activate fewer unique neurons and exhibit stronger cross-sample agreement in their activation patterns compared to incorrect responses. Based on these findings, they propose Neuron Agreement Decoding (NAD), an unsupervised best-of-N selection method that scores candidates based on activation sparsity and cross-sample neuron agreement. Experiments on math, science, and code generation benchmarks show that NAD is competitive with majority voting on tasks with canonical answers and outperforms average sampling on open-ended tasks. Furthermore, NAD enables an early stopping strategy that reduces token consumption by up to 99% with minimal impact on performance.2) Strengths
*   **Novel and Well-Motivated Approach:** The core idea of using internal neuron activations for ensemble decoding, rather than relying on potentially miscalibrated external signals, is a significant and novel contribution.
    *   The motivation is clearly established by questioning the reliability of output-based signals like token probabilities after post-training (Section 1, Paragraph 2).
    *   The pilot study in Section 3 provides a strong empirical foundation for the method. It demonstrates a correlation between neuron activation counts and traditional confidence metrics (Figure 2), suggesting internal states are a richer source of information.
    *   The key insights are visually and quantitatively supported by preliminary experiments showing that correct answers form clusters in the activation space and activate fewer neurons (Figure 4, Section 3.3). This provides a solid rationale for the proposed NAD methods.*   **Comprehensive Experimental Evaluation:** The paper validates the proposed method across a diverse set of models, tasks, and datasets, demonstrating the potential generalizability of the findings.
    *   The evaluation includes three different models (Qwen3-4B-Think, R1-Qwen3-8B, Qwen3-4B-Instruct), showing the approach is not model-specific (Table 1).
    *   It covers both scientific reasoning tasks with verifiable answers (AIME24+25, GPQA) and open-ended code generation tasks (HumanEval, LCBv5, MBPP), where majority voting is inapplicable. This highlights a key advantage of NAD (Section 5.1, Table 1).
    *   The results generally show that NAD variants outperform the Avg@64 baseline and perform competitively with or match the strong Cons@64 (majority voting) baseline where applicable (Table 1). For instance, NAD-kNN matches Cons@64 on R1-Qwen3-8B for both math reasoning datasets.*   **Significant Practical Impact via Efficiency Gains:** The proposed early stopping strategy offers a compelling practical benefit, making best-of-N sampling substantially more efficient and affordable.
    *   The experiments demonstrate a massive reduction in token consumption, often around 98-99%, by making a selection after only the first 32 tokens (Table 2, Table 3).
    *   Crucially, this efficiency gain is achieved with minimal loss in generation quality, and in some cases, NAD with early stopping still outperforms the Avg@64 baseline that uses full-length samples (Table 2).
    *   The analysis of performance versus early stopping position (Figure 8, Section 5.3) provides further insight, showing that performance does not always improve with more tokens, suggesting early signals are strong and can be less noisy.*   **Insightful Analyses and Ablations:** The paper goes beyond main results to provide several analyses that deepen the understanding of the method and its underlying principles.
    *   The comparison between selecting for minimal versus maximal neuron activations (Figure 6, Section 5.3) reveals an interesting task-dependent phenomenon, where sparsity is a good heuristic for reasoning but less so for coding. This adds nuance to the paper's claims.
    *   The analysis of the top-k setting for neuron identification (Figure 7, Section 5.3) shows how different filtering levels affect the separability of correct and incorrect responses, justifying the chosen approach.
    *   The paper explores multiple distinct methods for operationalizing neuron agreement (kNN, Medoid, DBSCAN) in addition to the sparsity-based MinAct, providing a thorough exploration of the design space (Section 4.1, Table 1).3) Weaknesses
*   **Contradictory Definition of Neuron Activation:** The technical definition of an "activated neuron," which is central to the entire method, is presented inconsistently, creating significant ambiguity that harms reproducibility.
    *   The main text in Section 3.1 describes a method based on chunking the sequence and taking the union of activated sets (Equation 3).
    *   However, Appendix B describes a different, more complex token-level top-k thresholding mechanism and explicitly states, "This token-level thresholding is always applied in our main method." These two descriptions appear to be mutually exclusive, making it unclear which method was actually used to produce the paper's results.
    *   The chunking-and-union strategy (Equation 3) is introduced to capture localized information, but its impact compared to a simpler, non-chunked global activation set is not analyzed or ablated.*   **Limited Comparison to Relevant Baselines:** The paper compares NAD primarily against average sampling (Avg@64) and majority voting (Cons@64), but omits direct comparisons to other confidence-based selection methods mentioned in the related work.
    *   The related work section discusses methods like self-certainty (Kang et al., 2025) and DeepConf (Fu et al., 2025), which also perform best-of-N selection using model-derived signals (token probabilities, entropy). A direct comparison, where these methods are used to select the best of the same 64 samples, is missing.
    *   Without this comparison, the central claim that internal signals are superior to external ones is not fully substantiated. It is unclear if NAD's gains over Avg@64 are greater than what could be achieved with these simpler, external-signal-based selectors.
    *   The paper claims that confidence-based approaches "still rely on the availability of comparable answers" (Section 2), which seems to conflate them with voting methods and may not be an accurate limitation for all such techniques.*   **Insufficient Analysis of Computational and Memory Overhead:** The practical implementation of NAD involves non-trivial computational and storage costs, which are only briefly mentioned as a limitation in the appendix.
    *   Storing the activated neuron sets for `n=64` samples, even for a short prefix, can require significant memory, especially for large models. The paper does not quantify this overhead (Appendix A).
    *   The proposed agreement-based methods require computing an `n x n` similarity matrix, which is an O(n^2) operation. The cost of this step, especially as `n` scales, is not discussed or measured against the baselines.
    *   A more complete picture of the method's practicality would include measurements of wall-clock time and peak memory usage required for the selection process, in addition to the reported token savings.*   **Inconsistent Generalizability of the Activation Sparsity Heuristic:** One of the two core insights motivating the method—that correct answers activate fewer neurons (Insight 2)—is shown to not hold universally across tasks.
    *   The analysis in Section 5.3 and Figure 6 explicitly shows that for code generation tasks, selecting for maximal activations can perform as well as or better than selecting for minimal activations. This contradicts the initial premise.
    *   This finding weakens the foundation of the NAD-MinAct variant and suggests that activation sparsity is a domain-specific heuristic rather than a general principle of correctness.
    *   While the authors offer a plausible explanation, the paper's main narrative and contributions section could more clearly frame neuron *agreement* as the primary, more robust signal and sparsity as a secondary, task-dependent one. The results in Table 1, where MinAct is often the weakest NAD variant, support this view.*   **Inconsistencies in Reported Experimental Results:** The manuscript contains several inconsistencies in the reporting of experimental data and figures, which undermines the credibility of the results.
    *   Baseline performance for Avg@64 is reported with different values for the same model and dataset across tables. For Qwen3-4B-Think, Avg@64 on HumanEval is 96.0 in Table 1 but 97.0 in Table 3; on LCBv5 it is 61.9 in Table 1 but 58.7 in Table 3.
    *   Some early stopping results appear implausible. For Qwen3-4B-Think on AIME24+25, NAD-Medoid is reported with an identical accuracy of 81.7% for both full sequences (Table 1) and early stopping at 32 tokens (Table 2), which suggests a potential reporting error.
    *   There are numerous issues with figure numbering and referencing. For example, there appear to be two different diagrams labeled as Figure 5 (a mermaid diagram in Section 4 and a schematic), and a histogram of neuron counts is seemingly used for both Figure 4 and Figure 7 (Section 3.3, Section 5.3).4) Suggestions for Improvement
*   **Clarify and Unify the Neuron Activation Definition:**
    *   Reconcile the conflicting descriptions of the neuron activation definition between Section 3.1 and Appendix B. The main paper must provide a single, unambiguous definition of the exact method used to generate all reported results.
    *   Provide a brief justification for the chosen thresholding mechanism and its key hyperparameters (e.g., `k=500`).
    *   Include a short discussion or ablation on the effect of the chunking strategy (Equation 3) to justify its inclusion over a simpler global approach.*   **Strengthen Baseline Comparisons:**
    *   To directly validate the superiority of internal over external signals, implement and report results for at least one confidence-based selection baseline (e.g., select the sample with the highest average self-certainty or lowest average entropy) on the same set of 64 generated samples used for NAD.
    *   Re-evaluate and clarify the stated limitations of confidence-based methods in Section 2 to ensure the claims are precise and well-supported.*   **Quantify and Discuss Practical Overhead:**
    *   In the experimental section or an appendix, add a table or paragraph quantifying the practical overhead of NAD. This should include metrics like the additional wall-clock time and peak memory usage required for the selection process compared to the baselines.
    *   Discuss the scalability of the O(n^2) similarity matrix computation and provide guidance on its feasibility for `n` values larger than 64.*   **Refine the Narrative Around Activation Sparsity:**
    *   In the introduction and conclusion, more explicitly frame neuron *agreement* as the more generalizable signal for correctness, while positioning activation *sparsity* as a strong but more domain-specific heuristic, citing the evidence from Figure 6.
    *   In the discussion of the main results (Section 5.2), connect the relatively weaker performance of NAD-MinAct in Table 1 to the findings in the analysis section (Section 5.3), providing a more cohesive narrative about the relative strengths of the different NAD variants.*   **Correct and Clarify Experimental Reporting:**
    *   Thoroughly review all tables to ensure that baseline results (e.g., Avg@64) and method results are reported consistently throughout the main paper and appendix. Please correct the discrepancies identified between Table 1 and Table 3.
    *   Verify the early stopping results in Table 2 against the full-sequence results in Table 1 and correct any potential reporting errors, such as the identical scores for NAD-Medoid.
    *   Review all figure numbers, captions, and in-text references to ensure they are consistent and correctly linked. Resolve the duplicate Figure 5 and clarify the usage of the histogram referenced in relation to both Figure 4 and Figure 7.5) Score
*   Overall (10): 6 — The paper presents a novel and promising direction, but its contributions are undermined by a contradictory definition of the core method (Section 3.1 vs. Appendix B) and multiple inconsistencies in the reported experimental data (Table 1 vs. Table 3), which harm reproducibility and credibility.
*   Novelty (10): 9 — The core idea of using internal neuron activation patterns for best-of-N selection is a fresh and significant departure from existing methods that rely on external signals.
*   Technical Quality (10): 4 — The work suffers from a critical contradiction in the core method's definition (Section 3.1 vs. Appendix B) and inconsistent reporting of baseline results (Table 1 vs. Table 3), severely impacting the work's reproducibility and technical soundness.
*   Clarity (10): 5 — The paper is difficult to follow due to a contradictory explanation of the core technical method and numerous errors in figure numbering and referencing (e.g., Figure 5, Figure 4/7).
*   Confidence (5): 5 — I have carefully reviewed the manuscript and am highly confident in my assessment, having cross-referenced claims with the provided tables and figures.