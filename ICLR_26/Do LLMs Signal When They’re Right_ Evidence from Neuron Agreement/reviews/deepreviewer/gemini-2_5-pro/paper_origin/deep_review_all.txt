Here are four distinct reviews of the paper.

***

### **Review 1**

**Summary**
This paper investigates the internal behavior of Large Language Models (LLMs) to predict the correctness of their generated responses. The authors find that external signals like token probabilities are low-dimensional projections of richer internal neuron activation dynamics. They present two key findings: (1) correct responses tend to activate fewer unique neurons, and (2) the activation patterns of correct responses show greater agreement across multiple samples. Based on these insights, they propose Neuron Agreement Decoding (NAD), an unsupervised best-of-N sampling method that selects the best response based on neuron activation sparsity and cross-sample agreement. A key advantage of NAD is its ability to perform this selection very early in the generation process, enabling an aggressive early stopping strategy that dramatically reduces computational cost. Experiments on math, science, and coding benchmarks show that NAD is competitive with majority voting on tasks with canonical answers and outperforms average sampling on open-ended tasks.

**Soundness**
The paper's methodology appears sound. The authors build their work on a clear hypothesis: internal dynamics are more informative than external outputs. They systematically test this by first establishing a correlation between internal activation counts and external confidence metrics (Figure 2), and then showing that activation patterns contain richer information not captured by these metrics (Figure 3). The proposed NAD method logically follows from the two core insights derived from their pilot study (Section 3.3). The experimental setup is robust, using multiple modern LLMs and a diverse set of benchmarks that cover both verifiable reasoning and open-ended generation. The baselines (Avg@64, Cons@64) are appropriate standards for comparison. The results strongly support the claims, particularly the impressive token savings from the early stopping strategy (Table 2).

**Presentation**
The paper is exceptionally well-written and organized. The narrative flows logically from the limitations of existing methods to the investigation of internal signals, the formulation of the NAD method, and finally, a comprehensive evaluation. The figures are a major strength, providing clear and intuitive visualizations of the core concepts. For instance, Figure 4 effectively illustrates the key insights about activation clustering and sparsity, while Figure 5 provides a clear schematic of the overall NAD framework. The tables are well-structured and present the results in a clear, comparative manner. The inclusion of an analysis section (5.3) to discuss nuances like the early stopping position and task-dependent effects further strengthens the paper.

**Contribution**
The contribution of this work is significant and novel. By shifting the focus from external model outputs to internal neuron activations for response selection, the authors open up a new and promising direction for improving LLM reasoning and efficiency. The primary contributions are:
1.  The novel empirical finding that neuron activation sparsity and agreement are strong indicators of response correctness (Insights 1 & 2, Section 3.3).
2.  The development of Neuron-Agreement Decoding (NAD), a practical and effective unsupervised ensemble method that operates without needing comparable text outputs.
3.  The demonstration of a highly effective early-stopping mechanism that can reduce token consumption by up to 99% with minimal performance degradation (Table 2), which has major practical implications for the cost and latency of LLM inference.

**Strengths**
- **Novelty:** The core idea of using neuron activation agreement for best-of-N selection is highly original and moves beyond the prevailing paradigms of text-based voting or confidence scores.
- **Efficiency Gains:** The demonstrated ~99% reduction in token usage via early stopping is a standout result. This makes parallel sampling, which is often prohibitively expensive, a much more viable strategy.
- **Broad Applicability:** NAD is shown to be effective on both reasoning tasks with single correct answers (where it matches majority voting) and open-ended tasks like coding (where majority voting is inapplicable), demonstrating its versatility.
- **Strong Empirical Evidence:** The claims are well-supported by extensive experiments and clear visualizations (e.g., Figures 3, 4, 8), which build a convincing case for the method's effectiveness.

**Weaknesses**
- The paper mentions that storing neuron activations requires substantial disk and memory (Appendix A). While the authors suggest mitigations, a more detailed analysis of this overhead in the main paper would be beneficial to fully assess the practical trade-offs.
- The analysis in Section 5.3 shows that the "minimal activation" heuristic is less effective for coding tasks. While the authors provide a plausible explanation, this suggests the underlying principles may not be universal across all task types, which could be explored further.

**Questions**
1.  The early stopping results are remarkable. Have you considered applying this principle of using early internal signals to other areas, such as speculative decoding, to select the most promising continuation from a draft model?
2.  The paper focuses on using NAD as a selection mechanism at inference time. Do you see potential for using neuron agreement as a reward signal for reinforcement learning (e.g., in a DPO-style framework) to teach the model to produce more "consistent" internal reasoning paths?
3.  The analysis of the early stopping position in Figure 8 is fascinating. The non-monotonic performance is counter-intuitive. Could this be because later parts of a generation (e.g., summarizing the final answer) are more formulaic and thus introduce "noisy" but consistent activations that dilute the core reasoning signal from earlier steps?

**Rating**
- Overall (10): 9 — The paper introduces a novel, effective, and highly efficient method for improving LLM reasoning, backed by strong empirical evidence.
- Novelty (10): 10 — The use of internal neuron activation agreement for response selection is a groundbreaking idea that departs significantly from prior work.
- Technical Quality (10): 9 — The methodology is sound, and the experiments are thorough and well-designed, though a deeper analysis of overhead would be welcome.
- Clarity (10): 10 — The paper is exceptionally clear, well-organized, and supported by excellent figures that make complex ideas easy to understand (e.g., Figure 4, Figure 5).
- Confidence (5): 5 — I am highly confident in my assessment; the paper's claims are well-supported by the provided data and analysis.

***

### **Review 2**

**Summary**
This paper proposes Neuron Agreement Decoding (NAD), a method for selecting the best response from a set of `n` samples generated by an LLM. The authors argue that methods based on external signals (e.g., token probabilities, text outputs) are limited. Instead, they leverage internal neuron activations. Their core claims are that correct responses activate fewer neurons and that the sets of activated neurons for correct responses have higher pairwise similarity (agreement). NAD operationalizes these claims by selecting the sample with the minimum number of activated neurons (MinAct) or the sample that has the highest agreement with its peers (kNN, Medoid, DBSCAN). The method is shown to match majority voting on math problems and improve over average sampling on coding problems, while an early-stopping variant drastically cuts token usage.

**Soundness**
The paper's premise is interesting, but the methodological soundness has some points of concern. The entire method hinges on the definition of an "activated neuron" (Section 3.1). This definition, adapted from Cao et al. (2025), is quite specific and its justification is not fully established within this paper. More critically, the process for determining the activation threshold `η` is complex and involves multiple hyperparameters (e.g., top-k=64 per layer, then top-k=500 across layers per token, as described in Appendix B). The sensitivity of the results to these choices is not explored. Without an ablation study on these parameters, it is difficult to assess the robustness of the findings. For example, would a simpler thresholding method yield similar results?

The empirical results are promising, but the analysis could be deeper. The observation that MinAct works well for math but less so for coding (Figure 6) is a crucial finding that seems to challenge the universality of "Insight 2". The paper offers a brief explanation, but this discrepancy warrants a more thorough investigation as it might suggest the core heuristic is task-dependent and not a general property of "correctness". While the correlation between activation count and entropy is shown (Figure 2), the link between the proposed NAD method and these external metrics could be more explicitly compared in the main results table.

**Presentation**
The paper is generally well-written, but the presentation of key methodological details is lacking. The crucial definition of the threshold function `η` is relegated to Appendix B, making it difficult for the reader to fully grasp the method from the main text. The description in Section 3.1 simply states "the implementation... can be found in Appendix B," which is insufficient for such a central component.

The figures are visually appealing but have some inconsistencies. For example, the caption for Figure 3 refers to coloring by "average entropy," while the image itself (Block 16) has a color bar labeled "Heat Value." The paper text refers to Figure 4(b) multiple times, but this figure (a histogram) is not explicitly labeled and appears to be located in the conclusion section (Block 34), which is confusing. These small presentation issues detract from the overall clarity.

**Contribution**
The paper's main contribution is the idea of using internal activation patterns for best-of-N selection. This is a novel concept. The proposed NAD method, particularly its agreement-based variants, offers a new way to perform ensemble decoding that does not rely on text-level similarity, which is a valuable contribution for open-ended tasks. The efficiency gains from early stopping are also a significant practical contribution. However, the contribution's impact is slightly tempered by the methodological questions surrounding the definition of "activated neuron" and the lack of universality of the "minimal activation" heuristic.

**Strengths**
- **Novel Idea:** Proposing to use internal neuron activation patterns for response selection is a creative and promising research direction.
- **Strong Results on Open-Ended Tasks:** NAD provides a principled way to perform best-of-N selection on tasks like code generation where majority voting is not applicable, and shows clear gains over the sampling average (Table 1).
- **Efficiency:** The early stopping strategy and its massive token savings are a major practical strength.

**Weaknesses**
- **Methodological Opacity:** The definition of an "activated neuron" relies on a complex, hyperparameter-dependent thresholding function (`η`) that is not well-justified or analyzed in the main paper (Section 3.1, Appendix B).
- **Lack of Robustness Analysis:** The paper does not include sensitivity analyses for key hyperparameters in the activation definition, making it hard to judge the robustness of the findings.
- **Inconsistent Heuristic:** The core insight that "fewer activations is better" (Insight 2) is shown to not hold consistently across all tasks (Figure 6), which weakens the central claim of the paper.
- **Minor Presentation Flaws:** Confusing figure numbering and relegation of critical methodological details to the appendix hinder readability.

**Questions**
1.  How were the hyperparameters for the threshold function `η` (top-k=64 and top-k=500 in Appendix B) selected? Can you provide a sensitivity analysis to show how the results change with different values?
2.  In Section 3.1, you refine the activation set definition by taking a union over chunks (Eq. 3). Have you explored other aggregation methods? For instance, an intersection might identify neurons crucial to the entire reasoning process, while a weighted union could give more importance to earlier or later chunks.
3.  The explanation for why MinAct is less effective on coding tasks (Section 5.3) is that coding is more "open-ended." Could an alternative hypothesis be that correct code often requires calling upon more diverse functionalities, thus *correctly* activating more neurons, and that your heuristic is simply not applicable here?
4.  The paper claims external signals are "low dimensional projections" (Abstract). While Figure 3 supports this, have you tried to directly predict correctness using NAD vs. using a classifier trained on external signals (like entropy/confidence) to provide a more direct comparison of their predictive power?

**Rating**
- Overall (10): 6 — The paper presents a very interesting idea with impressive efficiency gains, but the methodological soundness is questionable due to a complex, unanalyzed core definition.
- Novelty (10): 8 — The core concept of using internal activation agreement is novel, though it builds on prior work in neuron-level analysis.
- Technical Quality (10): 5 — The lack of justification and sensitivity analysis for the "activated neuron" definition is a significant technical weakness.
- Clarity (10): 7 — The paper is mostly clear, but hiding key methodological details in the appendix and minor figure inconsistencies are notable issues.
- Confidence (5): 4 — I am reasonably confident in my assessment, but a deeper understanding of the thresholding function might change my view slightly.

***

### **Review 3**

**Summary**
This paper introduces Neuron Agreement Decoding (NAD), a suite of techniques for selecting the best candidate from multiple LLM-generated responses. Instead of relying on output text or token probabilities, NAD analyzes the internal state of the model by identifying sets of "activated neurons" for each response. The authors' key findings are that correct responses use fewer neurons and their activation patterns are more similar to each other. NAD leverages this by either picking the response with the fewest activated neurons (MinAct) or the one most central to a cluster of responses based on activation similarity (kNN, Medoid, DBSCAN). A major selling point is that this analysis can be done on just the first 32 tokens, allowing for an early-stopping strategy that prunes unpromising generation paths and saves over 97% of tokens.

**Soundness**
The paper's approach is logically sound and the experimental results are compelling from a performance perspective. The connection drawn between activation sparsity/agreement and correctness is demonstrated convincingly in the pilot study (Section 3.3). The proposed methods are direct applications of these findings. The evaluation across math, science, and code benchmarks is thorough.

However, the paper does not sufficiently address the practical, engineering aspects of the proposed method. The core of NAD requires computing and storing a set of activated neurons for each of `n` parallel samples. The authors acknowledge this in the limitations (Appendix A), stating it "requires substantial disk and memory." This is a critical point that should be discussed in the main body. For a production system, the memory, storage, and computational overhead of calculating neuron contributions (Eq. 1), applying thresholds (Appendix B), and then computing an `n x n` Jaccard similarity matrix could be prohibitive, potentially negating the benefits of token savings. For example, the compute for the Jaccard matrix is O(n^2), which scales poorly. While early stopping at 32 tokens mitigates this, the overhead is still non-zero and unquantified. The method is also tied to a specific FFN architecture (SwiGLU), and its applicability to other common architectures (like those with GELU activations) is not discussed, limiting its perceived generality.

**Presentation**
The paper is well-structured and the main ideas are communicated clearly. The use of diagrams like Figure 5 is effective for explaining the NAD workflow. The results in Tables 1 and 2 are presented clearly and make a strong case for the method's performance and efficiency.

My main criticism of the presentation is that it downplays the implementation complexity and resource costs. The "free lunch" framing in the introduction is appealing, but NAD is not free; it trades token compute for a different kind of compute and memory/storage overhead. A more balanced presentation would include a section discussing these practical trade-offs, perhaps with wall-clock time and peak memory usage measurements to complement the token savings data.

**Contribution**
The paper makes a valuable contribution by demonstrating that internal model dynamics can be a powerful signal for response selection. The novelty lies in operationalizing this insight into a practical decoding strategy (NAD) that works for a variety of tasks, including open-ended ones. The most significant contribution is arguably the early-stopping strategy, which offers a concrete path toward making expensive multi-sample generation techniques more economically viable. This work provides a new tool for the LLM inference toolbox and a new lens through which to analyze model behavior.

**Strengths**
- **Practical Efficiency:** The token savings demonstrated with early stopping are enormous (Table 2) and represent a significant practical benefit.
- **Performance:** NAD is shown to be a high-performing selection strategy, matching the strong majority vote baseline where applicable and outperforming average sampling elsewhere (Table 1).
- **Generality for Open-Ended Tasks:** NAD provides a solution for best-of-N selection in domains like code generation where text-based voting is difficult, a notable advantage over methods like self-consistency.
- **Clear Conceptual Framework:** The paper does a good job of motivating the work and explaining the core insights that drive the method.

**Weaknesses**
- **Unquantified Overhead:** The paper lacks a quantitative analysis of the computational and memory/storage overhead of NAD. This makes it difficult to assess the true net benefit compared to the token savings.
- **Architectural Specificity:** The neuron activation definition is specific to SwiGLU-based FFNs (Section 3.1). The paper does not discuss how or if the method could be adapted to other model architectures, which limits its generalizability.
- **Scalability Concerns:** The pairwise similarity calculation is O(n^2), which may not scale well to very large numbers of samples (high `n`). While `n=64` is used here, some approaches use hundreds of samples.

**Questions**
1.  Could you provide concrete measurements of the overhead? For example, for one problem in the AIME dataset with `n=64`, what is the peak memory usage for storing the activation sets, and what is the wall-clock time required for the NAD selection process itself?
2.  Your definition of neuron contribution in Eq. 1 is specific to SwiGLU FFNs. What changes would be necessary to apply NAD to models with different activation functions or FFN structures, such as a standard Transformer with GELU? Do you expect the core findings to hold?
3.  The early stopping is performed after just 32 tokens. While the results are great, this seems very early for complex reasoning problems. Does this imply that the "reasoning path" is largely determined by the model's initial "hunch" in the first few dozen tokens?
4.  Given the O(n^2) complexity of building the similarity matrix, have you considered more scalable clustering or selection algorithms, such as locality-sensitive hashing (LSH), to approximate the Jaccard similarity and find the consensus sample more efficiently for very large `n`?

**Rating**
- Overall (10): 8 — A strong paper with a novel method and impressive results, but with practical overhead concerns that need to be addressed.
- Novelty (10): 9 — The approach of using internal activation agreement is a fresh and valuable idea in the space of ensemble decoding.
- Technical Quality (10): 7 — The experiments are well-executed, but the lack of analysis on computational overhead and architectural generalizability is a notable omission.
- Clarity (10): 8 — The paper is clearly written, though it could be more upfront about the practical costs of the method.
- Confidence (5): 5 — I am confident in my assessment of the paper's strengths and its practical limitations.

***

### **Review 4**

**Summary**
This paper explores the use of LLMs' internal neuron activation patterns as a signal for response quality. The authors posit that external signals like confidence scores are insufficient and that internal dynamics are more revealing. They make two central empirical claims: 1) correct responses activate fewer neurons than incorrect ones, and 2) correct responses exhibit higher agreement in their neuron activation patterns across multiple samples. Based on this, they propose Neuron-Agreement Decoding (NAD), an unsupervised method to select the best of `N` generated responses. NAD includes variants that select based on minimum activation count or on consensus within a similarity matrix of activation sets. A key result is that NAD can be applied very early in generation (e.g., 32 tokens), enabling an early-stopping strategy that reduces token usage by up to 99% while maintaining or improving accuracy over average sampling.

**Soundness**
The paper is methodologically sound for the most part. The pilot study (Section 3.3) provides a good foundation for the main hypotheses, with clear visualizations supporting the claims (Figure 4). The proposed NAD methods are logical consequences of the insights from this study. The experimental design is strong, featuring multiple models and a good range of tasks that test the method in different scenarios (verifiable answers vs. open-ended generation).

However, there are a few areas where the soundness could be improved. First, the comparison to other confidence-based methods is somewhat indirect. The paper motivates the work by criticizing methods based on token probabilities (Section 1), but the main results table (Table 1) does not include a direct comparison to a state-of-the-art confidence-based baseline like self-certainty (Kang et al., 2025). Such a comparison would more directly substantiate the claim that internal activations are a *better* signal.

Second, the analysis of the results, while good, could be expanded. The finding that MinAct works for math but not for coding (Figure 6) is very interesting and is a point of weakness for the universality of the proposed heuristics. The authors' explanation is plausible but brief. A deeper dive—perhaps analyzing the complexity or diversity of reasoning steps in each domain—would strengthen the paper. Similarly, the non-monotonic relationship between early stopping position and accuracy (Figure 8) is a fascinating and important result that deserves more than a brief mention of "noise accumulation."

**Presentation**
The paper is well-written, clear, and logically structured. The abstract provides a concise and accurate summary of the work. The introduction effectively motivates the problem. The figures are generally high quality and aid understanding significantly. Figure 4, in particular, does an excellent job of visually communicating the core intuitions behind the method.

There are minor issues that could be polished. The figure numbering and referencing seem to be inconsistent in the provided draft (e.g., Figure 3's caption and image label, the location of Figure 4b), which should be corrected for the final version. The core technical detail of the threshold function `η` is defined in Appendix B, which makes the method in Section 3.1 feel incomplete without flipping to the back. A brief, high-level summary of the thresholding mechanism in the main text would improve flow and clarity.

**Contribution**
This paper makes a solid and novel contribution to the field. The primary contribution is the introduction and successful validation of a new class of signals—neuron activation patterns—for unsupervised response selection. This moves the field beyond relying solely on output-level information. The NAD method itself is a valuable contribution, especially its applicability to open-ended tasks where methods like majority voting fail. The demonstration of massive efficiency gains via early stopping is a highly significant result with immediate practical relevance for anyone using ensemble methods with LLMs. The work also contributes new interpretability insights into how LLMs internally represent correctness.

**Strengths**
- **Novel and Insightful Premise:** The idea to look at internal activation agreement is a significant departure from prior work and yields interesting insights.
- **Strong Empirical Results:** The method is shown to be effective across a range of tasks and models, performing comparably to or better than standard baselines (Table 1).
- **Exceptional Efficiency Gains:** The early stopping results are a major highlight, demonstrating a practical way to make best-of-N sampling far more affordable (Table 2).
- **Good Analysis:** The paper includes a dedicated analysis section (5.3) that explores important aspects like the effect of the early stopping position and task-specific differences, adding depth to the results.

**Weaknesses**
- **Missing Direct Baseline Comparison:** The paper argues against confidence-based methods but does not include a direct experimental comparison against a strong baseline from that category in the main results.
- **Generalizability Questions:** The method's performance varies by task type (Figure 6), and it has only been tested on models up to 8B parameters. Its effectiveness on much larger models remains an open question.
- **Presentation of Key Details:** Critical methodological details about the activation threshold are confined to the appendix, making the core method harder to understand from the main text alone.

**Questions**
1.  The paper argues that NAD is superior to methods based on external signals like confidence. To make this claim more concrete, could you add results comparing NAD directly to a strong confidence-based selection baseline (e.g., self-certainty) on your benchmark suite?
2.  The non-monotonic performance with respect to the early stopping position (Figure 8) is a very interesting finding. Could you elaborate on your "noise accumulation" hypothesis? Is it possible that later tokens, which might involve formatting or boilerplate text, introduce activation patterns that are consistent but not relevant to the core reasoning, thereby confusing the agreement metric?
3.  Your analysis in Figure 6 shows MinAct is strong for math/science but weaker for coding. Do you think this is because "correct" code can be written in many equally complex ways, whereas "correct" mathematical reasoning often follows a more singular, efficient path? How might NAD be adapted to better handle tasks with diverse valid solutions?
4.  The study was conducted on 4B and 8B models. Do you have any preliminary results or hypotheses on whether these findings about activation sparsity and agreement will hold for much larger models (e.g., 70B+)? It's possible that larger models with more redundancy might behave differently.

**Rating**
- Overall (10): 8 — A very strong paper with a novel method, impressive results, and important practical implications, with some room for a more direct baseline comparison and deeper analysis.
- Novelty (10): 9 — The use of internal activation agreement is a genuinely new approach for this problem.
- Technical Quality (10): 8 — The experiments are well-designed and convincing, though a direct comparison to confidence-based methods would strengthen the technical evaluation.
- Clarity (10): 8 — The paper is very clearly written, but could be improved by bringing some key methodological details from the appendix into the main text.
- Confidence (5): 5 — I am confident in my evaluation of this paper's strengths and areas for minor improvement.