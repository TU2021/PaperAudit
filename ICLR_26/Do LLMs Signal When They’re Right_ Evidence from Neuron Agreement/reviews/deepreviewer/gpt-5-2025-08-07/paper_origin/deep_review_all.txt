Summary
The paper studies whether large language models encode signals of correctness in their internal neuron activations. It reports three empirical findings: (1) external confidence metrics (entropy, self-certainty) correlate with—and are argued to be low-dimensional projections of—richer internal activated-neuron states (Section 3.2, Figure 2, Figure 3); (2) correct responses activate substantially fewer unique neurons than incorrect ones throughout generation (Section 3.3, Figure 4b–c, Figure 34–36); and (3) correct responses show higher cross-sample agreement in which neurons are activated (Section 3.3, Figure 3, Figure 4a). Leveraging these, the authors propose Neuron-Agreement Decoding (NAD), which selects a “best” trajectory among n samples using either cross-sample Jaccard similarity of activated-neuron sets (kNN, medoid, DBSCAN) or sparsity (MinAct) (Section 4.1, Figure 5, 21). An early stopping strategy truncates generation after B=32 tokens, computes internal signals, and continues only the selected trajectory, yielding large token savings (Section 4.2, Table 2, Table 3). Experiments on math/science datasets and code generation show NAD is competitive with majority voting when applicable and consistently better than averaging over samples (Table 1), while reducing tokens by ~98–99% with small accuracy loss (Table 2, Table 3).

Soundness
The methodological core builds on an operational definition of “activated neurons” via per-token FFN contribution scores under SwiGLU (Eq. 1–3; Appendix B Eq. 5–6), then aggregates sets over chunks and compares samples using Jaccard. The approach is internally consistent and aligns with recent neuron-level analyses (Section 3.1). The correlations with self-certainty and entropy (Figure 2) and the clustering agreement (Figure 3, Figure 4a) are plausible; the early-stage sparsity-vs-correctness signal (Figure 4b–c) is repeatedly supported across tasks (Figure 6, Figure 34–36). However, several assumptions and omissions limit rigor: (i) the “low-dimensional projection” claim is suggestive rather than proven—the correlations (|r| ~0.63) do not demonstrate sufficiency or causality (Section 3.2); (ii) activation-count signals may be confounded by length, diversity, or prompt-induced exploration; the paper counters via token-aligned curves (Figure 4c) but broader length-control analyses are limited; (iii) only FFN contributions are considered, not attention or residual pathways, potentially missing important internal dynamics; (iv) Jaccard over union of chunk-level sets ignores activation magnitude, duration, and timing—weighted or temporal similarity could strengthen the case; and (v) hyperparameter choices (B=32, k in kNN, DBSCAN parameters, threshold k=500 in Appendix B) lack sensitivity analyses beyond the early-stopping sweep (Figure 8–10) and the top-k aggregation ablation (Section 5.3, Figure 7). Overall, the empirical evidence is promising, but more controls and alternative similarity/activation formulations would improve robustness.

Presentation
The manuscript is generally clear and well-organized: motivation (Section 1), related work (Section 2), preliminaries (Section 3.1–3.3), method (Section 4.1–4.2), experiments (Section 5), and limitations (Appendix A). Figures effectively illustrate core claims (Figure 2–6, Table 1–3). Notation is mostly consistent, though some equations mix informal and formal descriptions (e.g., the operational nature of Eq. 1 and Appendix B Eq. 5–6), and the distinction between token-level vs sequence-level top-k thresholding requires careful reading (Appendix B, Section 5.3). The systems aspects (storage, compute, instrumentation) are briefly noted (Appendix A) but could use clearer reporting (e.g., wall-clock savings, memory footprints). The justification for single-execution code protocol is stated (Section 5.1), but readers unfamiliar with code evaluation regimes may want more explicit comparison to standard pass@k/testing practices.

Contribution
The main contribution is a novel, label-free ensemble-decoding strategy that uses only internal activation signals to select and to early-prune reasoning trajectories, without requiring comparable textual outputs (Section 4.1–4.2). Empirical insights that correct chains activate fewer, more-agreed neurons (Section 3.3) are interesting and, if general, may inform inference-time control and interpretability. NAD shows competitive accuracy with majority voting on math/science, outperforms Avg@64 on code where voting is inapplicable, and achieves large token savings via early stopping (Table 1–3). The work advances the idea that internal signals can guide best-of-n selection efficiently, adding to confidence-based pruning literature by moving from output distributions to neuron-level states.

Strengths
- Clear empirical insights linking internal activation sparsity/agreement to correctness (Section 3.3; Figure 4b–c, Figure 34–36).
- Practical, unsupervised selector family (kNN/medoid/DBSCAN/MinAct) that uses only internal signals (Section 4.1).
- Strong efficiency gains via early stopping at B=32 with minimal accuracy impact (Section 4.2; Table 2, Table 3, Figure 8–10).
- Competitive performance with majority voting on verifiable tasks and improvement over Avg@64 on open-ended code (Table 1).
- Careful ablation on activation aggregation (Section 5.3, Figure 7) and early-stopping position (Figure 8–10).

Weaknesses
- The “low-dimensional projection” framing remains a hypothesis supported by correlation and visualization, not by formal analysis or reconstruction experiments (Section 3.2; Figure 2–3).
- Activation definition focuses on FFN/SwiGLU and unembedding; attention and residual contributions are not modeled, limiting generality (Section 3.1; Eq. 1).
- Jaccard similarity over unioned sets ignores magnitude and temporal structure; weighted/temporal variants could be more informative (Section 4.1).
- Baselines on code omit common multi-execution/pass@k or test-based selection regimes; comparison to stronger external methods would sharpen claims (Section 5.1–5.2).
- Limited statistical reporting (no confidence intervals or multiple seeds), and sensitivity to sampling hyperparameters (temperature/top-p, n) is not studied (Section 5.1–5.2).
- Practical deployment: reliance on internal access and storage of activation sets may preclude closed models and introduces non-trivial systems overhead (Appendix A).

Questions
1. Can you quantify wall-clock speedups and memory overhead for NAD at B=32 vs full sampling (e.g., per-instance latency, peak memory), beyond token counts (Table 2–3)?
2. How sensitive are results to temperature/top-p and to the number of samples n? Does the sparsity/agreement signal degrade at low n?
3. Have you tried weighted Jaccard or temporal alignment (per-chunk similarity trajectories) instead of set unions to capture magnitude and timing?
4. To what extent do attention/residual pathways change the agreement/sparsity signal if included in the activation definition?
5. Can you provide per-dataset statistical uncertainty (e.g., multiple seeds, CIs) for Table 1–3?
6. For code, how would NAD compare under a standard multi-execution/test harness regime (e.g., run k candidates), and could internal signals prioritize which candidates to execute?
7. Is the “fewer neurons ⇒ correctness” effect consistent across tasks with comparable output length/complexity controls?

Rating
- Overall (10): 7 — Strong empirical promise and efficiency gains (Table 1–3, Section 4.2) but theory and baselines need strengthening (Section 3.2, Section 5.2).
- Novelty (10): 8 — Internal activation-based selection and early pruning are fresh contributions versus output-only confidence (Section 4.1–4.2).
- Technical Quality (10): 6 — Method is coherent (Eq. 1–3; Appendix B) but lacks attention/residual modeling and stronger controls (Figure 2–3, Section 5.2).
- Clarity (10): 7 — Clear figures and flow (Figure 2–6, Table 1–3), though systems details and thresholding nuances could be crisper (Appendix A–B).
- Confidence (5): 4 — Assessment based on detailed reading of methods/experiments and cross-checking anchors, but absent code and statistical CIs.

---

Summary
The paper proposes Neuron-Agreement Decoding (NAD), a label-free ensemble method that selects among multiple sampled reasoning trajectories using internal neuron activation signals rather than external outputs. It empirically shows that external confidence metrics correlate with activated-neuron counts (Figure 2), correct answers activate fewer unique neurons and exhibit stronger cross-sample agreement (Section 3.3, Figure 4b–c), and that early selection at B=32 tokens preserves accuracy while cutting token usage by ~98–99% (Section 4.2, Table 2–3). NAD includes several selectors (kNN, medoid, DBSCAN) based on Jaccard similarity over activated-neuron sets and a MinAct heuristic choosing the sparsest activation trajectory (Section 4.1).

Soundness
The approach is grounded in an explicit activation definition (Eq. 1–3; Appendix B) and operational consensus metrics (Jaccard, Section 4.1). The empirical regularities (Figure 2–4) are consistent with the intuition that successful chains “commit” earlier, while incorrect ones wander, engaging more neurons. However, several methodological gaps remain: (a) the activation model excludes attention and residual connections, which could materially affect the internal state (Section 3.1); (b) the Jaccard index over unioned sets discards activation magnitude, frequency, and order; (c) the claim that external metrics are low-dimensional projections is not validated beyond correlation (Figure 2) and t-SNE visualization (Figure 3); (d) experimental design uses a single n=64, T=0.6, p=0.9 regime (Section 5.1) with no sensitivity; (e) lack of statistical uncertainty (no error bars, seeds) in Table 1–3 raises questions about stability; and (f) code evaluation uses single execution, omitting stronger baselines (e.g., automated testing with multiple runs), making open-ended gains less conclusive (Section 5.1–5.2). Despite these, the early-stopping analysis (Figure 8–10) and top-k ablation (Figure 7) add credibility.

Presentation
The paper presents a coherent narrative with illustrative figures (Figure 2–6) and detailed tables (Table 1–3). Equations and algorithmic steps are described, but some notation lacks precision for reproducibility (e.g., exact tensor shapes and operations in Eq. 1; chosen DBSCAN parameters; kNN k). The systems section (Appendix A) acknowledges storage overhead but does not present end-to-end runtime or memory metrics, which are important for practical deployment. Overall readability is good, though clarity would improve with a consolidated algorithm box and a schematic of activation computation across layers/tokens.

Contribution
The contribution lies in reframing best-of-n selection around internal neuron dynamics: consensus via activation-set similarity and sparsity as early correctness signals (Section 4.1–4.2). This is novel relative to output-only confidence methods (Section 2) and practically valuable where textual outputs are not comparable (e.g., code with multiple valid solutions). The large token savings (Table 2–3) demonstrate efficiency impact. The insights may also influence interpretability and calibration work by linking activation patterns to correctness.

Strengths
- Clear empirical evidence that correct chains are sparser and more mutually aligned (Section 3.3; Figure 4b–c).
- Multiple selector variants with consistent gains over Avg@64 (Table 1).
- Early stopping yields substantial compute savings with modest accuracy changes (Table 2–3; Figure 8–10).
- Ablations on activation aggregation (Figure 7) and early-stopping position (Figure 8–10) show thoughtfulness.
- Applicability to tasks where majority voting is hard (code), with consistent improvements over a curated baseline (Table 1).

Weaknesses
- Missing attention/residual components in activation modeling may limit generality (Section 3.1).
- Weak code baselines (single-execution) compared to standard multi-run/test harnesses; no comparison to confidence-based pruning with external signals in code settings (Section 5.1–5.2).
- No statistical uncertainty reporting; stability across seeds/sampling regimes not demonstrated (Table 1–3).
- Jaccard over set unions may be too coarse; no exploration of weighted or temporal similarities (Section 4.1).
- Practical constraints: internal access to model activations is required; closed models cannot benefit (Appendix A).

Questions
1. What are the exact DBSCAN parameters used, and how sensitive are results to these choices relative to kNN/medoid?
2. Can you include attention and residual pathways in the activation definition and report whether the sparsity/agreement signals strengthen or weaken?
3. Would a weighted Jaccard (by contribution magnitude) or chunk-wise temporal similarity give better separation than plain set Jaccard?
4. Can you provide multiple-seed averages and confidence intervals for Table 1–3?
5. For code, how does NAD compare when a small number of candidates are executed using unit tests (pass@k), and can NAD reduce how many candidates must be executed for comparable accuracy?
6. How do results change for different n (e.g., 8, 16, 32) and temperatures?

Rating
- Overall (10): 6 — Interesting and promising internal-signal approach with efficiency gains (Table 2–3) but limited baselines and incomplete activation modeling (Section 3.1, Section 5.2).
- Novelty (10): 7 — Leveraging neuron-set agreement and sparsity for selection/early pruning is new relative to output-based methods (Section 4.1–4.2).
- Technical Quality (10): 5 — Sound operationalization but misses key components (attention/residual) and statistical rigor (Figure 2–3, Table 1–3).
- Clarity (10): 6 — Generally readable with strong figures, but algorithmic specifics and systems metrics are under-specified (Appendix A–B).
- Confidence (5): 3 — Moderate confidence based on manuscript details; absence of code and statistical CIs limits certainty.

---

Summary
This work introduces Neuron-Agreement Decoding (NAD), which selects the best-of-n reasoning trajectory using internal neuron activation patterns rather than external outputs. The authors find that activated-neuron counts correlate with entropy/self-certainty (Figure 2), that correct answers activate fewer neurons and cluster together by activation sets (Section 3.3, Figure 3–4), and that early selection at 32 tokens preserves accuracy while saving ~99% tokens (Section 4.2, Table 2–3). NAD variants (kNN, medoid, DBSCAN, MinAct) use Jaccard similarity over chunk-aggregated activated-neuron sets (Section 4.1). Experiments on Qwen and DeepSeek models across math/science and coding show gains over Avg@64 and competitiveness with majority voting when applicable (Table 1).

Soundness
The methodology is carefully constructed: activation definition for SwiGLU FFNs (Eq. 1–3), per-token thresholding (Appendix B), chunked aggregation (Eq. 3), Jaccard-based consensus (Eq. 4), and early stopping (Section 4.2). Empirical evidence—the clustering and sparsity signals—is compelling across datasets (Figure 3, Figure 4, Figure 6). The top-k aggregation ablation (Figure 7) supports the choice of unioning chunk-level sets. The early-stopping analysis (Figure 8–10) shows non-monotonic accuracy with later stops, arguing for early signal reliability. Limitations include the focus on FFN-only contributions and simplistic set similarity; further, the causal reading of “external metrics are projections” remains provisional (Section 3.2). Still, the approach is operationally sound and validated across several models and tasks.

Presentation
Figures are informative and captions tie claims to visuals (Figure 2–6, 7–10). Tables summarize results cleanly (Table 1–3). The method flow (Figure 5, Figure 21) clarifies the pipeline. Notation is serviceable, though Eq. 1 could be unpacked more for readers less familiar with SwiGLU internals. The early-stopping choice (B=32) is justified and explored (Figure 8). The limitations section is candid about storage and selector choice (Appendix A). Minor clarity improvements could include a consolidated pseudocode block and reporting DBSCAN/kNN hyperparameters.

Contribution
Conceptually, NAD advances sample-evaluate-ensemble by using internal neuron activity for selection and early pruning—distinct from token-probability confidence or self-evaluation (Section 2). Practically, it demonstrates large efficiency gains with little accuracy trade-off (Table 2–3) and works where textual outputs are not directly comparable (code). Empirical insights about sparsity and agreement in correct chains may inform future interpretability and calibration efforts.

Strengths
- Strong, repeated evidence for activation sparsity and cross-sample agreement in correct responses (Section 3.3; Figure 4b–c, Figure 34–36).
- Operational selectors that do not require comparable outputs (Section 4.1), competitive with majority voting on math/science (Table 1).
- Early stopping delivers two orders of magnitude token savings (Table 2–3).
- Ablations and analysis (Figure 7–10) demonstrate thoughtful probing of design choices.
- Clear positioning vs outputs-based confidence and self-consistency (Section 2).

Weaknesses
- Activation model excludes attention/residual streams; generality may be limited (Section 3.1).
- Jaccard on union-only sets discards magnitude and timing; more expressive similarities could improve selection (Section 4.1).
- Statistical rigor: no error bars or multiple seeds in Table 1–3; sensitivity to sampling hyperparameters is untested (Section 5.1–5.2).
- Code baselines use single-execution; comparison to standard multi-run test frameworks (pass@k) is missing (Section 5.1).
- Applicability to closed-source models is constrained due to required internal access (Appendix A).

Questions
1. Could you incorporate attention/residual activations into the neuron set and report whether consensus/sparsity signals strengthen?
2. Have you tried weighted Jaccard (by contribution scores) or per-chunk similarity trajectories to capture the temporal structure of reasoning?
3. What are the exact hyperparameters for kNN (k) and DBSCAN (eps, min_samples), and how sensitive are results to them?
4. Can you provide statistical uncertainty (CIs across seeds) for Table 1–3 and sensitivity to T/top-p and n?
5. For code, can NAD reduce the number of candidates that must be executed in a pass@k setting while maintaining accuracy?

Rating
- Overall (10): 8 — Novel, well-supported internal-signal approach with substantial efficiency gains (Table 2–3) and competitive accuracy (Table 1), despite omitted attention/residual and statistical details.
- Novelty (10): 9 — Using neuron-activation agreement/sparsity for label-free selection and early pruning is an original angle (Section 4.1–4.2).
- Technical Quality (10): 7 — Method is coherent and ablated (Eq. 1–3; Figure 7–10) but leaves out attention/residual and stronger baselines/statistics.
- Clarity (10): 8 — Clear figures and flow (Figure 2–6; Table 1–3); minor algorithmic specifics could be expanded (Appendix B).
- Confidence (5): 4 — High-level methodology and results are clear, but lack of code/statistical CIs tempers certainty.

---

Summary
The paper argues that LLMs internally signal correctness through neuron activation patterns: correct chains are sparser and more mutually consistent (Section 3.3). It proposes NAD, which selects among n parallel samples using internal signals—either cross-sample agreement (Jaccard-based kNN/medoid/DBSCAN) or minimal activation count (MinAct) (Section 4.1). Early stopping at 32 tokens enables aggressive pruning with large token savings (Section 4.2). Experiments on Qwen and DeepSeek across math/science (AIME24/25, GPQA) and code (HumanEval, MBPP, LCBv5) show NAD ≈ majority voting on math/science and > Avg@64 on code, with ~99% token reduction (Table 1–3).

Soundness
The approach is well-motivated and consistent with the notion that successful reasoning stabilizes early. Activation computation is defined (Eq. 1–3; Appendix B), and consensus is operationalized via Jaccard (Eq. 4). Empirical evidence supports the main claims (Figure 3–4, Figure 6). Still, the method relies on internal access to activations and FFN-specific signals; attention/residual pathways are not included (Section 3.1). The set-based similarity is coarse; magnitude and positional information are ignored. The projection claim (Section 3.2) is supported by correlation and visualization but not by reconstruction or formal analysis. Early stopping analyses (Figure 8–10) are informative, but statistical stability (seeds, CIs) is missing, and practical performance metrics (latency, memory) are not reported (Appendix A).

Presentation
The paper is readable, with strong visuals (Figure 2–6, 7–10) and tables (Table 1–3). The pipeline is clearly depicted (Figure 5, 21). Some implementation details (DBSCAN/kNN hyperparameters, exact storage footprint, time per selection) are not specified, and Eq. 1 could be unpacked further for clarity. The limitations section is useful but could quantify overheads more concretely.

Contribution
The work contributes a new, internal-signal-driven selection paradigm for label-free ensemble decoding with substantial efficiency gains and applicability to open-ended tasks. It provides empirical insights into neuron-level agreement and sparsity as correctness signals, potentially informing future interpretability and inference-time control strategies.

Strengths
- Demonstrated correctness signals in internal activations (Section 3.3; Figure 4b–c).
- Unsupervised, label-free selectors that do not depend on comparable outputs (Section 4.1).
- Early stopping yielding large token savings with preserved accuracy (Table 2–3).
- Consistent gains over sampling average and competitiveness with majority voting (Table 1).
- Ablation and analysis on activation aggregation and stopping positions (Figure 7–10).

Weaknesses
- Dependence on internal model access; not usable with closed APIs (Appendix A).
- FFN-only activation modeling; attention/residual neglected (Section 3.1).
- Coarse set-based similarity; no magnitude/temporal weighting (Section 4.1).
- Limited baselines and statistical reporting; sensitivity to sampling hyperparameters untested (Section 5.1–5.2).
- Practical metrics (latency/memory/time-to-first-token) not provided (Appendix A).

Questions
1. What is the wall-clock impact of NAD selection at B=32 vs full sampling (per-instance latency, throughput)?
2. How do results change if attention activations or residual streams are incorporated into the neuron sets?
3. Can you report sensitivity to k in kNN, eps/min_samples in DBSCAN, and the threshold k=500 (Appendix B)?
4. Would weighted/temporal Jaccard improve selection robustness, especially on code where Max-Activation sometimes wins (Figure 6)?
5. Can NAD be combined with simple output-based confidence to further improve accuracy or reduce selection mistakes?
6. Do the sparsity/agreement signals hold under different temperatures and smaller n?

Rating
- Overall (10): 6 — Solid idea with strong efficiency benefits (Table 2–3) but practical constraints and methodological simplifications limit generality (Section 3.1, Appendix A).
- Novelty (10): 7 — Internal activation-set agreement and sparsity used for selection/early pruning is a novel twist on ensemble decoding (Section 4.1–4.2).
- Technical Quality (10): 6 — Coherent operationalization (Eq. 1–3) but missing attention/residual, stronger baselines, and statistical robustness (Table 1–3).
- Clarity (10): 7 — Clear figures and framing; more implementation specifics would help (Appendix B, selector hyperparameters).
- Confidence (5): 4 — Based on careful reading of method/results, but lack of code and statistical CIs reduces certainty.