{
  "paper": "Do LLMs Signal When They’re Right_ Evidence from Neuron Agreement",
  "judge_model": "gpt-5.1",
  "review_agent": "deepreviewer",
  "ai_model": "gpt-5-2025-08-07",
  "ai_review_file": "deep_review_all.txt",
  "inputs": {
    "human_review": "review.txt",
    "ai_review": "reviews/deepreviewer/gpt-5-2025-08-07/paper_origin/deep_review_all.txt"
  },
  "multi_review": {
    "detected": false,
    "num_reviews": 1,
    "summarize_used": false,
    "summary_note": null
  },
  "ai_review_consolidated": null,
  "alignment": {
    "motivation_strength_alignment": 0.95,
    "weakness_error_alignment": 0.8,
    "overall_alignment": 0.9,
    "explanation": {
      "strength": "Both reviews strongly agree on the main motivations and contributions. They emphasize that (i) internal neuron activation patterns (sparsity and cross-sample agreement) are used as correctness signals; (ii) these signals are leveraged to design Neuron Agreement Decoding (NAD), a label-free best-of-N decoding method; (iii) NAD is unsupervised, simple to implement, and operates via Jaccard similarity and MinAct; (iv) early stopping at around 32 tokens yields substantial token savings with limited accuracy loss; and (v) NAD is competitive with majority voting on math/science tasks and improves over Avg@64 on open-ended code, where majority voting is less meaningful. Both also highlight that the work provides empirical insight into internal dynamics (correct chains are sparser and more clustered) and positions this as an interpretability/inference-time control contribution. The AI review simply adds more implementation detail but is fully consistent with the human review on all core strengths.",
      "weakness": "There is substantial but not perfect alignment in weaknesses. Strongly overlapping concerns include: (1) insufficient statistical rigor (no error bars, few seeds) and lack of sensitivity analyses over sampling hyperparameters (T, top-p, N); (2) missing or weak baselines, especially stronger external/confidence-based or standard code-evaluation regimes; (3) limited discussion and quantification of systems/compute overhead (activations storage, Jaccard cost, latency/memory); and (4) over-interpretation or under-justification of the claim that internal activations add value beyond simple entropy/self-confidence, with correlation-based evidence seen as suggestive but not conclusive. The human review additionally stresses: limited model diversity (no frontier-scale/closed models, unclear behavior in very-high/very-low accuracy regimes) and questions about the generality/scope of claims across broader task types. The AI review instead adds some different technical caveats: focus on FFN-only activations (no attention/residual), coarseness of pure set-based Jaccard, and lack of alternative similarity formulations. These extra AI-identified issues do not conflict with the human review but go beyond it, so the overlap is strong but not complete.",
      "overall": "In substance and judgment, the reviews are highly aligned. Both view the core idea—using internal neuron sparsity and agreement for unsupervised best-of-N selection and early pruning—as novel and interesting, with empirically promising results and strong efficiency benefits. Both also see the current evidence as incomplete: baselines and statistical robustness are not yet where they should be, some claims (especially about internal signals vs entropy/confidence and about broad generality) are viewed as somewhat overstated, and practical deployment costs are under-characterized. The AI review provides a more fine-grained methodological critique (FFN-only modeling, similarity metrics) and more detailed commentary on equations and figures, while the human review places more emphasis on generality across models/tasks and positioning against a broader ecosystem of related methods. Despite these differences in emphasis and granularity, the overall narrative and evaluation direction are very similar, justifying a high overall alignment score."
    }
  },
  "generated_at": "2025-12-27T19:27:32",
  "runs": [
    {
      "config": {
        "judge_model": "gpt-5.1",
        "review_agent": "paper_audit",
        "ai_model": "gpt-5-2025-08-07",
        "ai_review_file": "baseline_review.txt",
        "temperature": 0.0,
        "max_tokens": 20000,
        "timeout_s": 180
      },
      "config_key": "3b28d70d5731760031a4514ebc98b0afd78edf6d",
      "inputs": {
        "human_review": "review.txt",
        "ai_review": "reviews/paper_audit/gpt-5-2025-08-07/paper_origin/baseline_review.txt"
      },
      "multi_review": {
        "detected": false,
        "num_reviews": 1,
        "summarize_used": false
      },
      "ai_review_consolidated": null,
      "alignment": {
        "motivation_strength_alignment": 0.9,
        "weakness_error_alignment": 0.78,
        "overall_alignment": 0.84,
        "explanation": {
          "strength": "Both reviews clearly agree on the core motivation: using internal neuron activation patterns (sparsity and cross-sample agreement) as unsupervised signals for best-of-N decoding and early stopping, and they both emphasize NAD’s novelty relative to output-based confidence, its label-free simplicity, suitability for open-ended code, and large token savings with accuracy comparable to majority voting/Avg@64. The AI review adds more granular detail (specific figures, variants like kNN/DBSCAN/MinAct, activation definitions), but these elaborate rather than diverge from the human reviewer’s highlighted strengths.",
          "weakness": "There is strong overlap on key weaknesses: limited model/task diversity and external validity, missing or weak comparisons to strong entropy/confidence-based baselines, underdeveloped justification that activations add value beyond entropy, sensitivity and incomplete analysis of hyperparameters/early-stopping choice, and unquantified practical overhead of pairwise Jaccard and activation storage. The AI review introduces extra concerns (theoretical grounding, t-SNE robustness, precise hyperparameter reporting, specific aggregation/similarity choices), which are mostly finer-grained methodological issues not explicitly raised in the human review, hence only slightly lowering alignment.",
          "overall": "In substance and judgment, the reviews are largely consistent: both see the work as a novel and promising internal-signal-based decoding framework with impressive efficiency gains, but tempered by gaps in baselines, robustness, and generality/overhead characterization. Differences are mainly in the AI review’s greater level of technical detail and additional, more fine-grained methodological critiques, rather than disagreements about the main contributions or limitations."
        }
      },
      "generated_at": "2025-12-27T19:50:34"
    },
    {
      "config": {
        "judge_model": "gpt-5.1",
        "review_agent": "paper_audit",
        "ai_model": "gpt-5-2025-08-07",
        "ai_review_file": "final_review_all.txt",
        "temperature": 0.0,
        "max_tokens": 20000,
        "timeout_s": 180
      },
      "config_key": "4569a6a97a67c8e2b360606eb74d6eb53b892d06",
      "inputs": {
        "human_review": "review.txt",
        "ai_review": "reviews/paper_audit/gpt-5-2025-08-07/paper_origin/final_review_all.txt"
      },
      "multi_review": {
        "detected": false,
        "num_reviews": 1,
        "summarize_used": false
      },
      "ai_review_consolidated": null,
      "alignment": {
        "motivation_strength_alignment": 0.86,
        "weakness_error_alignment": 0.9,
        "overall_alignment": 0.88,
        "explanation": {
          "strength": "Both reviews emphasize the same core contributions: using internal neuron activation patterns as unsupervised signals for answer selection, NAD’s conceptual simplicity and label-free nature, its suitability for open-ended/code tasks where majority voting is hard, and early-stopping–driven efficiency gains. The AI review adds extra strengths around breadth of empirical evaluation, detailed methodological exposition, and ablations that the human review does not foreground, but these are secondary rather than contradictory.",
          "weakness": "Both reviews stress similar central weaknesses: limited model/task diversity and external validity, missing or weak comparisons to strong confidence/entropy-based and other decoding baselines, insufficiently justified claims that neuron patterns add value beyond entropy, sensitivity to hyperparameters and design choices, lack of full statistical rigor, and incomplete treatment of computational overhead. The AI review introduces additional, more granular concerns (e.g., cross-table inconsistencies, missing citations, specific hyperparameter reporting and similarity-metric choices), but these extend rather than conflict with the human review’s critiques.",
          "overall": "Taken together, the two reviews deliver a closely aligned substantive judgment: NAD is a novel and promising internal-signal decoding scheme with compelling efficiency benefits, but the current evidence is not yet fully convincing as a general, robust principle due to baseline, generalization, and rigor gaps. Differences lie mainly in the AI review’s greater level of detail and additional specific issues, while the overarching evaluation and focus are highly consistent."
        }
      },
      "generated_at": "2025-12-27T19:53:15"
    }
  ]
}