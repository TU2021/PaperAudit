OpenReview.net
Search OpenReview...
Login
back arrowGo to ICLR 2026 Conference homepage
Do LLMs Signal When They＊re Right? Evidence from Neuron Agreement
Download PDF
ICLR 2026 Conference Submission25050 Authors
20 Sept 2025 (modified: 24 Dec 2025)
ICLR 2026 Conference Submission
Everyone
Revisions
BibTeX
CC BY 4.0
Keywords: Neuron-Agreement Decoding (NAD); Neuron activation patterns; Unsupervised answer selection; Chain-of-thought ensembling; Token efficiency
Abstract:
Large language models (LLMs) commonly boost reasoning via sample-evaluate-ensemble decoders (e.g., majority voting), achieving label free gains without ground truth. However, prevailing strategies score candidates using only external outputs such as token probabilities, entropies, or self evaluations, and these signals can be poorly calibrated after post training. We instead analyze internal behavior based on neuron activations and uncover three findings: (1) external signals are low dimensional projections of richer internal dynamics; (2) correct responses activate substantially fewer unique neurons than incorrect ones throughout generation; and (3) activations from correct responses exhibit stronger cross sample agreement, whereas incorrect ones diverge. Motivated by these observations, we propose Neuron Agreement Decoding (NAD), an unsupervised best of N method that selects candidates using activation sparsity and cross sample neuron agreement, operating solely on internal signals and without requiring comparable textual outputs. NAD enables early correctness prediction within the first 32 generated tokens and supports aggressive early stopping. Across math and science benchmarks with verifiable answers, NAD matches majority voting; on open ended coding benchmarks where majority voting is inapplicable, NAD consistently outperforms Avg@64. By pruning unpromising trajectories early, NAD reduces token usage by 99% with minimal loss in generation quality, showing that internal signals provide reliable, scalable, and efficient guidance for label free ensemble decoding.

Primary Area: interpretability and explainable AI
Submission Number: 25050
Filter by reply type...
Filter by author...
Search keywords...

Sort: Newest First
9 / 9 replies shown
Global Revision Summary
Official Commentby Authors04 Dec 2025, 19:53 (modified: 04 Dec 2025, 20:00)EveryoneRevisions
Comment:
Dear reviewers,

Thank you again for your detailed and thoughtful feedback. Your suggestions have helped us significantly improve both the experiments and the overall presentation of the paper. Below is a concise summary of the key changes we've made in response to your reviews.

Additional Experiments and Analyses
1. Evaluation on Larger Models and Varied Difficulty Levels [Reviewers: gDoM, nzaY]

Experiments: We extended our analysis to larger-scale models (Qwen2.5-72B-Instruct) and included both easier and more challenging math datasets (HMMT25 and MATH500-L1).

Results: NAD consistently shows strong performance even with larger models and across a wider range of dataset difficulties. The improvements hold across math, science, and coding tasks, confirming robustness.

Action: We added these results into the main paper (Tables 4每5).

2. Stronger and More Comprehensive Baselines [Reviewers: jKnt, nzaY, NEts]

Experiments: To better position NAD, we included additional competitive baselines: Short-1@64, Self-Certainty, DeepConf, and TTRL.

Results: NAD remains competitive with, and often surpasses, these baselines〞particularly on coding tasks, where majority-voting-based methods fall short.

Action: Updated our main comparison tables (Table 1) and included TTRL in the appendix (Table 6) to clearly illustrate NAD's advantages.

3. Robustness to Different Sampling Conditions [Reviewers: nzaY, NEts]

Experiments: We tested NAD under varying sample sizes and different random seeds to ensure results aren't driven by particular sampling conditions.

Results: NAD consistently outperformed the average across different sample sizes, and performance variance across seeds was minimal, confirming reliability.

Action: Added new robustness analyses (Figure 11 and Table 8) in the appendix.

4. Sensitivity Analyses of Method Design Choices [Reviewers: gDoM, jKnt, NEts]

(a) Neuron Activation Definitions:

Experiments: Explored multiple strategies for aggregating neuron activations across tokens.

Results: Demonstrated robustness and justified our selected strategy clearly.

Action: Clarified this choice in the main text and appendix (Figure 7).

(b) Temperature Settings:

Experiments: Varied sampling temperatures to assess NAD＊s stability.

Results: NAD maintained strong performance across all tested temperatures.

Action: Included temperature sensitivity analysis in Table 7.

(c) Early Stopping Positions:

Experiments: Tested a wide range of early stopping positions.

Results: Showed that stopping after only a small number of tokens (e.g., 32 tokens) achieves a strong accuracy-efficiency trade-off.

Action: Clarified this in figures and tables (Figures 8每10, Tables 2每3).

5. Computational Efficiency and Practical Considerations [Reviewers: jKnt, NEts]

Experiments: Measured computational overhead, memory usage, and runtime of NAD.

Results: Demonstrated practical feasibility with modest overhead.

Action: Included these analyses in a dedicated section (Appendix E).

Clarifications and Presentation Improvements
Clarified Early-Correctness Claims and OOD Validity [Reviewers: jKnt, NEts, gDoM]

Clearly defined early correctness prediction and validated its effectiveness across datasets not used for internal statistics.
Enhanced Related Work, Figures, and Terminology [Reviewers: nzaY, NEts]

Improved literature coverage, corrected minor errors, improved dataset descriptions, and clarified figure presentations for readability.
Official Review of Submission25050 by Reviewer gDoM
Official Reviewby Reviewer gDoM01 Nov 2025, 15:13 (modified: 12 Nov 2025, 18:28)EveryoneRevisions
Summary:
This paper studies internal activation patterns in LLMs and reports two empirical regularities: (i) correct generations activate fewer unique neurons than incorrect ones; and (ii) correct generations show higher cross-sample neuron-set agreement. Building on these observations, the authors introduce Neuron Agreement Decoding (NAD)

Soundness: 3: good
Presentation: 3: good
Contribution: 3: good
Strengths:
Novel internal-signal criterion: Selecting candidates via Jaccard agreement of activated-neuron sets rather than output-space agreement is intellectually novel

Computational savings: Early pruning at 32 tokens yields two orders of magnitude fewer tokens with modest accuracy impact

Method simplicity: NAD relies on inexpensive set operations over FFN activations; the MinAct variant is parameter-light, aiding adoption.

Weaknesses:
External validity to large/closed models: All results are on small/medium, open models. It is unclear whether the ※fewer-neurons-when-correct§ regularity and NAD＊s gains hold for frontier models (70B每>100B)

Definition and sensitivity of ※activated neuron set§: The operational definition depends on thresholds/top-k within layers and across chunks. Although ablations exist, a more systematic sensitivity analysis (varying k, chunk size B, layer subsets, and gating functions) would strengthen your claims.

Sampling hyperparameters: Results are reported for T=0.6, top-p=0.9; robustness to temperature/top-p and to different N would help verify the effectiveness.

Questions:
How does NAD＊s advantage change with larger models and larger N?
Could you provide a more solid theoretical analysis for your arguments?
Flag For Ethics Review: No ethics review needed.
Rating: 6: marginally above the acceptance threshold. But would not mind if paper is rejected
Confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.
Code Of Conduct: Yes
Official Comment by Authors
Official Commentby Authors04 Dec 2025, 13:55 (modified: 04 Dec 2025, 19:06)EveryoneRevisions
Comment:
W1ㄩ
We have added results for the larger Qwen2.5-72B-Instruct, which can be found in Table 4 in the Appendix. The results demonstrate that our method remains effective on larger models.

W2ㄩ
Choice of 
: We have already conducted ablation studies on 
 in Section 5.3 (Figure 7), where we also explain our reason for choosing the value of 
 (a larger 
 provides stronger discriminative power between correct and incorrect samples).

Choice of the gating function: In current mainstream LLMs (including Qwen and Llama), the gating function is uniformly SwiGLU. Therefore, our discussion of gating functions in the paper is limited to this setting.

Choice of the layer subset & choice of the chunk size 
: We believe these factors have relatively small impacts.

Using more layers contains richer information, but we will continue exploring the use of fewer layers to further reduce computation cost (e.g., using only the second-to-last or last layer).
The choice of chunk size mainly affects the top-k results within each chunk. However, since the activation values of active neurons are much larger than those of inactive neurons, using different granularities of top-k leads to only minor differences. Due to time constraints, we leave these two minor factors to future work.
W3ㄩ
We have added the corresponding results in the Appendix (Table 7),. Our method shows reasonably robust performance across different settings.

Q1ㄩ
Please refer to W1 and W3.

Q2ㄩ
Our paper, similar to many mechanistic-interpretability-related works (e.g., scaling law studies), belongs to the category of empirical analyses. More solid theoretical analysis requires further investigation.

Official Review of Submission25050 by Reviewer jKnt
Official Reviewby Reviewer jKnt01 Nov 2025, 06:13 (modified: 12 Nov 2025, 18:28)EveryoneRevisions
Summary:
Core idea: Use internal neuron activations (not output logits/entropies) to score and select reasoning traces. Two signals: (i) activation sparsity (correct traces activate fewer unique neurons) and (ii) cross-sample neuron agreement (correct traces share more similar activation sets).
Method: Neuron Agreement Decoding (NAD): build a Jaccard-similarity matrix over activated-neuron sets across sampled traces and select via kNN/medoid/DBSCAN; a MinAct variant selects the fewest-unique-neuron trajectory. Early stopping uses the same signals after the first 32 tokens (B=32) to prune low-quality paths.
Findings: On AIME24/25 & GPQA, NAD matches majority vote while enabling aggressive early stop; on code (HumanEval, MBPP, LiveCodeBench), where voting is hard, NAD beats Avg@64. Reported token reductions up to ~99% with small accuracy loss.
Soundness: 3: good
Presentation: 3: good
Contribution: 3: good
Strengths:
Insightful internal analysis: shows entropy/self-certainty are low-dimensional projections of richer activation dynamics; correct traces are sparser and more aligned across samples.
Simple selection rules: kNN/medoid/DBSCAN over Jaccard of activated-neuron sets; unsupervised and label-free at test time.
Early-stop lever: practical chunked early-stop at 32 tokens with large token savings in parallel sampling.
Weaknesses:
Positioning vs token-confidence baselines: Conceptually close to self-consistency / DeepConf (token-level confidence/entropy) but at the neuron level; however, there is no apples-to-apples comparison against DeepConf under the same sampling regime (accuracy + compute).
※Early correctness within 32 tokens§ needs clarification: Paper sets early stop at B=32 and infers quality from internal signals〞not ground-truth correctness mid-generation. Clarify how ※NAD enables early correctness prediction within the first 32 generated tokens§ is quantified and whether OOD checks were made to avoid overfitting to seen patterns.
Scope & generality: Signals are shown strongly on AIME-style math; for open-ended tasks (code), MinAct can underperform, and neuron-agreement advantages shrink〞casting doubt on broad generality (e.g., free-form scientific discovery).
Cost reporting is incomplete: Paper emphasizes token savings, but wall-clock, activation extraction overhead, pairwise Jaccard construction, and memory/storage (noted in Limitations) aren＊t benchmarked vs strong external baselines.
Baselines: Mainly Avg@64 and Cons@64; missing self-evaluate before ensemble and confidence-based (e.g., DeepConf) under matched budgets.
Questions:
Meaning of ※early correctness§: When you say ※enables early correctness prediction within the first 32 tokens§, do you mean ranking traces by internal signals at 32 tokens and later verifying with ground truth, or a calibrated correctness probability? How is this measured, and did you test OOD prompts to check robustness?
DeepConf comparison: Please provide matched-budget comparisons to DeepConf (token-level entropy pruning): final accuracy, token count, wall-clock, and memory. This will isolate the incremental value of neuron-level signals over token-level confidence.
Generalisation beyond math: Your Figure 6 suggests weaker or minimal gains (even reversals) for code. Can you evaluate on open-form science benchmarks to test whether neuron sparsity/agreement remains predictive when answers are not short-form/numeric?
Computation & storage: Please report the per-token/trace overhead of computing activation sets, building the n℅n Jaccard matrix, and memory footprint (with/without bitset compression), compared to token-confidence baselines.
Ablations: How sensitive are results to the activation thresholding (top-k per token), chunk size B=32, and the choice among kNN/medoid/DBSCAN? Could later chunks introduce noise (as hinted by Figure 8), and how does this vary by task?
Flag For Ethics Review: No ethics review needed.
Rating: 6: marginally above the acceptance threshold. But would not mind if paper is rejected
Confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.
Code Of Conduct: Yes
Official Comment by Authors
Official Commentby Authors04 Dec 2025, 13:57 (modified: 04 Dec 2025, 19:40)EveryoneRevisions
Comment:
W1ㄩ
We have added results for three additional baselines (Short-1@k, Self-Certainty, and DeepConf, a contemporaneous work of ours), as shown in Table 1. Compared with these methods, our method is competitive. Moreover, the implementations of Self-Certainty and DeepConf rely on majority voting, which cannot handle open-domain tasks such as code generation, whereas NAD can, which constitutes an advantage of our method.

W2ㄩ
The statement ※NAD enables early correctness prediction within the first 32 generated tokens§ is obtained by first generating the full sequence (so the ground truth is known), but using only the neuron features from the first 32 tokens. For OOD evaluation, our statistical patterns are derived solely from AIME24 (as described in Section 3.2), while evaluation is conducted on more datasets (e.g., GPQA, LiveCodeBench), thereby validating the generalization ability of our method.

W3ㄩ
We currently focus on reasoning tasks, but we are also very interested in whether the proposed method is equally effective for other tasks. We plan to explore this in future work. Thank you for the suggestion.

W4ㄩ
We have added the results to a table in the Appendix.E. All the above information can be efficiently maintained through a single forward pass, which does not incur significant computational overhead.

W5ㄩ
Please refer to W1.

Q1ㄩ
Please refer to W2. The description ※ranking traces by internal signals at 32 tokens and later verifying with ground truth§ is appropriate. Here, however, ※ranking§ refers to reaching consensus through structure-discovery methods (e.g., kNN) or by choosing the trace with the fewest activated neurons (MinAct). For OOD discussions, please refer to W2.

Q2ㄩ
Please refer to W1 and W4.

Q3ㄩ
Figure 6 does not refer to a reversal of the gains produced by our method (i.e., no performance degradation). Instead, it refers to the reversal between MinAct and its opposite MaxAct. A detailed explanation of this phenomenon can be found in Section 5.3 (※Relationship between neuron activation and performance§).

Q4ㄩ
Please refer to W1 and W4.

Q5ㄩ
Choice of 
: As described in the ablation studies in Section 5.3 (Figure 7), we justify our choice of 
 (a larger 
 provides stronger discriminative power between correct and incorrect samples).

Choice of chunk size 
: The chunk size mainly affects top-k results within each chunk. However, because active neurons exhibit much larger activation values than inactive ones, different granularities of top-k introduce only minor differences. Due to time constraints, we leave this as a minor factor for future exploration.

Choice of different structure discovery methods: Different structure discovery methods may be suitable for different response patterns. We leave a more systematic investigation to future work.

Official Review of Submission25050 by Reviewer nzaY
Official Reviewby Reviewer nzaY30 Oct 2025, 01:36 (modified: 12 Nov 2025, 18:28)EveryoneRevisions
Summary:
The paper investigates whether an LLM＊s neuron activations can be used to determine if its generated response is correct. The authors contrast this with prevailing methods that rely on "external" signals like token probabilities, output entropy, or model self-evaluations, which the paper argues can be poorly calibrated.

The paper shows that these external signals are effectively low-dimensional projections of richer, high-dimensional internal dynamics. The authors' analysis uncovers two key findings. Sparsity: Correct responses activate "substantially fewer unique neurons" than incorrect responses during generation. Agreement: The activation patterns from correct responses exhibit "stronger cross-sample agreement," while incorrect responses tend to diverge.

Motivated by these observations, the paper proposes a novel unsupervised method called Neuron Agreement Decoding (NAD). NAD selects the best response from a batch of 
 samples by identifying the candidate with the highest activation agreement with its peers or, alternatively, the one with the fewest activated neurons (activation sparsity).

Soundness: 3: good
Presentation: 3: good
Contribution: 2: fair
Strengths:
The paper's primary strength is its novel investigation that successfully links an LLM's internal neuron activation patterns to the external correctness of its reasoning. Specifically, looking at the number of activated neurons and how they overlap between different inputs can provide a signal for whether the answer is correct. This is pretty cool and I haven＊t seen such an exploration before.

One strength of NAD is its ability to operate without requiring comparable textual outputs, unlike majority voting. This makes it applicable to open-ended tasks like code generation, where majority voting is often inapplicable. This is an important direction for research these days.

NAD matches or outperforms the performance of majority voting on math and science benchmarks and open-ended coding benchmarks. This is pretty strong evidence that NAD can work well, and without using too much inference time (or even reducing compared to Majority voting over many samples).

Weaknesses:
One area I am quite skeptical about is whether this method works when the base model has relatively high or relatively lower accuracy on the task in the first place. The experiments right now show that NAD works in the ※middle ground§ regime, with about 50-70% accuracy. However, for high accuracy (>90%) then NAD seems to degrade performance. Similarly if the original performance is low, I can imagine that the neuron activations could be more ※random§ so that NAD method doesn＊t work.

Also the value of Avg@64 on these tasks is surprisingly high (since you are averaging over 64 outputs), which means the model is inherently very confident on these tasks. It could very well be the case that NAD only improves performance if Avg@64 is similar to Pass@1 or something. Basically the model is not very creative and only tries the same types of solutions.

There are no baselines in this paper. The paper only compares its own variants and the base models. I generally am skeptical about a paper without any other methods in the experiments. I understand that NAD is kind of a unique method, but there are a lot of test-time methods for improving reasoning these days. For example, TTRL (https://arxiv.org/abs/2504.16084) or the authors already discuss DeepConf (https://arxiv.org/abs/2508.15260). I am less interested in the 3 different clustering methods (which are basically an ablation for NAD).

Questions:
For the evaluations, although the datasets are varied, the performance is somewhat clustered. What happens when the base model has high accuracy (e.g., GSM8k)? Or low accuracy, on some harder benchmarks (e.g., Humanities Last Exam or some of the newer benchmarks with search like SealQA https://arxiv.org/abs/2506.01062). Do we see any benefit or does it also degrade?

What happens if you perform the analyses with different numbers of samples? The analysis right now is very focused on 64. However, it is not clear if this is kind of a local maximum for NAD performance or whether 32 and 128 also exhibit good performance.

Similarly, what if Avg@64 is low because the model can output a lot of wrong answers if you keep sampling. This seems like the dominant regime if we are looking forward to AGI and harder tasks. Can you say something about if NAD will work then?

How does NAD compare to other method＊s performance on the same models/benchmarks? The paper cites a few majority voting variants, or compare against TTRL or DeepConf or any of the methods the paper mentions about ※external§ rewards. These are still very valid approaches for the task at hand.

This is more minor, but there is a lot of work on decoding methods for improving model outputs. I would say it is worth citing these, and perhaps comparing against them. For example, Factuality Decoding methods also use internal signals (internal layers) to improve the output, e.g., DoLA (https://arxiv.org/abs/2309.03883) and SLED (https://arxiv.org/abs/2411.02433). I would take a look at TTRL (https://arxiv.org/abs/2504.16084) and the forward citations as well. I think currently there are only 4 papers cited in the related work section, which is quite limited.

Flag For Ethics Review: No ethics review needed.
Rating: 4: marginally below the acceptance threshold. But would not mind if paper is accepted
Confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.
Code Of Conduct: Yes
Official Comment by Authors
Official Commentby Authors04 Dec 2025, 14:02 (modified: 04 Dec 2025, 19:54)EveryoneRevisions
Comment:
W1ㄩ
We have added experiments on both simpler and more challenging datasets, as shown in the Appendix (Table 5). In both settings, our method achieves improvements over the baselines.

W2ㄩ
Please refer to W1. The Pass@1 you mentioned is likely meant to be Pass@k (since Avg@64 is the unbiased estimator of Pass@1). To demonstrate that our method remains effective even when the model is uncertain, we report the standard deviation of accuracies over 64 samples in the Appendix (Table 8). Standard deviation quantitatively captures the dispersion of accuracy across sampling runs and thus reflects the model＊s uncertainty. The results show that the model often exhibits low certainty (e.g., AIME), indicated by a relatively large standard deviation. Nevertheless, our method remains effective even under such low-certainty conditions.

W3ㄩ
We have added results for three additional baselines (Short-1@k, Self-Certainty, and DeepConf, a contemporaneous work of ours), as shown in Table 1. Compared to these methods, our approach is competitive. Moreover, because Self-Certainty and DeepConf rely on majority voting, they cannot handle open-domain tasks such as code generation, whereas NAD can, which is an advantage of our method.

Q1ㄩ
Please refer to W1. Our method is effective on both simpler and more challenging problems.

Q2ㄩ
We have added additional results to the tables in the Appendix (Table 8 ,Figure 11). Our method consistently improves performance across different sampling budgets.

Q3ㄩ
Please refer to W1 and Q1.

Q4ㄩ
Please refer to W3.

Q5ㄩ
We have added the results for TTRL in the Appendix (Table 6). In addition, TTRL incurs substantial training cost, so comparing our method with TTRL is, in some sense, an unfair comparison.

Official Review of Submission25050 by Reviewer NEts
Official Reviewby Reviewer NEts28 Oct 2025, 06:42 (modified: 12 Nov 2025, 18:28)EveryoneRevisions
Summary:
This paper proposes Neuron Agreement Decoding (NAD), an unsupervised best-of-N method that selects candidates using activation sparsity and cross sample neuron agreement. NAD is motivated by three observations:

External signals are low dimensional projections of richer internal dynamics;
Correct responses activate substantially fewer unique neurons than incorrect ones throughout generation; and
Activations from correct responses exhibit stronger cross sample agreement, whereas incorrect ones diverge
The authors claim that:

NAD enables early correctness prediction within the first 32 generated tokens and supports aggressive early stopping
NAD matches the accuracy of majority voting in math and science benchmarks and outperforms Average@64 in open-ended coding benchmarks.
NAD reduces token usage by 99% with minimal loss in generation quality
Soundness: 2: fair
Presentation: 3: good
Contribution: 2: fair
Strengths:
The paper proposes a promising method which uses mechanistic interpretability for selecting best reasoning traces in Best-of-N sampling
The early stopping analysis may be of interest to the efficient inference community.
Weaknesses:
Major Weaknesses
Preliminary claims are poorly justified
Section 3.2: The authors claim that neuron activation patterns capture structure beyond what entropy represents, citing that samples within clusters have varying entropy values. However, this conclusion is poorly justified.
First, they have shown that the number of activated neurons correlates with entropy, suggesting the clustering is partially driven by a scalar feature that entropy already captures.
Second, any high-dimensional representation will trivially contain structure that a single scalar cannot fully represent, which is not evidence of meaningful structure. The variation in entropy within clusters could simply reflect noise, measurement artifacts, or the fact that t-SNE on Jaccard distances emphasizes pattern overlap rather than distributional properties. At the moment, the more likely conclusion from Figure 3 is that one metric does not perfectly predict another.
The preliminary experiments are done with only one model (Qwen3-4B) which may not be generalizable.
Lack of motivation on the experimental setup
It is unclear why the models are selected (Qwen3-4B-thinking-0527, Qwen3-4B-Instruct-0527 and DeepSeek-R1-0528-Qwen3-8B). Is it because of the different reasoning training regime? or are there specific reasons?
Lack of baselines.
This is very critical especially because the authors claim that the method captures structure beyond what the ※external behaviors§ can, thus it is natural to expect that NAD would outperform prior works which are based on these external behaviors:
Majority-based selection: Universal Self Consistency [1]
Confidence-based selection: Self-Certainty[2], DeepConf [3], PiCSAR [4]
Length-based selection: short-1@k [5]
Lack of statistical rigor
As the paper is dealing with sampling, the authors should try to run the experiments with multiple random seeds to account for stochasticity.
Additional Suggestions
L56: Please cite the GPT-4 reports
Figure 3: Update the colorbar label (§Average Entropy§)
Section 3.2: I believe the Jaccard index is calculated pairwise among all responses across all questions. Please add that explanation in the paragraph
Figure 6, 7, and 8 are ordered awkwardly. Figures 7 and 8 are mentioned in the text earlier than Figure 6.
The model name Qwen3-4B-thinking-0527 is perhaps a typo? it should have been Qwen3-4B-thinking-2507
References
[1] Universal Self-Consistency for Large Language Model Generation
[2] Scalable Best-of-N Selection for Large Language Models via Self-Certainty
[3] Deep think with confidence
[4] PiCSAR: Probabilistic Confidence Selection And Ranking for Reasoning Chains
[5] Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM Reasoning
Questions:
Figure 2:
Have you tried separating the correct vs incorrect instances in Figure 2? The trend may differ between the two categories.
Have you tried plotting Figure 2 in log-log scale? I suspect that there is a power-law relation there, which may be interesting.
Why are the AIME24 and AIME25 reported as one task?
Why is GPQA under Math Reasoning? Which subset of GPQA did you use?
Table 2: It is rather awkward to report the total token consumption. Any particular reason why you choose to report that? I believe we are more interested in the average number of tokens saved per question (with confidence interval).
In Section 5.3 analysis of the top-k method, what is the metric used to decide the separation? You should consider using statistical test to quantify it.
Figure 8:
Is this averaged across questions? If yes, please provide the confidence interval bars.
Am I understanding it incorrectly? Because the B=16k seems to achieve the highest accuracy, which contradicts the conclusion mentioned in the text.
What should I interpret from the token consumption line in the plot?
Is there a way to automatically decide the early stopping position? If not, it seems like a difficult hyperparameter to tune.
Flag For Ethics Review: No ethics review needed.
Rating: 2: reject, not good enough
Confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.
Code Of Conduct: Yes
Official Comment by Authors
Official Commentby Authors04 Dec 2025, 14:08 (modified: 04 Dec 2025, 19:56)EveryoneRevisions
Comment:
W1:
Intuitively, model behavior is determined by all neurons, including the output distribution in the final layer, which directly determines entropy. These two indicators are therefore not independent, and we argue that the activation patterns of all neurons naturally subsume entropy; the reverse, however, does not hold.
Because the above reasoning is quite straightforward, we only conducted a simple experiment and justification: namely, that a certain projection of neuron activations is correlated with entropy, whereas the reverse is not true. Thank you for the suggestion, we will expand the discussion on this justification.
W2:
On selected models: The models we selected are all state-of-the-art, including commonly used Qwen models and the DeepSeek distillation series.

On baselines: We have added results for three additional baselines (Short-1@k, Self-Certainty, and DeepConf, a contemporaneous work of ours), as shown in Table 1. Our method is competitive compared to theirs. Moreover, Self-Certainty and DeepConf rely on majority voting and therefore cannot handle open-domain tasks such as code generation, whereas NAD can, which is an advantage of our method.

W3:
We supplemented our experiments using the bootstrap method with larger sample sizes, as shown in the Appendix (Figure 11). The results demonstrate that our method is robust.

Q1:
Regarding correctness: We have added additional results in the appendix(Figure 12). The results show that the trends for correct and incorrect samples are similar, and the difference in neuron counts further supports our Insight 2.

Regarding the power law: We tried various fitting strategies, but none led to a substantial increase in the Pearson correlation coefficient, i.e., they did not exhibit a power-law relationship. We still appreciate your suggestion.

Q2:
Since we report results on a large number of tasks and aim to present our method＊s performance on a broader range of datasets, we combine AIME24 & AIME25 for reporting.

Q3:
We apologize for the inconsistency. Our intention was to contrast these tasks with open-ended code generation. However, the current draft of the paper still uses the term math reasoning due to an oversight. We will revise the terminology in the next version of the paper to ensure consistency with standard usage. We use the most commonly adopted GPQA-Diamond subset.

Q4:
The total token cost is proportional to the average. Dividing by N = 64 gives the average number of saved tokens.

Q5:
We will include statistical significance metrics in the revised version.

Q6:
The figure shows the total token consumption across the entire task, not an expectation; therefore confidence intervals are unnecessary.
There is no contradiction. Our goal is to use as few tokens as possible while achieving high prediction accuracy, not merely the highest final accuracy. Indeed, the highest accuracy occurs at B = 16k, but token consumption is also the largest. From the curve, with 32 tokens, we already achieve ~80% accuracy, and accuracy increases only gradually afterward until reaching its maximum at 16k.
The curve simulates a token-by-token generation process. We perform early stopping at several positions: at each early-stop point, we use the currently generated tokens to predict the correctness of all future responses (not yet generated). The accuracy of this prediction corresponds to the blue curve. As the early-stop position becomes later, token consumption (red curve) increases, and accuracy generally increases as well. Nevertheless, stopping at 32 tokens already yields strong performance.
Q7:
The choice of the early-stop position currently relies on heuristic experience. As shown in Figure 8, we consider 32 tokens to be a well-balanced choice, but the figure also shows the method is not sensitive to this choice (anywhere from 32每256 works). Thank you for the suggestion, it motivates us to explore selecting the early-stop position using small-scale datasets/models as future work.

Suggestions:
Thank you for the suggestions. We have updated the paper accordingly.

About OpenReview
Hosting a Venue
All Venues
Contact
Sponsors
Donate
FAQ
Terms of Use / Privacy Policy
News
OpenReview is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the OpenReview Sponsors. ? 2025 OpenReview

Do LLMs Signal When They＊re Right? Evidence from Neuron Agreement | OpenReview