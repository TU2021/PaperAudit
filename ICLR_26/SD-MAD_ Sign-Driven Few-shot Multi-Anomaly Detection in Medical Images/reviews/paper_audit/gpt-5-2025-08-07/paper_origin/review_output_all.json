{
  "baseline_review": "Summary\n- The paper addresses few-shot medical anomaly detection when multiple anomaly categories may co-exist, moving beyond one-class normal-vs-abnormal settings (Introduction; Fig. 1(a–b)). It proposes SD-MAD, a CLIP-based framework that links each anomaly class to a small set of radiological “sign” prompts and learns inter-anomaly separation with a new anchor-based loss (Method §3.2; Eq. 3–5; Fig. 2 left). At inference, it performs automatic sign selection to discard outlier prompts via a decision-region distance criterion (Method §3.3; Def. 3.2; Remark 3.3; Eq. 6; Fig. 2 right). The model uses shift adapters inserted into selected transformer layers for both image and text encoders (Method §3.2; Eq. 1–2; Fig. 2). Three evaluation protocols are proposed: general AD (image/pixel AUROC), multi-label prediction (Hamming and subset accuracy), and category-wise AUROC (Experiments §4.1–§4.3). Results across seven datasets show competitive general AD performance (Table 1) and clear gains in multi-anomaly settings over CLIP/MedCLIP (Table 2; Table 3), supported by visualization and ablation (Fig. 3–4).Strengths\n- Bold new problem formulation for few-shot multi-anomaly medical AD\n  - The paper explicitly formalizes few-shot multi-anomaly detection with multi-label outputs c∈{0,1}^d (Method §3.1, “Few-shot medical anomaly detection with multiple anomaly categories”) and distinguishes it from traditional one-class AD (Introduction; Fig. 1(a–b)). This matters for impact and scope, aligning evaluation and modeling with clinical realities of co-existing conditions.\n  - It proposes three complementary protocols—general AD AUROC, multi-label Hamming/subset accuracy, and category-wise AUROC—to comprehensively capture performance (Experiments §4.1–§4.3). This improves evaluation rigor and task coverage.\n  - The multi-anomaly setup is instantiated on brain MRI with six categories drawn from fastMRI+ (Experiments §4.1–§4.3), grounding the new setting in a public dataset and enabling reproducible benchmarking in principle.- Inter-anomaly anchor-based loss that integrates normal/abnormal separation and category discrimination\n  - The loss augments standard CLIP alignment (Eq. 3) by positioning an anchor (normal prompt) between positives and negatives via hinge-style constraints (Remark 3.1; Eq. 4; Eq. 5). This is technically motivated by contrastive geometry and should reduce false positives across categories (Method §3.2).\n  - Fig. 2 (left) conceptually illustrates the separation role of the anchor; the formulation explicitly aggregates all negative categories (Eq. 4), promoting inter-class margins—important for multi-category discrimination.\n  - Empirically, on general AD, the approach matches or improves state-of-the-art despite not being designed for that setting (Table 1; e.g., LiverCT image-level AUROC 86.9 vs. 81.2/72.5/59.6), suggesting soundness and practical utility.- Trainable shift adapters to adapt VLMs under few-shot constraints\n  - The shift adapter introduces a light-weight two-layer MLP per selected transformer block (Eq. 1) and interpolates with frozen outputs via λ (Eq. 2), limiting overfitting risk and preserving prior knowledge (Method §3.2).\n  - The adapter is applied to four image layers (6,8,18,24) and the last text layer (Experiments §4.1 “Training details”), showing design discipline for parameter efficiency. This matters for practicality (4000 MiB GPU memory; Experiments §4.1).\n  - Ablation on λ (Fig. 3) shows robustness across a range, indicating stable training dynamics under few-shot data—important for reliability.- Inference-time sign selection to mitigate uncertain prompts\n  - The paper formalizes a criterion using D_inf and δ to filter out outlier sign prompts that violate class-region proximity (Def. 3.2; Remark 3.3; Eq. 6; Fig. 2 right). This addresses known VLM prompt uncertainty in medical images (Introduction; Method §3.3).\n  - The method produces consistent gains in multi-label prediction when adding sign selection (Table 2: e.g., slice 0 subset accuracy 60.8% vs. 34.6% w/o SS; Hamming 87.2% vs. 85.8%), evidencing effectiveness.\n  - Visualization (Fig. 4) shows improved alignment post-training but also cases of overconfident cross-category prompts, which the sign-selection rationale directly targets—improving interpretability and practical robustness.- Comprehensive experimental coverage and clear improvements on the target setting\n  - Across seven datasets, the approach remains competitive on general AD (Table 1; average pixel-level AUROC 92.5 vs. 92.2 MVFA, and notable LiverCT image-level improvement 86.9). This suggests generality.\n  - In the new multi-anomaly setting, the method outperforms CLIP/MedCLIP on multi-label metrics across slices (Table 2), which is the central target task.\n  - Category-wise AUROC shows improved averages for slice 0 and 5 (Table 3: 68.2/63.9 vs. 57.5–60.3 baselines), supporting the claim that inter-anomaly modeling helps specific-class recognition.Weaknesses\n- Limited details and reproducibility for sign/prompt generation and curation\n  - The paper states that diverse textual descriptions for each category are “generated by a large language model” (Abstract; Introduction), but provides no reproducible protocol (prompt templates, LLM identity/version, temperature, prompt count per class). No direct evidence found in the manuscript for a complete prompt list beyond a few examples (Fig. 4(c)).\n  - The potential multi-mapping of prompts to categories is acknowledged only in the limitation (Conclusion: Limitation), but criteria for mapping or de-duplication before training are unspecified. No direct evidence found in the manuscript.\n  - It is unclear how many prompts per category are finally used in each experiment and how they are split between training and inference sign selection (Method §3.2–§3.3; Experiments §4). No direct evidence found in the manuscript. This hampers reproducibility and interpretability.- Sparse baselines for the multi-anomaly setting\n  - The multi-label experiments compare only CLIP and MedCLIP (Experiments §4.3: “we only compare the baseline model CLIP and… MedCLIP”), omitting strong few-shot AD baselines adapted to multi-label classification (e.g., MVFA adaptation, one-vs-rest heads, or open-set AD variants). This limits the strength of empirical claims on the new task.\n  - No general-purpose multi-label methods (e.g., binary relevance with calibrated thresholds) are included as baselines for the category-wise AUROC protocol (Table 3). No direct evidence found in the manuscript.\n  - The claim that “previous few-shot anomaly detection methods cannot handle multi-category scenarios” (Experiments §4.3) is not empirically validated by attempted adaptations; thus superiority over a broader set remains untested.- Unclear algorithmic realization of sign selection (D_inf, δ, h)\n  - The definition of D_inf relies on a labeling function h over the text feature space (Def. 3.2; Remark 3.3), but h is not operationally defined—how is h estimated from a finite set of prompts? No direct evidence found in the manuscript.\n  - δ is defined via an infimum over other categories (Remark 3.3; Eq. 6), but the practical computation (finite-sample proxy, neighborhood choice) is unspecified. No direct evidence found in the manuscript.\n  - Fig. 2 mentions “Equation 6: D_ng > δ,” yet the algorithmic steps for computing D_inf and performing selection (e.g., graph/neighborhood construction, thresholds) are absent. This obscures the implementability of the key inference step.- Ambiguities in the inter-anomaly loss and anchor construction\n  - Remark 3.1 posits an ordering of distances between image, class prompts, and anchor (normal prompt), but the paper does not specify how the sup/inf over prompt sets is computed during training with minibatches or how it scales with many classes (Method §3.2; Eq. 4). No direct evidence found in the manuscript.\n  - The anchor is defined as the “normal” prompt feature (Method §3.2), but the normal prompt design (single vs. multiple, wording, LLM generation or hand-crafted, balancing) is unspecified. No direct evidence found in the manuscript.\n  - The loss lacks temperature/margin hyperparameters typical in contrastive objectives (Eq. 3–5), and training stability choices are not discussed; yet optimization over hinge terms can be sensitive. No direct evidence found in the manuscript.- Experimental reporting lacks statistical rigor for few-shot variability\n  - Tables 1–3 report single-number metrics without standard deviations or confidence intervals, despite 1-shot and 4-shot regimes being highly sensitive to sampling (Experiments §4.2–§4.3). No direct evidence found in the manuscript.\n  - The selection of few-shot support samples and seeds is not documented (Experiments §4.1 “Training details” omits sampling protocol). No direct evidence found in the manuscript.\n  - No cross-validation or multiple episodic evaluations are described, raising uncertainty about robustness of gains (especially Table 2 subset accuracy differences).- Dataset construction and protocol details are insufficient for replication\n  - The multi-anomaly brain MRI setup mentions six categories and using slices 0, 5, 10 (Experiments §4.1 “Dataset”) but does not report class counts, train/val/test splits, label prevalence per slice, or co-occurrence statistics. No direct evidence found in the manuscript.\n  - For general AD pixel-level evaluations, the method is combined with MVFA losses (Experiments §4.2), but the exact combination (weights, training schedule) is not described; hence fairness of comparison to MVFA is hard to assess. No direct evidence found in the manuscript.\n  - Preprocessing (image size 240×240 is stated in §4.1), but other steps (windowing for CT/MRI, normalization, data augmentation) are not described, which can affect AD performance. No direct evidence found in the manuscript.Suggestions for Improvement\n- Provide complete, reproducible details for sign/prompt generation and mapping\n  - Specify the LLM(s) used, prompt templates, sampling parameters (e.g., temperature, n), and the exact number of prompts per category; include full prompt lists or a link (Abstract; Introduction; Method §3.2–§3.3; Fig. 4(c)). This will directly address reproducibility.\n  - Describe the procedure for handling prompts that map to multiple categories (acknowledged in Conclusion: Limitation), e.g., multi-label prompts, soft assignment, or prompt deduplication criteria.\n  - Report, per experiment, how many prompts are used during training vs. inference-time sign selection and whether the sets differ, with rationale.- Broaden and strengthen multi-anomaly baselines\n  - Adapt strong few-shot AD baselines (e.g., MVFA) to multi-label classification by adding per-class decision heads or thresholded similarity and include them in Table 2–3 (Experiments §4.3).\n  - Include a simple but competitive multi-label baseline (e.g., binary relevance with calibrated thresholds against a normal anchor) using CLIP features to contextualize gains.\n  - Where claims assert that prior methods “cannot handle the multi-category scenarios,” provide empirical evidence showing attempted adaptations and their performance.- Make the sign selection algorithm concrete and implementable\n  - Replace abstract D_inf with a finite-sample estimator: e.g., distance to the nearest in-class prompt cluster or k-NN margin in text-embedding space; detail the algorithm used to compute D_inf and δ (Def. 3.2; Remark 3.3; Eq. 6; Fig. 2).\n  - Provide pseudocode for sign selection, including hyperparameters (k, thresholds), and clarify whether δ is global, per-class, or data-dependent.\n  - Report an ablation that varies the selection threshold and neighborhood size, with impact on Table 2 metrics, to show robustness.- Clarify and extend the inter-anomaly loss specification\n  - Detail how sup/inf over prompts are computed in minibatches, how negatives are sampled across categories, and provide complexity analysis (Method §3.2; Eq. 3–5; Remark 3.1).\n  - Describe the anchor prompt(s): exact text(s), whether multiple anchors are averaged, and if anchors are learned or fixed; release the anchor text used.\n  - Consider adding a temperature or margin hyperparameter to Eq. 4 and report sensitivity; include an ablation comparing L_img-text alone vs. L_anchor vs. the full objective (Table 2–3).- Improve statistical rigor for few-shot experiments\n  - Report means and standard deviations over multiple episodes/seeds for all few-shot metrics (Tables 1–3), and describe the episode construction protocol (Experiments §4.1–§4.3).\n  - Specify the few-shot support selection procedure and random seeds; where feasible, use stratified sampling to control class imbalance.\n  - Add confidence intervals to plots (e.g., Fig. 3) and provide per-episode results in the appendix to assess stability.- Expand dataset and protocol transparency for replication\n  - Provide detailed statistics for the multi-anomaly brain MRI setup: per-class counts, co-occurrence matrix, splits, and slice-specific prevalence; include a data card describing preprocessing (Experiments §4.1).\n  - For the MVFA integration on pixel-level metrics (Experiments §4.2), state exact loss weights, training schedules, and whether any components were frozen; add a control that trains MVFA with identical preprocessing and epochs for fairness.\n  - Document image normalization, windowing, and augmentations per modality (CT, MRI, OCT, X-ray, histopathology) and evaluate sensitivity if these choices materially affect performance.Score\n- Overall (10): 6 — Introduces a well-motivated multi-anomaly few-shot setting with a principled anchor-based loss and sign selection showing gains (Method §3.2–§3.3; Table 2–3), but lacks algorithmic/detail clarity and strong baselines (Experiments §4.3; Eq. 6).\n- Novelty (10): 7 — Combines inter-anomaly anchor margin (Remark 3.1; Eq. 4–5) with inference-time sign selection (Def. 3.2; Eq. 6) for multi-label medical AD, a setting clearly framed as new (Introduction; Method §3.1).\n- Technical Quality (10): 5 — Core ideas are reasonable and supported by results (Table 2–3; Fig. 4), but key components lack operational definitions and ablations (sign selection computation; anchor design; Experiments §4.3).\n- Clarity (10): 6 — High-level pipeline and equations are clear (Fig. 2; Eq. 1–8), but important implementation specifics and dataset/protocol details are missing (Experiments §4.1–§4.3; Conclusion: Limitation).\n- Confidence (5): 4 — Assessment is moderately confident based on provided sections, figures, and tables (Table 1–3; Fig. 2–4), but missing appendices and implementation details limit full verification.",
  "final_review": "Summary\n- The paper addresses few-shot medical anomaly detection when multiple anomaly categories may co-exist, extending beyond one-class normal-vs-abnormal settings (Introduction; Fig. 1(a–b)). It proposes SD-MAD, a CLIP-based framework that links each anomaly class to a small set of radiological “sign” prompts and learns inter-anomaly separation with a new anchor-based loss (Method §3.2; Eq. 3–5; Fig. 2 left). At inference, it performs automatic sign selection to discard outlier prompts via a decision-region distance criterion (Method §3.3; Def. 3.2; Remark 3.3; Eq. 6; Fig. 2 right). The model uses shift adapters inserted into selected transformer layers for both image and text encoders (Method §3.2; Eq. 1–2; Fig. 2). Three evaluation protocols are proposed: general AD (image/pixel AUROC), multi-label prediction (Hamming and subset accuracy), and category-wise AUROC (Experiments §4.1–§4.3). Results across seven datasets show competitive general AD performance (Table 1) and gains on multi-anomaly tasks over CLIP/MedCLIP in many cases (Table 2; Table 3), supported by visualization and ablation (Fig. 3–4).Strengths\n- Bold new problem formulation for few-shot multi-anomaly medical AD\n  - The paper explicitly formalizes few-shot multi-anomaly detection with multi-label outputs c∈{0,1}^d (Method §3.1, “Few-shot medical anomaly detection with multiple anomaly categories”) and distinguishes it from traditional one-class AD (Introduction; Fig. 1(a–b)). This matters for impact and scope, aligning evaluation and modeling with clinical realities of co-existing conditions.\n  - It proposes three complementary protocols—general AD AUROC, multi-label Hamming/subset accuracy, and category-wise AUROC—to comprehensively capture performance (Experiments §4.1–§4.3). This improves evaluation rigor and task coverage.\n  - The multi-anomaly setup is instantiated on brain MRI with six categories drawn from fastMRI+ (Experiments §4.1–§4.3), grounding the new setting in a public dataset and enabling reproducible benchmarking in principle.\n- Inter-anomaly anchor-based loss that integrates normal/abnormal separation and category discrimination\n  - The loss augments standard CLIP alignment (Eq. 3) by positioning an anchor (normal prompt) between positives and negatives via hinge-style constraints (Remark 3.1; Eq. 4; Eq. 5). This is technically motivated by contrastive geometry and should reduce false positives across categories (Method §3.2).\n  - Fig. 2 (left) conceptually illustrates the separation role of the anchor; the formulation explicitly aggregates all negative categories (Eq. 4), promoting inter-class margins—important for multi-category discrimination.\n  - Empirically, on general AD, the approach matches or improves state-of-the-art on several datasets (Table 1; e.g., LiverCT image-level AUROC 86.9 vs. 81.2/72.5/59.6), suggesting soundness and practical utility.\n- Trainable shift adapters to adapt VLMs under few-shot constraints\n  - The shift adapter introduces a light-weight two-layer MLP per selected transformer block (Eq. 1) and interpolates with frozen outputs via λ (Eq. 2), limiting overfitting risk and preserving prior knowledge (Method §3.2).\n  - The adapter is applied to four image layers (6,8,18,24) and the last text layer (Experiments §4.1 “Training details”), showing design discipline for parameter efficiency. This matters for practicality (4000 MiB GPU memory; Experiments §4.1).\n  - Ablation on λ (Fig. 3) shows robustness across a range, indicating stable training dynamics under few-shot data—important for reliability.\n- Inference-time sign selection to mitigate uncertain prompts\n  - The paper formalizes a criterion using D_inf and δ to filter out outlier sign prompts that violate class-region proximity (Def. 3.2; Remark 3.3; Eq. 6; Fig. 2 right). This addresses known VLM prompt uncertainty in medical images (Introduction; Method §3.3).\n  - The method often yields gains in multi-label prediction when adding sign selection (Table 2: e.g., slice 0 subset accuracy 60.8% vs. 34.6% w/o SS; Hamming 87.2% vs. 85.8%), evidencing effectiveness.\n  - Visualization (Fig. 4) shows improved alignment post-training but also cases of overconfident cross-category prompts, which the sign-selection rationale directly targets—improving interpretability and practical robustness.\n- Comprehensive experimental coverage and improvements on the target setting\n  - Across seven datasets, the approach remains competitive on general AD (Table 1; notable LiverCT image-level improvement 86.9; reported pixel-level averages are comparable to MVFA). This suggests generality.\n  - In the new multi-anomaly setting, the method outperforms CLIP/MedCLIP on multi-label metrics across slices in many entries (Table 2), which is the central target task.\n  - Category-wise AUROC shows improved averages for slice 0 and 5 (Table 3: 68.2/63.9 vs. 57.5–60.3 baselines), supporting the claim that inter-anomaly modeling helps specific-class recognition.Weaknesses\n- Limited details and reproducibility for sign/prompt generation and curation\n  - The paper states that diverse textual descriptions for each category are “generated by a large language model” (Abstract; Introduction), but provides no reproducible protocol (prompt templates, LLM identity/version, temperature, prompt count per class). No direct evidence found in the manuscript for a complete prompt list beyond a few examples (Fig. 4(c)).\n  - The potential multi-mapping of prompts to categories is acknowledged only in the limitation (Conclusion: Limitation), but criteria for mapping or de-duplication before training are unspecified. No direct evidence found in the manuscript.\n  - It is unclear how many prompts per category are finally used in each experiment and how they are split between training and inference sign selection (Method §3.2–§3.3; Experiments §4). No direct evidence found in the manuscript. This hampers reproducibility and interpretability.\n- Sparse baselines for the multi-anomaly setting\n  - The multi-label experiments compare only CLIP and MedCLIP (Experiments §4.3: “we only compare the baseline model CLIP and… MedCLIP”), omitting strong few-shot AD baselines adapted to multi-label classification (e.g., MVFA adaptation, one-vs-rest heads, or open-set AD variants). This limits the strength of empirical claims on the new task.\n  - No general-purpose multi-label methods (e.g., binary relevance with calibrated thresholds) are included as baselines for the category-wise AUROC protocol (Table 3). No direct evidence found in the manuscript.\n  - The claim that “previous few-shot anomaly detection methods cannot handle multi-category scenarios” (Experiments §4.3) is not empirically validated by attempted adaptations; thus superiority over a broader set remains untested.\n- Unclear algorithmic realization of sign selection (D_inf, δ, h)\n  - The definition of D_inf relies on a labeling function h over the text feature space (Def. 3.2; Remark 3.3), but h is not operationally defined—how is h estimated from a finite set of prompts? No direct evidence found in the manuscript.\n  - δ is defined via an infimum over other categories (Remark 3.3; Eq. 6), but the practical computation (finite-sample proxy, neighborhood choice) is unspecified. No direct evidence found in the manuscript.\n  - Fig. 2 mentions “Equation 6: D_ng > δ,” yet D_ng is not defined elsewhere and the algorithmic steps for computing D_inf and performing selection (e.g., neighborhood construction, thresholds) are absent (Fig. 2 right; Method §3.3). This obscures implementability.\n  - The scoring aggregates over retained prompts using sup for continuous scoring and inf for binary prediction (Eq. 7–8), but no justification is provided for this change in aggregation, which appears inconsistent with the intent to keep informative prompts (Method §3.3).\n  - The behavior is undefined if all prompts of a category are discarded by Eq. 6; sup/inf in Eq. 7–8 would be undefined in that case. No direct evidence found in the manuscript.\n- Ambiguities in the inter-anomaly loss and anchor construction\n  - Remark 3.1 posits an ordering of distances between image, class prompts, and anchor (normal prompt), but the paper does not specify how the sup/inf over prompt sets is computed during training with minibatches or how it scales with many classes (Method §3.2; Eq. 4). No direct evidence found in the manuscript.\n  - The anchor is defined as the “normal” prompt feature (Method §3.2), but the normal prompt design (single vs. multiple, wording, LLM generation or hand-crafted, balancing) is unspecified. No direct evidence found in the manuscript.\n  - The loss lacks temperature/margin hyperparameters typical in contrastive objectives (Eq. 3–5), and training stability choices are not discussed; yet optimization over hinge terms can be sensitive. No direct evidence found in the manuscript.\n  - Training uses cosine distance d(·,·) (Eq. 3–4), whereas inference uses cosine_similarity (Eq. 7–8); the paper does not define their exact relationship (e.g., d = 1 − cosine_similarity), making it harder to ensure consistency between training and inference.\n  - The overall loss L = L_img-text + L_anchor (Eq. 5) is unweighted in the text; no normalization/weights are reported to balance terms. No direct evidence found in the manuscript.\n- Experimental reporting lacks statistical rigor for few-shot variability\n  - Tables 1–3 report single-number metrics without standard deviations or confidence intervals, despite 1-shot and 4-shot regimes being highly sensitive to sampling (Experiments §4.2–§4.3). No direct evidence found in the manuscript.\n  - The selection of few-shot support samples and seeds is not documented (Experiments §4.1 “Training details” omits sampling protocol). No direct evidence found in the manuscript.\n  - No cross-validation or multiple episodic evaluations are described, raising uncertainty about robustness of gains (especially Table 2 subset accuracy differences).\n  - The pixel-level “Avg.” values in Table 1 appear inconsistent with the per-dataset numbers in the same table (Table 1), creating uncertainty about the reported averages.\n  - Overgeneralized statements such as “we still on average outperform other methods” (Experiments §4.2) and “our full model demonstrates superior performance” (Experiments §4.3.1) are not fully qualified by the numbers (e.g., slice 5 subset accuracy 27.3% < 29.0% without SS in Table 2).\n- Dataset construction and protocol details are insufficient for replication\n  - The multi-anomaly brain MRI setup mentions six categories and using slices 0, 5, 10 (Experiments §4.1 “Dataset”) but does not report class counts, train/val/test splits, label prevalence per slice, or co-occurrence statistics. No direct evidence found in the manuscript.\n  - For general AD pixel-level evaluations, the method is combined with MVFA losses (Experiments §4.2), but the exact combination (weights, training schedule) is not described; hence fairness of comparison to MVFA is hard to assess. No direct evidence found in the manuscript.\n  - Preprocessing (image size 240×240 is stated in §4.1), but other steps (windowing for CT/MRI, normalization, data augmentation) are not described, which can affect AD performance. No direct evidence found in the manuscript.Suggestions for Improvement\n- Provide complete, reproducible details for sign/prompt generation and mapping\n  - Specify the LLM(s) used, prompt templates, sampling parameters (e.g., temperature, n), and the exact number of prompts per category; include full prompt lists or a link (Abstract; Introduction; Method §3.2–§3.3; Fig. 4(c)). This will directly address reproducibility.\n  - Describe the procedure for handling prompts that map to multiple categories (acknowledged in Conclusion: Limitation), e.g., multi-label prompts, soft assignment, or prompt deduplication criteria.\n  - Report, per experiment, how many prompts are used during training vs. inference-time sign selection and whether the sets differ, with rationale.\n- Broaden and strengthen multi-anomaly baselines\n  - Adapt strong few-shot AD baselines (e.g., MVFA) to multi-label classification by adding per-class decision heads or thresholded similarity and include them in Table 2–3 (Experiments §4.3).\n  - Include a simple but competitive multi-label baseline (e.g., binary relevance with calibrated thresholds against a normal anchor) using CLIP features to contextualize gains.\n  - Where claims assert that prior methods “cannot handle the multi-category scenarios,” provide empirical evidence showing attempted adaptations and their performance.\n- Make the sign selection algorithm concrete and implementable\n  - Replace abstract D_inf with a finite-sample estimator: e.g., distance to the nearest in-class prompt cluster or k-NN margin in text-embedding space; detail the algorithm used to compute D_inf and δ (Def. 3.2; Remark 3.3; Eq. 6; Fig. 2).\n  - Provide pseudocode for sign selection, including hyperparameters (k, thresholds), and clarify whether δ is global, per-class, or data-dependent.\n  - Report an ablation that varies the selection threshold and neighborhood size, with impact on Table 2 metrics, to show robustness.\n  - Justify and harmonize the aggregation rules in Eq. 7–8 (sup vs. inf), or adopt a consistent “most-informative” aggregation aligned with the stated goal (Method §3.3).\n  - Specify the fallback behavior when a class’s prompt set becomes empty after Eq. 6 (e.g., revert to anchor-only decision or reinsert a default prompt), and document its effect on metrics (Eq. 6–8).\n  - Align Fig. 2’s annotation with Eq. 6 (replace undefined D_ng with D_inf and clarify the inequality) to prevent misimplementation (Fig. 2 right; Method §3.3).\n- Clarify and extend the inter-anomaly loss specification\n  - Detail how sup/inf over prompts are computed in minibatches, how negatives are sampled across categories, and provide complexity analysis (Method §3.2; Eq. 3–5; Remark 3.1).\n  - Describe the anchor prompt(s): exact text(s), whether multiple anchors are averaged, and if anchors are learned or fixed; release the anchor text used.\n  - Consider adding a temperature or margin hyperparameter to Eq. 4 and report sensitivity; include an ablation comparing L_img-text alone vs. L_anchor vs. the full objective (Table 2–3).\n  - Define and state the relationship between cosine distance and cosine similarity used across training and inference (Eq. 3–4 vs. Eq. 7–8) to ensure consistency.\n  - Report any weighting/normalization used between L_img-text and L_anchor in Eq. 5 and include a sensitivity study.\n- Improve statistical rigor for few-shot experiments\n  - Report means and standard deviations over multiple episodes/seeds for all few-shot metrics (Tables 1–3), and describe the episode construction protocol (Experiments §4.1–§4.3).\n  - Specify the few-shot support selection procedure and random seeds; where feasible, use stratified sampling to control class imbalance.\n  - Add confidence intervals to plots (e.g., Fig. 3) and provide per-episode results in the appendix to assess stability.\n  - Verify and correct the pixel-level “Avg.” values in Table 1 to match the per-dataset entries, and update any text that relies on these averages (Table 1; Experiments §4.2).\n  - Qualify claims in §4.2 and §4.3.1 to reflect exceptions (e.g., slice 5 subset accuracy in Table 2), or provide additional analyses that reconcile these discrepancies.\n- Expand dataset and protocol transparency for replication\n  - Provide detailed statistics for the multi-anomaly brain MRI setup: per-class counts, co-occurrence matrix, splits, and slice-specific prevalence; include a data card describing preprocessing (Experiments §4.1).\n  - For the MVFA integration on pixel-level metrics (Experiments §4.2), state exact loss weights, training schedules, and whether any components were frozen; add a control that trains MVFA with identical preprocessing and epochs for fairness.\n  - Document image normalization, windowing, and augmentations per modality (CT, MRI, OCT, X-ray, histopathology) and evaluate sensitivity if these choices materially affect performance.Score\n- Overall (10): 6 — Addresses a clinically relevant multi-anomaly few-shot setting with a principled anchor-based loss and an inference-time sign selection mechanism showing gains in many cases (Method §3.2–§3.3; Table 2–3), but important algorithmic and reporting details remain unclear, including Eq. 7–8 aggregation and Table 1 averages (Method §3.3; Table 1).\n- Novelty (10): 7 — Combines an inter-anomaly anchor margin (Remark 3.1; Eq. 4–5) with decision-region-based sign selection (Def. 3.2; Eq. 6) for multi-label medical AD, a setting framed as new (Introduction; Method §3.1).\n- Technical Quality (10): 5 — Sound core ideas with supportive results (Table 2–3; Fig. 4), but key components lack operational definitions and consistency (e.g., h/δ and sup vs. inf in Eq. 6–8; loss weighting in Eq. 5; Table 1 avg inconsistency) limiting reproducibility.\n- Clarity (10): 6 — The high-level pipeline and equations are accessible (Fig. 2; Eq. 1–8), yet crucial implementation specifics and some notational/figure inconsistencies (Fig. 2 right; Method §3.3; Eq. 7) reduce clarity.\n- Confidence (5): 4 — Moderate confidence based on provided sections, figures, and tables (Table 1–3; Fig. 2–4); missing methodological specifics and minor inconsistencies limit full verification.",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 6,
        "novelty": 7,
        "technical_quality": 5,
        "clarity": 6,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 6,
        "novelty": 7,
        "technical_quality": 5,
        "clarity": 6,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "Summary\n- The paper addresses few-shot medical anomaly detection when multiple anomaly categories may co-exist, extending beyond one-class normal-vs-abnormal settings (Introduction; Fig. 1(a–b)). It proposes SD-MAD, a CLIP-based framework that links each anomaly class to a small set of radiological “sign” prompts and learns inter-anomaly separation with a new anchor-based loss (Method §3.2; Eq. 3–5; Fig. 2 left). At inference, it performs automatic sign selection to discard outlier prompts via a decision-region distance criterion (Method §3.3; Def. 3.2; Remark 3.3; Eq. 6; Fig. 2 right). The model uses shift adapters inserted into selected transformer layers for both image and text encoders (Method §3.2; Eq. 1–2; Fig. 2). Three evaluation protocols are proposed: general AD (image/pixel AUROC), multi-label prediction (Hamming and subset accuracy), and category-wise AUROC (Experiments §4.1–§4.3). Results across seven datasets show competitive general AD performance (Table 1) and gains on multi-anomaly tasks over CLIP/MedCLIP in many cases (Table 2; Table 3), supported by visualization and ablation (Fig. 3–4).Strengths\n- Bold new problem formulation for few-shot multi-anomaly medical AD\n  - The paper explicitly formalizes few-shot multi-anomaly detection with multi-label outputs c∈{0,1}^d (Method §3.1, “Few-shot medical anomaly detection with multiple anomaly categories”) and distinguishes it from traditional one-class AD (Introduction; Fig. 1(a–b)). This matters for impact and scope, aligning evaluation and modeling with clinical realities of co-existing conditions.\n  - It proposes three complementary protocols—general AD AUROC, multi-label Hamming/subset accuracy, and category-wise AUROC—to comprehensively capture performance (Experiments §4.1–§4.3). This improves evaluation rigor and task coverage.\n  - The multi-anomaly setup is instantiated on brain MRI with six categories drawn from fastMRI+ (Experiments §4.1–§4.3), grounding the new setting in a public dataset and enabling reproducible benchmarking in principle.\n- Inter-anomaly anchor-based loss that integrates normal/abnormal separation and category discrimination\n  - The loss augments standard CLIP alignment (Eq. 3) by positioning an anchor (normal prompt) between positives and negatives via hinge-style constraints (Remark 3.1; Eq. 4; Eq. 5). This is technically motivated by contrastive geometry and should reduce false positives across categories (Method §3.2).\n  - Fig. 2 (left) conceptually illustrates the separation role of the anchor; the formulation explicitly aggregates all negative categories (Eq. 4), promoting inter-class margins—important for multi-category discrimination.\n  - Empirically, on general AD, the approach matches or improves state-of-the-art on several datasets (Table 1; e.g., LiverCT image-level AUROC 86.9 vs. 81.2/72.5/59.6), suggesting soundness and practical utility.\n- Trainable shift adapters to adapt VLMs under few-shot constraints\n  - The shift adapter introduces a light-weight two-layer MLP per selected transformer block (Eq. 1) and interpolates with frozen outputs via λ (Eq. 2), limiting overfitting risk and preserving prior knowledge (Method §3.2).\n  - The adapter is applied to four image layers (6,8,18,24) and the last text layer (Experiments §4.1 “Training details”), showing design discipline for parameter efficiency. This matters for practicality (4000 MiB GPU memory; Experiments §4.1).\n  - Ablation on λ (Fig. 3) shows robustness across a range, indicating stable training dynamics under few-shot data—important for reliability.\n- Inference-time sign selection to mitigate uncertain prompts\n  - The paper formalizes a criterion using D_inf and δ to filter out outlier sign prompts that violate class-region proximity (Def. 3.2; Remark 3.3; Eq. 6; Fig. 2 right). This addresses known VLM prompt uncertainty in medical images (Introduction; Method §3.3).\n  - The method often yields gains in multi-label prediction when adding sign selection (Table 2: e.g., slice 0 subset accuracy 60.8% vs. 34.6% w/o SS; Hamming 87.2% vs. 85.8%), evidencing effectiveness.\n  - Visualization (Fig. 4) shows improved alignment post-training but also cases of overconfident cross-category prompts, which the sign-selection rationale directly targets—improving interpretability and practical robustness.\n- Comprehensive experimental coverage and improvements on the target setting\n  - Across seven datasets, the approach remains competitive on general AD (Table 1; notable LiverCT image-level improvement 86.9; reported pixel-level averages are comparable to MVFA). This suggests generality.\n  - In the new multi-anomaly setting, the method outperforms CLIP/MedCLIP on multi-label metrics across slices in many entries (Table 2), which is the central target task.\n  - Category-wise AUROC shows improved averages for slice 0 and 5 (Table 3: 68.2/63.9 vs. 57.5–60.3 baselines), supporting the claim that inter-anomaly modeling helps specific-class recognition.Weaknesses\n- Limited details and reproducibility for sign/prompt generation and curation\n  - The paper states that diverse textual descriptions for each category are “generated by a large language model” (Abstract; Introduction), but provides no reproducible protocol (prompt templates, LLM identity/version, temperature, prompt count per class). No direct evidence found in the manuscript for a complete prompt list beyond a few examples (Fig. 4(c)).\n  - The potential multi-mapping of prompts to categories is acknowledged only in the limitation (Conclusion: Limitation), but criteria for mapping or de-duplication before training are unspecified. No direct evidence found in the manuscript.\n  - It is unclear how many prompts per category are finally used in each experiment and how they are split between training and inference sign selection (Method §3.2–§3.3; Experiments §4). No direct evidence found in the manuscript. This hampers reproducibility and interpretability.\n- Sparse baselines for the multi-anomaly setting\n  - The multi-label experiments compare only CLIP and MedCLIP (Experiments §4.3: “we only compare the baseline model CLIP and… MedCLIP”), omitting strong few-shot AD baselines adapted to multi-label classification (e.g., MVFA adaptation, one-vs-rest heads, or open-set AD variants). This limits the strength of empirical claims on the new task.\n  - No general-purpose multi-label methods (e.g., binary relevance with calibrated thresholds) are included as baselines for the category-wise AUROC protocol (Table 3). No direct evidence found in the manuscript.\n  - The claim that “previous few-shot anomaly detection methods cannot handle multi-category scenarios” (Experiments §4.3) is not empirically validated by attempted adaptations; thus superiority over a broader set remains untested.\n- Unclear algorithmic realization of sign selection (D_inf, δ, h)\n  - The definition of D_inf relies on a labeling function h over the text feature space (Def. 3.2; Remark 3.3), but h is not operationally defined—how is h estimated from a finite set of prompts? No direct evidence found in the manuscript.\n  - δ is defined via an infimum over other categories (Remark 3.3; Eq. 6), but the practical computation (finite-sample proxy, neighborhood choice) is unspecified. No direct evidence found in the manuscript.\n  - Fig. 2 mentions “Equation 6: D_ng > δ,” yet D_ng is not defined elsewhere and the algorithmic steps for computing D_inf and performing selection (e.g., neighborhood construction, thresholds) are absent (Fig. 2 right; Method §3.3). This obscures implementability.\n  - The scoring aggregates over retained prompts using sup for continuous scoring and inf for binary prediction (Eq. 7–8), but no justification is provided for this change in aggregation, which appears inconsistent with the intent to keep informative prompts (Method §3.3).\n  - The behavior is undefined if all prompts of a category are discarded by Eq. 6; sup/inf in Eq. 7–8 would be undefined in that case. No direct evidence found in the manuscript.\n- Ambiguities in the inter-anomaly loss and anchor construction\n  - Remark 3.1 posits an ordering of distances between image, class prompts, and anchor (normal prompt), but the paper does not specify how the sup/inf over prompt sets is computed during training with minibatches or how it scales with many classes (Method §3.2; Eq. 4). No direct evidence found in the manuscript.\n  - The anchor is defined as the “normal” prompt feature (Method §3.2), but the normal prompt design (single vs. multiple, wording, LLM generation or hand-crafted, balancing) is unspecified. No direct evidence found in the manuscript.\n  - The loss lacks temperature/margin hyperparameters typical in contrastive objectives (Eq. 3–5), and training stability choices are not discussed; yet optimization over hinge terms can be sensitive. No direct evidence found in the manuscript.\n  - Training uses cosine distance d(·,·) (Eq. 3–4), whereas inference uses cosine_similarity (Eq. 7–8); the paper does not define their exact relationship (e.g., d = 1 − cosine_similarity), making it harder to ensure consistency between training and inference.\n  - The overall loss L = L_img-text + L_anchor (Eq. 5) is unweighted in the text; no normalization/weights are reported to balance terms. No direct evidence found in the manuscript.\n- Experimental reporting lacks statistical rigor for few-shot variability\n  - Tables 1–3 report single-number metrics without standard deviations or confidence intervals, despite 1-shot and 4-shot regimes being highly sensitive to sampling (Experiments §4.2–§4.3). No direct evidence found in the manuscript.\n  - The selection of few-shot support samples and seeds is not documented (Experiments §4.1 “Training details” omits sampling protocol). No direct evidence found in the manuscript.\n  - No cross-validation or multiple episodic evaluations are described, raising uncertainty about robustness of gains (especially Table 2 subset accuracy differences).\n  - The pixel-level “Avg.” values in Table 1 appear inconsistent with the per-dataset numbers in the same table (Table 1), creating uncertainty about the reported averages.\n  - Overgeneralized statements such as “we still on average outperform other methods” (Experiments §4.2) and “our full model demonstrates superior performance” (Experiments §4.3.1) are not fully qualified by the numbers (e.g., slice 5 subset accuracy 27.3% < 29.0% without SS in Table 2).\n- Dataset construction and protocol details are insufficient for replication\n  - The multi-anomaly brain MRI setup mentions six categories and using slices 0, 5, 10 (Experiments §4.1 “Dataset”) but does not report class counts, train/val/test splits, label prevalence per slice, or co-occurrence statistics. No direct evidence found in the manuscript.\n  - For general AD pixel-level evaluations, the method is combined with MVFA losses (Experiments §4.2), but the exact combination (weights, training schedule) is not described; hence fairness of comparison to MVFA is hard to assess. No direct evidence found in the manuscript.\n  - Preprocessing (image size 240×240 is stated in §4.1), but other steps (windowing for CT/MRI, normalization, data augmentation) are not described, which can affect AD performance. No direct evidence found in the manuscript.Suggestions for Improvement\n- Provide complete, reproducible details for sign/prompt generation and mapping\n  - Specify the LLM(s) used, prompt templates, sampling parameters (e.g., temperature, n), and the exact number of prompts per category; include full prompt lists or a link (Abstract; Introduction; Method §3.2–§3.3; Fig. 4(c)). This will directly address reproducibility.\n  - Describe the procedure for handling prompts that map to multiple categories (acknowledged in Conclusion: Limitation), e.g., multi-label prompts, soft assignment, or prompt deduplication criteria.\n  - Report, per experiment, how many prompts are used during training vs. inference-time sign selection and whether the sets differ, with rationale.\n- Broaden and strengthen multi-anomaly baselines\n  - Adapt strong few-shot AD baselines (e.g., MVFA) to multi-label classification by adding per-class decision heads or thresholded similarity and include them in Table 2–3 (Experiments §4.3).\n  - Include a simple but competitive multi-label baseline (e.g., binary relevance with calibrated thresholds against a normal anchor) using CLIP features to contextualize gains.\n  - Where claims assert that prior methods “cannot handle the multi-category scenarios,” provide empirical evidence showing attempted adaptations and their performance.\n- Make the sign selection algorithm concrete and implementable\n  - Replace abstract D_inf with a finite-sample estimator: e.g., distance to the nearest in-class prompt cluster or k-NN margin in text-embedding space; detail the algorithm used to compute D_inf and δ (Def. 3.2; Remark 3.3; Eq. 6; Fig. 2).\n  - Provide pseudocode for sign selection, including hyperparameters (k, thresholds), and clarify whether δ is global, per-class, or data-dependent.\n  - Report an ablation that varies the selection threshold and neighborhood size, with impact on Table 2 metrics, to show robustness.\n  - Justify and harmonize the aggregation rules in Eq. 7–8 (sup vs. inf), or adopt a consistent “most-informative” aggregation aligned with the stated goal (Method §3.3).\n  - Specify the fallback behavior when a class’s prompt set becomes empty after Eq. 6 (e.g., revert to anchor-only decision or reinsert a default prompt), and document its effect on metrics (Eq. 6–8).\n  - Align Fig. 2’s annotation with Eq. 6 (replace undefined D_ng with D_inf and clarify the inequality) to prevent misimplementation (Fig. 2 right; Method §3.3).\n- Clarify and extend the inter-anomaly loss specification\n  - Detail how sup/inf over prompts are computed in minibatches, how negatives are sampled across categories, and provide complexity analysis (Method §3.2; Eq. 3–5; Remark 3.1).\n  - Describe the anchor prompt(s): exact text(s), whether multiple anchors are averaged, and if anchors are learned or fixed; release the anchor text used.\n  - Consider adding a temperature or margin hyperparameter to Eq. 4 and report sensitivity; include an ablation comparing L_img-text alone vs. L_anchor vs. the full objective (Table 2–3).\n  - Define and state the relationship between cosine distance and cosine similarity used across training and inference (Eq. 3–4 vs. Eq. 7–8) to ensure consistency.\n  - Report any weighting/normalization used between L_img-text and L_anchor in Eq. 5 and include a sensitivity study.\n- Improve statistical rigor for few-shot experiments\n  - Report means and standard deviations over multiple episodes/seeds for all few-shot metrics (Tables 1–3), and describe the episode construction protocol (Experiments §4.1–§4.3).\n  - Specify the few-shot support selection procedure and random seeds; where feasible, use stratified sampling to control class imbalance.\n  - Add confidence intervals to plots (e.g., Fig. 3) and provide per-episode results in the appendix to assess stability.\n  - Verify and correct the pixel-level “Avg.” values in Table 1 to match the per-dataset entries, and update any text that relies on these averages (Table 1; Experiments §4.2).\n  - Qualify claims in §4.2 and §4.3.1 to reflect exceptions (e.g., slice 5 subset accuracy in Table 2), or provide additional analyses that reconcile these discrepancies.\n- Expand dataset and protocol transparency for replication\n  - Provide detailed statistics for the multi-anomaly brain MRI setup: per-class counts, co-occurrence matrix, splits, and slice-specific prevalence; include a data card describing preprocessing (Experiments §4.1).\n  - For the MVFA integration on pixel-level metrics (Experiments §4.2), state exact loss weights, training schedules, and whether any components were frozen; add a control that trains MVFA with identical preprocessing and epochs for fairness.\n  - Document image normalization, windowing, and augmentations per modality (CT, MRI, OCT, X-ray, histopathology) and evaluate sensitivity if these choices materially affect performance.Score\n- Overall (10): 6 — Addresses a clinically relevant multi-anomaly few-shot setting with a principled anchor-based loss and an inference-time sign selection mechanism showing gains in many cases (Method §3.2–§3.3; Table 2–3), but important algorithmic and reporting details remain unclear, including Eq. 7–8 aggregation and Table 1 averages (Method §3.3; Table 1).\n- Novelty (10): 7 — Combines an inter-anomaly anchor margin (Remark 3.1; Eq. 4–5) with decision-region-based sign selection (Def. 3.2; Eq. 6) for multi-label medical AD, a setting framed as new (Introduction; Method §3.1).\n- Technical Quality (10): 5 — Sound core ideas with supportive results (Table 2–3; Fig. 4), but key components lack operational definitions and consistency (e.g., h/δ and sup vs. inf in Eq. 6–8; loss weighting in Eq. 5; Table 1 avg inconsistency) limiting reproducibility.\n- Clarity (10): 6 — The high-level pipeline and equations are accessible (Fig. 2; Eq. 1–8), yet crucial implementation specifics and some notational/figure inconsistencies (Fig. 2 right; Method §3.3; Eq. 7) reduce clarity.\n- Confidence (5): 4 — Moderate confidence based on provided sections, figures, and tables (Table 1–3; Fig. 2–4); missing methodological specifics and minor inconsistencies limit full verification."
}