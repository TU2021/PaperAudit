# Global Summary
The paper addresses few-shot medical anomaly detection when multiple anomaly categories must be distinguished. It introduces SD-MAD, a two-stage, sign-driven framework built on CLIP/MedCLIP-style vision–language alignment. Stage (i) trains with Shift Adapters and an inter-anomaly loss that uses a “normal” anchor to amplify separability among anomaly categories by aligning image features to anomaly-specific radiological sign prompts. Stage (ii) performs automatic sign selection at inference to discard outlier prompts, mitigating intra-anomaly uncertainty due to few-shot data and prompt noise. Evaluation spans three protocols: general anomaly detection (image- and pixel-level AUROC), multi-label prediction (Hamming score and subset accuracy), and category-wise AUROC for specific anomaly types. Experiments across seven medical datasets show the approach is effective, including 4-shot general AD and 1-shot multi-anomaly detection on fastMRI+ brain slices. Key quantitative outcomes include: in general AD (Table 1), pixel-level average AUROC of “92.5%” for SD-MAD versus “92.2%” (MVFA), “88.0%” (BGAD), and “77.6%” (BRA; across RESC, BrainMRI, LiverCT). Image-level AUROC per dataset includes LiverCT “86.9%” (ours) vs “81.2%” (MVFA), ChestXRay “82.7%” (ours) vs “82.0%” (MVFA), OCT “99.8%” (ours) vs “99.4%” (MVFA). In multi-label prediction (1-shot, fastMRI+ brain slices), the full model achieves Hamming scores “87.2%”, “76.5%”, “79.2%” and subset accuracies “60.8%”, “27.3%”, “21.7%” on slices 0/5/10, respectively, outperforming CLIP and MedCLIP in most entries and showing benefit from sign selection. Category-wise AUROC averages rise to “68.2%” (slice 0) and “63.9%” (slice 5) with SD-MAD; slice 10 average is “61.5%”. Explicit caveats: the method depends on the correspondence between prompts and anomaly categories; some prompts may naturally correspond to multiple anomaly types, potentially causing false predictions. Pixel-level labels are not considered for the multi-anomaly setting, and the specific LLM used to generate radiological signs is not specified.

# Abstract
- Problem: Few-shot medical anomaly detection is constrained by limited, siloed data; prior works often reduce detection to one-class (normal vs abnormal), overlooking distinctions among multiple anomaly categories.
- Approach: SD-MAD, a two-stage Sign-Driven few-shot Multi-Anomaly Detection framework.
  - Stage (i): Align radiological signs with anomaly categories to amplify inter-anomaly discrepancies using diverse textual sign descriptions (generated by a large language model; specific model not specified).
  - Stage (ii): Automatically select aligned signs during inference to mitigate underfitting and uncertain-sample issues due to limited data.
- Contribution: Propose three performance quantification protocols for multi-anomaly detection.
- Claim: Extensive experiments illustrate effectiveness. No quantitative numbers reported in Abstract.

# Introduction
- Motivation: Distinguishing among multiple medical anomaly categories (e.g., lung tumor vs pneumonia) is clinically important; few-shot AD must handle multi-category scenarios.
- Hypothesis: Different anomalies share common radiological signs yet also exhibit category-specific signs; aligning image embeddings with sign prompts can capture fine-grained inter-anomaly distinctions.
- Method overview: 
  - Align anomalies with small sets of textual “sign” descriptions and measure similarity to image features (CLIP-based).
  - Automatic prompt (sign) selection at inference to address VLM prompt uncertainty and intra-class noise.
- Evaluation protocols (three layers):
  1) Normal vs abnormal discrimination.
  2) Multi-label prediction over distinct anomaly types.
  3) Correct identification of specific anomaly types (category-wise).
- Contributions:
  1) Few-shot detector handling multiple anomaly classes via sign–category alignment.
  2) Inter- and intra-anomaly alignment: training with anomaly-specific prompts; inference prompt selection.
  3) Rigorous evaluation on seven datasets across single-class and multi-anomaly settings.
- Figures: Illustrate task differences and radiological signs in brain MRI. No new quantitative details in this section.

# Related Work
- Medical anomaly detection: Traditional AD models normal data distributions to detect deviations (often one-class; region-specific). Open-set AD begins to handle multiple anomalies but requires sufficient training data—challenging in few-shot.
- Few-shot AD: Approaches include disentangled anomaly representations, contrastive mechanisms, multi-level adaptation (MVFA), unified training-free methods (UniVAD), and CLIP-based two-stage advances (AA-CLIP). Gap: few-shot multi-anomaly detection for medical data remains underexplored.
- Vision–language models: CLIP aligns images and text; MedCLIP adapts to medical image-text. Recent works leverage CLIP for language-guided AD/segmentation. No quantitative results in this section.

# Method
- Problem formulation:
  - Few-shot medical AD: Dataset D_few = {(x_i, c_i, s_i)} with image labels c_i ∈ {0,1} and pixel masks s_i ∈ {0,1}^{h×w}; evaluate image- and pixel-level AUROC.
  - Few-shot multi-anomaly AD: Dataset Ď_few = {(x_i, c⃗_i)} with multi-label c⃗_i ∈ {0,1}^d; pixel-level labels are not considered.
- Training: Amplify inter-anomaly discrepancy.
  - Shift Adapter (for both image and text encoders):
    - Adapter output at transformer layer i: f̂_i^ada = α(W_i^2 α(W_i^1 f̂_i^in)) (Eq. 1).
    - Residual interpolation: f_i^out = λ f̂_i^out + (1−λ) f̂_i^ada (Eq. 2), with λ a hyperparameter.
    - To mitigate overfitting: adapters applied to four layers in the image encoder and one layer in the text encoder.
  - Inter-anomaly loss:
    - Baseline alignment minimizes cosine distances for positive image–text pairs: L_img-text = min_θ ∑_{i∈[1,N_c]} d(f_img^c, f_text,i^c) (Eq. 3).
    - Introduce an anchor feature f_anchor (prompt for normal images) to bound positives and negatives (Remark 3.1).
    - Define hinge-like distances relative to anchor for positives and negatives:
      - d̂_positive,i^c = max(0, d(f_img^c, f_text,i^c) − d(f_img^c, f_anchor))
      - d̂_negative,j^{c,k} = max(0, d(f_img^c, f_anchor) − d(f_img^c, f_text,j^k))
      - L_anchor = min_θ ∑ d̂_positive + ∑ d̂_negative (Eq. 4).
    - Overall loss: L = L_img-text + L_anchor (Eq. 5).
- Inference: Mitigate intra-anomaly uncertain-sample issue.
  - Sign selection:
    - Define distance from a text feature to category decision region: D_inf(f_text, c) = inf_{h(f'_text)=c, f'_text≠f_text} d(f'_text, f_text) (Def. 3.2).
    - Ideal relation: D_inf(f_text^c, c) < δ, where δ = inf_{k≠c} D_inf(f_text^c, k) (Remark 3.3).
    - New labeling function discards outlier prompts: h_new(f_text^c) = c if D_inf(f_text^c, c) < δ, else −1 (Eq. 6). δ value not specified.
  - Scoring protocols:
    - Scenario 1 (continuous scoring, no anchor): s_c(f_img) = sup_{h_new(f_text)=c} cosine_similarity(f_img, f_text) (Eq. 7); vectorize over categories.
    - Scenario 2 (binary with anchor): p_c(f_img) = 1 if I_c(f_img) > cosine_similarity(f_img, f_anchor), else 0, where I_c(f_img) = inf_{h_new(f_text)=c} cosine_similarity(f_img, f_text) (Eq. 8). Used for Hamming score and subset accuracy.
- Additional implementation details appear in Experiments. Specific LLM for prompt generation: Not specified in this section.

# Experiments
- 4.1 Experimental Setup:
  - Evaluation protocols (three):
    1) General AD: AUROC at image- and pixel-level (follows prior works).
    2) Multi-anomaly multi-label prediction: Hamming score and subset accuracy.
    3) Category-wise AUROC: per-anomaly-class AUROC; for a given type c, positive label set to 1 for images of type c, 0 otherwise.
  - Datasets:
    - General AD: BMAD benchmark (6 datasets): Brain MRI, Liver CT, retinal OCT (OCT17), Chest X-ray (ChestXray8), Digital Histopathology (HIS), and RESC (retinal edema segmentation). Image- and pixel-level metrics for BrainMRI, LiverCT, and RESC; image-level only for OCT17, ChestXray, HIS. Dataset sizes: Not specified.
    - Multi-anomaly AD: fastMRI+ brain MRI; 6 anomaly categories; evaluate slices 0, 5, and 10. Category details in Table 3; specifics in Appendix (not included here).
  - Training details:
    - Backbone: CLIP ViT-L/14; input size 240×240.
    - Shift Adapter applied to CLIP image encoder layers 6, 8, 18, and 24; text encoder last transformer layer.
    - 50 training epochs.
    - GPU memory: 4000 MiB; hardware: A100 GPU.
    - Shots: 4-shot for general AD; 1-shot for multi-anomaly experiments. Number of runs or seeds: Not specified.
- 4.2 General Few-shot Medical Anomaly Detection:
  - Comparison with MVFA (state-of-the-art few-shot medical AD), BRA, and BGAD under 4-shot setting. Pixel-level adaptation by aggregating SD-MAD inter-anomaly loss with MVFA losses.
  - Table 1 (AUROC %):
    - Image-level:
      - BrainMRI: BRA 80.6, BGAD 83.6, MVFA 92.4, Ours 91.4.
      - LiverCT: BRA 59.6, BGAD 72.5, MVFA 81.2, Ours 86.9.
      - RESC: BRA 90.9, BGAD 86.2, MVFA 96.2, Ours 95.2.
      - HIS: BRA 68.7, BGAD -, MVFA 82.7, Ours 81.6.
      - ChestXRay: BRA 75.8, BGAD -, MVFA 82.0, Ours 82.7.
      - OCT: BRA 99.0, BGAD -, MVFA 99.4, Ours 99.8.
    - Pixel-level:
      - BrainMRI: BRA 74.8, BGAD 92.7, MVFA 97.3, Ours 96.5.
      - LiverCT: BRA 71.8, BGAD 98.9, MVFA 99.7, Ours 99.5.
      - RESC: BRA 77.3, BGAD 93.8, MVFA 99.0, Ours 99.0.
      - Avg.: BRA 77.6, BGAD 88.0, MVFA 92.2, Ours 92.5.
  - Claim: Despite not being designed for general few-shot AD, SD-MAD on average outperforms other methods (noted via pixel-level Avg. “92.5%” vs MVFA “92.2%”).
- 4.3 Multi-category Few-shot Medical Anomaly Detection:
  - Baselines: CLIP and MedCLIP; prior few-shot AD methods do not natively handle multi-category scenarios.
  - 4.3.1 Multi-label Prediction (1-shot; metrics: Hamming score %, Subset accuracy %):
    - Table 2:
      - Slice 0: Hamming — CLIP 80.2, MedCLIP 77.1, Ours (no SS) 85.8, Ours (full) 87.2. Subset accuracy — CLIP 0.4, MedCLIP 18.5, Ours (no SS) 34.6, Ours (full) 60.8.
      - Slice 5: Hamming — CLIP 77.6, MedCLIP 63.5, Ours (no SS) 72.7, Ours (full) 76.5. Subset accuracy — CLIP 0.0, MedCLIP 0.0, Ours (no SS) 29.0, Ours (full) 27.3.
      - Slice 10: Hamming — CLIP 78.3, MedCLIP 73.2, Ours (no SS) 73.8, Ours (full) 79.2. Subset accuracy — CLIP 0.0, MedCLIP 1.9, Ours (no SS) 19.8, Ours (full) 21.7.
    - Claim: Full model demonstrates superior performance vs baselines; sign selection yields gains especially on slice 0.
  - 4.3.2 Category-wise AUROC (1-shot; AUROC % per class):
    - Table 3 summary (slice averages):
      - Slice 0 Avg.: CLIP 57.5, MedCLIP 50.3, Ours (no SS) 67.3, Ours (full) 68.2.
      - Slice 5 Avg.: CLIP 60.3, MedCLIP 51.6, Ours (no SS) 63.0, Ours (full) 63.9.
      - Slice 10 Avg.: CLIP 60.5, MedCLIP 47.1, Ours (no SS) 67.1, Ours (full) 61.5.
    - Selected per-category entries:
      - Slice 0 “Craniotomy”: CLIP 42.7, MedCLIP 50.0, Ours (no SS) 68.2, Ours (full) 70.9.
      - Slice 0 “Enlarged ventricles”: CLIP 65.3, MedCLIP 68.7, Ours (no SS) 62.5, Ours (full) 71.9.
      - Slice 10 “Enlarged ventricles”: CLIP 98.1, MedCLIP 40.1, Ours (no SS) 100.0, Ours (full) 70.5.
    - Observation stated by authors: Sign selection may not always improve category-wise performance; outlier prompts can fit some test samples via prior knowledge, e.g., “Enlarged ventricles.”
- 4.4 Visualization of Image-Text Similarity:
  - Heatmaps show improved alignment between abnormal images and corresponding prompts after training. Some prompts (e.g., “surgical scaring”) show high similarities across multiple categories (Craniotomy, Posttreatment change), motivating sign selection to address uncertain samples.
- 4.5 Ablation study on λ:
  - Conducted on slice 5 multi-label prediction. Figure indicates limited sensitivity to λ changes; trade-off noted: as λ increases, Hamming score may drop slightly while subset accuracy may increase. Exact λ values tested and numeric scores per λ are not specified in text.

# Conclusion
- Summary: Introduces multi-anomaly detection setting for medical images, recognizing multiple co-existing anomalies per image. Proposes SD-MAD with Shift Adapters and an inter-anomaly loss for training, plus sign selection at inference for intra-anomaly uncertainty. Adds two new evaluation protocols (multi-label prediction and category-wise AUROC) alongside general AD metrics. Extensive experiments show effectiveness across seven datasets.
- Limitation: Performance depends on accurate prompt–anomaly correspondence; some prompts inherently map to multiple anomaly types, potentially causing false predictions if ambiguity is ignored. Addressing ambiguous prompt–category mappings is planned for future work.

# References
- Key cited datasets and benchmarks: BMAD (six datasets: Brain MRI, Liver CT, OCT17, ChestXray8, HIS, RESC), fastMRI+ (brain MRI with clinical pathology annotations).
- Core models and baselines: CLIP, MedCLIP; few-shot AD baselines MVFA, BRA, BGAD; VLM-based AD advances (WinCLIP, AA-CLIP) and related trustworthiness benchmark CARES.
- Methodological references: contrastive learning (CPC, FaceNet), residual learning, and VLM prompt augmentation/selection. Specific numeric details from references are not provided in this section.