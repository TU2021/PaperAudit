Academic Integrity and Consistency Risk Report

Summary
The manuscript proposes SD-MAD for few-shot multi-anomaly detection in medical images. While the approach is clearly motivated, several high-impact inconsistencies and missing details materially affect scientific correctness, interpretability, and reproducibility. Key issues are enumerated below with explicit anchors to the manuscript.

1) Numerical inconsistency in Table 1 (pixel-level “Avg.”)
Evidence: Section 4.2; Table 1.
- The “Avg.” row under Pixel-level (AUROC(%)) is inconsistent with the per-dataset values.
  • BRA: (74.8 + 71.8 + 77.3) / 3 = 74.63, but “Avg.” shown is 77.6.
  • BGAD: (92.7 + 98.9 + 93.8) / 3 = 95.13, but “Avg.” shown is 88.0.
  • MVFA: (97.3 + 99.7 + 99.0) / 3 = 98.67, but “Avg.” shown is 92.2.
  • Ours: (96.5 + 99.5 + 99.0) / 3 = 98.33, but “Avg.” shown is 92.5.
Impact: This undermines the claimed average improvements in Section 4.2 and raises concerns about result aggregation and reporting correctness. The statement “we still on average outperform other methods” (Section 4.2) is not supported by these incorrect averages.

2) Internal inconsistency in scoring definitions for inference
Evidence: Section 3.3; Equations (7) and (8).
- Scenario 1 (continuous scoring): s_c uses the supremum (maximum) over prompts in category c: s_c(f_img) = sup_{h_new(f_text)=c} cosine_similarity(f_img, f_text). [Eq. (7)]
- Scenario 2 (binary prediction): I_c uses the infimum (minimum) over prompts in category c: I_c(f_img) = inf_{h_new(f_text)=c} cosine_similarity(f_img, f_text). [Eq. (8)]
Issue: Using sup (max) for continuous scoring but inf (min) for binary classification across the same filtered prompt set is logically inconsistent with the stated goal of “automatically select the most informative prompts” (Abstract; Section 3.3). After sign selection, relying on the minimum similarity across retained prompts will be overly conservative and can be dominated by the least informative prompt, contradicting the intent to focus on informative prompts. No justification is provided for this change from sup to inf.

3) Undefined behavior when the prompt set becomes empty after sign selection
Evidence: Section 3.3; Equation (6); Scenarios in Equations (7) and (8).
- The sign selection discards prompts when D_inf(f_text^c, c) ≥ δ (Equation 6). If all prompts for category c are discarded, both sup and inf in Equations (7) and (8) are undefined.
No direct evidence found in the manuscript specifying:
  • How empty prompt sets are handled during scoring.
  • Any fallback strategy (e.g., class-level defaults, re-insertion of anchor, or rethresholding).
Impact: This affects the correctness of inference in multi-label and category-wise evaluations and could lead to implementation-dependent results.

4) Missing or ambiguous methodological details critical to reproducibility
Evidence: Multiple sections.
- Distance vs similarity: The training loss uses cosine distance d(·,·) (Section 3.2; Equations (3) and (4)), while inference uses cosine_similarity (Section 3.3; Equations (7) and (8)). No direct evidence found in the manuscript defining the exact relationship between d and cosine_similarity (e.g., d = 1 − cosine_similarity), which is necessary to ensure the inequalities enforced in training translate consistently to inference.
- Loss weighting: The overall loss is L = L_img-text + L_anchor (Equation (5)). No direct evidence found in the manuscript on weighting coefficients or normalization that balance these terms.
- Pixel-level adaptation: Section 4.2 states “we aggregate our inter-anomaly loss with the losses of MVFA” to adapt to pixel-level scoring. No direct evidence found in the manuscript detailing:
  • Which MVFA components are used.
  • How the aggregation is implemented (weights, schedules).
  • How pixel-level maps are produced within SD-MAD (architectural changes, training targets).
- Prompt generation via LLM: Abstract and Introduction indicate prompts are generated by a large language model. No direct evidence found in the manuscript specifying:
  • Which LLM was used.
  • The generation procedure (templates, number of prompts per category, filtering criteria).
  • The exact “normal” anchor prompt used (wording and source).
Impact: These omissions materially limit reproducibility and make it difficult to assess whether reported gains are attributable to the proposed losses, the adapter placement, prompt quality, or other training details.

5) Figure-to-text mismatch in sign selection inequality
Evidence: Section 3.3; Figure 2 (right panel; Block #14).
- The figure annotates “Equation 6: D_ng > δ” (and uses a variable name D_ng not defined elsewhere). Equation (6) in text specifies keeping a prompt if D_inf(f_text^c, c) < δ and discarding otherwise.
Impact: This creates confusion about the intended selection criterion and may lead to misimplementation if the figure is followed instead of the text.

6) Notation inconsistency that can cause confusion
Evidence: Section 3.1; Equation (7).
- K is used for the number of few-shot samples in Equation set for D_few (Section 3.1). Later, in Equation (7), K is reused to denote the number of anomaly categories when forming the score vector s_c = {s_{c_i}(f_img)}_{i=1}^K.
Impact: This reuse of K for two different quantities within the same methodological section is error-prone and can cause confusion in implementation or interpretation.

7) Claims not fully supported by the presented numbers
Evidence: Section 4.2; Table 1; Section 4.3.1; Table 2.
- Section 4.2 claims “we still on average outperform other methods.” As noted, the pixel-level “Avg.” values are incorrect (Issue 1), and no image-level average is provided; thus the claim lacks support in the manuscript.
- Section 4.3.1 states “our full model demonstrates superior performance compared to the other methods” and that sign selection is effective. While improvements are seen in many entries (Table 2), the full model’s subset accuracy is lower than “Ours (no SS)” for slice 5 (27.3 vs 29.0), indicating sign selection does not consistently improve multi-label binary accuracy across slices. This nuance is not acknowledged in the corresponding discussion.
Impact: Overgeneralized claims reduce trust in the interpretation of results.

8) Reference list inconsistencies and duplications
Evidence: References (Blocks #41–#44).
- Multiple duplicated entries and numbering anomalies:
  • [10] and [11] are identical (Ding et al., 2022).
  • [12] and [13] are identical (Ding et al., 2022, INIP).
  • [23] and [24] are identical (Huang et al., CVPR 2024).
  • [44] and [45] are identical (Yao et al., CVPR 2023).
  • Standalone “11” appears (Block #42), suggesting a formatting/numbering error.
Impact: While bibliographic, such inconsistencies reflect on the rigor of manuscript preparation and can complicate verification of cited methods and baselines.

Recommendations
- Correct Table 1 averages and re-evaluate claims of average superiority in Section 4.2.
- Harmonize inference scoring definitions: justify the use of sup vs inf, or adopt a consistent aggregation aligned with “most informative” prompt selection. Consider using max across retained prompts after sign selection for both continuous and binary cases, or clearly justify the current choice.
- Specify behavior when the retained prompt set for a class is empty after sign selection.
- Provide essential methodological details:
  • Explicit formula relating cosine distance d and cosine similarity used across training/inference.
  • Loss weights for L_img-text and L_anchor; details of integration with MVFA for pixel-level evaluation.
  • Full prompt generation protocol (LLM identity, templates, number per category, filtering), and the exact normal anchor prompt text.
- Align Figure 2’s annotation with Equation (6) and fix variable naming (D_inf vs D_ng).
- Fix notation collisions (do not reuse K for different meanings).
- Clean and de-duplicate the reference list; correct numbering.

Conclusion
Substantive integrity and consistency issues were identified, most notably the miscomputed averages in Table 1, inconsistent inference scoring definitions, undefined behavior under prompt pruning, and missing methodological details crucial for reproducibility. Addressing these points is necessary to restore confidence in the reported findings and ensure the method can be reliably implemented and evaluated.