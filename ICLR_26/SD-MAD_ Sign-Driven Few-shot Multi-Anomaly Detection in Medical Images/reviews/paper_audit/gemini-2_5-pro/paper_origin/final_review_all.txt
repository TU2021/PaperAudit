1) Summary
This paper addresses the problem of few-shot medical anomaly detection, extending it from the typical one-class (normal vs. abnormal) setting to a more clinically relevant multi-anomaly detection scenario where multiple distinct anomaly types must be identified. The authors propose SD-MAD, a two-stage, sign-driven framework built upon a vision-language model. In the training stage, it uses a novel inter-anomaly loss with a "normal" anchor to amplify the distinctions between different anomaly categories, which are represented by textual descriptions of radiological signs. In the inference stage, it employs an automatic sign selection strategy to filter out noisy or ambiguous text prompts, aiming to mitigate uncertainty. The paper also introduces new evaluation protocols for this task. Experiments on seven datasets show that SD-MAD outperforms baselines on the proposed multi-anomaly task and remains competitive in the general anomaly detection setting.2) Strengths
*   **Novel and Clinically Relevant Problem Formulation:** The paper compellingly argues for and formalizes the task of few-shot *multi-anomaly* detection. This moves beyond the standard paradigm of simply distinguishing normal from abnormal, addressing a more realistic clinical need where differentiating between pathologies is critical for treatment planning.
    *   The introduction clearly motivates this shift by providing a clinical example of distinguishing a lung tumor from pneumonia, which require different interventions (Section 1, Paragraph 2).
    *   Figure 1(a) and 1(b) provide a clear conceptual visualization of the difference between the existing one-class setting and the proposed multi-anomaly setting.
    *   The problem formulation in Section 3.1 formally defines the multi-label nature of the task, where the label `c_i` is a d-dimensional vector, contrasting it with the scalar label in standard few-shot anomaly detection.*   **Technically Sound and Well-Motivated Method:** The core components of the proposed SD-MAD framework are well-justified and directly address the challenges of the new task. The two-stage approach tackles inter-anomaly and intra-anomaly issues separately but cohesively.
    *   The **Inter-anomaly Loss** (Section 3.2, Equation 4) uses a normal text prompt as an anchor, creating a triplet-like objective that explicitly learns a decision boundary to separate different anomaly classes in the embedding space. This is a more direct approach than simply maximizing negative-pair distances.
    *   The **Sign Selection** strategy at inference (Section 3.3, Equation 6) is motivated by the potential for noisy or ambiguous text prompts, a known issue in medical vision-language alignment. This is qualitatively supported by the visualization in Figure 4, which shows some prompts having high similarity to incorrect anomaly classes.
    *   The use of lightweight **Shift Adapters** (Section 3.2, Equation 1) is a standard and effective technique for fine-tuning large pre-trained models like CLIP on small datasets without catastrophic forgetting.*   **Comprehensive and Rigorous Evaluation:** The authors establish a strong evaluation framework for the newly proposed task, assessing performance from multiple perspectives. This goes beyond simply reporting a single metric and provides a more holistic understanding of the model's capabilities.
    *   The paper introduces three distinct evaluation protocols: general anomaly detection, multi-label prediction, and category-wise performance (Section 4.1). This is crucial for a new task formulation.
    *   For multi-label prediction, the use of both Hamming score and subset accuracy (Table 2) provides complementary views on performance (per-label accuracy vs. exact-match accuracy).
    *   The inclusion of experiments on the standard BMAD benchmark (Table 1) demonstrates that the method is not over-specialized and remains competitive on the traditional anomaly detection task.*   **Strong Empirical Results and Analysis:** The proposed method demonstrates significant improvements over relevant baselines on its primary task, and the paper includes valuable ablations and visualizations to support its claims.
    *   In the 1-shot multi-label prediction task, the full model achieves substantial gains over both CLIP and MedCLIP, particularly in the strict subset accuracy metric (e.g., 60.8% vs. 18.5% on slice 0, Table 2).
    *   The ablation study in Table 2 clearly disentangles the contributions of the training loss ("Ours (no SS)") and the inference-time sign selection ("Ours (full model)"), showing that both components are generally beneficial.
    *   The visualization in Figure 4 provides qualitative evidence that the proposed training method improves the alignment between image features and their corresponding text prompts compared to vanilla CLIP.3) Weaknesses
*   **Clarity and Justification of the Sign Selection Mechanism:** The inference-time sign selection is a key contribution, but its description in Section 3.3 is difficult to follow and seems to rely on unstated assumptions.
    *   The mechanism is based on a "labeling function `h_{text}(·)`" (Section 3.3, Paragraph 1), but this function is never formally defined. It appears to be the ground-truth mapping from a text prompt to its anomaly class, but this should be stated explicitly.
    *   The process described in Definition 3.2 and Equation 6 seems to operate solely on the set of text features, without leveraging any information from the few-shot image support set. This makes the selection static and not adapted to the specific few-shot task instance, which feels like a missed opportunity.
    *   The practical computation of `D_inf` and the threshold `δ` is not detailed, making it hard to understand how the filtering is implemented and to reproduce it.*   **Inconsistent Performance of Sign Selection:** While the sign selection strategy shows clear benefits on some metrics, its impact is mixed and sometimes negative on others, which is not fully addressed.
    *   In the multi-label prediction task, the full model with sign selection underperforms the model without it on subset accuracy for slice 5 (27.3% vs. 29.0%, Table 2). This contradicts the general claim of effectiveness for the inference strategy.
    *   In the category-wise evaluation (Table 3), the full model with sign selection underperforms the model without it on several categories and on the average AUROC for slice 10 (61.5% vs. 67.1%). For "Nonspecific lesion" on slice 0, the score drops from 65.1% to 56.7%.
    *   The paper's explanation that "the outlier prompt may fit some images in the test samples" (Section 4.3.2) is a plausible post-hoc hypothesis but also suggests that the selection criterion is not robust and may be overly aggressive, discarding genuinely useful signs.*   **Methodological Ambiguity in Handling Shared Prompts:** The mathematical formulation appears inconsistent with the data structure shown in the experiments, creating a critical ambiguity.
    *   Figure 4(c) explicitly shows that the text prompt "brain after surgical resection" is used for two distinct anomaly categories: "Craniotomy" and "Posttreatment change".
    *   However, the formulation of the inter-anomaly loss (Equation 4) relies on partitioning prompts into disjoint sets for each category by summing over negative classes `k ≠ c`. The manuscript provides no explanation for how prompts shared across multiple ground-truth categories are handled by this loss function.
    *   Similarly, the sign selection mechanism (Definition 3.2, Remark 3.3) assumes a single, unique class label `c` for each text feature, which is contradicted by the data example in Figure 4(c).*   **Insufficient Detail on Dataset and Prompt Engineering:** The paper introduces a new multi-anomaly benchmark derived from fastMRI+, which is central to its main claims. However, crucial details regarding its construction and the creation of the textual prompts are sparse.
    *   The paper states, "More details can be viewed in the Appendix" (Section 4.1), but no appendix is provided in the manuscript. This omits key information needed for reproducibility, such as the number of normal/abnormal samples, the distribution of anomalies per image, and the train/test splits.
    *   The process for generating the textual "sign" prompts is not described in the main methodology. While the abstract mentions they were "generated by a large language model" (Abstract, Paragraph 2), this detail is missing from the experimental setup and no information is given on the specific model or process used.*   **Vague Description of Pixel-Level Adaptation:** For the pixel-level anomaly detection task, the paper states that the method is combined with a prior work, MVFA, by aggregating their losses. This description is too high-level to be informative.
    *   The statement "we aggregate our inter-anomaly loss with the losses of MVFA" (Section 4.2) is ambiguous. It does not specify how the losses are weighted or combined, nor does it explain how the proposed image-level framework contributes to generating pixel-level segmentation masks.
    *   This lack of detail makes it difficult to attribute the strong pixel-level performance reported in Table 1 directly to the novel contributions of this paper, as the heavy lifting might be done by the MVFA framework.4) Suggestions for Improvement
*   **Clarify the Sign Selection Mechanism:** Please revise Section 3.3 to provide a more self-contained and formal description of the sign selection process.
    *   Explicitly define the labeling function `h(·)` and provide a clear, step-by-step algorithm or pseudocode for how `D_inf` and `δ` are computed from the set of text prompts for each class.
    *   Justify the design choice of performing selection based only on text features. It would strengthen the paper to discuss whether an adaptive selection mechanism that incorporates the few-shot support images was considered and why the current approach was chosen.
    *   Consider adding a small visualization in an appendix showing a 2D projection of text embeddings for a few classes, highlighting which prompts are filtered out by the selection process.*   **Investigate and Discuss Inconsistent Sign Selection Performance:** A deeper analysis and more transparent discussion of the mixed results in Tables 2 and 3 are needed to provide a balanced view of the sign selection module.
    *   Please explicitly acknowledge and discuss the case in Table 2 where sign selection harms subset accuracy.
    *   Perform an error analysis to identify specific cases where sign selection hurts performance. For example, are there certain anomaly categories whose signs are inherently more diverse, making the filtering criterion less suitable?
    *   Consider exploring a "soft" selection alternative, such as learning weights for each prompt during inference based on its similarity to the test image, rather than applying a hard binary filter.*   **Clarify Handling of Shared Prompts:** Please address the apparent contradiction between the method's formulation and the use of shared text prompts.
    *   Explain how prompts that belong to multiple ground-truth anomaly classes (as shown in Figure 4(c)) are handled during the computation of the inter-anomaly loss in Equation 4.
    *   Clarify how the sign selection mechanism, which assumes a single class label per prompt, is applied in this multi-label prompt scenario.
    *   If necessary, revise the mathematical formulations to explicitly account for this data structure.*   **Provide More Dataset and Prompt Generation Details:** To ensure reproducibility and allow for a fair assessment of the work, please include the missing experimental details, either in the main text or a dedicated appendix.
    *   Provide a table with statistics for the new multi-anomaly benchmark from fastMRI+, including the number of images per slice, the number of images per anomaly category, and the average number of labels per image.
    *   Describe the methodology used to generate the textual sign prompts. If an LLM was used, specify the model and the prompts given to it. If experts were involved, describe the process they followed.*   **Elaborate on Pixel-Level Adaptation:** Please expand the description in Section 4.2 to clarify how SD-MAD is integrated with MVFA for pixel-level tasks.
    *   Provide the exact mathematical formulation of the combined loss function used for the pixel-level experiments.
    *   Briefly explain the architecture of the combined model, clarifying how the image-level features and scores from SD-MAD are used to influence the pixel-level segmentation output of MVFA.5) Score
*   Overall (10): 7 — The paper introduces a novel and important problem setting with a well-motivated method, but its contributions are weakened by significant clarity issues and unaddressed inconsistencies in results.
*   Novelty (10): 9 — The formulation of few-shot multi-anomaly detection is highly novel, and the proposed method is a creative synthesis of existing ideas for this new task.
*   Technical Quality (10): 6 — The core training method is sound, but the sign selection component has unaddressed negative results (Table 2, Table 3) and a critical methodological ambiguity regarding shared prompts (Figure 4c, Equation 4).
*   Clarity (10): 6 — The paper is generally well-written, but key details regarding the sign selection mechanism (Section 3.3), dataset construction (Section 4.1), pixel-level adaptation (Section 4.2), and handling of shared prompts are vague or missing.
*   Confidence (5): 5 — I am highly confident in my assessment, as it is based on a thorough reading of the manuscript and its presented evidence.