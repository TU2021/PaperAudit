Based on a critical review of the manuscript, several significant internal inconsistencies and methodological ambiguities have been identified that affect the paper's scientific validity and trustworthiness.

### Integrity Risk / Inconsistency Report

**1. Contradictory Results Regarding the Efficacy of the Proposed "Sign Selection" Module**

The paper proposes a "Sign Selection" (SS) mechanism as a key contribution to "mitigate the effects of underfitting and uncertain-sample issues" (Abstract, Block #2) and "mitigate misleading matches" (Introduction, Block #4). However, the experimental results presented in the paper directly contradict this claim of universal benefit.

*   **Evidence 1 (Multi-label Prediction):** In Table 2, for "slice 5", the model *without* Sign Selection ("Ours (no SS)") achieves a higher Subset Accuracy (29.0%) than the full model *with* Sign Selection (27.3%). The accompanying text (Block #32) claims these results "illustrate the effectiveness of our inference strategy," which is an inaccurate summary of the data presented.
*   **Evidence 2 (Category-wise AUROC):** In Table 3, for "slice 10", the model *without* Sign Selection achieves a substantially higher average AUROC (67.1%) than the full model (61.5%). The performance on specific categories drops dramatically with Sign Selection (e.g., "Enlarged ventricles" drops from 100% to 70.5%; "Nonspecific lesion" drops from 71.3% to 58.0%).

**Impact:** The data shows that a core component of the proposed method can significantly harm performance under certain evaluation protocols. The claims in the Abstract and Introduction regarding the benefits of this module are not fully supported by the paper's own results, and the discussion of Table 2 overlooks a key negative result.

**2. Methodological Ambiguity in Handling Shared Text Prompts**

The proposed method's mathematical formulation appears inconsistent with the data structure described in the experiments.

*   **Evidence:** Figure 4(c) explicitly shows that the text prompt "brain after surgical resection" is used for two distinct anomaly categories: "Craniotomy" and "Posttreatment change". However, the formulation of the inter-anomaly loss (Eq. 4) and the sign selection mechanism (Remark 3.3) relies on partitioning prompts into disjoint sets for each category (e.g., using summations over `k â‰  c`). The manuscript provides no explanation for how prompts shared across multiple "ground truth" categories are handled by the proposed loss function or inference strategy.

**Impact:** This is a critical methodological omission. The method as described is not equipped to handle the data as described, calling into question how the experiments were actually implemented and whether the mathematical model accurately reflects the experimental procedure.

**3. Lack of Transparency in "General Anomaly Detection" Experiments**

The evaluation under the "general few-shot anomaly detection" setting (Section 4.2) lacks crucial details, making the results difficult to interpret and potentially misleading.

*   **Evidence 1 (Pixel-level Evaluation):** The authors state, "To adapt our method to pixel-level score, we combine our method with MVFA. Specifically, we aggregate our inter-anomaly loss with the losses of MVFA" (Block #30). However, the results in Table 1 are presented under the column "Ours," directly comparing this hybrid model ("MVFA + Our Loss") against the baseline MVFA. This is not a direct comparison of the proposed SD-MAD framework against MVFA.
*   **Evidence 2 (Image-level Evaluation):** The paper does not specify how the multi-class scores generated by the proposed method are converted into a single binary anomaly score required for the general AUROC evaluation in Table 1.

**Impact:** This lack of clarity obscures the true performance of the proposed method in isolation. The pixel-level results do not evaluate the proposed framework but rather a modification of a different baseline method, and the image-level evaluation is not reproducible without knowing the score aggregation method.

**4. Inconsistency Between Visualization and Accompanying Text**

There is a factual mismatch between the description of the visualization in Figure 4 and the figure's content.

*   **Evidence:** The discussion of Figure 4b in Section 4.4 (Block #34) refers to the text characteristic "surgical scaring". However, the list of prompts provided in Figure 4(c) does not contain this phrase. The heatmap in the figure (Block #39) appears to contain more prompts than are listed in Figure 4(c), indicating the provided list is incomplete or incorrect.

**Impact:** While a minor issue compared to the others, this inconsistency points to a lack of thoroughness in the manuscript's preparation and reduces confidence in the presented evidence.

**Conclusion:**

The manuscript contains significant internal inconsistencies, particularly between the claims made about the "Sign Selection" module and the results presented in Tables 2 and 3. Furthermore, a critical methodological ambiguity exists regarding the handling of shared text prompts, and the experimental setup for the general anomaly detection task lacks transparency. These issues materially affect the scientific validity and trustworthiness of the paper's contributions.