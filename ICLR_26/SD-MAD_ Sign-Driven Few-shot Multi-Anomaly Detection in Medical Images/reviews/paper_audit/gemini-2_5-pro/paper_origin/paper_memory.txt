# Global Summary
This paper introduces SD-MAD, a framework for few-shot multi-anomaly detection in medical images, addressing the limitation of prior work that typically treats anomaly detection as a one-class (normal vs. abnormal) problem. The core idea is to leverage vision-language models (VLMs) by aligning image features with detailed textual descriptions of radiological "signs" for multiple anomaly categories. The proposed method is a two-stage process: 1) a training stage that uses a novel "inter-anomaly loss" with a normal-class anchor to amplify distinctions between different anomaly types, implemented with trainable "shift adapters" to fine-tune a frozen CLIP model; 2) an inference stage that employs an automatic "sign selection" strategy to filter out noisy or misleading text prompts, mitigating intra-class uncertainty. The authors propose three new evaluation protocols for this multi-anomaly task: general anomaly detection, multi-label prediction, and category-wise performance. Experiments are conducted on seven medical imaging datasets. In a 1-shot multi-label prediction task on a brain MRI dataset, the full model achieves a Hamming score of 87.2% and subset accuracy of 60.8% (on slice 0), significantly outperforming CLIP and MedCLIP baselines. The paper also notes a limitation: some textual signs can ambiguously correspond to multiple anomaly types, which is a direction for future work.

# Abstract
The paper addresses the challenge of few-shot medical anomaly detection (AD), particularly in scenarios requiring the identification of multiple distinct anomaly categories. Traditional few-shot AD methods often treat this as a one-class classification problem (normal vs. abnormal). The proposed framework, SD-MAD (Sign-Driven few-shot Multi-Anomaly Detection), is a two-stage approach that leverages vision-language models (VLMs). It uses diverse textual descriptions of radiological signs for each anomaly category, generated by a large language model. The first stage aligns these signs with anomaly categories by amplifying inter-anomaly discrepancies. The second stage, during inference, uses an automatic sign selection strategy to select the most relevant signs, mitigating underfitting and uncertainty issues from limited data. The authors also introduce three new evaluation protocols to quantify multi-anomaly detection performance and demonstrate the method's effectiveness through extensive experiments.

# Introduction
- The paper motivates the need for multi-category medical anomaly detection, as different anomalies (e.g., lung tumor vs. pneumonia) require distinct clinical responses. Existing few-shot AD methods typically perform one-class classification (anomaly vs. not).
- The core hypothesis is that different medical anomalies may share common radiological signs while also having unique, category-specific signs.
- The proposed solution has two main components:
    1.  **Inter-anomaly alignment:** During training, the framework aligns visual embeddings with sets of textual "sign" prompts specific to each anomaly class to learn fine-grained distinctions.
    2.  **Intra-anomaly mitigation:** At inference, an automatic selection of the most informative prompts is performed to reduce misleading matches within the same anomaly class, addressing uncertainty in vision-language alignment.
- The paper introduces a three-layered evaluation protocol for multi-category medical AD:
    1.  Distinguishing between normal and abnormal instances.
    2.  Performing multi-label prediction across distinct anomaly types.
    3.  Correctly identifying specific types of anomalies.
- The main contributions are: (1) a framework for few-shot multi-anomaly detection, (2) a method for inter- and intra-anomaly alignment, and (3) a rigorous evaluation protocol on seven medical datasets.

# Related Work
- **Medical Anomaly detection:** Traditional methods often model the normal data distribution and treat AD as a one-class classification problem. Open-set AD methods can handle multiple anomalies but require more training data than is available in few-shot settings.
- **Few-shot Anomaly detection:** Previous works have used techniques like disentangled representations or contrastive learning. VLM-based methods like MVFA, UniVAD, and AA-CLIP have shown good performance but are not designed for few-shot multi-anomaly detection.
- **Vision-language model:** Foundational models like CLIP and MedCLIP are noted as key technologies that enable language-guided anomaly detection, upon which recent studies have built.

# Method
- **Problem Formulation:**
    - **Few-shot medical AD:** Training data consists of K samples with image-level and pixel-level binary labels (normal/abnormal).
    - **Few-shot multi-anomaly AD:** Training data consists of K samples where each image has a d-dimensional binary label vector `c_i`, indicating the presence of `d` possible anomaly types. Pixel-level labels are not considered in this setting.
- **Training: Amplify Inter-anomaly Discrepancy:**
    - **Shift Adapter:** A lightweight, trainable module inserted into the frozen CLIP encoder to adapt it to few-shot data while preserving prior knowledge. It consists of two linear layers and its output is combined with the original transformer layer's output via linear interpolation controlled by a hyperparameter λ. Adapters are applied to the 6th, 8th, 18th, and 24th layers of the image encoder and the last layer of the text encoder.
    - **Inter-anomaly Loss:** Standard image-text contrastive loss is insufficient as it only minimizes the distance for positive pairs. The proposed loss introduces an anchor feature `f_anchor` (the text feature for "normal") to create a decision boundary. The loss function `L_anchor` enforces that the distance to positive-class text features is smaller than the distance to the anchor, which in turn is smaller than the distance to negative-class text features. The total training loss is `L = L_img-text + L_anchor`.
- **Inference: Mitigate Intra-anomaly Uncertain-sample Issue:**
    - **Sign Selection:** To handle outlier or under-fitted text prompts (signs) for a given anomaly class, an automatic selection mechanism is proposed. It works by calculating the distance of each prompt feature to the decision region of its own class. If a prompt feature is too far from others in its class (i.e., `D_inf(f_text, c) > δ`), it is discarded for the subsequent scoring step.
    - **Inference Scenarios:**
        - **Scenario 1 (Continuous scoring):** Used for AUROC. The score for an anomaly class `c` is the maximum cosine similarity between the image feature and the selected (non-discarded) text features for that class.
        - **Scenario 2 (Binary prediction):** Used for Hamming score and subset accuracy. A prediction for class `c` is positive if the minimum cosine similarity to its selected text features is greater than the cosine similarity to the anchor feature.

# Experiments
- **Experimental Setup:**
    - **Evaluation Protocols:** Three protocols are used: (1) General AD (image/pixel AUROC), (2) Multi-label prediction (Hamming score, subset accuracy), and (3) Category-wise AD (per-class AUROC).
    - **Datasets:** For general AD, 6 datasets from the BMAD benchmark are used (Brain MRI, Liver CT, RESC, OCT, Chest X-ray, HIS). For multi-anomaly AD, a dataset is created from fastMRI+ brain MRI data, with 6 anomaly categories evaluated on slices 0, 5, and 10.
    - **Training Details:** The backbone is CLIP ViT-L/14 with 240x240 input. Training runs for 50 epochs on an A100 GPU, requiring 4000 MiB of memory.
- **General Few-shot Medical Anomaly Detection:**
    - In 4-shot experiments, the proposed method is compared to BRA, BGAD, and MVFA.
    - From Table 1, the method achieves an average pixel-level AUROC of 92.5%, outperforming the SOTA MVFA (92.2%). It shows a significant improvement on LiverCT image-level AUROC (86.9% vs. 81.2% for MVFA).
- **Multi-category Few-shot Medical Anomaly Detection:**
    - Experiments are 1-shot, with CLIP and MedCLIP as baselines.
    - **Multi-label Prediction (Table 2):** The full model consistently outperforms baselines. On slice 0, it achieves a Hamming score of 87.2% and subset accuracy of 60.8%, compared to CLIP (80.2%, 0.4%) and MedCLIP (77.1%, 18.5%). The ablation "Ours (no SS)" shows the training strategy is effective, and the full model shows the benefit of sign selection (SS).
    - **Category-wise AU-ROC (Table 3):** The method improves average AUROC across all slices compared to baselines. For slice 0, the average AUROC is 68.2% vs. 57.5% for CLIP. For slice 5, it is 63.9% vs. 60.3%. For slice 10, it is 61.5% vs. 60.5%. The paper notes that sign selection does not consistently improve performance for every single category and can sometimes hurt it.
- **Visualization of Image-Text Similarity:**
    - Figure 4 shows heatmaps of image-text similarity. The proposed training method improves alignment compared to vanilla CLIP. However, it also reveals that some prompts (e.g., "surgical scaring") can have high similarity to multiple anomaly types (Craniotomy and Posttreatment change), motivating the sign selection strategy.
- **Ablation study on λ:**
    - Figure 3 shows that the model's performance is robust to the choice of the adapter interpolation hyperparameter λ. A trade-off between Hamming score and Subset accuracy is observed as λ increases, which is attributed to potential slight overfitting.

# Conclusion
- The paper introduces a new task, multi-anomaly detection in medical images, to handle scenarios with co-existing anomalies.
- It proposes SD-MAD, a VLM-based method that uses inter-anomaly loss for training and a sign selection strategy for inference to manage inter- and intra-anomaly alignment.
- Two new evaluation protocols (multi-label prediction and category-wise AUROC) are introduced for this task.
- Extensive experiments demonstrate the effectiveness of the proposed method.
- **Limitation:** A key limitation is that some text prompts can correspond to more than one anomaly type, creating ambiguity. Addressing this is identified as future work.

# References
This section contains a list of 50 references cited throughout the manuscript.