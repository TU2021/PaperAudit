1) Summary
This paper addresses the problem of few-shot medical anomaly detection, extending it from the typical one-class (normal vs. abnormal) setting to a more clinically relevant multi-anomaly detection scenario where multiple distinct anomaly types must be identified. The authors propose SD-MAD, a two-stage, sign-driven framework built upon a vision-language model. In the training stage, it uses a novel inter-anomaly loss with a "normal" anchor to amplify the distinctions between different anomaly categories, which are represented by textual descriptions of radiological signs. In the inference stage, it employs an automatic sign selection strategy to filter out noisy or ambiguous text prompts, aiming to mitigate uncertainty. The paper also introduces new evaluation protocols for this task. Experiments on seven datasets show that SD-MAD outperforms baselines on the proposed multi-anomaly task and remains competitive in the general anomaly detection setting.2) Strengths
*   **Novel and Clinically Relevant Problem Formulation:** The paper compellingly argues for and formalizes the task of few-shot *multi-anomaly* detection. This moves beyond the standard paradigm of simply distinguishing normal from abnormal, addressing a more realistic clinical need where differentiating between pathologies is critical for treatment planning.
    *   The introduction clearly motivates this shift by providing a clinical example of distinguishing a lung tumor from pneumonia, which require different interventions (Section 1, Paragraph 2).
    *   Figure 1(a) and 1(b) provide a clear conceptual visualization of the difference between the existing one-class setting and the proposed multi-anomaly setting.
    *   The problem formulation in Section 3.1 formally defines the multi-label nature of the task, where the label `c_i` is a d-dimensional vector, contrasting it with the scalar label in standard few-shot anomaly detection.*   **Technically Sound and Well-Motivated Method:** The core components of the proposed SD-MAD framework are well-justified and directly address the challenges of the new task. The two-stage approach tackles inter-anomaly and intra-anomaly issues separately but cohesively.
    *   The **Inter-anomaly Loss** (Section 3.2, Equation 4) uses a normal text prompt as an anchor, creating a triplet-like objective that explicitly learns a decision boundary to separate different anomaly classes in the embedding space. This is a more direct approach than simply maximizing negative-pair distances.
    *   The **Sign Selection** strategy at inference (Section 3.3, Equation 6) is motivated by the potential for noisy or ambiguous text prompts, a known issue in medical vision-language alignment. This is qualitatively supported by the visualization in Figure 4, which shows some prompts having high similarity to incorrect anomaly classes.
    *   The use of lightweight **Shift Adapters** (Section 3.2, Equation 1) is a standard and effective technique for fine-tuning large pre-trained models like CLIP on small datasets without catastrophic forgetting.*   **Comprehensive and Rigorous Evaluation:** The authors establish a strong evaluation framework for the newly proposed task, assessing performance from multiple perspectives. This goes beyond simply reporting a single metric and provides a more holistic understanding of the model's capabilities.
    *   The paper introduces three distinct evaluation protocols: general anomaly detection, multi-label prediction, and category-wise performance (Section 4.1). This is crucial for a new task formulation.
    *   For multi-label prediction, the use of both Hamming score and subset accuracy (Table 2) provides complementary views on performance (per-label accuracy vs. exact-match accuracy).
    *   The inclusion of experiments on the standard BMAD benchmark (Table 1) demonstrates that the method is not over-specialized and remains competitive on the traditional anomaly detection task.*   **Strong Empirical Results and Analysis:** The proposed method demonstrates significant improvements over relevant baselines on its primary task, and the paper includes valuable ablations and visualizations to support its claims.
    *   In the 1-shot multi-label prediction task, the full model achieves substantial gains over both CLIP and MedCLIP, particularly in the strict subset accuracy metric (e.g., 60.8% vs. 18.5% on slice 0, Table 2).
    *   The ablation study in Table 2 clearly disentangles the contributions of the training loss ("Ours (no SS)") and the inference-time sign selection ("Ours (full model)"), showing that both components are beneficial.
    *   The visualization in Figure 4 provides qualitative evidence that the proposed training method improves the alignment between image features and their corresponding text prompts compared to vanilla CLIP.3) Weaknesses
*   **Clarity and Justification of the Sign Selection Mechanism:** The inference-time sign selection is a key contribution, but its description in Section 3.3 is difficult to follow and seems to rely on unstated assumptions.
    *   The mechanism is based on a "labeling function `h_{text}(·)`" (Section 3.3, Paragraph 1), but this function is never formally defined. It appears to be the ground-truth mapping from a text prompt to its anomaly class, but this should be stated explicitly.
    *   The process described in Definition 3.2 and Equation 6 seems to operate solely on the set of text features, without leveraging any information from the few-shot image support set. This makes the selection static and not adapted to the specific few-shot task instance, which feels like a missed opportunity.
    *   The practical computation of `D_inf` and the threshold `δ` is not detailed, making it hard to understand how the filtering is implemented and to reproduce it.*   **Inconsistent Performance of Sign Selection:** While the sign selection strategy shows clear benefits for multi-label prediction metrics (Table 2), its impact on category-wise AUROC is mixed and sometimes negative.
    *   In Table 3, the full model with sign selection underperforms the model without it on several categories. For example, on slice 10, the average AUROC drops from 67.1% to 61.5%. For "Nonspecific lesion," the score drops from 65.1% to 56.7% on slice 0.
    *   The paper's explanation that "the outlier prompt may fit some images in the test samples" (Section 4.3.2) is a plausible post-hoc hypothesis but also suggests that the selection criterion is not robust and may be overly aggressive, discarding genuinely useful, albeit niche, radiological signs.*   **Insufficient Detail on Dataset and Prompt Engineering:** The paper introduces a new multi-anomaly benchmark derived from fastMRI+, which is central to its main claims. However, crucial details regarding its construction and the creation of the textual prompts are sparse.
    *   The paper states, "More details can be viewed in the Appendix" (Section 4.1), but no appendix is provided in the manuscript. This omits key information needed for reproducibility, such as the number of normal/abnormal samples, the distribution of anomalies per image, and the train/test splits.
    *   The process for generating the textual "sign" prompts is not described. It is unclear if they were written by clinical experts, generated by an LLM, or extracted from reports. This information is vital, as the quality and nature of the prompts directly influence the model's performance. Figure 4(c) provides examples, but not the methodology.*   **Vague Description of Pixel-Level Adaptation:** For the pixel-level anomaly detection task, the paper states that the method is combined with a prior work, MVFA, by aggregating their losses. This description is too high-level to be informative.
    *   The statement "we aggregate our inter-anomaly loss with the losses of MVFA" (Section 4.2) is ambiguous. It does not specify how the losses are weighted or combined, nor does it explain how the proposed image-level framework contributes to generating pixel-level segmentation masks.
    *   This lack of detail makes it difficult to attribute the strong pixel-level performance reported in Table 1 directly to the novel contributions of this paper, as the heavy lifting might be done by the MVFA framework.4) Suggestions for Improvement
*   **Clarify the Sign Selection Mechanism:** Please revise Section 3.3 to provide a more self-contained and formal description of the sign selection process.
    *   Explicitly define the labeling function `h(·)` and provide a clear, step-by-step algorithm or pseudocode for how `D_inf` and `δ` are computed from the set of text prompts for each class.
    *   Justify the design choice of performing selection based only on text features. It would strengthen the paper to discuss whether an adaptive selection mechanism that incorporates the few-shot support images was considered and why the current approach was chosen.
    *   Consider adding a small visualization in the appendix showing a 2D projection of text embeddings for a few classes, highlighting which prompts are filtered out by the selection process.*   **Investigate Inconsistent Sign Selection Performance:** A deeper analysis of the mixed results in Table 3 is needed to strengthen the paper's claims about the sign selection module.
    *   Perform an error analysis to identify specific cases where sign selection hurts performance. For example, are there certain anomaly categories whose signs are inherently more diverse, making the filtering criterion less suitable?
    *   Consider exploring a "soft" selection alternative, such as learning weights for each prompt during inference based on its similarity to the test image, rather than applying a hard binary filter. This might offer more robustness.*   **Provide More Dataset and Prompt Generation Details:** To ensure reproducibility and allow for a fair assessment of the work, please include the missing experimental details, either in the main text or a dedicated appendix.
    *   Provide a table with statistics for the new multi-anomaly benchmark from fastMRI+, including the number of images per slice, the number of images per anomaly category, and the average number of labels per image.
    *   Describe the methodology used to generate the textual sign prompts. If an LLM was used, specify the model and the prompts given to it. If experts were involved, describe the process they followed.*   **Elaborate on Pixel-Level Adaptation:** Please expand the description in Section 4.2 to clarify how SD-MAD is integrated with MVFA for pixel-level tasks.
    *   Provide the exact mathematical formulation of the combined loss function used for the pixel-level experiments.
    *   Briefly explain the architecture of the combined model, clarifying how the image-level features and scores from SD-MAD are used to influence the pixel-level segmentation output of MVFA.5) Score
*   Overall (10): 8 — The paper introduces a novel and important problem setting with a well-motivated method and strong results on its primary task (Table 2).
*   Novelty (10): 9 — The formulation of few-shot multi-anomaly detection is highly novel, and the proposed method combining an anchor-based loss with inference-time prompt selection is a creative approach.
*   Technical Quality (10): 7 — The core training method is sound, but the sign selection component has some unaddressed inconsistencies (Table 3) and lacks clarity in its description (Section 3.3).
*   Clarity (10): 7 — The paper is generally well-written, but key details regarding the sign selection mechanism (Section 3.3), dataset construction (Section 4.1), and pixel-level adaptation (Section 4.2) are either vague or missing.
*   Confidence (5): 5 — I am highly confident in my assessment, as it is based on a thorough reading of the manuscript and its presented evidence.