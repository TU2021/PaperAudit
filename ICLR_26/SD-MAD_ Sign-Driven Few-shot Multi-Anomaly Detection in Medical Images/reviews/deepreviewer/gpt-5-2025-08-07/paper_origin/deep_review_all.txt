Summary
The paper proposes SD-MAD, a sign-driven few-shot multi-anomaly detection framework for medical images. It augments CLIP/MedCLIP with (a) a “Shift Adapter” applied to selected transformer layers and (b) an inter-anomaly anchor loss that uses a “normal” prompt as an anchor to widen margins between anomaly categories. At inference, it introduces an automatic sign selection that filters out outlier textual prompts based on a distance criterion to reduce intra-class uncertainty. The authors also introduce three evaluation protocols: general anomaly detection (AUROC at image/pixel), multi-label prediction (Hamming score and subset accuracy), and category-wise AUROC. Experiments on BMAD datasets (general AD) and fastMRI+ brain MRI slices (multi-anomaly) show SD-MAD outperforming CLIP, MedCLIP, and competitive few-shot AD baselines in most settings.

Soundness
The overall approach—aligning anomaly-specific textual prompts to image features and enforcing inter-class margins—has a plausible rationale. The Shift Adapter (Eq. 1–2) is a conventional lightweight residual adapter and is reasonable for few-shot finetuning. The inter-anomaly loss (Eq. 3–5) uses an anchor (normal prompt) to enforce inequalities in Remark 3.1; the hinge-like formulation is coherent and likely stabilizes training by preventing false positives from irrelevant categories. However, the inference-time sign selection hinges on Definition 3.2 and Eq. 6, which rely on a labeling function and distances to “decision regions” that are not operationally defined; how D_inf and δ are computed in practice is unclear. The integration of pixel-level scores in Sec. 4.2 “by combining with MVFA losses” is underspecified, and the theoretical motivation for the anchor in prediction (Eq. 8) assumes a fixed normal boundary that may be dataset-specific. Overall, the training objective is sound, but the key selection mechanism and some evaluation adaptations need more concrete algorithms and ablations.

Presentation
The paper is generally readable and logically structured (Sec. 1–5). Figures 1–2 convey the task and pipeline; Tables 1–3 present results clearly; an ablation on λ is provided (Fig. 3). Nonetheless, several critical implementation details are missing or ambiguous: the definition and computation of D_inf and δ (Sec. 3.3), the exact number and source of signs per category and the LLM used to generate them (Sec. 1, 3.2; Fig. 4c), and training hyperparameters (optimizer, LR, batch size). There are repeated or duplicated references (e.g., [10]/[11], [23]/[24], [44]/[45]) and minor notation inconsistencies (e.g., “Equation 6: D_ng > δ” in Fig. 2 legend). Some results are surprising (subset accuracy near zero for baselines in Table 2), warranting more explanation of thresholding or decision rules.

Contribution
SD-MAD targets few-shot multi-anomaly detection with multi-label outputs, which is less explored in medical AD. The anchor-based inter-anomaly loss is a useful twist on CLIP alignment for medical anomalies, and the explicit multi-label evaluation protocols (Hamming, subset accuracy, class-wise AUROC) address practical clinical scenarios where multiple findings coexist. While prompt-based anomaly detection with VLMs is not new, the combination of anomaly-specific sign sets, anchor-margin training, and post-hoc sign selection for multi-anomaly few-shot settings is a reasonable and moderately novel contribution. The main advance is in task formulation and evaluation plus a tailored training/inference strategy rather than in strong theoretical novelty.

Strengths
- Clear identification of the gap: multi-anomaly few-shot medical AD is important and under-studied (Sec. 1; Fig. 1).
- Practical training strategy with limited finetuning via adapters preserves CLIP priors (Sec. 3.2; Eq. 1–2).
- Anchor loss that explicitly separates normal vs. anomalies and anomalies among themselves (Sec. 3.2; Remark 3.1; Eq. 4–5).
- New evaluation protocols tailored to multi-label medical settings (Sec. 4.1–4.3), with consistent improvements over CLIP/MedCLIP on fastMRI+ (Tables 2–3).
- Visualizations that diagnose prompt ambiguity motivating sign selection (Sec. 4.4; Fig. 4).

Weaknesses
- Insufficient specification of sign generation: which LLM, prompt design, number of signs per category, quality control by experts, and how signs are standardized across datasets (Sec. 1, Fig. 4c).
- The sign selection mechanism is not executable as written: D_inf and δ (Definition 3.2, Eq. 6) require a labeling function and decision regions that are undefined; no algorithmic procedure or hyperparameters are given (Sec. 3.3; Fig. 2).
- Pixel-level adaptation “combined with MVFA” lacks methodological detail (Sec. 4.2), limiting reproducibility.
- Limited scope of multi-anomaly experiments (brain MRI slices only; 1-shot), with no shot-scaling analysis, cross-organ validation, or statistical significance testing (Sec. 4.3–4.4).
- Some surprising results (e.g., subset accuracy near 0 for CLIP/MedCLIP in Table 2; perfect AUROC 100 for “Enlarged ventricles” in Table 3) without explanation of class imbalance, thresholds, or confidence intervals.
- Minor inconsistencies and reference duplications (Sec. References), and notation issues (Fig. 2 legend).

Questions
1. How exactly are signs generated and curated? Which LLM, prompt templates, number of signs per category, and were radiologists involved in validation? Provide counts per category and examples beyond Fig. 4c.
2. How is D_inf computed in practice and how is δ set in Eq. 6? Is it nearest-neighbor among sign features within a category vs. other categories, or margin to a learned classifier? Please provide the algorithm, hyperparameters, and sensitivity analysis.
3. What λ is used by default in Eq. 2? Please add optimizer, LR, batch size, temperature, and normalization details for training.
4. For pixel-level detection on BMAD, how is SD-MAD combined with MVFA? Which losses are summed, how are weights chosen, and how are pixel maps derived from text-image similarities?
5. Why is subset accuracy so low for CLIP/MedCLIP in Table 2? What thresholding/decision strategy is used? Are classes highly imbalanced? Provide per-class prevalence and confidence intervals.
6. Can you report results for different shots (e.g., 1-, 4-, 8-shot) and across more organs/modalities for multi-anomaly settings? Any cross-dataset generalization?
7. The anchor prompt is “normal”; how sensitive are results to the exact normal text? Did you test alternative normal anchors or multiple normal prompts?
8. Could ambiguous prompts be modeled as shared across categories rather than discarded? Have you tried soft weighting instead of hard filtering (Eq. 6)?

Rating
- Overall (10): 7 — Solid task framing and promising results, but key inference details (Sec. 3.3, Eq. 6) and sign generation are under-specified; Tables 2–3 and Fig. 4 support impact.
- Novelty (10): 7 — Moderately novel combination of anchors, adapters, and multi-label protocols for few-shot multi-anomaly detection; see Sec. 1–3 and evaluation in Sec. 4.3.
- Technical Quality (10): 6 — Training losses are sound (Eq. 3–5), but the sign selection and pixel-level integration lack concrete algorithms; see Sec. 3.3 and Sec. 4.2.
- Clarity (10): 6 — Clear pipeline figures and tables (Fig. 2; Tables 1–3), but missing implementation specifics and some notation/reference issues; Sec. 3.3 and References.
- Confidence (5): 4 — High-level understanding from methods and experiments, but incomplete reproducibility prevents maximum confidence; based on thorough reading of Sec. 3–4 and Tables/Figures.

Summary
The manuscript introduces SD-MAD, a CLIP-based approach for few-shot multi-anomaly detection using anomaly-specific radiological sign prompts. It proposes a Shift Adapter for limited-layer finetuning and an inter-anomaly anchor loss to separate normal and multiple anomaly categories. At inference, the method filters out outlier prompts via a distance-based sign selection to handle intra-class uncertainty. New evaluation protocols are defined for multi-label prediction and category-wise AUROC. Experiments across BMAD datasets (general AD) and fastMRI+ brain MRI slices (multi-anomaly) indicate improvements over CLIP/MedCLIP and competitive baselines.

Soundness
The adapter and anchor loss are technically reasonable: Eq. 1–2 implement a residual adapter, and Eq. 4 introduces hinge terms to enforce the inequalities in Remark 3.1, promoting margins between correct and incorrect categories using the normal anchor. However, the sign selection mechanism (Definition 3.2 and Eq. 6) depends on an undefined labeling function over text-feature “decision regions” and provides no practical computation of D_inf or δ. The integration to pixel-level detection is also vague (“aggregate our inter-anomaly loss with the losses of MVFA” without specific weighting, Sec. 4.2), which weakens methodological rigor.

Presentation
The paper is well-organized (Sec. 1–5), with helpful diagrams (Fig. 2) and clear result tables (Tables 1–3). The visualization (Fig. 4) compellingly motivates sign ambiguity. Yet several essential details are missing: number and source of prompts per category, LLM used and curation, computation of D_inf/δ, thresholds for binary decisions (Eq. 8), optimizer and training hyperparameters, and how pixel-level maps are produced. Minor inconsistencies exist (e.g., “Equation 6: D_ng > δ” in Fig. 2; duplicated references like [23]/[24], [44]/[45]).

Contribution
The work’s main contribution is in task formulation and evaluation for multi-label few-shot medical AD, plus a practical anchor-based training loss and post-hoc prompt selection. Given the prevalence of multiple coexisting findings in clinical images, this is relevant. While prompt-based AD with CLIP derivatives has prior art, the explicit handling of multi-anomaly multi-label predictions, along with tailored protocols and empirical gains, provides a meaningful, if incremental, advance.

Strengths
- Addresses a clinically realistic multi-anomaly setting and introduces appropriate evaluation metrics (Sec. 4.1, 4.3).
- Anchor loss clearly targets false positives from irrelevant categories (Sec. 3.2; Eq. 4–5; Fig. 2).
- Layer-limited adapters preserve CLIP priors while enabling few-shot adaptation (Sec. 3.2; training details Sec. 4.1).
- Demonstrated gains in multi-label and class-wise AUROC on fastMRI+ (Tables 2–3), and competitive performance on BMAD general AD (Table 1).
- Insightful similarity heatmaps and discussion of prompt ambiguity motivating sign selection (Sec. 4.4; Fig. 4).

Weaknesses
- Sign selection not concretely implementable from the current text (Sec. 3.3): D_inf, δ, and h(·) lack operational definitions.
- No details on the LLM prompt generation process or clinical validation; risk of noisy or overlapping prompts (Sec. 1; Fig. 4c; Limitation).
- Limited breadth of multi-anomaly experiments (single modality/site, 1-shot only), no statistical tests or shot-scaling (Sec. 4.3).
- Pixel-level adaptation to MVFA is underspecified (Sec. 4.2).
- Some surprising metrics (near-zero subset accuracy for CLIP/MedCLIP) without explaining thresholds, class imbalance, or calibration (Table 2).

Questions
1. Provide the exact algorithm to compute D_inf and δ in Eq. 6, including distance metrics, neighborhood definitions, and any hyperparameters.
2. How many sign prompts per category are used, and how were they generated and vetted? Which LLM and prompt templates? Were radiologists involved?
3. What are the default training hyperparameters (optimizer, learning rate, batch size, temperature for CLIP, λ value), and how sensitive are results to them?
4. How are pixel-level anomaly maps computed when “combined with MVFA”? Please specify the loss combination and inference pipeline.
5. Why do CLIP/MedCLIP have extremely low subset accuracy in Table 2? What is the exact decision rule and thresholding? Report per-class prevalence and confidence intervals.
6. Can you extend multi-anomaly experiments to other organs or datasets and report results across multiple shots (e.g., 1/4/8-shot)?
7. Did you evaluate soft prompt weighting instead of hard filtering (Eq. 6) to handle shared prompts (as acknowledged in Limitation)?

Rating
- Overall (10): 6 — Useful task and promising empirical gains, but key inference/method details are missing (Sec. 3.3, 4.2) and experiments are narrow; Tables 2–3 and Fig. 2 show promise.
- Novelty (10): 6 — Incremental innovation combining adapters, anchor loss, and multi-label protocols in medical few-shot AD; see Sec. 1–3.
- Technical Quality (10): 6 — Training objective is coherent (Eq. 4–5), but reproducibility is limited by unspecified sign selection and pixel-level integration; Sec. 3.3, 4.2.
- Clarity (10): 6 — Clear pipeline and results (Fig. 2; Tables 1–3), but missing critical implementation details and minor inconsistencies; Sec. 3.3; References.
- Confidence (5): 4 — Careful read of methods/experiments; uncertainty remains due to omissions in implementation specifics and thresholds.

Summary
SD-MAD is a sign-driven few-shot framework for multi-anomaly medical image detection built atop CLIP/MedCLIP. It adds a residual “Shift Adapter” to selected transformer layers and an anchor-based inter-anomaly loss to enlarge margins between categories using a normal-prompt anchor. At inference, an automatic sign-selection filter removes prompt outliers to combat intra-class ambiguity. The paper proposes multi-label evaluation (Hamming, subset accuracy) and category-wise AUROC protocols and reports improvements on fastMRI+ brain MRI slices and competitive performance on BMAD benchmarks.

Soundness
The approach is conceptually sound: anomaly-specific signs align text-image embeddings; the anchor loss in Eq. 4–5 enforces desirable inequalities (Remark 3.1). The adapter is consistent with few-shot efficient finetuning. However, the sign-selection step (Definition 3.2, Eq. 6) depends on uninstantiated notions of category decision regions in text space. Without a concrete procedure to compute D_inf and δ, reproducibility and theoretical grounding are weak. The binary decision rule in Scenario 2 (Eq. 8) uses inf/sup similarities; practical stability across classes and the choice of anchor prompt are not analyzed.

Presentation
The manuscript is well-structured with clear figures (Figs. 1–2) and result tables (1–3). The visualization in Fig. 4 supports the motivation for sign selection. Yet crucial implementation specifics are missing: how signs are made (LLM, templates, counts), exact algorithms for Eq. 6, training hyperparameters, and details for pixel-level scoring with MVFA (Sec. 4.2). References include duplicates, and there are minor notation issues (e.g., “Equation 6: D_ng > δ” in Fig. 2). Some numerical results (e.g., perfect 100 AUROC for “Enlarged ventricles” in Table 3) should be contextualized.

Contribution
The paper advances the field by formalizing multi-anomaly few-shot detection and proposing evaluation protocols that reflect multi-label clinical scenarios. While text-prompt alignment and CLIP-based AD are established, the anchor-margin framing and inference-time sign filtering tailored to multi-anomaly settings are valuable. The contribution is more practical/empirical than theoretical, with moderate novelty.

Strengths
- Addresses a clinically important, understudied multi-anomaly few-shot setting (Sec. 1; Fig. 1).
- Intuitive anchor-based inter-anomaly loss (Sec. 3.2; Eq. 4–5) and limited-layer adapters (Eq. 1–2).
- Multi-label and per-category evaluation protocols with strong gains on fastMRI+ (Tables 2–3).
- Visual diagnostic evidence of prompt ambiguity motivating the method (Sec. 4.4; Fig. 4).
- Ablation on λ indicating robustness (Sec. 4.5; Fig. 3).

Weaknesses
- Unclear and likely non-computable formulation of sign selection (Definition 3.2, Eq. 6); missing algorithm and parameters.
- Sign generation pipeline not described or clinically validated; risk of noisy/overlapping signs (Sec. 1; Fig. 4c; Limitation).
- Narrow experimental scope (single modality/site; 1-shot only for multi-anomaly) without statistical tests or calibration analyses (Sec. 4.3).
- Pixel-level evaluation adaptation lacks detail (Sec. 4.2).
- Reference duplication and minor presentation issues; surprising metrics unexplained (Table 2 subset accuracy; Table 3 extremes).

Questions
1. Provide a concrete algorithm for computing D_inf and δ in Eq. 6: is it nearest-neighbor within a category vs. others? What metric and thresholds?
2. Detail the sign generation: which LLM, prompts/templates, number of signs per category, and expert validation?
3. Add training hyperparameters and default λ; include optimizer, LR, batch size, temperature scaling, schedule.
4. How are pixel-level AUROC maps obtained when “combined with MVFA”? Please specify training and inference steps.
5. Explain thresholding and calibration for multi-label decisions; why are baseline subset accuracies near zero? Include per-class prevalence and confidence intervals.
6. Report multi-shot (1/4/8) and cross-organ multi-anomaly experiments; any generalization to other datasets?
7. Analyze sensitivity to the normal anchor prompt; do multiple normal prompts or learned anchors help?
8. Consider soft selection/weights for shared prompts rather than hard discard (related to Limitation section).

Rating
- Overall (10): 6 — Good problem setting and promising empirical results, but ambiguous sign selection and limited experimental breadth hinder impact; see Sec. 3.3 and Tables 2–3.
- Novelty (10): 6 — Moderate novelty in combining anchor-margin training and multi-label protocols for medical few-shot AD; Sec. 1–3.
- Technical Quality (10): 6 — Sound training objective, but incomplete inference algorithm and pixel-level integration reduce rigor; Sec. 3.3, 4.2.
- Clarity (10): 6 — Clear visuals and tables (Fig. 2; Tables 1–3), but missing implementation details and minor inconsistencies; Sec. 3.3; References.
- Confidence (5): 4 — Careful reading and cross-checking; confidence limited by unspecified algorithms and thresholds.

Summary
This paper proposes SD-MAD, a CLIP-based few-shot multi-anomaly detection framework leveraging radiological sign prompts. A Shift Adapter finetunes selected layers (Eq. 1–2), and an anchor-based inter-anomaly loss (Eq. 4–5) increases margins between correct and incorrect categories using a normal prompt as anchor. At inference, a sign-selection rule (Eq. 6) aims to discard outlier prompts. The authors introduce multi-label and category-wise AUROC protocols and show improved performance on fastMRI+ brain MRI slices and competitive results on BMAD datasets.

Soundness
The training formulation is sensible: the inter-anomaly anchor loss operationalizes margin constraints (Remark 3.1) and should reduce false positives. The adapter mechanism is conventional and suited for few-shot. The inference-time sign selection, however, rests on Definition 3.2 that involves a labeling function and category decision regions in text space that are not defined or learned; practical computation of D_inf and δ is absent. The binary prediction with anchor (Eq. 8) may be sensitive to anchor choice and class imbalance; no calibration or sensitivity analysis is provided. The claim of handling pixel-level AD by “combining with MVFA” lacks a method-level description.

Presentation
Writing is clear and structured; the pipeline and evaluations are easy to follow (Fig. 2; Sec. 4.1–4.3). Experimental tables are informative (Tables 1–3), and heatmaps in Fig. 4 provide qualitative insight. Missing details include: LLM used and prompt-generation protocol, the number of signs per category, explicit computation of Eq. 6 quantities, and training hyperparameters. Some references are duplicated, and minor notation issues appear.

Contribution
The paper contributes (1) a task framing for few-shot multi-anomaly detection in medical imaging with multi-label outputs and (2) a training/inference procedure adapted to this task. While VLM-guided AD and prompt use are known, the specific anchor-margin training and multi-label protocols are beneficial and of moderate novelty. The empirical gains are promising, making this a potentially useful baseline for future work on multi-anomaly medical AD.

Strengths
- Addresses a realistic clinical need for multi-anomaly detection and provides evaluation protocols (Sec. 1; 4.1–4.3).
- Inter-anomaly anchor loss clearly targets confusions across categories (Sec. 3.2; Eq. 4–5).
- Effective limited-layer adaptation through adapters (Eq. 1–2) preserving CLIP priors.
- Empirical improvements in multi-label metrics and per-category AUROC (Tables 2–3).
- Visual evidence and discussion of prompt ambiguity (Sec. 4.4; Fig. 4).

Weaknesses
- Sign selection lacks implementable detail (Definition 3.2; Eq. 6); no algorithm for D_inf/δ estimation.
- Prompt generation pipeline missing; clinical validation and standardization of signs unclear.
- Narrow experiments (brain MRI slices only, 1-shot for multi-anomaly) without statistical testing or shot-size analysis.
- Pixel-level integration with MVFA is underspecified (Sec. 4.2).
- Some results appear extreme or counterintuitive (Table 2 subset accuracy; Table 3 AUROC=100) without explanation or confidence intervals.

Questions
1. Precisely define and implement D_inf and δ; provide pseudocode or equations for computing them from finite sign sets.
2. Detail the sign creation process (LLM name, templates, number per category, expert curation).
3. Provide full training hyperparameters and default λ; discuss sensitivity and reproducibility.
4. Explain how pixel-level maps are produced when combining with MVFA; training weights and inference.
5. Clarify decision thresholds and calibration for multi-label predictions; explain low subset accuracies for baselines.
6. Expand multi-anomaly experiments to other modalities/organs and multiple shots; include statistical significance.
7. Analyze the impact of different normal anchors and consider multiple anchors.
8. Evaluate soft weighting or probabilistic filtering for shared prompts acknowledged in Limitation.

Rating
- Overall (10): 7 — Addresses an important gap with a practical method and shows convincing gains, but lacks critical implementation details for sign selection and broader validation; see Sec. 3.3 and Tables 2–3.
- Novelty (10): 7 — Task framing plus anchor-margin training and multi-label protocols provide moderate novelty in medical few-shot AD; Sec. 1–3.
- Technical Quality (10): 6 — Training is sound (Eq. 4–5), but inference algorithm and pixel-level integration are under-specified; Sec. 3.3, 4.2.
- Clarity (10): 7 — Generally clear with effective figures and tables (Fig. 2; Tables 1–3), though key implementation details are missing and references have duplicates.
- Confidence (5): 4 — Strong understanding of the manuscript; uncertainty remains due to incomplete algorithmic details and limited experimental scope.