Here are four distinct reviews of the paper "SD-MAD: Sign-Driven Few-shot Multi-Anomaly Detection in Medical Images".

***

### **Review 1**

**Summary**
This paper introduces a new task setting for few-shot medical anomaly detection, which they term "multi-anomaly detection," where the goal is to identify and distinguish between multiple co-existing anomaly types in a single image. The authors propose SD-MAD, a two-stage framework built upon a vision-language model (VLM). The core idea is to leverage detailed textual descriptions of radiological "signs" for each anomaly class. In the training stage, an "inter-anomaly loss" is used to amplify the distinction between different anomaly categories by aligning image features with their corresponding sign features. In the inference stage, an automatic "sign selection" strategy is employed to filter out noisy or uninformative sign prompts, thereby mitigating intra-class uncertainty. The paper also proposes a comprehensive three-protocol evaluation scheme to assess performance on this new task.

**Soundness**
The methodology is sound and logically motivated. The problem formulation of multi-anomaly detection (Section 3.1) is a timely and necessary extension of the conventional one-class anomaly detection paradigm. The use of a VLM as a backbone is appropriate, given the need to align visual data with textual sign descriptions. The proposed inter-anomaly loss (Equation 4) is a well-reasoned extension of contrastive learning principles, using a "normal" anchor to structure the embedding space effectively. The inference-time sign selection (Section 3.3) is a clever approach to handle the inherent noise and variability in text prompts, a known issue in VLM applications. The experimental setup is thorough, using multiple datasets and introducing new, relevant metrics like Hamming score and subset accuracy for the multi-label task (Section 4.1). The results, particularly the significant gains in subset accuracy shown in Table 2, provide strong evidence for the efficacy of the proposed method.

**Presentation**
The paper is well-written and generally easy to follow. The introduction clearly motivates the problem by providing a clinical example (distinguishing a tumor from pneumonia) and contrasting the proposed task with existing work (Figure 1). The methodology is broken down into logical components (training and inference), which aids comprehension. The use of figures to illustrate the core concepts (Figure 1) and the model pipeline (Figure 2) is helpful. The experimental results are presented clearly in tables, and the inclusion of ablation studies (Section 4.5) and visualizations (Figure 4) strengthens the paper.

**Contribution**
The paper makes several significant contributions:
1.  It formally introduces and tackles the problem of few-shot *multi-anomaly* detection in medical imaging, moving beyond the simpler normal/abnormal binary classification. This is a novel and clinically relevant problem setting.
2.  It proposes an effective framework, SD-MAD, that leverages radiological signs through a two-stage alignment process (inter-anomaly training loss and intra-anomaly inference-time selection). This sign-driven approach is a novel way to inject fine-grained domain knowledge into VLMs.
3.  It introduces a rigorous set of evaluation protocols tailored for this new task, providing a blueprint for future research in this area.

**Strengths**
- **Novel Problem Formulation:** The shift from single-class to multi-class, multi-label anomaly detection is the paper's greatest strength. It addresses a clear gap in the literature and has high clinical relevance.
- **Strong Empirical Results:** The method demonstrates substantial improvements over strong baselines (CLIP, MedCLIP) on the proposed multi-anomaly task, especially in the challenging subset accuracy metric (Table 2), validating the effectiveness of the proposed components.
- **Intuitive and Effective Method:** The core idea of using radiological signs to guide the model is intuitive. The two-stage approach to handle both inter-anomaly and intra-anomaly variations is elegant and shown to be effective through ablation studies.
- **Comprehensive Evaluation:** The introduction of three distinct evaluation protocols (Section 4.1) shows a deep consideration for the nuances of the task and sets a high standard for future work.

**Weaknesses**
- The performance on the general anomaly detection task (Table 1) is not consistently superior to the baseline MVFA, showing slightly lower AUROC on BrainMRI and RESC datasets. While the average is higher, this suggests the method's main strength is specifically in the multi-class setting it was designed for.
- The process for generating the textual "sign" prompts is not detailed. The paper states they are generated by a large language model (Abstract), but details on the LLM used, the prompting strategy, and any human validation are missing.

**Questions**
1.  Could you elaborate on the generation process for the radiological sign prompts? Was a specific LLM used? Were the generated signs reviewed or curated by a medical expert to ensure their clinical accuracy and relevance?
2.  In Table 2, the performance jump in subset accuracy from "Ours (no SS)" to "Ours (full model)" is dramatic (e.g., 34.6% to 60.8% on slice 0). This highlights the importance of sign selection. Did you analyze which specific signs were frequently filtered out? What characteristics do these "bad" signs have?

**Rating**
- Overall (10): 9 — The paper introduces a novel, important problem and proposes an effective, well-evaluated solution with strong results.
- Novelty (10): 9 — The formulation of few-shot multi-anomaly detection and the sign-driven alignment approach are highly novel.
- Technical Quality (10): 9 — The methodology is technically sound, and the experiments are rigorous and well-designed.
- Clarity (10): 8 — The paper is mostly clear, but lacks detail on the prompt generation process.
- Confidence (5): 5 — I am highly confident in my assessment as the paper's claims are well-supported by the evidence provided.

***

### **Review 2**

**Summary**
This paper proposes SD-MAD, a framework for few-shot multi-anomaly detection in medical images. The authors argue that existing methods are limited to one-class classification (normal vs. abnormal) and cannot distinguish between different anomaly types. Their method uses a pre-trained vision-language model (VLM) and fine-tunes it using textual descriptions of "radiological signs" for each anomaly. The training process uses a custom loss function (`L_anchor`) to separate different anomaly classes in the embedding space. At inference, a "sign selection" mechanism filters out potentially noisy text prompts before scoring. The authors evaluate their method on a standard anomaly detection benchmark and a newly curated multi-anomaly dataset from fastMRI+.

**Soundness**
The technical soundness of the paper has some questionable aspects.
1.  **Sign Selection Mechanism:** The sign selection method described in Section 3.3 is not fully convincing. The calculation of `D_inf` (Definition 3.2) is defined as an `infimum` over a set of text features, which in practice is a `minimum` over a small, finite set of prompts. The threshold `δ` is defined based on inter-class distances. This process seems brittle and highly dependent on the initial quality and distribution of the hand-crafted or LLM-generated prompts. A single poorly chosen prompt in a neighboring class could incorrectly cause a good prompt to be filtered out.
2.  **Inconsistent Results:** The empirical evidence is mixed. In Table 1, the method's performance on general AD is inconsistent; it underperforms the SOTA baseline MVFA on 3 out of 6 datasets. The claim of "on average outperform[ing] other methods" (Section 4.2) is based on a marginal average improvement (92.5 vs 92.2) and masks these inconsistencies. More critically, in Table 3, the full model with sign selection ("Ours (full model)") performs worse than the model without it ("Ours (no SS)") on average for slice 10 (61.5 vs 67.1) and for several individual categories. The authors' explanation that "outlier prompt may fit some images" undermines the core motivation for the sign selection module, suggesting it can be detrimental.
3.  **Loss Formulation:** The inter-anomaly loss in Equation 4 is a form of triplet loss. While reasonable, it's not fundamentally novel. The main novelty lies in its application with a "normal" anchor, but the paper does not sufficiently compare this against a more standard multi-class contrastive loss.

**Presentation**
The presentation has significant flaws that hinder understanding.
1.  **Figure Quality and Numbering:** The figures are confusing. Figure 2 uses mermaid diagrams, which are unconventional and difficult to interpret quickly. The legend is separate and its symbols (e.g., triangles) are used inconsistently across different diagrams. Furthermore, the paper contains figures with non-sequential numbering (e.g., Figure 1, 2, then later Figure 13, 14, then Figure 3, 4). This suggests a poorly assembled manuscript. Figures 13/14 appear to be more detailed versions of the components in Figure 2, but they are not referenced in the main text of Section 3.
2.  **Clarity of Methodology:** Section 3.3 is dense and relies heavily on mathematical formalism (Definition 3.2, Remark 3.3) without sufficient intuitive explanation. It is not immediately clear how `h_text(·)` is defined or how `D_inf` is practically computed. The flow from the problem (under-fitted features) to the proposed solution (Eq. 6) is abrupt.

**Contribution**
The primary contribution is the formulation of the multi-anomaly detection task. While this is valuable, the proposed technical solution is a combination of existing techniques (adapter tuning, triplet loss) with a novel but questionably effective sign selection mechanism. The claim of a "rigorous evaluation protocol" is also standard practice for introducing a new task (i.e., selecting appropriate metrics). The contribution is therefore less about a breakthrough method and more about highlighting a new problem direction.

**Strengths**
- The paper correctly identifies an important limitation of existing medical anomaly detection methods and proposes a clinically relevant task.
- The core idea of using fine-grained textual signs to differentiate between anomalies is promising.
- The results on the multi-label prediction task (Table 2) are strong and show a clear benefit over baselines.

**Weaknesses**
- **Technically Weak Component:** The sign selection mechanism is not robustly justified and its empirical results are inconsistent (Table 3), sometimes harming performance.
- **Marginal Performance on General AD:** The method does not offer a clear advantage in the standard AD setting (Table 1), limiting its generality.
- **Poor Presentation:** The paper suffers from confusing figures, non-sequential figure numbering, and a lack of clarity in the methodology section, making it difficult to review and reproduce.
- **Lack of Detail on Prompts:** The origin, quality control, and potential biases of the LLM-generated "signs" are not discussed, which is a critical detail for a "sign-driven" method.

**Questions**
1.  Regarding the sign selection in Section 3.3: How do you practically compute `D_inf` and `δ`? Given that this is a post-hoc filter, have you considered learning which prompts to use during the training phase itself?
2.  Can you explain the significant performance degradation when using sign selection for slice 10 in Table 3 (Avg. AUROC drops from 67.1 to 61.5)? Does this not suggest a fundamental flaw in the sign selection strategy, at least for category-wise evaluation?
3.  The adapter is applied to specific layers (6, 8, 18, 24). What was the rationale for choosing these specific layers? Was a search performed?

**Rating**
- Overall (10): 4 — The paper addresses an interesting problem but the proposed method has technical weaknesses, inconsistent results, and significant presentation issues.
- Novelty (10): 7 — The problem formulation is novel, but the technical components are largely adaptations of existing ideas.
- Technical Quality (10): 4 — The sign selection module is not well-justified and shows mixed results, and the overall experimental evidence is not consistently strong.
- Clarity (10): 3 — The paper is difficult to understand due to confusing figures, poor organization, and opaque methodological descriptions.
- Confidence (5): 5 — I am confident in my assessment of the paper's technical and presentational shortcomings.

***

### **Review 3**

**Summary**
This paper presents SD-MAD, a new framework for few-shot multi-anomaly detection in medical images. The work departs from the traditional normal-vs-abnormal paradigm by aiming to identify multiple, specific anomaly categories within an image. The method leverages a CLIP-based model and fine-tunes it using textual descriptions of radiological signs. The training phase focuses on amplifying inter-anomaly discrepancy via a specialized loss function. The inference phase introduces a sign selection strategy to prune unhelpful text prompts to reduce intra-anomaly uncertainty. The authors also propose three evaluation protocols to benchmark performance on this new task, demonstrating their method's effectiveness on a multi-label brain MRI dataset.

**Soundness**
The overall methodology is plausible. The use of adapters for efficient fine-tuning of a VLM is a standard and sound choice. The core idea of using a triplet-style loss (`L_anchor` in Eq. 4) with a "normal" anchor to structure the embedding space for multi-class separation is logical. The recognition that text prompts can be noisy and the attempt to mitigate this with a selection strategy (Section 3.3) is thoughtful. However, the soundness of this selection strategy is debatable. It is a heuristic applied at inference time, and as the results in Table 3 show, it does not uniformly improve performance, suggesting it may not be a robust solution. The experimental design is mostly sound, with relevant baselines for the new task and appropriate metrics. The combination of SD-MAD with MVFA for pixel-level tasks (Section 4.2) is a pragmatic choice, though it makes it harder to isolate the contribution of SD-MAD itself in that setting.

**Presentation**
The paper's presentation is its biggest weakness and significantly detracts from its quality.
- **Clarity of Methodology:** The method section is difficult to parse. Section 3.2 introduces the "Shift Adapter" and "Inter-anomaly Loss" with dense equations but lacks high-level intuition. Section 3.3 on "Sign Selection" is particularly opaque, with undefined terms (what is the function `h_text` initially?) and a reliance on formal remarks that are not well-explained. The connection between the problem of "under-fitted features" and the proposed filtering in Equation 6 is not clearly articulated.
- **Figures and Diagrams:** The figures are a major point of concern. Figure 2, intended as the main pipeline diagram, is confusing. The use of mermaid code is highly unconventional for a research paper and results in a low-quality, hard-to-read diagram. The figure legend is disconnected from the diagrams. The paper also appears to have duplicate and mis-numbered figures (e.g., Figure 4 and Figure 39 are identical; Figure 3 and Figure 35 are identical). This level of disorganization is not acceptable for a publication. The more detailed diagrams (Figures 13 and 14) are better but are not properly integrated into the main text.
- **Organization:** The flow of the paper feels disjointed, especially between the high-level concepts in the introduction and the dense technical details in the methodology. A clearer, step-by-step walkthrough of the method with a running example would have been immensely helpful.

**Contribution**
The paper's main contribution is the conceptualization of the few-shot multi-anomaly detection task. This is a valuable step forward for the field. The proposed SD-MAD framework is a reasonable first attempt at solving this task, and the idea of a sign-driven approach is compelling. The proposed evaluation protocols are also a useful contribution for standardizing future research. However, the technical novelty of the components themselves (adapters, contrastive-style loss) is moderate. The sign selection mechanism is novel but its effectiveness is questionable.

**Strengths**
- **Important Problem:** The paper tackles a practical and important problem in medical diagnostics.
- **Good Core Idea:** The sign-driven approach is conceptually strong and provides a good framework for incorporating domain knowledge.
- **Strong Multi-label Results:** The method shows very strong performance on the multi-label prediction task (Table 2), which is the core focus of the paper.

**Weaknesses**
- **Major Presentation Issues:** The paper is poorly presented, with confusing and unprofessional figures, non-sequential numbering, and an unclear methodology section. This severely impacts readability and reproducibility.
- **Inconsistent Performance of Sign Selection:** The key inference-time component, sign selection, does not consistently improve results (Table 3), weakening the claims about its utility.
- **Lack of Implementation Details:** Critical details, such as the source and curation of the textual signs, are omitted.

**Questions**
1.  Could you please redraw Figure 2 as a standard, high-quality diagram and ensure all figures are correctly numbered and referenced in the text?
2.  In Section 3.3, could you provide a more intuitive, step-by-step explanation of the sign selection process? Specifically, what is the initial labeling function `h_text`, and can you provide a concrete example of a prompt being filtered out by Equation 6?
3.  Given the mixed results of the sign selection strategy, have you considered alternative approaches to handle noisy prompts, such as weighting them by their quality or learning to ignore them during training?

**Rating**
- Overall (10): 5 — The paper has a strong core idea and addresses an important problem, but it is severely hampered by major presentation flaws that make it difficult to understand and evaluate properly.
- Novelty (10): 8 — The problem formulation is highly novel. The solution has novel elements, though built on existing concepts.
- Technical Quality (10): 6 — The core training method is sound, but a key component (sign selection) has questionable effectiveness, and the experimental evidence is mixed.
- Clarity (10): 2 — The paper is very difficult to read due to poor figures, confusing organization, and an opaque methodology section.
- Confidence (5): 4 — I am confident that the presentation is a major issue, but it is difficult to be fully confident in the technical assessment due to the lack of clarity.

***

### **Review 4**

**Summary**
This paper introduces SD-MAD, a method for few-shot multi-anomaly detection in medical images, designed to go beyond simple normal/abnormal classification and identify specific types of pathologies. The approach is "sign-driven," meaning it relies on aligning image features from a VLM with textual descriptions of radiological signs associated with each anomaly. The method consists of a training stage with a specific loss function to enhance separation between anomaly classes and an inference stage that includes a "sign selection" step to filter out unreliable textual prompts. The authors test their method on several datasets, introducing a new multi-anomaly benchmark based on fastMRI+ brain images and proposing new evaluation metrics for this task.

**Soundness**
The methodology is generally sound from a machine learning perspective. The use of a pre-trained VLM, parameter-efficient fine-tuning with adapters, and a contrastive loss are all reasonable choices. The central hypothesis—that different anomalies share common signs but also have unique ones that can be exploited for classification (Section 1)—is clinically plausible and provides a strong foundation for the work. The two-stage approach to first separate classes (inter-anomaly) and then refine predictions within a class (intra-anomaly) is logical. The experimental validation on the multi-label task (Table 2) shows impressive gains, suggesting the approach is effective for its primary goal.

However, the clinical and practical soundness could be improved.
- **Dataset Curation:** The multi-anomaly detection experiments are performed on a dataset derived from fastMRI+ by selecting 6 anomaly categories and specific slices (0, 5, 10) (Section 4.1). This setup feels somewhat artificial. Are these 6 anomalies commonly co-occurring? Why were these specific slices chosen? A real-world clinical application would require robustness across the entire volume, not just pre-selected slices.
- **Sign Generation and Validity:** The entire method is "sign-driven," making the quality of the signs paramount. The paper mentions they are generated by an LLM (Abstract) but provides no further details. From a clinical standpoint, this is a critical omission. Are these signs clinically accurate? Do they represent the full spectrum of how a disease manifests? Were they validated by a radiologist? Without this information, it's hard to assess whether the model is learning meaningful radiological features or just exploiting biases in the LLM's output. For example, in Figure 4c, "brain after surgical resection" is listed as a sign for both "Craniotomy" and "Posttreatment change," which is reasonable but also highlights the ambiguity the method must handle.

**Presentation**
The paper is reasonably well-written, and the motivation is clear. The introduction provides a good clinical context that grounds the research problem. The structure of the paper is logical. However, the figures are a weak point. Figure 2 is not clear, and the inconsistent numbering of figures throughout the manuscript (e.g., Figure 3 appearing after Figure 14 is mentioned) is distracting and suggests a lack of careful preparation. The visualization in Figure 4 is effective at showing the improved alignment post-training.

**Contribution**
The main contribution is the shift in focus to the more clinically realistic problem of multi-anomaly detection in a few-shot setting. This is a significant and welcome direction for the field. The proposed sign-driven framework is a novel and promising way to tackle this problem by injecting explicit, fine-grained knowledge into the model. The introduction of new evaluation protocols (multi-label prediction, category-wise AUROC) is also a valuable contribution that will aid future research.

**Strengths**
- **Clinical Relevance:** The paper addresses a problem of high clinical importance—differentiating between multiple pathologies, which is a core task of a radiologist.
- **Novel Sign-Driven Approach:** The idea of using explicit radiological signs as textual anchors is a powerful and innovative way to guide VLM-based medical image analysis.
- **Strong Performance on Core Task:** The method achieves excellent results on the multi-label prediction task (Table 2), demonstrating its ability to handle complex scenes with multiple anomalies.
- **Thoughtful Evaluation:** The paper proposes a multi-faceted evaluation that captures different aspects of the model's performance, from binary classification to multi-label accuracy.

**Weaknesses**
- **Lack of Clinical Grounding in Implementation:** The details regarding the generation and validation of the crucial "radiological signs" are missing. This is a major weakness for a method that claims to be "sign-driven."
- **Artificial Experimental Setting:** The multi-anomaly dataset construction (specific slices and categories from fastMRI+) may not fully reflect the complexity and variability of real clinical scenarios.
- **Presentation Flaws:** Confusing and poorly organized figures detract from the paper's overall quality.

**Questions**
1.  Could the authors provide a detailed description of the process used to generate and select the textual signs for each anomaly? Specifically, which LLM was used, what prompts were given to it, and most importantly, was there any form of validation by medical professionals?
2.  What was the rationale for selecting the 6 specific anomaly categories from the fastMRI+ dataset and for evaluating only on slices 0, 5, and 10? How representative is this setup of a real-world diagnostic task?
3.  The limitation section mentions that some prompts may correspond to more than one anomaly type. Your method seems to handle this via the loss function and sign selection, but Figure 4b shows that "brain after surgical resection" still has high similarity for two different classes. How does the model ultimately disambiguate in such cases, especially for the binary predictions in Scenario 2 (Eq. 8)?

**Rating**
- Overall (10): 7 — A strong paper with a novel, clinically relevant idea and impressive results, but it is held back by a lack of detail on its clinical grounding and some presentation issues.
- Novelty (10): 9 — The problem formulation and sign-driven approach are highly novel and impactful.
- Technical Quality (10): 7 — The ML methodology is solid, but the experimental setup feels somewhat artificial, and the effectiveness of the sign selection is mixed.
- Clarity (10): 6 — Generally clear, but key implementation details are missing, and the figures are confusing.
- Confidence (5): 5 — I am confident in my assessment, particularly regarding the clinical relevance and the need for more detail on the "sign-driven" aspect.