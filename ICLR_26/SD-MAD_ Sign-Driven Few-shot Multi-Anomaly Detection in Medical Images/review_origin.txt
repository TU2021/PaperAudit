OpenReview.net
Search OpenReview...
Login
back arrowGo to ICLR 2026 Conference homepage
SD-MAD: Sign-Driven Few-shot Multi-Anomaly Detection in Medical Images
Download PDF
ICLR 2026 Conference Submission25499 Authors
20 Sept 2025 (modified: 24 Dec 2025)
ICLR 2026 Conference Submission
Everyone
Revisions
BibTeX
CC BY 4.0
Keywords: Anomaly Detection, Medical Image, Few-shot Learning
Abstract:
Medical anomaly detection (AD) is crucial for early clinical intervention, yet it faces challenges due to limited access to high-quality medical imaging data, caused by privacy concerns and data silos. Few-shot learning has emerged as a promising approach to alleviate these limitations by leveraging the large-scale prior knowledge embedded in vision-language models (VLMs). Recent advancements in few-shot medical AD have treated normal and abnormal cases as a one-class classification problem, often overlooking the distinction among multiple anomaly categories. Thus, in this paper, we propose a framework tailored for few-shot medical anomaly detection in the scenario where the identification of multiple anomaly categories is required. We propose that separating anomalies relies on distinct radiological signs, routinely used by clinicians to bridge knowledge and images. To capture the detailed radiological signs of medical anomaly categories, our framework incorporates diverse textual descriptions for each category generated by a Large-Language model, under the assumption that different anomalies in medical images may share common radiological signs in each category. Specifically, we introduce SD-MAD, a two-stage \textbf{S}ign-\textbf{D}riven few-shot \textbf{M}ulti-\textbf{A}nomaly \textbf{D}etection framework: (i) Radiological signs are aligned with anomaly categories and distinguished by amplifying inter-anomaly discrepancy; (ii) Aligned signs are selected further to mitigate the effect of the under-fitting and uncertain-sample issue caused by limited medical data, employing an automatic sign selection strategy at inference. Moreover, we propose two protocols to comprehensively quantify the performance of multi-anomaly detection. Extensive experiments illustrate the effectiveness of our method.

Primary Area: applications to physical sciences (physics, chemistry, biology, etc.)
Submission Number: 25499
Filter by reply type...
Filter by author...
Search keywords...

Sort: Newest First
8 / 8 replies shown
Official Review of Submission25499 by Reviewer k1bu
Official Reviewby Reviewer k1bu09 Nov 2025, 01:46 (modified: 12 Nov 2025, 18:30)EveryoneRevisions
Summary:
This paper introduces a novel framework for few-shot multi-anomaly detection in medical images. Existing one-class few-shot anomaly detection approaches struggle to distinguish between multiple anomaly categories, which is often critical in real-world clinical scenarios. The authors enhance existing few-shot AD methods by leveraging vision每language alignment, adapting them to handle multi-anomaly detection tasks, and achieving favorable experimental results.

Soundness: 3: good
Presentation: 2: fair
Contribution: 3: good
Strengths:
The paper proposes a few-shot anomaly detection model that can inherently handle multiple anomaly classes within a single framework. By aligning radiological signs with anomaly categories, the method introduces a novel and innovative approach.

Experiments conducted across multiple evaluation metrics demonstrate strong and consistent performance gains over baseline methods such as vanilla CLIP, highlighting the effectiveness of the proposed approach. The provided visualizations further corroborate these findings.

Weaknesses:
The methodology section contains a relatively large number of mathematical formulations, yet several key equations and symbols are insufficiently explained. Clearer definitions and intuitive interpretations would improve readability and reproducibility.

The paper devotes substantial space to the Sign Selection process during inference; however, this component appears to have a limited positive effect in certain experiments. The authors should analyze and discuss potential reasons behind this phenomenon.

The comparisons in Tables 1每4 are not sufficiently comprehensive. It is unclear why the authors do not include comparisons with CLIP-based approaches, such as AnomalyCLIP, which represent strong and relevant baselines for this task. Incorporating such comparisons would significantly strengthen the validity and credibility of the reported results.

Questions:
See Weaknesses

Flag For Ethics Review: No ethics review needed.
Rating: 6: marginally above the acceptance threshold. But would not mind if paper is rejected
Confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
Code Of Conduct: Yes
Official Comment by Authors
Official Commentby Authors03 Dec 2025, 21:34Everyone
Comment:
We thank the reviewer for the constructive comments. Below, we provide detailed responses to each point.

W1-clearer definition: We provide a notation table summarizing the key inputs and outputs used in our models:

symbol	explanation
input images
labels corresponding to 
one anomaly category
input feauture at 
-th layer
output feature at 
-th layer
final output at the 
-th layer mixing the outputs for original layer and adapter layer
the image feature represetnation corresponding to 
-th anomaly category
the representation of the 
-th textual description corresponding the 
-th anomaly category
anchor feature used to determine whether the anomaly category appears; can be used for binary anomaly detection.
We will correct the misnotated symbols and provide a more detailed and comprehensive notation table in the revised manuscript.

W2-reason for limited positive effect: As discussed in the last section of our paper, different anomalies may share the same textual descriptions. For instance, ※surgical resection§ is a description associated with Craniotomy, but Post-treatment Change may exhibit the same characteristic. This ambiguity can lead to failure cases and limited improvement in some scenarios. Moreover, if the text features are already well separated in the embedding space, the proposed selection strategy is expected to yield only marginal additional gains.

W3-limited CLIP-based methods: We include only CLIP and MedCLIP in our main comparison because these are the only existing methods directly applicable to few-shot multi-label anomaly detection. Other approaches, such as AnomalyCLIP, are specifically designed for single-label anomaly detection and cannot be straightforwardly adapted to our multi-label setting. For completeness, we additionally provide comparisons with these methods under their original evaluation settings in the appendix.

Official Review of Submission25499 by Reviewer 7E6r
Official Reviewby Reviewer 7E6r05 Nov 2025, 05:53 (modified: 12 Nov 2025, 18:30)EveryoneRevisions
Summary:
The paper introduces SD-MAD, a framework for few-shot multi-anomaly detection (MAD) in medical images. Unlike prior methods treating anomaly detection as a one-class classification problem (normal vs. abnormal), SD-MAD targets scenarios where multiple anomaly categories coexist. The framework is evaluated on diverse datasets using new multi-anomaly metrics, showing clear gains over CLIP and MedCLIP baselines.

Soundness: 3: good
Presentation: 3: good
Contribution: 3: good
Strengths:
1.The paper is well written and organized, with clear diagrams (Fig. 1每3) and detailed appendices.

2.It defines and benchmarks few-shot multi-anomaly detection in medical AD area.

Weaknesses:
1.Only limited baseline (CLIP and MedCLIP) are compared.

2.The framework relies heavily on manually designed textual signs to represent anomaly semantics. It may make the system sensitive to the quality and accuracy of these textual descriptions. In practice, such signs may contain noise, redundancy, or mismatched terminology relative to the specific medical domain or dataset. When the textual prompts do not align well with the actual imaging characteristics, the performance of SD-MAD may degrade significantly.

3.The novelty of this framework is limited. The core components〞prompt-based supervision, adapter tuning, and margin-based loss〞are primarily adapted from existing CLIP or few-shot learning paradigms.

Questions:
Have the authors examined performance variance across different few-shot sample sets or seeds?

Flag For Ethics Review: No ethics review needed.
Rating: 4: marginally below the acceptance threshold. But would not mind if paper is accepted
Confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
Code Of Conduct: Yes
Official Comment by Authors
Official Commentby Authors03 Dec 2025, 21:40 (modified: 04 Dec 2025, 00:32)EveryoneRevisions
Comment:
We appreciate the reviewer＊s constructive comments. Below, we provide detailed responses to each point.

W1-limited baseline: We include only CLIP and MedCLIP in our main comparison because these are the only existing methods directly applicable to few-shot multi-label anomaly detection. Other approaches, such as AnomalyCLIP, are designed specifically for single-label anomaly detection and cannot be directly adapted to our multi-label setting. For completeness, we additionally provide comparisons with these methods under their original evaluation settings in the appendix.

W2-textual-quality issue: We agree that the performance is related to the quality of the textual descriptions, which also motivates us to propose sign selection at inference time to filter out misaligned textual representations. In addition, images are the primary inputs, and in practice, we do not need to generate textual descriptions multiple times for the same image. Therefore, obtaining high-quality text is feasible in real-world settings.

W3-novelty: Existing methods and benchmarks for medical anomaly detection are not designed to handle scenarios in which multiple anomalies appear within the same image. As illustrated in Figure 1 of our paper, our proposed setting explicitly considers this more complex and realistic case using a vision每language model. Therefore, we believe our work addresses a meaningful and underexplored challenge and that the proposed research paradigm is computationally efficient.

Q1-results over different seeds: We report the results of our full model on the FastMRI slice 5 over 3 different seeds under the setting of integrated anomaly detection:

mean	std
subset acc	22.1	5.2
Hamming score	66.8	9.7
The mean performance still outperforms both CLIP and MedCLIP.

Official Review of Submission25499 by Reviewer LmK7
Official Reviewby Reviewer LmK703 Nov 2025, 00:02 (modified: 12 Nov 2025, 18:30)EveryoneRevisions
Summary:
The paper proposes a CLIP-based few-shot framework for medical abnormality recognition, incorporating (1) radiological ※sign prompts,§ (2) an inter-anomaly contrastive loss, and (3) a sign selection mechanism at inference. The authors position the work as extending few-shot anomaly detection to multi-anomaly settings and evaluate on ChestX-ray8, OCT-17, and FastMRI+.

Soundness: 2: fair
Presentation: 3: good
Contribution: 2: fair
Strengths:
Incorporating radiological sign descriptions into visual-language alignment is an interesting idea.
The few-shot multi-label formulation is practically relevant for real-world medical datasets.
Weaknesses:
The work is not anomaly detection but closed-set multi-label classification. The method assumes all anomaly labels are known during training and requires per-class support images. It does not detect abnormality in an open-set or unsupervised manner, which is the central definition of anomaly detection. The title and claims are therefore misleading.
No open-set or unseen-class evaluation. There is no experiment for detecting unseen anomaly types. In real clinical anomaly detection, new or rare abnormalities are the norm, yet the paper never addresses this problem. The method collapses to a standard supervised multi-label classifier.
Pixel-level AUROC is not shown in the main paper, and the appendix combines image-level and pixel-level AUROC into a single arithmetic ※average,§ which is mathematically invalid and masks the fact that the method trails AnomalyCLIP and MVFA on pixel-level AUROC.
Strongest baselines (MVFA, AnomalyCLIP) are moved to the appendix and perform comparably or better. The main tables omit the most relevant recent methods, and the appendix shows that the proposed approach does not consistently outperform them. This selective reporting undermines the claimed contribution.
The radiological sign prompts are not reproducible or clinically validated. The paper does not describe how the signs were generated, who verified them, how many exist per class, or whether they will be released. This prevents scientific reproducibility.
Lack of evidence that the datasets actually support ※multi-anomaly§ detection. ChestX-ray8 contains 14 abnormality labels, OCT-17 contains 3, and FastMRI+ contains 6, but the paper provides no statistics on how many anomalies occur per image, nor how often co-occurrence happens. Without reporting label cardinality or multi-label frequency, it is unclear whether the task is truly ※multi-anomaly detection§ or simply multi-label classification where most samples contain only one abnormality. Since the paper＊s motivation depends on co-occurring anomalies, the absence of dataset analysis leaves the main claim unsupported.
Questions:
How does the method behave when a novel anomaly class appears at inference time?
Why are MVFA and AnomalyCLIP excluded from the main comparison tables?
Can the authors provide localization maps?
What percentage of samples in each dataset actually contain more than two anomalies?
Flag For Ethics Review: No ethics review needed.
Rating: 2: reject, not good enough
Confidence: 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.
Code Of Conduct: Yes
Official Comment by Authors
Official Commentby Authors03 Dec 2025, 21:44Everyone
Comment:
We appreciate the reviewer＊s constructive comments. Below, we provide detailed responses to each point.

W1, W2, Q1 每 Exploiting prior abnormal knowledge: As discussed in MVFA [1], methods that make no assumptions about prior abnormal knowledge typically require a large number of normal samples per class to detect anomalies as deviations from the normal distribution. However, acquiring such large-scale data is impractical in medical anomaly detection. Following the experimental setting of previous works [1,2], our study specifically addresses the challenge of having only limited known anomalies during training, a scenario that often leads to model bias and poor generalization. To tackle these issues, we propose a strategy that enhances the discrepancies between different anomaly categories while mitigating uncertainty within the same anomaly category.

W3, Q3 每 Pixel-level results: We cannot report pixel-level results in the main experiments because pixel-level annotations are not available in the datasets with multi-category anomalies. Instead, we provide pixel-level results for each dataset under the single-category anomaly detection setting in Table 5 of the appendix.

W4, Q2 每 Limited baseline: We include only CLIP and MedCLIP in our main comparison because these are the only existing methods applicable to few-shot multi-label anomaly detection. More details are provided in the response to Reviewer k1bu under W3. To the best of our knowledge, this is the first work that explicitly focuses on multi-anomaly detection in the medical domain.

W5 每 Generation of sign descriptions: We use an LLM to generate 15每20 candidate descriptions for each anomaly category. Among these, 3每5 distinct descriptions per anomaly are selected by medical experts. These descriptions are then inserted into different templates, such as ※an abnormal photo of {obj} with {anomaly}.§ In total, we construct 375 different templates for text generation. We will add these implementation details to the revised manuscript.

W6 每 Existence of multiple anomalies: As discussed in the experiments, the OCT dataset does not contain co-occurring anomalies. For the other datasets, we provide the statistics below:

dataset	number of images with multi-anomaly	number of all images
Chest	2567	16806
fastMRI(selected)	79	293
[1] Huang, Chaoqin, et al. "Adapting visual-language models for generalizable anomaly detection in medical images." CVPR 2024.

Official Review of Submission25499 by Reviewer SUm2
Official Reviewby Reviewer SUm231 Oct 2025, 15:18 (modified: 12 Nov 2025, 18:30)EveryoneRevisions
Summary:
This paper proposes a method for "few-shot" anomaly detection in the medical domain.

Soundness: 2: fair
Presentation: 3: good
Contribution: 2: fair
Strengths:
Pros:

A notable contribution is that, beyond conventional anomaly detection, the proposed method can also identify anomaly categories.
The paper is well organized and generally easy to follow.
Weaknesses:
Cons:

There are several unclear or conflicting notations. For instance, in Sec. 3.1, 
 denotes the number of samples, while in Sec. 3.3, it represents the number of anomaly categories. Similarly, both 
 and 
 seem to denote categories. These inconsistencies significantly hinder readability.
If 
 indeed refers to the number of anomaly categories, how is it determined? This is not explained in the paper. If it is empirically set, the authors should discuss how its choice influences model performance.
w/ SS vs. w/o SS. The paper claims that identifying anomaly categories improves performance, yet provides no clear explanation or theoretical reasoning for why this is the case. A deeper discussion or analysis would strengthen the contribution.
The results are without statistical measures (e.g., mean ㊣ std over 10 trials), which is standard practice in few-shot anomaly detection. This omission weakens the reliability of the comparisons.
It is unclear how sensitive the model performance is to the selection of samples, which is an important practical consideration.
The paper lacks any discussion of computational cost or inference time, which are crucial for clinical applicability.
Why did the authors choose to use the general CLIP model instead of a medical-domain variant? This decision should be justified.
The paper's definition of "few-shot anomaly detection" seems problematic. In the conventional sense, few-shot anomaly detection extends unsupervised anomaly detection〞training with a few normal samples and testing on unseen tasks (which is in line with few-shot learning). However, this work trains on both normal and abnormal samples, which aligns more closely with small sample image classification rather than anomaly detection. The authors should reconsider and clarify the task formulation.
Questions:
See Weaknesses.

Flag For Ethics Review: No ethics review needed.
Rating: 4: marginally below the acceptance threshold. But would not mind if paper is accepted
Confidence: 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.
Code Of Conduct: Yes
Official Comment by Authors
Official Commentby Authors03 Dec 2025, 21:51Everyone
Comment:
We appreciate the reviewer＊s constructive comments. Below, we provide detailed responses to each point.

W1, W2 每 Symbol notation: We appreciate the reviewer for pointing out the notation issue. The numbers of samples and categories are indeed not the same, and the current notation is misleading. We will revise the notation accordingly in the paper.

W3 每 Why we need SS: As discussed in the paper, SS is designed to address the uncertainty problem in the model. Different symptoms manifest to varying degrees in the images, and their alignment with the textual descriptions also varies. Consequently, some textual descriptions may not be well aligned with their visual representations. To mitigate this issue, we employ SS to filter out outlier textual representations.

W4, W5 每 Sensitivity of the model: Our experiments are built on the medical AD setting [1], where standard deviations are not reported. For completeness, we report the standard deviation over 3 different seeds, which can be found in the response to Q1 from Reviewer 7E6r

W6 每 Computational cost: The computational cost is primarily dominated by the vision每language encoder, which is comparable to existing CLIP-based approaches. Since we utilize only a few-shot set of samples per anomaly category, the additional computation introduced by category-specific prompts is minimal.

W7 每 Model choice: We choose CLIP instead of MedCLIP because chest images constitute the majority of the training data in MedCLIP, which may hinder its ability to generalize to other organs. Our training process requires a model that can generalize across different body parts, such as the brain and eyes.

W8 每 Setting of anomaly detection: As discussed in MVFA [1], prior methods that make no assumptions about prior abnormal knowledge typically require large numbers of normal samples per class to detect anomalies as deviations from the normal distribution. However, acquiring such large-scale data is impractical in medical anomaly detection. Following the experimental setting of previous works [1,2], our research tackles the challenge of limited known anomalies during training, a scenario that often leads to model bias and poor generalization. To address these challenges, we propose a strategy that enhances the discrepancies between different anomaly categories while mitigating uncertainty within the same anomaly category.

[1] Huang, Chaoqin, et al. "Adapting visual-language models for generalizable anomaly detection in medical images." CVPR 2024.

About OpenReview
Hosting a Venue
All Venues
Contact
Sponsors
Donate
FAQ
Terms of Use / Privacy Policy
News
OpenReview is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the OpenReview Sponsors. ? 2025 OpenReview

SD-MAD: Sign-Driven Few-shot Multi-Anomaly Detection in Medical Images | OpenReview