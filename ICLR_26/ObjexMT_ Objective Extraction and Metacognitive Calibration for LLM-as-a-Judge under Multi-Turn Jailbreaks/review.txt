### Summary

This paper introduces **ObjexMT**, a benchmark designed to assess the ability of **Large Language Models (LLMs)** to **extract latent objectives** from **multi-turn jailbreak conversations** and to **calibrate their confidence** in such extractions. It evaluates how well an LLM can recover a hidden goal in a dialogue and report its confidence in the inference. The evaluation considers **semantic similarity**, **confidence calibration**, and error analysis, with a focus on **adversarial settings** where models may be misled or perform poorly due to obfuscation. Results reveal that current models, although improving, still exhibit significant **high-confidence errors**, underscoring the need for **confidence-based gating** and **human oversight** in sensitive applications.

---

### Strengths

1. **Important Problem**:

   * The paper addresses the **safety** and **reliability** of **LLM-as-a-Judge systems**, which is highly relevant given the increasing deployment of LLMs in critical tasks.
   * It proposes a benchmark for **latent objective extraction** and **confidence calibration**, two key aspects of model reliability in adversarial contexts.

2. **Novel Evaluation Paradigm**:

   * ObjexMT introduces a **dual-evaluation framework** that measures **objective extraction** (semantic similarity) and **confidence calibration** (using metrics like ECE, Brier score, and Wrong@High-Confidence). This is a unique and practical contribution, helping to understand not just the accuracy of predictions but also how well models calibrate their confidence.

3. **Comprehensive Experiments**:

   * The paper uses a **diverse set of models** (e.g., GPT-4.1, Claude-Sonnet-4, Kimi-K2) and **datasets** (SafeMTData, MHJ) with varying levels of **obfuscation**. This provides a thorough comparison of different models' capabilities across adversarial contexts.

4. **Transparency and Reproducibility**:

   * Full **dataset release**, including 2,817 instances, human labels, and model outputs, enhances transparency and allows the community to validate and extend the research.

5. **Clear Practical Implications**:

   * The paper provides **actionable insights** for deploying LLM judges, such as **confidence-based gating** and **human supervision** in high-stakes settings, which is critical for AI safety.

---

### Weaknesses

1. **Overstatement of Metacognition**:

   * The paper uses **"metacognition"** to describe models' **confidence calibration**. However, true metacognition involves more than just confidence estimation¡ªit includes **self-awareness** of knowledge boundaries and uncertainty sources. The term "confidence calibration" would be more accurate and should replace "metacognition" in the paper.

2. **Threshold Optimization**:

   * The **threshold selection process** (¦Ó*=0.66) used to calibrate **semantic similarity** is based on 300 labeled samples from the same dataset used for evaluation. This introduces a **multiple comparison problem** and risks overfitting. The paper would benefit from **cross-validation** or a separate training-validation split to ensure generalization.

3. **Unclear Contribution**:

   * The paper claims to introduce **ObjexMT** but repurposes existing datasets with **latent objectives**. The actual contributions are the **benchmark setup** (objective extraction + confidence calibration) and the **evaluation protocol**, which should be more clearly articulated as a new methodology, rather than implying a completely new corpus.

4. **Limitations in Confidence Reporting**:

   * **Self-reported confidence** is used as the sole measure for metacognition, but this can be **misleading** if models interpret the elicitation prompts inconsistently. The paper does not sufficiently validate that self-reported confidence is a reliable proxy for model uncertainty, especially when compared to **token-level probabilities** or **entropy-based uncertainty**.

5. **Dataset Heterogeneity**:

   * The **dataset variance** (24.3% accuracy in Attack600 vs. 80.9% in MHJ) is significant, but the paper does not fully explain the **reasons behind this disparity**. A **deeper analysis** of why certain datasets are more challenging and how they influence model performance would be useful.

6. **Lack of Inter-Annotator Agreement**:

   * The paper does not report **inter-annotator agreement** for the 300 calibration samples. This is essential to establish the **reliability** of the human labels used to set the threshold. This information should be provided, along with **Cohen's ¦Ê** or **Fleiss' ¦Ê** values.
