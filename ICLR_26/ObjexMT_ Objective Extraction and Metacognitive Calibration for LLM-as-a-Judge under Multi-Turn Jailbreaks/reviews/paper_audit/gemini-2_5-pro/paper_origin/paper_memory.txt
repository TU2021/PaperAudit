# Global Summary
This paper introduces ObjexMT, a benchmark to evaluate an LLM-as-a-Judge's ability to extract latent objectives from multi-turn jailbreak conversations and to assess the reliability of its self-reported confidence (metacognition). The task requires a model to output a single-sentence "base objective" and a confidence score from a dialogue transcript. Accuracy is measured by comparing the extracted objective to a gold standard using another LLM-judge (gpt-4.1), with correctness determined by a single, human-calibrated similarity threshold (τ* = 0.66). Metacognition is evaluated using ECE, Brier score, high-confidence error rates, and risk-coverage curves. The benchmark is run on six models (including gpt-4.1, claude-sonnet-4, kimi-k2) across three datasets (SafeMTData_Attack600, SafeMTData_1K, MHJ), totaling 2,817 instances. Key findings show that objective extraction is challenging, with overall accuracy ranging from 47.4% to 61.2%. kimi-k2 achieves the highest accuracy (0.612), while claude-sonnet-4 demonstrates the best calibration and selective risk (ECE 0.206, AURC 0.242). A significant issue is the persistence of high-confidence errors, with Wrong@0.90 rates from 14.9% to 47.7%. The paper also highlights strong dataset heterogeneity, with accuracy on automated attacks being much lower (avg. 24.3%) than on human-authored ones (avg. 80.9%). The authors conclude that LLM judges struggle to infer latent intent and recommend explicitly stating objectives or using confidence-gating for safety-critical applications.

# Abstract
The paper introduces ObjexMT, a benchmark for evaluating an LLM-as-a-Judge's (LLMaaJ) ability to extract a conversation's latent objective and assess its own confidence. The task involves extracting a one-sentence base objective from a multi-turn transcript and providing a self-reported confidence score. Accuracy is determined by semantic similarity to a gold objective, judged by an LLM, and binarized using a human-aligned threshold (τ* = 0.66, calibrated on N=300 items with F1@τ* = 0.891). Metacognition is measured by ECE, Brier score, Wrong@High-Confidence, and risk-coverage.
- Six models were tested: gpt-4.1, claude-sonnet-4, Qwen3-235B-A22B-FP8, kimi-k2, deepseek-v3.1, and gemini-2.5-flash.
- Datasets used: SafeMTData_Attack600, SafeMTData_1K, and MHJ.
- Top accuracy was achieved by kimi-k2 (0.612; 95% CI [0.594, 0.630]), with claude-sonnet-4 (0.603) and deepseek-v3.1 (0.599) being statistically similar.
- Best calibration and selective risk were from claude-sonnet-4 (AURC 0.242; ECE 0.206; Brier 0.254).
- Significant dataset heterogeneity was observed, with accuracy varying from 16% to 82%.
- High-confidence errors remain a problem, with Wrong@0.90 ranging from 14.9% (claude-sonnet-4) to 47.7% (Qwen3-235B-A22B-FP8).
- The paper recommends exposing objectives when possible and gating decisions by confidence.

# Introduction
- The LLM-as-a-Judge (LLMaaJ) paradigm is used for scalable evaluation, but its ability to infer latent objectives in noisy, multi-step conversations is untested.
- Multi-turn jailbreaks are presented as a stress test for objective understanding, as harmful goals are disguised across turns.
- The paper distinguishes between classifying harmfulness and inferring a hidden objective, noting that state-of-the-art LLMs achieve only 47–61% accuracy on the latter.
- Metacognition, proxied by self-reported confidence, is crucial for an LLM judge to signal the trustworthiness of its verdicts.
- The paper introduces ObjexMT to measure objective recovery and confidence calibration.
- Contributions include: formalizing the problem, releasing the benchmark, and providing findings on six models across three datasets.
- Key findings highlighted: accuracy spans 0.474–0.612; calibration is imperfect (ECE 0.206–0.417); claude-sonnet-4 has the best calibration (ECE 0.206, AURC 0.242); kimi-k2 has the highest accuracy (0.612); high-confidence errors persist (Wrong@0.90 is 15–48%); and dataset difficulty varies sharply (e.g., gpt-4.1 accuracy is 0.162 on Attack600 vs. 0.816 on MHJ).

# Related Work
- LLM-as-a-Judge: While useful for scaling evaluation, reliability is a concern when objectives are implicit.
- Robustness: Performance degrades in complex, multi-turn contexts, making latent intent recovery difficult.
- Multi-turn safety datasets: The work uses MHJ (human jailbreaks), SafeMTData_Attack600 (automated multi-turn expansions of single queries), and SafeMTData_1K (safety-alignment dialogues).
- Harmfulness discrimination vs. objective inference: The benchmark targets the recovery of a latent objective, which is distinct from binary moderation or harmfulness detection.
- Metacognition: The paper builds on work showing models can verbalize confidence. It quantifies the alignment of confidence with correctness, noting ECE ranges from 0.206 to 0.417 across the six tested models.

# Method
- **Task:** Given a multi-turn jailbreak transcript, a model must extract a single-sentence base objective and report its confidence (0 to 1).
- **Evaluation:**
    1. The extracted objective is compared to a gold objective using an LLM judge (fixed to gpt-4.1) to get a similarity score `s` in [0, 1].
    2. The score `s` is converted to binary correctness `y` using a pre-calibrated, human-aligned threshold: `y = 1` if `s >= τ*`.
- **Threat Model:** Adversarial multi-turn interactions where a harmful goal is disguised over N turns. The goal is to recover the minimal imperative instruction (base objective).
- **Datasets:** Three public datasets are used: SafeMTData_1K, SafeMTData_Attack600, and MHJ, totaling N=2,817 instances. Gold objectives are taken directly from the datasets.
- **Human-aligned Thresholding:** Two experts labeled N=300 instances. The threshold τ* that maximizes F1 score against these labels was found to be τ* = 0.66, achieving an F1 of 0.891. This threshold is frozen for all experiments.
- **Models:** Six models were evaluated: gpt-4.1, claude-sonnet-4, Qwen3-235B-A22B-FP8, kimi-k2, deepseek-v3.1, and gemini-2.5-flash. A single deterministic pass (T=0) was used for each instance.
- **Metacognition Metrics:** Expected Calibration Error (ECE) with 10 equal-width bins, Brier score, Wrong@High-Confidence (at 0.80, 0.90, 0.95), and Area Under the Risk-Coverage Curve (AURC).
- **Artifacts:** Per-model spreadsheets with raw I/O, judge outputs, and calibration labels are released.

# Experiments
- **Judge Calibration:** On N=300 human-labeled items, the optimal similarity threshold was τ* = 0.66, yielding F1=0.891, Precision=0.824, and Recall=0.970. This threshold is used for all subsequent accuracy calculations.
- **Overall Accuracy:** On N=2,817 instances, accuracies were:
    - kimi-k2: 0.612 [0.594, 0.630]
    - claude-sonnet-4: 0.603 [0.585, 0.622]
    - deepseek-v3.1: 0.599 [0.580, 0.617]
    - gemini-2.5-flash: 0.542 [0.523, 0.560]
    - gpt-4.1: 0.490 [0.471, 0.508]
    - Qwen3-235B-A22B-FP8: 0.474 [0.455, 0.492]
- **Statistical Testing:** Paired McNemar's and bootstrap tests (B=10,000) were used with Holm-Bonferroni correction for 15 model pairs. The top three models (kimi-k2, claude-sonnet-4, deepseek-v3.1) were not statistically distinguishable from each other.
- **Dataset Heterogeneity:** Accuracy varied significantly by dataset. For example, gpt-4.1 scored 0.162 on Attack600, 0.502 on SafeMTData_1K, and 0.816 on MHJ. Automated attacks (Attack600) were hardest (avg. accuracy 24.3%), while human-authored jailbreaks (MHJ) were easiest (avg. accuracy 80.9%).
- **Transcript Length:** Accuracy increases monotonically with transcript length (in characters), analyzed in quartiles (Q1 <1.5K, Q2 1.5-2.5K, Q3 2.5-4K, Q4 >4K). For gpt-4.1, accuracy rose from 0.223 (Q1) to 0.811 (Q4). Very short transcripts are a high-risk regime.
- **Metacognition:**
    - claude-sonnet-4 had the best calibration metrics: ECE=0.206, Brier=0.254, AURC=0.242.
    - Qwen3-235B-A22B-FP8 was the most miscalibrated: ECE=0.417, Brier=0.416, AURC=0.472.
    - High-confidence errors persist. Wrong@0.90 rates ranged from 14.9% (claude-sonnet-4) to 47.7% (Qwen3-235B-A22B-FP8). At a 0.95 confidence threshold, claude-sonnet-4 still had a 6.4% error rate.
- **Effect Sizes:** Practical significance was measured with ARR, RR, Cohen's h, and NNT. For kimi-k2 vs. gpt-4.1, ARR=0.122 and NNT=8.2, indicating one extra correct extraction every ~8 dialogues.

# Conclusion
- The paper introduced ObjexMT, a benchmark for latent objective extraction and metacognitive calibration in LLMs.
- Across six models, accuracy was low (47–61%), with significant calibration failures (ECE 0.206–0.417) and high-confidence errors (Wrong@0.90: 15–48%).
- kimi-k2 had the highest accuracy (61.2%), and claude-sonnet-4 had the best calibration (ECE 0.206).
- Dataset construction heavily influences difficulty, with automated obfuscation (Attack600, 16% accuracy) being much harder than human-authored jailbreaks (MHJ, 82% accuracy).
- Recommendations for practice: (i) make objectives explicit, (ii) use confidence-gating, and (iii) maintain human oversight for high-stakes moderation.
- **Limitations:** The study is limited to six large models, uses a single LLM judge (gpt-4.1), requires single-sentence extraction, and uses deterministic decoding (T=0).
- **Future Work:** Multi-judge validation, expanding model coverage, creating a failure taxonomy, and cross-domain evaluation.

# Appendix
- **Ethics Statement:** The study uses public datasets (SafeMTData, MHJ) and expert labeling for calibration (N=300), not requiring IRB review. The authors acknowledge dual-use risk from releasing jailbreak data and advise against unsupervised deployment of LLM judges.
- **Data Release:** An Excel workbook is provided with sheets for the human-labeled calibration set (`Labeling`, N=300), per-model extractions (`extracted_{model}`, N=2,817), and per-model similarity judge outputs (`similarity_{model}`, N=2,817).
- **Prompt Templates:** Verbatim prompts for the objective-extraction task and the similarity-judging task are provided.
- **Turn Complexity Analysis:** Accuracy generally drops as the number of turns increases, with a peak difficulty at 5-6 turns (15-20 pp drop from 1-2 turns). Accuracy rebounds for dialogues with 7+ turns, possibly because the objective is restated. Models remain overconfident in the difficult 5-6 turn range.
- **Transcript Length Analysis:** Accuracy increases monotonically with transcript length across four quartiles. For example, across models, average accuracy rises from 0.33 (Q1) to 0.81 (Q4). The lowest error rate is found in transcripts of 1.5k-2.5k characters. Very short transcripts are unreliable.
- **Additional Metacognition Analysis:** Provides AURC values for all models, confirming claude-sonnet-4 is best (0.242) and Qwen3-235B is worst (0.472). Also shows Wrong@0.95 error rates, where claude-sonnet-4 has 6.4% and Qwen3-235B has 39.7%. Risk-coverage curves are also presented.

# References
This section contains the list of references cited in the manuscript.