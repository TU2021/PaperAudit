Summary
The paper introduces ObjexMT, a benchmark and evaluation protocol for testing whether LLM-as-a-Judge systems can recover a conversation’s latent base objective in adversarial multi-turn jailbreak settings and calibrate their self-reported confidence. The task requires models to output a single-sentence imperative objective plus a confidence in [0,1], and uses a fixed LLM judge (gpt-4.1) to compute semantic similarity against dataset-provided gold objectives. A human-calibrated threshold (τ* = 0.66 from N=300; F1=0.891) maps similarity to binary correctness. Six models are evaluated on 2,817 dialogues drawn from SafeMTData_Attack600, SafeMTData_1K, and MHJ. Results show modest extraction accuracy (47–61%), pronounced dataset heterogeneity, and substantial miscalibration in self-reported confidence; kimi-k2 leads accuracy, while claude-sonnet-4 exhibits best calibration and selective risk.

Soundness
- Task formulation is clear and operationalized: extract a minimal single-sentence imperative objective and a confidence; compare via semantic similarity judged by a fixed model (§3.1–3.6). The single-pass, T=0 decode avoids run-to-run variance, aiding comparability (§3.4).
- The judge-to-binary mapping is human-aligned and documented (N=300, τ* selected to maximize F1 with ties broken toward smaller τ; Table 1; §3.6). This is an appropriate way to ground LLM-judge scores, and the resulting precision/recall values are reported.
- Statistical testing is sound: paired McNemar’s test for 2×2 disagreements and bootstrap CIs/p-values for accuracy differences; Holm–Bonferroni corrects the 15 pairwise comparisons (§4.3; Tables 2–3). Effect sizes (ARR, RR, Cohen’s h, NNT) add practical meaning (Table 7).
- Metacognition metrics (ECE with equal-width bins, equal-mass ECE, Brier, Wrong@High-Confidence, AURC) are standard and computed against frozen correctness labels (§3.7; Table 5; Figs. 2–3).
- Potential threats to validity: the fixed single judge (gpt-4.1) both scores similarity and is an evaluated model (§3.5), risking judge-specific biases; the single-sentence constraint may oversimplify multi-goal attacks (§3.2); dataset schema is used “as-is” without normalization (§2.3, §3.3), which can entangle label style with accuracy. These concerns are acknowledged in Limitations (§6) but could be further mitigated experimentally.

Presentation
- Clarity and organization are strong: the paper states the problem, method, datasets, metrics, and findings in a coherent flow (Abstract; §§1–4). Anchored tables/figures communicate results well: Table 3 for overall accuracy, Table 5 for calibration metrics, Fig. 1 for dataset heterogeneity, and Fig. 3 for calibration–accuracy trade-offs.
- Prompts are provided verbatim (Appx. A.1–A.2), and artifacts for reproduction are detailed (Appx. §A, §B–§C; §3.8; Appendix “Artifacts for reproduction”).
- Minor inconsistencies/typos in some appendix panels (e.g., Block 27 includes an extra “Accuracy” column with 0.8/0.6/0.4 not directly explained; Dataset Performance Heatmap in Appx. shows an unexplained final column) slightly detract from polish, but core tables/figures are consistent with the text.

Contribution
- The benchmark targets a capability not directly measured by prior LLMaaJ work: latent objective extraction under multi-turn obfuscation plus metacognitive calibration (§1; §§2.4–2.5). The explicit coupling of extraction accuracy with confidence calibration and selective-risk analysis is novel and operationally relevant for safety pipelines.
- Empirical insights are actionable: accuracy is limited (47–61%), calibration is imperfect (ECE 0.206–0.417), high-confidence errors persist (Wrong@0.90 up to 47.7%), and dataset construction strongly affects difficulty (Table 4; Fig. 1).
- Releasing per-model spreadsheets with raw I/O, judge scores, and labels fosters reproducibility (§3.8; Appendix release).

Strengths
- Clear, reproducible methodology with human-calibrated thresholding (Table 1; §3.6).
- Strong statistical rigor (paired tests, bootstrap CIs, multiple-comparison control; §4.3).
- Comprehensive calibration/selection analysis (Table 5; Figs. 2–3; Appx. selective-risk curves).
- Insightful heterogeneity and length/turn analyses (Fig. 1; Table 4; Appx. B/C; Table 9) that inform practical deployment decisions (e.g., confidence gating, prompting for explicit objectives).
- Transparent limitations and ethics, plus artifact release (§6; Appendix Ethics/Artifacts).

Weaknesses
- Reliance on a single LLM judge (gpt-4.1) that also appears among evaluated systems may bias similarity scoring (§3.5); multi-judge or human-ensemble sensitivity analyses are not reported beyond the N=300 threshold calibration.
- The single-sentence imperative constraint may under-represent multi-objective or conditional goals (§3.2), potentially penalizing faithful extractions; no ablation quantifies this constraint’s impact.
- Schema heterogeneity across datasets is used “as-is” (§2.3; §3.3); style differences between golds could influence judge similarity, confounding accuracy. A normalization or ontology mapping baseline would help.
- Confidence elicitation is model-internal and uncalibrated; although ECE/Brier are reported, post-hoc calibration (e.g., isotonic, temperature scaling on confidence) is not attempted, nor are coverage rates at high-confidence thresholds reported alongside Wrong@High-Conf (Table 6 lists conditional error, but not coverage).
- Minor presentation issues in appendix tables/plots (Block 27; Appx. Dataset Heatmap extra column) require cleanup.

Questions
- How sensitive are accuracy rankings and calibration metrics to the choice of judge and τ*? Could you report a multi-judge ensemble and a threshold sweep sensitivity on the full set (beyond the N=300 calibration), or a calibration curve aligning judge scores to human labels (§3.5–§3.6; Table 1)?
- Did any models produce invalid JSON, and if so, how many items were excluded from accuracy vs. calibration computations (§3.7)? Please report per-model exclusion counts and whether missing entries could bias ECE/AURC.
- Could you provide per-dataset calibration metrics (ECE/Brier/AURC) to see if miscalibration correlates with dataset construction (Table 4; Fig. 1)?
- Would a two-sentence or structured base objective (where gold supports it) materially change the relative rankings (§3.2)? A small ablation on MHJ (high coherence) could be informative.
- Can you quantify any judge bias toward syntactic similarity vs. semantic equivalence? For example, adversarial paraphrase tests where golds are paraphrased to different styles (§3.5–§3.6).

Rating
- Overall (10): 8 — Well-scoped benchmark with rigorous analysis and actionable findings, tempered by single-judge reliance and schema heterogeneity (§§3.5–3.6; Table 3; Table 5; Fig. 1).
- Novelty (10): 8 — Latent objective extraction plus metacognitive calibration for LLM judges is a fresh, operationally relevant angle with new metrics integration (§1; §§2.4–2.5; §3.7).
- Technical Quality (10): 8 — Sound threshold calibration, paired testing, bootstraps, and effect sizes; missing multi-judge sensitivity and calibration ablations are the main gaps (Table 1; §4.3; Table 7).
- Clarity (10): 8 — Clear method and results with strong artifact release; minor appendix inconsistencies reduce polish (Table 3; Table 5; Appx. A; Block 27).
- Confidence (5): 4 — High confidence based on careful reading and cross-checks of tables/figures (Tables 1–5; Fig. 1); some concerns about single-judge bias and appendix inconsistencies.

---

Summary
ObjexMT proposes a benchmark to evaluate whether LLM judges can infer a dialogue’s latent base objective under multi-turn jailbreaks and whether their self-reported confidence is calibrated. The method uses a fixed LLM judge to score semantic similarity between model-extracted objectives and gold objectives, converts similarity to correctness via a human-calibrated threshold (τ* = 0.66 on N=300), and assesses calibration using ECE, Brier, Wrong@High-Confidence, and selective risk (AURC). Experiments across six models on three datasets show modest objective-extraction accuracy and notable calibration failures, with kimi-k2 leading accuracy and claude-sonnet-4 leading calibration.

Soundness
- The problem setting is well motivated: harmful intent can be obfuscated across turns; judges need to recover objectives and know when they are uncertain (§1; §2.2–§2.5).
- Evaluation pipeline is coherent: extraction prompt (Appx. A.1) yields base_prompt and confidence; similarity judging (Appx. A.2) returns a score/category; binary correctness uses a single τ* calibrated on human labels (§3.5–§3.6). This provides a reproducible mapping from text similarity to correctness (Table 1).
- Statistical methodology is appropriate: bootstrap CIs, McNemar tests, Holm–Bonferroni for family-wise control (§4.2–§4.3; Tables 2–3).
- However, the choice of a single judge and a single similarity threshold may induce systematic biases. For example, a judge that prefers certain phrasings may advantage/disadvantage models that align with its style. The authors acknowledge this limitation (§6) but do not report sensitivity analyses (e.g., alternate judge models, ensemble judges, τ sweeps on full data).
- The single-sentence imperative constraint is practical, yet it may force models to collapse multiple intents or conditional requirements, possibly reducing true-positive matches (§3.2). There is no ablation to quantify this trade-off.

Presentation
- The manuscript is clearly written, with strong use of tables/figures to summarize findings (Table 3 accuracy, Table 5 calibration, Fig. 1 dataset heterogeneity, Fig. 3 calibration–accuracy trade-off).
- Prompts and artifacts are shared and well documented (§3.8; Appx. A; Appendix Artifacts), aiding reproducibility.
- A few appendix figures/tables have formatting or unexplained columns (Block 27; Appx. Dataset Heatmap extra column), which can confuse readers but do not affect main results.

Contribution
- The benchmark bridges a gap between harmfulness detection and intent inference, measuring a capability crucial for LLM-as-a-Judge deployments (§2.4; §1).
- It integrates metacognitive calibration with task effectiveness, providing operational metrics (ECE, Brier, Wrong@High-Conf, AURC) and effect sizes to inform deployment decisions (§3.7; Table 5; Table 7).
- Dataset heterogeneity analysis (Table 4; Fig. 1) and length/turn analyses (Table 9; Appx. B/C) reveal important failure modes and contexts where gating helps.

Strengths
- Clear problem formalization and rigorous evaluation pipeline with human-calibrated threshold (Table 1; §3.6).
- Comparative study with uncertainty quantification and statistical tests (Tables 2–3; §4.3).
- Thorough calibration and selective-risk assessment (Table 5; Fig. 2; Fig. 3).
- Practical insights on dataset heterogeneity and transcript length effects (Table 4; Fig. 1; Table 9).
- Reproducible artifacts and transparent ethics/limitations (Appendix).

Weaknesses
- Single judge (gpt-4.1) introduces potential evaluation bias; using the same model as both competitor and judge is not ideal (§3.5). Multi-judge/ensemble analysis is missing.
- Threshold τ* is calibrated on N=300 items; while reasonable, sensitivity of overall rankings to τ is not shown (§4.1–§4.2). A robustness sweep or ROC-style analysis would strengthen claims.
- Confidence calibration lacks post-hoc methods (e.g., isotonic regression); Wrong@High-Conf is reported without coverage, limiting operational interpretation (Table 5–6).
- The single-sentence constraint may penalize faithful multi-objective recoveries; no structured-output fallback is considered (§3.2).
- Minor presentation flaws in the appendices.

Questions
- How often did the judge disagree with human labels at τ*? Beyond F1, can you report confusion between judge categories and human labels to identify systematic judge errors (Table 1)?
- What fraction of items per model had invalid JSON or missing confidence (§3.7), and how were they handled for accuracy vs. calibration metrics?
- Could you report per-dataset calibration metrics (ECE/Brier/AURC) to see if miscalibration is driven by dataset construction (Table 4; Fig. 1)?
- Did you test alternative prompts for confidence elicitation (e.g., scalar vs. verbal confidence prompts as in [11]) and their impact on ECE (§3.7; Appx. A.1)?
- Would an ensemble judge (e.g., claude + deepseek + gpt-4.1) materially change pairwise accuracy conclusions (Tables 2–3)?

Rating
- Overall (10): 7 — Valuable benchmark and analysis with practical insights, but single-judge dependence and limited sensitivity analyses reduce robustness (§§3.5–3.6; Table 5; Fig. 1).
- Novelty (10): 8 — Intent extraction under multi-turn obfuscation plus metacognitive calibration is a meaningful advancement over standard LLMaaJ evaluation (§1; §2.4–§2.5).
- Technical Quality (10): 7 — Solid statistics and metrics; missing multi-judge sensitivity, threshold sweeps, and post-hoc calibration ablations are notable gaps (Table 1; §4.3; Table 6).
- Clarity (10): 8 — Clear main text and artifacts; minor appendix inconsistencies need polish (Tables 2–3; Table 5; Appx. A/B/C).
- Confidence (5): 4 — High confidence from cross-checking tables/figures and methods; reservations about single-judge bias.

---

Summary
The authors present ObjexMT, a benchmark evaluating LLM judges’ ability to extract hidden base objectives from multi-turn jailbreak transcripts and to calibrate confidence. Using a fixed LLM judge (gpt-4.1) to score semantic similarity and a human-aligned threshold (τ* = 0.66) to determine correctness, they compare six models across three datasets (N=2,817). They report modest accuracies (kimi-k2 0.612, claude-sonnet-4 0.603, deepseek-v3.1 0.599; Table 3), large dataset effects (MHJ much easier than Attack600; Fig. 1; Table 4), and varying calibration (claude-sonnet-4 ECE 0.206 vs. Qwen3 0.417; Table 5).

Soundness
- The approach appropriately separates extraction (semantic equivalence to gold objectives) from harmfulness classification, aligning with the stated research question (§1; §2.4).
- A single calibrated threshold provides a transparent binary mapping (Table 1; §3.6), and paired testing with Holm–Bonferroni control is methodologically sound (§4.3; Tables 2–3).
- The metacognitive evaluation employs established measures (ECE, Brier, Wrong@High-Conf, AURC; §3.7; Table 5), and selective prediction analysis is operationally relevant (Fig. 3; Appx. Fig. 8).
- Risks: The fixed judge and single-sentence constraint can introduce evaluation artifacts (§3.2; §3.5). Also, dataset heterogeneity is not normalized (§3.3), so accuracy may reflect gold label style/coherence as much as model capability.

Presentation
- The paper is well organized, with clearly labeled results and a tight narrative from problem to findings (Abstract; §§1–4). Tables and figures directly support claims (Table 3, Table 5, Fig. 1).
- Reproducibility is emphasized (prompts in Appx. A; artifacts in §3.8 and Appendix).
- Minor formatting issues in appendix tables/plots (Block 27 and some heatmaps) should be corrected but do not undermine main conclusions.

Contribution
- The work offers a concrete and replicable test for a critical capability: inferring latent objectives under obfuscation and calibrating confidence. This moves beyond binary safety detection and addresses practical deployment needs (confidence gating, human deferral).
- The empirical insights (e.g., high Wrong@0.90, pronounced dataset-driven difficulty; Table 5; Table 4) have immediate safety implications and suggest design recommendations (make objectives explicit; gate by confidence; §5; §40).

Strengths
- Strong empirical rigor and breadth of metrics (Tables 2–3, 5; §4.3).
- Human-aligned thresholding and transparent release of per-item artifacts (§3.6; §3.8).
- Useful secondary analyses (turn complexity and length effects; Appx. B/C; Table 9).
- Clear guidance and limitations for appropriate use (Appendix; §6; §40).

Weaknesses
- Evaluation depends on a single judge that is also an evaluated system (§3.5), which may bias similarity scoring; sensitivity to judge identity is unreported.
- Threshold robustness is not explored beyond N=300 calibration items; a full-dataset sensitivity sweep would strengthen conclusions (Table 1).
- Confidence coverage is not reported alongside Wrong@High-Conf (Table 6), limiting deployment trade-off discussions (risk vs. number of accepted items).
- The single-sentence constraint could mask multi-faceted objectives; no ablation tests the constraint (§3.2).
- Minor inconsistencies in appendix visuals could confuse readers.

Questions
- Can you report coverage at high-confidence thresholds (0.80/0.90/0.95) to complement Wrong@High-Conf (Table 6), enabling risk–benefit assessment?
- How sensitive are rankings to alternate judges (e.g., claude-sonnet-4 or deepseek-v3.1) or judge ensembles (§3.5)? A brief comparison would address bias concerns.
- Have you compared equal-width vs. equal-mass ECE across datasets (noted in §3.7) and do conclusions change materially?
- Could structured outputs (e.g., objective + constraints + target) be supported when gold labels permit them (§3.2), and how would this affect judge similarity?

Rating
- Overall (10): 7 — Solid benchmark with practical insights; single-judge dependence and limited robustness checks constrain conclusiveness (Table 3; Table 5; §3.5–§3.6).
- Novelty (10): 7 — Measuring latent objective extraction and metacognitive calibration for LLM judges is an important and underexplored area (§1; §2.4–§2.5).
- Technical Quality (10): 7 — Appropriate statistics and calibration metrics; missing sensitivity analyses and post-hoc calibration baselines are main gaps (Table 1; §4.3; Table 6).
- Clarity (10): 8 — Clear exposition and artifacts; minor appendix inconsistencies aside (Fig. 1; Table 5; Appx. A/B/C).
- Confidence (5): 4 — High confidence from cross-verification of tables/figures; reservations due to single-judge bias.

---

Summary
This paper proposes ObjexMT, a benchmark and evaluation framework for testing whether LLM judges can infer latent objectives from adversarial multi-turn transcripts and whether their self-reported confidence aligns with correctness. The authors calibrate a similarity-to-correctness threshold using human labels (τ* = 0.66; F1=0.891; Table 1), then evaluate six models across 2,817 instances from SafeMTData_Attack600, SafeMTData_1K, and MHJ. They report accuracy rankings (kimi-k2 highest; Table 3), dataset heterogeneity (MHJ easiest; Fig. 1; Table 4), and calibration metrics (claude-sonnet-4 best ECE/Brier/AURC; Table 5), emphasizing persistent high-confidence errors.

Soundness
- The methodology blends semantic similarity judging with human-calibrated binarization, yielding a defensible correctness label (§3.5–§3.6). Using McNemar’s test and bootstrap CIs for paired comparisons is appropriate (§4.3).
- Confidence evaluation with ECE/Brier/Wrong@High-Conf/AURC is standard and informative (§3.7; Table 5); the selective risk–coverage curves (Appx. Fig. 8; Fig. 3) align with claims.
- Secondary analyses (turn complexity and transcript length; Appx. B/C; Table 9) support nuanced operational recommendations.
- Limitations are candid (§6), notably the single judge and restricted model coverage. The absence of sensitivity to judge choice and τ* is a methodological gap.

Presentation
- The manuscript is well structured, and key results are presented succinctly (Tables 2–3, 5; Figs. 1–3). Prompts and artifacts enhance reproducibility (Appx. A; §3.8).
- Minor presentation issues in the appendix (e.g., unexplained columns/values in Block 27 and some dataset heatmaps) should be corrected but do not undermine core findings.

Contribution
- ObjexMT advances evaluation of LLM judges from surface harmfulness detection to the more challenging latent-intent recovery, adding calibrated confidence as a core axis. This contributes a practically relevant benchmark, metrics suite, and insights that transfer to related domains (multi-hop QA, tool-use auditing; §1; §5).

Strengths
- Human-aligned correctness mapping and transparent methodology (Table 1; §3.6).
- Robust statistical comparisons with uncertainty quantification and multiple-comparison correction (Tables 2–3; §4.3).
- Comprehensive calibration and selective-risk reporting (Table 5; Fig. 3; Appx. Fig. 8).
- Clear evidence of dataset-driven difficulty and contextual factors (Table 4; Fig. 1; Table 9).
- Reproducibility: detailed artifacts and prompts (Appx. A; §3.8).

Weaknesses
- Single judge (gpt-4.1) may bias evaluation; the judge also being an evaluated model is suboptimal (§3.5).
- No sensitivity analysis for τ* beyond N=300 calibration; rankings may shift with alternate thresholds (§4.1–§4.2).
- Confidence coverage at high thresholds is missing (Table 6 reports conditional error only), limiting deployment decisions.
- The single-sentence constraint may underfit multi-faceted objectives; no ablation on structured outputs (§3.2).
- Some appendix visuals contain formatting inconsistencies.

Questions
- Please report the coverage (fraction of items) at confidence ≥0.80/0.90/0.95 for each model to contextualize Wrong@High-Conf (Table 6).
- How would results change with a different judge (e.g., claude-sonnet-4) or an ensemble? Even a small-scale sensitivity would be useful (§3.5).
- Could you provide calibration metrics per dataset (ECE/Brier/AURC) to see whether miscalibration is tied to dataset construction (Table 4; Fig. 1)?
- Did you attempt post-hoc calibration of self-reported confidence (isotonic/Platt scaling) and, if so, how did AURC/Wrong@High-Conf change (§3.7)?
- Are there qualitative failure taxonomies for Wrong@0.90 cases (noted as a future priority in §6)? A small illustrative set would add interpretability.

Rating
- Overall (10): 8 — Strong, timely benchmark with rigorous analysis and practical guidance; main caveat is reliance on a single judge and limited robustness checks (Tables 2–3; Table 5; §3.5–§3.6).
- Novelty (10): 8 — Evaluating latent objective extraction with metacognitive calibration under multi-turn obfuscation is a distinctive contribution (§1; §2.4–§2.5).
- Technical Quality (10): 8 — Solid design and statistics; sensitivity to judge/threshold and calibration ablations would further strengthen validity (Table 1; §4.3; Table 6).
- Clarity (10): 8 — Clear exposition and useful figures/tables; minor appendix formatting issues remain (Table 3; Fig. 1; Table 9).
- Confidence (5): 4 — High confidence after cross-checking claims against tables/figures; remaining uncertainty due to single-judge bias.