Summary
The paper introduces ObjexMT, a benchmark and evaluation protocol for assessing whether LLM-as-a-Judge systems can recover a conversation’s latent base objective in adversarial multi-turn jailbreak settings and whether their self-reported confidence is calibrated. The task constrains models to produce a single-sentence imperative statement of the inferred objective plus a confidence in [0,1]. A fixed LLM judge (gpt-4.1) scores semantic similarity between model outputs and dataset-provided gold objectives. A human-calibrated threshold (τ* = 0.66, derived from N=300 human-labeled items; F1=0.891) maps similarity scores to binary correctness. Six models are evaluated on 2,817 dialogues drawn from SafeMTData_Attack600, SafeMTData_1K, and MHJ, using a single-pass, temperature-zero decode to standardize runs. Statistical analysis employs paired McNemar tests, bootstrap confidence intervals and p-values for accuracy differences, Holm–Bonferroni correction across pairwise comparisons, and effect sizes (ARR, RR, Cohen’s h, NNT). Metacognitive evaluation uses ECE (equal-width and equal-mass), Brier score, Wrong@High-Confidence, and selective risk (AURC) computed against the frozen correctness labels. Results show modest extraction accuracy (roughly 47–61%), pronounced dataset heterogeneity (with MHJ much easier than Attack600), and substantial miscalibration in self-reported confidence. kimi-k2 leads accuracy, while claude-sonnet-4 exhibits the best calibration and selective-risk trade-offs. High-confidence errors remain frequent (e.g., Wrong@0.90 up to 47.7%). The manuscript is clearly written, with prompts and artifacts provided for reproducibility, though minor appendix formatting inconsistencies are noted.

Strengths
- Clear, well-motivated problem formulation focused on latent objective extraction under multi-turn obfuscation, distinct from standard harmfulness classification.
- Reproducible evaluation pipeline: single-sentence imperative output plus confidence, fixed judge scoring, and a human-aligned similarity-to-correctness mapping via a calibrated threshold.
- Strong statistical rigor: paired McNemar tests for disagreement, bootstrap uncertainty, Holm–Bonferroni correction for multiple comparisons, and reporting of practical effect sizes.
- Comprehensive metacognition assessment: ECE (equal-width and equal-mass), Brier, Wrong@High-Confidence, and AURC, with selective prediction curves that illuminate deployment-relevant trade-offs.
- Insightful analysis of dataset heterogeneity and contextual factors (e.g., transcript length and turns), explaining large variance in difficulty and informing decisions like confidence gating or prompting for explicit objectives.
- Single-pass, temperature-zero decoding reduces run-to-run variance, aiding comparability across models.
- Actionable empirical insights: limited extraction accuracy, imperfect calibration, prevalence of high-confidence errors, and strong dependence on dataset construction.
- High-quality presentation of results (accuracy rankings, calibration metrics, trade-off plots) and transparent limitations and ethics discussion.
- Robust reproducibility: verbatim prompts and per-model artifacts (raw I/O, judge scores, labels) enabling independent verification.

Weaknesses
- Reliance on a single LLM judge (gpt-4.1) for similarity scoring introduces potential evaluation bias; the judge is also among the evaluated systems, which can advantage models aligned with its style. No multi-judge, ensemble, or alternate-judge sensitivity analyses are reported.
- Threshold robustness is underexplored: τ* is calibrated on N=300 items, but there is no full-dataset threshold sweep or calibration curve to assess stability of rankings and metrics across τ choices.
- The single-sentence imperative constraint may under-represent multi-objective or conditional goals, potentially penalizing faithful extractions in complex attacks; no ablation quantifies how this constraint affects correctness or model comparisons, and no structured-output fallback is considered where gold labels permit it.
- Dataset schema and label style are used “as-is” without normalization or ontology mapping, risking confounds where stylistic similarity influences judge scores; an explicit normalization baseline or paraphrase sensitivity test would clarify judge bias toward syntax versus semantics.
- Confidence elicitation relies on model-internal, uncalibrated self-report; while ECE, Brier, and AURC are reported, no post-hoc calibration baselines (e.g., isotonic regression or temperature scaling) are attempted to quantify potential improvements.
- Wrong@High-Confidence is reported without the corresponding coverage rates at those thresholds, limiting operational interpretation of risk vs. throughput when applying confidence gating.
- Practical details about invalid outputs (e.g., malformed JSON or missing confidence) and how exclusions were handled for accuracy vs. calibration metrics are not clearly documented, leaving possible biases in ECE/AURC unaddressed.
- Minor presentation issues in appendix tables/figures (e.g., unexplained columns/values in certain blocks and heatmaps) detract from polish, though they do not undermine core findings.
