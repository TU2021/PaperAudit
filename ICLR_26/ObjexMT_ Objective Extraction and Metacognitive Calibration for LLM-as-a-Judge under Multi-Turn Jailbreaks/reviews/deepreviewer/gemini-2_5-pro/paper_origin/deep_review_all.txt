Here are four distinct reviews of the paper "ObjexMT: Objective Extraction and Metacognitive Calibration for LLM-as-a-Judge under Multi-Turn Jailbreaks".

***

### **Review 1**

**Summary**
This paper introduces ObjexMT, a new benchmark designed to evaluate two critical capabilities of LLM-as-a-Judge (LLMaaJ): 1) extracting the latent objective from multi-turn, adversarial conversations (jailbreaks), and 2) assessing the model's own confidence in this extraction (metacognition). The authors evaluate six large language models on three datasets, measuring extraction accuracy via semantic similarity to a gold objective (judged by GPT-4.1 and binarized with a human-aligned threshold) and metacognition using standard calibration metrics like ECE, Brier score, and AURC. The key findings are that current models have modest accuracy (47-61%), exhibit poor calibration, and make a significant number of high-confidence errors, with performance varying dramatically across datasets.

**Soundness**
The methodology is generally sound and rigorous. The task of objective extraction is well-defined, and the choice to constrain it to a single imperative sentence is a pragmatic way to ensure evaluability. The evaluation framework is a key strength:
1.  **Human-aligned thresholding:** The process of calibrating the LLM judge's similarity score against human labels (N=300) to find an optimal threshold (τ* = 0.66) is a thoughtful approach to grounding the automated metric (Sec. 3.7, Table 1).
2.  **Comprehensive metrics:** The paper uses a robust suite of metrics for both accuracy (with confidence intervals) and metacognition (ECE, Brier, Wrong@High-Conf, AURC), providing a multi-faceted view of model performance (Table 5).
3.  **Statistical rigor:** The use of paired statistical tests (McNemar's) with multiple comparison correction (Holm-Bonferroni) and bootstrap CIs for uncertainty quantification is commendable and adds significant credibility to the comparisons between models (Sec. 4.3, Table 2).

However, the entire evaluation pipeline hinges on a single LLM judge (gpt-4.1). While the authors acknowledge this limitation, it remains a significant methodological concern. The performance rankings could be sensitive to the specific biases of the chosen judge model. For instance, the judge might be inherently better at recognizing similarities for outputs generated by models from the same family or with similar architectural properties.

**Presentation**
The paper is exceptionally well-written, clear, and dense with information. The structure is logical, flowing from problem definition to methodology, results, and conclusion. Tables and figures are, for the most part, effective and well-designed. Table 2, which combines accuracy, CIs, and pairwise significance tests, is particularly efficient at conveying the core performance comparisons. Figure 3 provides an excellent visualization of the accuracy-calibration trade-off. The inclusion of verbatim prompts in the appendix (Appx. A) is a great practice for reproducibility.

**Contribution**
The primary contribution is the formalization of the "objective extraction" problem in the context of LLMaaJ and the creation of the ObjexMT benchmark to measure it. While the components (LLM-as-a-judge, calibration metrics) are not new in isolation, their synthesis into a focused, rigorous benchmark for this specific, critical task is novel and valuable. The paper moves beyond simple harmfulness classification to probe a deeper level of understanding: latent intent recovery. The findings, particularly the stark dataset heterogeneity (Fig. 1) and the persistence of high-confidence errors (Table 6), provide important, empirically-grounded insights for the LLM safety and evaluation community.

**Strengths**
- **Problem Formulation:** The paper clearly articulates a critical and previously under-examined problem: whether an LLM judge can reliably infer a user's latent objective from noisy, multi-turn dialogue.
- **Methodological Rigor:** The use of human-aligned thresholding, comprehensive metrics, and robust statistical testing makes the results highly credible.
- **Actionable Insights:** The findings lead to concrete recommendations, such as gating decisions by confidence and being wary of medium-turn-count dialogues (Appx. B).
- **Transparency and Reproducibility:** The authors provide detailed methods, prompts, and a commitment to release all data and code, setting a high standard for experimental research.

**Weaknesses**
- **Single Judge Model:** The reliance on gpt-4.1 as the sole arbiter of semantic similarity is the most significant weakness. The results are conditioned on this judge's specific behavior, and using a different judge could potentially alter the model rankings. This is noted in the limitations (Sec. 6) but it fundamentally affects the generalizability of the core accuracy results.
- **Calibration Set Size:** The human-calibration set of N=300 is relatively small. While the reported F1 score of 0.891 is high, the precision (0.824) is notably lower than recall (0.970), suggesting the threshold τ*=0.66 is lenient and may classify some moderately similar extractions as correct, potentially inflating accuracy scores.

**Questions**
1.  How sensitive are the model accuracy rankings to the choice of the judge model? Have you performed an ablation study where another powerful model, like `claude-sonnet-4`, is used as the judge to see if the relative performance of `kimi-k2`, `claude-sonnet-4`, and `deepseek-v3.1` holds?
2.  The calibration of the judge metric resulted in τ*=0.66 with P=0.824 and R=0.970 (Table 1). This implies a bias towards calling an extraction "correct" to maximize F1. What was the rationale for prioritizing F1 over, for example, a threshold that balances precision and recall, and how might this choice affect the interpretation of the overall accuracy and calibration results?
3.  The analysis of transcript length (Appx. C, Table 9) shows accuracy increasing with length, which is an interesting counterpoint to the "lost in the middle" problem. Do you believe this is a feature of the objective extraction task itself (more context clarifies intent) or an artifact of the datasets, where longer conversations happen to contain more explicit restatements of the goal?

**Rating**
- Overall (10): 8 — A strong, well-executed paper with a novel benchmark and important findings, slightly held back by its reliance on a single LLM judge.
- Novelty (10): 8 — The benchmark and problem formulation are novel, even if the underlying techniques are established.
- Technical Quality (10): 9 — The methodology is rigorous, with excellent statistical analysis and transparent reporting; the single-judge design is the main technical limitation.
- Clarity (10): 10 — The paper is exceptionally clear, well-structured, and easy to follow despite its technical density.
- Confidence (5): 5 — I am very confident in my assessment; I have expertise in LLM evaluation and safety.

***

### **Review 2**

**Summary**
The paper proposes a benchmark, ObjexMT, to evaluate the ability of Large Language Models (LLMs) to extract a user's underlying objective from multi-turn jailbreak dialogues and to correctly estimate their confidence in doing so. The authors test six models on three datasets, finding that accuracy is generally low (around 60% for the best models) and that models are poorly calibrated, frequently making high-confidence mistakes. The paper concludes that LLM judges are not yet reliable for inferring latent intent and recommends exposing objectives explicitly or using confidence scores to gate decisions.

**Soundness**
The methodology is a straightforward application of the LLM-as-a-Judge paradigm. An LLM's output (the extracted objective) is judged by another LLM (gpt-4.1) based on semantic similarity to a ground-truth label. This approach is inherently circular and suffers from potential biases, which the paper acknowledges but does not solve. The "human-aligned thresholding" (Sec. 3.7) is an attempt to mitigate this, but it's a patch on a fundamentally weak foundation. Calibrating on just 300 examples to create a single threshold that is then applied to thousands of diverse instances feels fragile. The rest of the analysis applies standard calibration metrics (ECE, Brier) to the model's self-reported confidence, which is technically fine but not novel.

**Presentation**
The paper is well-organized and clearly written. However, the naming and framing can be slightly misleading. "ObjexMT" sounds like a new method or model, but it is simply an evaluation suite. The abstract and introduction frame the work as a major step forward, but the findings—that LLMs struggle with complex contexts and are overconfident—are largely confirmatory of existing knowledge in the field. Some figures in the appendix appear to be raw outputs from an analysis script and are not well-integrated into the narrative (e.g., the collection of plots in Fig. 4 / `[Block #51]`). There are also duplicate tables in the appendix (e.g., Table 8 in `[Block #49]`).

**Contribution**
The contribution of this paper feels incremental. The core ideas are not new: using LLMs to judge other LLMs is a well-established (and debated) paradigm, and analyzing model calibration is a standard practice. The paper's main contribution is to combine these ideas and apply them to the specific sub-problem of objective extraction from jailbreak dialogues. While this is a useful exercise, it may not represent a significant conceptual advance. The conclusion that one should "expose objectives when feasible" (Sec. 5) is a rather obvious takeaway. The value lies in the empirical quantification, but the results themselves are not particularly surprising.

**Strengths**
- **Clear Problem Definition:** The paper does a good job of isolating and defining the task of "objective extraction" as distinct from harmfulness classification.
- **Empirical Data:** It provides a solid set of empirical results on a range of modern, powerful LLMs, offering a useful snapshot of current capabilities.
- **Public Artifacts:** The promise to release data and code is a positive contribution to the community, allowing others to build on this work.

**Weaknesses**
- **Limited Novelty:** The work is primarily an application of existing methods to a specific problem. It does not propose a new technique for objective extraction or for improving calibration.
- **Circular Evaluation:** The core accuracy metric relies on an LLM judge, which introduces unquantified biases and makes the evaluation self-referential. The results are less about absolute model capability and more about how well models align with GPT-4.1's interpretation of objectives.
- **Obvious Conclusions:** The primary takeaways (models are imperfect, calibration is a problem, be careful with LLM judges) are widely known. The paper provides numbers for these intuitions but doesn't fundamentally change our understanding.

**Questions**
1.  The paper's main contribution is the ObjexMT benchmark. How is the task of "objective extraction" fundamentally different from established NLP tasks like query rewriting or dialogue summarization, which also aim to distill user intent from context?
2.  The findings largely confirm that LLMs are not perfectly reliable. Was there any result that was genuinely surprising or counter-intuitive to you? For example, was the strong performance of `kimi-k2` on accuracy or the poor performance of `gpt-4.1` expected?
3.  Given the heavy reliance on a single LLM judge, how can you be sure that the observed performance differences between models (Table 2) are not just artifacts of their relative similarity to the judge model's own internal representations and reasoning patterns?

**Rating**
- Overall (10): 5 — A solid but incremental evaluation paper that provides useful data points but lacks significant novelty or surprising insights.
- Novelty (10): 4 — The work combines existing methods in a new context but does not introduce new concepts or techniques.
- Technical Quality (10): 6 — The methodology is sound on its own terms, but the foundational reliance on a single LLM judge is a major limitation.
- Clarity (10): 8 — The paper is clearly written, though the contribution could be framed more modestly and the appendix needs cleaning.
- Confidence (5): 4 — I am confident in my assessment, though I am not a specialist in multi-turn jailbreaks specifically.

***

### **Review 3**

**Summary**
This paper introduces ObjexMT, a benchmark for evaluating an LLM's ability to extract the core user objective from complex, multi-turn jailbreak conversations. Beyond simple accuracy, the benchmark also measures the model's metacognitive ability to know when its extraction is likely correct by evaluating its self-reported confidence. The authors test six leading models and find that even the best ones only achieve ~61% accuracy and struggle with calibration, often being highly confident when they are wrong. The work highlights significant performance differences based on how the conversational data was generated (automated vs. human) and provides actionable advice for practitioners using LLM-as-a-Judge systems.

**Soundness**
The methodology is practical and well-considered for the problem at hand. Using an LLM judge for semantic similarity is a scalable solution, and the authors take the crucial extra step of calibrating this judge against human annotations (Sec. 3.7). This grounds the automated metric in human judgment, even if imperfectly, which is a major step up from using raw similarity scores. The dual focus on both extraction accuracy and metacognitive reliability (calibration, selective risk) is excellent and reflects a mature understanding of what makes an AI system trustworthy in practice. The analyses of how performance varies with dataset source (Table 4), transcript length (Table 9), and turn complexity (Appx. B) are particularly insightful for anyone looking to deploy such a system.

**Presentation**
The paper is very well-written and organized. The contributions are stated clearly upfront. The results are presented in a way that is easy to digest and directly supports the main arguments. The accuracy-calibration trade-off plot (Fig. 3) is a fantastic summary of the top-level findings, immediately showing that no single model is best at everything. The detailed tables providing effect sizes (Table 7) and high-confidence error rates (Table 6) are extremely useful for understanding the practical significance of the results. The authors have done a great job of making a complex set of experiments accessible.

**Contribution**
This paper makes a significant practical contribution to the field of LLM evaluation and safety. While the concept of an LLM judge isn't new, this work operationalizes a critical and difficult aspect of it: understanding latent intent in adversarial settings. ObjexMT provides a ready-made tool for developers and researchers to probe this capability in their own models. The findings are not just academic; they have immediate implications for building safer and more reliable AI systems. The observation that medium-complexity dialogues (5-6 turns) are a "high-risk zone" (Appx. B) is a novel and highly practical insight. The release of the full dataset and analysis scripts is a major plus that will facilitate follow-up work.

**Strengths**
- **Practical Relevance:** The problem is highly relevant for real-world applications of LLMs in moderation, evaluation, and user interaction.
- **Actionable Findings:** The paper produces clear, actionable insights, such as the need for confidence-gating and the identification of high-risk conversation patterns (e.g., by length or turn count).
- **Comprehensive Analysis:** The study goes beyond simple accuracy leaderboards to provide a nuanced analysis of calibration, selective prediction, and the impact of data characteristics.
- **Excellent Artifacts:** The commitment to releasing detailed, per-instance data and code is exemplary and provides immense value to the community.

**Weaknesses**
- **Scope of Models:** The evaluation is limited to six large, mostly proprietary models. It would be very valuable to see how popular open-source models (e.g., Llama 3, Mistral) perform on this benchmark, as they are widely used by the community.
- **Single-Sentence Constraint:** The requirement to extract a single-sentence objective (Sec. 3.2) is a reasonable simplification for creating the benchmark, but it may not capture complex, multi-part objectives that can appear in real-world adversarial attacks.
- **Deterministic Decoding:** The use of deterministic decoding (T=0) provides a stable point estimate but doesn't capture the variability one might see in a real deployment where temperature is non-zero.

**Questions**
1.  Your results in Table 6 show that even the best model (`claude-sonnet-4`) has a 14.9% error rate when its confidence is >= 0.90. For a safety-critical application, what do you believe is an acceptable level of "Wrong@High-Confidence" risk, and what strategies beyond simple thresholding could be used to mitigate it?
2.  The analysis of transcript length in Appendix C is fascinating. It suggests that for this task, more context is generally helpful. How do you think this finding generalizes to other LLM tasks? Is objective extraction a special case where the "lost in the middle" problem is less severe?
3.  You've focused on the safety domain. Do you have plans to apply the ObjexMT framework to other domains where latent intent recovery is crucial, such as customer support, multi-hop QA, or complex instruction following?

**Rating**
- Overall (10): 9 — An excellent, practical, and rigorous paper that introduces a valuable benchmark and delivers clear, actionable insights for the field.
- Novelty (10): 8 — The synthesis of existing techniques into a novel and important benchmark is a strong contribution.
- Technical Quality (10): 9 — High-quality experimental design and analysis, with minor limitations in scope (models, objective complexity).
- Clarity (10): 10 — The paper is exceptionally clear, well-structured, and presents its findings effectively.
- Confidence (5): 5 — I am very confident in my assessment and find this work highly relevant to my own interests in applied AI.

***

### **Review 4**

**Summary**
This paper presents ObjexMT, a benchmark for evaluating an LLM's ability to extract a single-sentence objective from multi-turn jailbreak dialogues and to calibrate its confidence in that extraction. The authors conduct a thorough evaluation of six large models across three datasets. Accuracy is measured using a GPT-4.1 judge whose similarity scores are converted to binary correctness via a human-calibrated threshold. Metacognition is assessed with a suite of standard metrics (ECE, Brier, etc.). The results show that current models are only moderately successful (best accuracy is 0.612), are generally miscalibrated, and that performance is highly dependent on the dataset's origin.

**Soundness**
The paper demonstrates a very high level of technical and statistical rigor. The experimental design is carefully thought out. I particularly appreciate the detailed statistical testing protocol (Sec. 4.3), including paired tests and Holm-Bonferroni correction for multiple comparisons, which is crucial for drawing reliable conclusions about model differences. Reporting effect sizes (Table 7) is another sign of methodological maturity. The appendix contains valuable supplementary analyses on turn complexity (Appx. B) and transcript length (Appx. C), which add significant depth to the main findings. The process for establishing the evaluation threshold τ* (Sec. 3.7, Table 1) is transparent and well-justified.

My main critique on soundness, which the authors rightly identify as a limitation, is the use of a single, fixed LLM judge. While this ensures consistency, it makes the entire benchmark's accuracy metric dependent on the idiosyncrasies of `gpt-4.1`. A multi-judge ensemble or at least a sensitivity analysis with a different judge model would have made the results more robust.

**Presentation**
The paper is very well-written, dense, and professional. The structure is logical and the arguments are well-supported by data. However, there are several minor presentational issues that detract from its polish.
- There is some inconsistency in referencing figures and tables. For example, the text block `[Block #27]` appears to be a malformed table that was meant to be part of the Figure 1 caption or a separate table.
- The appendix, while containing excellent analyses, feels somewhat unpolished. For instance, in `[Block #49]`, Table 8 is presented, and then an almost identical table appears right below it without a caption or number. The collection of plots in Figure 4 (`[Block #51]`) and Figure 5 (`[Block #58]`) seem to be direct outputs from a plotting script and lack the clear, individual captions and narrative integration expected in a final paper. These should be cleaned up, properly labeled, and referenced from the main text where appropriate.
- The model names are extremely specific, including version dates (e.g., `gpt-4.1-2025-04-14`). This is excellent for reproducibility but also highlights the ephemeral nature of the findings.

**Contribution**
The paper's contribution is twofold: (1) the ObjexMT benchmark itself, which operationalizes the important but fuzzy concept of "latent intent recovery" in an adversarial context, and (2) a detailed empirical study that yields several important findings. The most significant findings are the quantification of the accuracy/calibration gap, the dramatic effect of dataset construction on difficulty (Fig. 1, Table 4), and the nuanced relationship between performance, transcript length, and turn count. This work provides a valuable diagnostic tool and a strong empirical foundation for future research on the reliability of LLM judges.

**Strengths**
- **Thoroughness:** The experimental evaluation is exhaustive, covering multiple models, datasets, and a wide range of metrics.
- **Statistical Rigor:** The statistical analysis is a model of good practice, lending high confidence to the paper's conclusions about relative model performance.
- **In-depth Analysis:** The appendix analyses on turn complexity and transcript length go beyond surface-level results and provide deeper insights into model behavior.
- **Transparency:** The paper is transparent about its methodology, including providing the exact prompts used (Appx. A) and releasing its data.

**Weaknesses**
- **Presentation Polish:** The appendix and some figures/tables in the main text require cleanup and better integration to improve clarity and professionalism.
- **Justification for Sampling:** The paper mentions using "adaptive importance sampling" for the human calibration set (Sec. 3.3) but doesn't fully justify the resulting dataset distribution. A brief explanation of why that specific mix was chosen would be helpful.
- **Single-Judge Dependency:** As mentioned, the reliance on a single judge model is a core limitation that tempers the generalizability of the accuracy results.

**Questions**
1.  In Section 4.3, you mention using both McNemar's test and a bootstrap test for paired significance. Did these two tests ever produce conflicting results for any of the 15 model pairs, and if so, what was your protocol for interpreting such a disagreement?
2.  The appendix contains a wealth of information, particularly in the analyses of turn complexity (Appx. B) and transcript length (Appx. C). These two factors are clearly correlated. Have you considered a joint analysis? For example, are the dialogues in the "Long (≥7)" turn bin also the ones in the "Q4 (>4K)" length bin, and does this explain the surprising performance rebound in long conversations?
3.  The model versions are specified with dates in 2025 (e.g., `claude-sonnet-4-20250514`). I assume these are placeholders. Could you clarify the actual model versions used for the experiments to ensure precise reproducibility?

**Rating**
- Overall (10): 8 — A very strong and thorough paper with a valuable contribution. Minor presentational issues and the single-judge design are the main points for improvement.
- Novelty (10): 7 — The benchmark itself is a novel and useful contribution, though it builds upon existing paradigms.
- Technical Quality (10): 9 — The experimental and statistical methodology is excellent, with the only major caveat being the reliance on a single judge.
- Clarity (10): 8 — The main body is very clear, but the appendix and some figures need significant polishing to match the quality of the prose.
- Confidence (5): 5 — I am highly confident in this review, based on my expertise in LLM evaluation and experimental methodology.