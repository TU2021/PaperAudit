Academic integrity and internal consistency assessment

Summary of major, evidence-based issues that materially affect correctness, trustworthiness, and reproducibility.

1) Conflicting headline performance claims across sections
- Abstract: â€œEmboBrain-7B surpasses the 671B DeepSeek-R1 baseline by 9.5% on two challenging embodied decision-making benchmarks.â€ [Abstract, Block #2]
- Introduction Figure 1 caption: â€œEmboMatrix-augmented training boosts a 7B base model by +58.8 percentage points in success rateâ€¦â€ [Introduction, Block #4; image Block #6]
- Experiments (Section 5.1) Results text: â€œâ€¦for the 7B model, EmboMatrix improves its performance by 60.3% and 58.8%, respectively, in two benchmarks.â€ [Experiments, Block #25]
- Conclusion: â€œâ€¦EmboBrain-7B achieving a 14.2% performance gain over a strong baseline.â€ [Conclusion, Block #37]
These statements are numerically inconsistent with each other (9.5% vs 58.8% vs 60.3% vs 14.2%). Additionally, units oscillate between â€œpercentage pointsâ€ and â€œpercent,â€ which changes the magnitude interpretation. This inconsistency undermines confidence in the central claimed contribution.

2) Missing key numeric evidence for EmboBrain-7B in Table 1
- Table 1 is presented as the â€œComprehensive comparison of model success rates (%) on two benchmarks,â€ but the row for EmboBrain-7B is absent; only EmboBrain-1.5B appears. [Experiments, Block #21]
- The main claim requires showing EmboBrain-7B vs DeepSeek-R1 and others across both benchmarks and categories, yet the table does not provide EmboBrain-7Bâ€™s actual numbers. No direct evidence found in the manuscript supporting the Abstractâ€™s â€œ9.5% averageâ€ improvement or the â€œ60.3%/58.8%â€ improvements claimed in Section 5.1.

3) Simulator inconsistency without clarification
- Data and scene generation repeatedly state use of OmniGibson/BEHAVIOR-1K: â€œpreprocessing 45 diverse scenes from the Omnigibson simulation platform,â€ and relations â€œsupported by Omnigibson.â€ [Appendix A.1â€“A.2, Blocks #55â€“#59]
- The distributed backend figure legend labels the simulator as â€œIsaac Sim simulator.â€ [Appendix B.2 figure/legend, Block #63]
The manuscript does not explain how OmniGibson scenes and BEHAVIOR-1K tasks are executed in Isaac Sim (conversion pipeline, compatibility layer, or dual-simulator setup). This creates a material reproducibility gap at the system level.

4) Action schema mismatch across sections and figures
- Action primitive table specifies 13 primitives such as pick_up, place with â€œrelation,â€ heat_object_with_source, cook_object_with_tool, etc. [Appendix, Table 8, Block #82]
- Earlier figures and text use different names and schemas: moveto/pickup/open/toggle_on (Figure 1a), move("microwave"), pickup("chicken wing"), cook("chicken wing", "heat") (Figure 4/Block #24; Figure 1 caption/Block #4). The primitive â€œcookâ€ is not defined in Table 8; â€œmoveâ€ vs â€œmovetoâ€ and â€œpickupâ€ vs â€œpick_upâ€ also differ.
Given Section 4.3â€™s format adherence reward r_f depends on strict schema compliance, this inconsistency in the defined action API vs examples threatens internal coherence and reproducibility of training and evaluation (what exact schema was enforced?).

5) Reward scale and reporting inconsistencies
- The hierarchical reward is defined as r_s = r_f + r_r + r_g, with r_r = Î²|O_goal âˆ© O_a|, and r_g = Î± Î£ ğ•€[g_k(s_H)=1]. [Method, Block #20]
- Appendix C specifies concrete coefficients: parsing penalty âˆ’1, base reward 0.5 for a parsed sequence; Î² = 0.2 (capped at 1); total task reward 30 with Î± = 30/N_sub. [Appendix C, Block #64]
- However, Figure 4 reports â€œscore 0 / 0.5 / 1â€ for the same task examples without defining how these â€œscoresâ€ map to r_s or r_g (which can reach up to 30). [Experiments, Block #24]
The discrepancy between qualitative â€œscoreâ€ scales (0â€“1) and the quantitative reward scale (up to 30) is not reconciled, making it unclear how example scores relate to the training/evaluation metrics.

6) Metric definition vs reporting mismatch in scene generation ablation
- Section 5.2.1 states evaluation across â€œfour metrics: generation success rate, time, verification pass rate, and scene aesthetics.â€ [Experiments, Block #29]
- Table 2 only reports three metrics (Generation Rate, Aesthetic Score, Verification Pass Rate); Time is not shown. [Experiments, Block #29/Table 2]
This omission contradicts the stated setup and affects transparency of the claimed efficiency/quality improvements.

7) Minor quantitative overstatement in efficiency ablation
- Table 3 shows latency reduction from 3.48s to 0.07s (~49.7Ã—). [Experiments, Block #35]
- The caption claims â€œover 50Ã—â€ reduction. [Experiments, Block #35]
While minor, overstating the reduction contradicts the presented numbers.

Additional notes
- Figure numbering and captions appear slightly conflated around Figures 5/6 descriptions of social simulation and layout tree (Blocks #26â€“#27, #34), but this is editorial rather than substantive.

Overall assessment
The manuscript contains multiple high-impact internal inconsistencies: conflicting headline performance figures (including unit ambiguity), missing EmboBrain-7B numerical results in the main comparison table, unexplained simulator mismatch (OmniGibson vs Isaac Sim), and divergent action schemas despite a format adherence reward. These issues materially affect the credibility and reproducibility of the core claims. Until these points are reconciled with precise, consistent numbers and clear system descriptions, the integrity and correctness of the reported results remain uncertain.

Recommendations to remediate
- Provide complete, explicit EmboBrain-7B metrics across all columns in Table 1 for both benchmarks and categories; reconcile with all stated improvements (percent vs percentage points) in Abstract, Introduction, Experiments, and Conclusion.
- Clarify simulator usage: specify whether OmniGibson or Isaac Sim is used for training/evaluation; if both, detail conversion pipelines and compatibility layers; align figure legends and text.
- Standardize and document the action schema used in training/evaluation; update examples to match Table 8 or update Table 8 to match actual primitives; specify the exact schema enforced for r_f.
- Define the mapping between â€œscoreâ€ in qualitative examples and the quantitative reward used during training/evaluation.
- Include the â€œTimeâ€ metric in Table 2 or correct the experimental setup description.
- Align efficiency claims with the presented numbers (49.7Ã— vs â€œover 50Ã—â€).