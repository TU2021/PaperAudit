# Global Summary
This paper introduces EmboMatrix, a comprehensive infrastructure termed a "training ground" for developing embodied decision-making skills in Large Language Models (LLMs). The core problem addressed is that LLMs trained solely on text lack the physical grounding necessary for effective interaction in the real world. EmboMatrix provides a one-stop solution with three key components: a multi-agent data factory for generating massive, diverse, and solvable tasks; a scalable, distributed simulation backend for high-throughput interaction; and a hierarchical reward architecture for precise, dense supervision. Using this training ground, the authors cultivate EmboBrain, an LLM whose embodied abilities emerge from extensive, interactive, environment-grounded learning. The primary model, EmboBrain-7B, is shown to significantly outperform much larger models, including the 671B parameter DeepSeek-R1 baseline by an average of 9.5% across two challenging embodied decision-making benchmarks. The work argues for a paradigm shift from static, text-based learning to dynamic, interactive learning for creating truly intelligent embodied agents.

# Abstract
The paper addresses the gap between the general decision-making capabilities of Large Language Models (LLMs) and the requirements of embodied intelligence, which demands understanding of the physical world. The authors propose the concept of a "training ground," an infrastructure for task simulation, embodied interaction, and feedback signals to teach LLMs genuine embodied skills. They present EmboMatrix, the first such training ground, which features three novel techniques: a multi-agent data engine for scalable task/scene generation, a distributed heterogeneous-hardware system for efficient simulation, and a multi-level reward architecture for precise supervision. By training an LLM within EmboMatrix, they create EmboBrain. Experiments show that EmboBrain-7B surpasses the 671B DeepSeek-R1 baseline by 9.5% on two challenging embodied decision-making benchmarks, demonstrating the effectiveness of interactive, environment-grounded learning.

# Introduction
- Embodied decision-making is defined as translating high-level goals into actions via continuous interaction with the physical world. The paper adopts a hierarchical approach where a high-level model handles reasoning and task decomposition.
- The core problem is that LLMs, while strong at reasoning, lack physical grounding when trained only on text, a state described as a "brain in a vat." This leads to memorization rather than true understanding.
- The proposed solution is a "training ground," a comprehensive infrastructure for interactive learning in high-fidelity simulation.
- Building such a training ground faces three main challenges:
    1.  **Data:** Generating a massive, diverse, and solvable curriculum of tasks.
    2.  **System:** Achieving high-throughput simulation to train large models, which existing platforms lack.
    3.  **Algorithm:** Designing an informative reward architecture for dense supervision in embodied tasks.
- The paper introduces EmboMatrix to solve these challenges. Its key features are:
    - A multi-agent automated data factory for data diversity.
    - A scalable simulation backend using distributed, heterogeneous hardware and semantic abstraction.
    - A hierarchical reward architecture for informative supervision.
- Using EmboMatrix, the authors train EmboBrain.
- Key claims:
    - EmboMatrix-augmented training boosts a 7B base model's success rate by +58.8 percentage points on the Embodied Agent Interface (EAI) benchmark (Figure 1).
    - EmboBrain-7B surpasses the DeepSeek-R1 baseline by 9.5% on average across multiple benchmarks.

# Related Work
- **Embodied Decision Making:** The paper situates its work among methods using LLMs for generating action sequences, code, or intermediate representations for robotic tasks.
- **Simulator and Embodied Data Generation:** It reviews existing simulators and benchmarks (e.g., Behavior1K, ALFRED, iGibson, SAPIEN, Habitat) and notes their reliance on manual scene authoring, which limits scale. It also discusses recent generative methods (e.g., HOLODECK, ARCHITECT, RoboGen) but identifies their limitations in task-awareness or layout simplicity. EmboMatrix's multi-agent data factory is proposed as a more scalable and task-aware solution.
- **RL for Large Language Models:** The paper acknowledges work on LLM alignment via RL (e.g., RLHF). It identifies Fei et al. (2025) as the most closely related work but distinguishes itself by instantiating a concrete physics-based simulation system, providing larger-scale data, and designing a more informative, tailored reward system.

# Preliminaries
- The problem is formulated as learning a model `B_theta` that takes a scene `S`, instruction `I`, and skill library `A` to produce an action sequence `a`.
- A "training ground" `F` is defined as a system parameterized by a task distribution `D_task`, a physical simulator `M`, and a reward architecture `R`.
- The optimization objective is to find model parameters `theta*` that maximize the expected reward `R` obtained by executing the model's action sequence in the simulator `M` for tasks sampled from `D_task`.
- EmboMatrix is presented as the concrete implementation of this training ground `F`.

# Method
- EmboMatrix is a system with three synergistic components: a data factory, a simulation backend, and a reward architecture.
- **Multi-Agent–Driven Automated Data Factory:**
    - **Instruction Generation:** Uses multi-agent social simulation. It starts with BEHAVIOR's scenes, has a VLM generate a description, and then socially simulated agents (e.g., father, mother) engage in dialogue to create natural tasks. An LLM agent converts these tasks to a structured BDDL format.
    - **Scene Generation:** A multi-level process creates a 3D scene for the task. Agents distribute objects across rooms, build a multi-scale layout tree to ensure plausible spatial relations (e.g., `ontop`, `inside`, `faceto`), and then sample object placements.
- **Scalable Simulation Backend:**
    - **Semantic Abstraction:** A pre-cached language-physics interface accelerates simulation. For common interactions, it bypasses full physics simulation by directly instantiating a valid, pre-computed outcome.
    - **Architectural Decoupling:** The LLM trainer is separated from a distributed, heterogeneous pool of simulation workers. A Resource-Scheduler pre-loads scenes onto idle workers, and a Task-Dispatcher maps action sequences to these "pre-warmed" simulators to maximize throughput.
- **Hierarchical Reward Architecture:**
    - The total reward `r_s = r_f + r_r + r_g` provides a multi-stage curriculum.
    - **Format Adherence (r_f):** A binary reward for generating syntactically correct, executable action sequences.
    - **Semantic Relevance (r_r):** A dense reward for interacting with goal-relevant objects, calculated as `r_r = β |O_goal ∩ O_a|`. This shapes exploration.
    - **Goal-Oriented Success (r_g):** A sparse reward based on the completion of individual goal predicates.
- **Learning Algorithms:**
    - The model is trained using Group Relative Policy Optimization (GRPO).
    - For each task, `G` action sequences are sampled. Advantages are computed by normalizing rewards within this group (mean and stddev).
    - The policy is updated using a clipped surrogate objective with a KL divergence penalty against a reference model.

# Experiments
- **Overall Performance on Embodied Decision-Making:**
    - **Models:** EmboBrain-1.5B and EmboBrain-7B, initialized from distilled Qwen checkpoints.
    - **Setup:** Trained on an 8xA100 cluster (LLM) and 16 GPUs (simulation). Training data was procedurally generated from 45 BEHAVIOR-1K scenes.
    - **Evaluation:** Assessed on two held-out benchmarks of 100 tasks each: an internal, manually verified set and the public Embodied Agent Interface (EAI) benchmark.
    - **Results (Table 1):**
        - EmboBrain-7B improves over its 7B base model by +60.3% on the agent-generated benchmark and +58.8% on the EAI benchmark.
        - EmboBrain-7B (65.8% success) outperforms GPT-4o (45.0%) and DeepSeek-R1 (51.6%) on the agent-generated benchmark.
        - EmboBrain-7B (64.3% success) outperforms GPT-4o (44.8%) and DeepSeek-R1 (58.2%) on the EAI benchmark.
        - A qualitative example shows EmboBrain-7B succeeding on a "Heat Chicken" task where GPT-4o and DeepSeek-R1 fail due to logical errors (e.g., forgetting to open the microwave).
- **Scalability of EmboMatrix:**
    - **Data Diversity:** Using social simulation increased the task diversity score from 4.70 to 8.42 (evaluated by GPT-4).
    - **Scene Quality:** The multi-level layout tree method achieved a 71.43% scene generation rate and 98.00% verification pass rate, significantly outperforming baselines (LayoutGPT and no tree).
    - **Simulation Efficiency:** The full system reduced per-rollout simulation latency from 3.48s (naive) to 0.07s, a nearly 50-fold speedup. The components contributed as follows: Pre-cached Execution (0.85s), Resource Scheduler (0.14s), and Task Dispatcher (0.07s).
- **Hierarchical Reward Architecture Improves Training Efficiency:**
    - An ablation study shows that the semantic relevance reward (r_r) is critical. Without it, the agent's goal-oriented reward stagnates near 2. With it, the reward steadily increases to around 12, demonstrating that it effectively solves the credit assignment problem.

# Conclusion
The paper advocates for a shift from learning from static language data to interactive learning for embodied agents. It presents EmboMatrix as the first scalable training ground to enable this, addressing challenges in data generation, system scalability, and algorithmic design. The resulting EmboBrain models demonstrate superior performance, with EmboBrain-7B achieving a 14.2% performance gain over a strong baseline, validating the effectiveness of the proposed framework.

# References
This section contains a list of references cited throughout the paper.

# Appendix
- **A: Detailed Explanation of our Multi-Agent Data Factory:** Provides more details on the two stages: (1) social simulation for instruction generation using Role Playing, Social Simulation, and Summarization agents, and (2) multi-level scene generation at the scene, room, planar, and object levels.
- **B: Detailed Explanation of our Scalable Simulation Backend:** Elaborates on the Pre-Cached Language-Physics Interface, which claims a 5x to 100x speedup on individual skills, and the Distributed Simulation System, which uses a manager, heterogeneous workers, a resource scheduler, and a task dispatcher. System overhead is stated to be about 20% of total training time.
- **C: Reward Coefficients:** Specifies the reward values: -1 for a parse error, 0.5 for a successful parse, `β = 0.2` for semantic relevance, and a total of 30 for goal-oriented success, scaled by the number of sub-tasks (`α = 30 / N_sub`).
- **D: Technical Appendices:** Table 8 lists the 13 available action primitives, such as `move`, `pick_up`, `place`, `toggle_on`.
- **E: Comparison of Embodied Data Generation:** Table 4 compares EmboMatrix's data factory to prior work (Behavior-1k, ProcTHOR, Holodeck, RoboGen) across six features, claiming theirs is the only one to be fully automated, use multi-agent social simulation, generate scenes from tasks, support multi-room, be interpretable, and be self-verifying.
- **F: Experiment Details:** Includes the prompts used for the GPT-4 based evaluators for task diversity and scene aesthetics.
- **G: LLM Usage Statement:** Clarifies that LLMs were used only for auxiliary writing tasks like grammar polishing and not for core research contributions.
- The appendix also includes additional tables and figures with task examples, evaluator prompts, and visual comparisons of scene generation methods.