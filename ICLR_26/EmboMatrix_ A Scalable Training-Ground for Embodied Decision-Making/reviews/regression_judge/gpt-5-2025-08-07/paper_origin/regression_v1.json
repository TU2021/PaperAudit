{
  "paper": "EmboMatrix_ A Scalable Training-Ground for Embodied Decision-Making",
  "runs": [
    {
      "config": {
        "judge_model": "gpt-5-2025-08-07",
        "review_agent": "paper_audit",
        "ai_model": "gpt-5-2025-08-07",
        "baseline_file": "baseline_review.txt",
        "final_file": "final_review_all.txt",
        "temperature": 0.0,
        "max_tokens": 20000,
        "timeout_s": 600,
        "metric": "regression_v2_minimal",
        "prompt_hash": "2142c04332"
      },
      "config_key": "1a46f14f5902ebb0fc2c14773cec6644d2fa5338",
      "inputs": {
        "baseline_review": "reviews/paper_audit/gpt-5-2025-08-07/paper_origin/baseline_review.txt",
        "final_review": "reviews/paper_audit/gpt-5-2025-08-07/paper_origin/final_review_all.txt"
      },
      "regression": {
        "score_delta": {
          "baseline_score": 7.0,
          "final_score": 6.0,
          "delta": -1.0,
          "scale_hint": "1-10"
        },
        "differences": [
          {
            "diff_type": "new_critique",
            "summary": "Simulator usage inconsistency (OmniGibson/BEHAVIOR-1K vs Isaac Sim) without integration details",
            "paperaudit_types": [
              "METHOD_LOGIC_CONSISTENCY",
              "CONTEXT_MISALIGNMENT_INCOHERENCE",
              "EXPERIMENTAL_DESIGN_PROTOCOL"
            ],
            "why_impacts_score": "uncertainty about environment parity and reproducibility",
            "evidence": {
              "baseline_quote": "Section 4.2 and Appendix B.2 detail architectural decoupling; Figure 8 illustrates the distributed design.",
              "final_quote": "Figure legend labels simulator as 'Isaac Sim' without explaining the relationship or conversion pipeline."
            }
          },
          {
            "diff_type": "intensified_critique",
            "summary": "Expanded inconsistencies in reported performance numbers and units",
            "paperaudit_types": [
              "CLAIM_RESULT_DISTORTION",
              "RHETORICAL_PRESENTATION_MANIPULATION",
              "CONTEXT_MISALIGNMENT_INCOHERENCE"
            ],
            "why_impacts_score": "undermines trust in claimed improvements",
            "evidence": {
              "baseline_quote": "Abstract claims +9.5% over DeepSeek-R1 on average, whereas Conclusion states '14.2% performance gain'.",
              "final_quote": "Abstract '9.5%', Section 5.1 '60.3%/58.8%', Conclusion '14.2%' with units alternating."
            }
          },
          {
            "diff_type": "new_critique",
            "summary": "Unclear mapping between qualitative example scores and quantitative rewards",
            "paperaudit_types": [
              "METHOD_LOGIC_CONSISTENCY",
              "RHETORICAL_PRESENTATION_MANIPULATION",
              "CONTEXT_MISALIGNMENT_INCOHERENCE"
            ],
            "why_impacts_score": "obscures evaluation interpretation and metric alignment",
            "evidence": {
              "baseline_quote": "Appendix C provides concrete coefficients (β=0.2 capped at 1; α=30/Nsub; parsing penalties).",
              "final_quote": "Qualitative 'score 0/0.5/1' scale is not reconciled with quantitative reward scale."
            }
          },
          {
            "diff_type": "intensified_critique",
            "summary": "Table 1 truncation escalated to missing EmboBrain-7B numerical row",
            "paperaudit_types": [
              "EVIDENCE_DATA_INTEGRITY",
              "RHETORICAL_PRESENTATION_MANIPULATION"
            ],
            "why_impacts_score": "missing key evidence reduces result credibility",
            "evidence": {
              "baseline_quote": "Table 1 appears truncated for the '7B Base' row and includes long composite columns.",
              "final_quote": "Missing EmboBrain-7B numerical row and inconsistent metrics in Table 1 and other sections."
            }
          },
          {
            "diff_type": "score_rationale_shift",
            "summary": "Final explicitly lowers score due to trustworthiness issues",
            "paperaudit_types": [
              "CLAIM_RESULT_DISTORTION",
              "EVIDENCE_DATA_INTEGRITY",
              "CONTEXT_MISALIGNMENT_INCOHERENCE"
            ],
            "why_impacts_score": "explicit link between inconsistencies and lower rating",
            "evidence": {
              "baseline_quote": "Overall (10): 7 — strong gains but lacks real-world validation and some fairness/reproducibility details.",
              "final_quote": "Overall (10): 6 — inconsistencies and missing details materially affect trustworthiness."
            }
          },
          {
            "diff_type": "new_critique",
            "summary": "Overstated efficiency claim ('over 50×' vs ~49.7×)",
            "paperaudit_types": [
              "CLAIM_RESULT_DISTORTION",
              "RHETORICAL_PRESENTATION_MANIPULATION"
            ],
            "why_impacts_score": "perceived exaggeration reduces confidence in reporting",
            "evidence": {
              "baseline_quote": "Table 3 reports per-rollout latency reduced from 3.48s to 0.07s—over 50× improvement.",
              "final_quote": "Efficiency caption claims 'over 50×' while Table 3 corresponds to ~49.7× (3.48s→0.07s)."
            }
          },
          {
            "diff_type": "new_critique",
            "summary": "Promised 'time' metric missing from Table 2",
            "paperaudit_types": [
              "EVIDENCE_DATA_INTEGRITY",
              "CONTEXT_MISALIGNMENT_INCOHERENCE"
            ],
            "why_impacts_score": "incomplete reporting weakens evidence for data-quality claims",
            "evidence": {
              "baseline_quote": "Figure 5 and Table 2 demonstrate increased task diversity and verification pass rate.",
              "final_quote": "Section 5.2.1 promises 'time' metric, but Table 2 reports only three metrics."
            }
          }
        ]
      },
      "generated_at": "2026-01-05T20:40:49"
    }
  ]
}