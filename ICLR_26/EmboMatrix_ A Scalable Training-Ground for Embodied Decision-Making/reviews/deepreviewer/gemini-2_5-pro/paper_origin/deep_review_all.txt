### Review 1

**Summary**

This paper introduces EmboMatrix, a comprehensive and scalable "training ground" designed to teach Large Language Models (LLMs) embodied decision-making skills through direct interaction with a simulated physical world. EmboMatrix is composed of three core components: a multi-agent data factory for generating diverse tasks and scenes, a scalable distributed simulation backend for high-throughput interaction, and a hierarchical reward architecture for providing precise learning signals. By training an LLM within this framework, the authors produce EmboBrain, a model that demonstrates significantly improved performance on embodied decision-making benchmarks, even outperforming much larger models.

**Soundness**

The methodology is generally sound, particularly the systems engineering aspect. The design of the scalable simulation backend is well-motivated and addresses critical bottlenecks in training agents via simulation. The architectural decoupling and semantic abstraction are principled solutions to the high computational cost and resource conflicts inherent in this domain. The ablation study presented in Table 3 provides strong evidence for the efficacy of these system optimizations, demonstrating a nearly 50-fold reduction in simulation latency. The reinforcement learning approach, while more of an application of existing methods (GRPO), is appropriate for the problem. The overall pipeline, from data generation to training, is logically coherent and well-executed.

**Presentation**

The paper is well-written and clearly structured. The core idea of a "training ground" is effectively communicated, and the three main components (Data, System, Algorithm) provide a clear organizing principle. Figure 2 (Block 10) provides a good high-level overview of the system architecture. The detailed explanation of the distributed simulation backend in Appendix B, particularly Figure 8, is excellent and provides crucial details for understanding the system's scalability. The main results in Table 1 are presented clearly and are easy to interpret.

**Contribution**

The primary contribution of this work is the design, implementation, and validation of EmboMatrix, a scalable and integrated system for training embodied agents. While individual components draw on existing ideas (e.g., procedural generation, distributed computing), their synthesis into a cohesive and high-performance "one-stop solution" is a significant engineering and research contribution. The demonstration that a 7B model trained with this system can outperform a 671B model (Table 1) is a powerful result that underscores the value of interactive, environment-grounded learning over passive, data-driven fine-tuning. This work provides a valuable blueprint and a practical tool for future research in embodied AI.

**Strengths**

1.  **Scalable System Architecture:** The most significant strength is the design of the distributed, heterogeneous simulation backend (Section 4.2, Appendix B). The combination of semantic abstraction via pre-caching and architectural decoupling with a resource scheduler and task dispatcher is a novel and highly effective solution to the simulation bottleneck, as proven by the impressive ~50x latency reduction (Table 3).
2.  **Strong Empirical Results:** The EmboBrain-7B model achieves state-of-the-art results, outperforming significantly larger models like DeepSeek-R1 and proprietary models like GPT-4o on two challenging benchmarks (Table 1). This provides compelling evidence for the effectiveness of the proposed training ground.
3.  **Comprehensive Solution:** The paper presents a complete end-to-end system, addressing the critical challenges of data generation, simulation throughput, and reward design. This holistic approach is a major strength compared to works that focus on only one aspect of the problem.

**Weaknesses**

1.  **Limited Scope of Pre-Caching:** The "Pre-Cached Physics Interface" (Section 4.2) is a key part of the speed-up, but its generality is questionable. The paper states it is for "common interactions," but it's unclear how the system handles more complex or novel physical interactions that cannot be easily pre-computed. This might limit the system's applicability to tasks with more dynamic or deformable objects.
2.  **Details on Hardware and Bottlenecks:** While Appendix B describes the architecture, the paper lacks a detailed analysis of the hardware configuration of the "Crowdsourced Heterogeneous Simulation Cluster" used in the experiments. It is also missing a discussion of remaining bottlenecks. For instance, is the system now compute-bound by the LLM, or are there still I/O or communication overheads that become significant at an even larger scale?

**Questions**

1.  Could you elaborate on the limitations of the pre-cached physics interface? What percentage of interactions in your experiments were handled by this system, and how does the simulation latency change for tasks requiring more novel, non-pre-cached physical dynamics?
2.  Regarding the distributed simulation system (Figure 8), what was the network latency between the training cluster and the simulation cluster? How does the system's performance degrade as this latency increases, and what strategies could mitigate this?
3.  The 50x speedup is impressive. Can you break down the 3.48s latency of the naive implementation? What are the primary contributors to this initial high latency (e.g., scene loading, physics steps, rendering)?

**Rating**

- Overall (10): 9 — The paper presents a highly effective and scalable system with strong empirical validation that significantly advances the field of embodied AI training (Table 1, Table 3).
- Novelty (10): 8 — While integrating existing concepts, the design of the scalable simulation backend and the holistic "training ground" framework is a novel and significant contribution (Section 4.2).
- Technical Quality (10): 9 — The system design is technically deep and the experimental validation, particularly the system ablations, is rigorous and convincing (Table 3, Figure 7).
- Clarity (10): 9 — The paper is very well-written and organized, with clear figures and a detailed appendix that explains the complex system architecture effectively (Figure 8, Appendix B).
- Confidence (5): 5 — I am an expert in distributed systems and simulation and am highly confident in my assessment of the paper's core technical contributions.

---

### Review 2

**Summary**

This paper introduces EmboMatrix, a framework for training Large Language Models (LLMs) in simulated environments to improve their embodied decision-making capabilities. The framework consists of three main parts: a multi-agent system for automatically generating diverse tasks and scenes, a scalable simulation backend to handle massive parallel interactions, and a hierarchical reward system to guide the learning process. Using EmboMatrix, the authors train a 7B parameter model called EmboBrain, which is shown to outperform much larger models, including the 671B DeepSeek-R1, on established embodied AI benchmarks.

**Soundness**

The methodology is sound and well-motivated. The approach of using multi-agent social simulation to generate diverse, goal-oriented tasks (Section 4.1) is a creative way to address the data bottleneck in embodied AI. The hierarchical reward structure (Section 4.3) is a sensible approach to credit assignment in long-horizon tasks, and the ablation study in Figure 7 clearly demonstrates its importance. The overall experimental design is solid, with comparisons against strong baselines on two different benchmarks. However, the evaluation of data diversity and scene aesthetics relies on a GPT-4 based evaluator (Section 5.2.1), which is a potential source of bias and lacks the rigor of human evaluation or objective metrics.

**Presentation**

The paper is clearly written and easy to follow. The introduction effectively motivates the need for a "training ground" and outlines the key challenges. The system overview diagrams (Figure 2 and Figure 10) are helpful for understanding the overall architecture. The qualitative comparison in Figure 4 is particularly effective at illustrating the practical difference between a textually plausible plan and a physically executable one. The appendices provide valuable details on the data generation pipeline and system architecture.

**Contribution**

The main contribution is the EmboMatrix framework itself—a complete and scalable pipeline for grounding LLMs in physical interaction. This work makes a strong case for the paradigm of interactive learning over static dataset fine-tuning for embodied intelligence. The paper's most significant finding is that this intensive, interaction-based training allows a smaller model (EmboBrain-7B) to acquire superior embodied reasoning skills compared to vastly larger models trained primarily on text data. This has important implications for the future development of capable and efficient embodied agents.

**Strengths**

1.  **Automated Data Factory:** The multi-agent-driven data factory (Section 4.1) is a major strength. It provides a scalable solution for generating a curriculum of tasks that are diverse, physically plausible, and guaranteed to be solvable, which is a long-standing challenge in the field. The ablation showing the diversity gain from social simulation is convincing (Figure 5).
2.  **Impressive Performance Gains:** The results are excellent. The fact that EmboBrain-7B surpasses GPT-4o and DeepSeek-R1 (671B) on both benchmarks is a very strong result (Table 1) and clearly demonstrates the value of the proposed training methodology.
3.  **Well-Designed Reward Structure:** The hierarchical reward architecture (Section 4.3) is thoughtfully designed to tackle the sparse reward problem. The inclusion of the `Semantic Relevance` reward is a key element, and its effectiveness is clearly demonstrated in the ablation study (Figure 7).

**Weaknesses**

1.  **Sim-to-Real Gap:** The paper operates entirely within simulation and does not address the significant challenge of transferring the learned policies to the real world. While this is a common limitation in the field, the claims about building "truly intelligent embodied agents" should be tempered by the absence of any real-world experiments.
2.  **High-Level Action Space:** The agent operates with a library of 13 high-level primitives like `pickup` and `open` (Table 8). This abstracts away the challenges of low-level motor control, perception, and error recovery. While this is a valid choice to focus on high-level reasoning, it limits the scope of the "embodied decision-making" being solved.
3.  **Reliance on LLM-as-Judge:** The use of GPT-4 to evaluate task diversity and scene aesthetics (Section 5.2.1, Tables 6 & 7) is a methodological weakness. LLM-based evaluators are known to have biases and may not be reliable proxies for human judgment or objective quality metrics.

**Questions**

1.  The paper focuses on high-level planning. How do you envision integrating this work with low-level controllers? Would the feedback loop change if the primitive skills could fail, and how would EmboBrain adapt to such failures?
2.  Given the impressive simulation results, what do you see as the primary obstacles to transferring EmboBrain to a physical robot? Are there plans for sim-to-real experiments?
3.  Could you comment on the choice to use a GPT-4 based evaluator for scene quality and task diversity? Were any human validation studies conducted to calibrate or verify the reliability of the LLM evaluator's scores?

**Rating**

- Overall (10): 8 — A strong paper with a significant contribution in the form of a scalable training framework and impressive empirical results, despite the unaddressed sim-to-real gap.
- Novelty (10): 8 — The concept of an integrated training ground with an automated multi-agent data factory is a novel and impactful contribution to the embodied AI community (Section 4.1).
- Technical Quality (10): 8 — The system is well-engineered and the experiments are thorough, though the reliance on an LLM-as-judge for some evaluations is a minor weakness (Section 5.2.1).
- Clarity (10): 9 — The paper is very well-written, with clear explanations and illustrative figures that effectively convey the core concepts and results (Figure 4, Figure 7).
- Confidence (5): 5 — I am an expert in embodied AI and robotics and am very confident in my evaluation.

---

### Review 3

**Summary**

This paper proposes EmboMatrix, a comprehensive system for training large language models (LLMs) on embodied decision-making tasks through reinforcement learning in a simulated environment. The system features three key innovations: a multi-agent data factory for task generation, a scalable simulation backend for efficient data collection, and a hierarchical reward architecture to facilitate learning. The authors use this system to train EmboBrain, a 7B LLM that achieves superior performance on two embodied task benchmarks compared to strong baselines, including models with hundreds of billions of parameters.

**Soundness**

The methodological approach is sound. The problem is well-formulated as a reinforcement learning objective (Equation 3), where the goal is to maximize expected rewards from a distribution of tasks. The hierarchical reward architecture (Section 4.3) is a well-justified form of reward shaping to address the credit assignment problem in long-horizon tasks. The ablation study in Figure 7 provides strong empirical evidence that the semantic relevance reward component is critical for successful training, confirming the soundness of this design choice. The use of Group Relative Policy Optimization (GRPO) is a reasonable choice for an on-policy RL algorithm for LLMs.

**Presentation**

The paper is presented clearly. The problem formulation in Section 3 is concise and formally defines the concept of a "training ground." The description of the hierarchical reward architecture in Section 4.3 is detailed and easy to understand. The learning algorithm is briefly but adequately described in Section 4.4. The learning curve in Figure 7 is a clear and compelling visualization of the algorithmic contribution of the reward design. The appendices provide useful supplementary information, such as the specific reward coefficients used (Appendix C).

**Contribution**

The paper makes a solid contribution to the field of RL for LLMs, particularly in the domain of embodied AI. While applying RL to LLMs is not new, this work provides a concrete and highly effective instantiation for the embodied setting. The main algorithmic contribution is the design and validation of the hierarchical reward architecture, which proves essential for making learning tractable. The paper demonstrates that with the right environment and reward signals, RL can be a powerful tool for instilling physical grounding and complex reasoning skills in LLMs, moving beyond standard supervised fine-tuning or RLHF on text-based preferences.

**Strengths**

1.  **Principled Reward Design:** The hierarchical reward architecture is a key strength. It systematically guides the agent from learning basic syntax (`r_f`) to exploring relevant objects (`r_r`) and finally to achieving the task goal (`r_g`). This is a much more effective approach than relying on a single sparse reward.
2.  **Strong Algorithmic Validation:** The ablation study on the semantic relevance reward (Figure 7) is a crucial experiment that cleanly demonstrates the impact of the reward design on learning efficiency and final performance. This provides strong support for the authors' algorithmic claims.
3.  **Successful Application of RL:** The paper serves as a strong case study for the successful application of on-policy RL to train a powerful embodied agent. The significant performance lift from the base model to EmboBrain (Table 1) showcases the power of interactive learning driven by an effective RL loop.

**Weaknesses**

1.  **Limited Justification for RL Algorithm Choice:** The paper uses Group Relative Policy Optimization (GRPO) but provides minimal justification for this choice over other policy gradient methods like PPO, or even off-policy or offline RL methods which could potentially be more sample-efficient. A brief discussion on why GRPO was selected would strengthen the paper.
2.  **Reward Hyperparameter Tuning:** Appendix C provides the reward coefficients, but there is no discussion of how these values (e.g., `β = 0.2`, total reward of 30) were chosen. The performance of RL algorithms can be highly sensitive to reward scaling and hyperparameters. A sensitivity analysis or at least a brief description of the tuning process would improve the reproducibility and rigor of the work.
3.  **Comparison to Other Learning Paradigms:** The training is purely RL-based after initialization. The paper could benefit from a comparison or discussion against alternative learning paradigms, such as offline RL from the generated data, or a hybrid approach combining supervised learning on expert trajectories (which could be generated by the data factory) and RL fine-tuning.

**Questions**

1.  Could you elaborate on the decision to use GRPO? Were other RL algorithms like PPO considered, and if so, how did they perform in comparison?
2.  How sensitive is the training process to the reward coefficients specified in Appendix C? For example, how does performance change with different values of `β` for the semantic relevance reward?
3.  The semantic relevance reward `r_r` rewards interaction with any goal-relevant object. Did you experiment with more nuanced intermediate rewards, such as rewarding the completion of necessary sub-goals in the correct order?

**Rating**

- Overall (10): 8 — A strong paper demonstrating a successful application of RL for embodied AI, with a well-designed reward system and compelling results.
- Novelty (10): 7 — The hierarchical reward design is a thoughtful, though not entirely novel, application of reward shaping principles; the main novelty is in its effective instantiation for this complex domain (Section 4.3).
- Technical Quality (10): 9 — The RL methodology is sound and the ablation study provides rigorous support for the core algorithmic claims, though more detail on hyperparameter tuning would be beneficial (Figure 7).
- Clarity (10): 9 — The problem formulation and the description of the reward architecture are exceptionally clear (Section 3, Section 4.3).
- Confidence (5): 4 — I have a strong background in reinforcement learning and feel confident in my assessment of the algorithmic aspects of the paper.

---

### Review 4

**Summary**

This paper presents EmboMatrix, a large-scale system designed to train LLMs for embodied decision-making. The system comprises an automated data factory to generate tasks, a distributed simulation backend for efficient interaction, and a multi-level reward function for training via reinforcement learning. The authors use this system to train a 7B model, EmboBrain, which they show outperforms much larger models on two embodied reasoning benchmarks. The central thesis is that interactive learning in a rich, simulated environment is key to developing genuine embodied intelligence.

**Soundness**

The overall methodology is ambitious but has some issues in its reporting and evaluation. A major concern is the use of a private, internally-generated benchmark ("Our Agent-Generated Benchmark") for a significant portion of the evaluation (Table 1). Without public access to this benchmark, the results are not fully reproducible or verifiable. Furthermore, there are inconsistencies in the reported performance gains. The abstract and Section 5.1 claim EmboBrain-7B surpasses DeepSeek-R1 by 9.5% on average, but the conclusion states a "14.2% performance gain over a strong baseline" (Block 37). This discrepancy undermines confidence in the reported results. The core ideas themselves are logical, but the scientific validation has some weak points.

**Presentation**

The paper is generally well-written, but there are several presentation flaws. There are two different diagrams both labeled as "Overview of the EmboMatrix training pipeline" (a Mermaid chart in Block 8 and a more detailed diagram in Block 10/Figure 2), which is confusing. Figure captions are sometimes unhelpful or mismatched; for example, the caption for Figure 6 (Block 27) refers to Figure 5 in its text ("as shown in Fig. 5"). The images in Figure 6 (Blocks 30-32) are low resolution and hard to interpret. These small but numerous presentation issues detract from the paper's professionalism and clarity.

**Contribution**

The paper's contribution is primarily one of engineering and scale. The authors have successfully integrated several complex components—procedural content generation, distributed simulation, and reinforcement learning—into a single, effective system. The resulting performance of the EmboBrain agent is impressive. However, the novelty of the individual components is limited. Multi-agent data generation, scalable simulators, and hierarchical rewards are all established concepts. The contribution lies in the successful synthesis and the demonstration of what can be achieved at scale, rather than a fundamental algorithmic or theoretical breakthrough.

**Strengths**

1.  **Ambitious Scope:** The project successfully tackles the end-to-end problem of training an embodied agent, from data generation to final policy, which is a commendable and challenging undertaking.
2.  **Strong Headline Results:** Despite some inconsistencies, the core result that a 7B model trained with EmboMatrix can outperform a 671B model is powerful and highlights the benefits of the proposed approach (Table 1).
3.  **Thorough System Ablations:** The paper includes good ablation studies that validate the key design choices for the data generation (Table 2), simulation efficiency (Table 3), and reward architecture (Figure 7).

**Weaknesses**

1.  **Inconsistent Claims:** The paper reports conflicting performance gains for its main model (9.5% in the abstract vs. 14.2% in the conclusion), which is a significant flaw that needs to be corrected.
2.  **Use of a Private Benchmark:** A large part of the evaluation is conducted on an internally-verified benchmark that is not public (Table 1). This makes the results on that benchmark difficult to interpret and impossible to reproduce.
3.  **Clarity and Presentation Issues:** The paper suffers from several minor but distracting presentation errors, including duplicated figures with the same purpose (Block 8 vs Block 10), confusing figure captions (Block 27), and low-quality images (Blocks 30-32).
4.  **Potentially Overstated Novelty:** The paper frames EmboMatrix as the "first training ground of its kind," but it builds heavily on prior work in procedural generation, simulation platforms, and RL for robotics. The contribution is more of a powerful integration and scaling-up of these ideas rather than a completely new paradigm.

**Questions**

1.  Can you please clarify the discrepancy in the reported performance gain over the baseline? Is it 9.5% (as stated in the abstract) or 14.2% (as stated in the conclusion), and what specific numbers are used to calculate this value?
2.  Are there plans to release the "Our Agent-Generated Benchmark" to allow for reproducible comparisons? If not, can you justify why the results on this benchmark should be considered convincing by the community?
3.  Could you please revise the figures to improve clarity? Specifically, please remove the redundant overview diagram (Block 8) and improve the resolution and labeling of the scene generation examples (e.g., Figure 6).

**Rating**

- Overall (10): 6 — The paper presents an impressive engineering effort with strong results, but is held back by inconsistent claims, reliance on a private benchmark, and several presentation flaws.
- Novelty (10): 6 — The contribution is more in the successful large-scale integration of existing ideas rather than the introduction of fundamentally new concepts (Section 4).
- Technical Quality (10): 7 — The system itself appears to be of high quality, but the scientific evaluation is weakened by the use of a non-public benchmark and inconsistent reporting of results (Table 1, Abstract vs Conclusion).
- Clarity (10): 6 — While generally readable, the paper has confusing duplicate figures, unclear captions, and inconsistent numerical claims that hinder clear communication (Block 8 vs 10, Block 37).
- Confidence (5): 5 — I am highly confident in my assessment, as the weaknesses identified are based on verifiable inconsistencies and standard practices for scientific evaluation.