OpenReview.net
Search OpenReview...
Login
back arrowGo to ICLR 2026 Conference homepage
EmboMatrix: A Scalable Training-Ground for Embodied Decision-Making
Download PDF
ICLR 2026 Conference Submission25164 Authors
20 Sept 2025 (modified: 24 Dec 2025)
ICLR 2026 Conference Submission
Everyone
Revisions
BibTeX
CC BY 4.0
Keywords: Embodied Decision Making, LLM, Embodied Brain
TL;DR: EmboMatrix is a scalable, annotation-free training ground that aligns data, system, and RL algorithm design to enable autonomous environment exploration by LLMs, yielding consistent gains on embodied decision making benchmarks.
Abstract:
Embodied decision-making enables agents to translate high-level goals into executable actions through continuous interactions within the physical world, forming a cornerstone of general-purpose embodied intelligence. Large language models (LLMs), with their general decision-making capabilities, offer a promising path to realize this potential; however, LLMs trained solely on language lack exposure to physical environments, limiting their true embodied understanding. To bridge this gap, we propose the concept of a \textbf{training ground}: a comprehensive infrastructure that provides task and scene simulation, embodied interaction, and feedback signals, offering a one-stop solution for LLM acquire genuine embodied decision-making skills. In this work, we present EmboMatrix, the first training ground of its kind, providing massive and diverse tasks with efficient simulation and precise rewards. EmboMatrix incorporates a series of novel techniques: a multi-agent data engine for large-scale task and scene generation, a distributed heterogeneous-hardware system for scalable simulation, and a multi-level reward architecture for precise supervision. Leveraging EmboMatrix, we cultivate \textbf{EmboBrain}, an LLM whose embodied decision-making abilities emerge from extensive embodied interactions. Experiments show that EmboBrain-7B surpasses the 671B DeepSeek-R1 baseline by 9.5% on two challenging embodied decision-making benchmarks, demonstrating the power of interactive, environment-grounded learning for building truly intelligent embodied agents. The code will be released upon the paper's acceptance.

Primary Area: applications to robotics, autonomy, planning
Submission Number: 25164
Filter by reply type...
Filter by author...
Search keywords...

Sort: Newest First
7 / 7 replies shown
Official Review of Submission25164 by Reviewer LYJB
Official Reviewby Reviewer LYJB04 Nov 2025, 10:31 (modified: 12 Nov 2025, 18:29)EveryoneRevisions
Summary:
This paper proposes a 'training ground' for LLM-based embodied agents. This environment, referred to as 'EmboMatrix', provides an end-to-end framework that encompasses scene generation, simulation for model rollouts, and reward signals for both training and evaluation. The authors introduce various techniques to optimize the simulation and rollout processes. LLM models were subsequently trained using the EmboMatrix. Experimental results indicate that the resulting model outperformed both the base models and general-purpose, large-parameter models when evaluated on scenes generated within the EmboMatrix.

Soundness: 3: good
Presentation: 3: good
Contribution: 3: good
Strengths:
The reviewer appreciates the framework's ability to enable high-throughput rollouts, noting that rollout speed is often a bottleneck in embodied AI RL training. The proposed components¡ªspecifically the pre-cached language-physics interface, resource scheduler, and task dispatcher¡ªappear to work well. The ablation study also shows they improve simulation speed, which the reviewer agrees is critical for RL practitioners.

This paper is well-structured and very easy to read. While some further clarification is needed, most of the technical details are clearly presented.

The experimental results look promising, the EmboBrain models outperform baseline models by and large general purpose LLMs.

Weaknesses:
The current documentation for the multi-agent-driven automated data factory lacks essential details regarding the intricate interactions between its various components. Specifically, the reviewer requires a more comprehensive explanation of how these multiple agents communicate, coordinate, and influence the generation of instructions and subsequent scene constructions. Without this crucial clarification, the underlying mechanisms and overall impact on the data factory's output remain ambiguous and difficult to assess.

Confidence intervals are not included in the results. Given the potentially high variance of RL methods, the absence of confidence intervals makes it challenging to accurately assess and compare the different approaches.

Questions:
In the pre-cached language-physics interface, physically plausible outcomes from a pre-computed set are used instead of running a full simulation. This raises two critical questions: first, what is the accuracy of this approximation, and second, is this technique employed solely during training or also during evaluation?

In the context of EmboMatrix rollout and training, which aspect consumes the most time: the environment step, model inference, or gradient update? Please provide a detailed explanation.

Regarding the parallel rollouts, are the model updates performed synchronously or asynchronously? If the updates are synchronous, could you please comment on the associated synchronization overhead? Conversely, if the updates are asynchronous, what impact does this have on training performance?

Regarding the semantic reward, could you elaborate on the design rationale for basing it on the intersection of needed objects and objects the agent has interacted with? How does this reward structure generalize to tasks that are not object-centric? Additionally, did you experiment with alternative reward formulations?

Given that intermediate rewards are often prone to reward hacking, did you observe any such unintended behaviors or policy exploitation stemming from this specific design?

Flag For Ethics Review: No ethics review needed.
Rating: 6: marginally above the acceptance threshold. But would not mind if paper is rejected
Confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.
Code Of Conduct: Yes
Response Reviewer LYJB Part 2.
Official Commentby Authors25 Nov 2025, 14:39Everyone
Comment:
Response to Question 1 by Reviewer LYJB:
In the pre-cached language-physics interface, physically plausible outcomes from a pre-computed set are used instead of running a full simulation. This raises two critical questions: first, what is the accuracy of this approximation, and second, is this technique employed solely during training or also during evaluation?

We observe that when training embodied models in a fully end-to-end manner, from high level to low level. The overwhelming majority of training compute and gradient updates are consumed by fitting low-level motor trajectories and other fine-grained control behaviors. This motivates our core objective: to decouple high-level planning from low-level manipulation during training. In this work, we focus exclusively on improving the capability of the high-level model. To achieve this, we introduce a pre-cached language¨Cphysics interface that provides a highly abstracted representation of low-level consequences, along with pre-sampled outcomes of low-level skill executions. This design allows the high-level policy to reason over semantically meaningful transitions without incurring the cost of simulating full-fidelity physical executions at training time.

Regarding the two questions you raised:

The accuracy of this approximation. To ensure the fidelity of our abstraction, we exhaustively sample all plausible physical states of task-relevant objects using the APIs provided by IsaacSim and Behavior. These sampled transitions are aligned with the action-sequencing conventions defined in the Embodied Agent Interface (EAI) , NeurIPS 2024. During task and dataset construction, we additionally verify that all initial and goal states defined in the BDDL file are physically reachable; tasks that fail this feasibility check are excluded from both training and evaluation. Furthermore, for every task, we enumerate and cache all predicates over relevant object pairs as defined by the simulator. This guarantees that whenever the preconditions of a skill are satisfied, the corresponding outcome in our cache reflects a 100% physically valid result. For example, when invoking Place(object1, object2, "ontop"), if object1 is grasped and the robot is within manipulable distance of object2, and the cached predicate object1 ontop object2 is feasible, we directly assign object1 to a valid sampled placement pose. Crucially, this design enables fast and accurate rollout validation: the high-level policy always encounters outcomes consistent with EAI¡¯s execution semantics, while avoiding costly low-level physics simulation. As a result, the training process remains focused on improving high-level decision making rather than repeatedly fitting low-level motor trajectories.

Consistency during training and evaluation. To ensure alignment with the Embodied Agent Interface, we apply the same abstraction and feasibility assumptions during both training and evaluation. This ensures that the high-level model is trained under the exact abstraction it will encounter at test time.

Response to Question 2 by Reviewer LYJB:
In the context of EmboMatrix rollout and training, which aspect consumes the most time: the environment step, model inference, or gradient update? Please provide a detailed explanation.

After incorporating our full set of simulation¨Coptimization techniques, the end-to-end training loop¡ªcomprising rollout sampling, simulator-based reward computation, and parameter updates¡ªtends to split roughly in a 4:3:3 time ratio. Of course, this ratio is also highly dependent on the underlying infrastructure. In our current Kubernetes-based distributed simulation cluster, if all nodes are equipped with high-performance SSDs for storing simulation-related data, the entire simulation pipeline can be further accelerated.

Official Review of Submission25164 by Reviewer F41E
Official Reviewby Reviewer F41E02 Nov 2025, 14:33 (modified: 12 Nov 2025, 18:29)EveryoneRevisions
Summary:
This paper proposes EmboMatrix, a training ground for embodied decision making. The system has three key components: a multi-agent data engine for scene synthesis, a decoupled simulator with pre-cached language-to-physics mappings, and a hierarchical reward curriculum. The curriculum progresses from format correctness to semantic relevance and goal completion. Models trained in EmboMatrix acquire robust planning abilities through interactive learning. The resulting system, EmboBrain, substantially outperforms competitive baselines.

Soundness: 3: good
Presentation: 3: good
Contribution: 3: good
Strengths:
The paper is well-written, with clear figures and detailed explanations that make the work easy to follow.
The paper tackles a central gap in embodied AI by proposing a unified training ground that connects language-only LLMs to physical, interactive decision-making.
The system is well-engineered, combining multi-agent task generation, a scalable decoupled simulator with a pre-cached language¨Cphysics interface, and a hierarchical reward curriculum.
The evaluations are compelling and scale-efficient. A smaller EmboBrain model consistently surpasses a much larger DeepSeek-R1 across multiple embodied decision-making benchmarks and task categories, underscoring the practical impact of the unified training ground.
Weaknesses:
The method relies on a fixed, predefined skill library; the high-level policy can only select from these primitives, so generalization is limited by their coverage and granularity, and the set is not learned or expanded online.

The evaluation relies on GPT-4 to score task diversity and scene aesthetics, which risks creating a self-referential loop. When an LLM-trained system is judged by another LLM, results may be biased toward the evaluator's preferences.

The paper has limited environmental coverage. The training corpus and evaluation are built on 45 Behavior-based scenes, leaving cross-domain generalization and robustness to distribution shift unclear.

Questions:
The experiments focus exclusively on the DeepSeek-distilled Qwen series. Including results from a different model family would help demonstrate the generalization more broadly.
Flag For Ethics Review: No ethics review needed.
Details Of Ethics Concerns:
NA

Rating: 6: marginally above the acceptance threshold. But would not mind if paper is rejected
Confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
Code Of Conduct: Yes
Official Review of Submission25164 by Reviewer SzXd
Official Reviewby Reviewer SzXd01 Nov 2025, 05:40 (modified: 12 Nov 2025, 18:29)EveryoneRevisions
Summary:
This paper introduce the EmboMatrix infrastructure and "training ground" for high-level embodied action sequencing using large language models (LLMs). EmboMatrix combines a multi-agent data factory, a distributed simulation backend , and a hierarchical reward architecture. The framework is used to train EmboBrain, an LLM adapted via reinforcement learning within simulated embodied environments.

Soundness: 1: poor
Presentation: 2: fair
Contribution: 2: fair
Strengths:
The idea of scaling embodied environments to create a data engine for LLM training is interesting and somewhat novel. If executed well, can make a great resource for training embodied planners.
Weaknesses:
Authors claim strong performance gains on the EAI benchmark in abstract L26. However, experimental results on embodied agent interface that the authors report (Overall, Pick and Place, Appliances Using, Kitchen Operation, Compound Task) are not the standard axis of evaluation for EAI (Goal Interpretation, Subgoal Decomposition, Action Sequencing, Transition Modeling). The authors fail to clarify exactly what subset of tasks and what number of examples are used for each track, and the "Overall" score is highly misleading. It seems that the paper is only working on the Action Sequencing subtrack, which should be properly stated in obvious locations.

If the trained model is only capable of action sequence generation and the training envionment is only capable of generation action sequence prediction task data, then it is an overstatement to claim that the environment and model is for embodied decision making.

There's not enough comparison of EmboMatrix to other existing benchmarks and data generation methods, also there isn't sufficient analysis / experiments with "Our Agent-Generated Benchmark" to demonstrate it being a reliable benchmark.

Questions:
Typos: we present a comprehensive experiments (L264)
Additional citations: RoboVerse, Embodied World Models, etc.
How many tasks are there in each of the EAI eval subtracks (Pick and Place, Appliances Using, Kitchen Operation, Compound Task)?
Flag For Ethics Review: No ethics review needed.
Rating: 2: reject, not good enough
Confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
Code Of Conduct: Yes
Response Reviewer SzXd Part 2.
Official Commentby Authors25 Nov 2025, 14:53Everyone
Comment:
Response to Weaknesses 3 by Reviewer SzXd:
There's not enough comparison of EmboMatrix to other existing benchmarks and data generation methods, also there isn't sufficient analysis/experiments with "Our Agent-Generated Benchmark" to demonstrate it being a reliable benchmark.

To clarify the distinct contributions of EmboMatrix in data generation, we provide a systematic comparison with existing methods (see Table). Existing approaches fall into four main categories, none of which simultaneously satisfy the need for scale, semantic richness, and physical-task coupling required for complex embodied AI training:

Manual Authoring (e.g., BEHAVIOR-1K, ALFRED) provides high-quality, coupled tasks but suffers from high cost and limited scale and diversity.
Procedural Generation (e.g., ProcTHOR, VIMA-Bench) is scalable but "rule-bound," resulting in scenes (S) that are decoupled from rich task semantics (I).
Visual/Layout Generation (e.g., WorldCraft, DiffuScene) focuses on visual fidelity but, as we note in our paper (line 148), "ignore[s] task constraints or object interactions."
LLM-Driven (Offline Dataset): This is the most related category, but it has critical gaps. While RoboGen is limited to "small, simple layouts," other recent works like RoboGPT and AgentSense are critically non-scenic: they use LLMs to generate (Task, Plan) pairs or (Persona, Action) sequences but do not generate or instantiate the corresponding, physically executable 3D scenes (S) for these tasks. More importantly, recent research (i.e., "Mind the Gap", arXiv: 2508.00282) has empirically identified the fundamental flaw of relying on zero-shot LLMs for task generation: tasks generated by LLMs, while 'novel', are inherently "asocial" and "disembodied" . The Multi-Agent-Driven Automated Data Factory in EmboMatrix is designed to directly address this "generation gap" with a two-stage framework:
To solve the "asocial" problem: Instead of simple prompts, our "Multi-Agent Social Simulation" module (Appendix A.1) generates semantically rich, long-horizon instructions (I) and goals (G) that are grounded in a social context, providing inherent purpose.
To solve the "disembodied" problem: Instead of generating decoupled scenes, our "Multi-Level Hierarchical Scene Generation" module (Appendix A.2) takes the task's goals (G) and initial conditions (IC) as hard constraints. It then generates a fully task-aware and physically executable scene (S) through a coarse-to-fine, multi-level process.
In summary, EmboMatrix is the first system to systematically couple socially-driven task semantics (I) with physically-grounded scene generation (S) at scale, providing the essential foundation for complex embodied decision-making.

Table: Comparison of Embodied Task and Scene Generation Methods

Method Category	Representative Work(s)	Task Instruction (I) Generation	Scene (S) Generation	Task-Scene Coupling	Key Limitation(s)
1. Manual Authoring	BEHAVIOR-1K, ALFRED, VirtualHome	Manually Designed	Manually Designed	High (but static)	Limited scale, diversity, and extremely high creation cost.
2. Procedural Generation	ProcTHOR, VIMA-Bench	Template-based	Rule-based Layouts	Low	"Rule-bound"; scalable but scenes are decoupled from rich task semantics.
3. Visual/Layout Generation	WorldCraft, DiffuScene, HOLODECK	N/A or Simple Prompt	LLM/Diffusion-aided Visuals	Very Low	Focuses on visual fidelity; ignores task constraints and physical interactivity.
4. LLM-Driven (Offline Dataset)	RoboGen	Single LLM	LLM-gen Simple Layouts	Medium	Generation is tied to small, simple layouts.
RoboGPT	LLM (Self-instruction)	N/A (Does not generate scene)	Medium (Task-Plan only)	Generates (Task, Plan) pairs, but not the corresponding 3D scene (S).
AgentSense	LLM (3-stage Persona)	N/A (Does not generate scene)	Medium (Task-Action only)	Generates (Persona, Action) sequences, but not the corresponding task scene (S).
5. EmboMatrix (Ours)	EmboMatrix	Multi-Agent Social Simulation	Multi-Level Hierarchical Generation	Very High (Systematic)	(Our Contribution) Systematically addresses all above limitations.
Response to Question 1 by Reviewer SzXd:
Typos: we present a comprehensive experiments (L264)

Thanks for pointing it out! We will check this typo in the subsequent version. We corrected this typo in the updated pdf.

Official Review of Submission25164 by Reviewer 8Z5X
Official Reviewby Reviewer 8Z5X26 Oct 2025, 13:29 (modified: 12 Nov 2025, 18:29)EveryoneRevisions
Summary:
To bridge the gap between Large Language Models' (LLMs) reasoning abilities and the physical understanding required for embodied decision-making, this paper introduces EmboMatrix, a scalable "training ground" for interactive, simulation-based learning. EmboMatrix features three key innovations: a multi-agent data factory for generating massive and diverse tasks, a distributed heterogeneous-hardware system for scalable simulation, and a multi-level reward architecture for precise supervision. By training an LLM within this framework to create "EmboBrain," the resulting 7B model significantly outperformed much larger baselines, including the 671B DeepSeek-R1, on two challenging embodied decision-making benchmarks.

Soundness: 2: fair
Presentation: 1: poor
Contribution: 3: good
Strengths:
This work introduces EmboMatrix, the first "training ground" framework specifically proposed to enhance the embodied decision-making capabilities of LLMs through interactive learning.
The framework's effectiveness is clearly demonstrated by significant performance improvements; models trained within EmboMatrix (EmboBrain) substantially outperform much larger models, including the 671B DeepSeek-R1, on challenging benchmarks.
The system's architecturally decoupled design, which separates the LLM trainer from a distributed pool of heterogeneous simulation workers, enables high-throughput parallel rollouts and significantly improves the efficiency of the entire training loop.
The paper effectively uses an ablation study and reward curves to prove the necessity of its hierarchical reward design, showing that the 'semantic relevance reward' (
) is critical for guiding the agent and enabling efficient, stable learning.
Weaknesses:
The pre-cached language-physics interface, which substitutes full physical simulation with pre-computed post-conditions, sacrifices dynamic fidelity and generalization for speed. This approximation may lead to policies that lack robustness in the real world, as it omits crucial details like contact dynamics, sequential dependencies, and realistic failure modes. The paper does not provide a systematic error analysis of this approximation or address potential inconsistencies from pre-rendering.
The model, though the main focus is built on top of LLMs, with its inputs consisting of simulator-generated textual descriptions of the scene, not raw sensory data like visual perception, which bypasses the critical end-to-end challenge of perception and action.
Key details regarding the pre-caching system are not disclosed, including the storage overhead of the pre-computed outcomes, the runtime cache hit rate, and the fallback mechanism used when an interaction is not in the cache (e.g., whether it reverts to a full, costly physical simulation).
The statistical rigor of the evaluation is limited. The main results table only reports the average of 10 samples per task, without reporting variance, confidence intervals, or results from multiple independent random seeds, and lacks statistical significance testing.
The paper's formatting appears to deviate from the official ICLR 2026 template, seemingly using \usepackage{geometry} to alter the page layout.
There are minor formatting and typographical errors. For example, the caption in Figure 5 overlaps with text in the image, and some figure captions (Figures 2, 3) lack sufficient detail. There are also typos, such as "A detailed comparation is shown in Tab 4" (line 126) instead of "comparison", and "EmboMatrix improve its performance by" (line 337).
Questions:
How does the pre-caching approximation, which substitutes physics with post-conditions, affect the policy's robustness to real-world dynamics and unscripted failure modes?

How does the multi-agent data factory ensure genuine task diversity, and isn't this diversity ultimately bottlenecked by the limited set of objects and 45 base scenes from BEHAVIOR-1K?

What are the technical specifications of the pre-caching system, such as its storage overhead, typical runtime cache hit rate, and the fallback mechanism used for a cache miss?

To improve statistical rigor, can the authors provide variance, confidence intervals, or results run with multiple independent random seeds for the main benchmark results in Table 1?

Flag For Ethics Review: No ethics review needed.
Rating: 4: marginally below the acceptance threshold. But would not mind if paper is accepted
Confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.
Code Of Conduct: Yes
Response Reviewer 8Z5X Part 3.
Official Commentby Authors25 Nov 2025, 15:04Everyone
Comment:
Response to Weaknesses 4/Question 4 by Reviewer 8Z5X:

The statistical rigor of the evaluation is limited. The main results table only reports the average of 10 samples per task, without reporting variance, confidence intervals, or results from multiple independent random seeds, and lacks statistical significance testing.

To improve statistical rigor, can the authors provide variance, confidence intervals, or results run with multiple independent random seeds for the main benchmark results in Table 1? We thank the reviewer for identifying this issue. Over the past week, we have augmented our experimental analysis with confidence interval reporting (results averaged across 3 random runs). Updated results are presented in the table below. Going forward, we will incorporate additional random runs in the final version to further ensure statistical reliability.

Model	(Our) Overall	(Our) Pick and Place	(Our) Appliances Using	(Our) Kitchen Operation	(Our) Compound Task	(EAI) Overall	(EAI) Pick and Place	(EAI) Appliances Using	(EAI) Kitchen Operation	(EAI) Compound Task
7B-Base	5.5	11.9	5.6	2.7	14.6	4.1	1.0	4.9	1.3	6.8
EmboBrain-7B	63.0 ¡À 3.1	69.4 ¡À 5.1	61.8 ¡À 4.8	57.5 ¡À 4.7	66.8 ¡À 3.8	60.0 ¡À 2.6	72.6 ¡À 3.0	66.1 ¡À 2.7	35.3 ¡À 2.5	58.6 ¡À 2.5
LLaMA3-8B	5.3	11.1	3.7	0	9.7	3.3	1.2	2.5	2.2	5.3
LLaMA3-8B Enhanced	57.9 ¡À 2.9	61.7 ¡À 6.3	57.9 ¡À 4.1	54.3 ¡À 3.5	60.1 ¡À 4.7	57.6 ¡À 2.3	70.0 ¡À 4.0	60.2 ¡À 3.9	33.1 ¡À 2.9	57.2 ¡À 2.4
Response to Weaknesses 5/6 by Reviewer 8Z5X:
The paper's formatting appears to deviate from the official ICLR 2026 template, seemingly using \usepackage{geometry} to alter the page layout.
There are minor formatting and typographical errors. For example, the caption in Figure 5 overlaps with text in the image, and some figure captions (Figures 2, 3) lack sufficient detail. There are also typos, such as "A detailed comparation is shown in Tab 4" (line 126) instead of "comparison", and "EmboMatrix improve its performance by" (line 337).
Thanks for pointing it out! We will check these typos and format issues in the subsequent version.

Response to Question 2 by Reviewer 8Z5X:
How does the multi-agent data factory ensure genuine task diversity, and isn't this diversity ultimately bottlenecked by the limited set of objects and 45 base scenes from BEHAVIOR-1K?

We thank the reviewer for this thoughtful comment regarding data diversity. We clarify that our multi-agent data factory ensures genuine diversity through combinatorial complexity and architectural extensibility, rather than being limited by the static count of base scenes.

Substantial Asset Scale Even within the current setup, the scale is extensive. We used BEHAVIOR-1K provided 7,722 objectsand 45 scenes spanning all major indoor categories (homes, hotels, offices, schools, gyms).

Combinatorial Task: Diversity Task diversity in our setting extends far beyond "one object in one scene." We leverage social simulation to generate tasks that vary along multiple orthogonal dimensions, creating a task space that is effectively exponential:

Temporal Complexity: Tasks range from short atomic actions to long-horizon procedures (e.g., 10 vs. 20+ steps).
Semantic Difficulty: Goals vary from simple interactions (heating a sandwich) to complex logic (preparing a full meal with dependency constraints).
Social Context: Uniquely, our factory introduces multi-agent dynamics, such as resolving conflicting requests or collaborative resource sharing, which adds a layer of variation unseen in single-agent benchmarks.
Combine substantial asset scale and above diversity, we generate more than 200k different tasks for agents to arrange suitable scene layout.

Open-Ended Architecture Crucially, our framework is not intrinsically bottlenecked by BEHAVIOR-1K. Built upon the Isaac Sim ecosystem, our pipeline supports loading any 3D assets in standard formats (e.g., USD, URDF). This means the system can readily incorporate new environments and objects from the broader graphics and robotics community(like omniverse assets library), ensuring that diversity is scalable and future-proof.
Summary
We sincerely thank the reviewer for the suggestions regarding the integration with low-level physics to enhance completeness. We fully agree that the effective coordination between high-level planning and low-level execution is a pivotal direction for the future of embodied AI. Meanwhile, we maintain that these two levels possess distinct characteristics, each meriting dedicated training resources to maximize their respective capabilities. Finally, we are grateful for the constructive comments on visual inputs and confidence intervals, which have significantly helped us make this work more rigorous and solid. Thank you for your time and guidance!

About OpenReview
Hosting a Venue
All Venues
Contact
Sponsors
Donate
FAQ
Terms of Use / Privacy Policy
News
OpenReview is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the OpenReview Sponsors. ? 2025 OpenReview

EmboMatrix: A Scalable Training-Ground for Embodied Decision-Making | OpenReview