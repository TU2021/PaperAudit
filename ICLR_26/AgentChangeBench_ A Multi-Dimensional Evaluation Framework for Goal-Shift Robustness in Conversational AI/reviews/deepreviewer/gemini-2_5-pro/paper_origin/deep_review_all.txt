Here are four complete and distinct reviews of the paper.

***

### Review 1

**Summary**

This paper introduces AgentChangeBench, a new benchmark designed to evaluate the robustness of tool-augmented language model agents when faced with mid-conversation goal shifts. The authors propose a multi-dimensional evaluation framework with four key metrics: Task Success Rate (TSR), Tool Use Efficiency (TUE), Tool-Call Redundancy Rate (TCRR), and Goal-Shift Recovery Time (GSRT). Using a dataset of 315 tasks across three enterprise domains (airline, banking, retail) and five user personas, the paper evaluates several state-of-the-art models. The results reveal significant performance differences in adaptation and efficiency that are not captured by traditional static benchmarks, demonstrating that high task success does not necessarily imply robustness to dynamic user goals.

**Soundness**

The methodological approach is generally sound and well-motivated. The decision to build upon the existing τ²-bench harness provides a solid foundation, and the extension to include explicit goal shifts and personas is a logical and necessary step forward for agent evaluation. The proposed metrics are thoughtfully designed; TSR's weighted components provide a nuanced view of success, while TUE, TCRR, and GSRT directly measure critical aspects of efficiency and adaptability. The use of an LLM-as-a-judge for the communication quality component of TSR is a pragmatic choice, though it introduces a potential source of noise and subjectivity that is not fully explored. The justification for the TSR component weights (Section 4.3) is based on their perceived importance, which seems reasonable but could benefit from a sensitivity analysis. The dataset construction process, combining human-written and LLM-generated examples with manual review, appears robust.

**Presentation**

The paper is well-written and the core ideas are communicated effectively. The introduction and related work sections clearly position the benchmark's contribution. The appendix is particularly strong, providing extensive details on task generation, personas, and full experimental results that enhance reproducibility. However, the main body of the paper suffers from some significant organizational and formatting issues. Section 4 ("Results and Evaluation Metrics") confusingly begins with misplaced "Motivation" and "Contributions" subsections (4.1, 4.2) that should be in the introduction. There is a major inconsistency in the labeling of figures and tables: a table in Section 4 is labeled "Figure 1", while a bar chart in Section 2 (Block 6) appears to be the intended Figure 1. Furthermore, the caption for the table labeled "Figure 1" is contradictory, referring to GSRT as turns (lower is better) while the table itself shows a recovery rate (higher is better). These presentation flaws detract from the paper's overall quality and should be corrected.

**Contribution**

The paper makes a significant and timely contribution to the field of conversational AI evaluation. By focusing on goal-shift robustness, it addresses a critical and under-explored dimension of agent performance that is paramount for real-world deployment. The introduction of a comprehensive, multi-dimensional metric set that moves beyond simple pass/fail rates is a major step forward. The empirical findings, which highlight stark differences in the adaptive capabilities of leading models (e.g., Table 5), are valuable and underscore the necessity of this new benchmark. AgentChangeBench provides the community with a valuable resource for building more resilient and efficient conversational agents.

**Strengths**

- **Novelty:** The paper is the first to propose a benchmark specifically designed for evaluating agent robustness to mid-conversation goal shifts, a crucial real-world phenomenon.
- **Comprehensive Metrics:** The proposed metrics (TSR, TUE, TCRR, GSRT) provide a multi-faceted and nuanced view of agent performance, capturing success, efficiency, and adaptability far better than traditional benchmarks.
- **Insightful Empirical Analysis:** The experiments reveal important and previously obscured weaknesses in state-of-the-art models, such as high tool redundancy in retail tasks (Table 5) and poor recovery rates for some models (Figure 1/Block 6).
- **Thorough Appendix:** The extensive appendix provides a high degree of transparency and detail, supporting reproducibility.

**Weaknesses**

- **Poor Organization:** The structure of Section 4 is confusing, with introductory material misplaced within the results section.
- **Inconsistent Presentation:** There are errors in figure/table numbering and contradictory captions (e.g., the table labeled "Figure 1" and its description of GSRT).
- **Data Inconsistency:** There appears to be a significant data inconsistency for Claude-3.7-Sonnet's TSR on new retail tasks, which is reported as 79.57% in Table 5 but 61.58% in Table 9. This undermines confidence in the reported results.
- **Simplified Goal Shifts:** The goal shifts are pre-declared and often explicitly signaled (Section 5), which may not fully capture the challenge of detecting more subtle or implicit shifts in user intent.

**Questions**

1.  Could you clarify the discrepancy in Claude-3.7-Sonnet's TSR for new retail tasks between Table 5 (79.57%) and Table 9 (61.58%)? Which value is correct?
2.  The TSR metric relies on weights (0.25, 0.45, 0.30) that are justified qualitatively. Have you performed any sensitivity analysis to see how model rankings change with different weightings?
3.  Regarding the LLM-as-a-judge for communication quality, what steps were taken to ensure consistency and mitigate potential biases of the judge model (GPT-4o-mini)? Was inter-rater reliability measured against human judges on a subset of the data?

**Rating**

- Overall (10): 8 — The paper introduces a highly novel and important benchmark, though it is marred by significant presentation issues and a data inconsistency that must be fixed.
- Novelty (10): 10 — This is the first benchmark to systematically and explicitly evaluate agent robustness to goal shifts, addressing a major gap in the literature.
- Technical Quality (10): 7 — The core methodology is strong, but the data inconsistency between Table 5 and Table 9 and the confusing presentation of results lower the score.
- Clarity (10): 6 — The paper is mostly well-written, but the poor organization of Section 4 and the confusing figure/table labeling significantly hinder readability.
- Confidence (5): 4 — I am confident in my assessment; the strengths are clear, but the presentation flaws and data errors are objectively present in the manuscript.

***

### Review 2

**Summary**

The paper presents AgentChangeBench, a benchmark for evaluating how conversational agents handle changes in user goals during a multi-turn interaction. The authors define four metrics—Task Success Rate (TSR), Tool Use Efficiency (TUE), Tool-Call Redundancy Rate (TCRR), and Goal-Shift Recovery Time (GSRT)—to provide a more granular assessment than existing benchmarks. The framework is tested on three models across banking, retail, and airline domains, concluding that current models show significant weaknesses in efficiency and adaptation that are not visible with standard success metrics.

**Soundness**

The paper's soundness is severely compromised by inconsistencies in the reported data and questionable methodological choices. A critical flaw is the discrepancy in the reported Task Success Rate (TSR) for Claude-3.7-Sonnet on new retail tasks: Table 5 claims a TSR of 79.57%, while Table 9 in the appendix reports it as 61.58%. This is not a minor typo; it fundamentally calls into question the validity of the analysis and conclusions drawn in the main text.

Methodologically, the benchmark's design simplifies a key aspect of the problem. The paper states that goal shifts are "pre-declared sequences" and "often explicitly signaled" (Sections 3.4, 5). This setup primarily tests the agent's ability to *react* to a clear signal, rather than the more challenging and realistic task of *detecting* an implicit or ambiguous shift in user intent. This limits the generalizability of the findings on "robustness."

Furthermore, some metric parameters feel arbitrary. For instance, the TCRR metric uses a "3-turn window" (Section 4.3), but no justification is provided for this specific value over, say, a 2-turn or 5-turn window. The TSR weights are also qualitatively justified (Section 4.3) without empirical validation, making the final score sensitive to the authors' subjective view of what constitutes success.

**Presentation**

The presentation is sloppy and contains numerous errors that should have been caught before submission. The organization is illogical, with a "Motivation and Problem Statement" section (4.1) appearing after the methodology. The paper incorrectly labels a table as "Figure 1" in Section 4, while a bar chart in Section 2 (Block 6) shares the same title, leading to confusion. The caption for this table is self-contradictory, defining GSRT in terms of turns (lower is better) but then referring to the table's contents as a recovery rate (higher is better). These errors significantly detract from the paper's credibility and make it difficult for a reader to trust the content.

**Contribution**

While the problem of evaluating agents on dynamic goals is undoubtedly important, the execution flaws in this paper diminish its contribution. A benchmark's primary value lies in its reliability and trustworthiness. The presence of significant data inconsistencies and confusing presentation makes it difficult to accept the results at face value. The core idea is valuable, but the work in its current form does not provide a reliable foundation for future research. The paper requires substantial revision and re-verification of all reported numbers before its contribution can be fully realized.

**Strengths**

- **Important Problem:** The paper addresses a critical and practical gap in agent evaluation by focusing on dynamic goal shifts.
- **Conceptually Strong Metrics:** The set of proposed metrics (TSR, TUE, TCRR, GSRT) is conceptually well-designed and moves beyond simplistic binary success measures.

**Weaknesses**

- **Critical Data Inconsistencies:** The TSR for Claude-3.7-Sonnet on new retail tasks is reported as 79.57% in Table 5 and 61.58% in Table 9. This is a major error that undermines the paper's credibility.
- **Poor Structure and Presentation:** The paper is poorly organized (e.g., Section 4.1's placement) and contains confusing and erroneous figure/table labeling and captions.
- **Oversimplified Task Design:** Goal shifts are explicitly signaled, which fails to test the crucial capability of detecting implicit goal changes, a key challenge in real-world interactions.
- **Arbitrary Metric Parameters:** Key parameters, such as the 3-turn window for TCRR, lack rigorous justification.

**Questions**

1.  Please provide a definitive explanation for the conflicting TSR values for Claude-3.7-Sonnet in Table 5 and Table 9. Which is correct, and what caused this error? Have all other results been double-checked for similar inconsistencies?
2.  Can you justify the choice of a 3-turn window for TCRR? How sensitive are the TCRR results and model rankings to this parameter?
3.  Given that goal shifts are explicitly signaled, how do you believe your findings would change if agents had to infer shifts from more subtle, implicit user utterances? Does this design choice limit the scope of your claims about agent "robustness"?

**Rating**

- Overall (10): 3 — The paper addresses an important problem, but the severe data inconsistencies and sloppy presentation make the results untrustworthy in their current form.
- Novelty (10): 8 — The focus on goal-shift evaluation is highly novel, though the implementation of the shifts themselves is somewhat simplified.
- Technical Quality (10): 2 — The technical quality is extremely low due to the presence of a major, unexplained data contradiction between the main text and the appendix.
- Clarity (10): 3 — The paper is difficult to follow due to its illogical structure, incorrect figure labels, and contradictory captions.
- Confidence (5): 5 — I am highly confident in this assessment, as the data inconsistencies and presentation errors are objectively verifiable in the manuscript.

***

### Review 3

**Summary**

This paper introduces AgentChangeBench, a practical framework for evaluating how well conversational AI agents adapt to changing user goals in enterprise settings. The authors move beyond simple success/failure metrics by proposing a suite of four metrics focused on task success (TSR), tool efficiency (TUE), redundancy (TCRR), and recovery time from goal shifts (GSRT). By testing modern LLM agents in airline, banking, and retail scenarios with different user personas, the work demonstrates that agents which appear successful on static tasks can be inefficient or slow to adapt in more realistic, dynamic conversations.

**Soundness**

The methodology is sound from a practical and applied perspective. The choice of domains—banking, retail, and airline—is highly relevant to enterprise use cases where such agents are being deployed. The inclusion of five user personas (Section 3.3) adds a valuable layer of realism, simulating the varied communication styles agents must handle. The metrics are directly tied to business outcomes: TSR provides a holistic view of success, TUE and TCRR relate to operational cost and API usage, and GSRT measures the agent's agility, which is critical for user experience. The worked example of GSRT (Section 4.3) is helpful for understanding its application. While some academic rigor could be added (e.g., sensitivity analysis on weights), the current approach is pragmatic and yields actionable insights.

**Presentation**

The paper is generally clear and accessible, particularly for a practitioner-focused audience. The motivation is compelling, and the explanation of the new metrics is straightforward. The tables in the results section (e.g., Table 5) effectively summarize the key takeaways, showing clear performance differences between models on metrics like Recovery and TCRR. The appendix is comprehensive and provides useful details for anyone looking to use or extend the benchmark. However, the organization of the main results section (Section 4) is somewhat confusing, as it mixes motivation and metric definitions with the presentation of results. A clearer separation would improve readability. The confusion around "Figure 1" (a table) and the bar chart in Section 2 is a minor but noticeable distraction.

**Contribution**

The main contribution of this work is its immense practical value. It provides a ready-to-use benchmark and a set of metrics that directly address the challenges faced when deploying conversational agents in the real world. For developers and businesses, the insights from AgentChangeBench are far more actionable than a simple `pass@k` score. For example, knowing that an agent has a high TCRR in retail (Table 5) points to a specific area for optimization (e.g., state management, caching) that could lead to significant cost savings. The finding that GPT-4o has a high recovery rate but also extremely high redundancy in retail is a perfect example of a practical trade-off that this benchmark uniquely reveals. This work successfully bridges the gap between academic agent evaluation and real-world deployment requirements.

**Strengths**

- **High Practical Relevance:** The benchmark addresses a real-world problem (goal shifts) in commercially important domains.
- **Actionable Metrics:** The proposed metrics (especially TCRR and GSRT) map directly to key business concerns like operational cost and customer satisfaction.
- **Realistic Scenarios:** The use of diverse user personas and tasks grounded in real customer service workflows enhances the benchmark's realism.
- **Clear Empirical Insights:** The results provide clear, comparative insights into the strengths and weaknesses of different models in dynamic settings (e.g., Claude's high TSR vs. GPT-4o's high recovery in certain domains).

**Weaknesses**

- **Confusing Structure in Section 4:** The flow of the results section is disrupted by misplaced subsections on motivation and metric definitions.
- **Barrier to Adoption:** The framework is built on the τ²-bench harness, which may present a setup and learning curve for teams not already familiar with it.
- **Limited Open-Source Focus:** The main evaluation focuses on proprietary models. While the inclusion of Qwen2 in the appendix is a good first step, a more thorough evaluation of open-source alternatives would increase the benchmark's utility for a broader audience.

**Questions**

1.  From a practitioner's standpoint, how much effort would be required to adapt AgentChangeBench to a new enterprise domain, such as insurance or healthcare? What are the main components that would need to be developed?
2.  Your results are excellent at diagnosing problems like high redundancy (TCRR). Based on your analysis, do you have any hypotheses or recommendations on *how* to improve agent performance on these specific metrics? For example, what prompting strategies or architectural changes might reduce TCRR?
3.  The persona-based analysis in Table 7 is very interesting. Did you observe any qualitative differences in *how* agents failed for different personas? For example, did the "HARD_1" persona trigger more policy violations or premature transfers to human agents?

**Rating**

- Overall (10): 9 — An excellent and highly practical contribution that provides an immediately useful tool for evaluating and improving real-world conversational agents.
- Novelty (10): 9 — While building on prior work, the specific focus on goal shifts and the corresponding metrics are highly novel and impactful.
- Technical Quality (10): 8 — The methodology is solid and well-executed for its purpose, though a data inconsistency noted by other reviewers should be addressed.
- Clarity (10): 8 — The paper is very clearly written and easy to understand, despite some minor organizational issues in the results section.
- Confidence (5): 5 — I am very confident in my assessment of the paper's practical value and its strengths from an applied perspective.

***

### Review 4

**Summary**

This paper introduces AgentChangeBench, a novel benchmark for evaluating the ability of tool-using LLM agents to adapt to mid-dialogue goal shifts. The authors propose a framework with four new metrics—Task Success Rate (TSR), Tool Use Efficiency (TUE), Tool-Call Redundancy Rate (TCRR), and Goal-Shift Recovery Time (GSRT)—to offer a more granular evaluation than existing methods. The benchmark includes 315 tasks across banking, retail, and airline domains, incorporating five user personas. The experimental evaluation of three leading LLM agents demonstrates that the proposed metrics can uncover significant differences in robustness and efficiency that are missed by traditional, static evaluation protocols.

**Soundness**

The paper's methodology is largely sound. It builds intelligently on prior work (`τ-bench`, `τ²-bench`) while introducing a well-justified new evaluation dimension. The four proposed metrics are clearly defined and motivated, each capturing a distinct and important aspect of agent performance in dynamic contexts. The dataset creation process appears systematic, and the inclusion of personas is a commendable feature that enhances realism. The appendix provides a wealth of detail that speaks to the technical depth of the work.

However, there are a few points that could be strengthened. The uneven distribution of tasks across personas (Table 8) is justified by appealing to real-world frequencies, but this could introduce a bias in the aggregate results, where performance on the `MEDIUM_1` persona is overrepresented. The paper also notes that goal shifts are pre-declared and often explicit, which is a reasonable starting point but represents a simplification of real-world ambiguity. Finally, a significant data inconsistency between Table 5 and Table 9 regarding Claude-3.7-Sonnet's performance needs to be rectified to ensure the soundness of the reported results.

**Presentation**

The paper is generally well-presented, with clear writing and a logical flow through the introduction, related work, and methodology sections. The metrics are explained well, with helpful formulas and a worked example (Section 4.3). The use of tables to present comparative results is effective. The appendix is exemplary in its thoroughness.

The main areas for improvement are in the organization and consistency of the main results section. Section 4 begins with subsections 4.1 and 4.2, which read like they belong in the introduction, disrupting the narrative flow from methods to results. There is also a clear error in labeling a table as "Figure 1" and providing a confusing caption that mixes up the concepts of recovery time (GSRT) and recovery rate. These are correctable issues but currently detract from the professionalism of the manuscript.

**Contribution**

This paper makes a solid and important contribution to the literature on LLM agent evaluation. It convincingly argues for the need to move beyond static task evaluations and provides a concrete, well-developed framework for doing so. The benchmark itself is a valuable resource, and the proposed metrics offer a new vocabulary for discussing and measuring agent performance. The empirical results are not just a demonstration of the benchmark but also provide genuine insights into the current capabilities and limitations of SOTA models, highlighting issues like tool redundancy as a key failure mode. This work will likely become a standard for evaluating agent robustness.

**Strengths**

- **Well-Defined Problem:** The paper clearly articulates the limitations of existing benchmarks and motivates the need for evaluating goal-shift robustness.
- **Comprehensive Evaluation Framework:** The combination of a new dataset, diverse personas, and a multi-dimensional metric suite is very thorough.
- **Insightful Metrics:** TCRR and GSRT, in particular, are novel and highly informative metrics for diagnosing specific agent behaviors.
- **Transparency and Reproducibility:** The detailed appendix, including task examples, persona definitions, and full results, is a major strength.

**Weaknesses**

- **Structural Issues:** The organization of Section 4 is confusing and should be revised for better readability.
- **Presentation Errors:** Incorrect figure/table numbering and contradictory captions need correction.
- **Data Inconsistency:** A key result for Claude-3.7-Sonnet differs between Table 5 and Table 9, which must be resolved.
- **Uneven Persona Distribution:** The non-uniform assignment of tasks to personas could skew the aggregate results, a limitation worth discussing more deeply.

**Questions**

1.  Please address the inconsistency in the reported TSR for Claude-3.7-Sonnet on new retail tasks between Table 5 (79.57%) and Table 9 (61.58%).
2.  The caption for the table labeled "Figure 1" in Section 4 seems to conflate GSRT (measured in turns) with the Goal-Shift Recovery Rate (a percentage). Could you clarify the exact definition of what is being reported in that table and the associated bar chart, and ensure the text and definitions are consistent throughout?
3.  The `MEDIUM_1` persona is assigned more than twice as many tasks as any other persona (Table 8). Could you comment on how this weighting might affect the aggregate persona-based results in Table 7? For example, are the high overall TSR and recovery rates driven disproportionately by this single "impatient, efficient" persona?

**Rating**

- Overall (10): 8 — A strong, valuable contribution with a well-designed benchmark and insightful results, pending correction of several presentation and data consistency issues.
- Novelty (10): 9 — The focus on explicitly modeling and measuring adaptation to goal shifts is a highly novel and needed contribution to agent evaluation.
- Technical Quality (10): 7 — The underlying technical work is strong, but the score is reduced due to a verifiable data inconsistency and some methodological simplifications (e.g., explicit shifts).
- Clarity (10): 7 — The paper is clearly written for the most part, but the structural and labeling errors in the results section create significant confusion.
- Confidence (5): 5 — I am very confident in my evaluation, having thoroughly reviewed the text, tables, and appendix.