# Global Summary
This paper introduces AgentChangeBench, a benchmark designed to evaluate how tool-augmented conversational AI agents handle mid-dialogue goal shifts. The authors argue that existing benchmarks oversimplify real-world interactions by focusing on static objectives. AgentChangeBench consists of 315 task sequences across three enterprise domains (banking, retail, airline), featuring five distinct user personas and explicit goal-shift triggers. The core contribution is a multi-dimensional evaluation framework with four new metrics: Task Success Rate (TSR), Tool Use Efficiency (TUE), Tool-Call Redundancy Rate (TCRR), and Goal-Shift Recovery Time (GSRT).

An empirical study on frontier models (GPT-4o, Claude-3.7-Sonnet, Gemini-2.5-Flash) reveals significant performance differences obscured by traditional metrics. For example, on airline tasks, GPT-4o achieves a 92.2% goal-shift recovery rate while Gemini-2.5-Flash only reaches 48.6%. In retail tasks, models exhibit high redundancy (TCRR > 80%) despite near-perfect tool parameter validity. The key claim is that high raw accuracy does not guarantee robustness to dynamic goals, and metrics for recovery and efficiency are essential for realistic agent evaluation. Stated limitations include the use of relatively benign personas, a focus on customer-service domains, and pre-declared goal shifts.

# Abstract
AgentChangeBench is a new benchmark for evaluating how tool-augmented language model agents adapt to mid-dialogue goal shifts. It covers three enterprise domains and uses four metrics: Task Success Rate (TSR), Tool Use Efficiency (TUE), Tool-Call Redundancy Rate (TCRR), and Goal-Shift Recovery Time (GSRT). The benchmark includes 2,835 task sequences and five user personas designed to trigger realistic goal shifts. Evaluations of frontier models show sharp contrasts: GPT-4o reaches 92.2% recovery on airline booking shifts, while Gemini drops to 48.6%. Retail tasks show redundancy rates above 80% despite high parameter validity. The paper concludes that high accuracy on static tasks does not imply robustness to dynamic goals and that measuring recovery and redundancy is critical.

# Introduction
- The paper argues that existing LLM agent benchmarks fail to capture real-world dynamics because they assume user goals are fixed. In reality, users often change objectives mid-conversation.
- To address this, the paper introduces AgentChangeBench, a framework to measure how agents handle multi-turn goal changes and adapt to different user personas.
- The work builds on persona-based user simulation and systematic benchmark creation.
- Contributions are listed as:
    1. A novel focus on evaluating mid-conversation goal shifts and persona adaptation.
    2. A dataset of 315 validated tasks across banking, retail, and airline domains, with five personas and explicit goal shifts.
    3. A methodological framework for evaluating goal shift recovery.
    4. An empirical study revealing significant performance differences among SOTA models in success, recovery time, efficiency, and redundancy, which pass@k metrics would miss.
- The full benchmark, evaluation harness, and experimental artifacts are released as supplementary material.

# Experiments
This section details the evaluation metrics, results, and analysis.

- **Evaluation Metrics (4.3):** The paper introduces a multi-dimensional framework to extend beyond pass@k.
    - **Task Success Rate (TSR):** A weighted average of three channels: `communicate_info_rate` (25%), `action_rate` (45%), and `nl_assertion_rate` (30%).
    - **Tool Usage Efficiency (TUE):** A composite score of tool correctness (T) and parameter validity (P), calculated as `TUE = 0.6T + 0.4P`. Parameter validity is reported to be near-saturated (mean P=0.986).
    - **Tool-Call Redundancy Ratio (TCRR):** Measures the fraction of duplicate tool calls within a 3-turn window or exceeding a batch threshold of 2 calls to the same function.
    - **Goal-Shift Recovery Turns (GSRT):** Measures recovery time in turns after a user-initiated goal shift across three events: acknowledgment, first relevant tool call, and outcome achievement. A shift is considered "recovered" if acknowledgment occurs and no human transfer is initiated.

- **Results and Analysis (4.4):**
    - **Overall Performance (Table 4):** Across all domains, Claude-3.7-Sonnet has the highest average TSR (Banking: 57.54%, Airline: 65.14%, Retail: 79.57%). GPT-4o is second, and Gemini-2.5-Flash is weakest, especially in Banking (47.36%).
    - **Goal-Shift Sensitivity (Table 5):** On new tasks, GPT-4o has the best recovery in Airline (92.2%) with low redundancy (TCRR 13.54%). Gemini-2.5-Flash has the lowest recovery in Airline (48.6%). Retail tasks show very high redundancy for GPT-4o (89.14%) and Gemini (66.45%).
    - **Key Findings:** New goal-shifted tasks are harder. Recovery rates vary widely (48.6% to 92.2%). Redundancy is high in retail (66-89%) but low in airline (13.5-24.1%). Parameter accuracy is saturated (mean 0.986), so TUE differences are driven by tool correctness.
    - **Domain-Specific Performance:**
        - **Airline:** Claude and GPT-4o lead in TSR (59.5%-69.9% on new tasks) with fast recovery (79.2-92.2%). Lowest redundancy.
        - **Retail:** Claude is strongest (79.6% TSR) with high recovery (89.5%). GPT-4o and Gemini have very high TCRR (89.1% and 66.5%).
        - **Banking:** Hardest domain (TSRs 26.9-57.5%). Moderate recovery and high redundancy.
    - **Dataset Quality:** AgentChangeBench has 315 tasks, 3 domains, 5 personas, and explicit goal shifts, compared to τ-bench (234 tasks, 2 domains) and τ²-bench (105 tasks, 1 domain).
    - **Persona-Based Analysis (Table 7):** The MEDIUM_2 persona has the highest TSR (0.580) and recovery rate (0.922). The HARD_1 persona has the lowest TSR (0.430) and a lower recovery rate (0.793). TUE is high across all personas (0.946-0.990).

# Related Work
- Existing benchmarks like τ-bench, τ²-bench, and AgentBench are foundational for tool-use and multi-turn evaluation but primarily assume static user goals.
- **τ-bench:** Introduced simulated multi-turn interactions but assumed static goals.
- **τ²-bench:** Extended τ-bench to telecom support but did not explicitly test adaptability to changing goals.
- **AgentBench:** Evaluates agents in diverse environments (OS, web) but under stable objectives.
- AgentChangeBench is distinguished by its explicit focus on goal sequences, five user personas, and metrics designed to measure recovery from goal shifts (GSRT, TSR, TUE, TCRR).

# Method
- **Dataset Design:** The benchmark contains 315 multi-turn tasks across three domains: banking (50 tasks), airline (100 tasks), and retail (165 tasks). Each task includes one of five user personas and explicit goal shifts.
- **Task Generation:** A mix of human-written and LLM-generated examples. It reuses 50 airline and 114 retail templates from τ²-bench, adds 50 new scenarios for airline/retail, and creates 50 original banking tasks. All tasks are annotated with explicit goal sequences.
- **User Personas:** Five personas are defined with distinct behavioral traits: EASY_1 (polite), EASY_2 (distracted), MEDIUM_1 (impatient), MEDIUM_2 (curious), and HARD_1 (suspicious). The distribution of tasks across personas is uneven to reflect real-world frequencies, with MEDIUM_1 having the most tasks (69).
- **Task Schema:** Tasks are defined in a JSON schema that specifies the persona, known/unknown information, and an ordered list of goals in a `goal_shifts` object. Goal transitions are triggered naturally by the user simulator. The user only provides information; the agent performs all tool calls.
- **Evaluation Harness:** The study uses the τ²-bench evaluation harness, extending it to handle goal shifts and persona-driven communication.

# Motivation
- **Problem Statement:** Existing benchmarks use binary success metrics like pass@k, which fail to capture nuanced performance. They treat all failures equally and do not measure efficiency, redundancy, or adaptation to goal shifts.
- **Key Contributions:**
    1. **Multi-Channel Success (TSR):** Replaces binary metrics with a weighted average of communication (25%), action (45%), and behavioral compliance (30%). Communication quality is assessed by a GPT-4o-mini judge model.
    2. **Efficiency Metrics (TUE, TCRR):** Measure tool selection accuracy, parameter validity, and redundancy, which are critical for cost and responsiveness.
    3. **Recovery Assessment (GSRT):** Quantifies adaptation latency after a goal shift across acknowledgment, tool use, and goal achievement.
    4. **Enterprise Alignment:** Metrics are designed to align with real-world deployment needs like efficiency and robustness.

# Conclusion
- **Limitations and Future Work:**
    - Personas are relatively benign and do not cover adversarial or deceptive behaviors.
    - The scope is limited to customer-service domains and APIs, excluding tools like code editors, OS control, or browsers. No unified tool protocol like MCP is used.
    - Goal shifts are pre-declared and explicitly signaled, not implicit or ambiguous.
    - Model coverage is limited to three major families.
- **Conclusion:**
    - The paper introduced AgentChangeBench, a benchmark with 315 tasks, 5 personas, and 3 domains to evaluate agents on dynamic goal shifts.
    - It proposed four metrics (TSR, TUE, TCRR, GSRT) to measure success, efficiency, redundancy, and recovery.
    - Experiments showed clear differences in robustness among models (e.g., Claude-3.7-Sonnet recovers fastest, Gemini-2.5-Flash lags in banking) that pass@k would miss.
    - Future work includes expanding to new domains, automated task generation, and multilingual settings.

# Appendix
- **Task Generation:** A hybrid LLM-assisted and manual review process ensures tasks are valid, coherent, and executable.
- **Metric Parameters:** TCRR uses a `window_size` of 3 turns and a `batch_threshold` of 2 calls.
- **Evaluation Protocol:** Each task is evaluated across 3 independent runs.
- **Tool Definitions:** Provides lists of available API tools for Banking, Retail, and Airline domains.
- **Additional Results:**
    - **Table 8:** Shows task distribution per persona: EASY_1 (33), EASY_2 (34), MEDIUM_1 (69), MEDIUM_2 (34), HARD_1 (31).
    - **Table 10:** Provides detailed efficiency and recovery data. Key examples:
        - GPT-4o on Airline-new: TUE 99.69%, TCRR 13.54%, 90 shifts, 92.2% recovery.
        - Gemini on Airline-new: TUE 99.64%, TCRR 17.63%. GSRT not reported due to credit limits.
        - GPT-4o on Retail-new: TCRR 89.14%.
        - Claude on Banking: 142 shifts, 58.5% recovery, 40.1% transfer rate.
- **Open-Source Model Evaluation:**
    - An evaluation of Qwen2.5-14B-Instruct was performed with a single run per domain.
    - Qwen performed competitively on airline tasks (TSR 59.75%, TCRR 5.57%, Recovery 84.8% on new tasks) but struggled in banking and retail.
    - Integration challenges were noted for Mistral (different function-calling format) and DeepSeek (internal "thinking" blocks disrupt turn-based evaluation).
- **Benchmark Roadmap:**
    - Near-term plans include adding an "education" domain with over 100 tasks.
    - Future scenarios will move beyond customer service to operations, RAG, and data/BI agents.
    - The team plans to incorporate more tool classes via the Model Context Protocol (MCP) and harden personas to be more adversarial.
- **System Architecture:** The framework extends the τ²-bench architecture. Goal sequences are defined in task data, and an LLM judge performs post-hoc analysis of transcripts to detect shift timing and calculate GSRT.

# References
The paper cites related work in persona-based models, benchmark creation (τ-bench, τ²-bench, AgentBench), tool-use in LLMs (Toolformer, ReAct), and various interactive agent environments (WebArena, ALFWorld).