{
  "baseline_review": "Summary\n- The paper introduces AgentChangeBench, a benchmark to evaluate tool-augmented LLM agents under mid-dialogue goal shifts across three enterprise domains (banking, retail, airline). The framework defines four metrics: Task Success Rate (TSR), Tool Usage Efficiency (TUE), Tool-Call Redundancy Ratio (TCRR), and Goal-Shift Recovery Turns/Rate (GSRT), with formal definitions for TSR and TUE (Section 4.3; Equations (1)–(2)). The dataset comprises 315 curated tasks with five personas and explicit goal sequences (Sections 3.1–3.4; Table 6; Table 2; Appendix A.1–A.3). Cross-model experiments reveal substantial differences in recovery under goal shifts and redundancy despite high parameter validity (Tables 4–5, 9–10; Figure 1). The authors release the benchmark and evaluation harness configurations (Introduction: Release; Appendix G).Strengths\n- Bolded Title: **Clear formalization of multi-dimensional metrics**\n  - TSR is precisely defined as a weighted sum across communication, action, and NL assertion channels with explicit weights (Section 4.3; Equation (1)), enhancing transparency over binary pass@k; this improves technical soundness and clarity.\n  - TUE is decomposed into Tool Correctness (T) and Parameter Validity (P) with a composite (Section 4.3; Equation (2)), enabling diagnosis beyond success/failure; this increases experimental rigor.\n  - TCRR and GSRT are formally specified, including window and batch definitions for redundancy and first-hitting times for recovery events (Section 4.3; Tool-Call Redundancy Ratio; Goal-Shift Recovery Turns), supporting nuanced analysis and impact.- Bolded Title: **Explicit focus on goal-shift robustness**\n  - The benchmark centers on mid-conversation goal changes, operationalized through pre-declared goal sequences (Section 3.4; “goal_shifts” schema) and LLM-judge-based detection (Appendix G; Figure 3), addressing a gap in prior tool-use benchmarks; this is novel and impactful.\n  - GSRT measures acknowledgment, first relevant tool call, and outcome turns (Section 4.3), and recovery rate is reported alongside TSR/TUE/TCRR (Tables 5, 10; Figure 1), enabling differentiation of agents that adapt quickly versus those that meander; this aids deployment decisions.\n  - Failure-mode analysis highlights late shift detection and over-confirmations linked to GSRT and communication channel drops (Section 4.3; Failure modes; Table 9), providing actionable diagnostic insights.- Bolded Title: **Broad domain and persona coverage with structured task schema**\n  - 315 tasks across banking, retail, and airline with five personas (Section 3.1; Table 6; Table 2; Appendix A.2–A.3), increasing coverage relative to τ-bench/τ²-bench (Table 6), which supports generality and impact.\n  - Persona definitions include characteristics and interaction styles (Table 2; Appendix A.3), and performance is analyzed by persona (Table 7), demonstrating attention to conversational diversity; this improves realism and clarity.\n  - Declarative JSON schema for goals and shifts (Section 3.4) and controlled user/agent tool boundaries (Section 3.4 “User/agent control”) strengthen reproducibility and technical soundness.- Bolded Title: **Comprehensive comparative experiments and granular reporting**\n  - Results span TSR and its channel components (Table 9), efficiency (TUE with TC/PA), redundancy (TCRR), and recovery (GSRT) across domains and sets (Table 10), showing thorough evaluation design and rigor.\n  - Domain-specific analysis identifies high redundancy in retail and banking versus lower redundancy in airline (Sections 4.4.2, 4.5; Tables 5, 10), motivating efficiency-focused improvements; this has practical impact.\n  - Persona performance and average turns are reported (Table 7), providing additional robustness diagnostics; this aids interpretability.- Bolded Title: **Reproducibility and system architecture integration**\n  - The benchmark builds on the τ²-bench harness with explicit constraints (Section 3.5) and integrates goal-shift management and detection with an LLM judge (Appendix G; Figure 3), ensuring modularity and reproducibility; this is technically sound.\n  - Release claims include benchmark and harness configurations (Introduction: Release), and evaluation protocol details (Appendix B.2) cover run counts, resets, and validation, supporting reusability and rigor.- Bolded Title: **Insightful findings about redundancy and parameter saturation**\n  - Parameter validity is near ceiling (mean P≈0.986; Section 4.3; Appendix D.5), and differences in TUE are driven by tool correctness with long tails (Appendix D.5; Figure 2), justifying TCRR tracking; this is technically motivated.\n  - Retail-new exhibits extreme redundancy (e.g., GPT-4o 89.14%; Gemini 66.45%) despite high PA (Table 5; Table 10), highlighting inefficiency masked by success-only metrics; this has operational impact.\n  - Recovery rates vary widely (e.g., airline-new: GPT-4o 92.2% vs Gemini 48.6%; Table 5; Figure 1), revealing resilience differences that pass@k obscures (Section 4.4.1), improving robustness analysis.Weaknesses\n- Bolded Title: **Dataset size inconsistency and unclear accounting of “task sequences”**\n  - Abstract claims 2,835 task sequences (Abstract), while the main text consistently reports 315 tasks (Sections 3.1–3.4; Table 6; Appendix A.1); this inconsistency undermines clarity and reproducibility.\n  - No direct breakdown is provided that maps 315 tasks to 2,835 sequences or runs; “Each task is evaluated across 3 independent runs” (Appendix B.2) still does not reconcile to 2,835; this impacts technical transparency.\n  - No direct evidence found in the manuscript quantifying per-domain sequence counts or persona-per-sequence distribution; this limits interpretability.- Bolded Title: **Terminology and metric-definition inconsistencies for GSRT**\n  - Figure 1 caption states “GSRT by model and domain” with “turns… lower is better” but labels show recovery rate percentages (Figure 1; Section 4.3), conflating “turns” and “rate,” which harms clarity.\n  - Section 4.3 defines GSRT as turns (ack/tool/outcome), yet recovery is treated as a separate rate metric; this dual usage is not cleanly separated in tables/figures (Tables 5, 10), impacting metric coherence.\n  - GSRT recovery success requires only acknowledgment and no human transfer (Section 4.3), while tool usage and outcome do not affect recovery classification, which may inflate “recovery” without goal achievement; this affects technical validity.- Bolded Title: **Weight choices lack validation and sensitivity analysis**\n  - TSR weights (0.25/0.45/0.30) are justified qualitatively (Section 4.3; Section 4.2; Appendix B.2 “TSR Component Weights”) without ablation or sensitivity study; this questions robustness of conclusions.\n  - TUE composite weights (0.6/0.4) are motivated by “operational cost analysis” (Section 4.3; Appendix B.2 “TUE Component Weights”) but no quantitative evidence or sensitivity analysis is reported; this impacts technical soundness.\n  - No direct evidence found in the manuscript for cross-domain calibration of weights, which may bias domain comparisons; this reduces experimental rigor.- Bolded Title: **Judge-model reliance and limited validation of communication scoring**\n  - Communication quality is scored via GPT-4o-mini (Section 4.2, Contribution 1), but the paper does not provide inter-rater reliability numbers or calibration against human judgments (Appendix B.2 mentions “statistical analysis” without metrics); this raises bias concerns.\n  - Potential family bias is not addressed when judging outputs from GPT-4o, Claude, and Gemini; no cross-judge or blind-judging protocol is described; this affects fairness.\n  - No direct evidence found in the manuscript of robustness checks (e.g., alternate judges, adjudication), limiting confidence in CI scores (Table 9).- Bolded Title: **Redundancy definition conflicts with harness constraints**\n  - TCRR penalizes exceeding a “batch threshold of 2 calls to the same function” (Section 4.3; Appendix B.2), yet the harness enforces “one-tool-per-turn” (Section 3.5), making intra-turn batch duplicates unlikely; this is contradictory.\n  - Appendix B.2 explicitly mentions “intra-turn batch inefficiencies,” but no protocol details reconcile this with one-tool-per-turn; this threatens metric validity.\n  - No direct evidence found in the manuscript clarifying whether “batch” refers to cross-turn batching or an exception in the harness; ambiguity affects interpretability of high TCRR in retail/banking (Tables 5, 10).- Bolded Title: **Incomplete and uneven evaluation coverage**\n  - GSRT is not reported for Airline-new Gemini due to “insufficient credits” (Appendix D.4; Table 10), reducing comparability across models.\n  - Banking lacks old/new splits (Tables 9–10 show “—”), while airline and retail have both; this unevenness complicates longitudinal or domain-consistent analysis.\n  - Open-source model evaluation (Qwen2.5-14B) uses single runs per domain (Appendix E; Table 11), diverging from the 3-run protocol (Appendix B.2); this limits generality and robustness.- Bolded Title: **Pass@k positioning without quantitative reporting**\n  - The paper asserts that pass@k drops to 0.0 on new sets (Section 4.4.1: “Key findings”) but provides no pass@k numbers or tables; this weakens the comparative claim.\n  - Table 3 contrasts τ²-bench vs AgentChangeBench but does not include actual pass@k results for the reported experiments; this reduces evidence strength.\n  - No direct evidence found in the manuscript showing correlation analyses between pass@k and TSR/GSRT/TCRR, limiting the argument that AgentChangeBench captures nuances pass@k misses.- Bolded Title: **Persona distribution may bias overall aggregates**\n  - Persona coverage is uneven (Table 8: MEDIUM_1 = 69 tasks vs 31–34 for others), justified narratively (Section 3.3), but aggregates (Tables 4–5, 9–10) do not report persona-weighted or balanced metrics; this can bias domain-level conclusions.\n  - While Table 7 reports per-persona averages, there is no correction when aggregating across domains or sets; this affects robustness of topline claims.\n  - No direct evidence found in the manuscript of stratified or reweighted analysis to control for persona skew.- Bolded Title: **Minor reporting and consistency issues across tables**\n  - Table 4 says values are averaged over old+new “where applicable,” but Retail value for Claude (79.57%) matches Retail-old in Table 9, and differs from Retail-new (61.58%), suggesting potential inconsistency in averaging; this affects clarity.\n  - Figure 1 annotates “Source: Appendix Tables 10, 5,” but Table 10’s GSRT entries mix rate and turns in the same column semantics (“Shifts/Rec/Trans”), which can confuse interpretation; this harms clarity.\n  - Some set labels are missing or inconsistent (Banking shows “—”; Tables 9–10), impeding clean comparisons.Suggestions for Improvement\n- Bolded Title: **Resolve dataset size accounting and document sequence/run mapping**\n  - Provide a clear reconciliation between 2,835 sequences (Abstract) and 315 tasks (Sections 3.1–3.4) by adding a table that maps tasks × runs × shifts × personas to the reported total; include per-domain counts (Appendix B.2 “Simulation Setup”).\n  - Explicitly state whether “task sequences” include multiple goal shifts per task and how those are counted; add an accounting formula in Section 3.1 or Appendix A.1.\n  - Include persona-by-sequence counts and domain breakdowns (augment Table 8) to improve reproducibility and interpretability.- Bolded Title: **Disambiguate GSRT “turns” vs “recovery rate” and tighten definitions**\n  - Split GSRT into “GSRT-turns” and “GSRT-recovery-rate” as distinct metrics and reflect that in figure/table captions (Figure 1; Section 4.3); add a small definitional box.\n  - Ensure all tables report the metric consistently (e.g., Table 5: “Recovery” as a rate; Table 10: separate columns for GSRT-turns vs recovery rate) to remove ambiguity.\n  - Consider counting recovery success only when both acknowledgment and achievement occur, or report a “partial recovery” label; add a sensitivity analysis showing impact on recovery rates.- Bolded Title: **Add quantitative validation for metric weights**\n  - Conduct and report sensitivity analyses for TSR and TUE weights (Equation (1) and (2); Section 4.3; Appendix B.2), showing stability of rankings across reasonable weight ranges.\n  - Provide domain-specific calibration rationale with quantitative backing (e.g., cost models, error impact) and supplement with ablations where a single channel weight is varied.\n  - Include cross-domain calibration checks to demonstrate that conclusions hold under different weighting schemes.- Bolded Title: **Strengthen judge-model validation and fairness**\n  - Report inter-rater reliability statistics (e.g., Cohen’s κ) for CI scoring, including a subset annotated by humans, and include numbers rather than narrative (Appendix B.2).\n  - Evaluate CI scoring with at least one alternate judge (e.g., another LLM family) and report consistency; include a blind adjudication protocol to reduce family bias (Section 4.2, Contribution 1).\n  - Add robustness checks (e.g., swapping judge model across runs) and report the effect on Table 9 CI scores to support fairness across evaluated agents.- Bolded Title: **Clarify TCRR vis-à-vis harness constraints and define “batch” precisely**\n  - Reconcile “one-tool-per-turn” (Section 3.5) with “batch threshold of 2 calls” (Section 4.3; Appendix B.2) by clarifying whether “batch” refers to consecutive turns or an allowed multi-call exception; update the metric definition accordingly.\n  - Provide pseudocode for TCRR computation, including window and batch components, and add per-domain examples illustrating how duplicates are counted (Section 4.3; Appendix D.4).\n  - Report TCRR split metrics separately (within-turn vs cross-turn) to identify where retail/banking redundancy arises (Tables 5, 10).- Bolded Title: **Complete and balance evaluation coverage**\n  - Fill missing GSRT entries (e.g., Airline-new Gemini; Table 10) by rerunning with sufficient credits or explicitly mark as “not evaluated” in all summaries; add a note in Section 4.4.1.\n  - Add banking old/new splits or explain why banking lacks them and adjust comparisons to avoid mixing apples/oranges (Tables 9–10; Section 4.4.2).\n  - For OSS models (Appendix E; Table 11), run the standard 3-run protocol and add adapters for Mistral/DeepSeek behaviors (Appendix E; Section 3.5) to ensure parity and generality.- Bolded Title: **Report pass@k quantitatively and relate to new metrics**\n  - Include pass@k results per domain/set/model in an additional table to substantiate claims about drops to 0.0 (Section 4.4.1; Table 3).\n  - Provide correlation analyses between pass@k and TSR/GSRT/TCRR to empirically support the argument that AgentChangeBench captures nuances beyond pass@k (Section 4.5).\n  - Add case studies where pass@k=0 but TSR>0, with transcript snippets and metric breakdowns (Appendix D), illustrating partial credit utility.- Bolded Title: **Control for persona skew in aggregates**\n  - Report persona-weighted aggregates or reweight to a uniform persona distribution; add balanced metrics alongside current aggregates (Tables 4–5, 9–10; Table 8).\n  - Provide stratified analyses (per persona per domain), and quantify how persona mix affects TSR/TUE/TCRR/GSRT to avoid biased conclusions (Table 7).\n  - Consider releasing a “balanced persona” subset and report results there to enable fair cross-benchmark comparisons.- Bolded Title: **Tighten table/caption consistency and set labeling**\n  - Verify averaging claims in Table 4 by adding explicit “Old/New/Both” columns or footnotes; ensure Retail values match the stated averaging procedure (Tables 4, 9).\n  - Standardize GSRT reporting across figures/tables (Figure 1; Table 10) with clear units and column semantics; add legend clarifying “Shifts/Recovery%/Transfer%.”\n  - Add consistent set labels for banking or note “no old/new split” in captions to improve readability (Tables 9–10).Score\n- Overall (10): 6 — Strong metric formalization and useful domain/persona coverage (Section 4.3; Tables 5, 9–10; Figure 1), but notable inconsistencies (Abstract vs Sections 3.1–3.4; GSRT definitions; TCRR vs harness) and incomplete reporting limit technical reliability.\n- Novelty (10): 7 — Explicit goal-shift evaluation with GSRT and redundancy metrics advances beyond pass@k-centered tools benchmarks (Table 3; Sections 3.4, 4.3, 4.5), though building on τ²-bench limits novelty scope.\n- Technical Quality (10): 6 — Formal metrics and comprehensive experiments (Equations (1)–(2); Tables 9–10) are commendable, but weight choices, judge validation, and metric/harness contradictions (Sections 3.5, 4.3; Appendix B.2) reduce rigor.\n- Clarity (10): 7 — Clear schema and architecture explanations (Sections 3.4–3.5; Appendix G; Figure 3) and extensive tabular reporting (Tables 4–10), tempered by GSRT terminology and dataset-size inconsistencies (Abstract vs Section 3.1).\n- Confidence (5): 4 — High confidence in identifying strengths and issues given many anchors (Sections 3–4; Tables/Figures; Appendix), but some missing quantitative details (pass@k numbers; judge reliability) prevent maximum confidence.",
  "final_review": "Summary\n- The paper introduces AgentChangeBench, a benchmark to evaluate tool-augmented LLM agents under mid-dialogue goal shifts across three enterprise domains (banking, retail, airline). The framework defines four metrics: Task Success Rate (TSR), Tool Usage Efficiency (TUE), Tool-Call Redundancy Ratio (TCRR), and Goal-Shift Recovery Turns/Rate (GSRT), with formal definitions for TSR and TUE (Section 4.3; Equations (1)–(2)). The dataset comprises 315 curated tasks with five personas and explicit goal sequences (Sections 3.1–3.4; Table 6; Table 2; Appendix A.1–A.3). Cross-model experiments reveal substantial differences in recovery under goal shifts and redundancy despite high parameter validity (Tables 4–5, 9–10; Figure 1). The authors release the benchmark and evaluation harness configurations (Introduction: Release; Appendix G).Strengths\n- Bolded Title: **Clear formalization of multi-dimensional metrics**\n  - TSR is precisely defined as a weighted sum across communication, action, and NL assertion channels with explicit weights (Section 4.3; Equation (1)), enhancing transparency over binary pass@k; this improves technical soundness and clarity.\n  - TUE is decomposed into Tool Correctness (T) and Parameter Validity (P) with a composite (Section 4.3; Equation (2)), enabling diagnosis beyond success/failure; this increases experimental rigor.\n  - TCRR and GSRT are formally specified, including window and batch definitions for redundancy and first-hitting times for recovery events (Section 4.3; Tool-Call Redundancy Ratio; Goal-Shift Recovery Turns), supporting nuanced analysis and impact.\n- Bolded Title: **Explicit focus on goal-shift robustness**\n  - The benchmark centers on mid-conversation goal changes, operationalized through pre-declared goal sequences (Section 3.4; “goal_shifts” schema) and LLM-judge-based detection (Appendix G; Figure 3), addressing a gap in prior tool-use benchmarks; this is novel and impactful.\n  - GSRT measures acknowledgment, first relevant tool call, and outcome turns (Section 4.3), and recovery rate is reported alongside TSR/TUE/TCRR (Tables 5, 10; Figure 1), enabling differentiation of agents that adapt quickly versus those that meander; this aids deployment decisions.\n  - Failure-mode analysis highlights late shift detection and over-confirmations linked to GSRT and communication channel drops (Section 4.3; Failure modes; Table 9), providing actionable diagnostic insights.\n- Bolded Title: **Broad domain and persona coverage with structured task schema**\n  - 315 tasks across banking, retail, and airline with five personas (Section 3.1; Table 6; Table 2; Appendix A.2–A.3), increasing coverage relative to τ-bench/τ²-bench (Table 6), which supports generality and impact.\n  - Persona definitions include characteristics and interaction styles (Table 2; Appendix A.3), and performance is analyzed by persona (Table 7), demonstrating attention to conversational diversity; this improves realism and clarity.\n  - Declarative JSON schema for goals and shifts (Section 3.4) and controlled user/agent tool boundaries (Section 3.4 “User/agent control”; Section 3.5) strengthen reproducibility and technical soundness.\n- Bolded Title: **Comprehensive comparative experiments and granular reporting**\n  - Results span TSR and its channel components (Table 9), efficiency (TUE with TC/PA), redundancy (TCRR), and recovery (GSRT) across domains and sets (Table 10), showing thorough evaluation design and rigor.\n  - Domain-specific analysis identifies high redundancy in retail and banking versus lower redundancy in airline (Sections 4.4.2, 4.5; Tables 5, 10), motivating efficiency-focused improvements; this has practical impact.\n  - Persona performance and average turns are reported (Table 7), providing additional robustness diagnostics; this aids interpretability.\n- Bolded Title: **Reproducibility and system architecture integration**\n  - The benchmark builds on the τ²-bench harness with explicit constraints (Section 3.5) and integrates goal-shift management and detection with an LLM judge (Appendix G; Figure 3), ensuring modularity and reproducibility; this is technically sound.\n  - Release claims include benchmark and harness configurations (Introduction: Release), and evaluation protocol details (Appendix B.2) cover run counts, resets, and validation, supporting reusability and rigor.\n- Bolded Title: **Insightful findings about redundancy and parameter saturation**\n  - Parameter validity is near ceiling (mean P≈0.986; Section 4.3; Appendix D.5; Figure 2), and differences in TUE are driven by tool correctness with long tails (Appendix D.5; Figure 2), justifying TCRR tracking; this is technically motivated.\n  - Retail-new exhibits extreme redundancy (e.g., GPT-4o 89.14%; Gemini 66.45%) despite high PA (Table 5; Table 10), highlighting inefficiency masked by success-only metrics; this has operational impact.\n  - Recovery rates vary widely (e.g., airline-new: GPT-4o 92.2% vs Gemini 48.6%; Table 5; Table 10; Figure 1), revealing resilience differences that pass@k obscures (Section 4.4.1), improving robustness analysis.Weaknesses\n- Bolded Title: **Dataset size inconsistency and unclear accounting of “task sequences”**\n  - Abstract claims 2,835 task sequences (Abstract), while the main text consistently reports 315 tasks (Sections 3.1–3.4; Table 6; Appendix A.1); this inconsistency undermines clarity and reproducibility. The sourcing numbers in Section 3.2 (reuse 50 airline + 114 retail + add 50 new + 50 banking) sum to 264, leaving 51 tasks unaccounted, further clouding provenance (Section 3.2).\n  - No direct breakdown is provided that maps 315 tasks to 2,835 sequences or runs; “Each task is evaluated across 3 independent runs” (Appendix B.2) still does not reconcile to 2,835; this impacts technical transparency.\n  - No direct evidence found in the manuscript quantifying per-domain sequence counts or persona-per-sequence distribution; this limits interpretability.\n- Bolded Title: **Terminology and metric-definition inconsistencies for GSRT**\n  - Figure 1 caption states “GSRT by model and domain” with “turns… lower is better” but labels show recovery rate percentages (Figure 1; Section 4.3), conflating “turns” and “rate,” which harms clarity.\n  - Section 4.3 defines GSRT as turns (ack/tool/outcome), yet recovery is treated as a separate rate metric; this dual usage is not cleanly separated in tables/figures (Tables 5, 10), impacting metric coherence.\n  - GSRT recovery success requires only acknowledgment and no human transfer (Section 4.3), while tool usage and outcome do not affect recovery classification, which may inflate “recovery” without goal achievement; this affects technical validity.\n- Bolded Title: **Weight choices lack validation and sensitivity analysis**\n  - TSR weights (0.25/0.45/0.30) are justified qualitatively (Section 4.3; Section 4.2; Appendix B.2 “TSR Component Weights”) without ablation or sensitivity study; this questions robustness of conclusions.\n  - TUE composite weights (0.6/0.4) are motivated by “operational cost analysis” (Section 4.3; Appendix B.2 “TUE Component Weights”) but no quantitative evidence or sensitivity analysis is reported; this impacts technical soundness.\n  - No direct evidence found in the manuscript for cross-domain calibration of weights, which may bias domain comparisons; this reduces experimental rigor.\n- Bolded Title: **Judge-model reliance and limited validation of communication scoring**\n  - Communication quality is scored via GPT-4o-mini (Section 4.2, Contribution 1), but the paper does not provide inter-rater reliability numbers or calibration against human judgments (Appendix B.2 mentions “statistical analysis” without metrics); this raises bias concerns.\n  - Potential family bias is not addressed when judging outputs from GPT-4o, Claude, and Gemini; no cross-judge or blind-judging protocol is described; this affects fairness.\n  - No direct evidence found in the manuscript of robustness checks (e.g., alternate judges, adjudication), limiting confidence in CI scores (Table 9).\n- Bolded Title: **Redundancy definition conflicts with harness constraints**\n  - TCRR penalizes exceeding a “batch threshold of 2 calls to the same function” (Section 4.3; Appendix B.2), yet the harness enforces “one-tool-per-turn” (Section 3.5), making intra-turn batch duplicates unlikely; this is contradictory.\n  - Appendix B.2 explicitly mentions “intra-turn batch inefficiencies,” but no protocol details reconcile this with one-tool-per-turn; this threatens metric validity.\n  - No direct evidence found in the manuscript clarifying whether “batch” refers to cross-turn batching or an exception in the harness; ambiguity affects interpretability of high TCRR in retail/banking (Tables 5, 10).\n- Bolded Title: **Incomplete and uneven evaluation coverage**\n  - GSRT is not reported for Airline-new Gemini due to “insufficient credits” (Appendix D.4; Table 10), reducing comparability across models.\n  - Banking lacks old/new splits (Tables 9–10 show “—”), while airline and retail have both; this unevenness complicates longitudinal or domain-consistent analysis.\n  - Text in Section 4.4.3 states “Retail domain comprises 165 tasks evaluated with GPT-4o,” yet Retail results are reported for Claude and Gemini elsewhere (Tables 4–5, 9–10), creating confusion about cross-model coverage.\n- Bolded Title: **Pass@k positioning without quantitative reporting**\n  - The paper asserts that pass@k drops to 0.0 on new sets (Section 4.4.1: “Key findings”) but provides no pass@k numbers or tables; this weakens the comparative claim.\n  - Table 3 contrasts τ²-bench vs AgentChangeBench but does not include actual pass@k results for the reported experiments; this reduces evidence strength.\n  - No direct evidence found in the manuscript showing correlation analyses between pass@k and TSR/GSRT/TCRR, limiting the argument that AgentChangeBench captures nuances pass@k misses.\n- Bolded Title: **Persona distribution may bias overall aggregates**\n  - Persona coverage is uneven (Table 8: MEDIUM_1 = 69 tasks vs 31–34 for others), justified narratively (Section 3.3), but aggregates (Tables 4–5, 9–10) do not report persona-weighted or balanced metrics; this can bias domain-level conclusions. Table 8’s counts also sum to 201 tasks (Appendix D.2), not 315, further complicating aggregate interpretation.\n  - While Table 7 reports per-persona averages, there is no correction when aggregating across domains or sets; this affects robustness of topline claims.\n  - No direct evidence found in the manuscript of stratified or reweighted analysis to control for persona skew.\n- Bolded Title: **Minor reporting and consistency issues across tables**\n  - Table 4 says values are averaged over old+new “where applicable,” but Retail value for Claude (79.57%) matches Retail-old in Table 9 and differs from Retail-new (61.58%), suggesting potential inconsistency in averaging; this affects clarity (Tables 4, 9).\n  - Figure 1 annotates “Source: Appendix Tables 10, 5,” but Table 10’s GSRT entries mix rate and turns in the same column semantics (“Shifts/Rec/Trans”), and GSRT values depicted in figures/tables for Airline/Gemini conflict (e.g., 22.3% in Figure 1 table vs 32.1% in the bar chart vs “—” in Table 10; Section Related Work; Figure “Goal-Shift Recovery Rate”), which can confuse interpretation; this harms clarity.\n  - Some set labels are missing or inconsistent (Banking shows “—”; Tables 9–10), and Section 4.4.1 states “parameter validity is 100% across runs” while Section 4.3 reports mean P=0.986 (with 98.6% ≥ 0.95) and Table 10 reports PA as 100.00%, suggesting rounding-versus-text discrepancies; this impedes clean comparisons.\n- Bolded Title: **Run-count inconsistency and unclear sampling**\n  - Appendix B.2 states “Each task is evaluated across 3 independent runs” (Appendix B.2), while Section 4.3 reports “14/15 runs contain at least one such low-T case,” implying a 15-run setup (Section 4.3), creating protocol ambiguity.\n  - Figures in Appendix D.5 reference “Run 1” through “Run 15” for TUE components (Figure 2; Appendix D.5; panels for Tool Correctness and Parameter Validity), which does not align with the 3-run protocol.\n  - No direct evidence found in the manuscript mapping which results use 3 vs 15 runs (e.g., whether plots are exploratory and tables are from the 3-run protocol), reducing confidence in sampling consistency.Suggestions for Improvement\n- Bolded Title: **Resolve dataset size accounting and document sequence/run mapping**\n  - Provide a clear reconciliation between 2,835 sequences (Abstract) and 315 tasks (Sections 3.1–3.4) by adding a table that maps tasks × runs × shifts × personas to the reported total; include per-domain counts (Appendix B.2 “Simulation Setup”). Clarify task provenance so the sourcing numbers in Section 3.2 sum to 315 (e.g., account for the additional 51 tasks).\n  - Explicitly state whether “task sequences” include multiple goal shifts per task and how those are counted; add an accounting formula in Section 3.1 or Appendix A.1.\n  - Include persona-by-sequence counts and domain breakdowns (augment Table 8) to improve reproducibility and interpretability.\n- Bolded Title: **Disambiguate GSRT “turns” vs “recovery rate” and tighten definitions**\n  - Split GSRT into “GSRT-turns” and “GSRT-recovery-rate” as distinct metrics and reflect that in figure/table captions (Figure 1; Section 4.3); add a small definitional box.\n  - Ensure all tables report the metric consistently (e.g., Table 5: “Recovery” as a rate; Table 10: separate columns for GSRT-turns vs recovery rate) to remove ambiguity.\n  - Consider counting recovery success only when both acknowledgment and achievement occur, or report a “partial recovery” label; add a sensitivity analysis showing impact on recovery rates.\n- Bolded Title: **Add quantitative validation for metric weights**\n  - Conduct and report sensitivity analyses for TSR and TUE weights (Equation (1) and (2); Section 4.3; Appendix B.2), showing stability of rankings across reasonable weight ranges.\n  - Provide domain-specific calibration rationale with quantitative backing (e.g., cost models, error impact) and supplement with ablations where a single channel weight is varied.\n  - Include cross-domain calibration checks to demonstrate that conclusions hold under different weighting schemes.\n- Bolded Title: **Strengthen judge-model validation and fairness**\n  - Report inter-rater reliability statistics (e.g., Cohen’s κ) for CI scoring, including a subset annotated by humans, and include numbers rather than narrative (Appendix B.2).\n  - Evaluate CI scoring with at least one alternate judge (e.g., another LLM family) and report consistency; include a blind adjudication protocol to reduce family bias (Section 4.2, Contribution 1).\n  - Add robustness checks (e.g., swapping judge model across runs) and report the effect on Table 9 CI scores to support fairness across evaluated agents.\n- Bolded Title: **Clarify TCRR vis-à-vis harness constraints and define “batch” precisely**\n  - Reconcile “one-tool-per-turn” (Section 3.5) with “batch threshold of 2 calls” (Section 4.3; Appendix B.2) by clarifying whether “batch” refers to consecutive turns or an allowed multi-call exception; update the metric definition accordingly.\n  - Provide pseudocode for TCRR computation, including window and batch components, and add per-domain examples illustrating how duplicates are counted (Section 4.3; Appendix D.4).\n  - Report TCRR split metrics separately (within-turn vs cross-turn) to identify where retail/banking redundancy arises (Tables 5, 10).\n- Bolded Title: **Complete and balance evaluation coverage**\n  - Fill missing GSRT entries (e.g., Airline-new Gemini; Table 10) by rerunning with sufficient credits or explicitly mark as “not evaluated” in all summaries; add a note in Section 4.4.1.\n  - Add banking old/new splits or explain why banking lacks them and adjust comparisons to avoid mixing apples/oranges (Tables 9–10; Section 4.4.2).\n  - Clarify cross-model coverage in Section 4.4.3 (Retail) to align with Tables 4–5, 9–10, and, for OSS models (Appendix E; Table 11), run the standard 3-run protocol and add adapters for Mistral/DeepSeek behaviors to ensure parity and generality.\n- Bolded Title: **Report pass@k quantitatively and relate to new metrics**\n  - Include pass@k results per domain/set/model in an additional table to substantiate claims about drops to 0.0 (Section 4.4.1; Table 3).\n  - Provide correlation analyses between pass@k and TSR/GSRT/TCRR to empirically support the argument that AgentChangeBench captures nuances beyond pass@k (Section 4.5).\n  - Add case studies where pass@k=0 but TSR>0, with transcript snippets and metric breakdowns (Appendix D), illustrating partial credit utility.\n- Bolded Title: **Control for persona skew in aggregates**\n  - Report persona-weighted aggregates or reweight to a uniform persona distribution; add balanced metrics alongside current aggregates (Tables 4–5, 9–10; Table 8), and reconcile Table 8’s total with the 315-task dataset.\n  - Provide stratified analyses (per persona per domain), and quantify how persona mix affects TSR/TUE/TCRR/GSRT to avoid biased conclusions (Table 7).\n  - Consider releasing a “balanced persona” subset and report results there to enable fair cross-benchmark comparisons.\n- Bolded Title: **Tighten table/caption consistency and set labeling**\n  - Verify averaging claims in Table 4 by adding explicit “Old/New/Both” columns or footnotes; ensure Retail values match the stated averaging procedure (Tables 4, 9).\n  - Standardize GSRT reporting across figures/tables (Figure 1; Table 10) with clear units and column semantics; add legend clarifying “Shifts/Recovery%/Transfer%,” and ensure plotted numbers (e.g., Airline/Gemini) match sources.\n  - Add consistent set labels for banking and harmonize parameter-validity descriptions (Section 4.3 vs 4.4.1 vs Table 10) to improve readability.\n- Bolded Title: **Clarify run counts and sampling used in plots vs tables**\n  - State explicitly whether 3 runs per task (Appendix B.2) or 15 runs (Section 4.3; Appendix D.5, Figure 2) were used for each reported analysis, and which results are exploratory vs core.\n  - If additional runs beyond the 3-run protocol were used for TUE component analysis, document their purpose, sampling, and how they relate to the main tables (Tables 9–10).\n  - Align figure/run labels with the protocol (e.g., replot with 3-run aggregates or provide a mapping table), ensuring consistent sampling across metrics.Score\n- Overall (10): 6 — Strong metric formalization and useful domain/persona coverage (Section 4.3; Tables 5, 9–10; Figure 1), but notable inconsistencies (Abstract vs Sections 3.1–3.4; GSRT definitions; TCRR vs harness; run-count ambiguity) and incomplete reporting limit technical reliability.\n- Novelty (10): 7 — Explicit goal-shift evaluation with GSRT and redundancy metrics advances beyond pass@k-centered tools benchmarks (Table 3; Sections 3.4, 4.3, 4.5), though building on τ²-bench limits novelty scope.\n- Technical Quality (10): 5 — Formal metrics and comparative experiments (Equations (1)–(2); Tables 9–10) are commendable, but weight choices, judge validation, metric/harness contradictions, and run-count inconsistencies (Sections 3.5, 4.3; Appendix B.2; Appendix D.5) reduce rigor.\n- Clarity (10): 6 — Clear schema and architecture explanations (Sections 3.4–3.5; Appendix G; Figure 3) and extensive tabular reporting (Tables 4–10), tempered by GSRT terminology and dataset-size inconsistencies (Abstract vs Section 3.1) and figure/table mismatches (Figure 1; Tables 5, 10).\n- Confidence (5): 4 — High confidence in identifying strengths and issues given many anchors (Sections 3–4; Tables/Figures; Appendix), but some missing quantitative details (pass@k numbers; judge reliability; run-count reconciliation) prevent maximum confidence.",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 6,
        "novelty": 7,
        "technical_quality": 6,
        "clarity": 7,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 6,
        "novelty": 7,
        "technical_quality": 5,
        "clarity": 6,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "Summary\n- The paper introduces AgentChangeBench, a benchmark to evaluate tool-augmented LLM agents under mid-dialogue goal shifts across three enterprise domains (banking, retail, airline). The framework defines four metrics: Task Success Rate (TSR), Tool Usage Efficiency (TUE), Tool-Call Redundancy Ratio (TCRR), and Goal-Shift Recovery Turns/Rate (GSRT), with formal definitions for TSR and TUE (Section 4.3; Equations (1)–(2)). The dataset comprises 315 curated tasks with five personas and explicit goal sequences (Sections 3.1–3.4; Table 6; Table 2; Appendix A.1–A.3). Cross-model experiments reveal substantial differences in recovery under goal shifts and redundancy despite high parameter validity (Tables 4–5, 9–10; Figure 1). The authors release the benchmark and evaluation harness configurations (Introduction: Release; Appendix G).Strengths\n- Bolded Title: **Clear formalization of multi-dimensional metrics**\n  - TSR is precisely defined as a weighted sum across communication, action, and NL assertion channels with explicit weights (Section 4.3; Equation (1)), enhancing transparency over binary pass@k; this improves technical soundness and clarity.\n  - TUE is decomposed into Tool Correctness (T) and Parameter Validity (P) with a composite (Section 4.3; Equation (2)), enabling diagnosis beyond success/failure; this increases experimental rigor.\n  - TCRR and GSRT are formally specified, including window and batch definitions for redundancy and first-hitting times for recovery events (Section 4.3; Tool-Call Redundancy Ratio; Goal-Shift Recovery Turns), supporting nuanced analysis and impact.\n- Bolded Title: **Explicit focus on goal-shift robustness**\n  - The benchmark centers on mid-conversation goal changes, operationalized through pre-declared goal sequences (Section 3.4; “goal_shifts” schema) and LLM-judge-based detection (Appendix G; Figure 3), addressing a gap in prior tool-use benchmarks; this is novel and impactful.\n  - GSRT measures acknowledgment, first relevant tool call, and outcome turns (Section 4.3), and recovery rate is reported alongside TSR/TUE/TCRR (Tables 5, 10; Figure 1), enabling differentiation of agents that adapt quickly versus those that meander; this aids deployment decisions.\n  - Failure-mode analysis highlights late shift detection and over-confirmations linked to GSRT and communication channel drops (Section 4.3; Failure modes; Table 9), providing actionable diagnostic insights.\n- Bolded Title: **Broad domain and persona coverage with structured task schema**\n  - 315 tasks across banking, retail, and airline with five personas (Section 3.1; Table 6; Table 2; Appendix A.2–A.3), increasing coverage relative to τ-bench/τ²-bench (Table 6), which supports generality and impact.\n  - Persona definitions include characteristics and interaction styles (Table 2; Appendix A.3), and performance is analyzed by persona (Table 7), demonstrating attention to conversational diversity; this improves realism and clarity.\n  - Declarative JSON schema for goals and shifts (Section 3.4) and controlled user/agent tool boundaries (Section 3.4 “User/agent control”; Section 3.5) strengthen reproducibility and technical soundness.\n- Bolded Title: **Comprehensive comparative experiments and granular reporting**\n  - Results span TSR and its channel components (Table 9), efficiency (TUE with TC/PA), redundancy (TCRR), and recovery (GSRT) across domains and sets (Table 10), showing thorough evaluation design and rigor.\n  - Domain-specific analysis identifies high redundancy in retail and banking versus lower redundancy in airline (Sections 4.4.2, 4.5; Tables 5, 10), motivating efficiency-focused improvements; this has practical impact.\n  - Persona performance and average turns are reported (Table 7), providing additional robustness diagnostics; this aids interpretability.\n- Bolded Title: **Reproducibility and system architecture integration**\n  - The benchmark builds on the τ²-bench harness with explicit constraints (Section 3.5) and integrates goal-shift management and detection with an LLM judge (Appendix G; Figure 3), ensuring modularity and reproducibility; this is technically sound.\n  - Release claims include benchmark and harness configurations (Introduction: Release), and evaluation protocol details (Appendix B.2) cover run counts, resets, and validation, supporting reusability and rigor.\n- Bolded Title: **Insightful findings about redundancy and parameter saturation**\n  - Parameter validity is near ceiling (mean P≈0.986; Section 4.3; Appendix D.5; Figure 2), and differences in TUE are driven by tool correctness with long tails (Appendix D.5; Figure 2), justifying TCRR tracking; this is technically motivated.\n  - Retail-new exhibits extreme redundancy (e.g., GPT-4o 89.14%; Gemini 66.45%) despite high PA (Table 5; Table 10), highlighting inefficiency masked by success-only metrics; this has operational impact.\n  - Recovery rates vary widely (e.g., airline-new: GPT-4o 92.2% vs Gemini 48.6%; Table 5; Table 10; Figure 1), revealing resilience differences that pass@k obscures (Section 4.4.1), improving robustness analysis.Weaknesses\n- Bolded Title: **Dataset size inconsistency and unclear accounting of “task sequences”**\n  - Abstract claims 2,835 task sequences (Abstract), while the main text consistently reports 315 tasks (Sections 3.1–3.4; Table 6; Appendix A.1); this inconsistency undermines clarity and reproducibility. The sourcing numbers in Section 3.2 (reuse 50 airline + 114 retail + add 50 new + 50 banking) sum to 264, leaving 51 tasks unaccounted, further clouding provenance (Section 3.2).\n  - No direct breakdown is provided that maps 315 tasks to 2,835 sequences or runs; “Each task is evaluated across 3 independent runs” (Appendix B.2) still does not reconcile to 2,835; this impacts technical transparency.\n  - No direct evidence found in the manuscript quantifying per-domain sequence counts or persona-per-sequence distribution; this limits interpretability.\n- Bolded Title: **Terminology and metric-definition inconsistencies for GSRT**\n  - Figure 1 caption states “GSRT by model and domain” with “turns… lower is better” but labels show recovery rate percentages (Figure 1; Section 4.3), conflating “turns” and “rate,” which harms clarity.\n  - Section 4.3 defines GSRT as turns (ack/tool/outcome), yet recovery is treated as a separate rate metric; this dual usage is not cleanly separated in tables/figures (Tables 5, 10), impacting metric coherence.\n  - GSRT recovery success requires only acknowledgment and no human transfer (Section 4.3), while tool usage and outcome do not affect recovery classification, which may inflate “recovery” without goal achievement; this affects technical validity.\n- Bolded Title: **Weight choices lack validation and sensitivity analysis**\n  - TSR weights (0.25/0.45/0.30) are justified qualitatively (Section 4.3; Section 4.2; Appendix B.2 “TSR Component Weights”) without ablation or sensitivity study; this questions robustness of conclusions.\n  - TUE composite weights (0.6/0.4) are motivated by “operational cost analysis” (Section 4.3; Appendix B.2 “TUE Component Weights”) but no quantitative evidence or sensitivity analysis is reported; this impacts technical soundness.\n  - No direct evidence found in the manuscript for cross-domain calibration of weights, which may bias domain comparisons; this reduces experimental rigor.\n- Bolded Title: **Judge-model reliance and limited validation of communication scoring**\n  - Communication quality is scored via GPT-4o-mini (Section 4.2, Contribution 1), but the paper does not provide inter-rater reliability numbers or calibration against human judgments (Appendix B.2 mentions “statistical analysis” without metrics); this raises bias concerns.\n  - Potential family bias is not addressed when judging outputs from GPT-4o, Claude, and Gemini; no cross-judge or blind-judging protocol is described; this affects fairness.\n  - No direct evidence found in the manuscript of robustness checks (e.g., alternate judges, adjudication), limiting confidence in CI scores (Table 9).\n- Bolded Title: **Redundancy definition conflicts with harness constraints**\n  - TCRR penalizes exceeding a “batch threshold of 2 calls to the same function” (Section 4.3; Appendix B.2), yet the harness enforces “one-tool-per-turn” (Section 3.5), making intra-turn batch duplicates unlikely; this is contradictory.\n  - Appendix B.2 explicitly mentions “intra-turn batch inefficiencies,” but no protocol details reconcile this with one-tool-per-turn; this threatens metric validity.\n  - No direct evidence found in the manuscript clarifying whether “batch” refers to cross-turn batching or an exception in the harness; ambiguity affects interpretability of high TCRR in retail/banking (Tables 5, 10).\n- Bolded Title: **Incomplete and uneven evaluation coverage**\n  - GSRT is not reported for Airline-new Gemini due to “insufficient credits” (Appendix D.4; Table 10), reducing comparability across models.\n  - Banking lacks old/new splits (Tables 9–10 show “—”), while airline and retail have both; this unevenness complicates longitudinal or domain-consistent analysis.\n  - Text in Section 4.4.3 states “Retail domain comprises 165 tasks evaluated with GPT-4o,” yet Retail results are reported for Claude and Gemini elsewhere (Tables 4–5, 9–10), creating confusion about cross-model coverage.\n- Bolded Title: **Pass@k positioning without quantitative reporting**\n  - The paper asserts that pass@k drops to 0.0 on new sets (Section 4.4.1: “Key findings”) but provides no pass@k numbers or tables; this weakens the comparative claim.\n  - Table 3 contrasts τ²-bench vs AgentChangeBench but does not include actual pass@k results for the reported experiments; this reduces evidence strength.\n  - No direct evidence found in the manuscript showing correlation analyses between pass@k and TSR/GSRT/TCRR, limiting the argument that AgentChangeBench captures nuances pass@k misses.\n- Bolded Title: **Persona distribution may bias overall aggregates**\n  - Persona coverage is uneven (Table 8: MEDIUM_1 = 69 tasks vs 31–34 for others), justified narratively (Section 3.3), but aggregates (Tables 4–5, 9–10) do not report persona-weighted or balanced metrics; this can bias domain-level conclusions. Table 8’s counts also sum to 201 tasks (Appendix D.2), not 315, further complicating aggregate interpretation.\n  - While Table 7 reports per-persona averages, there is no correction when aggregating across domains or sets; this affects robustness of topline claims.\n  - No direct evidence found in the manuscript of stratified or reweighted analysis to control for persona skew.\n- Bolded Title: **Minor reporting and consistency issues across tables**\n  - Table 4 says values are averaged over old+new “where applicable,” but Retail value for Claude (79.57%) matches Retail-old in Table 9 and differs from Retail-new (61.58%), suggesting potential inconsistency in averaging; this affects clarity (Tables 4, 9).\n  - Figure 1 annotates “Source: Appendix Tables 10, 5,” but Table 10’s GSRT entries mix rate and turns in the same column semantics (“Shifts/Rec/Trans”), and GSRT values depicted in figures/tables for Airline/Gemini conflict (e.g., 22.3% in Figure 1 table vs 32.1% in the bar chart vs “—” in Table 10; Section Related Work; Figure “Goal-Shift Recovery Rate”), which can confuse interpretation; this harms clarity.\n  - Some set labels are missing or inconsistent (Banking shows “—”; Tables 9–10), and Section 4.4.1 states “parameter validity is 100% across runs” while Section 4.3 reports mean P=0.986 (with 98.6% ≥ 0.95) and Table 10 reports PA as 100.00%, suggesting rounding-versus-text discrepancies; this impedes clean comparisons.\n- Bolded Title: **Run-count inconsistency and unclear sampling**\n  - Appendix B.2 states “Each task is evaluated across 3 independent runs” (Appendix B.2), while Section 4.3 reports “14/15 runs contain at least one such low-T case,” implying a 15-run setup (Section 4.3), creating protocol ambiguity.\n  - Figures in Appendix D.5 reference “Run 1” through “Run 15” for TUE components (Figure 2; Appendix D.5; panels for Tool Correctness and Parameter Validity), which does not align with the 3-run protocol.\n  - No direct evidence found in the manuscript mapping which results use 3 vs 15 runs (e.g., whether plots are exploratory and tables are from the 3-run protocol), reducing confidence in sampling consistency.Suggestions for Improvement\n- Bolded Title: **Resolve dataset size accounting and document sequence/run mapping**\n  - Provide a clear reconciliation between 2,835 sequences (Abstract) and 315 tasks (Sections 3.1–3.4) by adding a table that maps tasks × runs × shifts × personas to the reported total; include per-domain counts (Appendix B.2 “Simulation Setup”). Clarify task provenance so the sourcing numbers in Section 3.2 sum to 315 (e.g., account for the additional 51 tasks).\n  - Explicitly state whether “task sequences” include multiple goal shifts per task and how those are counted; add an accounting formula in Section 3.1 or Appendix A.1.\n  - Include persona-by-sequence counts and domain breakdowns (augment Table 8) to improve reproducibility and interpretability.\n- Bolded Title: **Disambiguate GSRT “turns” vs “recovery rate” and tighten definitions**\n  - Split GSRT into “GSRT-turns” and “GSRT-recovery-rate” as distinct metrics and reflect that in figure/table captions (Figure 1; Section 4.3); add a small definitional box.\n  - Ensure all tables report the metric consistently (e.g., Table 5: “Recovery” as a rate; Table 10: separate columns for GSRT-turns vs recovery rate) to remove ambiguity.\n  - Consider counting recovery success only when both acknowledgment and achievement occur, or report a “partial recovery” label; add a sensitivity analysis showing impact on recovery rates.\n- Bolded Title: **Add quantitative validation for metric weights**\n  - Conduct and report sensitivity analyses for TSR and TUE weights (Equation (1) and (2); Section 4.3; Appendix B.2), showing stability of rankings across reasonable weight ranges.\n  - Provide domain-specific calibration rationale with quantitative backing (e.g., cost models, error impact) and supplement with ablations where a single channel weight is varied.\n  - Include cross-domain calibration checks to demonstrate that conclusions hold under different weighting schemes.\n- Bolded Title: **Strengthen judge-model validation and fairness**\n  - Report inter-rater reliability statistics (e.g., Cohen’s κ) for CI scoring, including a subset annotated by humans, and include numbers rather than narrative (Appendix B.2).\n  - Evaluate CI scoring with at least one alternate judge (e.g., another LLM family) and report consistency; include a blind adjudication protocol to reduce family bias (Section 4.2, Contribution 1).\n  - Add robustness checks (e.g., swapping judge model across runs) and report the effect on Table 9 CI scores to support fairness across evaluated agents.\n- Bolded Title: **Clarify TCRR vis-à-vis harness constraints and define “batch” precisely**\n  - Reconcile “one-tool-per-turn” (Section 3.5) with “batch threshold of 2 calls” (Section 4.3; Appendix B.2) by clarifying whether “batch” refers to consecutive turns or an allowed multi-call exception; update the metric definition accordingly.\n  - Provide pseudocode for TCRR computation, including window and batch components, and add per-domain examples illustrating how duplicates are counted (Section 4.3; Appendix D.4).\n  - Report TCRR split metrics separately (within-turn vs cross-turn) to identify where retail/banking redundancy arises (Tables 5, 10).\n- Bolded Title: **Complete and balance evaluation coverage**\n  - Fill missing GSRT entries (e.g., Airline-new Gemini; Table 10) by rerunning with sufficient credits or explicitly mark as “not evaluated” in all summaries; add a note in Section 4.4.1.\n  - Add banking old/new splits or explain why banking lacks them and adjust comparisons to avoid mixing apples/oranges (Tables 9–10; Section 4.4.2).\n  - Clarify cross-model coverage in Section 4.4.3 (Retail) to align with Tables 4–5, 9–10, and, for OSS models (Appendix E; Table 11), run the standard 3-run protocol and add adapters for Mistral/DeepSeek behaviors to ensure parity and generality.\n- Bolded Title: **Report pass@k quantitatively and relate to new metrics**\n  - Include pass@k results per domain/set/model in an additional table to substantiate claims about drops to 0.0 (Section 4.4.1; Table 3).\n  - Provide correlation analyses between pass@k and TSR/GSRT/TCRR to empirically support the argument that AgentChangeBench captures nuances beyond pass@k (Section 4.5).\n  - Add case studies where pass@k=0 but TSR>0, with transcript snippets and metric breakdowns (Appendix D), illustrating partial credit utility.\n- Bolded Title: **Control for persona skew in aggregates**\n  - Report persona-weighted aggregates or reweight to a uniform persona distribution; add balanced metrics alongside current aggregates (Tables 4–5, 9–10; Table 8), and reconcile Table 8’s total with the 315-task dataset.\n  - Provide stratified analyses (per persona per domain), and quantify how persona mix affects TSR/TUE/TCRR/GSRT to avoid biased conclusions (Table 7).\n  - Consider releasing a “balanced persona” subset and report results there to enable fair cross-benchmark comparisons.\n- Bolded Title: **Tighten table/caption consistency and set labeling**\n  - Verify averaging claims in Table 4 by adding explicit “Old/New/Both” columns or footnotes; ensure Retail values match the stated averaging procedure (Tables 4, 9).\n  - Standardize GSRT reporting across figures/tables (Figure 1; Table 10) with clear units and column semantics; add legend clarifying “Shifts/Recovery%/Transfer%,” and ensure plotted numbers (e.g., Airline/Gemini) match sources.\n  - Add consistent set labels for banking and harmonize parameter-validity descriptions (Section 4.3 vs 4.4.1 vs Table 10) to improve readability.\n- Bolded Title: **Clarify run counts and sampling used in plots vs tables**\n  - State explicitly whether 3 runs per task (Appendix B.2) or 15 runs (Section 4.3; Appendix D.5, Figure 2) were used for each reported analysis, and which results are exploratory vs core.\n  - If additional runs beyond the 3-run protocol were used for TUE component analysis, document their purpose, sampling, and how they relate to the main tables (Tables 9–10).\n  - Align figure/run labels with the protocol (e.g., replot with 3-run aggregates or provide a mapping table), ensuring consistent sampling across metrics.Score\n- Overall (10): 6 — Strong metric formalization and useful domain/persona coverage (Section 4.3; Tables 5, 9–10; Figure 1), but notable inconsistencies (Abstract vs Sections 3.1–3.4; GSRT definitions; TCRR vs harness; run-count ambiguity) and incomplete reporting limit technical reliability.\n- Novelty (10): 7 — Explicit goal-shift evaluation with GSRT and redundancy metrics advances beyond pass@k-centered tools benchmarks (Table 3; Sections 3.4, 4.3, 4.5), though building on τ²-bench limits novelty scope.\n- Technical Quality (10): 5 — Formal metrics and comparative experiments (Equations (1)–(2); Tables 9–10) are commendable, but weight choices, judge validation, metric/harness contradictions, and run-count inconsistencies (Sections 3.5, 4.3; Appendix B.2; Appendix D.5) reduce rigor.\n- Clarity (10): 6 — Clear schema and architecture explanations (Sections 3.4–3.5; Appendix G; Figure 3) and extensive tabular reporting (Tables 4–10), tempered by GSRT terminology and dataset-size inconsistencies (Abstract vs Section 3.1) and figure/table mismatches (Figure 1; Tables 5, 10).\n- Confidence (5): 4 — High confidence in identifying strengths and issues given many anchors (Sections 3–4; Tables/Figures; Appendix), but some missing quantitative details (pass@k numbers; judge reliability; run-count reconciliation) prevent maximum confidence."
}