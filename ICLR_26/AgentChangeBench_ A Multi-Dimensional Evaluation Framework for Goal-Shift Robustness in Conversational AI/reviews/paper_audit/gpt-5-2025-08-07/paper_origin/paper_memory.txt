# Global Summary
- Problem: Existing agent benchmarks assume static user goals and rely on binary success (pass@k), overlooking how agents adapt when objectives change mid-conversation. The paper introduces AgentChangeBench to evaluate tool-augmented LLM agents under explicit, mid-dialogue goal shifts.
- Core approach: A benchmark with 315 curated tasks across three enterprise domains (banking: 50, airline: 100, retail: 165), five user personas, and explicit goal-sequence annotations. The framework defines four metrics: TSR (Task Success Rate; weighted multi-channel), TUE (Tool Usage Efficiency; correctness and parameter validity), TCRR (Tool-Call Redundancy Ratio), and GSRT (Goal-Shift Recovery Turns/rate).
- Evaluation scope: Three proprietary model families (GPT-4o, Claude-3.7-Sonnet, Gemini-2.5-Flash) across old and new (goal-shifted) task sets, plus an open-source baseline (Qwen2.5-14B-Instruct, single-run per domain). 2,835 task sequences are reported in the abstract; main text analyzes 315 tasks (banking 50, airline 100, retail 165) with three runs per task (Appendix).
- Key findings:
  - Airline new tasks: GPT-4o achieves "Recovery" 92.2% with TCRR 13.54%; Claude-3.7-Sonnet Recovery 79.2% (TCRR 24.11%); Gemini-2.5-Flash Recovery 48.6% (TCRR 14.46%).
  - Retail new tasks: Claude-3.7-Sonnet TSR 79.57% with Recovery 89.5%; GPT-4o TSR 50.68% with Recovery 88.0% and extreme redundancy TCRR 89.14%; Gemini-2.5-Flash TSR 51.26% with Recovery 53.5% and TCRR 66.45%.
  - Overall TSR (averaged old+new where applicable): Banking—Claude 57.54%, GPT-4o 51.25%, Gemini 47.36%; Airline—Claude 65.14%, GPT-4o 62.19%, Gemini 46.98%; Retail—Claude 79.57%, Gemini 58.03%, GPT-4o 56.48%.
  - Parameter validity saturates: mean P=0.986, 98.6% of traces ≥0.95; redundancy is domain-skewed (Retail new up to 89.14%).
  - GSRT rate contrasts: aggregated recovery rates (figure) show GPT-4o 91.9%/79.3%/90.0% (Airline/Banking/Retail), Gemini 22.3%/57.7%/60.6%, Claude 85.0%/58.5%/90.7%.
- Caveats explicitly stated: Personas are relatively benign; domains limited to banking/retail/airline; goal shifts are pre-declared and often explicit; GSRT detection of implicit or concurrent goal drift not yet evaluated; limited model breadth; open-source integration requires adapters. Some GSRT results missing (Airline-new for Gemini) due to insufficient credits.

# Abstract
- Benchmark: AgentChangeBench measures agent adaptation to mid-dialogue goal shifts in three enterprise domains.
- Metrics: TSR (effectiveness), TUE (reliability), TCRR (wasted effort), GSRT (adaptation latency).
- Scale: 2,835 task sequences; five user personas; realistic shift points.
- Highlighted contrasts:
  - Airline booking shifts: GPT-4o "reaches 92.2% recovery" vs Gemini "48.6%".
  - Retail tasks: near-perfect parameter validity with redundancy rates "above 80%".
- Claims: High raw accuracy does not imply robustness; measuring recovery time and redundancy is essential; AgentChangeBench offers a reproducible testbed for agent resilience.

# Introduction
- Problem framing: Real-world multi-turn interactions include frequent goal changes (e.g., authentication → transactions → dispute in banking), absent from most benchmarks.
- Contribution summary:
  1) First benchmark explicitly testing mid-conversation goal shifts and persona-adaptive communication.
  2) Coverage: 315 validated tasks across banking, retail, airline; five personas; explicit shifts.
  3) Methodology: Evaluation protocols for goal-shift recovery tailored to realistic personas.
  4) Empirical study: Cross-model evaluation revealing divergences in success, recovery time, efficiency, redundancy that pass^k misses.
- Release: Full benchmark, evaluation harness configurations, and experimental artifacts are released for reproducibility.
- Not specified in this section: Exact number of runs per task (given in Appendix).

# Experiments
- Metrics comparison (Table 3):
  - AgentChangeBench vs τ²-bench:
    - TSR (weighted multi-channel) replaces pass@k-only task completion.
    - Efficiency: TUE (tool correctness and parameter validity) vs not measured explicitly.
    - Redundancy: TCRR (duplicates within 3-turn window or batch >2 per function).
    - Goal shifts: GSRT (acknowledgment, tool usage, outcome timing).
    - Robustness: Retention/drop in TSR/TUE across clean vs shifted conditions.
- TSR definition and weights: TSR = 0.25 communicate_info_rate + 0.45 action_rate + 0.30 nl_assertion_rate. Action is overweighted (0.45), NL assertions 0.30, communicate_info 0.25.
- TUE: TUE = 0.6 Tool correctness (T) + 0.4 Parameter validity (P).
  - P near-saturated: mean P=0.986; 98.6% of traces ≥0.95.
  - T has long tail: 4.3% of traces <0.70; 14/15 runs contain at least one low-T case.
- TCRR: Fraction of exact duplicate tool calls within a 3-turn window or exceeding batch threshold of 2 to the same function.
- GSRT: Defined via first-hitting times after shift at τ_s:
  - ack_s: first explicit acknowledgment of new goal.
  - tool_s: first relevant tool call.
  - outcome_s: first marked goal achievement.
  - Recovery counted if acknowledgment occurs and no transfer-to-human; missing events set to ∞. Example: (ack, tool, outcome) = (2, 3, 5).
- Failure modes highlighted:
  - Late shift detection: Airline-new—Gemini recovers 48.6% vs GPT-4o 92.2%.
  - Redundant tool calls: Retail-new TCRR—GPT-4o 89.1%, Gemini 66.5%.
  - Over-confirmations: Retail-new—GPT-4o communication channel 11.44%.
- Overall TSR (Table 4; averaged old+new where applicable):
  - Banking: GPT-4o 51.25%, Claude 57.54%, Gemini 47.36%.
  - Airline: GPT-4o 62.19%, Claude 65.14%, Gemini 46.98%.
  - Retail: GPT-4o 56.48%, Claude 79.57%, Gemini 58.03%.
- Goal-shift sensitivity on new tasks (Table 5):
  - Airline: GPT-4o TSR 59.53%, Recovery 92.2%, TCRR 13.54%; Claude TSR 69.90%, Recovery 79.2%, TCRR 24.11%; Gemini TSR 39.97%, Recovery 48.6%, TCRR 14.46%.
  - Retail: GPT-4o TSR 50.68%, Recovery 88.0%, TCRR 89.14%; Claude TSR 79.57%, Recovery 89.5%, TCRR 65.38%; Gemini TSR 51.26%, Recovery 53.5%, TCRR 66.45%.
- Extended robustness metrics: Per-action CTCS and PAS; retention/drop of TSR/TUE across clean vs shifted; macro-averages across tasks/personas.
- Results across model families (315 tasks: banking 50, airline 100, retail 165):
  - New goal-shifted tasks are harder: pass@k often drops to 0.0 on new sets; TSR remains 40–60%.
  - Recovery spans 48.6%–92.2% (airline new), explaining gaps among similar TSRs.
  - Redundancy domain-skew: airline new low (13.5% GPT-4o; 17.6% Gemini) vs retail new very high (66–89%).
  - Parameter accuracy at ceiling; TUE differences largely reflect tool correctness (mean P=0.986; 98.6% ≥0.95).
- Domain-specific performance:
  - Airline: Claude/GPT-4o lead new TSR 69.9%/59.5%; recovery 79.2–92.2%; lowest redundancy TCRR 13.5–24.1%.
  - Retail: Claude strongest (79.6% TSR new; recovery 89.5%); GPT-4o/Gemini mid-50s TSR new; redundancy high (TCRR 89.1%/66.5%).
  - Banking: Hardest (TSR 26.9–57.5%); recovery moderate (e.g., Claude 58.5%, GPT-4o 79.3%); redundancy high (TCRR 61.5% GPT-4o, 71.8% Claude).
- Dataset comparison (Table 6):
  - τ-bench: 234 tasks, 2 domains, 3 personas, no goal shifts, pass@k only.
  - τ²-bench: 105 tasks, 1 domain, 5 personas, implicit shifts, pass@k + modes.
  - AgentChangeBench: 315 tasks, 3 domains, 5 personas, explicit shifts, multi-dimensional metrics.
- Persona-based performance (Table 7):
  - TSR: EASY_1 0.533; EASY_2 0.475; MEDIUM_1 0.554; MEDIUM_2 0.580; HARD_1 0.430.
  - TUE: EASY_1 0.960; EASY_2 0.971; MEDIUM_1 0.978; MEDIUM_2 0.990; HARD_1 0.946.
  - GSRT Recovery rate: EASY_1 0.792; EASY_2 0.907; MEDIUM_1 0.916; MEDIUM_2 0.922; HARD_1 0.793.
  - Average turns: 19.2 (HARD_1) to 28.7 (EASY_1).
- Topline channel breakdown (Table 9 examples):
  - Retail new—GPT-4o: TSR 50.68%; CI 11.44%; Actions 63.00%; NL 64.92%.
  - Airline new—Claude: TSR 69.90%; CI 61.56%; Actions 66.92%; NL 81.33%.
- Efficiency and recovery (Table 10 examples; TUE reported as overall (TC/PA); GSRT as Shifts/Recovery%/Transfer%):
  - Airline new—GPT-4o: TUE 99.69 (99.48/100.00); TCRR 13.54 (11.07/2.48); Redun./Calls 339/2503; GSRT 90/92.2/7.8.
  - Banking—GPT-4o: TUE 95.38 (92.31/100.00); TCRR 61.54 (34.71/26.83); Redun./Calls 328/533; GSRT 140/79.3/20.7.
  - Retail new—Claude: TUE 97.74 (96.24/100.00); TCRR 75.85 (44.08/31.77); Redun./Calls 807/1064; GSRT 134/91.8/6.7.
  - Airline new—Gemini: GSRT not reported ("—") due to insufficient credits.
- Additional insights:
  - Agents with similar pass@k may have different TUE/TCRR; GSRT reveals adaptation latency differences.
  - Organizations can prioritize metrics based on domain needs (e.g., cost vs user experience).

# Related Work
- τ-bench: Simulated multi-turn interactions, tool usage, pass^k metric; assumes static user goals and full agent control; limited dynamics.
- τ²-bench: Telecom support scenarios with compositional task generation; several personas; mostly static goals; no explicit adaptability testing to changing goals or runtime constraints.
- AgentBench: Evaluates agents across eight interactive environments (OS, databases, web); stable objectives; does not address explicit goal shifts or persona variability.
- Contrast (Table 1): This work employs explicit goal sequences, five personas, domain APIs, and multi-dimensional metrics TSR/TUE/TCRR/GSRT.
- Figure (Goal-Shift Recovery Rate, aggregated): GPT-4o 91.9% (Airline), 79.3% (Banking), 90.0% (Retail); Gemini 32.1% (Airline old per figure), 57.7% (Banking), 60.6% (Retail); Claude 85.0% (Airline), 58.5% (Banking), 90.7% (Retail).

# Method
- Dataset design: 315 tasks across banking (50), airline (100), retail (165); realistic goal transitions; five personas; explicit goal shifts aligned with customer-service workflows.
- Task generation:
  - Seeded from τ²-bench: reuse 50 airline and 114 retail templates; add 50 newly generated scenarios across airline/retail; banking coverage entirely original (50).
  - Mix of human-written and LLM-generated tasks; added explicit goal-sequence annotations, broader persona coverage, uniform shift-triggering rules.
- User personas (Table 2):
  - EASY_1: Polite, detail-oriented, step-by-step.
  - EASY_2: Easily distracted, casual, confused.
  - MEDIUM_1: Business-focused, impatient, efficient.
  - MEDIUM_2: Curious learner, asks questions.
  - HARD_1: Suspicious, questioning, demands proof.
  - Persona distribution (Appendix Table 8): EASY_1 33; EASY_2 34; MEDIUM_1 69; MEDIUM_2 34; HARD_1 31.
- Task schema: Declarative JSON specifying persona, known/unknown info, ordered goals via goal_shifts with required_shifts = len(goals) − 1. Transitions triggered naturally (e.g., after several turns on same goal or agent’s “anything else?”). Agents do not see markers.
- Examples: >150 unique goal labels across domains (e.g., airline reservation/baggage/cancellation; retail returns/exchange/order_tracking; banking statements/fraud_response/payments). Sequences include ["authentication","transactions","dispute"].
- User/agent control: User discloses known_info; assistant performs all tool interactions. One-tool-per-turn enforced by harness.
- Evaluation harness: Built on τ²-bench; enforces constraints (one-tool-per-turn, policy adherence), correct sequencing of goal shifts; enables systematic re-planning and persona-adjusted communication.

# Motivation
- Problem statement: Binary pass@k fails to capture nuanced progress and differences in efficiency, redundancy, and adaptation under dynamic goal changes.
- Key contributions:
  1) Multi-channel success (TSR) via weighted average: communication (25%), action execution (45%), behavioral compliance/NL assertions (30). Communication scored via LLM-as-judge (GPT-4o-mini) evaluating relevance, clarity, helpfulness.
  2) Tool efficiency metrics (TUE and TCRR) measuring tool selection correctness, parameter validity, and redundancy patterns.
  3) GSRT for adaptation latency across acknowledgment, tool usage, and goal achievement.
  4) Alignment with enterprise deployment needs (efficiency, robustness, communication quality).

# Conclusion
- Limitations and future work:
  - Personas: Currently benign; lack adversarial/deceptive/hostile behaviors; limited long-horizon memory or multi-goal juggling. Plan to add harder personas (adversarial, interruptions, implicit constraints, multilingual).
  - Domain/tool scope: Focus on banking/retail/airline with domain APIs; excludes IDE/code tools, OS/shell, spreadsheets/BI, browsers, robotics/IoT; no unified tool protocol (MCP). Plan to broaden and add MCP adapters.
  - Goal-shift specification: Pre-declared and often explicit; does not evaluate implicit drift, overlapping objectives, conflicts. Future: latent/ambiguous shifts, partial reversions, concurrent subgoals.
  - Model breadth: Three major families; plan to expand to more sizes/architectures (including open weights) and domains (healthcare, education, tech support).
- Summary conclusions:
  - AgentChangeBench evaluates dynamic goal shifts with four metrics (TSR, TUE, TCRR, GSRT).
  - Cross-model experiments show clear differences in robustness and adaptation: Claude-3.7-Sonnet strong recovery, GPT-4o balanced performance, Gemini-2.5-Flash weaker in banking but competitive in retail (as stated).
  - Future directions: New domains, automated task generation, multilingual settings.

# Appendix
- Acknowledgments: τ-bench and τ²-bench teams, MultiWOZ community.
- Dataset and task details:
  - Systematic five-stage task generation (domain analysis, tool definition, template creation, persona integration, goal-shift integration).
  - LLM-assisted pipeline with manual review ensuring tool parameter validity, coherent known_info, and communicate_info sourced from tool responses.
- Example task (banking MEDIUM_1): Unlock card then file dispute for transaction tx_303; known info includes phone, DOB, transaction details; goals ["cards","dispute"]; evaluation includes verify_identity, unlock_card_request; natural language assertions and communicate_info items ($149.99, acc_303, tx_303).
- Persona definitions: Detailed behavioral, speaking styles, expertise, technology comfort, goal-change behavior, common phrases for five personas (EASY_1/EASY_2/MEDIUM_1/MEDIUM_2/HARD_1).
- Evaluation protocol:
  - Simulation: 3 independent runs per task; persona-specific user simulator; environment reset between runs; tool calls validated.
  - Scoring: monitor required components; validate tool correctness/parameters; assess communication quality; evaluate NL assertions; measure GSRT for all shifts.
  - Quality assurance: Manual review of 10% tasks; consistency checks; inter-rater reliability; updates based on feedback.
- Metric parameters:
  - TSR weights (0.25, 0.45, 0.30) based on task importance.
  - TUE weights (0.6 T, 0.4 P) based on operational cost; higher weight for tool correctness.
  - TCRR: window_size=3 turns; batch_threshold=2 calls.
- Tool definitions:
  - Banking tools include get_customer_by_phone, unlock_card, file_dispute, transfer_to_human_agents, etc.
  - Retail tools include get_order_details, return_delivered_order_items, modify_pending_order_items, transfer_to_human_agents.
  - Airline tools include book_reservation, update_reservation_flights, search_direct_flight, transfer_to_human_agents, etc.
- Additional results:
  - Patterns: Retail and Banking show high redundancy (Retail new 65–89%; Banking 58–72%); Airline new low redundancy (14–24%). Tool correctness typically high (≥95%); strong goal-shift recovery for GPT-4o and Sonnet on new sets; weaker for Gemini on new sets.
  - Full topline metrics (Table 9) and efficiency/recovery (Table 10) provide domain/model/set-specific numbers (examples cited above).
- Open-source model evaluation:
  - Qwen2.5-14B-Instruct (single-run per domain):
    - Airline new: TSR 59.75%; TUE 96.87%; TCRR 5.57%; GSRT Recovery 84.8%; Transfer 9.1%.
    - Airline old: TSR 47.29%; TUE 92.71%; TCRR 16.47%; Recovery 88.5%; Transfer 5.2%.
    - Banking: TSR 45.44%; TUE 80.08%; TCRR 47.27%; Recovery 77.8%; Transfer 13.7%.
    - Retail new: TSR 48.15%; TUE 84.77%; TCRR 36.94%; Recovery 75.9%; Transfer 20.7%.
    - Retail old: TSR 57.96%; TUE 85.20%; TCRR 28.26%; Recovery 85.0%; Transfer 10.9%.
  - Integration challenges: Mistral-based models’ tool-call format differs from OpenAI’s; DeepSeek “thinking” blocks violate turn protocol; adapters needed for standardized tool invocation and turn-taking.
- Benchmark roadmap:
  - Near-term growth: +100 education tasks planned; maintain explicit goal-shift structure and persona coverage.
  - Beyond customer service: scenarios for operations agents, RAG copilots, workflow builders, data/BI helpers, all with goal shifts.
  - MCP integration: plan to snapshot datasets, containerize servers, add contract tests and record/replay fixtures; seedable simulators to ensure repeatability.
  - Persona hardening: introduce adversarial traits (evasion, sarcasm, misleading/conflicting instructions).
- System architecture: Extensions to τ²-bench orchestrator with goal sequence management and LLM judge for GSRT detection; metrics computed from SimulationRun data covering TSR, TUE, TCRR, GSRT.
- Noted counts from GSRT (Table 10):
  - Airline old—GPT-4o: 179 shifts; Recovery 91.6%; Transfer 8.4%.
  - Retail old—Claude: 324 shifts; Recovery 89.5%; Transfer 9.0%.

# References
- Prior persona-based dialogue and user simulation: Li et al. (ACL 2016), Zhang et al. (ACL 2018), Schatzmann et al. (SIGdial 2007).
- Holistic evaluation and capability measurement: Liang et al. (arXiv 2022), Srivastava et al. (arXiv 2022).
- Tool-use and agent benchmarks: τ-bench (Yao et al., 2024), τ²-bench (Barres et al., 2025), AgentBench (ICLR 2024), API-Bank (2023), Gorilla (2023), Toolformer (2023).
- Web/OS agent environments: WebArena (2023), ALFWorld (2020), BrowserGym (2023), OSWorld (2024), AppAgent (2023), WebShop (ICLR 2022).
- Reasoning/acting strategies: ReAct (2023), Chain-of-Thought (2022), PAL (ICML 2022), Reflexion (2023), AutoGen (2023), TaskMatrix.AI (2023).
- Domain guidance: FFIEC (2021); MultiWOZ 2.1 (LREC 2020).