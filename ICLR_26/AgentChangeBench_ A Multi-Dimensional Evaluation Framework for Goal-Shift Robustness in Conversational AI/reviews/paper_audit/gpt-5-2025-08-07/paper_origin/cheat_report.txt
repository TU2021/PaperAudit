Academic Integrity and Consistency Risk Report

Overview
The manuscript presents a new benchmark (AgentChangeBench) and extensive results across multiple models and domains. Several high-impact inconsistencies and numerical mismatches materially affect interpretability and trustworthiness of the reported findings. Below are evidence-anchored issues that warrant correction or clarification.

1) Dataset size contradiction
- Claim: “AgentChangeBench comprises 2,835 task sequences” (Abstract; Block #2).
- Multiple sections state and use 315 tasks across three domains (Methodology 3.1 and 3.2; Blocks #8–#9; Dataset comparison Table 6; Block #31; Appendix growth plan; Block #100).
- Impact: Core scale of the benchmark is unclear. If analyses and tables are based on 315 tasks, the 2,835 figure misleads readers about statistical power and coverage.
- Evidence: Abstract (Block #2) vs. Methodology (Block #8, #9), Table 6 (Block #31), Appendix B.2 (Block #74), Appendix F (Block #100).

2) GSRT metric labeling and “lower is better” contradiction
- Figure 1 caption: “GSRT by model and domain. Turns after a user goal shift...; lower is better. … labels show recovery rate, percent of shifts acknowledged…” (Block #4).
- GSRT is defined as recovery time in turns (Section 4.3; Block #22). Recovery rate is a percentage computed separately and does not have “lower is better.”
- Impact: The caption conflates recovery time (lower is better) with recovery rate (higher is better), creating ambiguity about what is actually plotted and how to interpret the bars.
- Evidence: Figure 1 caption (Block #4) vs. GSRT definition (Block #22).

3) Recovery rate numeric mismatches across Figure/Table/Image
- Gemini (Airline):
  - Table under “Goal-Shift Recovery Rate”: 22.3% (Block #4).
  - Bar chart image: 32.1% (Block #6).
  - Table 5 (new tasks only): 48.6% (Block #27).
  - Table 10 (Appendix): Airline-new GSRT not reported (“—”) due to insufficient credits (Block #85).
- GPT-4o (Airline):
  - Figure shows 91.9% (Block #6).
  - Table 10: 91.6% (old) and 92.2% (new) (Block #85).
- Impact: Conflicting values for the same metric prevent reliable interpretation. The “Source: Appendix Tables 10, 5” (Block #4 caption) does not consistently match the plotted numbers.
- Evidence: Blocks #4, #6, #27, #85.

4) “New tasks only” table (Table 5) appears to mix sets; conflicts with Appendix reporting
- Table 5 claims “new tasks only” (Block #27), yet reports Claude Retail TSR = 79.57% and Recovery = 89.5%, which match “Retail-old” values in Table 9 (79.57% TSR; Block #85) and Table 10 (Retail-old GSRT Recovery 89.5%; Block #85).
- Additionally, Appendix D.4 states GSRT is not reported for Airline-new (Gemini) (Block #83, #85), yet Table 5 reports Airline-new (Gemini) Recovery = 48.6% (Block #27).
- Impact: Mislabeling of dataset split (old vs. new) and inconsistent reporting of availability undermines validity of comparative analyses on “new” goal-shifted tasks.
- Evidence: Table 5 (Block #27) vs. Tables 9–10 (Block #85) and Appendix D.4 (Blocks #83, #85).

5) Table 4 “averaged over old+new” inconsistency for Claude (Retail)
- Table 4 states values are averaged over old+new “where applicable” (Block #26).
- For Retail/Claude, Table 4 reports 79.57%, which matches Retail-old (Table 9; Block #85), not the average of Retail-new (61.58%) and Retail-old (79.57%), which would be ~70.6% (assuming equal weighting).
- Impact: Misleading topline comparative claims about model ranking on Retail.
- Evidence: Table 4 (Block #26) vs. Table 9 (Block #85).

6) Persona coverage counts do not match total tasks
- Table 8 (Appendix D.2) lists task counts per persona summing to 201 (33 + 34 + 69 + 34 + 31 = 201) (Block #81), while the dataset is repeatedly stated as 315 tasks (Blocks #8, #31).
- Impact: Incomplete or incorrect accounting of persona distribution undermines persona-level analyses (e.g., Table 7) and questions sampling balance.
- Evidence: Table 8 (Block #81) vs. dataset statements (Blocks #8, #31).

7) Number of simulation runs: 3 vs 15
- Protocol: “Each task is evaluated across 3 independent runs” (Appendix B.2; Block #74).
- Analysis: “14/15 runs contain at least one such low-T case” (Section 4.3; Block #20); figures and tables in Appendix refer to Run 1–Run 15 (Blocks #89–#95).
- Impact: Unclear experimental design—if 15 runs exist, the 3-run protocol is contradicted; if only 3 runs were used for core results, the 15-run analysis may not apply.
- Evidence: Block #74 vs. Blocks #20, #89–#95.

8) Parameter validity saturation contradictions
- Section 4.3: “Across 315 tasks, mean P=0.986… 98.6% of traces at or above 0.95” (Block #20).
- Later claim: “Parameter accuracy saturates: parameter validity is 100% across runs” (Section 4.4.1; Block #29).
- Table 10 reports PA = 100.00% for all listed cells (Block #85).
- Impact: Internal inconsistency about parameter validity level (98.6% ≥ 0.95 vs. 100.00%) confuses interpretation of TUE and whether PA truly sits at a ceiling.
- Evidence: Blocks #20, #29, #85.

9) Retail domain coverage statement contradicts multi-model evaluation
- Section 4.4.3: “Retail domain comprises 165 tasks evaluated with GPT-4o” (Block #31).
- However, Retail results are reported for Claude and Gemini throughout Tables 4, 5, 9, 10 (Blocks #26–#27, #85).
- Impact: Confusion about whether Retail was evaluated across all three models or only GPT-4o, affecting claims of cross-model comparisons.
- Evidence: Block #31 vs. Blocks #26–#27, #85.

10) Unsubstantiated pass@k claims
- Text states “pass^k frequently drops to 0.0 on new sets” (Section 4.4.1; Block #29) and contrasts with TSR. No table or figure presents pass@k values.
- Impact: Without supporting data, conclusions drawn from pass@k comparisons lack evidential grounding.
- Evidence: No direct evidence found in the manuscript beyond narrative in Block #29.

11) Task sourcing/accounting ambiguity
- Task generation accounting: “reuse 50 airline and 114 retail templates, and contribute 50 newly generated scenarios across airline and retail. Banking coverage is entirely original (50 tasks)” (Block #9).
- Summation yields 264 tasks (50 airline reused + 114 retail reused + 50 new airline/retail + 50 banking), whereas the dataset is 315 tasks (Blocks #8, #31), leaving 51 tasks unaccounted by the described sourcing.
- Impact: Unclear provenance of a significant portion of the dataset complicates reproducibility and attribution.
- Evidence: Blocks #9 vs. Blocks #8, #31.

12) Figure 1 source and metric alignment
- Caption cites “Source: Appendix Tables 10, 5” (Block #4), but the plotted values (e.g., Gemini-Airline) do not match either table consistently (see Issue 3).
- Impact: Weakens the credibility of the visual summary.

Summary
The manuscript contains multiple high-impact internal inconsistencies in dataset size, metric definitions/labels, numerical results across tables/figures, experimental protocols (number of runs), and coverage statements. These issues materially affect the validity and reproducibility of the reported conclusions, particularly around GSRT, TSR comparisons, and persona analyses. Addressing these contradictions—by reconciling numbers, clarifying metric usage, ensuring consistent splits (old vs. new), and providing complete, aligned data tables—will be necessary to restore confidence in the findings. If these are resolved, the framework could offer valuable multifaceted evaluation; as written, the inconsistencies impede reliable interpretation.