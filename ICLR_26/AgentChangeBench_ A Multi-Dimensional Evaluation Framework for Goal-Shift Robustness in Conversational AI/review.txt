### Summary

This submission introduces **AgentChangeBench**, a **multi-turn benchmark** for evaluating **goal-shift robustness** in tool-augmented conversational agents. The core motivation is that real user–agent interactions rarely follow a single static objective; instead, users often **change goals mid-dialogue**, and agents must detect, adapt, and recover efficiently. To capture this, AgentChangeBench provides **sequence-annotated goal shifts** across multiple enterprise-style service domains and **five user personas**, and proposes a **multi-dimensional evaluation framework** beyond success rate: **Task Success Rate (TSR)**, **Tool Use Efficiency (TUE)**, **Tool Call Redundancy Rate (TCRR)**, and **Goal-Shift Recovery Time (GSRT)**. Reviewers generally agree the problem is important and that explicitly measuring recovery time and redundancy is a promising direction. However, several reviewers judge the work **insufficiently rigorous and poorly presented in its current form**, citing missing/appendix-only key experiments, unclear statistical reliability, inconsistent reporting, and limited positioning and analysis. Overall sentiment is mixed-to-negative, with multiple reviewers leaning toward rejection unless the paper is substantially strengthened.

### Strengths

* **Highly relevant evaluation gap:** Reviewers agree that most existing agent/tool-use benchmarks implicitly assume **static objectives**, whereas goal shifts are common in real customer-service and enterprise workflows.
* **Benchmark design targets operationally meaningful behaviors:** The proposed metrics (TSR/TUE/TCRR/GSRT) are widely viewed as valuable because they capture **efficiency, cost, wasted tool calls, and adaptation latency**, not just pass/fail.
* **Multi-domain and persona-conditioned setup increases realism:** The inclusion of multiple domains and personas is seen as a reasonable way to introduce **behavioral diversity** and stress-test robustness under different user styles.
* **Empirical comparisons suggest non-trivial trade-offs:** At least one reviewer finds the cross-model study informative, arguing that the benchmark reveals contrasts (e.g., recovery speed vs. tool efficiency) that single metrics might hide.

### Weaknesses

* **Key experimental results are poorly surfaced (appendix-heavy):** Multiple reviewers criticize that the main paper does not read like a complete benchmark study because **core experimental results are placed in the appendix**, weakening the narrative and the evidence for the benchmark’s utility.
* **Statistical reliability is unclear:** Reviewers note results are mostly **point estimates** with very few runs per task (e.g., n≈3), lacking **confidence intervals, variance estimates, or significance testing**, making claims of model differences harder to trust.
* **Inconsistent reporting and missing details:** Reviewers flag **inconsistent task counts** (e.g., totals reported differently across sections/tables), uneven numerical precision, and missing important setup details (e.g., user simulation procedure, temperature/decoding settings), which reduces interpretability and reproducibility.
* **Limited analysis of “redundancy” and failure modes:** While high redundancy rates are reported, reviewers want deeper interpretation—whether redundancy reflects **agent reasoning issues**, **tool design constraints**, or **domain/task artifacts**—plus more structured error analysis.
* **Positioning and coverage concerns:** Some reviewers view the benchmark as an **incremental extension** of existing tool-use evaluations (adding persona + explicit goal shift) and request stronger related-work grounding (including human–agent interaction literature) and broader model coverage (open-source and newer reasoning models).
* **Scope limitations of goal shifts and personas:** The benchmark focuses on **explicit** goal shifts and “benign” personas; reviewers note this may not capture more subtle, implicit, adversarial, or security-sensitive real-world shifts.
