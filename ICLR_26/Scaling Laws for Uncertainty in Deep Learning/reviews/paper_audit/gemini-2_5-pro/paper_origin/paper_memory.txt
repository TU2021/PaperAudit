# Global Summary
This paper empirically investigates whether predictive uncertainties in deep learning follow scaling laws with respect to dataset size (N), similar to those observed for model performance. Through extensive experiments on vision (ResNet, ViT on CIFAR, ImageNet) and language (GPT-2, Phi-2) tasks, the authors demonstrate that various measures of uncertainty—Total (TU), Aleatoric (AU), and Epistemic (EU)—consistently exhibit power-law scaling. This behavior is observed across multiple uncertainty quantification (UQ) methods, including MC Dropout, Deep Ensembles, and MCMC. A key finding is that epistemic uncertainty, while decreasing with N, does not vanish quickly, supporting the relevance of Bayesian methods even for large datasets. The paper also provides a theoretical connection for linear models, linking Watanabe's generalization error from Singular Learning Theory (SLT) to total predictive uncertainty, showing that EU scales as O(1/N) in this setting. The authors state limitations regarding the lack of clear scaling with model size and computational budget.

# Abstract
The paper explores whether scaling laws, similar to those for model performance, govern predictive uncertainty in deep learning. While identifiable parametric models have derivable O(1/N) contraction rates for epistemic uncertainty, this is unexplored for over-parameterized models. The work empirically demonstrates the existence of scaling laws for various uncertainty measures with respect to both dataset and model sizes. Experiments on vision and language tasks show these scaling laws hold for popular UQ methods like approximate Bayesian inference and ensembles, for both in- and out-of-distribution data. The findings are presented as evidence against the skepticism that large datasets make Bayesian approaches unnecessary, showing that "so much data" is often insufficient to make epistemic uncertainty negligible.

# Introduction
- The paper is motivated by the discovery of empirical scaling laws for deep learning performance, where test loss scales as a power-law of model size, data, or compute. It asks if similar laws govern predictive uncertainty.
- Bayesian deep learning is presented as a principled framework for uncertainty quantification (UQ). The paper mentions various UQ methods: Laplace approximation, variational inference (VI), MC dropout, Deep Ensembles, and MCMC methods like SGLD and SGHMC.
- The study focuses on how predictive uncertainties, decomposed into aleatoric (AU) and epistemic (EU) parts, scale with training data size N.
- Figure 1 shows initial evidence on CIFAR-10 with a ResNet-18, reporting power-law exponents (γ) for epistemic uncertainty vs. data size N: MCMC (γ = -0.44), MC Dropout (γ = -0.36), and Deep Ensembles (γ = -0.80).
- The paper connects its investigation to theoretical frameworks like Singular Learning Theory (SLT), which predicts generalization error scales as L - L0 = λ / N^γ.
- Contributions are stated as: (i) a comprehensive empirical study of uncertainty scaling laws, (ii) demonstrating power-law trends for in- and out-of-distribution uncertainty, and (iii) deriving a formal connection between generalization error in SLT and total uncertainty in linear models.

# Preliminaries
- Predictive uncertainty is decomposed into Aleatoric Uncertainty (AU, from data variability) and Epistemic Uncertainty (EU, from model/data limitations).
- Total Uncertainty (TU) is defined as the entropy of the mean predictive distribution. AU is the average entropy of individual predictions. EU is the difference: EU = TU - AU.
- The paper studies the power-law scaling of these metrics averaged over a test set, as a function of training data size N.
- Criticisms of this decomposition are noted: Wimmer et al. (2023) argue the additive separation may not hold, and others point to issues of disentanglement and epistemic uncertainty collapse in large models.
- The section on Scaling Laws and Generalization recaps that test loss typically follows a power-law `L(x) = L_inf + (x_0/x)^alpha`, where x can be dataset size N, model size P, or compute C.

# Method
The paper employs several UQ methods to test the hypothesis that uncertainty scaling laws are a general phenomenon.
- **MC Dropout**: Standard dropout is used at training time, and at test time, multiple stochastic forward passes with different dropout masks generate an ensemble of predictions.
- **Gaussian Approximations**: This family includes Laplace approximations (using curvature at a MAP estimate) and variational inference (minimizing KL divergence).
- **Markov Chain Monte Carlo (MCMC)**: Gradient-based MCMC methods like SGHMC and SGLD are used to sample from the posterior over model parameters. Both global parameter priors and Gaussian process functional priors are considered.
- **Deep Ensembles**: Multiple networks (M=5 or M=10) are trained independently from different random initializations.
- **Partially stochastic networks**: A subset of network layers are treated stochastically while the rest are optimized deterministically.

# Experiments
The paper presents experiments across vision and language tasks to demonstrate uncertainty scaling laws.

### Image classification
- Architectures: ResNet, WideResNet, Vision Transformer (ViT).
- Datasets: CIFAR-10, ImageNet32, CIFAR-100 (Appendix), CIFAR-10-C (OOD).

- **ResNet and WideResNet**:
    - Figure 2 (CIFAR-10): Shows power-law scaling for MC Dropout and Deep Ensembles.
        - ResNet-18 (p=0.5): γTU=-0.42, γAU=-0.44, γEU=-0.36. (Average over 10 folds).
        - WideResNet(28,10) (M=10): γTU=-0.66, γAU=-0.72, γEU=-0.55. (Single fold).
    - Figure 3 (CIFAR-10 with SGD+SAM): Shows that for ResNets, EU can *increase* with data size N.
        - ResNet-18: γEU=0.14. ResNet-50: γEU=0.41.
    - Figure 6 (OOD on CIFAR-10-C): Uncertainty scaling is also observed, though exponents are smaller (flatter slope).
        - ResNet-18 (p=0.5): γTU=-0.18, γAU=-0.25, γEU=-0.06.
    - Figure 8 (ImageNet-32): Scaling is observed on a larger dataset.
        - ResNet-18 MCD (p=0.5): γTU=-0.03, γAU=-0.01, γEU=-0.28.
        - ResNet-18 MCMC (first layer stochastic): γTU=-0.08, γAU=-0.08, γEU=-0.08.

- **Vision Transformer (ViT)**:
    - A 6-layer ViT is trained on CIFAR-10 and ImageNet32.
    - Figure 5 (CIFAR-10): Shows that optimization strategy heavily influences uncertainty scaling.
        - With cosine annealing (500 epochs): EU increases (γ=0.01).
        - With fixed learning rate (2500 epochs): EU decreases (γ=-0.35). TU and AU scaling exponents are similar in both cases (γTU=-0.37).

### Text classification
- **Phi-2 with Bayesian LoRA**:
    - A pre-trained Phi-2 model was fine-tuned on QQP and ARC datasets using Laplace approximation on LoRA parameters.
    - Figure 15 shows that uncertainties remained flat (γ=0.00) across different fine-tuning data subset sizes, which is attributed to the massive pre-training dataset size.

- **GPT-2 on Algorithmic Dataset**:
    - A GPT-2 model was trained on a synthetic dataset for modular arithmetic (MODULO 97).
    - Figure 4 shows very steep power-law scaling after extensive training (10,000 epochs).
    - Exponents: γTU=-2.86, γAU=-2.82, γEU=-2.95.
    - The emergence of scaling is linked to "grokking" dynamics.

# Discussion
This section connects the empirical findings to theory, focusing on Bayesian linear regression and Singular Learning Theory (SLT).

- **Identifiable Parametric Models**:
    - In Bayesian linear regression, the predictive variance decomposes into data noise (AU) and parameter uncertainty (EU).
    - The EU term, `φ(x*)' * S_N * φ(x*)`, contracts as more data is observed (N increases), and the posterior covariance `S_N` shrinks.
    - In the limit N -> ∞, the predictive uncertainty converges to the irreducible AU.

- **Singular Learning Theory**:
    - The paper proposes a link between Watanabe's generalization error (G_N) and Total Uncertainty (TU) for Bayesian linear regression.
    - G_N is the expected increase in negative marginal log-likelihood from one additional data point.
    - It is shown that G_N can be expressed as the sum of the true data entropy (aleatoric part) and the KL divergence between the true predictive and the model's posterior predictive distribution (epistemic part): `G_N = H[p(y)] + KL[p(y) || q_N(y)]`.
    - The asymptotic expansion for TU in linear regression is derived, showing the EU term decays as `O(1/(N+1))`.
    - The authors hypothesize that SLT may provide a foundation for explaining the observed scaling behaviors in over-parameterized deep models.

- **Conclusions and Limitations**:
    - The paper concludes that predictive uncertainties in deep learning exhibit power-law scaling with dataset size across various settings.
    - A novel theoretical link between generalization error and predictive uncertainty is provided for linear models.
    - **Limitations**:
        - No clear scaling laws were observed with respect to model parameters (P), unlike for test performance (Figure 7).
        - The effect of computational budget was not studied due to resource constraints.
        - More applications to test extrapolation to very large N would have been beneficial.
    - **Broader Impacts**: The results should encourage wider adoption of UQ methods, even with large datasets.

# References
This section contains a list of citations for the works mentioned in the paper, including Kaplan et al. (2020) on performance scaling laws, Watanabe (2009) on Singular Learning Theory, and various papers on UQ methods like MC Dropout (Gal and Ghahramani, 2016) and Deep Ensembles (Lakshminarayanan et al., 2017).

# Appendix
- **A.1 Total Uncertainty in Bayesian Linear Regression**: Provides a detailed derivation showing that for Bayesian linear regression, the predictive variance decreases as N increases, and the EU term decays with a rate of 1/N.
- **A.2 Total Uncertainty and Generalization Error for linear models**: Formally derives the connection between Watanabe's generalization error G_N and TU. It shows G_N can be decomposed into an AU term (entropy of the true process) and an EU term (KL divergence). It also provides the asymptotic expansion for G_N from SLT: `E[G_N] = λ/n + ...`.
- **B Additional results**: Contains supplementary figures and tables with more experimental results.
    - Fig 9: ViT on ImageNet-32, showing scaling with N up to 1.2M.
    - Fig 10 & 11: ResNets and WideResNets on CIFAR-100 with MC Dropout.
    - Fig 12: Extrapolation of uncertainty scaling for WideResNets on CIFAR-10 to hypothetical large N.
    - Fig 13: Deep Ensembles on CIFAR-100.
    - Fig 14: MCMC on ResNet-18 for CIFAR-10/100, showing ID and OOD scaling.
    - Fig 15: Flat uncertainty curves for Phi-2 with Bayesian LoRA.
- **C Experimental setup**: Provides tables with hyperparameters for the ViT (Table 1) and GPT-2 (Table 2) experiments.