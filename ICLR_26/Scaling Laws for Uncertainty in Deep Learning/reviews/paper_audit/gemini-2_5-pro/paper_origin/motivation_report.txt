# Innovation & Motivation Report

## 1. The Authors' Claimed Contribution
- **Core Problem**: To determine if the predictive uncertainty of deep learning models follows predictable, law-like behavior as a function of available resources, particularly dataset size.
- **Claimed Gap**: The authors explicitly state that while scaling laws for model *performance* (e.g., test loss) are well-established, the behavior of predictive *uncertainty* remains largely unexplored in the over-parameterized regime. The abstract notes: "While identifiable parametric models have derivable O(1/N) contraction rates for epistemic uncertainty, this is unexplored for over-parameterized models."
- **Proposed Solution**: The authors conduct a comprehensive empirical study across vision (ResNets, ViTs) and language (GPT-2, Phi-2) models, using multiple standard uncertainty quantification (UQ) methods (MC Dropout, Deep Ensembles, MCMC). They measure how Total, Aleatoric, and Epistemic uncertainty scale with training dataset size (N) and demonstrate the consistent emergence of power-law relationships. They supplement this with a theoretical derivation for linear models connecting total uncertainty to generalization error from Singular Learning Theory (SLT).

## 2. Comparative Scrutiny (The "Trial")
*Here, we analyze how the paper stands against the identified similar works.*

### vs. [Deep Bayesian Multi-Target Learning for Recommender Systems (as a proxy for Kaplan et al., 2020)]
- **Identified Overlap**: This work is representative of the established paradigm of discovering empirical scaling laws for model *performance* (test loss) as a function of data, model size, or compute. The manuscript under review adopts the exact same experimental paradigm: training models on varying data subsets and fitting a power-law to the resulting metric on a log-log plot.
- **Manuscript's Defense**: The authors do not need to defend against this; they explicitly build upon it. Their motivation, as stated in the Introduction, is a direct extension of this prior work: "The paper is motivated by the discovery of empirical scaling laws for deep learning performance... It asks if similar laws govern predictive uncertainty." The defense is that they are applying a known scientific methodology to a new, unstudied quantity (uncertainty).
- **Reviewer's Assessment**: This distinction is significant and valid. The novelty does not come from the experimental methodology but from the discovery of a new empirical phenomenon. By demonstrating that uncertainty, like performance, follows predictable scaling laws, the paper makes a substantive new contribution to our understanding of deep learning models. The connection is one of inspiration, not duplication.

### vs. [On the Importance of Strong Baselines in Bayesian Deep Learning (Mukhoti et al.)]
- **Identified Overlap**: The manuscript heavily relies on MC Dropout as one of its primary UQ methods. The similar work established MC Dropout as a strong, non-trivial baseline that must be treated rigorously in any empirical comparison within Bayesian Deep Learning.
- **Manuscript's Defense**: The manuscript does not claim superiority over MC Dropout; it uses it as a scientific instrument. In Section "Method" and "Experiments" (e.g., Figure 2), MC Dropout is presented as one of several canonical methods to test the universality of the scaling law hypothesis. The paper's contribution is orthogonal to the one made by Mukhoti et al. The former discovers a property *of* the uncertainty produced by methods like MC Dropout, while the latter argues for the validity *of* MC Dropout as a method.
- **Reviewer's Assessment**: The manuscript's use of MC Dropout is appropriate and strengthens its claims. By showing that the scaling phenomenon holds across MC Dropout, Deep Ensembles, and MCMC, the authors successfully argue that their finding is a general property of deep learning uncertainty, not an artifact of a single UQ method. This aligns perfectly with the principle of rigorous empiricism championed by the similar work. The overlap is foundational, not competitive.

### vs. [Active learning for data streams: a survey (Cacciarelli et al.)]
- **Identified Overlap**: The field of active learning, as surveyed in this paper, is fundamentally motivated by the principle that acquiring more data reduces model uncertainty. The most common strategies involve querying points where the model is most uncertain (i.e., has high Epistemic Uncertainty).
- **Manuscript's Defense**: The manuscript provides the quantitative foundation for the core assumption of active learning. It does not propose an active learning method but instead characterizes the baseline "passive learning" decay rate of uncertainty. The finding that epistemic uncertainty "does not vanish quickly" (as stated in the Abstract) provides a strong justification for *why* active learning is necessary and valuable, even for large datasets.
- **Reviewer's Assessment**: This comparison highlights the significance of the manuscript's contribution. The paper's findings are not redundant with the field of active learning; rather, they provide a crucial piece of underlying theory. The measured power-law exponents (e.g., γEU = -0.36) quantify the very problem that active learning seeks to solve more efficiently. The manuscript's contribution is therefore complementary and foundational to the work described in the survey.

## 3. Novelty Verdict
- **Innovation Type**: **Substantive**
- **Assessment**:
  The paper successfully survives the comparative scrutiny. Its core claim—the existence of power-law scaling for predictive uncertainty—is a novel empirical discovery. The provided similar works are not competitors but rather represent the foundational pillars upon which this work is built: the scaling law paradigm (Kaplan et al.), standard UQ methods (MC Dropout), and application areas that benefit from a deeper understanding of uncertainty (Active Learning). The manuscript clearly articulates its gap in the literature and provides comprehensive evidence to support its claims. The added theoretical connection to SLT for linear models further elevates the work beyond a purely empirical observation.
  - **Strength**: The primary strength is the discovery of a new, seemingly general phenomenon in deep learning, supported by extensive experiments across different domains, architectures, and UQ methods. The theoretical connection to SLT, while preliminary for deep models, provides a promising direction for future work.
  - **Weakness**: The authors are transparent about the limitations, which tempers the universality of the claims. The lack of clear scaling with model size (P) is a notable departure from the performance-scaling paradigm. Furthermore, the flat scaling observed for the pre-trained LLM (Phi-2) suggests these laws may not hold in transfer learning settings where the bulk of the data is from pre-training, a crucial modern use-case.

## 4. Key Evidence Anchors
- **Claimed Gap**: Abstract: "...this is unexplored for over-parameterized models."
- **Core Empirical Finding**: Figure 1, 2, 4, 8 show consistent power-law fits for TU, AU, and EU vs. dataset size N across different models and datasets.
- **Theoretical Contribution**: Discussion section and Appendix A.2, which derive the connection `G_N = H[p(y)] + KL[p(y) || q_N(y)]` and link Total Uncertainty to Watanabe's generalization error for linear models.
- **Acknowledged Limitation**: Conclusion/Limitations section and Figure 7, which explicitly state and show that "No clear scaling laws were observed with respect to model parameters (P)".
- **Boundary Condition**: Figure 15, showing flat uncertainty curves for Phi-2 fine-tuning, indicating the scaling law applies to training from scratch but not necessarily to fine-tuning on top of massive pre-training.