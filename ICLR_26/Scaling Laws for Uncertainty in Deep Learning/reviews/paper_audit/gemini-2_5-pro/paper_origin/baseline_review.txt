1) Summary
This paper investigates whether predictive uncertainties in deep learning models follow scaling laws with respect to dataset and model sizes, analogous to the established scaling laws for model performance. The authors conduct a comprehensive empirical study across various architectures (ResNet, ViT, GPT-2), datasets (CIFAR-10/100, ImageNet32, algorithmic), and uncertainty quantification (UQ) methods (MC Dropout, Deep Ensembles, MCMC). The core finding is that different measures of predictive uncertainty—Total (TU), Aleatoric (AU), and Epistemic (EU)—consistently exhibit a power-law decay with increasing training data size (N). The study also explores these trends on out-of-distribution data. To provide a theoretical basis, the authors derive a formal connection between total uncertainty and generalization error from Singular Learning Theory (SLT) in the context of Bayesian linear regression, showing that EU decays at a rate of O(1/N).2) Strengths
*   **Novel and Important Research Question**
    *   The paper addresses a natural and significant question that extends the well-known scaling laws of performance to the domain of uncertainty quantification (Section 1, paragraph 1). This is a timely and previously underexplored area.
    *   To the best of the authors' knowledge, this is the first systematic study of scaling laws for any form of uncertainty in deep learning (Contribution (i), Section 1). This pioneering aspect has the potential to open up a new line of research.
    *   The work provides strong empirical evidence to counter the skepticism that Bayesian methods are unnecessary with "so much data," by showing that epistemic uncertainty diminishes predictably but is often non-negligible even at large data scales (Abstract, Section 6).*   **Comprehensive and Rigorous Empirical Study**
    *   The investigation covers a wide range of experimental settings, which strengthens the generality of the findings. This includes multiple UQ methods like MC Dropout, Deep Ensembles, and MCMC (Section 3).
    *   The study spans different modalities and architectures, including CNNs (ResNet, WideResNet), Transformers for vision (ViT), and language (GPT-2), demonstrating the broad applicability of the observed phenomena (Section 4).
    *   Experiments are conducted on multiple standard vision datasets (CIFAR-10, CIFAR-100, ImageNet32) and a synthetic language task, providing a robust empirical foundation (Sections 4.1, 4.2).
    *   The analysis extends to out-of-distribution (OOD) scenarios by testing on corrupted datasets (CIFAR-10-C), which is a critical aspect for UQ evaluation (Figure 6, Figure 14).*   **Consistent and Insightful Empirical Findings**
    *   The central finding is the consistent observation of power-law scaling for TU, AU, and EU with respect to dataset size N across most configurations (Figure 1, Figure 2, Figure 4, Figure 10). This predictability is a valuable scientific contribution.
    *   The paper reports interesting and non-trivial behaviors under specific conditions. For example, the use of Sharpness-Aware Minimization (SAM) leads to an *increase* in Epistemic Uncertainty with data size, a counter-intuitive result that stimulates further thought (Figure 3, Section 4.1.1).
    *   The study highlights the sensitivity of uncertainty dynamics to optimization choices, such as the learning rate schedule, which can dramatically alter the scaling behavior of EU in ViTs (Figure 5). This provides a valuable lesson on the interplay between optimization and uncertainty.
    *   The extrapolation of these laws is shown to be a practical tool for predicting the amount of data needed to reduce uncertainty to a specific level (Figure 12, Section 6).*   **Principled Theoretical Grounding**
    *   The paper provides a theoretical analysis for Bayesian linear regression that formally connects Total Uncertainty to the generalization error from Singular Learning Theory (Section 5.2, Equation 10).
    *   This analysis successfully derives the O(1/N) contraction rate for the epistemic component of uncertainty in this identifiable setting (Appendix A.1.1, Equation 22), providing a solid theoretical anchor for the empirical observations.
    *   While acknowledging the gap to deep models, this theoretical link (Contribution (iii), Section 1) offers a promising and principled direction for future work aiming to build a deeper understanding of these phenomena.3) Weaknesses
*   **Inconsistent Experimental Protocols**
    *   The experimental settings for training vary across different figures, which makes it difficult to directly compare the reported scaling exponents (γ) and isolate the factors influencing them.
    *   For instance, ResNets in Figure 2 are trained for 400 epochs with SGD and an optional cosine annealing scheduler (Section 4.1.1), while ResNets in Figure 7 are trained for 400 epochs with a fixed learning rate.
    *   The ViT experiments in Figure 5 use two different training durations (500 vs. 2500 epochs) and schedulers (cosine annealing vs. fixed LR), presented as an ablation but making it hard to establish a baseline scaling behavior for this architecture. A more unified protocol for the main results would strengthen the comparative analysis.*   **Inconclusive Results on Scaling with Model Size**
    *   While the paper's title suggests scaling laws for both data and model sizes, the investigation into scaling with model parameters (P) is limited and does not yield clear power-law trends.
    *   Figure 7 (right panel) plots uncertainty against model size P, but the resulting curves are not linear on the log-log plot and lack the clear power-law relationship observed with data size N.
    *   The authors acknowledge this as a limitation due to computational resources (Section 6, Limitations), but this represents a significant missing piece of the "scaling laws" narrative promised by the paper's framing.*   **Clarity and Interpretation of Results**
    *   Several figures contain captions formatted as tables of γ values (e.g., Figure 2, Figure 6, Figure 7), which are dense and difficult to parse. A more narrative caption style with detailed tables moved to the appendix would improve readability.
    *   The interpretation of some key results is speculative. The explanation for why SAM increases EU (Section 4.1.1, paragraph 2) is presented as an intuition about the optimizer navigating a more curved loss landscape, but no direct evidence (e.g., Hessian measurements) is provided to support this hypothesis.
    *   In the GPT-2 experiment (Figure 4), the uncertainty metrics show a distinct upward tick at the largest data size (10K). This behavior is not explained in the text, which detracts from the otherwise clean power-law narrative for that experiment.*   **Substantial Gap Between Theory and Experiments**
    *   The theoretical analysis is confined to Bayesian linear regression, an identifiable parametric model (Section 5.1). This setting is fundamentally different from the over-parameterized, non-identifiable deep neural networks used in the experiments.
    *   The paper proposes SLT as a "speculative theoretical link" (Section 5.2) but does not bridge the large conceptual gap between the linear model derivation and the empirical results from deep networks.
    *   While the connection is valuable as a starting point, its explanatory power for the observed phenomena in complex models like ResNets or ViTs remains limited, and the paper could be more explicit about the assumptions that would need to hold for the theory to apply more broadly.4) Suggestions for Improvement
*   **Unify and Clarify Experimental Protocols**
    *   To enable more robust comparisons, the authors should establish a single, consistent training protocol (e.g., fixed optimizer, scheduler, and epoch count) for the core set of experiments demonstrating the main scaling phenomena.
    *   Deviations from this protocol, such as the ablations on learning rate schedulers or optimizers (e.g., Figure 3, Figure 5), should be clearly framed as such, with the baseline protocol as a reference point.
    *   A summary table of hyperparameters for each main figure, either in the main text or appendix, would greatly improve reproducibility and clarity.*   **Conduct a Focused Study on Model Size Scaling**
    *   A more systematic investigation of scaling with model size (P) is needed to fully address the paper's stated goals. This could involve fixing the dataset size N and training a family of models with varying P (e.g., the full range of ResNets or ViT variants).
    *   The results should be presented in a dedicated log-log plot of uncertainty vs. P. If a clear power law does not emerge, this negative result would itself be interesting and should be discussed, perhaps hypothesizing why performance scales cleanly with P while uncertainty does not.*   **Improve Presentation and Strengthen Interpretations**
    *   The authors should revise figure captions to be more descriptive and less like data tables. The detailed γ values could be moved to a summary table in the appendix for easier reference.
    *   For the SAM experiment, the authors could strengthen their interpretation by including an empirical measurement that supports their hypothesis, such as the trace of the Hessian or the loss curvature along random directions, to show that SAM indeed finds flatter minima in their setting.
    *   For the GPT-2 result in Figure 4, the authors should address the final uptick. If it is an artifact of noise (e.g., due to only 3 folds), they could increase the number of seeds. If the effect is robust, they should offer a concrete hypothesis or explicitly label it as an unexplained phenomenon requiring future work.*   **Explicitly Address the Theory-Practice Gap**
    *   The authors should more clearly delineate the theoretical results for linear models from the empirical findings in deep networks in Section 5.
    *   It would be beneficial to include a discussion on *why* the intuitions from the linear model might (or might not) transfer to deep learning. This could involve discussing concepts like the effective dimensionality of the data manifold or the local geometry of the loss landscape, which are central to theories like SLT.
    *   Explicitly stating the assumptions required to bridge this gap would frame the theoretical contribution more accurately and highlight clear directions for future theoretical work.5) Score
*   Overall (10): 8 — The paper introduces a novel and important research direction with a strong and comprehensive empirical study, though the presentation and analysis of model-size scaling could be improved.
*   Novelty (10): 9 — This appears to be the first systematic study to establish scaling laws for predictive uncertainty in deep learning, opening a new and valuable area of inquiry.
*   Technical Quality (10): 7 — The empirical work is extensive, but inconsistencies in experimental protocols and an inconclusive investigation into model-size scaling weaken the technical execution.
*   Clarity (10): 7 — The paper is generally well-written, but the presentation could be improved through clearer figure captions and more rigorous interpretations of some surprising results.
*   Confidence (5): 5 — I am highly confident in my assessment, as I have expertise in Bayesian deep learning and scaling laws, and the paper provides sufficient detail to evaluate its claims.