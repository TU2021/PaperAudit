{
  "baseline_review": "Summary\n- The paper investigates whether predictive uncertainty in deep learning exhibits empirical scaling laws as dataset size N (and, to a lesser extent, model size P) varies. It evaluates Total, Aleatoric, and Epistemic uncertainty (TU, AU, EU) defined via entropy-based metrics (Equations 1–4, Section 2.1) across UQ methods (MC Dropout, Gaussian approximations, MCMC, Deep Ensembles, partially stochastic networks; Section 3) and architectures (ResNets, WideResNets, ViT, GPT-2; Sections 4.1–4.2). The study reports power-law trends for uncertainties vs. N in vision and language tasks (e.g., Figure 2, Figure 3, Figure 5, Figure 8, Appendix Figures 9–14), including OOD settings (Figure 6). Theoretically, the paper derives TU contraction and links TU to Watanabe’s generalization error for Bayesian linear models (Section 5.1; Equations 11–12; Appendix A.1–A.2, Equations 22, 31–34). A practical extrapolation example is shown (Appendix Figure 12). The authors note limited evidence for model-size scaling (Figure 7) and resource constraints (Section 6, Limitations).Strengths\n- Bold empirical question: uncertainty scaling laws\n  - The paper poses a clear parallel to performance scaling laws, asking whether TU/AU/EU follow 1/N^γ power-law trends (Section 1, Section 2.2, Equation 5); this frames a novel empirical inquiry in UQ and deep learning.\n  - Multiple figures demonstrate consistent power-law fits for TU/AU across settings (e.g., Figure 2, Figure 3, Figure 5, Figure 8; Appendix Figures 10–11, 13–14), supporting the central question with concrete data.\n  - This question matters for impact and practical utility (Section 1; Summary in Section 6), as extrapolating uncertainty can inform data collection and deployment.- Broad experimental coverage across UQ methods and architectures\n  - Methods include MC Dropout, Gaussian approximations, MCMC, Deep Ensembles, partially stochastic networks (Section 3), addressing both Bayesian and non-Bayesian UQ, demonstrating robustness of observations across inference paradigms.\n  - Architectures span ResNet/WideResNet/ViT (Sections 4.1.1–4.1.2; Figures 2, 5, 7, 8; Appendix Figures 10–11), plus GPT-2 and Phi-2 with Bayesian LoRA (Sections 4.2, 4.2.1; Appendix Figure 15), strengthening modality breadth.\n  - The breadth matters for technical soundness and generality: cross-method, cross-architecture consistency improves confidence that observed laws are not artifacts of a single setup.- Clear and operational uncertainty metrics\n  - TU/AU/EU are rigorously defined via entropies of predictive distributions (Equations 1–3) and aggregated over test sets (Equation 4); this clarity improves reproducibility and interpretability.\n  - The decomposition is acknowledged as imperfect (Section 2.1), yet the paper consistently operationalizes the metrics, enabling cross-setting comparison (e.g., Figures 2–8; Appendix Figures 9–14).\n  - Clear metrics matter for experimental rigor and for future comparative studies.- Consistent empirical evidence of power-law behavior with N\n  - Vision: ResNet MC Dropout shows TU/AU/EU negative exponents (e.g., Figure 2a: ResNet-18 p=0.5 γTU=-0.42, γAU=-0.44, γEU=-0.36; Figure 3 TU γ=-0.48 to -0.66 and AU γ=-0.55 to -0.71).\n  - Ensembles: WideResNets with M=5/10 exhibit γTU≈-0.66 and γAU≈-0.70/0.71; γEU≈-0.56/0.61 (Figure 2b, Figure 19).\n  - Language: GPT-2 on the algorithmic dataset shows steep uncertainty scaling (Figure 4: γTU=-2.86, γAU=-2.82, γEU=-2.95).\n  - Such empirical consistency across domains bolsters novelty and potential impact.- OOD uncertainty scaling results\n  - CIFAR-10-C tests show power-law trends with smaller magnitudes (Figure 6: e.g., TU γResNet-18=-0.18 to -0.19, AU γResNet-18≈-0.24 to -0.25, EU often closer to zero or slightly negative), indicating robustness beyond ID data.\n  - MCMC OOD results for CIFAR-10/CIFAR-100 further support OOD scaling (Appendix Figure 14).\n  - OOD analysis matters for safety-critical deployment and practical relevance.- Optimization sensitivity and ablations\n  - SAM interaction: EU can increase with N under MC Dropout+SAM (Figure 3 EU γResNet-18=0.14; γResNet-50=0.41), highlighting nontrivial optimizer–uncertainty coupling.\n  - ViT training: learning-rate schedule vs. fixed LR produces distinct EU behavior (Figure 5: panel (a) EU γ≈0.01 vs. panel (b) EU γ=-0.35), emphasizing training dynamics’ role.\n  - These ablations add technical insight and caution for practitioners.- Theoretical insights for identifiable models and link to generalization\n  - TU for Bayesian linear regression derived with asymptotic expansion showing EU decays as O(1/N) (Section 5.1; Equations 11–12; Appendix A.1.1, Equation 22).\n  - Formal connection between Watanabe’s generalization error and TU/EU (Section 5.2; Equations 7–10; Appendix A.2, Equations 31–34) clarifies how epistemic uncertainty relates to KL to the true process.\n  - The theoretical bridge matters for conceptual soundness and motivates future theory for deep models.- Practical extrapolation demonstration\n  - Appendix Figure 12 demonstrates extrapolating uncertainty (variance of softmax predictions) to very large N (up to 10B) based on fitted γf, including operational thresholds, illustrating practical utility of scaling laws.\n  - Such extrapolation can inform data planning and stopping criteria.- Transparent limitations and thorough appendices\n  - The Limitations section explicitly notes missing model-size and compute scaling (Section 6, Limitations).\n  - Appendix C provides architecture/hyperparameters for ViT and GPT-2 (Tables 1–2), aiding reproducibility.\n  - Transparency and documentation improve clarity and replicability.Weaknesses\n- Limited statistical rigor in fitting and validating scaling laws\n  - Linear regression on log–log plots is used without goodness-of-fit diagnostics or alternative models (Figure 2, caption: “dashed lines represent linear regressions”; similar in Figures 3, 5, 6; Appendix Figures 10–11, 13), which matters for technical soundness (power-law validity vs. other functional forms).\n  - Small numbers of N points (e.g., 4 subsets in several CIFAR experiments: Figure 2; Appendix Figures 10–11) and occasional single-fold estimates (Figure 2b) reduce reliability of γ estimates, affecting experimental rigor.\n  - Confidence intervals or uncertainty on γ are generally absent (No direct evidence found in the manuscript), undermining inferential robustness.\n  - EU sometimes increases with N (e.g., Figure 3 EU γResNet-50=0.41; Figure 5a EU γ=0.01), yet the paper fits single power-laws without testing for regime changes or piecewise behavior, affecting interpretability.- Incomplete exploration of model-size and compute scaling\n  - Model-size scaling is briefly reported but inconclusive (Figure 7: “Uncertainty scaling with model parameters”), with no deeper analysis or broader architectures; this limits claims about P-scaling patterns (Section 6, Limitations).\n  - Compute scaling trends—central to modern scaling laws—are not studied (Section 6, Limitations), reducing completeness of the scaling law picture.\n  - Ensemble size variation (M=5 vs. M=10) is presented (Figure 2b; Appendix Figure 13) but not analyzed for its effect on γEU beyond reporting values, affecting impact and generality.\n  - Vision-only P-scaling (ResNets only in Figure 7) without transformers or language models on P limits generalization.- Missing critical MCMC and UQ procedural details\n  - For MCMC, chain lengths, burn-in, effective sample sizes, and diagnostics are not reported (Section 3; Section 4.1.1 references priors; Appendix B mentions MCMC (1)/(2)), impacting technical quality and reproducibility.\n  - Prior optimization details are referenced (Section 4.1.1; Tran et al., 2022) but concrete settings and outcomes are not specified in the main text (No direct evidence found in the manuscript), affecting interpretability of MCMC results.\n  - Number of posterior samples per method (MC Dropout passes, ensemble members, MCMC samples) is sometimes implicit (e.g., ensembles M ∈ {5,10} in Section 3) but not consistently stated per experiment, hindering clarity.\n  - Partially stochastic networks rely on pretrained weights (Section 4.1.1; Appendix Figure 14), but training details of the base models are not fully documented for those specific runs, impacting reproducibility.- Ambiguity in epistemic uncertainty interpretation and diagnostics\n  - The paper acknowledges limitations of TU/AU/EU decomposition (Section 2.1) but does not provide alternative metrics or diagnostics (e.g., calibration, mutual information proxies) to corroborate EU trends, affecting interpretability.\n  - EU collapse concerns are mentioned (Section 2.1) but not directly analyzed (No direct evidence found in the manuscript) via ensemble diversity measures or mutual information, limiting insight into epistemic behavior.\n  - EU increases with N in some settings (Figure 3; Figure 5a), but explanations are speculative (Section 4.1.1: SAM intuition) and not empirically validated (e.g., Hessian spectra not measured), impacting technical soundness.\n  - OOD EU trends are often small or near-flat (Figure 6; Appendix Figure 14), yet the paper does not connect these to detection/calibration metrics, limiting practical conclusions.- Variability in replication and folds across experiments\n  - WideResNet ensembles are reported from a single fold (Figure 2b caption), which reduces reliability of the fitted γ compared to the 10-fold protocol used elsewhere (Figure 2a; Appendix Figure 10).\n  - ViT experiments (Figure 5; Appendix Figure 9) do not state the number of folds/runs, affecting experimental rigor.\n  - GPT-2 algorithmic results average over only 3 folds (Section 4.2.1; Figure 4), which is limited for stable γ estimates.\n  - Some ImageNet-32 results are shown without clear replication details (Figure 8), potentially making slopes sensitive to seeds and data subsets.- Theory largely restricted to linear models; deep-net connection is speculative\n  - The main derivations establish O(1/N) EU in Bayesian linear regression (Section 5.1; Equations 11–12; Appendix A.1.1, Equation 22), which does not quantify γ for deep, non-identifiable models, limiting theoretical contribution.\n  - SLT link is framed as speculative (Section 5.2: “We hypothesize… may offer a promising foundation”), with no derivation of γ or effective dimensionality for the deep models tested, affecting novelty and technical depth.\n  - No alignment between measured γ values (e.g., Figure 2, Figure 3, Figure 5) and theoretical predictions is attempted, reducing explanatory power.\n  - The theory does not address optimizer effects (SAM, schedules) that empirically alter EU trends (Figure 3, Figure 5), limiting applicability.- Limited OOD scope and metrics\n  - OOD analysis is mostly CIFAR-10-C and CIFAR-100-C (Figure 6; Appendix Figure 14), which are controlled corruptions rather than diverse domain shifts; this affects impact on real-world settings.\n  - No OOD detection metrics (AUROC/AUPR) or calibration under shift are reported (No direct evidence found in the manuscript), limiting practical assessment.\n  - The OOD γ values are small (Figure 6, EU often near 0), but the paper does not interrogate whether this reflects saturation or method limitations.\n  - Absence of language OOD analyses (Appendix Figure 15 shows flat curves for Phi-2 fine-tuning but no OOD) reduces modality breadth.- Over-generalized conclusion about “so much data is not enough”\n  - The Abstract asserts broad relevance (“‘so much data’ is typically not enough to make epistemic uncertainty negligible,” Abstract Block #2), yet evidence is limited to CIFAR/ImageNet-32, a synthetic algorithmic dataset, and small-scale LLM fine-tuning (Sections 4.1–4.2; Figure 2, Figure 3, Figure 5, Figure 8; Appendix Figure 15), affecting generality.\n  - Many γEU values indicate decay but not direct demonstration of near-zero EU at practical scales (e.g., ResNet EU remains above 10^-2–10^-1 in several plots; Figures 2–3), making the claim context-dependent.\n  - For pre-trained LLM (Phi-2), uncertainties are flat across fine-tuning subsets (Appendix Figure 15), which suggests specific regimes where epistemic changes are minimal, complicating the generalized conclusion.\n  - The extrapolation example (Appendix Figure 12) is useful but not empirically validated at large N; practical thresholds are conceptual, not tested, affecting impact.Suggestions for Improvement\n- Strengthen statistical validation of scaling fits\n  - Complement linear regressions with goodness-of-fit tests and alternative functional forms (e.g., broken power-laws or log-linear) across Figures 2, 3, 5, 6, Appendix Figures 10–11, 13 to ensure power-law validity; report residual analyses and model selection criteria (AIC/BIC).\n  - Increase N resolution beyond four subsets where feasible (e.g., CIFAR/ImageNet-32), and apply bootstrapping over data subsamples and seeds to estimate confidence intervals on γ, improving robustness.\n  - Report uncertainty on γ (CIs or credible intervals) and provide sensitivity analyses to fold counts (contrast 10-fold vs. single-fold cases noted in Figure 2b) to quantify reliability.\n  - Test for regime changes (e.g., EU increases in Figure 3 and Figure 5a) via segmented regression or changepoint analysis to avoid forcing single power-laws when dynamics differ.- Expand model-size and compute scaling analyses\n  - Extend Figure 7 with more architectures (e.g., ViT variants, WideResNets of varying widths/depths) and report γ vs. P curves to probe whether uncertainty scaling depends on capacity.\n  - Include compute scaling experiments (Section 6, Limitations) by varying training epochs, optimizer steps, and training-time budget, analyzing TU/AU/EU behavior with compute.\n  - Analyze the effect of ensemble size M beyond {5,10} (Figure 2b; Appendix Figure 13), quantifying how γEU changes with M and whether ensembles approach epistemic collapse.\n  - Consider cross-modality P-scaling (language models of different sizes) to generalize findings beyond vision.- Provide comprehensive procedural details for MCMC and UQ\n  - Report MCMC settings (chains, warm-up/burn-in, total iterations, thinning, effective sample sizes, diagnostics) for experiments in Figure 8 and Appendix Figure 14 to ensure technical reproducibility.\n  - Detail prior optimization outcomes (Section 4.1.1) and specify priors used per experiment, including hyperparameters (means, variances), to clarify the effect of functional priors vs. global parameter priors.\n  - State the number of stochastic passes (MC Dropout), ensemble members (Deep Ensembles), and posterior samples (MCMC) for each figure, and evaluate sensitivity of γ to these counts.\n  - Describe training of the deterministic backbone for partially stochastic networks (Section 4.1.1; Appendix Figure 14) and assess how the pretrained state influences uncertainty scaling.- Deepen EU interpretation and diagnostics\n  - Complement TU/AU/EU with calibration metrics (ECE/NLL) and ensemble diversity measures (e.g., variance of logits, mutual information proxies), linking EU trends to practical reliability (Figures 2–3, 5–6; Appendix 10–14).\n  - Investigate epistemic collapse directly by tracking ensemble prediction variance and mutual information over N and P, verifying concerns raised in Section 2.1.\n  - Empirically test the SAM-based explanation in Section 4.1.1 by measuring Hessian spectra or sharpness vs. N and correlating with EU trends (Figure 3), replacing speculation with measurements.\n  - For OOD (Figure 6; Appendix Figure 14), include OOD detection performance (AUROC/AUPR) and calibration under shift to connect γEU trends to task-relevant metrics.- Improve replication consistency and reporting\n  - Adopt a uniform multi-fold protocol (e.g., 10 folds) across all settings, replacing single-fold reports noted in Figure 2b, and explicitly report the number of runs/folds for ViT (Figure 5; Appendix Figure 9) and ImageNet-32 (Figure 8).\n  - Increase folds for GPT-2 algorithmic experiments beyond three (Section 4.2.1; Figure 4) and quantify variability of γ across seeds/subsets.\n  - Provide per-figure tables with mean±std (or CIs) for TU/AU/EU and γ, standardizing reporting across Figures 2–8 and Appendix 9–14.\n  - Document data subsampling procedures (random seeds, stratification) to ensure comparability across folds and minimize spurious slopes.- Strengthen theoretical connection to deep models\n  - Extend the linear-model analysis (Section 5.1; Appendix A.1–A.2) by analyzing simplified over-parameterized models (e.g., linearized networks, NTK regimes) to derive γ predictions and compare to measured values (Figures 2–5).\n  - Operationalize SLT (Section 5.2) by estimating local learning coefficients/effective dimensionality (e.g., Lau et al., 2024) for tested models and correlating them with γTU/γEU, moving beyond hypothesis.\n  - Derive how optimizer choices (SAM, schedules) could affect EU via theoretical proxies (e.g., curvature-dependent priors), and test these predictions against Figure 3 and Figure 5.\n  - Provide a quantitative mapping between theoretical rates (O(1/N)) and observed γ across architectures and methods, discussing deviations and their causes.- Broaden OOD scope and metrics\n  - Evaluate diverse OOD settings beyond CIFAR-C (Figure 6; Appendix Figure 14), including domain shifts (e.g., SVHN for CIFAR models) and semantic shifts, and report γTU/γEU across shift types.\n  - Include OOD detection/calibration metrics to complement uncertainty scaling, assessing whether power-law behavior translates to improved detection as N grows.\n  - Explore language OOD (e.g., out-of-domain text corpora) for Phi-2 fine-tuning (Appendix Figure 15) to assess modality breadth.\n  - Analyze whether small γ in OOD reflects saturation or method limitations by varying UQ methods (MC Dropout vs. Ensembles vs. MCMC) under shift.- Calibrate the generality of conclusions about data sufficiency\n  - Rephrase broad claims (Abstract Block #2) to be conditional on the studied tasks and scales; add caveats reflecting flat curves in Phi-2 fine-tuning (Appendix Figure 15) and modest EU magnitudes in many vision settings (Figures 2–3).\n  - Empirically validate extrapolation thresholds in Appendix Figure 12 on larger real datasets (where feasible) to demonstrate practical indistinguishability of ensemble predictions.\n  - Include additional application domains (e.g., detection/segmentation) to test whether EU remains non-negligible across tasks, supporting generality.\n  - Quantify the “numerical precision” threshold proposed in Section 6 by reporting concrete criteria and sensitivity, making extrapolation actionable.Score\n- Overall (10): 7 — Strong empirical breadth and clear uncertainty definitions with consistent power-law observations (Figures 2–6, 8; Equations 1–4), but limited statistical rigor in fitting and theory restricted to linear models (Section 5; Appendix A).\n- Novelty (10): 8 — First systematic study of uncertainty scaling laws across UQ methods and modalities is well motivated (Section 1; Section 6 contributions), with new observations such as optimizer effects (Figure 3) and extrapolation (Appendix Figure 12).\n- Technical Quality (10): 6 — Empirical evidence is abundant (Figures 2–8; Appendix 9–15), but absence of fit diagnostics, sparse N points, and incomplete MCMC details (Section 3; Figure 8; Appendix Figure 14) limit rigor; theory does not predict deep-model γ.\n- Clarity (10): 8 — Metrics and methods are clearly defined (Equations 1–4; Section 3), figures are annotated with γ and captions (Figures 2–8; Appendix 9–14), and limitations are transparently stated (Section 6), though some procedural details (MCMC) are missing.\n- Confidence (5): 4 — The manuscript provides extensive results and clear definitions, enabling a well-informed assessment, but missing fit diagnostics and some procedural details (Figure 2b single-fold; Figure 8 MCMC settings) lower absolute confidence.",
  "final_review": "Summary\n- The paper investigates whether predictive uncertainty in deep learning exhibits empirical scaling laws as dataset size N (and, to a lesser extent, model size P) varies. It evaluates Total, Aleatoric, and Epistemic uncertainty (TU, AU, EU) defined via entropy-based metrics (Equations 1–4, Section 2.1) across UQ methods (MC Dropout, Gaussian approximations, MCMC, Deep Ensembles, partially stochastic networks; Section 3) and architectures (ResNets, WideResNets, ViT, GPT-2; Sections 4.1–4.2). The study reports power-law trends for uncertainties vs. N in vision and language tasks (e.g., Figure 2, Figure 3, Figure 5, Figure 8, Appendix Figures 9–14), including OOD settings (Figure 6). Theoretically, the paper derives TU contraction and links TU to Watanabe’s generalization error for Bayesian linear models (Section 5.1; Equations 11–12; Appendix A.1–A.2, Equations 22, 31–34). A practical extrapolation example is shown (Appendix Figure 12). The authors note limited evidence for model-size scaling (Figure 7) and resource constraints (Section 6, Limitations).Strengths\n- Bold empirical question: uncertainty scaling laws\n  - The paper poses a clear parallel to performance scaling laws, asking whether TU/AU/EU follow 1/N^γ power-law trends (Section 1, Section 2.2, Equation 5); this frames a novel empirical inquiry in UQ and deep learning.\n  - Multiple figures demonstrate consistent power-law fits for TU/AU across settings (e.g., Figure 2, Figure 3, Figure 5, Figure 8; Appendix Figures 10–11, 13–14), supporting the central question with concrete data.\n  - This question matters for impact and practical utility (Section 1; Summary in Section 6), as extrapolating uncertainty can inform data collection and deployment.\n- Broad experimental coverage across UQ methods and architectures\n  - Methods include MC Dropout, Gaussian approximations, MCMC, Deep Ensembles, partially stochastic networks (Section 3), addressing both Bayesian and non-Bayesian UQ, demonstrating robustness of observations across inference paradigms.\n  - Architectures span ResNet/WideResNet/ViT (Sections 4.1.1–4.1.2; Figures 2, 5, 7, 8; Appendix Figures 10–11), plus GPT-2 and Phi-2 with Bayesian LoRA (Sections 4.2, 4.2.1; Appendix Figure 15), strengthening modality breadth.\n  - The breadth matters for technical soundness and generality: cross-method, cross-architecture consistency improves confidence that observed laws are not artifacts of a single setup.\n- Clear and operational uncertainty metrics\n  - TU/AU/EU are rigorously defined via entropies of predictive distributions (Equations 1–3) and aggregated over test sets (Equation 4); this clarity improves reproducibility and interpretability.\n  - The decomposition is acknowledged as imperfect (Section 2.1), yet the paper consistently operationalizes the metrics, enabling cross-setting comparison (e.g., Figures 2–8; Appendix Figures 9–14).\n  - Clear metrics matter for experimental rigor and for future comparative studies.\n- Consistent empirical evidence of power-law behavior with N\n  - Vision: ResNet MC Dropout shows TU/AU/EU negative exponents (e.g., Figure 2a: ResNet-18 p=0.5 γTU=-0.42, γAU=-0.44, γEU=-0.36; Figure 3 TU γ=-0.48 to -0.66 and AU γ=-0.55 to -0.79).\n  - Ensembles: WideResNets with M=5/10 exhibit γTU≈-0.66 and γAU≈-0.70/0.71; γEU≈-0.55 to -0.61 (Figure 2b; Appendix Figure 13).\n  - Language: GPT-2 on the algorithmic dataset shows steep uncertainty scaling (Figure 4: γTU=-2.86, γAU=-2.82, γEU=-2.95).\n  - Such empirical consistency across domains bolsters novelty and potential impact.\n- OOD uncertainty scaling results\n  - CIFAR-10-C tests show power-law trends with smaller magnitudes (Figure 6: e.g., TU γResNet-18=-0.18 to -0.19, AU γResNet-18≈-0.24 to -0.25, EU often closer to zero or slightly negative), indicating robustness beyond ID data.\n  - MCMC OOD results for CIFAR-10/CIFAR-100 further support OOD scaling (Appendix Figure 14).\n  - OOD analysis matters for safety-critical deployment and practical relevance.\n- Optimization sensitivity and ablations\n  - SAM interaction: EU can increase with N under MC Dropout+SAM (Figure 3 EU γResNet-18=0.14; γResNet-50=0.41), highlighting nontrivial optimizer–uncertainty coupling.\n  - ViT training: learning-rate schedule vs. fixed LR produces distinct EU behavior (Figure 5: panel (a) EU γ≈0.01 vs. panel (b) EU γ=-0.35), emphasizing training dynamics’ role.\n  - These ablations add technical insight and caution for practitioners.\n- Theoretical insights for identifiable models and link to generalization\n  - TU for Bayesian linear regression derived with asymptotic expansion showing EU decays as O(1/N) (Section 5.1; Equations 11–12; Appendix A.1.1, Equation 22).\n  - Formal connection between Watanabe’s generalization error and TU/EU (Section 5.2; Equations 7–10; Appendix A.2, Equations 31–34) clarifies how epistemic uncertainty relates to KL to the true process.\n  - The theoretical bridge matters for conceptual soundness and motivates future theory for deep models.\n- Practical extrapolation demonstration\n  - Appendix Figure 12 demonstrates extrapolating uncertainty (variance of softmax predictions) to very large N (up to 10B) based on fitted γf, including operational thresholds, illustrating practical utility of scaling laws.\n  - Such extrapolation can inform data planning and stopping criteria.\n- Transparent limitations and thorough appendices\n  - The Limitations section explicitly notes missing model-size and compute scaling (Section 6, Limitations).\n  - Appendix C provides architecture/hyperparameters for ViT and GPT-2 (Tables 1–2), aiding reproducibility.\n  - Transparency and documentation improve clarity and replicability.Weaknesses\n- Limited statistical rigor in fitting and validating scaling laws\n  - Linear regression on log–log plots is used without goodness-of-fit diagnostics or alternative models (Figure 2, caption: “dashed lines represent linear regressions”; similar in Figures 3, 5, 6; Appendix Figures 10–11, 13), which matters for technical soundness (power-law validity vs. other functional forms).\n  - Small numbers of N points (e.g., 4 subsets in several CIFAR experiments: Figure 2; Appendix Figures 10–11) and occasional single-fold estimates (Figure 2b) reduce reliability of γ estimates, affecting experimental rigor.\n  - Confidence intervals or uncertainty on γ are generally absent (No direct evidence found in the manuscript), undermining inferential robustness.\n  - EU sometimes increases with N (e.g., Figure 3 EU γResNet-50=0.41; Figure 5a EU γ=0.01), yet the paper fits single power-laws without testing for regime changes or piecewise behavior, affecting interpretability.\n  - There is an internal inconsistency between the “power-law decay of the form 1/N^{γ}” stated in captions (Figure 2 caption, Section 4.1.1) and the reported negative γ values alongside decreasing curves (e.g., Figure 2a; Figure 5b; Appendix Figures 10–11), which complicates quantitative interpretation and extrapolation.\n- Incomplete exploration of model-size and compute scaling\n  - Model-size scaling is briefly reported but inconclusive (Figure 7: “Uncertainty scaling with model parameters”), with no deeper analysis or broader architectures; this limits claims about P-scaling patterns (Section 6, Limitations).\n  - Compute scaling trends—central to modern scaling laws—are not studied (Section 6, Limitations), reducing completeness of the scaling law picture.\n  - Ensemble size variation (M=5 vs. M=10) is presented (Figure 2b; Appendix Figure 13) but not analyzed for its effect on γEU beyond reporting values, affecting impact and generality.\n  - Vision-only P-scaling (ResNets only in Figure 7) without transformers or language models on P limits generalization.\n- Missing critical MCMC and UQ procedural details\n  - For MCMC, chain lengths, burn-in, effective sample sizes, and diagnostics are not reported (Section 3; Section 4.1.1 references priors; Appendix B mentions MCMC (1)/(2)), impacting technical quality and reproducibility.\n  - Prior optimization details are referenced (Section 4.1.1; Tran et al., 2022) but concrete settings and outcomes are not specified in the main text (No direct evidence found in the manuscript), affecting interpretability of MCMC results.\n  - Number of posterior samples per method (MC Dropout passes, ensemble members, MCMC samples) is sometimes implicit (e.g., ensembles M ∈ {5,10} in Section 3) but not consistently stated per experiment, hindering clarity.\n  - Partially stochastic networks rely on pretrained weights (Section 4.1.1; Appendix Figure 14), but training details of the base models are not fully documented for those specific runs, impacting reproducibility.\n- Ambiguity in epistemic uncertainty interpretation and diagnostics\n  - The paper acknowledges limitations of TU/AU/EU decomposition (Section 2.1) but does not provide alternative metrics or diagnostics (e.g., calibration, mutual information proxies) to corroborate EU trends, affecting interpretability.\n  - EU collapse concerns are mentioned (Section 2.1) but not directly analyzed (No direct evidence found in the manuscript) via ensemble diversity measures or mutual information, limiting insight into epistemic behavior.\n  - EU increases with N in some settings (Figure 3; Figure 5a), but explanations are speculative (Section 4.1.1: SAM intuition) and not empirically validated (e.g., Hessian spectra not measured), impacting technical soundness.\n  - OOD EU trends are often small or near-flat (Figure 6; Appendix Figure 14), yet the paper does not connect these to detection/calibration metrics, limiting practical conclusions.\n- Variability in replication and folds across experiments\n  - WideResNet ensembles are reported from a single fold (Figure 2b caption), which reduces reliability of the fitted γ compared to the 10-fold protocol used elsewhere (Figure 2a; Appendix Figure 10).\n  - ViT experiments (Figure 5; Appendix Figure 9) do not state the number of folds/runs, affecting experimental rigor.\n  - GPT-2 algorithmic results average over only 3 folds (Section 4.2.1; Figure 4), which is limited for stable γ estimates.\n  - Some ImageNet-32 results are shown without clear replication details (Figure 8), potentially making slopes sensitive to seeds and data subsets.\n- Theory largely restricted to linear models; deep-net connection is speculative\n  - The main derivations establish O(1/N) EU in Bayesian linear regression (Section 5.1; Equations 11–12; Appendix A.1.1, Equation 22), which does not quantify γ for deep, non-identifiable models, limiting theoretical contribution.\n  - SLT link is framed as speculative (Section 5.2: “We hypothesize… may offer a promising foundation”), with no derivation of γ or effective dimensionality for the deep models tested, affecting novelty and technical depth.\n  - No alignment between measured γ values (e.g., Figure 2, Figure 3, Figure 5) and theoretical predictions is attempted, reducing explanatory power.\n  - The theory does not address optimizer effects (SAM, schedules) that empirically alter EU trends (Figure 3, Figure 5), limiting applicability.\n  - An inconsistency appears between the generalization error decomposition in the main text and the appendix: Eq. (10) presents a negative sum, whereas Appendix Eq. (31) gives a positive sum (Section 5.2; Appendix A.2), creating confusion about the theoretical link.\n- Limited OOD scope and metrics\n  - OOD analysis is mostly CIFAR-10-C and CIFAR-100-C (Figure 6; Appendix Figure 14), which are controlled corruptions rather than diverse domain shifts; this affects impact on real-world settings.\n  - No OOD detection metrics (AUROC/AUPR) or calibration under shift are reported (No direct evidence found in the manuscript), limiting practical assessment.\n  - The OOD γ values are small (Figure 6, EU often near 0), but the paper does not interrogate whether this reflects saturation or method limitations.\n  - Absence of language OOD analyses (Appendix Figure 15 shows flat curves for Phi-2 fine-tuning but no OOD) reduces modality breadth.\n- Over-generalized conclusion about “so much data is not enough”\n  - The Abstract asserts broad relevance (“‘so much data’ is typically not enough to make epistemic uncertainty negligible,” Abstract Block #2), yet evidence is limited to CIFAR/ImageNet-32, a synthetic algorithmic dataset, and small-scale LLM fine-tuning (Sections 4.1–4.2; Figure 2, Figure 3, Figure 5, Figure 8; Appendix Figure 15), affecting generality.\n  - Many γEU values indicate decay but not direct demonstration of near-zero EU at practical scales (e.g., ResNet EU remains above 10^-2–10^-1 in several plots; Figures 2–3), making the claim context-dependent.\n  - For pre-trained LLM (Phi-2), uncertainties are flat across fine-tuning subsets (Appendix Figure 15), which suggests specific regimes where epistemic changes are minimal, complicating the generalized conclusion.\n  - The extrapolation example (Appendix Figure 12) is useful but not empirically validated at large N; practical thresholds are conceptual, not tested, affecting impact.Suggestions for Improvement\n- Strengthen statistical validation of scaling fits\n  - Complement linear regressions with goodness-of-fit tests and alternative functional forms (e.g., broken power-laws or log-linear) across Figures 2, 3, 5, 6, Appendix Figures 10–11, 13 to ensure power-law validity; report residual analyses and model selection criteria (AIC/BIC).\n  - Increase N resolution beyond four subsets where feasible (e.g., CIFAR/ImageNet-32), and apply bootstrapping over data subsamples and seeds to estimate confidence intervals on γ, improving robustness.\n  - Report uncertainty on γ (CIs or credible intervals) and provide sensitivity analyses to fold counts (contrast 10-fold vs. single-fold cases noted in Figure 2b) to quantify reliability.\n  - Test for regime changes (e.g., EU increases in Figure 3 and Figure 5a) via segmented regression or changepoint analysis to avoid forcing single power-laws when dynamics differ.\n  - Clarify and unify the scaling-law parameterization: ensure that the functional form and sign convention (e.g., Unc ∝ N^{γ} with γ<0 or Unc ∝ 1/N^{γ} with γ>0) match the plotted trends and captions (e.g., Figure 2 caption) to support consistent extrapolation.\n- Expand model-size and compute scaling analyses\n  - Extend Figure 7 with more architectures (e.g., ViT variants, WideResNets of varying widths/depths) and report γ vs. P curves to probe whether uncertainty scaling depends on capacity.\n  - Include compute scaling experiments (Section 6, Limitations) by varying training epochs, optimizer steps, and training-time budget, analyzing TU/AU/EU behavior with compute.\n  - Analyze the effect of ensemble size M beyond {5,10} (Figure 2b; Appendix Figure 13), quantifying how γEU changes with M and whether ensembles approach epistemic collapse.\n  - Consider cross-modality P-scaling (language models of different sizes) to generalize findings beyond vision.\n- Provide comprehensive procedural details for MCMC and UQ\n  - Report MCMC settings (chains, warm-up/burn-in, total iterations, thinning, effective sample sizes, diagnostics) for experiments in Figure 8 and Appendix Figure 14 to ensure technical reproducibility.\n  - Detail prior optimization outcomes (Section 4.1.1) and specify priors used per experiment, including hyperparameters (means, variances), to clarify the effect of functional priors vs. global parameter priors.\n  - State the number of stochastic passes (MC Dropout), ensemble members (Deep Ensembles), and posterior samples (MCMC) for each figure, and evaluate sensitivity of γ to these counts.\n  - Describe training of the deterministic backbone for partially stochastic networks (Section 4.1.1; Appendix Figure 14) and assess how the pretrained state influences uncertainty scaling.\n- Deepen EU interpretation and diagnostics\n  - Complement TU/AU/EU with calibration metrics (ECE/NLL) and ensemble diversity measures (e.g., variance of logits, mutual information proxies), linking EU trends to practical reliability (Figures 2–3, 5–6; Appendix 10–14).\n  - Investigate epistemic collapse directly by tracking ensemble prediction variance and mutual information over N and P, verifying concerns raised in Section 2.1.\n  - Empirically test the SAM-based explanation in Section 4.1.1 by measuring Hessian spectra or sharpness vs. N and correlating with EU trends (Figure 3), replacing speculation with measurements.\n  - For OOD (Figure 6; Appendix Figure 14), include OOD detection performance (AUROC/AUPR) and calibration under shift to connect γEU trends to task-relevant metrics.\n- Improve replication consistency and reporting\n  - Adopt a uniform multi-fold protocol (e.g., 10 folds) across all settings, replacing single-fold reports noted in Figure 2b, and explicitly report the number of runs/folds for ViT (Figure 5; Appendix Figure 9) and ImageNet-32 (Figure 8).\n  - Increase folds for GPT-2 algorithmic experiments beyond three (Section 4.2.1; Figure 4) and quantify variability of γ across seeds/subsets.\n  - Provide per-figure tables with mean±std (or CIs) for TU/AU/EU and γ, standardizing reporting across Figures 2–8 and Appendix 9–14.\n  - Document data subsampling procedures (random seeds, stratification) to ensure comparability across folds and minimize spurious slopes.\n- Strengthen theoretical connection to deep models\n  - Extend the linear-model analysis (Section 5.1; Appendix A.1–A.2) by analyzing simplified over-parameterized models (e.g., linearized networks, NTK regimes) to derive γ predictions and compare to measured values (Figures 2–5).\n  - Operationalize SLT (Section 5.2) by estimating local learning coefficients/effective dimensionality (e.g., Lau et al., 2024) for tested models and correlating them with γTU/γEU, moving beyond hypothesis.\n  - Derive how optimizer choices (SAM, schedules) could affect EU via theoretical proxies (e.g., curvature-dependent priors), and test these predictions against Figure 3 and Figure 5.\n  - Provide a quantitative mapping between theoretical rates (O(1/N)) and observed γ across architectures and methods, discussing deviations and their causes.\n  - Correct the generalization error decomposition in the main text to align with Appendix A.2 (Eq. 31), resolving the sign inconsistency with Eq. (10) and clarifying the TU–generalization link.\n- Broaden OOD scope and metrics\n  - Evaluate diverse OOD settings beyond CIFAR-C (Figure 6; Appendix Figure 14), including domain shifts (e.g., SVHN for CIFAR models) and semantic shifts, and report γTU/γEU across shift types.\n  - Include OOD detection/calibration metrics to complement uncertainty scaling, assessing whether power-law behavior translates to improved detection as N grows.\n  - Explore language OOD (e.g., out-of-domain text corpora) for Phi-2 fine-tuning (Appendix Figure 15) to assess modality breadth.\n  - Analyze whether small γ in OOD reflects saturation or method limitations by varying UQ methods (MC Dropout vs. Ensembles vs. MCMC) under shift.\n- Calibrate the generality of conclusions about data sufficiency\n  - Rephrase broad claims (Abstract Block #2) to be conditional on the studied tasks and scales; add caveats reflecting flat curves in Phi-2 fine-tuning (Appendix Figure 15) and modest EU magnitudes in many vision settings (Figures 2–3).\n  - Empirically validate extrapolation thresholds in Appendix Figure 12 on larger real datasets (where feasible) to demonstrate practical indistinguishability of ensemble predictions.\n  - Include additional application domains (e.g., detection/segmentation) to test whether EU remains non-negligible across tasks, supporting generality.\n  - Quantify the “numerical precision” threshold proposed in Section 6 by reporting concrete criteria and sensitivity, making extrapolation actionable.Score\n- Overall (10): 7 — Strong empirical breadth and clear uncertainty definitions with consistent power-law observations (Figures 2–6, 8; Equations 1–4), but limited fit diagnostics and theory confined to linear models (Section 5; Appendix A), with additional inconsistencies in scaling-law notation and generalization error signs (Figure 2 caption; Eq. 10 vs. Appendix Eq. 31).\n- Novelty (10): 8 — First systematic study of uncertainty scaling laws across UQ methods and modalities is well motivated (Section 1; Section 6 contributions), with new observations such as optimizer effects (Figure 3) and extrapolation (Appendix Figure 12).\n- Technical Quality (10): 6 — Empirical evidence is abundant (Figures 2–8; Appendix 9–15), but absence of fit diagnostics, sparse N points, and incomplete MCMC details (Section 3; Figure 8; Appendix Figure 14) limit rigor; theoretical presentation includes a sign inconsistency in Eq. (10) relative to Appendix Eq. (31).\n- Clarity (10): 7 — Metrics and methods are clearly defined (Equations 1–4; Section 3) with well-annotated figures (Figures 2–8; Appendix 9–14) and transparent limitations (Section 6), though inconsistent scaling-law notation in captions (Figure 2) and the Eq. (10) vs. Appendix Eq. (31) mismatch hinder readability.\n- Confidence (5): 4 — The manuscript provides extensive results and clear definitions, enabling a well-informed assessment, but missing fit diagnostics and some procedural details (Figure 2b single-fold; Figure 8 MCMC settings) lower absolute confidence.",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 7,
        "novelty": 8,
        "technical_quality": 6,
        "clarity": 8,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 7,
        "novelty": 8,
        "technical_quality": 6,
        "clarity": 7,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "Summary\n- The paper investigates whether predictive uncertainty in deep learning exhibits empirical scaling laws as dataset size N (and, to a lesser extent, model size P) varies. It evaluates Total, Aleatoric, and Epistemic uncertainty (TU, AU, EU) defined via entropy-based metrics (Equations 1–4, Section 2.1) across UQ methods (MC Dropout, Gaussian approximations, MCMC, Deep Ensembles, partially stochastic networks; Section 3) and architectures (ResNets, WideResNets, ViT, GPT-2; Sections 4.1–4.2). The study reports power-law trends for uncertainties vs. N in vision and language tasks (e.g., Figure 2, Figure 3, Figure 5, Figure 8, Appendix Figures 9–14), including OOD settings (Figure 6). Theoretically, the paper derives TU contraction and links TU to Watanabe’s generalization error for Bayesian linear models (Section 5.1; Equations 11–12; Appendix A.1–A.2, Equations 22, 31–34). A practical extrapolation example is shown (Appendix Figure 12). The authors note limited evidence for model-size scaling (Figure 7) and resource constraints (Section 6, Limitations).Strengths\n- Bold empirical question: uncertainty scaling laws\n  - The paper poses a clear parallel to performance scaling laws, asking whether TU/AU/EU follow 1/N^γ power-law trends (Section 1, Section 2.2, Equation 5); this frames a novel empirical inquiry in UQ and deep learning.\n  - Multiple figures demonstrate consistent power-law fits for TU/AU across settings (e.g., Figure 2, Figure 3, Figure 5, Figure 8; Appendix Figures 10–11, 13–14), supporting the central question with concrete data.\n  - This question matters for impact and practical utility (Section 1; Summary in Section 6), as extrapolating uncertainty can inform data collection and deployment.\n- Broad experimental coverage across UQ methods and architectures\n  - Methods include MC Dropout, Gaussian approximations, MCMC, Deep Ensembles, partially stochastic networks (Section 3), addressing both Bayesian and non-Bayesian UQ, demonstrating robustness of observations across inference paradigms.\n  - Architectures span ResNet/WideResNet/ViT (Sections 4.1.1–4.1.2; Figures 2, 5, 7, 8; Appendix Figures 10–11), plus GPT-2 and Phi-2 with Bayesian LoRA (Sections 4.2, 4.2.1; Appendix Figure 15), strengthening modality breadth.\n  - The breadth matters for technical soundness and generality: cross-method, cross-architecture consistency improves confidence that observed laws are not artifacts of a single setup.\n- Clear and operational uncertainty metrics\n  - TU/AU/EU are rigorously defined via entropies of predictive distributions (Equations 1–3) and aggregated over test sets (Equation 4); this clarity improves reproducibility and interpretability.\n  - The decomposition is acknowledged as imperfect (Section 2.1), yet the paper consistently operationalizes the metrics, enabling cross-setting comparison (e.g., Figures 2–8; Appendix Figures 9–14).\n  - Clear metrics matter for experimental rigor and for future comparative studies.\n- Consistent empirical evidence of power-law behavior with N\n  - Vision: ResNet MC Dropout shows TU/AU/EU negative exponents (e.g., Figure 2a: ResNet-18 p=0.5 γTU=-0.42, γAU=-0.44, γEU=-0.36; Figure 3 TU γ=-0.48 to -0.66 and AU γ=-0.55 to -0.79).\n  - Ensembles: WideResNets with M=5/10 exhibit γTU≈-0.66 and γAU≈-0.70/0.71; γEU≈-0.55 to -0.61 (Figure 2b; Appendix Figure 13).\n  - Language: GPT-2 on the algorithmic dataset shows steep uncertainty scaling (Figure 4: γTU=-2.86, γAU=-2.82, γEU=-2.95).\n  - Such empirical consistency across domains bolsters novelty and potential impact.\n- OOD uncertainty scaling results\n  - CIFAR-10-C tests show power-law trends with smaller magnitudes (Figure 6: e.g., TU γResNet-18=-0.18 to -0.19, AU γResNet-18≈-0.24 to -0.25, EU often closer to zero or slightly negative), indicating robustness beyond ID data.\n  - MCMC OOD results for CIFAR-10/CIFAR-100 further support OOD scaling (Appendix Figure 14).\n  - OOD analysis matters for safety-critical deployment and practical relevance.\n- Optimization sensitivity and ablations\n  - SAM interaction: EU can increase with N under MC Dropout+SAM (Figure 3 EU γResNet-18=0.14; γResNet-50=0.41), highlighting nontrivial optimizer–uncertainty coupling.\n  - ViT training: learning-rate schedule vs. fixed LR produces distinct EU behavior (Figure 5: panel (a) EU γ≈0.01 vs. panel (b) EU γ=-0.35), emphasizing training dynamics’ role.\n  - These ablations add technical insight and caution for practitioners.\n- Theoretical insights for identifiable models and link to generalization\n  - TU for Bayesian linear regression derived with asymptotic expansion showing EU decays as O(1/N) (Section 5.1; Equations 11–12; Appendix A.1.1, Equation 22).\n  - Formal connection between Watanabe’s generalization error and TU/EU (Section 5.2; Equations 7–10; Appendix A.2, Equations 31–34) clarifies how epistemic uncertainty relates to KL to the true process.\n  - The theoretical bridge matters for conceptual soundness and motivates future theory for deep models.\n- Practical extrapolation demonstration\n  - Appendix Figure 12 demonstrates extrapolating uncertainty (variance of softmax predictions) to very large N (up to 10B) based on fitted γf, including operational thresholds, illustrating practical utility of scaling laws.\n  - Such extrapolation can inform data planning and stopping criteria.\n- Transparent limitations and thorough appendices\n  - The Limitations section explicitly notes missing model-size and compute scaling (Section 6, Limitations).\n  - Appendix C provides architecture/hyperparameters for ViT and GPT-2 (Tables 1–2), aiding reproducibility.\n  - Transparency and documentation improve clarity and replicability.Weaknesses\n- Limited statistical rigor in fitting and validating scaling laws\n  - Linear regression on log–log plots is used without goodness-of-fit diagnostics or alternative models (Figure 2, caption: “dashed lines represent linear regressions”; similar in Figures 3, 5, 6; Appendix Figures 10–11, 13), which matters for technical soundness (power-law validity vs. other functional forms).\n  - Small numbers of N points (e.g., 4 subsets in several CIFAR experiments: Figure 2; Appendix Figures 10–11) and occasional single-fold estimates (Figure 2b) reduce reliability of γ estimates, affecting experimental rigor.\n  - Confidence intervals or uncertainty on γ are generally absent (No direct evidence found in the manuscript), undermining inferential robustness.\n  - EU sometimes increases with N (e.g., Figure 3 EU γResNet-50=0.41; Figure 5a EU γ=0.01), yet the paper fits single power-laws without testing for regime changes or piecewise behavior, affecting interpretability.\n  - There is an internal inconsistency between the “power-law decay of the form 1/N^{γ}” stated in captions (Figure 2 caption, Section 4.1.1) and the reported negative γ values alongside decreasing curves (e.g., Figure 2a; Figure 5b; Appendix Figures 10–11), which complicates quantitative interpretation and extrapolation.\n- Incomplete exploration of model-size and compute scaling\n  - Model-size scaling is briefly reported but inconclusive (Figure 7: “Uncertainty scaling with model parameters”), with no deeper analysis or broader architectures; this limits claims about P-scaling patterns (Section 6, Limitations).\n  - Compute scaling trends—central to modern scaling laws—are not studied (Section 6, Limitations), reducing completeness of the scaling law picture.\n  - Ensemble size variation (M=5 vs. M=10) is presented (Figure 2b; Appendix Figure 13) but not analyzed for its effect on γEU beyond reporting values, affecting impact and generality.\n  - Vision-only P-scaling (ResNets only in Figure 7) without transformers or language models on P limits generalization.\n- Missing critical MCMC and UQ procedural details\n  - For MCMC, chain lengths, burn-in, effective sample sizes, and diagnostics are not reported (Section 3; Section 4.1.1 references priors; Appendix B mentions MCMC (1)/(2)), impacting technical quality and reproducibility.\n  - Prior optimization details are referenced (Section 4.1.1; Tran et al., 2022) but concrete settings and outcomes are not specified in the main text (No direct evidence found in the manuscript), affecting interpretability of MCMC results.\n  - Number of posterior samples per method (MC Dropout passes, ensemble members, MCMC samples) is sometimes implicit (e.g., ensembles M ∈ {5,10} in Section 3) but not consistently stated per experiment, hindering clarity.\n  - Partially stochastic networks rely on pretrained weights (Section 4.1.1; Appendix Figure 14), but training details of the base models are not fully documented for those specific runs, impacting reproducibility.\n- Ambiguity in epistemic uncertainty interpretation and diagnostics\n  - The paper acknowledges limitations of TU/AU/EU decomposition (Section 2.1) but does not provide alternative metrics or diagnostics (e.g., calibration, mutual information proxies) to corroborate EU trends, affecting interpretability.\n  - EU collapse concerns are mentioned (Section 2.1) but not directly analyzed (No direct evidence found in the manuscript) via ensemble diversity measures or mutual information, limiting insight into epistemic behavior.\n  - EU increases with N in some settings (Figure 3; Figure 5a), but explanations are speculative (Section 4.1.1: SAM intuition) and not empirically validated (e.g., Hessian spectra not measured), impacting technical soundness.\n  - OOD EU trends are often small or near-flat (Figure 6; Appendix Figure 14), yet the paper does not connect these to detection/calibration metrics, limiting practical conclusions.\n- Variability in replication and folds across experiments\n  - WideResNet ensembles are reported from a single fold (Figure 2b caption), which reduces reliability of the fitted γ compared to the 10-fold protocol used elsewhere (Figure 2a; Appendix Figure 10).\n  - ViT experiments (Figure 5; Appendix Figure 9) do not state the number of folds/runs, affecting experimental rigor.\n  - GPT-2 algorithmic results average over only 3 folds (Section 4.2.1; Figure 4), which is limited for stable γ estimates.\n  - Some ImageNet-32 results are shown without clear replication details (Figure 8), potentially making slopes sensitive to seeds and data subsets.\n- Theory largely restricted to linear models; deep-net connection is speculative\n  - The main derivations establish O(1/N) EU in Bayesian linear regression (Section 5.1; Equations 11–12; Appendix A.1.1, Equation 22), which does not quantify γ for deep, non-identifiable models, limiting theoretical contribution.\n  - SLT link is framed as speculative (Section 5.2: “We hypothesize… may offer a promising foundation”), with no derivation of γ or effective dimensionality for the deep models tested, affecting novelty and technical depth.\n  - No alignment between measured γ values (e.g., Figure 2, Figure 3, Figure 5) and theoretical predictions is attempted, reducing explanatory power.\n  - The theory does not address optimizer effects (SAM, schedules) that empirically alter EU trends (Figure 3, Figure 5), limiting applicability.\n  - An inconsistency appears between the generalization error decomposition in the main text and the appendix: Eq. (10) presents a negative sum, whereas Appendix Eq. (31) gives a positive sum (Section 5.2; Appendix A.2), creating confusion about the theoretical link.\n- Limited OOD scope and metrics\n  - OOD analysis is mostly CIFAR-10-C and CIFAR-100-C (Figure 6; Appendix Figure 14), which are controlled corruptions rather than diverse domain shifts; this affects impact on real-world settings.\n  - No OOD detection metrics (AUROC/AUPR) or calibration under shift are reported (No direct evidence found in the manuscript), limiting practical assessment.\n  - The OOD γ values are small (Figure 6, EU often near 0), but the paper does not interrogate whether this reflects saturation or method limitations.\n  - Absence of language OOD analyses (Appendix Figure 15 shows flat curves for Phi-2 fine-tuning but no OOD) reduces modality breadth.\n- Over-generalized conclusion about “so much data is not enough”\n  - The Abstract asserts broad relevance (“‘so much data’ is typically not enough to make epistemic uncertainty negligible,” Abstract Block #2), yet evidence is limited to CIFAR/ImageNet-32, a synthetic algorithmic dataset, and small-scale LLM fine-tuning (Sections 4.1–4.2; Figure 2, Figure 3, Figure 5, Figure 8; Appendix Figure 15), affecting generality.\n  - Many γEU values indicate decay but not direct demonstration of near-zero EU at practical scales (e.g., ResNet EU remains above 10^-2–10^-1 in several plots; Figures 2–3), making the claim context-dependent.\n  - For pre-trained LLM (Phi-2), uncertainties are flat across fine-tuning subsets (Appendix Figure 15), which suggests specific regimes where epistemic changes are minimal, complicating the generalized conclusion.\n  - The extrapolation example (Appendix Figure 12) is useful but not empirically validated at large N; practical thresholds are conceptual, not tested, affecting impact.Suggestions for Improvement\n- Strengthen statistical validation of scaling fits\n  - Complement linear regressions with goodness-of-fit tests and alternative functional forms (e.g., broken power-laws or log-linear) across Figures 2, 3, 5, 6, Appendix Figures 10–11, 13 to ensure power-law validity; report residual analyses and model selection criteria (AIC/BIC).\n  - Increase N resolution beyond four subsets where feasible (e.g., CIFAR/ImageNet-32), and apply bootstrapping over data subsamples and seeds to estimate confidence intervals on γ, improving robustness.\n  - Report uncertainty on γ (CIs or credible intervals) and provide sensitivity analyses to fold counts (contrast 10-fold vs. single-fold cases noted in Figure 2b) to quantify reliability.\n  - Test for regime changes (e.g., EU increases in Figure 3 and Figure 5a) via segmented regression or changepoint analysis to avoid forcing single power-laws when dynamics differ.\n  - Clarify and unify the scaling-law parameterization: ensure that the functional form and sign convention (e.g., Unc ∝ N^{γ} with γ<0 or Unc ∝ 1/N^{γ} with γ>0) match the plotted trends and captions (e.g., Figure 2 caption) to support consistent extrapolation.\n- Expand model-size and compute scaling analyses\n  - Extend Figure 7 with more architectures (e.g., ViT variants, WideResNets of varying widths/depths) and report γ vs. P curves to probe whether uncertainty scaling depends on capacity.\n  - Include compute scaling experiments (Section 6, Limitations) by varying training epochs, optimizer steps, and training-time budget, analyzing TU/AU/EU behavior with compute.\n  - Analyze the effect of ensemble size M beyond {5,10} (Figure 2b; Appendix Figure 13), quantifying how γEU changes with M and whether ensembles approach epistemic collapse.\n  - Consider cross-modality P-scaling (language models of different sizes) to generalize findings beyond vision.\n- Provide comprehensive procedural details for MCMC and UQ\n  - Report MCMC settings (chains, warm-up/burn-in, total iterations, thinning, effective sample sizes, diagnostics) for experiments in Figure 8 and Appendix Figure 14 to ensure technical reproducibility.\n  - Detail prior optimization outcomes (Section 4.1.1) and specify priors used per experiment, including hyperparameters (means, variances), to clarify the effect of functional priors vs. global parameter priors.\n  - State the number of stochastic passes (MC Dropout), ensemble members (Deep Ensembles), and posterior samples (MCMC) for each figure, and evaluate sensitivity of γ to these counts.\n  - Describe training of the deterministic backbone for partially stochastic networks (Section 4.1.1; Appendix Figure 14) and assess how the pretrained state influences uncertainty scaling.\n- Deepen EU interpretation and diagnostics\n  - Complement TU/AU/EU with calibration metrics (ECE/NLL) and ensemble diversity measures (e.g., variance of logits, mutual information proxies), linking EU trends to practical reliability (Figures 2–3, 5–6; Appendix 10–14).\n  - Investigate epistemic collapse directly by tracking ensemble prediction variance and mutual information over N and P, verifying concerns raised in Section 2.1.\n  - Empirically test the SAM-based explanation in Section 4.1.1 by measuring Hessian spectra or sharpness vs. N and correlating with EU trends (Figure 3), replacing speculation with measurements.\n  - For OOD (Figure 6; Appendix Figure 14), include OOD detection performance (AUROC/AUPR) and calibration under shift to connect γEU trends to task-relevant metrics.\n- Improve replication consistency and reporting\n  - Adopt a uniform multi-fold protocol (e.g., 10 folds) across all settings, replacing single-fold reports noted in Figure 2b, and explicitly report the number of runs/folds for ViT (Figure 5; Appendix Figure 9) and ImageNet-32 (Figure 8).\n  - Increase folds for GPT-2 algorithmic experiments beyond three (Section 4.2.1; Figure 4) and quantify variability of γ across seeds/subsets.\n  - Provide per-figure tables with mean±std (or CIs) for TU/AU/EU and γ, standardizing reporting across Figures 2–8 and Appendix 9–14.\n  - Document data subsampling procedures (random seeds, stratification) to ensure comparability across folds and minimize spurious slopes.\n- Strengthen theoretical connection to deep models\n  - Extend the linear-model analysis (Section 5.1; Appendix A.1–A.2) by analyzing simplified over-parameterized models (e.g., linearized networks, NTK regimes) to derive γ predictions and compare to measured values (Figures 2–5).\n  - Operationalize SLT (Section 5.2) by estimating local learning coefficients/effective dimensionality (e.g., Lau et al., 2024) for tested models and correlating them with γTU/γEU, moving beyond hypothesis.\n  - Derive how optimizer choices (SAM, schedules) could affect EU via theoretical proxies (e.g., curvature-dependent priors), and test these predictions against Figure 3 and Figure 5.\n  - Provide a quantitative mapping between theoretical rates (O(1/N)) and observed γ across architectures and methods, discussing deviations and their causes.\n  - Correct the generalization error decomposition in the main text to align with Appendix A.2 (Eq. 31), resolving the sign inconsistency with Eq. (10) and clarifying the TU–generalization link.\n- Broaden OOD scope and metrics\n  - Evaluate diverse OOD settings beyond CIFAR-C (Figure 6; Appendix Figure 14), including domain shifts (e.g., SVHN for CIFAR models) and semantic shifts, and report γTU/γEU across shift types.\n  - Include OOD detection/calibration metrics to complement uncertainty scaling, assessing whether power-law behavior translates to improved detection as N grows.\n  - Explore language OOD (e.g., out-of-domain text corpora) for Phi-2 fine-tuning (Appendix Figure 15) to assess modality breadth.\n  - Analyze whether small γ in OOD reflects saturation or method limitations by varying UQ methods (MC Dropout vs. Ensembles vs. MCMC) under shift.\n- Calibrate the generality of conclusions about data sufficiency\n  - Rephrase broad claims (Abstract Block #2) to be conditional on the studied tasks and scales; add caveats reflecting flat curves in Phi-2 fine-tuning (Appendix Figure 15) and modest EU magnitudes in many vision settings (Figures 2–3).\n  - Empirically validate extrapolation thresholds in Appendix Figure 12 on larger real datasets (where feasible) to demonstrate practical indistinguishability of ensemble predictions.\n  - Include additional application domains (e.g., detection/segmentation) to test whether EU remains non-negligible across tasks, supporting generality.\n  - Quantify the “numerical precision” threshold proposed in Section 6 by reporting concrete criteria and sensitivity, making extrapolation actionable.Score\n- Overall (10): 7 — Strong empirical breadth and clear uncertainty definitions with consistent power-law observations (Figures 2–6, 8; Equations 1–4), but limited fit diagnostics and theory confined to linear models (Section 5; Appendix A), with additional inconsistencies in scaling-law notation and generalization error signs (Figure 2 caption; Eq. 10 vs. Appendix Eq. 31).\n- Novelty (10): 8 — First systematic study of uncertainty scaling laws across UQ methods and modalities is well motivated (Section 1; Section 6 contributions), with new observations such as optimizer effects (Figure 3) and extrapolation (Appendix Figure 12).\n- Technical Quality (10): 6 — Empirical evidence is abundant (Figures 2–8; Appendix 9–15), but absence of fit diagnostics, sparse N points, and incomplete MCMC details (Section 3; Figure 8; Appendix Figure 14) limit rigor; theoretical presentation includes a sign inconsistency in Eq. (10) relative to Appendix Eq. (31).\n- Clarity (10): 7 — Metrics and methods are clearly defined (Equations 1–4; Section 3) with well-annotated figures (Figures 2–8; Appendix 9–14) and transparent limitations (Section 6), though inconsistent scaling-law notation in captions (Figure 2) and the Eq. (10) vs. Appendix Eq. (31) mismatch hinder readability.\n- Confidence (5): 4 — The manuscript provides extensive results and clear definitions, enabling a well-informed assessment, but missing fit diagnostics and some procedural details (Figure 2b single-fold; Figure 8 MCMC settings) lower absolute confidence."
}