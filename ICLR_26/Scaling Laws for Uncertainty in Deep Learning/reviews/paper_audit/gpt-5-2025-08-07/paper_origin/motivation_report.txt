# Innovation & Motivation Report

## 1. The Authors' Claimed Contribution
- Core Problem: Determine whether predictive uncertainties in deep learning (Total, Aleatoric, Epistemic) obey empirical scaling laws—specifically power-law decay—with dataset size N and model size P, paralleling known loss scaling laws, for both in-distribution and out-of-distribution settings.
- Claimed Gap: “Empirical study across architectures, modalities, datasets; first to consider uncertainty scaling laws in deep learning.” (Introduction, Contributions)
  - The Abstract further states: “Predictive uncertainties scale as power-laws with N across methods and settings … [and] ‘so much data’ is typically not enough to make epistemic uncertainty negligible.”
- Proposed Solution: A broad empirical campaign over vision and language tasks using multiple uncertainty quantification methods (MC Dropout, Laplace/VI, Deep Ensembles, gradient-based MCMC; partially stochastic networks), measuring TU/AU/EU scaling exponents and linking TU to generalization error via Bayesian linear regression and Singular Learning Theory. They also demonstrate extrapolation of uncertainty trends to larger N.

## 2. Comparative Scrutiny (The "Trial")
Here, we analyze how the paper stands against the identified similar works.

### vs. Scaling Laws for Uncertainty in Deep Learning (Rosso et al.)
- Identified Overlap: Exact thematic alignment and near-identity of claims: empirical power-law scaling of predictive uncertainties with data/model size; multi-method, multi-task study; assertion that epistemic uncertainty rarely vanishes; SLT/Bayesian linear connections.
- Manuscript's Defense: The manuscript explicitly claims novelty as “first to consider uncertainty scaling laws in deep learning” (Introduction, Contributions) and executes the comprehensive empirical program with detailed exponents (e.g., Figure 1; CIFAR-10 ResNet-18 EU γ = −0.44 MCMC, −0.36 MC Dropout, −0.80 Ensembles) and SLT links (Discussion; Eqs. 35–36). This appears to be the same line of work instantiated concretely in the manuscript.
- Reviewer's Assessment: The overlap is complete; this “similar work” is effectively the same contribution. As such, it does not weaken motivation but confirms it. If this is prior art independent of the manuscript, then the manuscript would need to differentiate; based on the provided text, differentiation is not evident beyond executing the stated program.

### vs. Quantifying Aleatoric and Epistemic Uncertainty in Machine Learning: Are Conditional Entropy and Mutual Information Appropriate Measures? (Wimmer et al.)
- Identified Overlap: Both center on the TU/AU/EU (entropy and mutual-information) decomposition; Wimmer et al. critique incoherencies and non-additivity in these measures.
- Manuscript's Defense: The manuscript acknowledges these limitations: “TU decomposition additivity limitations acknowledged; ensemble collapse concerns cited.” (Preliminaries) and cites Wimmer et al. in References. It further provides empirical anomalies consistent with the critique (e.g., EU increasing with N under SAM; ViT training schedule flipping EU sign; OOD EU near-flat).
- Reviewer's Assessment: The defense is responsible and explicit. By foregrounding scaling behavior while acknowledging metric caveats and corroborating them empirically, the manuscript’s motivation remains valid. However, reliance on potentially incoherent measures constrains the strength of its theoretical generality.

### vs. Unreliable Uncertainty Estimates with Monte Carlo Dropout (Djupskås et al.)
- Identified Overlap: Both interrogate MC Dropout’s ability to capture true epistemic/aleatoric uncertainty, especially in extrapolation/OOD regimes; Djupskås et al. find MCD often unreliable compared to GP/BNN.
- Manuscript's Defense: The manuscript does not cite this work, but addresses the core concern empirically across methods: it shows MC Dropout often yields weak or inconsistent OOD EU scaling (e.g., CIFAR-10-C EU γ near 0 or weakly negative, Figure 6) and contrasts with Deep Ensembles/MCMC where EU contracts more consistently (e.g., CIFAR-10 ensembles EU γ ≈ −0.55 to −0.61; CIFAR-10/100 MCMC EU γ ≈ −0.31 to −0.53 ID, Figure 14).
- Reviewer's Assessment: The manuscript’s multi-method comparison substantively answers the reliability critique in a scaling-law lens. The lack of citation is a minor scholarly gap, but the motivation—to characterize scaling behavior across UQ methods—stands and is supported by evidence.

### vs. The Peril of Popular Deep Learning Uncertainty Estimation Methods (Liu et al.)
- Identified Overlap: Critique that MC Dropout/BNN fail to yield high OOD uncertainty, while GP methods do; calls for GP-like approaches.
- Manuscript's Defense: Not cited. The manuscript includes GP-like functional priors within MCMC and reports distinct OOD EU scalings under partially stochastic layers (e.g., CIFAR-10/100 OOD EU γ ≈ −0.26 to −0.42), offering empirical support for the benefits of GP-like structures. It simultaneously documents MC Dropout’s weaker OOD response.
- Reviewer's Assessment: The manuscript strengthens its motivation by quantitatively substantiating the method-dependent OOD behaviors and providing GP-like functional prior results as a contrast. Again, the absence of explicit citation doesn’t diminish the technical distinction.

### vs. Neural Ensemble Search for Uncertainty Estimation and Dataset Shift (Zaidi et al.)
- Identified Overlap: Ensembles for better calibration and robustness under dataset shift; architectural diversity as a source of epistemic signal.
- Manuscript's Defense: Not cited. The manuscript’s focus is different: it does not propose new ensemble construction but measures how standard ensembles’ uncertainty components scale with data (e.g., CIFAR-100 ensembles EU γ down to −0.74; Figure 13) and contrasts ID vs OOD scaling.
- Reviewer's Assessment: The difference is substantial: the manuscript aims to establish scaling laws, not methods. Its motivation isn’t undermined; rather, it provides a framework that could quantitatively evaluate the benefits claimed by ensemble-search methods.

### vs. Domain-Specific Medical Segmentation Studies (e.g., MC Dropout–Uncertainty–Error Correlation; Radiotherapy OOD Detection; Epistemic Map Comparisons)
- Identified Overlap: Use of MC Dropout/entropy/mutual information to assess uncertainty quality, calibration, and OOD detection in segmentation.
- Manuscript's Defense: These works are not cited, but the manuscript generalizes the lens to cross-domain scaling laws and includes OOD analyses that explain weak error correlation under high AU regimes (e.g., CIFAR-10-C EU γ near zero; EU often smaller than AU across settings; Figure 2, Figure 6).
- Reviewer's Assessment: The manuscript’s broader motivation is distinct and complementary: it supplies a meta-level empirical law governing uncertainty behaviors that these domain papers probe locally. Not citing them is a missed opportunity to strengthen motivation, but the conceptual distinction is clear.

## 3. Novelty Verdict
- Innovation Type: Application-Oriented
- Assessment:
  The paper’s core motivation—to establish empirical scaling laws for predictive uncertainties in deep learning and link them to generalization—is well articulated and largely survives comparison with similar works. Most “similar” studies are method- or domain-specific; the manuscript’s contribution is to synthesize across methods and tasks, quantify scaling exponents, and provide a theoretically grounded lens (via Bayesian linear regression and SLT) for interpreting uncertainty behaviors. The main theoretical development is modest (derivations in identifiable linear models and a TU–generalization link), while the empirical breadth and systematic quantification are the paper’s central novelty.
  - Strength:
    - Comprehensive, multi-method, multi-architecture, cross-modality empirical characterization of TU/AU/EU scaling with N, including ID and OOD regimes.
    - Clear, testable claims with quantitative exponents (e.g., Figures 1–3, 10–14), and practical extrapolation demonstrations (Figure 12).
    - Explicit acknowledgment and citation of known limitations of entropy-based decompositions (Wimmer et al.), with empirical evidence of anomalies.
    - Inclusion of GP-like functional priors via MCMC to contrast approximate methods and inform OOD behavior.
  - Weakness:
    - Limited engagement with several critical prior critiques of MC Dropout and OOD uncertainty (e.g., Djupskås et al.; Liu et al.)—not cited, though addressed empirically.
    - Theoretical novelty is confined to identifiable linear models; no predictive exponents γ are derived for modern deep nets.
    - Model-size scaling (P) findings are inconclusive (Figure 7), and compute scaling is omitted, tempering the generality of the “scaling law” claim.

## 4. Key Evidence Anchors
- Introduction, Contributions: “first to consider uncertainty scaling laws in deep learning.”
- Abstract: “Predictive uncertainties scale as power-laws with N across methods and settings…”; “epistemic uncertainty typically does not vanish.”
- Preliminaries: TU/AU/EU definitions and decomposition (Eq. 1–3); recognition of additivity limitations.
- Scaling-law background (Eq. 5) and empirical exponents:
  - Figure 1: CIFAR-10 ResNet-18 EU γ = −0.44 (MCMC), −0.36 (MC Dropout), −0.80 (Deep Ensembles).
  - Figure 2: MC Dropout CIFAR-10 TU/AU/EU exponents across ResNets and dropout rates.
  - Figure 3: SAM-induced EU increase with N (e.g., ResNet-50 γEU = 0.41).
  - Figures 6, 8, 9: OOD/ImageNet-32 scaling showing weak/near-flat EU under MC Dropout; ViT OOD EU near zero unless early points discarded.
  - Figures 10–14: CIFAR-100 scaling across methods; Ensembles EU γ as low as −0.74; MCMC partially stochastic layers ID/OOD EU γ ≈ −0.26 to −0.53.
  - Figure 12: Extrapolation of softmax prediction variance γ_f across dropout rates and architectures.
- Discussion and Appendix A:
  - Bayesian linear regression decomposition: predictive variance AU = σ²; EU = φ(x)ᵗ S_N φ(x); EU = O(1/N).
  - TU asymptotics (Eq. 12; Appendix A.1.1): TU → AU; EU = O(1/N).
  - SLT connection (Eqs. 35–36): E_n[G_N] = λ/n − (m−1)/(n log n) + o(1/(n log n)); framing TU/generalization via free energy.