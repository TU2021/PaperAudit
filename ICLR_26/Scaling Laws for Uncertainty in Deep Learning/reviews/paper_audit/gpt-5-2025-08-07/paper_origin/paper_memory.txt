# Global Summary
Problem: Investigate whether predictive uncertainties in deep learning obey empirical scaling laws (power-law decay) with dataset size N and model size P, analogous to known loss scaling laws. Focus on uncertainty metrics Total (TU), Aleatoric (AU), Epistemic (EU), and their behavior for in-distribution (ID) and out-of-distribution (OOD) evaluations.

Approach: Empirical study across vision and language tasks using multiple uncertainty quantification (UQ) methodsâ€”MC dropout, Gaussian approximations (Laplace, variational), gradient-based MCMC (SGLD/SGHMC) with both parameter and functional priors, Deep Ensembles (M âˆˆ {5,10}), and partially stochastic networks. Theoretically connect TU and generalization error via Bayesian linear regression and Singular Learning Theory (SLT).

Evaluation scope:
- Vision: CIFAR-10, CIFAR-100, ImageNet-32, CIFAR-10-C/100-C OOD; architectures include ResNet, WideResNet, ViT.
- Language: GPT-2 on a synthetic algorithmic dataset (MODULO 97) and Phi-2 fine-tuned with Bayesian LoRA on QQP/ARC.

Key findings (selected quantitative results):
- ResNet-18 on CIFAR-10 (Figure 1): EU scaling exponents Î³ with N show power-law decay:
  - MCMC: Î³EU = âˆ’0.44 (N: 1Kâ†’50K)
  - MC Dropout: Î³EU = âˆ’0.36 (N: 10Kâ†’50K)
  - Deep Ensembles: Î³EU = âˆ’0.80 (N: 10Kâ†’50K)
- MC Dropout on CIFAR-10 (Figure 2, p=0.2/0.5):
  - ResNet-18 p=0.2: Î³TU = âˆ’0.51; Î³AU = âˆ’0.53; Î³EU = âˆ’0.39
  - ResNet-50 p=0.2: Î³TU = âˆ’0.54; Î³AU = âˆ’0.56; Î³EU = âˆ’0.38
  - ResNet-18 p=0.5: Î³TU = âˆ’0.42; Î³AU = âˆ’0.44; Î³EU = âˆ’0.36
  - ResNet-50 p=0.5: Î³TU = âˆ’0.47; Î³AU = âˆ’0.44; Î³EU = âˆ’0.60
- Deep Ensembles on CIFAR-10 (WideResNet; Figure 2b; M=5/10): EU consistently decays with Î³EU â‰ˆ âˆ’0.55 to âˆ’0.61; TU and AU Î³ around âˆ’0.66 to âˆ’0.72.
- SGD+SAM with MC Dropout (p=0.5) on CIFAR-10 (Figure 3): EU increases with data (positive Î³EU):
  - ResNet-18: Î³TU = âˆ’0.48; Î³AU = âˆ’0.55; Î³EU = 0.14
  - ResNet-34: Î³TU = âˆ’0.71; Î³AU = âˆ’0.79; Î³EU = 0.13
  - ResNet-50: Î³TU = âˆ’0.66; Î³AU = âˆ’0.71; Î³EU = 0.41
- ViT-small on CIFAR-10 (MC Dropout p=0.5; Figure 5): 
  - 500 epochs with Adam + cosine: Î³TU = âˆ’0.37; Î³AU = âˆ’0.41; Î³EU = 0.01
  - 2500 epochs fixed lr 10â»â´: Î³TU = âˆ’0.37; Î³AU = âˆ’0.39; Î³EU = âˆ’0.35
- OOD (CIFAR-10-C; MC Dropout): 
  - p=0.2 (Figure 6): TU Î³ResNet-18 = âˆ’0.19; Î³ResNet-34 = âˆ’0.13; Î³ResNet-50 = âˆ’0.08. AU Î³ResNet-18 = âˆ’0.24; Î³ResNet-34 = âˆ’0.16; Î³ResNet-50 = âˆ’0.07. EU Î³ResNet-18 = 0.02; Î³ResNet-34 = âˆ’0.06; Î³ResNet-50 = âˆ’0.12.
  - p=0.5 (Figure 6): TU Î³ResNet-18 = âˆ’0.18; Î³ResNet-34 = âˆ’0.19; Î³ResNet-50 = âˆ’0.35. AU Î³ResNet-18 = âˆ’0.25; Î³ResNet-34 = âˆ’0.24; Î³ResNet-50 = âˆ’0.44. EU Î³ResNet-18 = âˆ’0.06; Î³ResNet-34 = âˆ’0.12; Î³ResNet-50 = 0.01.
- ImageNet-32 (ResNet; MC Dropout p=0.5; Figure 8): 
  - ResNet-18: Î³TU = âˆ’0.03; Î³AU = âˆ’0.01; Î³EU = âˆ’0.28 (N: 200Kâ†’1M)
  - ResNet-34: Î³TU = âˆ’0.04; Î³AU = âˆ’0.01; Î³EU = âˆ’0.33
  - MCMC (ResNet-18; First layer stochastic): Î³TU = âˆ’0.08; Î³AU = âˆ’0.08; Î³EU = âˆ’0.08 (N: 10Kâ†’1M). First+Second layers: Î³EU = âˆ’0.06.
- GPT-2 on MODULO 97 algorithmic dataset (Figure 4): Î³TU = âˆ’2.86; Î³AU = âˆ’2.82; Î³EU = âˆ’2.95; trained 10,000 epochs; averaged over 3 folds; N: 1Kâ†’10K.
- Phi-2 fine-tuning with Bayesian LoRA on QQP (Figure 15): TU, AU, EU all flat with Î³ = 0.00 for N: 50Kâ†’200K.
- CIFAR-100 (ResNets; MC Dropout; Figure 10): 
  - p=0.2: ResNet-50 Î³TU = âˆ’0.39; Î³AU = âˆ’0.39; Î³EU = âˆ’0.14. ResNet-18 Î³TU = âˆ’0.29; Î³AU = âˆ’0.29; Î³EU = âˆ’0.13.
  - p=0.5: ResNet-50 Î³TU = âˆ’0.37; Î³AU = âˆ’0.38; Î³EU = âˆ’0.25. ResNet-18 Î³TU = âˆ’0.29; Î³AU = âˆ’0.31; Î³EU = âˆ’0.11.
- CIFAR-100 (WideResNet; MC Dropout; Figure 11): p=0.2 EU Î³ â‰ˆ âˆ’0.13 to âˆ’0.26; p=0.5 EU Î³ â‰ˆ âˆ’0.13 to âˆ’0.27; TU/AU Î³ â‰ˆ âˆ’0.31 to âˆ’0.37.
- Deep Ensembles on CIFAR-100 (Figure 13; M=5/10): EU Î³ ranges from âˆ’0.47 (ResNet-18, M=5) to âˆ’0.74 (ResNet-50, M=5); TU/AU Î³ between âˆ’0.21 and âˆ’0.41.
- MCMC partially stochastic on CIFAR-10/100 (Figure 14): ID EU Î³ â‰ˆ âˆ’0.31 to âˆ’0.53; OOD EU Î³ â‰ˆ âˆ’0.26 to âˆ’0.42; TU/AU Î³ modestly negative.
- ViT on ImageNet-32 (MC Dropout p=0.1; Figure 9): TU Î³ = âˆ’0.26; AU Î³ = âˆ’0.28; EU Î³ = 0.04 (250Kâ†’1.2M). Discarding the first point: EU Î³ = âˆ’0.05 (500Kâ†’1M).
- Extrapolation (Figure 12): Softmax prediction variance scaling Î³_f for WideResNet(40,4): p=0.2 Î³_f = âˆ’0.40; p=0.3 Î³_f = âˆ’0.42; p=0.5 Î³_f = âˆ’0.41. WideResNet(28,10): Î³_f = âˆ’0.31, âˆ’0.35, âˆ’0.38.

Caveats and limitations:
- Predictive uncertainties did not exhibit clear scaling with model parameters P in this setup (Figure 7; specific numeric trends not provided).
- Compute-budget scaling not studied due to limited resources.
- Some results averaged over 10 folds (MC Dropout on CIFAR-10/100), while others report single-fold (WideResNet Deep Ensembles).
- For large pretrained LLMs (Phi-2), fine-tuning uncertainties saturated (Î³ = 0.00), limiting insight into scaling behavior on small fine-tuning data.

# Abstract
- Problem statement: Examine whether predictive uncertainties (TU, AU, EU) in deep learning follow empirical scaling laws with dataset/model size; classic parametric models exhibit O(1/N) epistemic contraction, but over-parameterized deep nets lack guarantees.
- Approach: Empirical evaluation on vision and language tasks using approximate Bayesian inference and ensemble methods; study in- and out-of-distribution uncertainty scaling.
- Key result: Predictive uncertainties scale as power-laws with N across methods and settings; reported exponents include ResNet-18 EU scaling on CIFAR-10:
  - MCMC: Î³ = âˆ’0.44 (N: 1Kâ†’50K)
  - MC Dropout: Î³ = âˆ’0.36 (N: 10Kâ†’50K)
  - Deep Ensembles: Î³ = âˆ’0.80 (N: 10Kâ†’50K)
- Claim: Even with large datasets, epistemic uncertainty typically does not vanish; scaling enables extrapolation to larger N/P.

# Introduction
- Context: Prior scaling laws show test loss follows power-law f(x) âˆ x^âˆ’Î³ with data/model/compute; double descent indicates larger models can generalize better.
- Question: Do similar power-laws govern predictive uncertainty?
- UQ background: Bayesian marginalization of weights vs point estimates; scalable approximations (Laplace, VI, MC dropout), Deep Ensembles, and gradient-based MCMC (SGLD, SGHMC).
- Metrics: Decomposition TU, AU, EU; interest in ensemble diversity (EU) and collapse regimes.
- Figure 1 quantitative: ResNet-18 (CIFAR-10) EU scaling Î³ values across methods:
  - MCMC: Î³ = âˆ’0.44 (N: 1Kâ†’50K)
  - MC Dropout: Î³ = âˆ’0.36 (N: 10Kâ†’50K)
  - Deep Ensembles: Î³ = âˆ’0.80 (N: 10Kâ†’50K)
- Contributions:
  - Empirical study across architectures, modalities, datasets; first to consider uncertainty scaling laws in deep learning.
  - Observe power-law trends for ID and OOD uncertainties; extrapolation to larger N to locate regimes where EU is negligible to a numerical precision.
  - Theoretical link between SLT generalization error and total uncertainty in linear models.
- Theoretical framing: SLT suggests generalization follows L âˆ’ L0 = Î»/N^Î³; authors derive a formal connection between generalization and TU for linear models.

# Preliminaries
- Uncertainty Quantification definitions:
  - TU(x) = entropy of mean predictive distribution across K ensemble members (Eq. 1).
  - AU(x) = average entropy across ensemble members (Eq. 2).
  - EU(x) = TU âˆ’ AU (Eq. 3).
- Evaluation metric: Mean test predictive uncertainty over N_test samples (Eq. 4); report Unc âˆˆ {TU, AU, EU}.
- Notes: TU decomposition additivity limitations acknowledged; ensemble collapse concerns cited.
- Scaling laws background: Test loss typically scales as L(x) = Lâˆ + (x0/x)^Î± (Eq. 5); Î± depends on domain; comprehensive understanding of uncertainty scaling with N/P/C remains open.

# Method
- MC Dropout: Train with dropout; test-time stochastic masks yield ensemble predictions; used as baseline.
- Gaussian Approximations: Laplace (curvature-based Gaussian around optimum) and VI (optimize KL to posterior).
- MCMC: Gradient-based samplers (SGHMC, SGLD) to sample posterior over network parameters; consider global parameter priors and Gaussian process functional priors.
- Deep Ensembles: Train multiple independent networks; ensemble sizes M âˆˆ {5, 10}.
- Partially stochastic networks: Infer only a subset of layers (e.g., first or first+second), others fixed to pretrained solutions.

# Experiments
- General setup: Extensive matrix across architectures (ResNet, WideResNet, ViT), datasets (CIFAR-10/100, ImageNet-32, algorithmic MODULO 97, QQP, ARC), methods (MC Dropout, Deep Ensembles, MCMC, Bayesian LoRA). Systematically vary optimization, priors, stochasticity, and inference.

- Training details (vision; unless specified):
  - ResNet/WideResNet: 400 epochs; SGD (momentum 0.9; weight decay 5Ã—10â»â´); cosine annealing sometimes used; dropout placement per Kim et al. (2023) and original WideResNet implementation.
  - ViT-small: 6-layer transformer; 8 heads; patch size 4; embedding dim 512; MLP hidden 256; MC dropout rate 0.5 on embeddings and transformer blocks; Adam optimizer with cosine (500 epochs) vs fixed lr 10â»â´ (2500 epochs).
  - MC Dropout rates explored: p âˆˆ {0.1, 0.2, 0.3, 0.5}.
  - Ensembles: M âˆˆ {5, 10}.
  - MCMC priors: weakly informative zero-mean std 10; functional priors tuned to GP target with isotropic covariance (log-length-scale (1/2) log D; log-marginal variance 2).
  - MCMC stochastic layers: first vs first+second layers stochastic, rest fixed.

- Image classification on CIFAR-10 (MC Dropout; Figure 2a):
  - Data subsets: 25%, 50%, 75%, 100%; each point averaged over 10 folds (varying subsampling and seeds).
  - ResNet-18 p=0.2: Î³TU = âˆ’0.51; Î³AU = âˆ’0.53; Î³EU = âˆ’0.39.
  - ResNet-50 p=0.2: Î³TU = âˆ’0.54; Î³AU = âˆ’0.56; Î³EU = âˆ’0.38.
  - ResNet-18 p=0.5: Î³TU = âˆ’0.42; Î³AU = âˆ’0.44; Î³EU = âˆ’0.36.
  - ResNet-50 p=0.5: Î³TU = âˆ’0.47; Î³AU = âˆ’0.44; Î³EU = âˆ’0.60.
  - Observation: EU typically smaller than AU.

- CIFAR-10 (Deep Ensembles; WideResNet; Figure 2b; single fold):
  - WideResNet(40,4) M=5: Î³TU = âˆ’0.67; Î³AU = âˆ’0.71; Î³EU = âˆ’0.61.
  - WideResNet(28,10) M=5: Î³TU = âˆ’0.66; Î³AU = âˆ’0.70; Î³EU = âˆ’0.56.
  - WideResNet(40,4) M=10: Î³TU = âˆ’0.66; Î³AU = âˆ’0.71; Î³EU = âˆ’0.61.
  - WideResNet(28,10) M=10: Î³TU = âˆ’0.66; Î³AU = âˆ’0.72; Î³EU = âˆ’0.55.

- SGD+SAM with MC Dropout (p=0.5; CIFAR-10; Figure 3):
  - Î³TU: ResNet-18 âˆ’0.48; ResNet-34 âˆ’0.71; ResNet-50 âˆ’0.66.
  - Î³AU: ResNet-18 âˆ’0.55; ResNet-34 âˆ’0.79; ResNet-50 âˆ’0.71.
  - Î³EU: ResNet-18 0.14; ResNet-34 0.13; ResNet-50 0.41 (EU increases with N).

- ViT on CIFAR-10 (Figure 5):
  - 500 epochs (Adam + cosine): Î³TU = âˆ’0.37; Î³AU = âˆ’0.41; Î³EU = 0.01; early-phase dynamics sensitive to schedule.
  - 2500 epochs (fixed lr 10â»â´): Î³TU = âˆ’0.37; Î³AU = âˆ’0.39; Î³EU = âˆ’0.35.
  - Per-epoch dynamics show convergence speed differs with optimizer and schedule; N âˆˆ {12,500; 25,000; 37,500; 50,000}.

- OOD uncertainty (CIFAR-10-C; MC Dropout; Figure 6):
  - p=0.2:
    - TU Î³ResNet-18 = âˆ’0.19; Î³ResNet-34 = âˆ’0.13; Î³ResNet-50 = âˆ’0.08.
    - AU Î³ResNet-18 = âˆ’0.24; Î³ResNet-34 = âˆ’0.16; Î³ResNet-50 = âˆ’0.07.
    - EU Î³ResNet-18 = 0.02; Î³ResNet-34 = âˆ’0.06; Î³ResNet-50 = âˆ’0.12.
  - p=0.5:
    - TU Î³ResNet-18 = âˆ’0.18; Î³ResNet-34 = âˆ’0.19; Î³ResNet-50 = âˆ’0.35.
    - AU Î³ResNet-18 = âˆ’0.25; Î³ResNet-34 = âˆ’0.24; Î³ResNet-50 = âˆ’0.44.
    - EU Î³ResNet-18 = âˆ’0.06; Î³ResNet-34 = âˆ’0.12; Î³ResNet-50 = 0.01.
  - Averaged over corruption levels 1â€“5 and types.

- ImageNet-32 (ResNet; MC Dropout and MCMC; Figure 8):
  - MC Dropout (p=0.5):
    - ResNet-18: Î³TU = âˆ’0.03; Î³AU = âˆ’0.01; Î³EU = âˆ’0.28 (N: 200Kâ†’1M).
    - ResNet-34: Î³TU = âˆ’0.04; Î³AU = âˆ’0.01; Î³EU = âˆ’0.33.
  - MCMC (ResNet-18):
    - First layer stochastic (MCMC (1)): Î³TU = âˆ’0.08; Î³AU = âˆ’0.08; Î³EU = âˆ’0.08 (N: 10Kâ†’1M).
    - First+Second layers stochastic (MCMC (2)): Î³TU = âˆ’0.08; Î³AU = âˆ’0.08; Î³EU = âˆ’0.06.

- Language â€” GPT-2 on Algorithmic Dataset (MODULO 97; Figures 4, 25; Table 2):
  - MC Dropout p=0.1; trained 10,000 epochs with AdamW (lr 10â»â´) and linear scheduler (100 warmup steps); averaged over 3 folds; N: 1Kâ†’10K; Î³TU = âˆ’2.86; Î³AU = âˆ’2.82; Î³EU = âˆ’2.95.
  - Uncertainty scaling patterns emerge after extensive training; potential link to grokking dynamics.

- Language â€” Phi-2 with Bayesian LoRA (QQP/ARC; Figure 15):
  - Laplace over LoRA parameters; uncertainties remained flat for all fine-tuning subset sizes; Î³TU = 0.00; Î³AU = 0.00; Î³EU = 0.00 (N: 50Kâ†’200K); attributed to massive pretraining.

- CIFAR-100 (MC Dropout; ResNets; Figure 10):
  - p=0.2:
    - ResNet-18: Î³TU = âˆ’0.29; Î³AU = âˆ’0.29; Î³EU = âˆ’0.13.
    - ResNet-50: Î³TU = âˆ’0.39; Î³AU = âˆ’0.39; Î³EU = âˆ’0.14.
  - p=0.5:
    - ResNet-18: Î³TU = âˆ’0.29; Î³AU = âˆ’0.31; Î³EU = âˆ’0.11.
    - ResNet-50: Î³TU = âˆ’0.37; Î³AU = âˆ’0.38; Î³EU = âˆ’0.25.

- CIFAR-100 (WideResNet; MC Dropout; Figure 11):
  - p=0.2:
    - WideResNet(40,4): Î³TU = âˆ’0.33; Î³AU = âˆ’0.37; Î³EU = âˆ’0.13.
    - WideResNet(28,10): Î³TU = âˆ’0.32; Î³AU = âˆ’0.32; Î³EU = âˆ’0.26.
  - p=0.5:
    - WideResNet(40,4): Î³TU = âˆ’0.35; Î³AU = âˆ’0.36; Î³EU = âˆ’0.27.
    - WideResNet(28,10): Î³TU = âˆ’0.31; Î³AU = âˆ’0.32; Î³EU = âˆ’0.24.

- CIFAR-100 (Deep Ensembles; ResNets; Figure 13):
  - M=5: ResNet-18 Î³TU = âˆ’0.26; Î³AU = âˆ’0.22; Î³EU = âˆ’0.47. ResNet-34 Î³TU = âˆ’0.34; Î³AU = âˆ’0.28; Î³EU = âˆ’0.57. ResNet-50 Î³TU = âˆ’0.40; Î³AU = âˆ’0.26; Î³EU = âˆ’0.74.
  - M=10: ResNet-18 Î³TU = âˆ’0.25; Î³AU = âˆ’0.21; Î³EU = âˆ’0.46. ResNet-34 Î³TU = âˆ’0.41; Î³AU = âˆ’0.32; Î³EU = âˆ’0.72. ResNet-50 Î³TU = âˆ’0.41; Î³AU = âˆ’0.27; Î³EU = âˆ’0.73.

- MCMC with partially stochastic layers (CIFAR-10 and CIFAR-100; Figure 14):
  - CIFAR-10 ID: First layer: Î³TU = âˆ’0.24; Î³AU = âˆ’0.21; Î³EU = âˆ’0.53; First+Second: Î³TU = âˆ’0.25; Î³AU = âˆ’0.22; Î³EU = âˆ’0.44.
  - CIFAR-10 OOD: First layer: Î³TU = âˆ’0.16; Î³AU = âˆ’0.14; Î³EU = âˆ’0.42; First+Second: Î³TU = âˆ’0.17; Î³AU = âˆ’0.15; Î³EU = âˆ’0.34.
  - CIFAR-100 ID: First: Î³TU = âˆ’0.20; Î³AU = âˆ’0.20; Î³EU = âˆ’0.43; First+Second: Î³TU = âˆ’0.20; Î³AU = âˆ’0.20; Î³EU = âˆ’0.31.
  - CIFAR-100 OOD: First: Î³TU = âˆ’0.17; Î³AU = âˆ’0.16; Î³EU = âˆ’0.34; First+Second: Î³TU = âˆ’0.17; Î³AU = âˆ’0.17; Î³EU = âˆ’0.26.

- ViT on ImageNet-32 (MC Dropout p=0.1; Figure 9):
  - 250Kâ†’1.2M: Î³TU = âˆ’0.26; Î³AU = âˆ’0.28; Î³EU = 0.04; discarding first point yields Î³EU = âˆ’0.05 (500Kâ†’1M).
  - Training for 200 epochs with SGD + cosine annealing.

- Extrapolation of uncertainty (Figure 12):
  - Softmax prediction variance across MC samples (ğ”™[Ïƒ(f)]), averaged over test samples; WideResNet(40,4): Î³_f = âˆ’0.40 (p=0.2), âˆ’0.42 (p=0.3), âˆ’0.41 (p=0.5); WideResNet(28,10): Î³_f = âˆ’0.31, âˆ’0.35, âˆ’0.38; extrapolated to N up to 10B.

- Model-parameter scaling (Figure 7): Large range of ResNet architectures on CIFAR-10 with MC Dropout p=0.5; trained 400 epochs with fixed lr 10â»Â³; the paper notes predictive uncertainties did not exhibit clear trends with model parameters; specific Î³-by-P numbers not specified in the text.

# Discussion
- Identifiable parametric models (Bayesian linear regression):
  - Model: y = Î¸áµ—Ï†(x) + Îµ; Îµ âˆ¼ ğ’©(0, ÏƒÂ²); Gaussian conjugate prior yields closed-form posterior predictive.
  - Predictive variance decomposes into AU (ÏƒÂ²) and EU (Ï†(x)áµ—S_NÏ†(x)); posterior predictive variance contracts: ÏƒÂ²_{N+1}(x) â‰¤ ÏƒÂ²_N(x); lim_{Nâ†’âˆ} ÏƒÂ²_N(x) = ÏƒÂ².
  - Asymptotic TU at test point x_{N+1} (Eq. 12): TU(x_{N+1}) = (1/2) log(2Ï€e Ïƒ_trueÂ²) + [1/(2(N+1))] x_{N+1}áµ— Î£_{X_{N+1}}^{-1} x_{N+1} + O(1/(N+1)Â²); EU decays as O(1/N).

- Connection to SLT and generalization error:
  - Free energy F_N = âˆ’log p(y_N|X_N) (Eq. 7, 24); generalization error G_N measures expected change in marginal likelihood when adding a data point (Eq. 8).
  - Manipulation (Eq. 10): G_N expressed via aleatoric uncertainty term and KL divergence between true predictive p(y) and posterior predictive q_N(y); elsewhere derived: G_N = AU + KL (Eq. 31), with KL = KL[ğ’©(y|Î¼_true, Ïƒ_trueÂ²) || ğ’©(y|Î¼_pred, Ïƒ_predÂ²)] (Eq. 33).
  - As q_N(y) â†’ p(y), KL â†’ 0 and G_N â†’ (1/2) log(2Ï€e Ïƒ_trueÂ²) (Appendix).
  - SLT asymptotics (Eq. 35â€“36): E_n[G_N] = Î»/n âˆ’ (mâˆ’1)/(n log n) + o(1/(n log n)); F_N = n S_n + Î» log n âˆ’ (mâˆ’1) log log n + O_p(1); Î» is learning coefficient; for regular models Î» = d/2, m = 1.

- Conclusions:
  - Empirical uncertainties exhibit power-law scaling across modalities, architectures, and UQ methods; theoretical insights link TU and generalization; exact Î³ coefficients depend on architecture/UQ/hyperparameters and are not directly predicted by current theory.
  - Practical utility: extrapolate uncertainty to large N to estimate data needed for ensemble collapse to a given precision.

- Limitations:
  - No clear scaling with model parameters P in this setup; compute-budget scaling omitted; limited capacity to test extrapolation at very large N beyond GPT-2 algorithmic task.

# References
- Cited works include scaling laws for losses and compute (Hestness et al., 2017; Kaplan et al., 2020; Hoffmann et al., 2022), dropout as Bayesian approximation (Gal and Ghahramani, 2016), Laplace/VI (Ritter et al., 2018; Graves, 2011), Deep Ensembles (Lakshminarayanan et al., 2017), MCMC for deep nets (Welling and Teh, 2011; Chen et al., 2014; Izmailov et al., 2021; Tran et al., 2022), SLT (Watanabe, 1999; 2009; 2018), and mechanistic interpretability/grokking (Power et al., 2022; Nanda et al., 2023). Specific references supporting entropy decomposition critiques (Wimmer et al., 2023; de Jong et al., 2024; Kirsch, 2024; Fellaji and Pennerath, 2024). Not specified beyond these for dataset licenses or additional implementation details.

# Appendix
- Theory (Appendix A):
  - A.1: Bayesian linear regression setup and derivations (Eqs. 13â€“21); predictive variance decomposition; contraction ÏƒÂ²_{N+1}(x) â‰¤ ÏƒÂ²_N(x); lim_{Nâ†’âˆ} ÏƒÂ²_N(x) = ÏƒÂ².
  - A.1.1: TU entropy expression (Eq. 22); under i.i.d. assumptions and LLN, TU â†’ AU and EU = O(1/N).
  - A.2: Formal derivation connecting generalization error G_N and TU (Eqs. 23â€“34); asymptotic behaviors (Eq. 35â€“36).

- Additional results (Appendix B):
  - ViT on ImageNet-32 (MC Dropout p=0.1; 200 epochs SGD + cosine): 
    - 250Kâ†’1.2M: Î³TU = âˆ’0.26; Î³AU = âˆ’0.28; Î³EU = 0.04; discarding first point gives Î³EU = âˆ’0.05 (500Kâ†’1M).
  - CIFAR-100 (ResNets; MC Dropout):
    - p=0.2: ResNet-18 Î³TU = âˆ’0.29; Î³AU = âˆ’0.29; Î³EU = âˆ’0.13. ResNet-50 Î³TU = âˆ’0.39; Î³AU = âˆ’0.39; Î³EU = âˆ’0.14.
    - p=0.5: ResNet-18 Î³TU = âˆ’0.29; Î³AU = âˆ’0.31; Î³EU = âˆ’0.11. ResNet-50 Î³TU = âˆ’0.37; Î³AU = âˆ’0.38; Î³EU = âˆ’0.25.
  - CIFAR-100 (WideResNet; MC Dropout):
    - p=0.2: WideResNet(40,4) Î³TU = âˆ’0.33; Î³AU = âˆ’0.37; Î³EU = âˆ’0.13. WideResNet(28,10) Î³TU = âˆ’0.32; Î³AU = âˆ’0.32; Î³EU = âˆ’0.26.
    - p=0.5: WideResNet(40,4) Î³TU = âˆ’0.35; Î³AU = âˆ’0.36; Î³EU = âˆ’0.27. WideResNet(28,10) Î³TU = âˆ’0.31; Î³AU = âˆ’0.32; Î³EU = âˆ’0.24.
  - Extrapolation (WideResNet on CIFAR-10; MC Dropout):
    - Softmax prediction variance Î³_f: WideResNet(40,4) p=0.2 âˆ’0.40; p=0.3 âˆ’0.42; p=0.5 âˆ’0.41. WideResNet(28,10) p=0.2 âˆ’0.31; p=0.3 âˆ’0.35; p=0.5 âˆ’0.38. N extrapolated from 10K to 10B.
  - Deep Ensembles on CIFAR-100 (ResNets):
    - M=5: ResNet-18 Î³TU = âˆ’0.26; Î³AU = âˆ’0.22; Î³EU = âˆ’0.47. ResNet-34 Î³TU = âˆ’0.34; Î³AU = âˆ’0.28; Î³EU = âˆ’0.57. ResNet-50 Î³TU = âˆ’0.40; Î³AU = âˆ’0.26; Î³EU = âˆ’0.74.
    - M=10: ResNet-18 Î³TU = âˆ’0.25; Î³AU = âˆ’0.21; Î³EU = âˆ’0.46. ResNet-34 Î³TU = âˆ’0.41; Î³AU = âˆ’0.32; Î³EU = âˆ’0.72. ResNet-50 Î³TU = âˆ’0.41; Î³AU = âˆ’0.27; Î³EU = âˆ’0.73.
  - MCMC (ResNet-18; CIFAR-10/100; ID and OOD):
    - CIFAR-10 ID First: Î³TU = âˆ’0.24; Î³AU = âˆ’0.21; Î³EU = âˆ’0.53. OOD First: Î³TU = âˆ’0.16; Î³AU = âˆ’0.14; Î³EU = âˆ’0.42.
    - CIFAR-10 ID First+Second: Î³TU = âˆ’0.25; Î³AU = âˆ’0.22; Î³EU = âˆ’0.44. OOD First+Second: Î³TU = âˆ’0.17; Î³AU = âˆ’0.15; Î³EU = âˆ’0.34.
    - CIFAR-100 ID First: Î³TU = âˆ’0.20; Î³AU = âˆ’0.20; Î³EU = âˆ’0.43. OOD First: Î³TU = âˆ’0.17; Î³AU = âˆ’0.16; Î³EU = âˆ’0.34.
    - CIFAR-100 ID First+Second: Î³TU = âˆ’0.20; Î³AU = âˆ’0.20; Î³EU = âˆ’0.31. OOD First+Second: Î³TU = âˆ’0.17; Î³AU = âˆ’0.17; Î³EU = âˆ’0.26.

- Experimental setup (Appendix C):
  - ViT architecture params: patch size 4; embed dim 512; depth 6; 8 heads; MLP hidden 256; dropout in transformer/embeddings {0.1, 0.5}; implementation link provided.
  - GPT-2 (Algorithmic task) training: Max sequence length 256; embedding size 128; 2 layers; 4 heads; dropout 0.1 (residuals, embeddings, attention); AdamW lr 10â»â´; 100 warmup steps; 10,000 epochs; implementation link provided.

- Additional notes:
  - Some plots show single-fold results (e.g., WideResNet Deep Ensembles on CIFAR-10); others average over 10 folds (ResNet MC Dropout on CIFAR-10/100).
  - Model-parameter scaling results (Figure 7) noted but specific Î³-by-P numeric values not specified in text.