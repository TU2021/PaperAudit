{
  "pdf": "paper.pdf",
  "content": [
    {
      "type": "text",
      "text": "# Bernoulli-LoRA: A Theoretical Framework for Randomized Low-Rank Adaptation\n\nIgor SokolovÂ¹, Abdurakhmon SadievÂ¹, Yury DemidovichÂ¹, Fawaz S. Al-QahtaniÂ², Peter RichtÃ¡rikÂ¹\n\nÂ¹ Center of Excellence for Generative AI, King Abdullah University of Science and Technology (KAUST), Saudi Arabia\nÂ² Saudi Data & AI Authority (SDAIA) & National Center of AI (NCAI), Saudi Arabia",
      "index": 1,
      "section": "Abstract"
    },
    {
      "type": "text",
      "text": "## Abstract\n\nParameter-efficient fine-tuning (PEFT) has emerged as a crucial approach for adapting large foundational models to specific tasks, particularly as model sizes continue to grow exponentially. Among PEFT methods, Low-Rank Adaptation (LoRA) [Hu et al., 2022] stands out for its effectiveness and simplicity, expressing adaptations as a product of two low-rank matrices. While extensive empirical studies demonstrate LoRAâ€™s practical utility, theoretical understanding of such methods remains limited. Recent work on RAC-LoRA [Malinovsky et al., 2024] took initial steps toward rigorous analysis. In this work, we introduce Bernoulli-LoRA, a novel theoretical framework that unifies and extends existing LoRA approaches. Our method introduces a probabilistic Bernoulli mechanism for selecting which matrix to update. This approach encompasses and generalizes various existing update strategies while maintaining theoretical tractability. Under standard assumptions from non-convex optimization literature, we analyze several variants of our framework: Bernoulli-LoRA-GD, Bernoulli-LoRA-SGD, Bernoulli-LoRA-PAGE, Bernoulli-LoRA-MVR, Bernoulli-LoRA-QGD, Bernoulli-LoRA-MARINA, and Bernoulli-LoRA-EF21, establishing convergence guarantees for each variant. Additionally, we extend our analysis to convex, non-smooth functions, providing convergence rates for both constant and adaptive (Polyak-type) stepsizes. Through extensive experiments on various tasks, we validate our theoretical findings and demonstrate the practical efficacy of our approach. This work is a step toward developing theoretically grounded yet practically effective PEFT methods.",
      "index": 2,
      "section": "Abstract"
    },
    {
      "type": "text",
      "text": "## Contents\n\n| 1 | Introduction                                                                                    | 2  |\n| - | ----------------------------------------------------------------------------------------------- | -- |\n| 2 | Problem Statement                                                                               | 3  |\n| 3 | Motivation                                                                                      | 4  |\n| 4 | Contributions                                                                                   | 5  |\n| 5 | Notation                                                                                        | 7  |\n| 6 | Bernoulli-LoRA Framework                                                                        | 8  |\n|   | 6.1 Reformulation as a Projected Gradient Step                                                  | 8  |\n|   | 6.2 Core Algorithmic Variants                                                                   | 9  |\n|   | 6.3 Extensions for Federated Learning                                                           | 10 |",
      "index": 3,
      "section": "Introduction"
    },
    {
      "type": "text",
      "text": "7 Convergence Results 11\n8 Experiments 15\n8.1 Linear Regression with Non-convex Regularization 15\n8.2 MLP on MNIST 16\nA Basic Facts and Useful Inequalities 24\nB Discussion on Positive Expected Projection (Assumption 1) 25\nC Proofs for Core Algorithmic Variants 27\nC.1 Analysis of Bernoulli-LoRA-GD 27\nC.1.1 Convergence for Smooth Non-Convex Functions 29\nC.1.2 Convergence under Polyak-Åojasiewicz Condition 31\nC.1.3 Convergence for Non-Smooth Convex Functions 32\nC.2 Analysis of Bernoulli-LoRA-SGD 39\nC.2.1 Convergence for Smooth Non-Convex Functions 39\nC.2.2 Convergence under Polyak-Åojasiewicz Condition 42\nC.3 Analysis of Bernoulli-LoRA-MVR 43\nC.3.1 Convergence for Smooth Non-Convex Functions 45\nC.3.2 Convergence under Polyak-Åojasiewicz Condition 46\nC.4 Analysis of Bernoulli-LoRA-PAGE 48\nC.4.1 Convergence for Smooth Non-Convex Functions 49\nC.4.2 Convergence under Polyak-Åojasiewicz Condition 50\nD Proofs for Federated Learning Extensions 51\nD.1 Analysis of Fed-Bernoulli-LoRA-QGD 51\nD.1.1 Convergence for Smooth Non-Convex Functions 53\nD.1.2 Convergence under Polyak-Åojasiewicz Condition 54\nD.2 Analysis of Fed-Bernoulli-LoRA-MARINA 55\nD.2.1 Convergence for Smooth Non-Convex Functions 56\nD.2.2 Convergence under Polyak-Åojasiewicz Condition 57\nD.3 Analysis of Fed-Bernoulli-LoRA-EF21 59\nD.3.1 Convergence for Smooth Non-Convex Functions 60\nD.3.2 Convergence under Polyak-Åojasiewicz Condition 61\nE Experiments: Missing Details 63\nE.1 Linear Regression with Non-convex Regularization 63",
      "index": 4,
      "section": "Introduction"
    },
    {
      "type": "text",
      "text": "1 Introduction\n\nFine-tuning is a transfer learning method, where a pre-trained neural network is trained on a new dataset. In modern deep learning, adapting large models to specific tasks via fine-tuning has become central, especially in natural language processing [Peters et al., 2018, Devlin et al., 2019]. While full fine-tuning often yields strong results, it is computationally intensive for large models. Parameter-Efficient Fine-Tuning (PEFT) [He et al., 2021] addresses this by updating only a small subset of parameters [RichtÃ¡rik and TakÃ¡Ä, 2016, Demidovich et al., 2023a], often with task-specific layers trained from scratch. PEFT offers performance close to full fine-tuning with reduced training time and resource use [Radford et al., 2019, Brown et al., 2020, Han et al., 2024], making it widely adopted in practice. Research on PEFT, especially for large foundation and language models, is rapidly growing.",
      "index": 5,
      "section": "Introduction"
    },
    {
      "type": "text",
      "text": "Pre-trained models are known to have an inherently low intrinsic dimensionality [Li et al., 2018, Aghajanyan et al., 2020]. This means that fine-tuning can be effectively achieved within a reduced-dimensional subspace rather than the full parameter space. Among the various methods for utilizing this property, Low-Rank Adaptation (LoRA) [Hu et al., 2022] stands out as the most prominent reparameterization technique. LoRA minimizes the need to update an entire large, dense weight matrix by leveraging the product of two trainable low-rank matrices. This method significantly reduces the number of parameters required for fine-tuning. The low-rank matrices are optimized so that their scaled product serves as the update applied to the weight matrix:\n\n$$W = W^0 + \\frac{\\alpha}{r}BA$$\n\nwhere $W^0 \\in \\mathbb{R}^{m \\times n}$, $B \\in \\mathbb{R}^{m \\times r}$, and $A \\in \\mathbb{R}^{r \\times n}$. The pre-trained weight matrix $W^0$ remains fixed, while $A$ and $B$ are the trainable matrices. Typically, $A$ is initialized randomly using a Gaussian distribution, while $B$ is set to zero to ensure that $\\Delta W = 0$ at the start. Various alternative initialization strategies have been investigated by [Zhu et al., 2024, Hayou et al., 2024, Meng et al., 2024, Wang et al., 2024].\n\nThe parameters of LoRA include the low rank $r$ and the scaling factor $\\alpha$. Since the dimensions $m$ and $n$ in deep learning models are usually large, selecting $r \\ll \\min\\{m, n\\}$ drastically reduces the number of trainable parameters. The scaling factor $\\alpha$ acts as the stepsize. While LoRA may not always achieve the performance of full fine-tuning, it is more effective at mitigating forgetting when compared to traditional regularization techniques like weight decay and dropout. Additionally, it enhances diversity in generated outputs [Biderman et al., 2024]. Furthermore, LoRA is straightforward to implement and achieves performance comparable to full fine-tuning across a wide range of downstream tasks [Hu et al., 2022]. Complementing its algorithmic utility, research has also focused on enhancing the computational efficiency of LoRA; for instance, Cherniuk et al. [2023] demonstrated that by optimizing the computation graph of LoRA operations based on layer dimensions and rank, significant speedups and memory savings can be achieved without sacrificing accuracy. For a detailed summary of recent advancements in LoRA, refer to [Mao et al., 2025].\n\nTo bridge the gap between full fine-tuning and LoRA, Xia et al. [2024] introduced Chain of LoRA (COLA), an iterative optimization framework that enhances model weights via higher-rank representations composed of multiple low-rank componentsâ€”without added computational or memory cost. COLA incrementally refines low-rank approximations by training a sequence of LoRA modules through structured fine-tuning, merging, and extension. Each iteration adds a new low-rank component, forming a chain whose length reflects the number of optimized modules. The core idea involves applying LoRA updates iteratively over $T$ steps: training a module, integrating its updates into fixed parameters, re-initializing, and repeating. This cyclic process builds higher-rank augmentations efficiently. In essence, COLA applies successive LoRA updates as:\n\n$$W = W^0 + \\frac{\\alpha}{r} \\sum_{t=0}^{T-1} B^t A^t$$\n\nEach pair $(A^t, B^t)$ is initialized like standard LoRA. Unlike traditional LoRA, which may struggle with non-low-rank adaptations, COLA uses sequential low-rank decompositions to approximate updates of intermediate-to-high rank. This leads to more accurate and efficient adaptation while simplifying optimization by avoiding a direct high-rank fit.",
      "index": 6,
      "section": "Introduction"
    },
    {
      "type": "text",
      "text": "2 Problem Statement\n\nSupervised machine learning typically frames the training process as an optimization problem, aiming to minimize a loss function that quantifies the discrepancy between model predictions and true targets. This research focuses on the intricacies of this optimization challenge within the fine-tuning paradigm, where a pre-trained model is adapted to a new, specific task or dataset. Effective fine-tuning hinges on",
      "index": 7,
      "section": "Preliminaries"
    },
    {
      "type": "text",
      "text": "making precise and efficient modifications to the modelâ€™s parameters to enhance its performance on the target task. We explore a generalized formulation of this problem, which is independent of the specific architecture of the underlying model:\n\n$$\\min_{\\Delta W \\in \\mathbb{R}^{m \\times n}} f(W^0 + \\Delta W).$$\n(1)\n\nHere, $W^0 \\in \\mathbb{R}^{m \\times n}$ represents the parameters of the pre-trained model (or, for instance, the parameters of a single linear layer if other layers are kept constant), and $\\Delta W \\in \\mathbb{R}^{m \\times n}$ is the adaptation term whose optimal value we seek. The function $f : \\mathbb{R}^{m \\times n} \\to \\mathbb{R}$ denotes the empirical loss computed over the specific target dataset. Given that the dimensionality $m \\times n$ is typically very large in contemporary deep learning models, the adjustment $\\Delta W$ must possess a sufficiently simple structure to be practically trainable and applicable.\n\nFor the stochastic optimization methods developed and analyzed in this paper, we consider objective functions with one of the following specific structures:\n\nâ€¢ Finite-Sum Setting: The objective is an average of individual loss functions, a structure we address with methods like Bernoulli-LoRA-PAGE:\n\n$$f(W^0 + \\Delta W) = \\frac{1}{N} \\sum_{i=1}^{N} f_i(W^0 + \\Delta W),$$\n(2)\n\nwhere each $f_i$ corresponds to the loss on a single data sample, and $N$ is the total number of data points in the training set.\n\nâ€¢ Expectation Setting: The objective is an expectation over a data distribution $\\mathcal{D}$, relevant for methods such as Bernoulli-LoRA-MVR:\n\n$$f(W^0 + \\Delta W) = \\mathbb{E}_{\\xi \\sim \\mathcal{D}} \\left[ f_{\\xi}(W^0 + \\Delta W) \\right],$$\n(3)\n\nwhere $f_{\\xi}$ is the loss function associated with a data sample $\\xi$ drawn from $\\mathcal{D}$.\n\nMoreover, this paper extends its investigation to the distributed optimization setting, which is central to the Federated Learning (FL) algorithms we propose (e.g., Fed-Bernoulli-LoRA-QGD, Fed-Bernoulli-LoRA-MARINA, and Fed-Bernoulli-LoRA-EF21). In this context, we address problems formulated as:\n\n$$f(W^0 + \\Delta W) = \\frac{1}{M} \\sum_{l=1}^{M} f_l(W^0 + \\Delta W),$$\n(4)\n\nwhere $M$ is the total number of participating clients, and $f_l$ represents the local loss function for client $l$, defined over its private dataset. The goal is to find a common adaptation $\\Delta W$ that minimizes this global, federated objective.",
      "index": 8,
      "section": "Preliminaries"
    },
    {
      "type": "text",
      "text": "\\n\\n\\n\\n\n## 3 Motivation\n\nDespite the widespread adoption and empirical success of Low-Rank Adaptation (LoRA) and its variants like Chain of LoRA (COLA), a comprehensive theoretical understanding underpinning these prevalent fine-tuning methods remains largely undeveloped. Several critical issues highlight this gap. Firstly, as pointed out by Sun et al. [2024], the LoRA re-parameterization inherently transforms a smooth Lipschitz loss into a non-smooth one. This alteration introduces significant theoretical complexities beyond those associated with managing the low-rank structure of updates, forming a key barrier to establishing robust theoretical frameworks. Secondly, the existing theoretical analysis of COLA by Xia et al. [2024] sidesteps",
      "index": 9,
      "section": "Motivation"
    },
    {
      "type": "text",
      "text": "The core mechanism of low-rank updates by focusing on full-rank matrix optimization (Î”W). Such an approach is unsatisfactory as it fails to model or explain the very essence of LoRAâ€™s efficiency.\n\nConsequently, most methods based on LoRA are, in essence, heuristics, developed through empirical investigation without strong theoretical convergence guarantees. This is problematic, as these methods can be highly sensitive to hyperparameter choices [Khodak et al., 2021, Kuang et al., 2024], and their reliability beyond current empirical validation is not assured. In fact, Malinovsky et al. [2024] provided a concrete example of COLAâ€™s potential divergence, further underscoring its heuristic nature. Their work introduced RAC-LoRA, the first comprehensive optimization framework designed to rigorously evaluate and establish convergence rates for methods utilizing LoRA-style updates, marking a significant step towards theoretically grounded PEFT.\n\nHowever, while RAC-LoRA provides a foundational theoretical lens, its scope does not encompass several critical aspects of modern optimization, particularly for non-convex problems and distributed settings. Specifically, the RAC-LoRA framework does not utilize optimal variance-reduced techniques for non-convex optimization, nor does it delve into sophisticated Federated Learning (FL) settings that incorporate crucial practical techniques such as communication compression [Alistarh et al., 2018, Wen et al., 2017, HorvÃ¡th et al., 2022, Panferov et al., 2024] and error feedback. Federated learning [KoneÄnÃ½ et al., 2016, KoneÄnÃ½ et al., 2016, McMahan et al., 2016, Kairouz et al., 2019] is a decentralized paradigm where multiple clients collaboratively train a model on their local, private data. The growing demand for training massive deep neural networks with billions of parameters on vast datasets [Brown et al., 2020, Kolesnikov et al., 2020] has intensified the ML communityâ€™s interest in distributed optimization. To achieve feasible training times [Li, 2020], distributing computation, especially stochastic gradient evaluations, is essential, driving the adoption of scalable algorithms [Goyal et al., 2017, You et al., 2019, Le Scao et al., 2023]. Our work is motivated by the need to bridge this gap by extending a theoretically sound LoRA framework to these advanced and practically vital optimization scenarios.",
      "index": 10,
      "section": "Motivation"
    },
    {
      "type": "text",
      "text": "4 Contributions\n\nThe performance of LoRA-based methods is notably sensitive to the selection of hyperparameters [Khodak et al., 2021, Kuang et al., 2024], and a robust theoretical understanding to guide their application is still developing. While recent work by Malinovsky et al. [2024] on RAC-LoRA provided initial steps towards a rigorous analytical framework, we aim to further advance the theoretical foundations and practical versatility of low-rank adaptation techniques.\n\nIn PEFT approaches based on low-rank adaptation, two matrices, A and B, are typically updated. Existing methods may update only A, only B, or alternate between them deterministically [Malinovsky et al., 2024, Xia et al., 2024, Zhu et al., 2024]. Our primary contribution is the introduction of Bernoulli-LoRA, a novel and generic low-rank adaptation framework. Bernoulli-LoRA is characterized by its unique probabilistic update mechanism: at each step of the adaptation process, a Bernoulli trial (akin to a coin flip) determines which of the two matrices (A or B) is selected for optimization, while the other matrix is sampled from a predefined distribution and remains fixed for that step. This randomized selection not only provides a flexible approach but also unifies and generalizes several existing update strategies within a single theoretical construct. Much like the iterative design of COLA [Xia et al., 2024], the Bernoulli-LoRA framework operates by applying a sequence of such probabilistically chosen low-rank updates.\n\nOur theoretical analysis is grounded in standard assumptions common in non-convex optimization literature, such as L-smoothness of the objective function. We instantiate the Bernoulli-LoRA framework by developing and analyzing several distinct algorithmic variants. These variants span a range of optimization techniques, from foundational gradient-based methods to more advanced stochastic, variance-reduced, and federated learning algorithms, each designed to address specific challenges in modern machine learning. For every proposed method within the Bernoulli-LoRA framework, we establish rigorous convergence guarantees. Our key contributions, which advance the theoretical understanding",
      "index": 11,
      "section": "Introduction"
    },
    {
      "type": "text",
      "text": "and practical applicability of LoRA-type methods, include:",
      "index": 12,
      "section": "Introduction"
    },
    {
      "type": "text",
      "text": "â—† Foundational Algorithmic Variants: We begin by establishing the theoretical properties of Bernoulli-LoRA with two fundamental optimization schemes. These methods lay the groundwork for understanding how the randomized selection of $A$ or $B$ interacts with standard descent procedures in the context of low-rank updates.\n\nâ€“ Bernoulli-LoRA-GD (Algorithm 2) serves as the simplest instantiation, employing full gradient descent to update the trainable low-rank matrix. While computing the full gradient is often impractical for large-scale models, this variant provides crucial foundational understanding of the framework's convergence behavior under idealized conditions, navigating the optimization landscape defined by the LoRA reparameterization.\n\nâ€“ Bernoulli-LoRA-SGD (Algorithm 4) offers a more practical and widely applicable alternative by utilizing stochastic gradients. This variant addresses the computational burden of full gradient methods and is a cornerstone for larger-scale learning tasks, providing insights into the interplay of stochasticity and randomized matrix adaptation.",
      "index": 13,
      "section": "Introduction"
    },
    {
      "type": "text",
      "text": "â—† Advanced Variance Reduction Techniques for Non-Convex Optimization: Stochastic gradients, while efficient, introduce variance that can impede convergence. Integrating variance reduction (VR) into the LoRA structure, particularly with the additional Bernoulli randomization, presents unique analytical challenges. Our work addresses this by developing specific VR-enhanced variants for Bernoulli-LoRA. To the best of our knowledge, we provide the first theoretical analyses demonstrating provable benefits for LoRA-type methods incorporating advanced VR schemes in $L$-smooth non-convex settings. Specifically, we propose:\n\nâ€“ Bernoulli-LoRA-PAGE (Algorithm 6): Tailored for the finite-sum setting (2), this method integrates the Probabilistic Gradient Estimator (PAGE) [Li et al., 2021]. PAGE is recognized for achieving optimal non-convex convergence rates and implementation simplicity, and we successfully adapt it to the Bernoulli-LoRA context.\n\nâ€“ Bernoulli-LoRA-MVR (Algorithm 5): For the infinite-sum (expectation) setting, this variant employs Momentum Variance Reduction techniques inspired by STORM [Cutkosky and Orabona, 2019]. MVR offers an efficient batch-free approach to VR, and our work demonstrates its compatibility and effectiveness within the Bernoulli-LoRA paradigm.",
      "index": 14,
      "section": "Introduction"
    },
    {
      "type": "text",
      "text": "â—† Communication-Efficient Federated Learning Extensions: The application of PEFT methods like LoRA in Federated Learning (FL) is promising but requires careful consideration of communication overhead and data heterogeneity. We extend Bernoulli-LoRA to FL by designing three specialized algorithms that combine our randomized adaptation with established FL communication-saving techniques. To the best of our knowledge, this constitutes the first comprehensive theoretical analysis of LoRA-type methods integrated with established communication-efficient FL techniques such as quantization, gradient difference compression, and error feedback. Our FL extensions include:\n\nâ€“ Fed-Bernoulli-LoRA-QGD (Algorithm 7): This method tackles high communication bandwidth by incorporating QSGD-style quantization [Alistarh et al., 2017, Wen et al., 2017, HorvÃ¡th et al., 2022, Panferov et al., 2024], enabling clients to transmit compressed gradient information, a crucial feature for practical FL deployments.\n\nâ€“ Fed-Bernoulli-LoRA-MARINA (Algorithm 8): We adapt the MARINA communication compression strategy [Gorbunov et al., 2021], which efficiently compresses gradient differences, to the Bernoulli-LoRA framework. This is particularly beneficial for non-convex distributed learning over potentially heterogeneous datasets.",
      "index": 15,
      "section": "Introduction"
    },
    {
      "type": "text",
      "text": "| Setting | Method | NC convergence rate | PÅ convergence rate |\n| ------- | -------------------------------------- | ------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------- |\n| (1) | Bernoulli-LoRA-GD (Alg. 2) | $\\dfrac{\\Delta^0}{\\gamma \\lambda_{\\min} T}$ | $(1 - \\gamma \\mu \\lambda_{\\min})^T \\Delta^0$ |\n| (1) | Bernoulli-LoRA-SGD (Alg. 4) | $\\dfrac{\\Delta^0}{\\gamma \\lambda_{\\min} T} + \\dfrac{\\gamma L C_1 \\lambda_{\\max}}{\\lambda_{\\min}}$ | $(1 - \\gamma \\mu \\lambda_{\\min})^T \\Delta^0 + \\dfrac{\\gamma L C_1 \\lambda_{\\max}}{\\mu \\lambda_{\\min}}$ |\n| (1)+(3) | Bernoulli-LoRA-MVR (Alg. 5) | $\\dfrac{\\Phi_1}{\\gamma \\lambda_{\\min} T} + \\dfrac{b \\sigma^2 \\lambda_{\\max}}{(2-b)\\lambda_{\\min}}$ (1) | $(1 - \\gamma \\mu \\lambda_{\\min})^T \\Phi_1 + \\dfrac{b \\sigma^2 \\lambda_{\\max}}{(2-b)\\mu \\lambda_{\\min}}$ (1) |\n| (1)+(2) | Bernoulli-LoRA-PAGE (Alg. 6) | $\\dfrac{\\Phi_2}{\\gamma \\lambda_{\\min} T}$ (2) | $(1 - \\gamma \\mu \\lambda_{\\min})^T \\Phi_2$ (2) |\n| (1)+(4) | Fed-Bernoulli-LoRA-QGD (Alg. 7) | $\\dfrac{\\Delta^0}{\\gamma \\lambda_{\\min} T} + \\dfrac{\\gamma L \\omega \\Delta^* \\lambda_{\\max}}{M \\lambda_{\\min}}$ | $(1 - \\gamma \\mu \\lambda_{\\min})^T \\Delta^0 + \\dfrac{\\gamma L^2 \\omega \\lambda_{\\max}}{M \\mu \\lambda_{\\min}}$ |\n| (1)+(4) | Fed-Bernoulli-LoRA-MARINA (Alg. 8) | $\\dfrac{\\Phi_2}{\\gamma \\lambda_{\\min} T}$ (2) | $(1 - \\gamma \\mu \\lambda_{\\min})^T \\Phi_2$ (2) |\n| (1)+(4) | Fed-Bernoulli-LoRA-EF21 (Alg. 9) | $\\dfrac{\\Phi_3}{\\gamma \\lambda_{\\min} T}$ (3) | $(1 - \\gamma \\mu \\lambda_{\\min})^T \\Phi_3$ (3) |\n\n(1) $\\Phi_1 := \\Delta^0 + \\dfrac{b \\gamma}{2-b}\\,\\mathcal{G}^0$;\n\n(2) $\\Phi_2 := \\Delta^0 + \\dfrac{\\gamma}{q}\\,\\mathcal{G}^0$;\n\n(3) $\\Phi_3 := \\Delta^0 + \\dfrac{\\gamma}{1-\\sqrt{1-\\beta}}\\,\\mathcal{G}^0$.\n\nTable 1: Summary of the convergence rates for the proposed methods, presented for smooth non-convex functions (â€œNCâ€) and for functions satisfying the PÅ-condition (â€œPÅâ€). Absolute constant factors are omitted. Notation: $\\Delta^0 := f(W^0) - f^*$; $\\mathcal{G}^0 := \\|G^0 - \\nabla f(W^0)\\|_F^2$; $\\hat{\\mathcal{G}}^0 := \\dfrac{1}{M}\\sum_{l=1}^M \\|G_l^0 - \\nabla f_l(W^0)\\|_F^2$; $T$ is the chain length; $\\omega$ is the compression parameter; $\\Delta^* := f^* - \\dfrac{1}{M}\\sum_{l=1}^M f_l^*$; $C_1$ is a constant from Asm. 4; $q$ is the probability of a full gradient computation; $\\beta$ is the contractive compression parameter; $b$ is the momentum parameter; $\\lambda_{\\min} = \\lambda_{\\min}^p := p\\,\\lambda_{\\min}^{HB} + (1-p)\\,\\lambda_{\\min}^{HA}$, and $\\lambda_{\\max} = \\lambda_{\\max}^p := p\\,\\lambda_{\\max}^{HB} + (1-p)\\,\\lambda_{\\max}^{HA}$.\n\nâ€” Fed-Bernoulli-LoRA-EF21 (Algorithm 9): This algorithm integrates the modern EF21 error-feedback mechanism [RichtÃ¡rik et al., 2021]. Error feedback is vital for stabilizing training with contractive compressors, and we show how Bernoulli-LoRA can leverage this for robust distributed fine-tuning.\n\nâ—† Analysis for Non-Smooth Convex Functions: Recognizing that not all machine learning objectives are smooth, we broaden the applicability of our framework. To the best of our knowledge, we present the first theoretical analysis of LoRA-type methods specifically for the important class of non-smooth convex optimization problems. For this setting, we provide versions of Bernoulli-LoRA-GD (Algorithm 3) and establish their convergence rates for both constant stepsize policies and adaptive Polyak-type stepsizes, showcasing the versatility of the Bernoulli-LoRA approach beyond smooth, non-convex settings.",
      "index": 16,
      "section": "Introduction"
    },
    {
      "type": "text",
      "text": "## 5 Notation\n\nFor matrices $W \\in \\mathbb{R}^{m \\times n}$, where $m$ and $n$ denote the input and output dimensions respectively, we employ the Frobenius norm $\\|\\cdot\\|_F$, defined as $\\|W\\|_F = \\sqrt{\\mathrm{Tr}(W^\\top W)}$, where $\\mathrm{Tr}(\\cdot)$ denotes the matrix trace. The inner product between two matrices $A$ and $B$ is denoted by $\\langle A, B \\rangle = \\mathrm{Tr}(A^\\top B)$. In our low-rank adaptation framework, $B \\in \\mathbb{R}^{m \\times r}$ and $A \\in \\mathbb{R}^{r \\times n}$ represent the factors of rank $r \\ll \\min\\{m,n\\}$. We use $\\mathcal{O}(\\cdot)$ to hide absolute constants. We denote $\\Delta^0 := f(W^0) - f^*$, $\\mathcal{G}^0 := \\|G^0 - \\nabla f(W^0)\\|_F^2$ and $\\hat{\\mathcal{G}}^0 := \\frac{1}{M}\\sum_{l=1}^M \\|G_l^0 - \\nabla f_l(W^0)\\|_F^2$. For differentiable functions $f$, the gradient $\\nabla f(W) \\in \\mathbb{R}^{m \\times n}$ is computed with respect to the trace inner product, while for non-smooth functions, the subgradient $\\partial f(W) \\in \\mathbb{R}^{m \\times n}$ is similarly defined. The superscript $^\\dagger$ denotes the Mooreâ€“Penrose pseudoinverse.",
      "index": 17,
      "section": "Preliminaries"
    },
    {
      "type": "text",
      "text": "6 Bernoulli-LoRA Framework\n\nIn this section, we introduce the Bernoulli-LoRA framework, a novel and generic approach for low-rank adaptation. The core idea is to perform a sequence of low-rank updates, where at each step, a probabilistic choice determines which of the two factor matrices (A or B) is trained. This randomized mechanism, formalized in Algorithm 1, not only provides a flexible and unifying theoretical construct for existing LoRA-style methods but also allows for a rigorous convergence analysis.\n\nAt each iteration, one of the two low-rank matrices is sampled from a fixed distribution and remains frozen, while the other is trained to minimize the objective. This strategy prevents optimization from being confined to a fixed subspace, reducing the risk of converging to a suboptimal point. We formalize these two configurations as Left and Right sketch updates.\n\nDefinition 1 (Left Sketch). The left sketch update rule is given by\nÎ”W = (Î±/r) B_S Ã‚, (5)\nwhere B_S âˆ¼ ð’Ÿ_B is sampled from a fixed distribution over â„^{mÃ—r} matrices, and only the matrix Ã‚ âˆˆ â„^{rÃ—n} is adjustable.\n\nDefinition 2 (Right Sketch). The right sketch update rule is given by\nÎ”W = (Î±/r) Ì‚B A_S, (6)\nwhere A_S âˆ¼ ð’Ÿ_A is sampled from a fixed distribution over â„^{rÃ—n} matrices, and only the matrix Ì‚B âˆˆ â„^{mÃ—r} is adjustable.\n\nAlgorithm 1 Bernoulli-LoRA Framework\n1: Parameters: pre-trained model W^0 âˆˆ â„^{mÃ—n}, rank r â‰ª min{m,n}, scaling factor Î± > 0, chain length T, sketch distributions ð’Ÿ_S^B and ð’Ÿ_S^A, Bernoulli probability p.\n2: for t = 0, 1, â€¦, T âˆ’ 1 do\n3: Sample c^t âˆ¼ Be(p) (Bernoulli random variable).\n4: if c^t = 1 then\n5: Sample B_S^t âˆ¼ ð’Ÿ_S^B (Left sketch).\n6: Using a chosen optimizer, approximately solve Ã‚^t â‰ˆ argmin_A f(W^t + (Î±/r) B_S^t A).\n7: W^{t+1} = W^t + (Î±/r) B_S^t Ã‚^t.\n8: else\n9: Sample A_S^t âˆ¼ ð’Ÿ_S^A (Right sketch).\n10: Using a chosen optimizer, approximately solve Ì‚B^t â‰ˆ argmin_B f(W^t + (Î±/r) B A_S^t).\n11: W^{t+1} = W^t + (Î±/r) Ì‚B^t A_S^t.\n12: end if\n13: end for",
      "index": 18,
      "section": "Method"
    },
    {
      "type": "text",
      "text": "6.1 Reformulation as a Projected Gradient Step\n\nBuilding upon the work of Malinovsky et al. [2024] on their RAC-LoRA framework, the update steps in Algorithm 1 can be reformulated as projected gradient steps. The subproblems in lines 6 and 10 are typically solved approximately; for instance, by taking a single step of a suitable optimizer like Gradient Descent (GD) or its variants.",
      "index": 19,
      "section": "Method"
    },
    {
      "type": "text",
      "text": "Following the approach of Malinovsky et al. [2024], letâ€™s consider the update for the trainable matrix $\\hat{A}^t$ in the Left Sketch case. Taking a single GD step on the subproblem corresponds to minimizing a quadratic approximation of the objective. This yields the solution for $\\hat{A}^t$:\n$$\\hat{A}^t = -\\eta \\left((B_S^t)^\\top B_S^t\\right)^\\dagger (B_S^t)^\\top \\nabla f(W^t),$$\nwhere $\\eta$ is a learning rate for the subproblem and \\dagger denotes the Moore-Penrose pseudoinverse. Substituting this into the update for $W^{t+1}$ gives:\n$$W^{t+1} = W^t + \\frac{\\alpha}{r} B_S^t \\hat{A}^t = W^t - \\frac{\\alpha\\eta}{r} B_S^t \\left((B_S^t)^\\top B_S^t\\right)^\\dagger (B_S^t)^\\top \\nabla f(W^t)$$\n$$= W^t - \\gamma H_B^t \\nabla f(W^t),$$\nwhere we define the effective stepsize $\\gamma := \\frac{\\alpha\\eta}{r}$ and the projection matrix $H_B^t := B_S^t \\left((B_S^t)^\\top B_S^t\\right)^\\dagger (B_S^t)^\\top.$\nA similar derivation for the Right Sketch case gives the update:\n$$W^{t+1} = W^t - \\gamma \\nabla f(W^t) H_A^t,$$\nwhere $H_A^t := (A_S^t)^\\top \\left(A_S^t (A_S^t)^\\top\\right)^\\dagger A_S^t$. This reformulation reveals that both Left and Right sketch updates are equivalent to applying a standard gradient-based update, but projected onto a randomly chosen low-rank subspace.\n\nWhile RAC-LoRA employs a deterministic choice for which matrix to update, our Bernoulli-LoRA framework generalizes this concept by introducing a probabilistic selection at each step. This allows us to express the update for any of our proposed methods in a single, unified form:\n$$W^{t+1} = W^t - \\gamma \\hat{G}^t, \\tag{7}$$\nwhere $\\hat{G}^t$ is the projected gradient estimator. It is formed by taking a base gradient estimator $G^t$ (e.g., a full gradient, a stochastic gradient, or a variance-reduced one) and projecting it based on the outcome of a Bernoulli trial:\n$$\\hat{G}^t = \\begin{cases}\nH_B^t G^t, & \\text{with probability } p \\\\\nG^t H_A^t, & \\text{with probability } 1-p\n\\end{cases}. \\tag{8}$$\nThe specific choice of the base estimator $G^t$ defines the particular algorithm within the Bernoulli-LoRA family. We summarize our proposed methods in Table 2 and describe them next.",
      "index": 20,
      "section": "Method"
    },
    {
      "type": "text",
      "text": "6.2 Core Algorithmic Variants\n\nBernoulli-LoRA-GD. The simplest instantiation of our framework is Bernoulli-LoRA-GD (Algorithm 2). This method serves as a foundational building block and a starting point for more elaborate variants. It uses the full gradient of the objective function as its base estimator, i.e., $G^t = \\nabla f(W^t)$. While impractical for large-scale deep learning, its analysis provides crucial insights into the convergence behavior of the Bernoulli-LoRA mechanism under idealized, deterministic conditions.\n\nBernoulli-LoRA-SGD. Stochastic Gradient Descent (SGD) [Robbins and Monro, 1951] is a highly effective and widely utilized algorithm for training a variety of machine learning models. The latest advancements in deep learning training methods are all based on different variations of SGD [Sun, 2020]. Its advantage over GD is that it uses stochastic gradients for updates, rather than relying on full gradients. Within our framework, we develop Bernoulli-LoRA-SGD, where the base estimator $G^t$ is a general unbiased stochastic gradient of $f$ at $W^t$.",
      "index": 21,
      "section": "Method"
    },
    {
      "type": "text",
      "text": "| Setting | Method | Base Gradient Estimator $G^t$ | Thms. # |\n| ------- | ------ | ----------------------------- | -------- |\n| (1) | Bernoulli-LoRA-GD (Algs. 2 & 3) | $G^t = \\nabla f(W^t)$ | 1, 9, 10 |\n| (1) | Bernoulli-LoRA-SGD (Alg. 4) | $G^t = g(W^t)$ | 11, 12 |\n| (1)+(3) | Bernoulli-LoRA-MVR (Alg. 5) | $G^t = \\nabla f_{\\xi^t}(W^t) + (1-b)\\big(G^{t-1} - \\nabla f_{\\xi^t}(W^{t-1})\\big)$ | 3, 14 |\n| (1)+(2) | Bernoulli-LoRA-PAGE (Alg. 6) | $G^t = \\begin{cases} \\nabla f(W^t), & \\text{w.p. } q \\\\[4pt] G^{t-1} + \\nabla f_{i_t}(W^t) - \\nabla f_{i_t}(W^{t-1}), & \\text{w.p. } 1-q \\end{cases}$ | 4, 16 |\n| (1)+(4) | Fed-Bernoulli-LoRA-QGD (Alg. 7) | $G^t = \\frac{1}{M} \\sum_{l=1}^M \\mathcal{Q}_l^t\\!\\big(\\nabla f_l(W^t)\\big)$ | 17, 18 |\n| (1)+(4) | Fed-Bernoulli-LoRA-MARINA (Alg. 8) | $\\forall l:\\; G_l^t = \\begin{cases} \\nabla f_l(W^t), & \\text{w.p. } q \\\\[4pt] G_l^{t-1} + \\mathcal{Q}_l^t\\!\\big(\\nabla f_l(W^t) - \\nabla f_l(W^{t-1})\\big), & \\text{w.p. } 1-q \\end{cases}$; $G^t = \\frac{1}{M} \\sum_{l=1}^M G_l^t$ | 6, 20 |\n| (1)+(4) | Fed-Bernoulli-LoRA-EF21 (Alg. 9) | $\\forall l:\\; G_l^t = G_l^{t-1} + \\mathcal{C}_l^t\\!\\big(\\nabla f_l(W^t) - G_l^{t-1}\\big)$; $G^t = \\frac{1}{M} \\sum_{l=1}^M G_l^t$ | 7, 22 |\n\nTable 2: Description of the methods developed and analyzed in this paper. All methods follow the general update rule $W^{t+1} = W^t - \\gamma \\hat{G}^t$, where the projected estimator $\\hat{G}^t$ is defined in (8). The table specifies the definition of the base gradient estimator $G^t$ for each method. The projection matrices are $H_A^t := (A_S^t)^\\top \\big( A_S^t (A_S^t)^\\top \\big)^{\\dagger} A_S^t$ and $H_B^t := B_S^t \\big( (B_S^t)^\\top B_S^t \\big)^{\\dagger} (B_S^t)^\\top$.",
      "index": 22,
      "section": "Method"
    },
    {
      "type": "text",
      "text": "Bernoulli-LoRA-PAGE. Several optimal algorithms exist for addressing non-convex optimization problems, such as SPIDER [Fang et al., 2018] and SARAH [Pham et al., 2020]. However, their optimality is supported by a known lower bound that applies only in the small data setting. In contrast, Probabilistic Gradient Estimator (PAGE) [Li et al., 2021] stands out for its simplicity, ease of implementation, and ability to achieve optimal convergence in non-convex optimization. PAGE alternates between a full gradient update with probability $q_t$ and a low-cost gradient adjustment with probability $1 - q_t$. Bernoulli-LoRA-PAGE is a new method based on PAGE within our Bernoulli-LoRA framework.",
      "index": 23,
      "section": "Method"
    },
    {
      "type": "text",
      "text": "Bernoulli-LoRA-MVR. VR methods outperform SGD in reaching first-order critical points but often require finely tuned learning rates and large batch sizes to be effective. To overcome these challenges, Momentum Variance Reduction (MVR) [Cutkosky and Orabona, 2019] was introduced for server-only stochastic non-convex optimization. MVR uses a modified momentum technique to reduce variance without relying on large batch sizes. Several works employ this powerful approach [Tyurin and RichtÃ¡rik, 2023, Karagulyan et al., 2024]. We propose Bernoulli-LoRA-MVR, where the base estimator $G^t$ is updated using the MVR rule: a combination of the current stochastic gradient and a momentum term that incorporates the difference between past estimators and gradients.",
      "index": 24,
      "section": "Method"
    },
    {
      "type": "text",
      "text": "6.3 Extensions for Federated Learning\n\nSun et al. [2024] identified instability in LoRA, arising from the mismatch between local clients simultaneously optimizing two low-rank matrices and the central server aggregating them independently. Factors such as data heterogeneity, multi-step local updates, and the amplification of additive noise applied to gradients for ensuring differential privacy (DP) significantly impact the process. Additionally, the final performance is highly sensitive to hyperparameter choices. Their proposed solution centers on keeping the randomly initialized non-zero matrices fixed while exclusively fine-tuning the zero-initialized ones. Based on this asymmetric approach, Malinovsky et al. [2024] proposed a distributed method Fed-RAC-LoRA. We develop the theory further by incorporating compression, VR and EF techniques into FL methods for LoRA within the novel Bernoulli-LoRA framework.",
      "index": 25,
      "section": "Method"
    },
    {
      "type": "text",
      "text": "The effectiveness of a distributed training method is primarily measured by its communication complexity, defined as the product of the required communication rounds and the communication volume per round. Following common practice, we assume client-to-server communication is the main bottleneck and exclude server-to-client communication from our analysis.",
      "index": 26,
      "section": "Method"
    },
    {
      "type": "text",
      "text": "Fed-Bernoulli-LoRA-QGD. A key challenge for distributed methods lies in the high communication cost of gradient updates. Lossy compression techniques, such as QSGD [Alistarh et al., 2017], address this by enabling clients to send quantized gradients. We design Fed-Bernoulli-LoRA-QGD based on QSGD. The clients send compressed versions of their gradients. The base estimator G^t is formed by averaging the compressed local gradients received from all clients.",
      "index": 27,
      "section": "Method"
    },
    {
      "type": "text",
      "text": "Fed-Bernoulli-LoRA-MARINA. MARINA [Gorbunov et al., 2021] is a communication-efficient method for non-convex distributed learning on heterogeneous datasets that uses a novel gradient difference compression strategy. Its biased gradient estimator underpins its strong theoretical and practical performance, with proven communication complexity bounds surpassing all prior first-order methods. We propose Fed-Bernoulli-LoRA-MARINA, where each clientâ€™s local estimator G_l^t is updated either with a full local gradient (with probability q) or by adding a compressed gradient difference to its previous estimator. The serverâ€™s base estimator G^t is the average of these local estimators.",
      "index": 28,
      "section": "Method"
    },
    {
      "type": "text",
      "text": "Fed-Bernoulli-LoRA-EF21. Error Feedback (EF) [Seide et al., 2014, Stich et al., 2018, Alistarh et al., 2018, RichtÃ¡rik et al., 2021] is a widely adopted technique for stabilizing training with contractive compressors. We propose Fed-Bernoulli-LoRA-EF21, based on the modern EF21. Here, each client updates its local estimator G_l^t by adding a compressed version of the difference between the current local gradient and the previous local estimator. The serverâ€™s base estimator G^t is again the average of the clientsâ€™ estimators.",
      "index": 29,
      "section": "Method"
    },
    {
      "type": "text",
      "text": "7 Convergence Results\n\nThe convergence properties of our framework hinge on the spectral properties of the expected projection matrix, which is introduced in Section 6.1. The magnitude of its eigenvalues, particularly the smallest (and in some cases, the largest), is a crucial factor that governs the optimization dynamics.\n\nAssumption 1. (Positive Expected Projection) Consider a projection matrix H generated through either Left Sketch (Definition 1) or Right Sketch (Definition 2). For the sampling distributions ð’Ÿ_S^B and ð’Ÿ_S^A, the smallest eigenvalue of the expected projection matrix is strictly positive:\n$$\\lambda_{\\min}^H = \\lambda_{\\min}[\\mathbb{E}[H]] > 0.$$\n\nAssumption 2. (Lower Bounded Function) The objective function f has a finite infimum f^* âˆˆ â„.\n\nRemark 1 (On the Practicality of Assumption 1). Assumption 1 is a mild and standard requirement, as it is satisfied by common practical choices for the sampling distributions ð’Ÿ_S^B and ð’Ÿ_S^A. For instance, a prevalent strategy [Xia et al., 2024, Mao et al., 2025] is to sample the entries of the fixed matrix from an i.i.d. Gaussian distribution. As shown in Appendix B (Lemma 2), this choice leads to an expected projection matrix ð”¼[H] = (r/n) I_n, where r is the rank and n is the relevant dimension. Consequently, Î»_min^H = r/n > 0, readily satisfying the assumption.\n\nFollowing classical optimization literature [Nemirovski et al., 2009, Beck, 2017, Duchi, 2018, Lan, 2020, Drusvyatskiy, 2020, Nesterov, 2018], we characterize convergence guarantees for two distinct",
      "index": 30,
      "section": "Method"
    },
    {
      "type": "text",
      "text": "In the non-smooth convex case, our objective is to find an $\\varepsilon$-suboptimal solution: a random matrix $\\hat{W} \\in \\mathbb{R}^{m \\times n}$ that satisfies\n$$\\mathbb{E}\\left[f(\\hat{W}) - f(W^*)\\right] \\leq \\varepsilon,$$\n(9)\nwhere $\\mathbb{E}[\\cdot]$ denotes the expectation with respect to the algorithm's randomness, and $W^*$ is any minimizer of $f$. This same measure of performanceâ€”function value suboptimalityâ€”is also used to characterize convergence under the Polyak-Åojasiewicz condition, which we introduce later. For the smooth non-convex setting, where finding global minima is generally intractable, we instead aim to locate an $\\varepsilon$-stationary point: a random matrix $\\hat{W} \\in \\mathbb{R}^{m \\times n}$ satisfying\n$$\\mathbb{E}\\left[\\left\\|\\nabla f(\\hat{W})\\right\\|_{\\mathrm{F}}^2\\right] \\leq \\varepsilon^2.$$\n(10)\nThis condition guarantees that the expected squared norm of the gradient at our solution is sufficiently small, indicating proximity to a stationary point. To quantify the efficiency of our algorithms, we analyze their iteration complexityâ€”the number of iterations required to achieve these criteria.\n\nA fundamental assumption in the convergence analysis of gradient-based optimization is the Lipschitz continuity of the gradient [Bubeck, 2015, Nesterov, 2018, Beck, 2017, Demidovich et al., 2023b, Khaled and RichtÃ¡rik, 2023]. This property, often referred to as Lipschitz smoothness, ensures the stability of the optimization trajectory and plays a crucial role in establishing convergence rates [Bottou et al., 2018, Sun, 2020].",
      "index": 31,
      "section": "Method"
    },
    {
      "type": "text",
      "text": "Assumption 3. (Lipschitz Smooth Gradient) A function $f$ is differentiable, and there exists a constant $L > 0$ such that\n$$\\|\\nabla f(W) - \\nabla f(V)\\|_{\\mathrm{F}} \\leq L \\|W - V\\|_{\\mathrm{F}},$$\nfor all $W, V \\in \\mathbb{R}^{m \\times n}$.",
      "index": 32,
      "section": "Method"
    },
    {
      "type": "text",
      "text": "A significant challenge arises when applying LoRA adaptation directly: the Lipschitz smoothness property is not preserved. Specifically, even if a function $f(W)$ satisfies Assumption 3, its composition with the LoRA parameterization, $f(W^0 + BA)$, generally fails to maintain Lipschitz smoothness with respect to the variables $\\{B, A\\}$. This breakdown complicates the analysis of standard gradient-based methods when applied directly to the LoRA parameterization, as formally demonstrated by Sun et al. [2024]. Our framework, by reformulating the updates as projected steps on the full parameter space, circumvents this issue.\n\nTo unify our analysis, we define a probability-weighted eigenvalue $\\lambda_{\\min(\\max)}^p := p\\lambda_{\\min(\\max)}^{H_B} + (1 - p)\\lambda_{\\min(\\max)}^{H_A}$. Let $\\widetilde{W}^T$ be an iterate drawn randomly from the sequence $\\{W^0, W^1, \\ldots, W^{T-1}\\}$, with the specific sampling distribution depending on the method.\n\nWe begin by presenting the convergence result for the foundational Bernoulli-LoRA-GD method. The proof can be found in Appendix C.1.",
      "index": 33,
      "section": "Method"
    },
    {
      "type": "text",
      "text": "Theorem 1 (Smooth Non-Convex Setting). Let Assumptions 1, 2, and 3 hold, and let the stepsize satisfy $0 < \\gamma \\leq \\frac{1}{L}$. Then the iterates of Bernoulli-LoRA-GD (Algorithm 2), with matrices $\\hat{A}^t$ and $\\hat{B}^t$ computed according to Lemma 3, satisfy\n$$\\mathbb{E}\\left[\\left\\|\\nabla f(\\widetilde{W}^T)\\right\\|_{\\mathrm{F}}^2\\right] \\leq \\frac{2\\Delta^0}{\\gamma \\lambda_{\\min}^p T},$$\nwhere $\\Delta^0 := f(W^0) - f^*$.",
      "index": 34,
      "section": "Method"
    },
    {
      "type": "text",
      "text": "While insightful, full-gradient methods are often impractical for large-scale problems. We therefore extend our analysis to the stochastic setting, where the gradient is replaced by an unbiased estimator g(W). For this, we use the general expected smoothness assumption.\n\nAssumption 4 (Expected Smoothness [Khaled and RichtÃ¡rik, 2023]). The stochastic gradient estimator g(W) satisfies E[||g(W)||_F^2] â‰¤ 2A_1 (f(W) âˆ’ f^*) + B_1 Â· ||âˆ‡f(W)||_F^2 + C_1, for some constants A_1, B_1, C_1 > 0 and all W âˆˆ R^{mÃ—n}.\n\nThe following theorem establishes the convergence for Bernoulli-LoRA-SGD. Its proof is in Appendix C.2.\n\nTheorem 2. Let Assumptions 2, 3, and 4 hold, and let the stepsize satisfy 0 < Î³ â‰¤ min{ 1/(âˆš(L A_1 Î»_max^p) T), (1/(L B_1)) Â· (Î»_max^p/Î»_min^p)^{-1} }. Then the iterates generated by Bernoulli-LoRA-SGD (Algorithm 4) satisfy E[||âˆ‡f(\\widetilde W^T)||_F^2] â‰¤ 6Î”^0/(Î³ Î»_min^p T) + Î³ L C_1 Â· (Î»_max^p/Î»_min^p), where Î”^0 := f(W^0) âˆ’ f^*.",
      "index": 35,
      "section": "Method"
    },
    {
      "type": "text",
      "text": "To analyze our variance-reduced methods, we also consider the more specific bounded variance assumption.\n\nAssumption 5 (Bounded Variance [Nemirovski et al., 2009]). There exists a constant Ïƒ > 0 such that, for all W âˆˆ R^{mÃ—n}, E[âˆ‡f_Î¾(W)] = âˆ‡f(W), E[||âˆ‡f_Î¾(W) âˆ’ âˆ‡f(W)||_F^2] â‰¤ Ïƒ^2.\n\nThe next result establishes convergence for Bernoulli-LoRA-MVR. The proof is in Appendix C.3.\n\nTheorem 3. Let Assumptions 1, 2, 3, and 5 hold, and let the stepsize satisfy 0 < Î³ â‰¤ 1/(L(1 + âˆš(2 Î»_max^p (1 âˆ’ b)^2 / b))). Then the iterates of Bernoulli-LoRA-MVR (Algorithm 5) satisfy E[||âˆ‡f(\\widetilde W^T)||_F^2] â‰¤ 2Î”^0/(Î³ Î»_min^p T) + (ð’¢^0/(bT) + 2bÏƒ^2/(2 âˆ’ b)) Â· (Î»_max^p/Î»_min^p), where Î”^0 := f(W^0) âˆ’ f^* and ð’¢^0 := ||G^0 âˆ’ âˆ‡f(W^0)||_F^2.",
      "index": 36,
      "section": "Method"
    },
    {
      "type": "text",
      "text": "For the finite-sum setting, we analyze Bernoulli-LoRA-PAGE, with its convergence detailed in the following theorem and proven in Appendix C.4.\n\nTheorem 4. Let Assumptions 1, 2, and 3 hold, and let the stepsize satisfy 0 < Î³ â‰¤ 1/(L(1 + âˆš((1 âˆ’ q)/q Â· Î»_max^p))). Then the iterates of Bernoulli-LoRA-PAGE (Algorithm 6) satisfy E[||âˆ‡f(\\widetilde W^T)||_F^2] â‰¤ 2Î”^0/(Î³ Î»_min^p T) + (ð’¢^0/(qT)) Â· (Î»_max^p/Î»_min^p), where Î”^0 := f(W^0) âˆ’ f^* and ð’¢^0 := ||G^0 âˆ’ âˆ‡f(W^0)||_F^2.",
      "index": 37,
      "section": "Method"
    },
    {
      "type": "text",
      "text": "We now shift to our Federated Learning variants. The following theorem provides convergence guarantees for Fed-Bernoulli-LoRA-QGD, with the proof available in Appendix D.1.\n\nTheorem 5. Let Assumptions 1, 2, 3, and 11 hold, and let the stepsize satisfy\n0 < Î³ â‰¤ min{ 1/(Lâˆš(Ï‰/M) Î»_max^p T), (1/L)(Î»_max^p/Î»_min^p)^{-1} }. Then the iterates of Fed-Bernoulli-LoRA-QGD (Algorithm 7) satisfy\nE[â€–âˆ‡f(áº†^T)â€–_F^2] â‰¤ 6Î”^0/(Î³ Î»_min^p T) + (2Î³ L Ï‰ Î”^*/M) Â· (Î»_max^p/Î»_min^p),\nwhere Î”^0 := f(W^0) âˆ’ f^*.\n\nNext, we present the convergence result for Fed-Bernoulli-LoRA-MARINA. The proof can be found in Appendix D.2.\n\nTheorem 6. Let Assumptions 1, 2, and 3 hold, and let the stepsize satisfy\n0 < Î³ â‰¤ 1/(L(1 + âˆš((1/Î»_max^p) Â· ((1âˆ’q)/q) Â· (Ï‰/M)))).\nThen the iterates of Fed-Bernoulli-LoRA-MARINA (Algorithm 8) satisfy\nE[â€–âˆ‡f(áº†^T)â€–_F^2] â‰¤ 2Î”^0/(Î³ Î»_min^p T) + (ð’¢^0/(qT)) Â· (Î»_max^p/Î»_min^p),\nwhere Î”^0 := f(W^0) âˆ’ f^* and ð’¢^0 := â€–G^0 âˆ’ âˆ‡f(W^0)â€–_F^2.\n\nThe convergence of Fed-Bernoulli-LoRA-EF21 is established below, with a detailed proof in Appendix D.3.\n\nTheorem 7. Let Assumptions 1, 2, and 3 hold, and let the stepsize satisfy\n0 < Î³ â‰¤ 1/(L(1 + âˆš(Î»_max^p(1âˆ’Î²)/(1âˆ’âˆš(1âˆ’Î²))))).\nThen the iterates of Fed-Bernoulli-LoRA-EF21 (Algorithm 9) satisfy\nE[â€–âˆ‡f(áº†^T)â€–_F^2] â‰¤ 2Î”^0/(Î³ Î»_min^p T) + (2Äœ^0/(Î²T)) Â· (Î»_max^p/Î»_min^p),\nwhere Î”^0 := f(W^0) âˆ’ f^* and Äœ^0 := (1/M)âˆ‘_{l=1}^M â€–G_l^0 âˆ’ âˆ‡f_l(W^0)â€–_F^2.",
      "index": 38,
      "section": "Method"
    },
    {
      "type": "text",
      "text": "To obtain stronger, linear convergence rates, we introduce the Polyakâ€“Åojasiewicz condition, a common generalization of strong convexity.\n\nAssumption 6 (Polyakâ€“Åojasiewicz condition [Polyak, 1963, Lojasiewicz, 1963]). There exists Î¼ > 0 such that\n(1/2)â€–âˆ‡f(W)â€–_F^2 â‰¥ Î¼(f(W) âˆ’ f^*).\n\nThe next theorem states the convergence of Bernoulli-LoRA-SGD under this condition. It is proven in Appendix C.2.\n\nTheorem 8. Let Assumptions 2, 3, 4, and 6 hold, and let the stepsize satisfy\n0 < Î³ â‰¤ min{ Î¼Î»_min^p/(2LA_1Î»_max^p), 2/(Î¼Î»_min^p), (1/(LB_1))(Î»_max^p/Î»_min^p)^{-1} }. Then the iterates of Bernoulli-LoRA-SGD (Algorithm 4) satisfy\nE[f(W^T) âˆ’ f^*] â‰¤ (1 âˆ’ Î³Î¼Î»_min^p/2)^T Î”^0 + (Î³LC_1/Î¼) Â· (Î»_max^p/Î»_min^p),\nwhere Î”^0 := f(W^0) âˆ’ f^*.\n\nAll other PÅ-condition results are relegated to the Appendix.",
      "index": 39,
      "section": "Method"
    },
    {
      "type": "text",
      "text": "## 8 Experiments\n\nTo validate our theoretical findings, we conducted numerical experiments across multiple machine learning tasks.",
      "index": 40,
      "section": "Experiments"
    },
    {
      "type": "text",
      "text": "\\n\\n\\n\\n\n### 8.1 Linear Regression with Non-convex Regularization.\n\nWe begin with a controlled linear regression problem with non-convex regularization, split into pre-training and fine-tuning phases. We use (~) for pre-training quantities and (^) for fine-tuning. During the pre-training phase, we solve\n\n$$\n\\min_{x \\in \\mathbb{R}^n} \\left\\{ \\tilde{f}(x) := \\frac{1}{2\\tilde{m}} \\left\\| \\tilde{D}x - \\tilde{b} \\right\\|_2^2 + \\tilde{\\lambda} \\sum_{j=1}^d \\frac{x_j^2}{1 + x_j^2} \\right\\},\n$$\n(11)\n\nwhere $\\tilde{D} \\in \\mathbb{R}^{\\tilde{m} \\times n}$, $\\tilde{b} \\in \\mathbb{R}^{\\tilde{m}}$, $\\tilde{m} = 9 \\times 10^4$, and $n = 4096$. We set $\\tilde{\\lambda} = \\left\\| \\tilde{D} \\right\\|_2 \\approx 18.2$, giving $\\tilde{L} \\approx 54.7$. We optimize until $\\|\\nabla \\tilde{f}(\\tilde{x}^*)\\|^2 \\le 10^{-8}$ to obtain $\\tilde{x}^*$. For the fine-tuning phase, we use $\\tilde{x}^*$ as the initialization and then solve\n\n$$\n\\min_{x \\in \\mathbb{R}^n} \\left\\{ \\hat{f}(x) := \\frac{1}{2\\hat{m}} \\left\\| \\hat{D}x - \\hat{b} \\right\\|_2^2 + \\hat{\\lambda} \\sum_{j=1}^d \\frac{x_j^2}{1 + x_j^2} \\right\\},\n$$\n(12)\n\nwhere $\\hat{D} \\in \\mathbb{R}^{\\hat{m} \\times n}$, $\\hat{b} \\in \\mathbb{R}^{\\hat{m}}$, and $\\hat{m} = 10^4$. We keep $n = 4096$ and set $\\hat{\\lambda} = \\left\\| \\hat{D} \\right\\|_2 \\approx 4101.7$, yielding $\\hat{L} \\approx 12305.3$. This second phase uses a dataset with notably different characteristics to mirror realistic domain shifts.\n\nStochastic setting. We consider the stochastic setting, comparing RAC-LoRA-SGD, Bernoulli-LoRA-SGD, and Bernoulli-LoRA-PAGE. In all experiments, we use a batch size of 100, which corresponds to 1% of the data.\n\n| ![Graph showing comparison of RAC-LoRA-SGD, Bernoulli-LoRA-SGD and Bernoulli-LoRA-PAGE on linear regression fine-tuning. The x-axis shows Data passes from 0 to 200, and the y-axis shows \\|\\nabla f(x)\\|^2 on a logarithmic scale from 10^{-15} to 10^3. Four curves are shown: Bernoulli-LoRA-PAGE (p=0.5) in red with star markers, Bernoulli-LoRA-SGD (p=0.5) in orange with inverted triangle markers, RAC-LoRA-SGD (A) in brown with left-pointing triangle markers, and RAC-LoRA-SGD (B) in green with right-pointing triangle markers. The Bernoulli-LoRA-PAGE curve shows the steepest descent, dropping from 10^3 to below 10^{-15}, while the other three curves plateau around 10^0 to 10^3.](figure1) |\n| --- |\n\nFigure 1: Comparison of RAC-LoRA-SGD, Bernoulli-LoRA-SGD and Bernoulli-LoRA-PAGE on linear regression fine-tuning. Curves with $p = 0.01, 0.2, \\ldots$ indicate Bernoulli-LoRA sampling parameters. RAC-LoRA-SGD (A) trains $B$ after resampling $A$, while RAC-LoRA-SGD (B) does the reverse. All methods use $\\gamma = c/\\hat{L}$ with $c$ tuned individually.\n\nFigure 1 shows that Bernoulli-LoRA-PAGE successfully reduces variance and converges to the target tolerance, whereas all SGD variants stall at a certain accuracy. This underscores the practical advantage of Bernoulli-LoRA-PAGE over the baseline RAC-LoRA-SGD in the stochastic setting from an optimization standpoint.",
      "index": 41,
      "section": "Experiments"
    },
    {
      "type": "image_url",
      "image_url": {
        "url": "data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEAiQCJAAD/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRT/2wBDAQMEBAUEBQkFBQkUDQsNFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBT/wAARCAETAaQDASIAAhEBAxEB/8QAHQAAAQQDAQEAAAAAAAAAAAAAAAQFBgcBAggDCf/EAGkQAAEDAwEDBAkLDQwGBwYHAAECAwQABREGBxIhCBMVMRQWIkFRVZOU0TJSVFZhcZGWstLTGCMkNDU2YnN0gbG01AkXMzhCRFNydpKhsyUmRmODlUV1goSjwcMnQ1eGxPAoZGakpeHx/8QAGgEBAAMBAQEAAAAAAAAAAAAAAAIDBAUBBv/EAC0RAQACAgIABAQGAwEBAAAAAAABAgMRBCESMUFhBVGBoRMUInGRwRXR8DIz/9oADAMBAAIRAxEAPwD6p0UUUBRRRQFFFFAUUUUCS5Pri26U80E8420paQrqyATxqjtmd624bQ9nGldVdPaAhi+2mJc+x+16crmueZQ5u57O443v8Ku69fcaf+Ic+SagfJl/i4bKf7J2n9TaoGyRA23x47zvbNoBW4gqx2uTu8Py73K87XG23z7dFldsugEc80lzd7XJ3DIzj7e92qb5cvKK19sosl30zD2foGnNTwHLVA1ym+FIjSnmVpUlUcMEocSN5ScrCVboIIIIHH2i+WvtQ2fbN5Wnl646NesUCJbtNQ27Sw/2UhLfMoDi3GyoKRuBxWT6lXWSK53I52HjW8N/PrqO/OdOlX4dnni/nJ1GPuNz7Rufs+l3RW3D2z6A+Lk79uo6K24e2fZ/8XJ37dXAfJ55bO27aRtf2fWK7auji1SryxbrtHFsil2VhJLnENJ5pKsAgJJOFA7wIxX1Rz4ONa8eWuWvir5MWTFbHFbW8rREx7xPlKq+iduPtn2f/Fyd+3UdE7cfbPs/+Lk79uq1s4HE0yXDXemrUZAnahtUMx97nuyJrSOa3c729lXDGDnNW7hUgnRO3H2z7P8A4uTv26jonbj7Z9n/AMXJ37dU7ja601Mkx48fUNrffkKCGWm5ralOKIJASArJOATw7wp8r3zFUdE7cfbPs/8Ai5O/bqOiduPtn2f/ABcnft1WtkeGgqAzxHDroKcvaNuFos86d2x7P3OxmVO7na5OG9gZ9nUt6J24+2fZ/wDFyd+3VO9bEdp974/zR35Jp7zQVT0Ttx9s+z/4uTv26jonbj7Z9n/xcnft1WtkUFQHWaCqeiduPtn2f/Fyd+3UdE7cfbPs/wDi5O/bqtYEHqNZoKo6J24+2fZ/8XJ37dR0Ttx9s+z/AOLk79uq1iQO/RvDHWKCqeiduPtn2f8Axcnft1HRO3H2z7P/AIuTv26rV3h4RWaCnLw3twtcND3bJoBzefYZ3e1yd/LdSj2d+FS3onbj7Z9n/wAXJ37dU71eQbQ1j2fC/Wmqet9OcZGaCquiduPtn2f/ABcnft1HRO3H2z7P/i5O/bqtbeA74rNBVHRO3H2z7P8A4uTv26jonbj7Z9n/AMXJ37dVrZHho3hx4jhQVT0Ttx9s+z/4uTv26jonbj7Z9n/xcnft1WtvDwijI8NBVPRO3H2z7P8A4uTv26jonbj7Z9n/AMXJ37dVrbwHfoCge/QU5cW9uMB2A32ybP19lSOZz2uTu57havZ34P8AjS3onbj7Z9n/AMXJ37dU81EQZlhwf+kB/ku09ZoKp6J24+2fZ/8AFyd+3UdE7cfbPs/+Lk79uq1sjw0bw48RwoKp6J24+2fZ/wDFyd+3UdE7cfbPs/8Ai5O/bqtbeHhozQVT0Ttx9s+z/wCLk79uo6J24+2fZ/8AFyd+3Vau+nOMis5oKp6J24+2fZ/8XJ37dR0Ttx9s+z/4uTv26rW3h4aMjw0FOSUbcI90gw+2PZ+rskOHf7XZ3c7oB9ne7S3onbj7Z9n/AMXJ37dU8un30WP+rI+SmnugqfovbkOrVGz8f/LU4/8A19FWuKKDNFFFAUUUUBWMjw1mksuEzcIr0aUw1JivJKHWH0BaHEngUqSeBHhHfoMw7lEuPP8AYkpmVzDpZd5lwL5twdaFYPBQyOB48aTQ9RWu4yXosO5RJkpkHfYYfQtacHBykHIweHv1zXpe4K0BsF5Q86wst29y1XrULsJEVAbQypDQKN1KcAYIHVinTWSHNJ6V5OqrKsRJAu8G1odCc/WX4DyXAod8HCVEd8oB71BZEPaNPvF6lafn6ectbrtjXdQrsjnVMJK9wNSEhIDbiskgAqB3HMHueJyZSPqcNlPH/ZO0/qbVMruj9T2a7P6plttOC36clRp0O0KQHr9LIQpDhQoNtpUnm1hBUocXlAlKRknJrvslHJ32WpFhuTgTpW1JC0cxhQ7Ea4jLuce/QbcoTk7WHlBRNOMapn3LoOwzHLku0Qng01PdLK20B5YHOAJStfqFJyFqBzwx8tOUPyZRpLbjdF7Odn+tndPogxFuras8yXDMxTO86qM4lo5bSlSU4UpRCudAITugfYedfpSoMoGwXMAtK4kscOB/3teGnr7KRYbakWG5OBMZoBaeYwruRxGXazZsFc9JrPW+voq5FLcjBPGtaYrP/fd8XNhmnL9auUhs8S/Ev+kxNv8AFgLnvWl1lUWUtBVHUOeaLe+UHO6oHeSQcbozX1s7W9tun4fN27WuldWq3yf9P2R2E7u+DnYzxTn/AIQr5x8rfXe0q2bbrPH2gytOwNS6WktXuzXGwwBGMslzMZb6S+4S4nscjmiTuhasKUFBVd+clraDtV1ts3iaq2iQrbNTfI7Fws7OnIiWg1FWjfSp1xclQWtYUk7qUgJwRlWe5w8XFSs2wVmd0/vydnN8Et8N+H8bPGTxUvExEb3ManuJj0j5bSKVrza/ZVttyNlVsvaQkb79i1SjBPuIkstEe9k+/XzW5cGp4h2u6mtDGziZpWdqliNMu7moLeyHmnm8tvOxHkFaHWnkIYQVJVwWl0+qUrd+tvT8vP3vXTH/AAPpa4Y5R3Ik2lbdtrOoNbG/Mx0yG0RbZb5NvSoRI7SSG2isSeorU4tRA63VYGMCpcvi3y4prE77iY384ne3Cz35OPFb8rP6pjXfu4+5G1ygaZ5SOgJ0+FNuEy03B5GIMRyXIXHVBkpb5pltJKj67dGSW1qOeuvrmvlF6SYRvSIWrY+O87o67j/6auO9hf7n/rbZHty0VtAk3Rq6M2N55UiExCSyp1DkdxruVGQoZBXkAjw8a756fl+166fDH+lq3j0zUxx451PyTpkz58VJz9WiIifp6q5f5WWzeKopfn3mOR1h7TVzRj4Y9fMflY7S9E3XaJreyaFuNwvTOpJaLi7cFuy2Da1Kz2ZF5t4pPdKSlSe43QmStII5sCvr709K72nrmB/wPpa+dnKX5JG37bftm1NrQWKzKgOEQ7PFcu+461Ba3g0gjmyEqWVLcUN4gKcUN7AFZ+Zgy5sWo7mJiY9PKf8AulefJyMOK08bu0xqOvn05y5Me0RVt2zaXu2qb/cFogC5RpLeXVtJKrbJbbUGm8lZWVoVnBIKicgA4+qo5YWygf7QzPfNiuH0FcO7D+RHtf2RbaLHri/2i1osMCLPbnmDcg66OcgyGml7pQjOFu4ODndJ4HFfS4aglDP+rt04/iPpau49ORGONzEfRpplvnx0tlr4bajf0639VafVg7JySBqOUo+5ZJ5/9Cvl7yn9pWkbxrnWmm9A3O4XWHd7kbkLzz8thUFpYCn4nNOlJBD4IT3G6GnMA5RX2N6flH/Z+5+/9Y+lr5ucoTkhcoHbLtg1ZrVzTtqcZmvhq3RlXdKVNQ2u4YbwUkBWMrVxxvrVxxiqOZgzZMca7mJiY115M3JzZ8WG/wCWjdpjX8qp5Eu1aJYOUJp3UmtdRTm2Isa4MSELLzrLD3NbqQhpsHIIJIUQTgjjX0uRyw9koBzqd/Hh6HnfQ1yByR+R1td2H8oi0601Bp2GqxphSY0pNuubbroWpvDawhQQDxwDx8B71fQpN/k4+926f+B9LWjBTPGOO4j6NcZ78ilL5K+G2o3HvHX3Vn9WJsiwc6qcHv2qaP8A0a+VfKI19pK66q1PpjZ5cZlxtrt4cubGoi7JaLDCzznYoQ7hWW5BWgHdwWm0gE5OPs+b7KVn/V66fCx9LXzN2zcjXlCbV9qurNbS9OWhb94nFbLXS6AWoqMNx2/UYyhpKckHirePfrNzcGbJjjXcxMTHoycrNycWG/5WN2mNfyh3IM2v2vTG3pvU2tL9JjMLsk+M62pDrrMeRz0bCG20BR6gvuyCSMZUc19GByxdkeOGqXVe9app/wDRrlDkWckjavsC2/vau1PpyOqxybNIgqTbLi08tD6nGClRSrc4FLWOBPqRXfXT8r2vXT4WPpa04MeeMcd69tNf41uRWMl6+GdRuPfXandS8rfZVOtjbbOon3SJUZxQ6Hm+pTIbUr/3PeSknHfr5abatoNjn3a6aS2e6gu9ztdvuzs6NqR2fMy7HJDkZgoeXvbzTqlJUSkZ7GQQTv8AD7K6pvUpy1thViuLeJsNQUvmMEiQ2QP4Q8TgD3z3uuvmztG5EXKK2j681HrC7WKwu3K+XBUp1tm9cGUEhLbYJZGUtNpQkHrIR4azczBnvSNdzE7jXXt38/8AbHy8vIxYbflY3a0aNP7n/tug6V22TNRa21NdDEk6blxlNP8AZEptmT2XEO4hpAVjAQsbxGeHFRyM/QlPLG2SEffO/wC90RO+hrl3kP8AJM2p8njbpetU6qsMVdkn2B+CkWmeh9aZKpEZeSlfN8FIZPEZ4pHhru4X+UP9nbp/+3+lrThpnikdxDbOa2fWS9fDMxHXvrv7qukcrzZE+w429qN9Ta0kKSbLOO8nHV/AV8kNq2t7JMuL+mdn0iVLt9iuT0iNqKQ9ISucyFlURlxDhCgUg4WVJSSWwRgKr7eu6gmoaWpGm7m4sDITmON4+Vr5a635E3KP1/rK/arvOmrVIu17nLmSebvLe63vHCUJyM7raAlA/BQO/wBeTm8fPkrW0dzWdxrr0mO/mxcvPysOGY4sbtbr77/p6fud+1zTmidqN/1Dqu+XAJn2BDKHXmX5JS72QCtsNtIUEBO5jqHURxrv76sLZICc6neR/XtE4fpZrmjkG8mHafydtqWq71q3TTabTdbWlhno6e0+tD4eQo5SSnCVAFXWcEH11dxG/wAo/wCzt0+GP9LWzDXPFIjem++f8xP4tq+GZiOvl12qm6crXY5cLZNjS9SPvQ3mltPITaJ/FCgQocGc8Rnqr5G7Rto7dwfYsmh7jcpbGm5Ty29QSZkgKu6ULUmKVNrWCglnBcBAO8vqAAz9vp2obkxBkuRNL3KTKQ2pTTK3I6A4oAkJKucO7k4GccK+V2oeQ1yj9Vahul/vGmLbMvN4muz5zqLy0E864pSlboPUgEhKU8cJCRWPm4c961vEbmJ611847+bn8vkcrDimOLG5tqPvv+tHbkC7YtN6K1rf7zqrUs5xuaxbltSZ6XpJBHZnOJCUJIRjfaG6AOPUMgmu908sjZD39Xf/AMZM+irmbkOcm/aVydNaakmay08GYV5dhiEmBOakELQmUXUY3k7uS7veDirv9fcwv0kdWnLnj32Ppa24qciKeevo33zfjz+JNIrvXSndU8q3Yrf9MXe2XLVj7ltmxHY0nse2zgsNLQUq3VIa3gd0niDkV8j9d7SXLoqDC0fMnrRpkut9MuSnz00tJUht7m3FZRvMALKVYIW8RjCeP3B1BqO+xrFcXrLpSbNuyIzq4cWU8y0088EncQtYcJQkqABUAcA5wa+WE/kE8oq73KZc7pp22zrtc5bs2fL6YaHOPuKLjjhG6MZWVcB1ZHeFYebgz3ito7tE9a6jvqd/Ng5fI5WHFMcWNzOvtO/6T/8Ac49tml9nlz2kTNT6luTqLvFtDsV+SzKmKc3VTd/g2hYbxvoGMAHvDgcdsjlfbKjn/WCYfesc/wCgqgf3Pjk7bRuTZcNo51npzdjXvo/o5NsmMv7gaMpTqVZUnA3nwR19Z8FdjC/ys/e7dMe+x9LW/HTPFYjxQ33y/j2/Emut96+Sltf8qDYtqjRN/s921XcItunwXosiRFtFwbeaQpBCltqDOUrSDkKHEEAivkrrfazPvzlve03LuUVGn2lMvzG5z6hdXyNxb6W1uK3AW+6Qg8QpZCs4xX271hqTUkXSt4d03pOVO1AiG6bdHmOsNsOSNw82lxQdyEb27vEccZxxr5XvcgXlDSpDsmbpaHOmzHnJEuU5eWN515ZK3HFdWSpRP5z79YObgz28N4jdonrXUanz389udy+Ty8GPXEruZmN/tE7+62P3Ozbjo/ZxC2gu6r1POcfukiC5Hky0Spq3kJaX3WUIUE8VdXDvYFdi/Vi7IRn/AFtIPgNsmD/0qpH9z32D7Q+ThaddQNZ6YW0i6TI0iB0fNZkAISlwLSrK043SpOOvIPuV110/KGf9Xbp/4H0tb8dM8VjuP4dO+WM1pyeHw7718le6W2/6D2l68s1s01fDcZqWZKy0YUhrud1PHLiEirfzULn3uQdRWhRsVxSUofwg8zlXBPV9c/TTz2xS/a9dPhY+lrTSLxGrzuVR7FFMY1BLP/QFyT7/ADP/AJOGirQ+0UUUBRRRQFJpTb64zojOIZkFJ5tbrZWgKxwJSFJJHuZHvilNFBUugNi9009atb2jUd9t+obRqmdNnSY8S0uQ1oMkBLiApUl3KcZHUDx6+Fe1p2NzCvQka93Rm427RZDlvDbJS7LdSyphp14kkAobWrgn1SzvdyO5q1KKBDevuNP/ABDnyTUD5Mn8XHZT/ZO0/qbVTy9fcaf+Ic+SagfJk/i47Kf7J2n9TaoLAuH3Nl/il/oNJ9M/e5avyRr5ApRcPubL/FL/AEGk+mfvbtX5I18gUFeaj5NOznU951tfLnpeJMvmroKLfdbhJ3nnVsIbS2htvfJDKQEIVhsJBUkKOVAGvHkzWW3aJ2WQdDwY64bmlHHLVIiuPOulCgsrSsF1SlBDiFocSneISlxKRgJwLY41WGtnmdnevrZq9bgi2W5JFqvTy1Ybb4kxX1k8AErK2if9+nJwkYpt+mYtH7S6HHn8eluPPn5x+8en1j76WiKzVb7F9vOmtujWrXdNCUG9N36Tp+SuSGwH3Wd3L7JQtW8yve7hZwVbp4DFWPvDwirYmJ8mCYms6nzZooor14KKKKBj1t9597/JHfkmnymPW33n3v8AJHfkmnygKKKKAooooCiiigKKKKBk1h9yGfy+F+tNU90yaw+5DP5fC/Wmqe6AooooCiiigKKKKAooooGTUX25Yf8ArAf5LtPdMmovtyw/9YD/ACXae6AooooCiiigKKKKAooooGS6ffRY/wCpI+SmnumS6ffRY/6kj5Kae6DAooFFBmiiigKKKKArGR4azSWXDRNivR3S6hDqChamXVNrwRxwtJCkn3QQR3iKBTmtXHEtNqWtQSlIJKicADvmuatF6nk7PNi23m9Q3ZEmRYb5fnoq5b65C8tNBTYK3FFSsYHqjW2p4y9F6c2JSbc6pudfJ8ey3SXklyc3KgvFa3VZytQdShzeVkgg8eJyFtQdpdr1RETHjtSmFXO1O3GAuQEhMuOnAK0YUSPVtnCsHCx7uEPJl/i47Kf7J2n9TaqG2LRt603adLz7pbJKWtGaMmWuQ3HCXVzHyljgyhJysFMYkZxkuJHWCA88mrUqG+TrssR0dc1bulbUneTEUQcRGuIPu0Fs3D7my/xS/wBBpNpkjtctX5Iz8gU3T9ToXBkgW26DLShkxF4HA0n09qVDdgtqOjrmrdjNJ3kxFEHuBxFBKc4z7lcy/ugOj7NceT3qHVdzv1609O0xFck25+0XF6Oh+Q4UNtx3mkKCXUOO80jiCpO8dwjJ3uge2lGPuXdPM1VoNSoA+5l0OeJ+w11G0RaJrPqsxXtivGSs6mJiY/eHyO2K7LJ+ids+kbFtG07r7RtjvsBydHiWFx6E7clM7qwlzmnQ5uttl5a0I+yEgJ7kbxJ+uumdVWrV9oZuVnnNTobqQUutn/Ag8QfCDxHfrxGpEn1VsuZH5Gqq3uE+JoDaZbrxb7VLh2rUaujrmymEW0mSAtyO93u6V9cbV31FbfHuax4sX5WsVrO6+/n2+i53P/z2a+fNWK5demorMRHlr0nUb95XQOqs0xdtTfiy5+Zqo7akeLLn5mqtz5k+0Ux9tKPFd08zVR20o8V3PzNVAa2P+p97/JHfkmnyoXq/UqH9LXdsW+4oK4rid5cVSUjKT1mnjtpR4runmaqB8opj7aUeK7p5mqjtpR4runmaqB8opj7aUeK7p5mqjtpR4runmaqB8opj7aUeK7p5mqjtpR4runmaqB8opj7aUeK7p5mqjtpR4runmaqA1gR0Qzx/n8L9aap8qF6p1Ih+2NpFvuSSJsRWVRVAYEls4984/wAaeO2lHiu6eZqoHyimPtpR4runmaqO2lHiu6eZqoHyimPtpR4runmaqO2lHiu6eZqoHyimPtpR4runmaqO2lHiu6eZqoHyimPtpR4runmaqO2lHiu6eZqoM6jI7MsPH/pAf5LtPdQy/akQ5KsxFuuSdycFYVEUN7604MD3eOfzU79tKPFd08zVQPlFMfbSjxXdPM1UdtKPFd08zVQPlFMfbSjxXdPM1UdtKPFd08zVQPlFMfbSjxXdPM1UdtKPFd08zVQPlFMfbSjxXdPM1UdtKPFd08zVQF0I7aLH/UkfJTT5ULuOpEK1FaF9H3JIQl8bpiqycpT1CnjtpR4runmaqB7FFMY1OjHC13M+/DUKKB9orGR4aMgd+gzRRRQFJpXPojOGM2h2QEktodcKEqUBwBUAoge7g+8aU0UFIaA2R6k7WNpmmtZQrOxa9XXG5TOdtNzekuIalpCC2pLkZoAgZ4gkdXClMPZXf72nZpb9QCK3E0U8iY7JjulZuMhqMthhSU4BQnDhcVvHIUAkbw7qrmrFAivX3Gn/AIhz5JqB8mT+Ljsp/snaf1Nqp5evuNO91hz5JqB8mT+Ljsp/snaf1NqgsC4fc2X+KX+g0n0z97lq/JGvkClFw+5sv8Uv9BpPpn73LV+SNfIFA5UUUUGvfqP6thw5sGOxcWg7DVISpadwq9SFOJIA45CkJPDwVIe9TfcAeyrb4DII/wDCcrzUa1L2szWfFE6mELi3y1s3RCFR2HbYlCkl/sTuietJI3c8AN33d6pdAh2m5RUSI8KOtlYylSo+7n8xGarvWhY0i0qbcXnEvSH1uMxYcF6S2QFZAUW0KIzw6x4fdpza276Udayg3VQHDCbLMwPc/gqqi8RMxaYdCeLky1jJhpM79vVOOhLcM/YEXyKfRQmy27j9gxfIp9FfPHlectvV17tV22f6c0vftn9z6SS9E1LJfLPZcSO9vocYG6FYcUhkKTnglakq68G+dgHLfZ2yMTJly0Be9L2llscxdQhydHmPBRS4hotN5ISR6rqJyOsVXHJxTaccT3DVf4J8Qx8evLtinwWmYifnMebo+VZLcYzwNvikbpHFhPg96vcWW2kH7Ai+59ZT6KgsjbnpZUdwb11HcnibLNx/lVOrTdGbxbmJscOFh9AWkutqbUQfClQCh7xFaK2i3k5eTj5cMbyVmGehLb7Ai+RT6KOhLb7Ai+RT6KWKSFJKT1EYxXJ+g2NOw9KbWJF0Y1BIkW/VN3jxJNuM5b0ZtChzaWn0HDQT1glSUp45wnjUlDqToO2+wIvkU+ijoS2+wIvkU+iqs2X7Qrnb9B6JsupHFar14/YUXKamyLYdStCQEqfLhWlohSlAApV3at4pBAOFDPKKsU6dAjWuzX68m42dV7hrgRm18+yhaUOICS4FhxClgFKkjJOATigsvoS2+wIvkU+ijoS2+wIvkU+iq6kbfrNCTq9cqz3qMzpOSiNdnltMlLKVIQ4l4YdJUgocCsJysAHKQcZks/aJAj3W6W2HEmXmZa2G5M9EBKFdjoWFKRnfWnKlJSSEp3lYAOO6GQkHQltH8wi+RT6KOg7b7Ai+RT6Khdp20Wa/Xp222iDPuebG3qGJKj8zzM6MtW6A0pTgIXngQ4Ee/UKuerIOvtVbCdWWtEyNDvT8tSGZCubWWV259YQ6hKikkFKTjjgjhQXFKstu5tI6PiHC0dbKfXD3Pz/mr36EtvsCL5FPornvQ99i6a2eQETHL8If74Uq3x3bZKHcFV0daZbfK1hSmCSElIycYGMCr91JCZn2C5R30lbTkdYUkKKcjBI4jq9+g9+hLb7Ai+RT6KOhLb7Ai+RT6KqrkhtlfJy0FNdefkzZtqZkypMl9bzrzhTxUpSyST+epQvbDYm4TtzIlKsDU821y9hKexUvB3mSD3e/uh3uN/d3cgnOONBLeg7b7Ai+RT6KOhLb7Ai+RT6K535vT0HlD7Ve2BVyFqiWi0zEGOuYpEVa+yudcTzJ+tE7ickbpO718DVhcn2bqJWzHs3VMxUhpUyW/bJUp4OvqtfOqVFU8sE7y+a3SSSTjG8SrJoLGFktp/mEXyKfRR0JbR/MIvkU+iqN2XX276f273213uY+9A1zbG9TWlqQrIjONEMvxkeDDJiLx4d81fMmO3LjusPJC2nUlK0+EEYIoPDoS2+wIvkU+ijoS2+wIvkU+iuRtnYgy+TlfHrTJvkraA1JvLNsdtciU7ND6JshMZGQSNwbrYO+eb3R3XAGrP2y2/W+ptjsCyxJhg69jWpu+yXIC91C5kUIcDQI/kOSAB7qQaC43rJbd9r7Ai55z+gT60+54P0179CW32BF8in0VRm0TbHI1XoLZMdLTXLaraPc4kNNwZxzsOOthx98oz6l0JaU2D1pUrPWmpfq7Yww5pxljSUmTp67tSYrq5rUpZdmNtvoW43IcUSp7fQFDeWSruusZOQsToS2+wIvkU+ijoS2+wIvkU+iufdp0i023lRWRN4F3k2p/SM6U9DtzcySlTrcqOlDhZjhXdJSVgKxniRniKegpvSWzqfqjTEibCY1g3akQY0uU8+YUiStLPOIS6TzeEutq3BgZbJxk8QuZFotboyiFEWMlOUtJPEHBHV4RW3Qlt9gRfIp9FQ24a3tuz2/6Y0SxYrpJcuUZ429cRLSmlcwElaVqW4FJVuqCt5Qwc9eeFetv2yaal2mZNdekwlQZy7ZMiPRVrfiyUDKm1pbC/5JCgoEhQUCCQRQS3oO3ewIvkU+ijoO3ewIvkU+ioknbbo9ZAFwlknq/wBFy/oqra7a0vGobLtuv0W5S4MnRzzsa0NsvrQ0hUeC1IKnGxgOb63FA74PcgAYoL26EtvsCL5FPorVdotTSSpcOIhI4EqaSB+ioxOvUjVWxuReIQkx5dxsKpbHYW9z7a3I5Wnm8AnfBUMYGc4qNbOLLZ7ZJuGlbpqHVGobrOgh8xtXyg445DzubzaUJQjGSAo45wbyd/GRQWOuyW0yGvsCKeCuPMp9zPe4VmCwiHcpDLKA20Wm182kYSFZWCQO91Dq8FRXZNeZF1052JMeXIm2efLtLr6/VOcw4ptC1eEqbCFE+Empe392nvydHyl0C4UUCigjO0HTytU6WlWwKWWXVNl9lEhTBkNJWFONFxJBSFJCknwg46iaqmbs1Yg6PtkPSsqDYkG4W+c/Y5F2LzUEtyWnH+xnTncyhK0FAAQrPDdyd68LnFMy2y44bYkF1pSOakp3mlkg9ysd9J6jw6qov9467cf/AGd7IP8AlK/oqC8e2C1+Mofl0+mlUaWxMb5xh5t9vON9tYUM++KoX9467/8Aw62P/wDKV/RVaGzLS7+k9PrhSLPp+yOF5S+xtNRyzGxgYUU7qe6ODnh3hQS+iqY5XdvS/wAnzW00SZsaTCtjzrJiTXo4yQPVpbUkLHuKBHuVnaleX5G0LY9pFZJtd7mS5E5k+okJiw1OIbXx4pLhQopOQdwcCKC5sisFQSknvDwVSuhHJju1ba3ouBc5NnttvNquUFUJLSjEMlpznUNpdQtCUlUff3d3GXFEYJqwIOkr1bXxIOsrzdi2lW7DntQUsuKwcbxajIWBn1qhQNMXahbdRTHrKiBcYMqXZ3LrF7MYSlL8cK3FKACiUEFSe5cCSd7gOBwn5Mn8XDZT/ZO0/qbVR1cfUVu1PM1VcLU/brc1piSNQ82hUtUiWjdU0iI2guOlKU9kHdSO65xIAUrISp5Neq4LXJ22WIU3cCpGlbUCUW2SoHERrqIbwRQWzcPudL/FL/QaTaZ+9u1fkjXyBTdP1bAVAkpDdxGWlAb1skgdR7/N0n09quC1YLY2pu4EoitDKbbJUDhA4ghvBFBLKKY+3C3/ANHcf+Vyvo6O3C3/ANHcf+Vyvo6B8pBcPtm2flB/ynKRduFv/o7j/wArlfR14L1DEnz7c2yiYlQfKsvQX2h/BL6ipAGePV7lA/FIUCCM++KQNNoiXFaN0BD6QpPuLHA/CMfAaWdko/3n9xXopl1LdVQYzT7UR58Nq5wuJGA2B3znvEbwJHV115MeqdJnfh+an9snI+01t82qwtU61utynWO32fo2HpuG6qK0h5Tri3ZK321BxRUCyAgFIBZBO+DgS7k8bFDsB2bN6JRf5Go7dDmynoEiYwlt5ph59TqWVlJwspK1ZWAkHPBKRwp1sGrZTb8lpUR+c7IdLzaW1hRbB6057yRgcfdqbiU3j+Xn8Wr0VCsV3No9fVflycilYw5LTqO4jfUe8Q1kNpTGdIAHcmvYAJHAUnkvoMd0YX6k9aFD/wAq9OyUf7zyavRVka9GXcz09VEhB3eJxwBqldnOjtpGgrfrJAs+lZcm83+deY+b7JCG0vkFKV/YWSRjjjrz1irn7Kb/AA/Jq9FAlNnq3/Jq9FBz3s05ML+yxdgmsG26rlx7Ku1XBm5JLLZUZTklLkfuHNxKVvOI3CPUFPHKSFTCz7Or/aNq9hvke2aehaehWGRa3GIL62VtOOyG3iWmQwUlI5vGd9JJOcDqq1Oym/A55NXoo7Kb8Dnk1eigoTVmyDXN8sO2m3xmNPpXrN9Crat65vgNoEZqOS9iKd0/Wt7Cd/1WM9+pXatAal0tqbV18tLVrkSNUMxXZMeVMcSmJLaY5klKw0S42UpQcEIIKfwu5tDspvwOeTV6KOym/A55NXooKV0RsMumy6VYRZJMW7R4GlRp1apjimVF0Pc6H8BKu5JUvKc5HAAnvN2jNkeu9P2HYtBkxrAtzRnOJuK0XN766kxXGAWR2PxJ394hRGOrJ66vvspvwOeTV6KOym/w/Jq9FBz+rZDrxGg2LUqLp3s1GtxqMkXV/m+xukTK3d7sXPOYO7jGM8c44VZkXZja9LN3adpyC4bzLYcbCZ91lKYWpRzx3i4Ece+lGal8iSgtgYX6tPWhQ/lD3K9eym/A55NXooKF0Hs+2qaL2K6O0CzC0oh+3MxYNwuib3KVhhC0l5TLfYY3lqQFABSkAE5zShzYNdzsmm7LUSIfaxInuOdJl5fZSYbkwyltc1zeOc7othe/j+VjPc1eXZTf4fk1eijspvwOeTV6KCsdJaD1LatueudVTWLSmwXuDAhxux5zq5Kex+f4uNKYSgbwdHUs4x381FdF7NNoWiNlI0DHi2O6xGpsloy5dxdjh2A9LU7upCWHCFpZWpsoI3STwVgcb47Kb/D8mr0Vjspv8PyavRQU3tL2IO9N6H1Bs40/pq0X2wXdMp5x1RgJehqbW0+xvMsLKt5KgQCAApKT3qnMDZtZ7NepWooEJ4X51LiyHbrJUwpxQyRuqUpAGeGQ3wHUO9Us7Kb/AA/Jq9FHZTfgc8mr0UFacnbQWodm+zw2LUjdtTNRcp0xC7ZLcfQpD8t18AlbTZBSHAk8DnGc0st+gXdS6lv1x1xpnTk1JdbZtRC+zlJipST3YdYRzaitS1EJKh3Q48OM+7Kb/D/uK9FHZTf4fk1eig5ysPJ01LF0c9puQ9ZrMiw6pc1DoqZBkOSuwUlxbiI77SmWwEYcdbKUKI3F4GCkGpHtW2W6j22aatlpven9PWubCuEOYueZSpqNxp9t1xpnLCF4WlBQd7d4HBChVzPSEEtYC+C88UK8B9yvTspvOO7z/UV6KCpr/oTWI272vWdogWF+yQNPyLOmPKub0d9S3X2XArdTGWkJSGsY3u+OqmrXGl9ZzbPe7ndrZZ7fBs67XLtFssc12WpSIkoSHysKYZCVKSkJSlAV6kceOKu7str8P+4r0VqZCD1hZ9zm1eigp3aNcHpG2zY9KtJizg/Hu7rfOPlCXGzHaOUqAPHiD1YPuddOmlNkFxhOatuVwv0m13nUl36UkdBOp3WUIZbYaZStxs74CGkkqKUkqJ4DAqaJ0lp1MqFJNjhLkwj9jPrhBTkcbxIDaiklIBycAjFPvZLfgc8mr0UELTsxlpUFHXmqlYOcKkRvoKiF+2Q3pELahZ7OuMuDrxRcVLfdKTBccjIjPqUgDuwUNhaQOtSik7o7qri7Kb/D8mr0Vnspv8PyavRQQadpC/r03cNM2ySxZrZGgxY9ouESWtMzeb9Wlz63hCSEISFJKzhSiRkDLfF0ZdrttYh67vyIlnjWazyLZFiMyy8pwvONOPOurKUgIAYQEp45ySd3qqyeym/w/Jq9FJZ7EW5w3okpkvx3U7q2ltqKVjwHhxHuUEJ2MQHGdPSro4gt9O3Wdd20KGDzTrpLRI7xLYQr89Tdv7tPfk6PlLrPOtpdaASsAAgJCDw/w9ym2VfI1uvSy6mUrejoxzMN13+Urr3EnFA/iimIatgnO7HuKvCejJA/S3RQOVwmJgQn31uMtpbSTvSHebRnvZUQcD3cGq7f0O1q5pmZqfWt1kJkPFhmFY7q9aIiHQSnm21RloecUN0j644oEgkJSOAsS42uHeYD8G4Q2J0J9HNux5LSXG3E+tUlXAj3ONUZrLZLo/Y3ZIl30xppxhLdya5yDG1FIs1tSlbpUVvIbVzBSk9SVt4J3UlQzmgnEbYjbLOQ5Y9Q6us8pIwl06kmTkJ9zmZjjzJ/Oj89TGwNXZiBzV4ejy5baikSYrZbDqR1LKCTuK7xAJHfGAcDwlakbh3K3x1sKVCnAIj3BpaVNc4QSEKAORkDKVYIPVkEjL7QVpyhdF6k2ibKNRaW0yxa3Jt3iLiF67TXYzTO9juvrbLpV72B79eOp9AXzUMnQmpFRoMXUmlprj/YTEtbjD7DrC2HmkuqbSclKgoZTjeSAeByLRooK/2e6DmWbWGtdW3MNs3LUr8YCK0vfTGjR2i2ygqwMrJU6tWOAK8AnGTYFFFAhvXGzz8/0DnyTUD5M3Hk4bKf7J2n9TaqeXrjZ5/4hz5JqB8mT+Ljsp/snaf1NqgsC4/c6X+KX+g0m0yP9XLV+SNfIFKbh9zZf4pf6DSfTP3uWr8ka+QKBx/PR+es0UGPz0huH23be99kH/KcpfSC4/bVs/KT/lOUC0fmpFd4C7jbZEVD/MF5O6XMb2B3+GfB+mtbjaUXNKAt2UzuE4MeQtrPv7pGfz0g7UGT/P7oP+/vfOrFfJmrbVMe4+e4hbWtNbm2nHtt2w7XbVrLUMAMaPgXLT9xftsmK/HmOFxHqmXf4fqcZUy6MYA38Y4VafJ+2y7StpO0u/Wa/wATTZ07Zbe25Kn2lmQ24JjqsssDnHFJOGkrWvvp32vXVD+VzpYbJZsDanBYuF0hPIRZL5FaU5IedK1EQHUgk8Q+sx8Dr7KQTwRVobD9jL+zDQUWLepExd9uLirpepNvlOJQZzvF1OEnilACGkHHqGk5xivj+LHxynxXLOad8eIiax1Hc+m/bX3h9Dmn4fk4VPw//r5TP7euvf8A2ueUfsV3+qa9hwzxqMO6bhFopTdLgtamy4lHSLuVJ8PquriPhFP8WMIrKGkqWtKAEhTiypR90k5J/PX2mLJltMxemvrEvnbVpEfptv6FBzund68cKpfRW1DXerrTq+cxaNPqVp67zbT2GqW832WY5GV84UK5vezwylWO/mrpUoISVKISkdZPerkvZffNm1701taZ1BrWHChydX3gqLOoSylTRUnug2lwJWCM8ClQVxBChwOlW6J2W6/g7U9nun9X21l6PBvMNuY0zIADjYUOKVY4ZByMjrxmpXkeGuQ9mV01LqWZope0SNbLDYHtKONwIt2t3NQnXxKWgqUxvtoaeVGEdaWyMpSpwJAAVhfYdGaRe2y6Q0te73F1I85oqYwt2SvseRcEplshgqTvb6ilvfCFbxOElQNB1bQCD364x1nO0laNO8pibEuFrizrdcWH4q2pDaTEkCFGPOI7oFDinUqJUMEqSc5IxVoMXi13LXe0N/XMuE5ZlQIT2nnpTqeZXDMfLzkY5wV89vbxR3XBr8Ggv7I8NQXWOtrlpzX2h7MxFiv27UEiVFeecUoOsrbjOPoUkDgQebwQao3ZgzKh6js8/aFGZj3W77OWm7oq4tpSqa6iQQUuhXq3A2tG8k5I3uNINmGr7ENI8lpJvMBCjzzYQqShKgoW2QgpxnIO8QkjrBIHXQXZobaJqHV+lEzl2OLJnNaik2iYiJLLTTTLEtbKpAKwSrAbCi2OJyePCp/e35cW0zX4KWVy2mlLaTIJ3CQM4URxH5q5GXqfS8jZdbZi7vaXFxtrJUy+ZLX1om9KUSlW9wyhWSQeKTnqrovpbVVvh3uVq1Om7dYWYrq0Soc10qTw7kuc4hKUjGeO910GNhWvrntT2Wae1fdIMS2u3mI3Nbhw3VOpZQoZCSpQGT7uBU/rl3k+bdNG6L5LmzFlzU1mXeJFvg2+LbRObL78l1SW0NhtKt4kk5OBwAPgpG/cbn+8xcXlKR+/SL84hlGQJvZHSJDSEj1fMdj7vD1HNEnqzQWrB2jawu21rWOkINushZsMSFNaekSHkqkokc9hCsJIQU80cqAVne6qkeybaZG2p6akXNqGu3S4VwlWqdEWvnOZkx3S26lK8ALRvDgrAyCMgHIFTafFl1fyotrlpRqByJPNks7SeirmWX21J7K5whKVcVJ3k5yDjeGeuvHZRtlsum+T3b0XGbZ9LajTIl2YRn1Iihc5MxccOKQoju1rKXVA8TvKPVk0FlbOdr51ttB1zpeRb+j12N1h2A4V57OhOBSOfHgHPNPpx4EpPfqZax1NG0bpS9agmJW5EtUN6a8lsZUUNoUsgfmSa5718H9hW1DZZrG+agsrdoe3tGzC1FMIlh1POMOKU5IXvBDzScnAwHVE8KtPUuptVWC16iu97tdhuGj4kKTJ3IUl52VIZCFFKS2W9w7w4EAnr4ZoMt601ZFTpSZItduuNvv0lpp5UB1YNtQ4hS0qJVkPJ4BO+AjiR3ODw89LbRL1tQdukzSca3x9NwZbsFi63HnHTcXWllt7mm0FO40FpUgOFSiSkkIwAVVdbdHac2eak0ZI2SasuDUC53NuLK0Yi7uTbeYakq59bcZ1SjELIws82UJSU7pTlQFPfJtukTZls4OzO/XKHY9V6bdlspRc1BCZTCnnFsS2wVJ51pSFJyUngoKSSCKBw1lt01FYNket9X9rEeFctK3B+O5bZ0lRTIQ0hJK0uIHAL38pOPU4JAOQJHqXaRf9m9tiXvVdvtr2mFOtNTbhbXnEuW8OKCEOLaWkhbYUoBSgoFIOd0jOKQ2rbTrnqnkp7YLhqqTZ4LbE2Zare7GjLhomNtkBt3666vfLqd1Q3eBGMbwqwNvmpbRtM2K3nQulLpB1DqPU0RNqixbdIRILXOYSt9zcJ5tttO8oqOAN0AHeIoLYl6lela3iaegbh5mN0hcH1DPNtKUpDSE/hLUlZz3ktK75BEnyPDVX7P4K7Xtb11EfWXXEW2zpZcWeK2ktvo+WlZ/PVEyItiteybbC1ZEMxNZxNU3NvT7Vvwma1MLoMZEdIO8ElZ4hGEkFzeHqqDsbIpg1zq+36B0lddQ3QrTBt7Cn3OawVqx1JSDwKicADwkVW97smqJVycdXZtXvOLSgrctmpWY0Yr3RvFtsuApGc8CKh21bRupblsa1vGFn1Mh8RGZLbd1vKJ4d5mS28pDaELUQspbPHv8AV36C5bRr113WDGmbxbm7ZdpVuNzihmTz7brSFpQ6nJSnC0KcbyMEEODB66R6/wBe6k01frVbNO6Kf1cqWw6++pi5x4vYqUqQlJIeI3gSv+Scjd6jUNv93gXLb5pfU7M9hWn9P6SuUy43FLgLTKJDkYslSh65LDqseBBNSK96i0pdtUodvjUVu0iyCbDvUu4BuO+04olwIbKhkoS22vnDxG+MEZ4g/wBy1PcLBc9Nm6MMMQ7otMF9DThc7FlqBLeFkDeQogozgHe3PCamNc42u4aivPJq0W9qAyV36RfLaGHJQIfdaF1QWHFggK3iwlKjkZPHPGujqDwX9sNe8qk6Pu093/sdHyl0oX9ste8qk7f3ae/J0fKXQLh36KBRQavPtx2luuuJabQCpa1nASB1knvVEtomibftH001ClQ7beoiX25aYV1ZEiFLwFYS4kggpIVkHCsEJVg4wZaQCDkZB8Iqp9PWTT+tYUu57O9dXWwREy3YrybK4y7F59CvriQxLZdQ3xyTzSUb2ScnOaBhtPJw0/s/2laUvehtC2iwxUqfVdFw7g4zHjZRgBmFuFpSlEqAcSGlJHDiFKSbzYmMSVuoZfbdWyvm3EoWCUKwDunHUcEHB8IqvGNld+kqKb3tN1RdYZOFRGEQoCXB4C7HjoeH/YcTU5stlg6ftrMC2xm4cNkHcaaHDiSSTnrJOSSeJJJJJPEF+R4aCoDviqk5Us+82PYfq692S/3CxT7bb3X23IHNAuHGMKUtClJx15QUn3a32katms6y2W6SjSnojepZkhc2QwsocVHjxVOlCVjBSVr5sEjBxvAEZoLYyPDRmqh0XqO/StoW03RUO5thVkXbZtul3Rpczm2pTKyppWHEKWAthwgleRv4JIAFS5uz62dbebnagsj7DjS0FESyvR3N4pIBC1S3AMEg+pNBuNdWLUDU+32+cJMjsWQ6jdbWEOIQdxakLI3VgLIGUk0x8mX+Ljsp/snaf1Nqq+2cLvGhNJ2CBOM9iy2TRz6dSKntOltiWyGgjm94YJKUyCQ3wKdw44pzKOTXq+0M8nXZW25NAUnSlqSobiuvsNr3KC2bh9zpf4pf6DSfTP3uWr8ka+QKbZ+sbMuDKSJqSS0rH1tfgPuUn09q+0NWC2NuTQFJitJI5tXXuD3KCWUUx9ull9nDya/RR26WX2cPJr9FA+UguH21bPyk/wCU5SLt0svs4eTX6KTr1JbLnPtrceUHVh8qxuHq5pzwigklFePZbXrv8KOy2vXf4UGHo6X07rjaXEgpUAsAgEHIPHvggHPuCiSl1yO4GFpbeKSELUMgH3qz2W1673eqohF2u6Ll7QHNCxtSQJOrm4q5ztojuc4+y0hSEqW6E55vi6gYWQSFZHfwEYeuUyFc1LckI7Lj70cIAG4EZIIHuZJ+AeCrOtS31QGVSXm5Dyk5LrKcJPgxTe/Z7Z2BJj9jtlp0qcWCDlSySd7PXnj15r1jyOj5HMEgxnFHm18cpPElJH5j+ioR+nzaL2jLHUamPudwMg9/36wBgmvIzGUJUSvgkZPA1CYW3jZ9cBIU1qu3pbjvmK+88stNNPAgFtS1gJSoEgFJOeNTZ06U0FghQ3h4FVkZAOePVXmJrKkhQWCkjII6qatPavs2qYb0y0XFi4xWZD0Rbsc7yUutLU24jPrkqSpJ96geQM5yK1KAcZAJTxGe9Uau20vTNkujlsk3RCrk20HnYcZtch5ls5w46htKi2g4PdrAHDrpysmqrPqJh1+1XKPcWGnOaU7FcDiAvAOMjI6lD4aByU0l0grQFbpynI6j4R4Ky0ylpAShISB1ADAH/wB8ajLG0/S0nVz2lWr1Hc1CygOu21OS8hCs7q1DHBJKSATwJB8FSXstr13ez1UGskYaBwOC08P+0K9AKi2tNpOmtDi3ov13Ztq58lDEQPJUefc3shCMDio4PDr4Uu0/rrT+qVSW7Vd4k96Nu9kMtOZcYKvUhxHqkZ72QM0D3+bPgHhrHNjO+U931Zxxx4KaLtq+zWOZaolwuLEOTdZHYsJp1WDId3FL3EjvndQo/mp27La9d/hQb44+ivFuI00868hltt10guuJSApeOCcnv4FMN12jadsk9yHNuqGX2UpcfwhakRkKzuqeWkbrQODgrIzXpN2g6atunWL9KvcKPZpCW1MznXQlt7nP4MIJPdFWRgDic8KB/HWf/OgoC0qSoAg8CCM+/Ubtu0fTl3mMwmLmhue6SlEGU24xJOElRPNOBKwN0E5xjAJ71NDO3jQDyp4TqeGE290sTHFBaW4qwAopdURuoIBB7ojgQaCbNQI7Mh59phtt93HOOoQApeOreI4n89ayYEeakIkx2n0IWFpDiAoJIOQRkcD114C/202w3ETmFW/meyOykuZaLeN7f3urdxxz1Y41rA1BCnWpq4odU3Fca58KeQUEIxnJB4jhxwaBW4CFtjjxXn3+Br2xgnw9dMdo1ZadT6ft18s8xu52uY2JUaTGJUl5spJBT4c96lVm1Fb79bGJ8GQH4rwyleCD1kEEHikhQIIPEEYoEc/TBXqqDf4jgYmNMqhyUkZTIjlW8EnwKSvukn8JY/lZDTs02fStBJ1AJNzauhut1kXXebh8wWlvK3loGVqyngMd/wAOe9Mey2vXf4Udlteu/wAKD2rTd4cRWnZbXrv8KOy2vXf4UHnGgsQgtMdhthK1Fag0kJClHrUcDiT4aHobMkAPMoeCFhaQ4gKCVDqIB748NenZbXrv8KOy2vXf4UDLdtMqveoLTNluhUK2KMhmMB6uQQUBxR8CUKVgDvryfUipBXj2W167/Cjstr13+FBhf2y17yqTt/dp78nR8pdei5LfPtHe7yu9TXLv0C2XpfZMgNb0dGO4V65XgoH4UUyo1faXASiWFAcM7ivRRQPWOGKoTWfJjuUqfAm6L2laq0j2NcETBblym7hAYACgeZYlNOFGN7AQlSUAZAAGMX5XOHJ115tM1Xs+k6kmSomsWem7rDNveQiHMaaYnPNIDLqcNuEIQkBDgQScku96gfza9vOim1v9tWh9dW5lJcdTdrXJs0ncHWC8wuQ31DrDA96rD2Xa+ibUNC2rU8FCW405srSG3Q63wUUncWAN9JI4KwMjGQDkD00xtBs+ppbsBpx2BeWk779ouLZYltjOCrm1cVoycc4jeQe8o1Jd3BPDjQVbynLJftU7FdV6d05p+ZqC63aC5FZZjPRmglSuorU+6hOPeyaTax0td79etmmtGLLNjzdMTZBl2p9xlUlUZ+OthwpKHFoUUktr3QvJSkjrwDb9FBWOzLR1wi7QtoWs58dcFGoXYUeHFdxzgjxWlJDiwPUla3XTuniE7ucHIFnUUUDfeQeh53e+sOdf9U1BeTN/Fx2Ve7pO1fqbVTy9nFmn/iHPkmoHyZP4uOyn+ydp/U2qCwLiP9Hy/wAUrq940m0z97lq7/2K18gUpuH3Nl/il/oNJ9M/e5avyRr5AoHHjRxrNFBjjSC4j7KtvH+cH/KcpwpBcPtq2flJ/wApygbNUTNSQkMHT9ngXZairnROuKoYQOGN3dZc3s5PgxgddMAvu0sf7Haf+Mjv7HU//NRjFR1PzWVvFY14Yl80+V9tx5Q1quMq2aptkjZfoxxzmmJem5BfZmg+pDlzSApsqIIDe7HUeruhUK5A4ulm5RiBpi0QJ8lWmrjvMypqoiN0yYRKytLThKs44Y47xOc9f1XmwGLlEeiS47cuK+gtusPoC0OJPAhSTkEHwGqg0FySdnmy/a07r/SFtf09Oetz9uctUN7FuKXXGllaGSCGlDmUAJbKUYJ7nPGss4bTljJ4vo7uL4pipwb8ScUbnXf7TvtJZF92lcw5vaP0/jdOcajd/Y6l7LU64WdkzmGYdwKErW0w8XUNODjhKylJUAR17oz4KXyx9iu/1TW7jZcbWnKkFQI3k9YrVEfOXDnJvuIiNEDN6jqXEadPNvyN4Jb6+6T6oE+4a5jsOoblpzZZt2lotMSdaUarvoluuvkqZZJAcdLO5h1KElSigLBUAQONWpeWH4NynlEh+SuIoYlJBJQSAck9QOT+fFZ0DsPs9o0Nq6yP6gu+p7ZrB+VLuJuBYSd+Snde5stNIKQR1BRVjvV5Wd9JZMcREWjykusUdnZTsGssC3XKTfRbbRGgQZ5bU69LUUJbaXuIBUrJKTgZOPDVe7B5sHQW2PXGhLczOYtF1jM6mtLdwgvxCp3dTHmpSHkJJytLLpI77yjVj2TY89Z4mlInbhfHYenVNBmKExeblIbaLTaX95lSjhJyVNqQSo57wAW6w2URtXa60pqvpq62m5acL4jot/Y4bfQ8kJdQ9zjK1KSd1JwFJ4pB6wDU1CDcjharnsViX6d9c1DerjPm3h5Y+uKl9lutqQrPH62EIbA7wQkDgMVIrVqLZ5siu93s3TUO1TLvc3Lm5CUrGH3t0KwAMd0pOffUadI2ylNivF0naZvtw041dHzKmW9htl2Kp9X8I+hDiCW3FcCd0hBPdFBJUTLLLZWbJATFZU453SluPvKy46tRypaiBgknJ6gBwAAAAAVLp/8Ajfa07/8AqbaiB/3ub/8A1/hUquW0dUrafE0LZFxukTb37pMlyW1OIYQ2ttoICEqSVLUpwHJVgBJ68jHhJ2Nb20e8a2iaw1Db7rcoLFtcZjogqYbZaW4tAQHIylZCnVnJUrrrTTuwuDpxcGczqG9zNRRVTCrUE1UdUuQiS6HXG3UpZS0UbyUEBKElPNpwRxyFU7UtoL+ubdoVIisovNi2pRLNKbDikMuPtJcIWk4JSlSFoVjiRvFOTgkzHTzDmo+VDcLjdSmyXqwacEVq1x3OeRPiyX94SlO7qeCVsKQEFIKSVE8FCna/8nqz3C1WOMxe71bFWy/J1GqRFVGU5NnlZJdfLjKwc76hhIQAMDAAAD7d9k0W4bRGtaRLpOtd56L6GkKj7hS/G54PDgpJ3VhWe6HeWoYJwQFT8p9atQW69TbfEucjUGixHutkMS1SZDZmtOJkOoLjbakp3m0IaOVcA4qrXG3LRMfTWn73Ov8AFt8K+w0ToJfUcuNKSFZGB3t4CnjSGil6Uscu3LvlyvbsmQ/JXPuSY/P7ziiSMNNIQQMnGUHhgHNN+yPZbG2Q6MjaXt95ut4tsRazFN1LBWwhSlK5pJaabG4CeAIOBgZwMUFcXaPtB0HqfV180jYrLtL0ffpCZ0u19I9h3SK6GENONtFTamn0FLSSELU2QVEZIPCNbOrnZdW7ddm8W2NPN6NtugDddOQ5adwtvqkJYcUU9XOtNbreP5POLwePG5k7MJMCfe5Nn1Xd7SLxKVKlMpDLyELUlKSpnnEHm1YSnwpzk7pJzXnP2KWB+z6ZiW5ybY5mmk7lpusBxIlRklIStJK0qS4lwAb6VpUCcHG8AQCHVt10WvbFomNc1S2dZRxLFp3bZJUl9tbSQ+kPBso3QC2pXdYBCckUy8nsA6p21DAI7dHsj/ucTP8A9+/UtY2VtS9X2LVF7u868XqytvtQlqS2yy2h5IS5ltCe6JCU8STgp7ndBILZprYe5pSXqWTbdd6madv9xXdJe8i3qw8ptDZ3MxO5G62gY6+HXQU5pa4SI1v15o1gEaZTtMj2aEhJ+ttxXex5EiOnvBAcU8ndHABZTXQG0+7NQdNJgrTJcF1fRBUmJGdkOc2rJdIbbSpX8ElzqHDhTLf9lES07OW7PpeGRKtk1q8xQ+6VuSZbb4fWpxxRJUt1QUCpRPFfgp8j6e7YNT2jVyLzdmmGIa2mrK42wiMC5ulS1hTXPBwboH8IAMEY4nIVbyX7pGsT+s9nTTcmNH01eVyLUzMiOxV9GzN99kBtxKVbqFl5oHGMNip5s4fMTXe0eyoyIsW4x5jKe8gyIza3APfcC1n3XDW9w2WxEbW2doib7dIU1NuFqfgs9j9hyGErU4nnAWS4VBROCHBjJA6zSzZxZpDcjUeoZjK40q/zxKQw4nC2o7bSGWQoHiFFDe+R3i4R1igm2KMVmigxijFZooMYoxWaKDGKMVmig8F/bDXgwrvUmbH+mnvcjo+UqlK/tlr3lUnb+7T35Oj5S6BYR+f81FbCigMjw1wdyddrW093Qtz0Psu2buzbpH1FeTJ1dqpSodkib9wfUCjGXJSglQyhvAHDjXc851cWFIdZ5rnENqWnshzcbyBkbygDgeE4OBVP/vqaxT1P7M+v22PZ+Hsagadn3JUdY1jbdebUNaXXaVrq3rLsFx4mHa7WtQwexYTZ3E8Mp3l76j19ddAZGcZ41S/76+svZGzH42PfstWFoK/3LUNjMq5KsipCXS3mwz1TI+BjHdqQg73HinHg40EmyPDQVAd8VUnKln3mx7D9XXuyX+4WKfbbe6+25A5oFw4xhSloUpOOvKCk+7W+0jVs1nWWy3SUaU9Eb1LMkLmyGFlDio8eKp0oSsYKStfNgkYON4AjNBbGR4aCoJSSSABxJPeqodF6jv0raFtN0VDubYVZF22bbpd0aXM5tqUysqaVhxClgLYcIJXkb+CSABUxjWbVzri27xfLLPtriFIeYh2V6O6tJSRhLhlrA/umgI2urJqe3TWbZOL7ioK5LRLTjYeZ4jnG1KSAtGSO6RkcU+EZY+TJ/Fx2U/2TtP6m1VeabsZ0/Msl4cmvW/R+jtGTYL0q7xewRHQeYKUvLWogqbbjKK1AhKcccZ4Svk1aysDHJ12WNuXy2tuI0rakqQqW2CkiG1kEZ4Ggti4fc2X+KX+g0n0z97lq/JGvkCm6drXTy4EpKb9bFKLawAJjeTwP4VJ9O6ysDNgtrbl8tqHERWkqQqW2CDuDIIzQSuimTt4054/tnnjfzqO3jTnj+2eeN/OoHukFw+2rZ+Un/KcpH28ac8f2zzxv51eC9T2e5XC2tQ7tBlOB8qKGJKFkDmnOOAeriPhoJFRUe1NtD0roxEdeodS2ixIkEhlVzntRw4RjITvqGcZHV4RTD9UFsu/+JGkv+eRfpKj4o+aM2rHnKedeaBjvca4d5eXLFjaa0VZNO7KtZW+Xqe5TETHrjZpjUkQY8daXAFFCiAXXA2jdPBSOdHVV8bKeVvs12i7PLDqOVrLT1inT4qVy7XPuzLT0N8dy60pK1BXcrCgDjugAocCKrrmpNppE9wpjPim80ie4/tc0v7Vd/qmveq7kcoDZeqM4BtH0kTunh05F+kqa22/2y8wI8633CLOhSEB1mTGeS424g9SkqBIIPhFWxMT5LotWfKShlhtAc3G0J5wlS91IG8T3z4TSFDSbVJbDaEoju4bKEpASlQHcke+AB+YUsVJYUkjnkAEd5QrlnTus1K0ltQkXTahNgXizahukG1CRLYcVzbJBYbDC0HnSTw4DfV1A54hKykxE69JdWhQrOR4aqzZttalXbZxCvusYTOnrsi1szJ0MrwEOlredQkEk8FAjGSeqmfk/a9vcu4a50trK4olX+y3FM1t4kALgS0B9gDwpbJdZz/ua98yazXzXXmjIOeI4VSGyG/zduenXtb3G+zbdZbhKkIstpt0nsYMRmnVtofcWnu3HVlBWUqVzYBSAnIJVN9mY1HAi35jVV1auTybs92BKCEN78PdRzWUpwN4YUDgAEhRAA4AinFGRVJ2W9XGRyo9T2V3UM1yxRNOwLlHtheQGEvvPyW1n1OVAhpHAkgd4catC8axtFikRY8yYBKl73MRmUKeecCRlRDaApRSMjJAwN4eEUDvJ4tD+uj5Qr1yPDVA7e9etvWzZzc7Dq5+2224asiWuc9DkJaSthXO86hzeGUKSpsA53VJKSDinfSmtrm1tp7XLTfDq3RirMuXKuDy2nejJiXkpbZD7YAWXEKWooXvKTzYOQFAELnyM4zxrNUXyidpN40OuxXuzTg3ZtOzo1w1IyjB5+A64Y6k/9gKceP4kVdiJ8ZaApL7akkZCgsEEeGg980ZrnybtfsKdqeq9Lau112kX1l5tGm48mWmIxJjqZbUH2t8huUsulxKkK3t3cACRnJUN6y1PctYaM2XuX4Qry1p4XvU18ihpTyglaGUtMbwUhJdd3yVbp3UowMEhSQvujNVs/Yr9ZNdaXNnv0iRpMqki6Qpz/ZDiV8yeZWl5ZK93fzlJJGd3dwAQYRsoZumr75tSjTdb35rojU7tvtzrUpo9js9jMOJSEqbUleFOLPdpVn3cUHQGRWvACqGse265y9F6ntU2RGOs7LqFOlDLaQEtvOurb5iWEd7LTyXFJ6gpCwOFY2vXFWltebI7Uxq2dZrLcZkuDcSJqUB9DcN11KlrUCQorSklQIz7tBez/qmQeor+Hga9RgVRmjdTTr7tY1DpmFfHtVbP27U045dFvIUYM9S1pVFbktgc5lvdcIJUptWBvd0AJ7sv1Su8WKZDuEtMi5Wee/apL6iAXlNq7lw+6ttTSz7qjQTiivHsxj+mb/vCjsxj+mb/ALwoPaivHsxj+mb/ALwo7MY/pm/7woPaivHsxj+mb/vCjsxj+mb/ALwoPaivHsxj+mb/ALwo7MY/pm/7woML+2WveVSdv7tPfk6PlLr0XKZ59s86jACv5Qprl6htdsvTnZlyhxAuOjdL76Eb3dL6smgfhRTMjWen1glN9tqhnvTG/nUUDo8wh9pxpxCVtrG6pC05SoHrBFR3963RntSsX/LGPmVKKKCL/vW6N9qVi/5ax8yni02K3WCIY1rt8W2xyd7mYjKWkZ75wkYzwHepwooKp5Tlkv2qdiuq9O6c0/M1BdbtBcissxnozQSpXUVqfdQnHvZNJtY6Wu9+vWzTWjFlmx5umJsgy7U+4yqSqM/HWw4UlDi0KKSW17oXkpSR14Bt+igrHZlo64RdoW0LWc+OuCjULsKPDiu45wR4rSkhxYHqStbrp3TxCd3ODkCzqKKBvvQPRM7hw5hz5JqB8mYf/hy2U49qlp49f80azU+vX3Gn/iHPkmoHyZP4uOyn+ydp/U2qCf3Ef6Olfilfm4Gk2mR/q5au/mK11/1BxpVcPubL/FL/AEGk+mfvctX5I18gUDjxo41migxxpvuA+yrb1/bB4/8ACcpxpBcSBJtpJwOyTx/4bg/SR8NB7qYQ6PrjaV++nNY7CYH/ALpH92vfFHeqHhie5iEJrWfONvlHt75M/KN2y7Y9VavkbL5Yjy5RYt7IvdrAZhNkoYSAZfckpytQ9e4vvGr8/c+Nlm2LY1eNUab13oR+zaQuKU3KLMeuUCQGJw3UOICGX1rw6gIVnGAWvw67gA66wE+5WOnDxUyzmje5/hgx/D8GPNPIiJ8U/wDeRHJhMCO6eaT6k96lKUBCQEjAHVjhWssYiu/1TXtitsVrXyh0IrEeUQwcpSSEk8PUiuabdso1HqLY/tpsF30o/bLnerxdrrYzPejOFS3k5jOoUy64W3EqAOTgg4rpmipJOYoVg1le9PaKtOpdFXV2JKXFZv8AIYlsIMfcbBcUvDqXCnnQE5a3sp3j4MyKboS4aR276W1Vo+zXXUFluFskWbUTy7uiQGGgoOxnPsqRvkoXzid1AOA6o4z13zu5znv02TWDFUqY0VZBCnEgnCkjgTjwgcfzVGI8Pkum05f/AFKrdk1gvmw2xP6KXp+dfNPQpb7tkuVsW0s9juurcTHeQtxKkLbKynewUFISSoHIqTq0JedRSmrrO1Pf7C6pfOdF22SyGW0A9yheW1ZVjG8UnGcgdQNT5C0qQFBQII4EVkKB74qSlSLcDVVl5R+q9Up0TdrhYpWnoFsjy4smAOeeZfkuLwlyShQG66jipI4+9SfS9l1zJ2k2/aNetNzYYdg3C0L08uXGdlQWlSW3GHcodLR30s92ErUQVJ6+OL3ooOXL/su1pBhWGUzp6XdH5W0kavlw4suKBbYndJCCXXkBTmClRDe8N5S8HGCbAesGpNJbfrlqC22ZyXpa+2Jlh9qEtCcXJqQrC3UkjBUy5jnOP8Huk+pzbsr+CH9dHyhXtQU+jZqztQ07qt/WFgv9nkXpciK7anLwUb8UJLbQ3I0lTJyjj3R61kGm3ZNojWt/2V6Wt+sZ2ptE3+yRhb5CYU2Grs9LfcIfJRzwypCUqOSCCpQOeuryooKlvKLleIOo9Paz0A9rCyOSFtW4BEaQ3LY5tOA6lxSebXvFY3ldzjByDwqI6a2O6o2YzdnmqoEZWo7taNOjTN7tjcpIefjb6XGlsOulKFLZWCO7UneQpWCCAD0RRQUzOsOptVbY9HaigR7/AGbTMNuWLvEmXZxluS4ptIjnsVLpSpKFIVnIAJWDhQyQg2UM6u0Ze9pkufs/vZRe9SO3ODzUy3K5xkx2G0k/ZXc5U0rgeIyKvWig5zf2ZT9E6UlaovRZN6uGtIeqbw3GXvtRWg42yG0qIG8llhKSVYGSlRwBUp2wWXUc/afssuto0vPvlusU+ZLnyIciI2G0uRHmUAB55tSiVLTwAIAPXVsy4TM+I9FktJfjvIU2404N5K0kYII74rztdvbtVvjw2S4WmEBtBcUVKAHAAqPE4HCgqW2aZ1LfOUFH1ouzv6TsMWzOWuQxLkMLeuz6nAttakMOOJCGgleFKUF5dIAxnMk2Ox1OJ1nd/wCb3bUcqRHPeU22luNvD3FFhSge/kVNbpFE2KY61uNJey2pbS91YBBzhQ4g+9W1stse0wI8GFHRFiR2w00y2kBKEAABIFAs40cazRQY40cazRQY40cazRQY40cazRQeCx9kNe8rhSZA/wBNvcf5uj5SqUr+2GveVSdsg3p/3I7ef7y6BWW0r9UjJHDNFbiigzRRRQFYzWaQXhme/AdTbJMeJNOObelR1Ptp48coStBPDPUofn6qBdkeGgEHviufbDtyvdu5KcTX9zMa4akkDsdshkNsrkuTTGZ7hJHcAqQSM5wOvJzUxkaxu2kdq2mtIzJ3TTOoLVMfjPSW22ltyovNFQ3m0gbi0uk4KSUlHWc4oLRyD+akSb5bVXdVpTcIpuiWufMEPJ58N5A3yjO9u5UkZxjiPDUbRcteqUEr0zpxKOolOoXyQPe7BqJ3W72ZvlIW6A9PhsynNKzULY59CHCTJjEZGc5KQSPcBNBPX9UWa9QrvEt13gz5caOsvsRZKHFtAhQBUlJJTxBHHwGotyZP4uOyn+ydp/U2qgeldU7P9Tavfuth1nZ5kTTWmpVsiRY14RLkdj7zanpMle+pSUJLLaUlw+vUTxFS/kz3u3N8nPZWhU+KlSdKWkKSXkgg9htcDxoLNuH3Nl/il/oNJ9M/e5avyRr5Aryn322m3ygLhFJLS8APp8B92k+nL3bm9PWxKp8VKkxWgUl5IIO4OHXQP9FIOnrZ4xieXT6aOnrZ4xieXT6aBfXhIity2i26jfSTnB9zv0n6etnjGJ5dPpo6etnjGJ5dPpoNuiGPXyfO3fnUdEMevk+du/OrXp62eMYnl0+mjp62eMYnl0+mg26IY9fJ87d+dR0Qx6+T52786tenrZ4xieXT6aOnrZ4xieXT6aBq1XCTB0zdZDD0pt5qM4tCuyneBCSfXU79EMevk+du/Opk1lerc7pK8oRPjLWqK6AlLySSd09QzTz09bPGMTy6fTQbdEMevk+du/Oo6IY9fJ87d+dWvT1s8YxPLp9NHT1s8YxPLp9NBt0Qx6+T52786kF2gKiWuZIiRptxlNMrW1ETOW2p9YSSlsKUsJBUQBk4AzkmlvT1s8YxPLp9NHT1s8YxPLp9NB8vdWcuTaTpXavAamW26WCw6WnPm86VuriWZ0htw5DLroUoFLTR3mnEhAWN0krHdV37sN1rctrehm9S3fRt+0Jz75ES33mcVSH2N1JS/upXlAUVKSELCVdzvYwU5pHXfIO0DtU1VtH1NrLWs25X7VMnnIEiFJTFbs7KWkNNtpaC1JfUENhKlOZCh6lCDlRtLYXOk7O2E7ONT6qXqS426I0/Cv8ANd7q6M7qQ6s7y1FK0OlWUFSsJW3xOeGHHGTFafHbdZnr2fT82/B53Hx24mKaZaV/X3/694j09/5XH0Qx6+T52786johj18nzt351ai/WzxjE8un00dPWzxjE8un01tfMGrU0JMO2tOMvSkLMyI3nsp3qVIbSR6rwE079EMevk+du/Opk1Zerc7amkonxlq7OhHCXkk4Epsk9fgBp56etnjGJ5dPpr0bdEMevk+du/Oo6IY9fJ87d+dWvT1s8YxPLp9NHT1s8YxPLp9NBt0Qx6+T52786johj18nzt351a9PWzxjE8un00dPWzxjE8un00G3RDHr5PnbvzqOiGPXyfO3fnVr09bPGMTy6fTR09bPGMTy6fTQbdEMevk+du/Oo6IY9fJ87d+dWvT1s8YxPLp9NHT1s8YxPLp9NA13uEmNJtCW3pSA9MDax2U6d5PNOHHqvCBTt0Qx6+T52786mXUF6ty5djKZ8ZQTPBVh5JwOadGTx90U8dPWzxjE8un00G3RDHr5PnbvzqOiGPXyfO3fnVr09bPGMTy6fTR09bPGMTy6fTQbdEMevk+du/Oo6IY9fJ87d+dWvT1s8YxPLp9NHT1s8YxPLp9NBt0Qx6+T52786johj18nzt351a9PWzxjE8un00dPWzxjE8un00G3RDHr5PnbvzqOiGPXyfO3fnVr09bPGMTy6fTR09bPGMTy6fTQNU+ElrUFpYS9KS08l8rT2U73WAnH8r3aeosFqGFhpJBWcqUtRUpXgySST8PhphuV6tytS2VQnxilKH95QeTgZSnGeNPPT1s8YxPLp9NAuFFIk322kcLhFP/HT6aKBdRRRQFILtcmrPAdlvtyVstjukxIzkhw5OODbaVKV194UvooOStMaPvGs+RezpqLZbvC1Ja3W7h0bdbXIguuLZuJlJbSHkI3ipKMDHfUM9dWdKhL2h7e9DahtzL/Q2m7TcFSJL7C2h2RJ5lCGO6APOJS24pQ609yDgmrnrQA+9Qb157uMkD3BXpRQNWoYrU2wXSPIaQ9HdjOtuNupCkLSUnIIOQQf0Gq+5NFogq5OeypRhR1E6UtJJLSeJMRr3Ksi9fcaf+Ic+SagfJk/i47Kf7J2n9TaoJpcLNAFvlEQY/BpWPrSfAfcpNpq0QVadtalQo6lGK0SS0niSge5TrcPubL/ABS/0Gk+mfvctX5I18gUHp0Jb/YMXyCfRR0Jb/YMXyCfRS6igQ9CW/2DF8gn0UdCW/2DF8gn0UuooEPQlv8AYMXyCfRR0Jb/AGDF8gn0UuooEPQlv9gxfIJ9FHQlv9gxfIJ9FLqKCMaztEFvSV5UiFHSoRHCFBpOQQk+5Tz0Jb/YMXyCfRSHW33n3v8AJHfkmnygQ9CW/wBgxfIJ9FHQlv8AYMXyCfRS6igQ9CW/2DF8gn0UdCW/2DF8gn0UuooG82W3p/mMXyKfRUL2l7Pnb0zabnYYcHp6zTES4yJA5tp5Byh5laglRAW2tYHAgKCCQcVYJGR7lG518TUZrFo1K3DltgvGSittPa6gS79G0/ftKvabvUgL5lElhDkWVuJ3lBp9IKFHd3jundXhJO7gGluyjXuittehbfrLR6mLpp64KdTHkqiFkr5p1bS8oWkKGFoV1jjwPUa5q/dMdNTrZs1g69hbQ71pp62y41vasUaTzca4LedLZKN3C0PhtxxW+CrLbak7vHNUX+596Y1Bedr900m/rrVWhIOlBGva9C5cjLnKUsBQdaeGUMgBnfARlYkJwpPfyxlvTLGKY3E97d//AB+DkcC/xDFeKTSYiadzM733Ht5f7fSHVtpgt2hoohR0kzoYyGk9RlN573gJ+GnnoS3+wYvkE+ikOsPuQz+Xwv1pqnvNbHzZF0Jb/YMXyCfRR0Jb/YMXyCfRS6igQ9CW/wBgxfIJ9FHQlv8AYMXyCfRS6igQ9CW/2DF8gn0UdCW/2DF8gn0UuooEPQlv9gxfIJ9FHQlv9gxfIJ9FLqKCM6htMJEuxBMKOAqeEnDSeI5p33PDinjoS3+wYvkE+ikWovtyw/8AWA/yXae6BD0Jb/YMXyCfRR0Jb/YMXyCfRS6igQ9CW/2DF8gn0UdCW/2DF8gn0UuooEPQlv8AYMXyCfRR0Jb/AGDF8gn0UuooEPQlv9gxfIJ9FHQlv9gxfIJ9FLqKCMXK0QRqWypEKOElEjI5pPHuU471PPQlv9gxfIJ9FIrp99Fj/qSPkpp7oEHQsAjhDjjvcGkj/wAqKXCigzRRRQFYzWaQXhme/AdTbJMeJNOObelR1Ptp48coStBPDPUofn6qBdkeGgEHviufbDtyvdu5KcTX9zMa4akkDsdshkNsrkuTTGZ7hJHcAqQSM5wOvJzUxkaxu2kdq2mtIzJ3TTOoLVMfjPSW22ltyovNFQ3m0gbi0uk4KSUlHWc4oLRyD+akSb5bVXdVqFwim6Ja58wg8nnw3kDf3M727lSRnGOI8NRtFy16pQSvTOnEo6iU6hfJA97sGorc77aofKVtkYzIouB0tMBjJdTzy1dkxlBO7nJO6CQPBQTKPrvTWrrdeWLFqG1Xp+Ky4JDdumtPqZOFJwsIUSniCOPfBqP8mT+Ljsp/snaf1NqmDS+qOx7lFslrdE60S9PSblMK0NJet7yVNpbQ6WuAKwtwYUpSgWDgnBp95MriBycdlI3hntTtI6//AMm1QWFcPubL/FL/AEGk+mfvctX5I18gV7XBxPR0obwzzS+/7hpPplxPa5ahvD7VaHX+AKB0orXnEeuT8NHOI9cn4aDaitecR65Pw0c4j1yfhoNqK15xHrk/DRziPXJ+Gg2orXnEeuT8NHOI9cn4aBl1t9597/JHfkmnymLWziTo+9jeGexHR1/gmnvnEeuT8NBtRWvOI9cn4aOcR65Pw0G1Fa84j1yfho5xHrk/DQbUVrziPXJ+GjnEeuT8NA2XSwW2+KhquVuiXDsOQiZFMphLhYfTkJdb3gd1Y3lYUOIyePGqu1xDt2y/ak1tIetAcgTLY5bbvc48Ln34SUKS624SlJWlohCg4RwJbYKhhAIuIrQB6ofmNJ5bMefFdjSG23mHUlC23BlKkkYII74qFq+KGnjZoxX/AFRusxqY9v8AfrHuadTSWpVjjutOJcQqdBIUk5BHZTVPxIJ6+HuVRmrtFSdl1zhXLRdmVKsMhbDU+zRpaWkodTKaWw4ylxQQnOXUKAxnfQf5NSnZvygtHbUNT3DTtmmudPW63RbnNgSEBK46Hy4A2rifriCghaR6neTxOajW+58M9SuzcXwUjNhnxUn19Y9pj0nr6+azqK1DiCAd5Pw0c4j1yfhq1gbUVrziPXJ+GjnEeuT8NBtRWvOI9cn4aOcR65Pw0G1Fa84j1yfho5xHrk/DQM2ovtyw/wDWA/yXae6Y9ROJ7LsJ3hjpAd//AHTtPXOI9cn4aDaitecR65Pw0c4j1yfhoNqK15xHrk/DRziPXJ+Gg2orXnEeuT8NHOI9cn4aDaitecR65Pw0c4j1yfhoGa6ffRY/6kj5Kae6Yrm4k6nsZCgRuSO/+CmnvnEeuT8NBkUVpz7aetxI98iig9KKKKApBdrk1Z4Dst9uStlsd0mJGckOHJxwbbSpSuvvCl9FByVpjR941nyL2dNRbLd4WpLW63cOjbra5EF1xbNxMpLaQ8hG8VJRgY76hnrqzpUJe0Pb3obUNuZf6G03abgqRJfYW0OyJPMoQx3QB5xKW3FKHWnuQcE1c9aAH3qDetN3j6a3ooGTU9ng3PTF6gzoMeXCmxXmpUV9lK230LbKVpWkghQIJBB6xwquOTVpKwvcnXZY65ZbetbmlbUpS1RW1FRMRriTu8atW8pUbROShClqLCwEoGSo7p4Ad+ufthu2iHpDYts/sN10br+Nc7Xp+3wZbPaXc1c281HbQtOQxxwpJ40F2XDR1gRAlEWO2pIaUciG34D7lJtPaPsLtgtrjtjty3FxmlKUqI2So7g6zu8TUKufKPsSbdLKtLa+QkNKypWirmABg/7ikul+UhYHNNWlTemNeutqiMlLjei7mpKgUAgghjiKC0e0vT/iK2eZt/No7S9P+IrZ5m382oF9UbZPaptA+JN0+go+qNsntU2gfEm6fQUE97S9P+IrZ5m382jtL0/4itnmbfzagX1Rtk9qm0D4k3T6Cj6o2ye1TaB8Sbp9BQT3tL0/4itnmbfzaO0vT/iK2eZt/NqBfVG2T2qbQPiTdPoKPqjbJ7VNoHxJun0FBPe0vT/iK2eZt/No7S9P+IrZ5m382oF9UbZPaptA+JN0+go+qNsftU2gfEm6fQUEj1fpKxx9K3d1qy29pxuK4UrRFbBSd08QQngaeu0vT/iK2eZt/Nqpdc8pLT7Wjb4t7TWvGGUxHSt57RlzQhACTkklnAHu0+jlHWM9WlNoHxJun0FBPe0vT/iK2eZt/No7S9P+IrZ5m382oF9UbZPaptA+JN0+go+qNsntU2gfEm6fQUE97S9P+IrZ5m382jtL0/4itnmbfzagX1Rtk9qm0D4k3T6Cj6o2ye1TaB8Sbp9BQT3tL0/4itnmbfzaO0vT/iK2eZt/NqBfVG2T2qbQPiTdPoKPqjbJ7VNoHxJun0FBPe0vT/iK2eZt/No7S9P+IrZ5m382oF9UbZPaptA+JN0+go+qOsftU2gfEm6fQUCDlD7IrRrLZVc7VEuLmhX3H4pTfrKER34v2S3lW9j1HE7wyMjIyM5r5K7K7BdtR6u0XcFXvUmk4Gs70LOjXjceQhExTzgBShxRCVqUptobpUQkgnuubUK+rGtuUZpw2RtMnS+ukNLnQkK7I0Xc0pUVSmkhPFniVEhIHfJA79LDtm0cq1RLWdAayNrh8yY8I6CuPMs80pJZ3Edj7qdwoQU4HclIxjArNlwRmmtpmf0z6O18O+K5Ph+LNhrStoyRqdxvXvHunOkdldg0npW0WQxBejbobMTpK7ttyJkotoCedec3RvuKxlSsDJJPfp57S9P+IrZ5m382oF9UbY/aptA+JN0+go+qNsntU2gfEm6fQVpcVPe0vT/iK2eZt/No7S9P+IrZ5m382oF9UbZPaptA+JN0+go+qNsntU2gfEm6fQUE97S9P+IrZ5m382jtL0/4itnmbfzagX1Rtk9qm0D4k3T6Cj6o2ye1TaB8Sbp9BQT3tL0/4itnmbfzaO0vT/iK2eZt/NqBfVG2T2qbQPiTdPoKPqjbJ7VNoHxJun0FBJL9pKxtS7KG7Nb0Bc0IVuxWxvJ5pw7p7niOHV71PPaXp/xFbPM2/m1UupOUjYG5unw5prXjSl3EJQHNGXNJWrmXe5TlnicAnA7wNPv1R1j4/wCqm0Dh/wDom6fQUE97S9P+IrZ5m382jtL0/wCIrZ5m382oD9UdYz/srtA+JN0+grP1Rtk9qm0D4k3T6CgnvaXp/wARWzzNv5tHaXp/xFbPM2/m1AvqjbJ7VNoHxJun0FH1Rtk9qm0D4k3T6CgnvaXp/wARWzzNv5tHaXp/xFbPM2/m1AvqjbJ7VNoHxJun0FH1Rtk9qm0D4k3T6CgnvaXp/wARWzzNv5tHaXp/xFbPM2/m1AvqjbH7VNoHxJun0FH1Rtj9qm0D4k3T6Cgkly0lYkajs7Qs1vCHEP7yBFbwrCU9Y3eNPPaXp/xFbPM2/m1Ul15SWn06qsSFaa14h1SJO40rRlzC19ykndHM5OMjOOqn36o6x+1XaB8Sbp9BQTwaRsKMgWa3tjwCMgZ93qoqAnlG2XPDSu0Af/JVz+gooLZooooCiiigKKKKAooooCiiigrraTreXZdW6D0nbXOxZupp7za5W6lamI7DC33SgKBSVHdQkZBACiccBRs91rOuOvtd6OuLvZj+nXIjzE0oShT0eU0paAsJAG8lbbqcgDICTjOSWra9p6UnaJsu1kyw5Ig6euEtmeGWy4ppmTFW1zu6MndS4G97HUFFR4JNY2V2OVJ2r7UNZFlbVqvS7dAgLdbKC+iKysLeAPEoLjy0pPfCMjgQSFt0UUUBRRRQFFFFAV4SnCzEfWk4UlClD8wr3po1Qq6tabuq7G1HfvKIrqoTUsEsrfCDzYXgg7pUBnBB49YoKs2MTNTyzEl6m2r2vVRdhCUi0QLVHiOoBAKi6pLi1L3N4DuEtjOMgg4pja23XobF7NtZdlA2yfc46l2ost823b35gjowvG/ziULbcKt4gkEbuCMeurWZ20XUWySJZIciPMsdzM68ym7e9Djw43YjzTrOHBw5xbiEhvKjgZIwnNQ9nQl2kcmjTux1cKQnUcS5RLbIBYVuJix56XVSyvG7zamW94KzxUoJ9VkUHWNFFFAUUVjIoM0Vis0BVU7VdXal0trvZxHgXGG3Zb5fk22XFMLekKR2LIdP14rIAy0jgEA/hcatXPCqO2/3JaNbbKQzab5cE23UqZ8xy22SZMaYZMOU3vqW00tIwpaRjOe6zQSZzWE/Ue2S76Kt85dri2azR7jKksNtrccekOupaQOcSoBKUsLUeGSVp44BzB9mO3W7bU9Saf0uh5NruDUO5yL3KitpJU5Em9hBLSV74SHFhbhyCQEhIPfp9ahK0ZygdQatmtSE2HUWnoTDcpMdxXNSYzj5LSwASlSkPpUkEZJSoDugBVbbE9m142V66sGsr7EehQ75b7y1NBbKjAdk3Ls5hLwA7gc2paSTwCk7p4qFBd2x3XU3Wdu1JFuJQu5advsuxyJCEhAkc0Uqbc3RwSVNuNlQHDe3sADAFhVVPJ/03Ps9v1rdp8V2GdSaom3mPHkIKHUsENstFaTgpKkMJVuniN4A9VWtQFFFFAUUUUBVU3nVuprRt/0pptU+E5p27Wu5SzFaglDyVsGMEb7pWre/hVnuUo6+IOKtaqK1tdS3ymdAyU2i/PwbfabrFlTo1imuxmnHlROaSXktFGDuL4g4G7xoFWs7hql/aTcGIm1O06LsEVqMHLbMtUd59zf3itxp5x1IQTwT3SHE5A4cafDq64ah2wXPRMC5O2+PZLJGuMqW000t556Q66hsd2lSQlKWFqOEjJWMEAEFBq/UcuJA2hWu/WgTVTGlsWiJa7W+67cGFRwkJcc7pC1lxS047kITgqAT3Rg2xfRd92L6qgTdYOPSDcNE2m2S57SFvpTPhc9zjSlJBO8pLw3T/L3VAZOKC19iGv5G0nQLN1mobbuceXLtk3mUkIL0aQ4wtSQSSAot7wBJxvVYFVXybdHXHRuzFLd2jLhXC53O4Xl2K4nC2BJluvIQod5QQtII8OatSgKKKKAooooNFqKUKISVED1IrnrXW0nW2jdnmntoS3p8WVLu8KHM0ZdoUdA5uRKTH5potguJdSFhaVFxQVg5SAcJ6GJ3QTg8OPDv1zfK2vWnWGuocu46N17Mftc3m7FbJGirszERIKi32Y9IXHDQOFK3VE7raCT6pXAHidtZv9z0HtP1ta5SWYuk506NCtxZQW5aII+vF1RG/la0upG6pO6Ak4znLtpjaRctq+pNXx9P3FVot9kgwTGcSy24X5MmP2QC5vhXcJQtkYTuk5Xx6sQWRpm6aV2T7Z9BIt8mTe77crs5ZW2mFKTMbuGVIWlQG7hC3VBZyNzcycAinjZ1px7YXqfXzM+PJkQrpBtsq2vRYy3eyXY8JEV1hISDl3LKFBPWoOcBwVQWhsa2gfvpbLdM6sSwIy7pCQ+6yg9y27xDiRnvBQUKKa+TtoWds62JaO05dmgi5woCRKbSrIQ6ola057+FKIz7lFBZVFFFAUUUUBRRRQFFFFAUUUUGhSCDkZ/NQB19fereigKKKKAooooCiiigKKKKDTBNGDW9FAUUUUBVVfvv3eRtUv8AoW36RXKnWmDHuK5blybbadYeW6lvdG6SFZaXlJ6sjjVq1R+lP4420H+yNn/WZvoNBPtFbSYWrrpdbM9Bl2LUVq3FTbTP3OdSheebeQpClJcaVuqwpJ60kKCVAipjkeGuatpN4XaeVDIvkC5RbTHsGzyeu8XGUCqPGLkhC4fPBPE4U08sJ6yN7HXTpo/Xd3G0vStnnPXSHatQaXkzpXSj27vym3YqUvMbzhcZC+yFDm17hHcYSkg0F5Wy7xLu0+5DfDyGX3I7ikgjdcbWULTx8CgRn3KX9Vcu7G5sPZtsA1XdxqR63S2ZmoDHmXyZImR2S3cJAStSCpRUQrczugqUVYGScVMNDa4usbbHC0/NFyg2i5aZF0Ma9vIWtMpMlpoLbJWpaA4HQObWQQQnuUneBC7xxB8ArPe4YP56qjlR3K52TYdqa62e6zLLdIDbciPLhO7i0q51Iwe8UkE5Seum/Vuo7hsy2r6DhIvbz9m1G3cWLkzdHedSyuPFXJTLSo4LYHNlC0jCPricAYBoLpHD9FGR4a5r0ntSvSNQbKpj0+Wq26oYnC4z5fcW+buRlSG5UdpxZdZTlBxlKAUL4pPckImtomrrfCtTN8F00xe5LMpUPVDk1qdp28umI8pogpWVRhndeCVstjCCMrwKDqHI48eqiuTddbbtU7PtM3eTKsWo9J6pjWZaWYN3ktXKBMcXJjNKmsSEOqB5kOFRbcDR3V5KMZqZa5TtF03Phr0yZTtskxm2ZUDUF2ZbkTZaZDWExnQpfNLea59JxuoCtzAQSVUHQFZqq9jerHtRXnU8WbE1DYLhEMYr07qRSHXYSVJWAtp9tx1DzThSohQcUQUKBxgAWpQFFFFBolP/APlGD+at6KAooooCiiigKKKKAooooNMZ71ABFb0UBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFV+5sR0pI1dddWc3eGb/cWkRpUuNqC4M77TalltAQh8ISlJWogJA6zRRQLY2yfSbWm75p9Vnbk2y9NKFzEt1x96dvpKVF55ai44d0BIKlEgAAYAFNds2UaZmT7VdpMOTKuVpK4sOVIuEhxaGQUnm1FTh305bbUUqyCUgnJ40UUG37zejrjH1LaJVnD9rubjqpMNyQ6Wip9QceUhO/hsqWAslG6d7iONZtWzDTkq42rUkiG/JvkJCo7M1+dIcXzQWFhCt5ZC0hbaFgKBwobw4kklFAycrFpMnk5a4Q4N5C4aEqHhBdRkVKrfs1045PkzJNvVcJPMO2tLlwkOyiiKr1bSOdUrdCsDe3cb26nOcCiigYbdsT0dNh2+DItsh6LYF9j2tDlxkkxWlNFtTaTzmdwoUUFJJBTgEEAY9WNj+knrW5pd62OP6ehwQ1Htz019bLSXGnGlhIUs47gqSD1jeOMZOSigc7foCwz+z2J0DpJhtDtrQzcHnJKExlJG+2EuKUMKwAT1kAAnApMNk2mbpG7FnRZk1qFJR2KZNzlOrjltSFoKFqcKkkKCTkEE7qc9QoooH/Rtkhwo79ybQ4ufOCRIkvPLdcWEFQQnKiSEgE4SMDKlHGSSZLRRQFFFFAUUUUBRRRQFFFFAUUUUBRRRQFFFFAUUUUH/2Q=="
      },
      "index": 42,
      "section": "Experiments"
    },
    {
      "type": "text",
      "text": "## 8.2 MLP on MNIST\n\nIn this section, we evaluate Bernoulli-LoRA against established baselines in parameter-efficient fine-tuning, following the setup of Malinovsky et al. [2024]. Source code of our experiments is available at https://github.com/IgorSokoloff/Bernoulli-LoRA_experiments.\n\nMethodology. We first pre-train a three-layer MLP on MNIST digits 0â€“4 [LeCun et al., 1998], then adapt it with various LoRA-type methods to classify digits 5â€“9. Only unseen classes are used for evaluation. All adaptations use rank r = 1 and train for 50 epochs with AdamW [Loshchilov, 2017] (Î²1 = 0.9, Î²2 = 0.999, Îµ = 10^{-8}), a fixed learning rate of 2 Ã— 10^{-4}, and batch size 128. Each method is run 20 times using different seeds, and Table 3 reports the median accuracy (with standard deviation). For Bernoulli-LoRA, we show the best median accuracy among all tested settings.\n\n| Method           | ð’Ÿ_A             | ð’Ÿ_B             | Acc. (test)  | Train Params |\n| ---------------- | ---------------- | ---------------- | ------------ | ------------ |\n| FPFT             | -                | -                | 99.5         | 54,700       |\n| LoRA             | Gaussian         | Zero             | 85.69 Â± 1.60 | 1K           |\n| LoRA             | Zero             | Gaussian         | 89.82 Â± 0.90 | 1K           |\n| COLA             | Gaussian         | Zero             | 93.32 Â± 0.50 | 1K           |\n| COLA             | Zero             | Gaussian         | 96.55 Â± 0.20 | 1K           |\n| AsymmLoRA        | Gaussian         | Zero             | 64.04 Â± 6.90 | 133          |\n| AsymmLoRA        | Zero             | Gaussian         | 74.52 Â± 7.20 | 912          |\n| RAC-LoRA         | Gaussian         | Zero             | 93.02 Â± 0.50 | 133          |\n| RAC-LoRA         | Zero             | Gaussian         | 96.49 Â± 0.20 | 912          |\n| Bernoulli-LoRA^2 | Zero^1           | Gaussian         | 96.46 Â± 0.17 | â‰ˆ 904        |\n\n^1 Although Bernoulli-LoRA prescribes probabilistic selection from the first iteration, a deterministic assignment of fixed and trainable matrices at initialization yielded better performance.\n\n^2 Achieved with p = 0.99, giving an expected trainable parameter count pÂ·912 + (1 âˆ’ p)Â·133 â‰ˆ 904. Here, 912 and 133 are the parameter counts for matrices A and B, respectively.\n\nTable 3: Performance on MNIST classification using an MLP with rank r and scaling Î± = 1. For AsymmLoRA and RAC-LoRA, only the zero-initialized matrix is trained.\n\nDiscussion. From Table 3, standard LoRA attains roughly 86% of full-parameter fine-tuning (FPFT) accuracy, indicating room for improvements via chaining. COLA improves upon vanilla LoRA, though both lack formal convergence guarantees. AsymmLoRA approximates LoRA in practice [Sun et al., 2024] but similarly lacks convergence analysis, whereas RAC-LoRA and Bernoulli-LoRA both boost accuracy and have theoretical backing. Notably, Bernoulli-LoRA matches RAC-LoRA in generalization and also guarantees convergence. An additional benefit is that RAC-LoRA and Bernoulli-LoRA each train only one matrix per LoRA block, whereas COLA needs two. In RAC-LoRA, either A or B is trained deterministically; in Bernoulli-LoRA, the choice is probabilistic, yielding an expected pmr + (1 âˆ’ p)rn trainable parameters. This advantage is especially valuable in resource-constrained settings such as Federated Learning.\n\nDetailed configurations, hardware specs, and dataset descriptions are provided in Appendix E.",
      "index": 43,
      "section": "Experiments"
    },
    {
      "type": "text",
      "text": "## Impact Statement\n\nThis paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.",
      "index": 44,
      "section": "Conclusion"
    },
    {
      "type": "text",
      "text": "17",
      "index": 45,
      "section": "Conclusion"
    },
    {
      "type": "text",
      "text": "\\n\\n\\n\\n\nAcknowledgements\n\nThe research reported in this publication was supported by funding from King Abdullah University of Science and Technology (KAUST): i) KAUST Baseline Research Scheme, ii) CRG Grant ORFS-CRG12-2024-6460, iii) Center of Excellence for Generative AI, under award number 5940, and iv) SDAIA-KAUST Center of Excellence in Artificial Intelligence and Data Science.",
      "index": 46,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "References\n\nArmen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. arXiv preprint arXiv:2012.13255, 2020.\n\nDan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD: Communication-efficient SGD via gradient quantization and encoding. Advances in Neural Information Processing Systems, 30, 2017.\n\nDan Alistarh, Torsten Hoefler, Mikael Johansson, Nikola Konstantinov, Sarit Khirirat, and CÃ©dric Renggli. The convergence of sparsified gradient methods. Advances in Neural Information Processing Systems, 31, 2018.\n\nAmir Beck. First-order methods in optimization. SIAM, 2017.\n\nDan Biderman, Jacob Portes, Jose Javier Gonzalez Ortiz, Mansheej Paul, Philip Greengard, Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, et al. LoRA learns less and forgets less. arXiv preprint arXiv:2405.09673, 2024.\n\nLÃ©on Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. SIAM Review, 60(2):223â€“311, 2018.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33:1877â€“1901, 2020.\n\nSÃ©bastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends in Machine Learning, 8(3â€“4):231â€“357, 2015.\n\nDaria Cherniuk, Aleksandr Mikhalev, and Ivan Oseledets. Run LoRA Run: Faster and lighter LoRA implementations. arXiv preprint arXiv:2312.03415, 2023.\n\nAshok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex SGD. Advances in Neural Information Processing Systems, 32, 2019.\n\nYury Demidovich, Grigory Malinovsky, Egor Shulgin, and Peter RichtÃ¡rik. MAST: Model-agnostic sparsified training. arXiv preprint arXiv:2311.16086, 2023a.\n\nYury Demidovich, Grigory Malinovsky, Igor Sokolov, and Peter RichtÃ¡rik. A guide through the zoo of biased SGD. Advances in Neural Information Processing Systems, 36:23158â€“23171, 2023b.\n\nYury Demidovich, Grigory Malinovsky, and Peter RichtÃ¡rik. Streamlining in the Riemannian realm: Efficient Riemannian optimization with loopless variance reduction. arXiv preprint arXiv:2403.06677, 2024a.\n\nYury Demidovich, Petr Ostroukhov, Grigory Malinovsky, Samuel HorvÃ¡th, Martin TakÃ¡Ä, Peter RichtÃ¡rik, and Eduard Gorbunov. Methods with local steps and random reshuffling for generally smooth non-convex federated optimization. arXiv preprint arXiv:2412.02781, 2024b.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2â€“7, 2019, Volume 1 (Long and Short Papers), pages 4171â€“4186. Association for Computational Linguistics, 2019. doi: 10.18653/v1/N19-1423. URL https://doi.org/10.18653/v1/N19-1423.",
      "index": 47,
      "section": "References"
    },
    {
      "type": "text",
      "text": "Dmitriy Drusvyatskiy. Convex analysis and nonsmooth optimization. *University Lecture*, 2020.\n\nJohn C. Duchi. Introductory lectures on stochastic optimization. *The Mathematics of Data*, 25:99â€“186, 2018.\n\nCong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. SPIDER: near-optimal non-convex optimization via stochastic path-integrated differential estimator. *Advances in Neural Information Processing Systems*, 31, 2018.\n\nIlyas Fatkhullin, Igor Sokolov, Eduard Gorbunov, Zhize Li, and Peter RichtÃ¡rik. EF21 with bells & whistles: Practical algorithmic extensions of modern error feedback. *arXiv preprint arXiv:2110.03294*, 2021.\n\nEduard Gorbunov, Konstantin P. Burlachenko, Zhize Li, and Peter RichtÃ¡rik. MARINA: Faster non-convex distributed learning with compression. In *International Conference on Machine Learning*, pages 3788â€“3798. PMLR, 2021.\n\nRobert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Peter RichtÃ¡rÃ­k. SGD: General analysis and improved rates. In *International Conference on Machine Learning*, pages 5200â€“5209. PMLR, 2019.\n\nPriya Goyal, Piotr DollÃ¡r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet in 1 hour. *arXiv preprint arXiv:1706.02677*, 2017.\n\nBrian C. Hall. Lie groups, Lie algebras, and representations. In *Quantum Theory for Mathematicians*, pages 333â€“366. Springer, 2013.\n\nZeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, and Sai Qian Zhang. Parameter-efficient fine-tuning for large models: A comprehensive survey. *arXiv preprint arXiv:2403.14608*, 2024.\n\nSoufiane Hayou, Nikhil Ghosh, and Bin Yu. The impact of initialization on LoRA finetuning dynamics. *Advances in Neural Information Processing Systems*, 37:117015â€“117040, 2024.\n\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. *arXiv preprint arXiv:2110.04366*, 2021.\n\nSamuel HorvÃ¡th, Chen-Yu Ho, Ludovit Horvath, Atal Narayan Sahu, Marco Canini, and Peter RichtÃ¡rik. Natural compression for distributed deep learning. In *Mathematical and Scientific Machine Learning*, pages 129â€“141. PMLR, 2022.\n\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. LoRA: Low-rank adaptation of large language models. *ICLR*, 2022.\n\nSashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alexander J. Smola. On variance reduction in stochastic gradient descent and its asynchronous variants. *Advances in Neural Information Processing Systems*, 28, 2015.\n\nPeter Kairouz, H. B. McMahan, Brendan Avent, AurÃ©lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Keith Bonawitz, Zachary B. Charles, Graham Cormode, Rachel Cummings, Rafael G. L. Dâ€™Oliveira, Salim Y. El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, AdriÃ  GascÃ³n, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, ZaÃ¯d Harchaoui, Chaoyang He, Li He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub KoneÄnÃ½, Aleksandra Korolova, Farinaz Koushanfar, Oluwasanmi Koyejo, TancrÃ¨de Lepoint, Yang Liu, Prateek Mittal,",
      "index": 48,
      "section": "References"
    },
    {
      "type": "text",
      "text": "Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer Ã–zgÃ¼r, R. Pagh, Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian TramÃ¨r, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances and open problems in federated learning. Found. Trends Mach. Learn., 14:1â€“210, 2019.\n\nAvetik Karagulyan, Egor Shulgin, Abdurakhmon Sadiev, and Peter RichtÃ¡rik. SPAM: Stochastic proximal point method with momentum variance reduction for nonconvex cross-device federated learning. arXiv preprint arXiv:2405.20127, 2024.\n\nAhmed Khaled and Peter RichtÃ¡rik. Better theory for SGD in the nonconvex world. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=AU4qHN2VkS.\n\nAhmed Khaled, Othmane Sebbouh, Nicolas Loizou, Robert M. Gower, and Peter RichtÃ¡rik. Unified analysis of stochastic gradient methods for composite convex and smooth optimization. Journal of Optimization Theory and Applications, 199(2):499â€“540, 2023.\n\nSarit Khirirat, Abdurakhmon Sadiev, Artem Riabinin, Eduard Gorbunov, and Peter RichtÃ¡rik. Error feedback under (L_0, L_1)-smoothness: Normalization and momentum. arXiv preprint arXiv:2410.16871, 2024.\n\nMikhail Khodak, Renbo Tu, Tian Li, Liam Li, Maria-Florina F. Balcan, Virginia Smith, and Ameet Talwalkar. Federated hyperparameter tuning: Challenges, baselines, and connections to weight-sharing. Advances in Neural Information Processing Systems, 34:19184â€“19197, 2021.\n\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big Transfer (BiT): General visual representation learning. In Computer Vision â€“ ECCV 2020: 16th European Conference, Glasgow, UK, August 23â€“28, 2020, Proceedings, Part V, pages 491â€“507. Springer, 2020.\n\nJakub KoneÄnÃ½, H. Brendan McMahan, Felix X. Yu, Peter RichtÃ¡rik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492, 2016.\n\nJakub KoneÄnÃ½, H. Brendan McMahan, Daniel Ramage, and Peter RichtÃ¡rik. Federated Optimization: Distributed Machine Learning for On-Device Intelligence. arXiv preprint arXiv:1610.02527, 2016.\n\nWeirui Kuang, Bingchen Qian, Zitao Li, Daoyuan Chen, Dawei Gao, Xuchen Pan, Yuexiang Xie, Yaliang Li, Bolin Ding, and Jingren Zhou. FederatedScope-LLM: A comprehensive package for fine-tuning large language models in federated learning. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 5260â€“5271, 2024.\n\nGuanghui Lan. First-Order and Stochastic Optimization Methods for Machine Learning, volume 1. Springer, 2020.\n\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana IliÄ‡, Daniel Hesslow, Roman CastagnÃ©, Alexandra Sasha Luccioni, FranÃ§ois Yvon, Matthias GallÃ©, et al. BLOOM: A 176B-parameter open-access multilingual language model, 2023.\n\nYann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278â€“2324, 1998.",
      "index": 49,
      "section": "References"
    },
    {
      "type": "text",
      "text": "Chuan Li. Demystifying GPT-3 language model: A technical overview, 2020. URL https://lambdalabs.com/blog/demystifying-gpt-3.\n\nChunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the intrinsic dimension of objective landscapes. arXiv preprint arXiv:1804.08838, 2018.\n\nZhize Li, Hongyan Bao, Xiangliang Zhang, and Peter RichtÃ¡rik. PAGE: A simple and optimal probabilistic gradient estimator for nonconvex optimization. In International Conference on Machine Learning, pages 6286â€“6295. PMLR, 2021.\n\nStanislaw Lojasiewicz. A topological property of real analytic subsets. Coll. du CNRS, Les Ã©quations aux dÃ©rivÃ©es partielles, 117(87â€“89):2, 1963.\n\nI. Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\n\nGrigory Malinovsky, Kai Yi, and Peter RichtÃ¡rik. Variance reduced ProxSkip: Algorithm, theory and application to federated learning. Advances in Neural Information Processing Systems, 35:15176â€“15189, 2022.\n\nGrigory Malinovsky, Umberto Michieli, Hasan Abed Al Kader Hammoud, Taha Ceritli, Hayder Elesedy, Mete Ozay, and Peter RichtÃ¡rik. Randomized asymmetric chain of LoRA: The first meaningful theoretical framework for low-rank adaptation. arXiv preprint arXiv:2410.08305, 2024.\n\nYuren Mao, Yuhang Ge, Yijiang Fan, Wenyi Xu, Yu Mi, Zhonghao Hu, and Yunjun Gao. A survey on LoRA of large language models. Frontiers of Computer Science, 19(7):197605, 2025. doi: 10.1007/s11704-024-40663-9. URL https://journal.hep.com.cn/fcs/EN/abstract/article_47717.shtml.\n\nH. B. McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise AgÃ¼era y Arcas. Communication-efficient learning of deep networks from decentralized data. In International Conference on Artificial Intelligence and Statistics, 2016.\n\nFanxu Meng, Zhaohui Wang, and Muhan Zhang. PiSSA: Principal singular values and singular vectors adaptation of large language models. Advances in Neural Information Processing Systems, 37:121038â€“121072, 2024.\n\nArkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574â€“1609, 2009.\n\nYurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2013.\n\nYurii Nesterov. Lectures on convex optimization, volume 137. Springer, 2018.\n\nAndrei Panferov, Yury Demidovich, Ahmad Rammal, and Peter RichtÃ¡rik. Correlated quantization for faster nonconvex distributed optimization. arXiv preprint arXiv:2401.05518, 2024.\n\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Marilyn A. Walker, Heng Ji, and Amanda Stent, editors, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1â€“6, 2018, Volume 1 (Long Papers), pages 2227â€“2237. Association for Computational Linguistics, 2018. doi: 10.18653/v1/N18-1202. URL https://doi.org/10.18653/v1/N18-1202.",
      "index": 50,
      "section": "References"
    },
    {
      "type": "text",
      "text": "Nhan H. Pham, Lam M. Nguyen, Dzung T. Phan, and Quoc Tran-Dinh. ProxSARAH: An efficient algorithmic framework for stochastic composite nonconvex optimization. Journal of Machine Learning Research, 21(110):1â€“48, 2020.\n\nBoris Polyak. Gradient methods for the minimization of functionals. USSR Computational Mathematics and Mathematical Physics, 3(4):864â€“878, 1963.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI Blog, 2019.\n\nPeter RichtÃ¡rik and Martin TakÃ¡Ä. Parallel coordinate descent methods for big data optimization. Mathematical Programming, 156:433â€“484, 2016.\n\nPeter RichtÃ¡rik, Igor Sokolov, and Ilyas Fatkhullin. EF21: A new, simpler, theoretically better, and practically faster error feedback. Advances in Neural Information Processing Systems, 34:4384â€“4396, 2021.\n\nPeter RichtÃ¡rik, Igor Sokolov, Elnur Gasanov, Ilyas Fatkhullin, Zhize Li, and Eduard Gorbunov. 3PC: Three point compressors for communication-efficient distributed training and a better theory for lazy aggregation. In International Conference on Machine Learning, pages 18596â€“18648. PMLR, 2022.\n\nPeter RichtÃ¡rik, Abdurakhmon Sadiev, and Yury Demidovich. A unified theory of stochastic proximal point methods without smoothness. arXiv preprint arXiv:2405.15941, 2024.\n\nHerbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical Statistics, 22(3):400â€“407, 1951.\n\nAbdurakhmon Sadiev, Grigory Malinovsky, Eduard Gorbunov, Igor Sokolov, Ahmed Khaled, Konstantin Burlachenko, and Peter RichtÃ¡rik. Don't compress gradients in random reshuffling: Compress gradient differences. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=CzPtBzgfae.\n\nIssai Schur. Neue BegrÃ¼ndung der Theorie der Gruppencharaktere. 1905.\n\nFrank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs. In Fifteenth Annual Conference of the International Speech Communication Association, 2014.\n\nFanhua Shang, Kaiwen Zhou, Hongying Liu, James Cheng, Ivor W. Tsang, Lijun Zhang, Dacheng Tao, and Licheng Jiao. VR-SGD: A simple stochastic variance reduction method for machine learning. IEEE Transactions on Knowledge and Data Engineering, 32(1):188â€“202, 2018.\n\nIgor Sokolov and Peter RichtÃ¡rik. MARINA-P: Superior performance in non-smooth federated optimization with adaptive stepsizes. arXiv preprint arXiv:2412.17082, 2024.\n\nSebastian U. Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsified SGD with memory. Advances in Neural Information Processing Systems, 31, 2018.\n\nRuo-Yu Sun. Optimization for deep learning: An overview. Journal of the Operations Research Society of China, 8(2):249â€“294, 2020.\n\nYoubang Sun, Zitao Li, Yaliang Li, and Bolin Ding. Improving LoRA in privacy-preserving federated learning. arXiv preprint arXiv:2403.12313, 2024.",
      "index": 51,
      "section": "References"
    },
    {
      "type": "text",
      "text": "Alexander Tyurin and Peter RichtÃ¡rik. DASHA: Distributed Nonconvex Optimization with Communication Compression and Optimal Oracle Complexity. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=VA1YpcNr7ul.\n\nRoman Vershynin. High-Dimensional Probability, 2009.\n\nEvgeniya Vorontsova, Roland Hildebrand, Alexander Gasnikov, and Fedor Stonyakin. Convex Optimization. arXiv preprint arXiv:2106.01946, 2021.\n\nHanqing Wang, Yixia Li, Shuo Wang, Guanhua Chen, and Yun Chen. MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Fine-Tuning. arXiv preprint arXiv:2406.09044, 2024.\n\nWei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning. Advances in Neural Information Processing Systems, 30, 2017.\n\nWenhan Xia, Chengwei Qin, and Elad Hazan. Chain of LoRA: Efficient Fine-Tuning of Language Models via Residual Learning. arXiv preprint arXiv:2401.04151, 2024.\n\nKai Yi, Timur Khamisov, Igor Sokolov, and Peter RichtÃ¡rik. Cohort Squeeze: Beyond a Single Communication Round per Cohort in Cross-Device Federated Learning. arXiv preprint arXiv:2406.01115, 2024.\n\nYang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large Batch Optimization for Deep Learning: Training BERT in 76 Minutes. arXiv preprint arXiv:1904.00962, 2019.\n\nJiacheng Zhu, Kristjan Greenewald, Kimia Nadjahi, Haitz Saez De Ocariz Borde, Rickard BrÃ¼el Gabrielsson, Leshem Choshen, Marzyeh Ghassemi, Mikhail Yurochkin, and Justin Solomon. Asymmetry in Low-Rank Adapters of Foundation Models. arXiv preprint arXiv:2402.16842, 2024.",
      "index": 52,
      "section": "References"
    },
    {
      "type": "text",
      "text": "APPENDIX",
      "index": 53,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "A Basic Facts and Useful Inequalities\n\nTower property. For any random variables X and Y, we have\n\\mathbb{E}[\\mathbb{E}[X \\mid Y]] = \\mathbb{E}[X]. (13)\n\nCauchyâ€“Bunyakovskyâ€“Schwarz inequality. For any random variables X and Y, we have\n|\\mathbb{E[XY]}| \\leq \\sqrt{\\mathbb{E}[X^2]\\mathbb{E}[Y^2]}. (14)\n\nVariance decomposition. For any random vector X \\in \\mathbb{R}^d and any non-random c \\in \\mathbb{R}^d, we have\n\\mathbb{E}[\\|X - c\\|_2^2] = \\mathbb{E}[\\|X - \\mathbb{E}[X]\\|_2^2] + \\|\\mathbb{E}[X] - c\\|_2^2. (15)\n\nJensenâ€™s inequality. For any random vector X \\in \\mathbb{R}^d and any convex function g : \\mathbb{R}^d \\to \\mathbb{R}, we have\ng(\\mathbb{E}[X]) \\leq \\mathbb{E}[g(X)]. (16)",
      "index": 54,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "B Discussion on Positive Expected Projection (Assumption 1)\n\nAssumption 1 merits further discussion. While any single projection matrix has eigenvalues that are either 0 or 1 (with the smallest being 0), the expected value of a random projection matrix can have all its eigenvalues strictly greater than zero. This property is crucial for ensuring stable convergence behavior in our framework.\n\nLater in this section, we will utilize the following lemma, which is a classical result from linear algebra, often known as a direct consequence of Schurâ€™s Lemma [Hall, 2013, Schur, 2024].\n\nLemma 1 (Rotational Invariance Implies Scalar Matrix). Let M âˆˆ â„^{nÃ—n} be a matrix satisfying\nM = QMQ^\\top for all orthonormal matrices Q âˆˆ â„^{nÃ—n}. (17)\nThen M = Î± I_n for some scalar Î± âˆˆ â„.\n\nProof. The condition M = QMQ^\\top is equivalent to MQ = QM, which means that M commutes with every orthonormal matrix Q. Since M is a real symmetric matrix, it is guaranteed to have at least one real eigenvector. Let v be such an eigenvector with corresponding eigenvalue Î». We can normalize this eigenvector to create a unit vector uâ‚ = v/âˆ¥vâˆ¥, which is also an eigenvector with the same eigenvalue:\nMuâ‚ = M(v/âˆ¥vâˆ¥) = (1/âˆ¥vâˆ¥)Mv = (1/âˆ¥vâˆ¥)(Î»v) = Î»(v/âˆ¥vâˆ¥) = Î»uâ‚.\n\nNow, let u be any other arbitrary unit vector in â„^n. Because both uâ‚ and u are unit vectors (i.e., they lie on the unit sphere), there always exists an orthonormal matrix Q (specifically, a rotation) that maps uâ‚ to u. That is, u = Quâ‚.\n\nWe now examine the action of M on this arbitrary unit vector u:\nMu = M(Quâ‚) = (MQ)uâ‚ = (QM)uâ‚ = Q(Muâ‚) = Q(Î»uâ‚) = Î»(Quâ‚) = Î»u.\n\nWe have shown that any arbitrary unit vector u is an eigenvector of M with the same eigenvalue Î». If every unit vector is an eigenvector with eigenvalue Î», then for any non-zero vector x âˆˆ â„^n, we can write x = âˆ¥xâˆ¥ Â· (x/âˆ¥xâˆ¥). Let u_x := x/âˆ¥xâˆ¥ be the corresponding unit vector. Then:\nMx = M(âˆ¥xâˆ¥u_x) = âˆ¥xâˆ¥(Mu_x) = âˆ¥xâˆ¥(Î»u_x) = Î»(âˆ¥xâˆ¥u_x) = Î»x.\n\nSince Mx = Î»x for all vectors x âˆˆ â„^n, the matrix M must be a scalar multiple of the identity matrix, i.e., M = Î»I_n. â–¡\n\nIn practice, LoRA-type methods often employ Gaussian sampling for the matrices A_S or B_S [Xia et al., 2024, Mao et al., 2025]. The following lemma, a standard result in multivariate statistics, demonstrates that under such Gaussian sampling, Assumption 1 is naturally satisfied.\n\nLemma 2 (Expected Eigenvalues of Random Projection Matrices). Consider a projection matrix H_B generated by a random matrix B âˆˆ â„^{nÃ—r} whose entries are i.i.d. ð’©(0,1) with r < n, defined as:\nH_B = B(B^\\top B)^\\dagger B^\\top,\nwhere â€  denotes the Mooreâ€“Penrose pseudoinverse. Similarly, for a random matrix A âˆˆ â„^{rÃ—n} with i.i.d. ð’©(0,1) entries, we define:\nH_A = A^\\top(AA^\\top)^\\dagger A.",
      "index": 55,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "For these matrices, we have:\n$$\\mathbb{E}[H_B]=\\mathbb{E}[H_A]=\\frac{r}{n}I_n,$$\nwhich implies:\n$$\\lambda_{\\min}(\\mathbb{E}[H_B])=\\lambda_{\\min}(\\mathbb{E}[H_A])=\\frac{r}{n}.$$",
      "index": 56,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Proof. The proof leverages the rotational invariance property of the standard Gaussian distribution. We will prove the result for $H_B$; the argument for $H_A$ is analogous.\n\nFirst, we establish that $\\mathbb{E}[H_B]$ must be a scalar multiple of the identity matrix. Let $Q\\in\\mathbb{R}^{n\\times n}$ be an arbitrary orthonormal matrix. Due to the rotational invariance of the multivariate standard normal distribution, the random matrix $QB$ has the same distribution as $B$.\n\nConsider the projection matrix $H_{QB}$ generated by $QB$:\n\\[\n\\begin{aligned}\nH_{QB}\n&=(QB)\\bigl((QB)^\\top QB\\bigr)^\\dagger (QB)^\\top\\\\\n&=QB\\bigl(B^\\top Q^\\top QB\\bigr)^\\dagger B^\\top Q^\\top\\\\\n&=QB\\bigl(B^\\top B\\bigr)^\\dagger B^\\top Q^\\top\\\\\n&=Q\\bigl(B(B^\\top B)^\\dagger B^\\top\\bigr)Q^\\top\n=QH_BQ^\\top.\n\\end{aligned}\n\\]\nSince $QB$ and $B$ are identically distributed, their expectations must be equal: $\\mathbb{E}[H_{QB}]=\\mathbb{E}[H_B]$. This implies:\n$$\\mathbb{E}[H_B]=Q\\,\\mathbb{E}[H_B]\\,Q^\\top,$$\nfor every orthonormal matrix $Q$. By Lemma 1, $\\mathbb{E}[H_B]$ must be a scalar multiple of the identity matrix, so $\\mathbb{E}[H_B]=\\alpha I_n$ for some scalar $\\alpha\\in\\mathbb{R}$.\n\nTo determine this scalar, we use the property that the trace of a projection matrix is equal to its rank. Since the columns of $B$ are drawn from a continuous distribution, they are linearly independent almost surely (as $r\\le n$). Thus, the rank of $H_B$ is $r$:\n$$\\mathbb{E}[\\operatorname{Tr}(H_B)]=\\mathbb{E}[\\operatorname{rank}(H_B)]=r.$$\nBy linearity of expectation and trace, we also have:\n$$\\mathbb{E}[\\operatorname{Tr}(H_B)]=\\operatorname{Tr}(\\mathbb{E}[H_B])=\\operatorname{Tr}(\\alpha I_n)=\\alpha n.$$\nEquating the two expressions gives $\\alpha n=r$, which implies $\\alpha=\\frac{r}{n}$. Therefore,\n$$\\mathbb{E}[H_B]=\\frac{r}{n}I_n.$$\n\nThe same argument applies to $H_A$ by observing that $A^\\top$ is an $n\\times r$ matrix with i.i.d. $\\mathcal{N}(0,1)$ entries, which completes the proof. â–¡",
      "index": 57,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Remark 2. This result is foundational in the study of random projections and can be found in standard textbooks on multivariate statistics; for example, see Lemma 5.3.2 in [Vershynin, 2009].",
      "index": 58,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "C Proofs for Core Algorithmic Variants",
      "index": 59,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "C.1 Analysis of Bernoulli-LoRA-GD\n\nAlgorithm 2 Bernoulli-LoRA-GD\n\n1: Parameters: pre-trained model W^0 âˆˆ R^{mÃ—n}, rank r â‰ª min{m,n}, scaling factor Î± > 0, stepsize Î³_t, chain length T, sketch distribution D_S^B or D_S^A, Bernoulli probability p\n2: for t = 0, 1, â€¦, T âˆ’ 1 do\n3: Sample c^t âˆ¼ Be(p)\n\nThe following lemma establishes that the Bernoulli-LoRA update can be reformulated as a standard projected gradient descent step, providing a crucial foundation for our subsequent convergence analysis.\n\nLemma 3. Consider the updates Ã‚^t and BÌ‚^t from Algorithm 2 computed as solutions to the following optimization problems:\nÃ‚^t := arg min_A { f(W^t) + (Î±/r) âŸ¨âˆ‡f(W^t), B_S^t AâŸ©_F + (Î±^2/(2Î³^2)) âˆ¥B_S^t Aâˆ¥_F^2 },\nBÌ‚^t := arg min_B { f(W^t) + (Î±/r) âŸ¨âˆ‡f(W^t), B A_S^tâŸ©_F + (Î±^2/(2Î³^2)) âˆ¥B A_S^tâˆ¥_F^2 }. (18)\n\nThen the Left and Right sketch updates can be expressed as a gradient descent step:\nW^{t+1} = W^t âˆ’ Î³ G^t. (19)\n\nwhere G^t is defined by\nG^t = { H_B^t âˆ‡f(W^t), with probability p\n        âˆ‡f(W^t) H_A^t, with probability 1 âˆ’ p } (20)\n\nwith projection matrices H_A^t and H_B^t given by:\nH_A^t := (A_S^t)^âŠ¤ (A_S^t (A_S^t)^âŠ¤)^â€  A_S^t and H_B^t := B_S^t ((B_S^t)^âŠ¤ B_S^t)^â€  (B_S^t)^âŠ¤, (21)\n\nwhere â€  denotes the Mooreâ€“Penrose pseudoinverse.\n\nProof. Following Algorithm 2, at each iteration we randomly select either the Left sketch (with probability p) or the Right sketch (with probability 1 âˆ’ p). We analyze both cases separately and then combine them into a unified update rule.",
      "index": 60,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Left Sketch Analysis. When the Left sketch is selected, the update takes the form:\n$$W^{t+1} = W^t + \\frac{\\alpha}{r} B_S^t \\hat{A}^t.$$\n(22)\n\nMinimizing the right-hand side with respect to $\\hat{A}^t$ yields:\n$$\\frac{\\alpha}{r} (B_S^t)^\\top \\nabla f(W^t) + \\frac{\\alpha^2}{\\gamma r^2} (B_S^t)^\\top B_S^t \\hat{A}^t = 0;$$\n$$(B_S^t)^\\top B_S^t \\hat{A}^t = -\\frac{\\gamma r}{\\alpha} (B_S^t)^\\top \\nabla f(W^t);$$\n$$\\hat{A}^t = -\\frac{\\gamma r}{\\alpha} \\left( (B_S^t)^\\top B_S^t \\right)^\\dagger (B_S^t)^\\top \\nabla f(W^t).$$\n(23)\n\nThis leads to the Left sketch update:\n$$W^{t+1} = W^t + \\frac{\\alpha}{r} B_S^t \\hat{A}^t$$\n$$= W^t - \\gamma B_S^t \\left( (B_S^t)^\\top B_S^t \\right)^\\dagger (B_S^t)^\\top \\nabla f(W^t)$$\n$$= W^t - \\gamma H_B^t \\nabla f(W^t),$$\n(24)\nwhere $H_B^t := B_S^t \\left( (B_S^t)^\\top B_S^t \\right)^\\dagger (B_S^t)^\\top$ is a projection matrix.",
      "index": 61,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Right Sketch Analysis. For the Right sketch, we follow a similar approach. The update rule is:\n$$W^{t+1} = W^t + \\frac{\\alpha}{r} \\hat{B}^t A_S^t.$$\n(25)\n\nFirst, observe that:\n$$\\left\\| \\hat{B}^t A_S^t \\right\\|_F^2 = \\left\\langle \\hat{B}^t A_S^t, \\hat{B}^t A_S^t \\right\\rangle_F = \\left\\langle A_S^t, \\left( \\hat{B}^t \\right)^\\top \\hat{B}^t A_S^t \\right\\rangle_F.$$\n(26)\n\nFor the linear term from (18):\n$$\\frac{\\alpha}{r} \\left\\langle \\nabla f(W^t), \\hat{B}^t A_S^t \\right\\rangle_F = \\frac{\\alpha}{r} \\mathrm{Tr} \\left( (\\nabla f(W^t))^\\top \\hat{B}^t A_S^t \\right),$$\n(27)\nwith gradient $\\nabla f(W^t) (A_S^t)^\\top$ with respect to $\\hat{B}^t$. Using the matrix calculus identity $\\nabla_X \\|X\\|_F^2 = 2X$, the gradient of the quadratic term is:\n$$\\frac{\\alpha^2}{\\gamma r^2} \\hat{B}^t A_S^t (A_S^t)^\\top.$$\n(28)\n\nSetting the total gradient to zero and solving for $\\hat{B}^t$:\n$$\\hat{B}^t = -\\frac{\\gamma r}{\\alpha} \\nabla f(W^t) (A_S^t)^\\top \\left( A_S^t (A_S^t)^\\top \\right)^\\dagger,$$\n(29)\nwhich yields the Right sketch update:\n$$W^{t+1} = W^t + \\frac{\\alpha}{r} \\hat{B}^t A_S^t$$\n$$= W^t - \\gamma \\nabla f(W^t) (A_S^t)^\\top \\left( A_S^t (A_S^t)^\\top \\right)^\\dagger A_S^t$$\n$$= W^t - \\gamma \\nabla f(W^t) H_A^t,$$\n(30)",
      "index": 62,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "where H_A^t := (A_S^t)^\\top \\left(A_S^t (A_S^t)^\\top\\right)^\\dagger A_S^t is a projection matrix.",
      "index": 63,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Combined Update Rule. Combining equations (24) and (30), we obtain the unified update:\nW^{t+1} = W^t - \\gamma G^t, (31)\nwhere G^t takes the form given in the lemma statement, completing the proof. â–¡\n\nWith these assumptions in place, we can now state our main convergence result for RAC-LoRA with Gradient Descent updates.",
      "index": 64,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "C.1.1 Convergence for Smooth Non-Convex Functions\n\nTheorem 1. Let Assumptions 1, 3, and 2 hold, and let the stepsize satisfy 0 < \\gamma \\le 1/L. Then the iterates of Bernoulli-LoRA-GD (Algorithm 2), with matrices \\hat{A}^t and \\hat{B}^t computed according to Lemma 3, satisfy\n\\mathbb{E}\\!\\left[\\|\\nabla f(\\widetilde{W}^T)\\|_F^2\\right] \\le \\frac{2\\,(f(W^0)-f^*)}{\\gamma\\,\\lambda_{\\min}^p\\,T}, (32)\nwhere \\lambda_{\\min}^p := p\\,\\lambda_{\\min}^{H_B} + (1-p)\\,\\lambda_{\\min}^{H_A} and \\widetilde{W}^T is drawn uniformly at random from the iterate sequence {W^0, W^1, ..., W^{T-1}}.\n\nProof. From Lemma 3, we know that Bernoulli-LoRA updates can be expressed as\nW^{t+1} = W^t - \\gamma G^t, (33)\nwhere G^t takes the form\nG^t = \\begin{cases}\nH_B^t \\nabla f(W^t), & \\text{with probability } p, \\\\\n\\nabla f(W^t) H_A^t, & \\text{with probability } 1-p,\n\\end{cases} (34)\nwith projection matrices H_A^t and H_B^t as defined in the lemma.\n\nTo analyze the convergence, we first compute the conditional expectation and second moment of G^t:\n\\mathbb{E}[G^t \\mid W^t, H^t] = p\\,H_B^t \\nabla f(W^t) + (1-p)\\,\\nabla f(W^t) H_A^t,\n\\mathbb{E}\\!\\left[\\|G^t\\|_F^2 \\mid W^t, H^t\\right] = p\\,\\|H_B^t \\nabla f(W^t)\\|_F^2 + (1-p)\\,\\|\\nabla f(W^t) H_A^t\\|_F^2, (35)\nwhere we defined H^t := {H_A^t, H_B^t}.\n\nWe begin by establishing several key auxiliary bounds. For the Left sketch term:\n-\\gamma p \\left\\langle \\nabla f(W^t), H_B^t \\nabla f(W^t) \\right\\rangle_F\n+ \\frac{L\\gamma^2}{2}\\, p \\left\\| H_B^t \\nabla f(W^t) \\right\\|_F^2\n= -\\gamma p \\left\\langle \\nabla f(W^t), H_B^t \\nabla f(W^t) \\right\\rangle_F\n+ \\frac{L\\gamma^2}{2}\\, p \\left\\langle H_B^t \\nabla f(W^t), H_B^t \\nabla f(W^t) \\right\\rangle_F\n= -\\gamma p \\left\\langle \\nabla f(W^t), H_B^t \\nabla f(W^t) \\right\\rangle_F\n+ \\frac{L\\gamma^2}{2}\\, p \\left\\langle \\nabla f(W^t), (H_B^t)^\\top H_B^t \\nabla f(W^t) \\right\\rangle_F\n= p \\left( -\\gamma \\left\\langle \\nabla f(W^t), H_B^t \\nabla f(W^t) \\right\\rangle_F\n+ \\frac{L\\gamma^2}{2} \\left\\langle \\nabla f(W^t), H_B^t \\nabla f(W^t) \\right\\rangle_F \\right)\n\\stackrel{\\gamma \\le 1/L}{\\le} -\\frac{\\gamma}{2}\\, p \\left\\langle \\nabla f(W^t), H_B^t \\nabla f(W^t) \\right\\rangle_F. (36)",
      "index": 65,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "For any projection matrix H_A^t, we have:\nâŸ¨âˆ‡f(W^t)H_A^t, âˆ‡f(W^t)H_A^tâŸ©_F\n= Tr((H_A^t)^\\top (âˆ‡f(W^t))^\\top âˆ‡f(W^t)H_A^t)\n= Tr((âˆ‡f(W^t))^\\top âˆ‡f(W^t)H_A^t (H_A^t)^\\top)\n= Tr((âˆ‡f(W^t))^\\top âˆ‡f(W^t)H_A^t)\n= âŸ¨âˆ‡f(W^t), âˆ‡f(W^t)H_A^tâŸ©_F. (37)",
      "index": 66,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Therefore:\n-Î³(1-p) âŸ¨âˆ‡f(W^t), âˆ‡f(W^t)H_A^tâŸ©_F + (LÎ³^2/2)(1-p) â€–âˆ‡f(W^t)H_A^tâ€–_F^2\n= -Î³(1-p) âŸ¨âˆ‡f(W^t), âˆ‡f(W^t)H_A^tâŸ©_F + (LÎ³^2/2)(1-p) âŸ¨âˆ‡f(W^t)H_A^t, âˆ‡f(W^t)H_A^tâŸ©_F\n= -Î³(1-p) âŸ¨âˆ‡f(W^t), âˆ‡f(W^t)H_A^tâŸ©_F + (LÎ³^2/2)(1-p) âŸ¨âˆ‡f(W^t), âˆ‡f(W^t)H_A^tâŸ©_F\nÎ³ â‰¤ 1/L\nâ‰¤ -Î³/2 (1-p) âŸ¨âˆ‡f(W^t), âˆ‡f(W^t)H_A^tâŸ©_F. (38)",
      "index": 67,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Using the Lipschitz gradient condition and the above bounds:\nE[f(W^{t+1}) | W^t, H^t] â‰¤ f(W^t) + E[âŸ¨âˆ‡f(W^t), W^{t+1} - W^tâŸ©_F | W^t, H^t]\n+ (L/2) E[â€–W^{t+1} - W^tâ€–_F^2 | W^t, H^t]\n= f(W^t) - Î³ âŸ¨âˆ‡f(W^t), E[G^t | W^t, H^t]âŸ©_F + (LÎ³^2/2) E[â€–G^tâ€–_F^2 | W^t, H^t]\n= f(W^t) - Î³p âŸ¨âˆ‡f(W^t), H_B^t âˆ‡f(W^t)âŸ©_F - Î³(1-p) âŸ¨âˆ‡f(W^t), âˆ‡f(W^t)H_A^tâŸ©_F\n+ (LÎ³^2/2)p â€–H_B^t âˆ‡f(W^t)â€–_F^2 + (LÎ³^2/2)(1-p) â€–âˆ‡f(W^t)H_A^tâ€–_F^2\n(36),(38)\nâ‰¤ f(W^t) - Î³/2 ( p âŸ¨âˆ‡f(W^t), H_B^t âˆ‡f(W^t)âŸ©_F + (1-p) âŸ¨âˆ‡f(W^t), âˆ‡f(W^t)H_A^tâŸ©_F ). (39)",
      "index": 68,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "For the first term:\n-âŸ¨âˆ‡f(W^t), E[H_B^t] âˆ‡f(W^t)âŸ©_F\n= -Tr((âˆ‡f(W^t))^\\top E[H_B^t] âˆ‡f(W^t))\nâ‰¤ -Î»_min(E[H_B^t]) Tr((âˆ‡f(W^t))^\\top âˆ‡f(W^t))\n= -Î»_min^{H_B} â€–âˆ‡f(W^t)â€–_F^2. (40)",
      "index": 69,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Similarly, for the second term:\n-âŸ¨âˆ‡f(W^t), âˆ‡f(W^t) E[H_A^t]âŸ©_F\n= -Tr((âˆ‡f(W^t))^\\top âˆ‡f(W^t) E[H_A^t])\n= -Tr(E[H_A^t] (âˆ‡f(W^t))^\\top âˆ‡f(W^t))\nâ‰¤ -Î»_min^{H_A} â€–âˆ‡f(W^t)â€–_F^2. (41)",
      "index": 70,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Therefore:\n\n$$\n\\mathbb{E}\\left[f(W^{t+1}) \\mid W^t\\right]\n= \\mathbb{E}\\left[\\mathbb{E}\\left[f(W^{t+1}) \\mid W^t, H^t\\right] \\mid W^t\\right]\n$$\n\n$$\n\\leq f(W^t) - \\frac{\\gamma}{2}\\left(p\\left\\langle\\nabla f(W^t),\\, \\mathbb{E}[H_B]\\,\\nabla f(W^t)\\right\\rangle_F + (1-p)\\left\\langle\\nabla f(W^t),\\, \\nabla f(W^t)\\,\\mathbb{E}[H_A]\\right\\rangle_F\\right)\n$$\n\n$$\n\\leq f(W^t) - \\frac{\\gamma}{2}\\left(p\\,\\lambda_{\\min}^{H_B} + (1-p)\\,\\lambda_{\\min}^{H_A}\\right)\\|\\nabla f(W^t)\\|_F^2\n$$\n\n$$\n= f(W^t) - \\frac{\\gamma}{2}\\,\\lambda_{\\min}^p\\,\\|\\nabla f(W^t)\\|_F^2,\n$$\n(42)\n\nwhere $\\lambda_{\\min}^p := p\\,\\lambda_{\\min}^{H_B} + (1-p)\\,\\lambda_{\\min}^{H_A}$. Further,\n\n$$\n\\mathbb{E}\\left[\\mathbb{E}\\left[f(W^{t+1}) \\mid W^t, H^t\\right] \\mid W^t\\right] - f^\\star\n\\leq f(W^t) - f^\\star - \\frac{\\gamma}{2}\\,\\lambda_{\\min}^p\\,\\|\\nabla f(W^t)\\|_F^2.\n$$\n(43)\n\nTaking the sum over $t = 0, \\ldots, T-1$ and using the tower property of expectation:\n\n$$\n\\mathbb{E}\\left[f(W^T) - f^\\star\\right] \\leq f(W^0) - f^\\star - \\frac{\\gamma}{2}\\,\\lambda_{\\min}^p \\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\|\\nabla f(W^t)\\|_F^2\\right].\n$$\n(44)\n\nBy rearranging terms, we get:\n\n$$\n\\frac{\\gamma}{2}\\,\\lambda_{\\min}^p \\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\|\\nabla f(W^t)\\|_F^2\\right] \\leq f(W^0) - f^\\star.\n$$\n(45)\n\nFinally, dividing both sides by $\\frac{\\gamma T}{2}\\,\\lambda_{\\min}^p$ yields:\n\n$$\n\\mathbb{E}\\left[\\left\\|\\nabla f(\\widetilde{W}^T)\\right\\|_F^2\\right] \\leq \\frac{2\\big(f(W^0) - f^\\star\\big)}{\\gamma\\,\\lambda_{\\min}^p\\,T},\n$$\n(46)\n\nwhere $\\widetilde{W}^T$ is chosen uniformly at random from $\\{W^0, W^1, \\ldots, W^{T-1}\\}$, completing the proof.\n\n$\\Box$",
      "index": 71,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "C.1.2 Convergence under Polyak-Åojasiewicz Condition\n\nTheorem 9. Let Assumptions 1, 2, 3, and 6 hold, and let the stepsize satisfy $0 < \\gamma \\leq \\frac{1}{L}$. Then the iterates of Bernoulli-LoRA-GD (Algorithm 2), with matrices $\\hat{A}^t$ and $\\hat{B}^t$ computed according to Lemma 3, satisfy\n$$\n\\mathbb{E}\\left[f(W^T) - f^\\star\\right] \\leq \\left(1 - \\gamma\\mu\\,\\lambda_{\\min}^p\\right)^T\\left(f(W^0) - f^\\star\\right),\n$$\nwhere $\\lambda_{\\min}^p := p\\,\\lambda_{\\min}^{H_B} + (1-p)\\,\\lambda_{\\min}^{H_A}$.\n\nProof. We begin our analysis from a key inequality derived in the proof of Theorem 1:\n$$\n\\mathbb{E}\\left[f(W^{t+1}) \\mid W^t\\right] \\leq f(W^t) - \\frac{\\gamma}{2}\\,\\lambda_{\\min}^p\\,\\|\\nabla f(W^t)\\|_F^2.\n$$\n(47)\n\nBy invoking the Polyak-Åojasiewicz condition (Assumption 6), which states that $\\frac{1}{2}\\|\\nabla f(W)\\|_F^2 \\geq \\mu\\big(f(W) - f^\\star\\big)$, we can further bound the right-hand side of (47):\n$$\n\\mathbb{E}\\left[f(W^{t+1}) \\mid W^t\\right] \\leq f(W^t) - \\gamma\\,\\lambda_{\\min}^p\\left(\\mu\\left(f(W^t) - f^\\star\\right)\\right).\n$$\n\nTherefore,\n$$\n\\mathbb{E}\\left[f(W^T) - f^\\star\\right] \\leq \\left(1 - \\gamma\\mu\\,\\lambda_{\\min}^p\\right)^T\\left(f(W^0) - f^\\star\\right).\n$$\n$\\Box$",
      "index": 72,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Subtracting the optimal function value f* from both sides, we get a recursive relationship for the expected suboptimality gap:\nE[f(W^{t+1}) âˆ’ f* | W^t] â‰¤ (f(W^t) âˆ’ f*) âˆ’ Î³Î¼Î»_min^p (f(W^t) âˆ’ f*)\n= (1 âˆ’ Î³Î¼Î»_min^p) (f(W^t) âˆ’ f*).\n\nBy taking the full expectation over all randomness up to iteration t and applying the tower property, we obtain:\nE[f(W^{t+1}) âˆ’ f*] â‰¤ (1 âˆ’ Î³Î¼Î»_min^p) E[f(W^t) âˆ’ f*].\n\nUnrolling this recursion from t = T âˆ’ 1 down to t = 0 yields the final linear convergence result:\nE[f(W^T) âˆ’ f*] â‰¤ (1 âˆ’ Î³Î¼Î»_min^p)^T (f(W^0) âˆ’ f*).\n\nThis completes the proof. â–¡",
      "index": 73,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "C.1.3 Convergence for Non-Smooth Convex Functions\n\nAlgorithm 3 Bernoulli-LoRA-GD (Non-smooth setting)\n\n1: Parameters: pre-trained model W^0 âˆˆ R^{mÃ—n}, rank r â‰ª min{m,n}, scaling factor Î± > 0, stepsize Î³_t, chain length T, sketch distribution D_S^B or D_S^A, Bernoulli probability p.\n2: for t = 0, 1, â€¦, T âˆ’ 1 do\n3:     Sample c^t âˆ¼ Be(p) (Bernoulli random variable)\n4:     If c^t = 1 then (Right sketch)\n5:         Sample A_S^t âˆ¼ D_S^A\n6:         BÌ‚^t = arg min_B { f(W^t) + (Î±/r) âŸ¨âˆ‚f(W^t), B A_S^tâŸ©_F + (Î±^2/(2 Î³_t r^2)) ||B A_S^t||_F^2 }\n7:         W^{t+1} = W^t + (Î±/r) BÌ‚^t A_S^t\n8:     else (Left sketch)\n9:         Sample B_S^t âˆ¼ D_S^B\n10:        Ã‚^t = arg min_A { f(W^t) + (Î±/r) âŸ¨âˆ‚f(W^t), B_S^t AâŸ©_F + (Î±^2/(2 Î³_t r^2)) ||B_S^t A||_F^2 }\n11:        W^{t+1} = W^t + (Î±/r) B_S^t Ã‚^t\n12: end for\n\nOur analysis relies on the following standard assumptions that are widely used in non-smooth optimization theory:\n\nAssumption 7. The function f has at least one minimizer, denoted by W*.\n\nAssumption 8. The function f is convex.\n\nAssumption 9 (Lipschitz continuity). The function f is L0-Lipschitz continuous. That is, there exists L0 > 0 such that\n|f(W) âˆ’ f(V)| â‰¤ L0 ||W âˆ’ V||_F, âˆ€ W, V âˆˆ R^{mÃ—n}. (48)\n\nThe combination of convexity and Lipschitz continuity represents a standard framework in non-smooth optimization [Vorontsova et al., 2021, Nesterov, 2013, Bubeck, 2015, Beck, 2017, Duchi, 2018].",
      "index": 74,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "[Lan, 2020, Drusvyatskiy, 2020]. Notably, the L_0-Lipschitz continuity implies uniformly bounded subgradients [Beck, 2017], a property that plays a crucial role in our analysis:\n\n||âˆ‚f(W)||_F â‰¤ L_0, âˆ€ W âˆˆ â„^{mÃ—n}. (49)\n\nThis boundedness of subgradients ensures the stability of our optimization process and enables us to establish rigorous convergence guarantees.",
      "index": 75,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "The following lemma establishes that the Bernoulli-LoRA update in the non-smooth case can also be reformulated as a subgradient descent step, which plays a central role in our convergence analysis for non-smooth objectives.\n\nLemma 4. Consider the updates Ã‚^t and BÌ‚^t from Algorithm 3 computed as solutions to the following optimization problems:\n\nÃ‚^t := arg min_A { f(W^t) + (Î±/r) âŸ¨âˆ‚f(W^t), B_S^t AâŸ©_F + (Î±^2/(2Î³_t r^2)) ||B_S^t A||_F^2 },\n\nBÌ‚^t := arg min_B { f(W^t) + (Î±/r) âŸ¨âˆ‚f(W^t), B A_S^tâŸ©_F + (Î±^2/(2Î³_t r^2)) ||B A_S^t||_F^2 }. (50)\n\nThen the Left and Right sketch updates can be expressed as a subgradient descent step:\n\nW^{t+1} = W^t âˆ’ Î³_t G^t. (51)\n\nwhere G^t is defined by\n\nG^t = { H_B^t âˆ‚f(W^t), with probability p\n        âˆ‚f(W^t) H_A^t, with probability 1 âˆ’ p } (52)\n\nwith projection matrices H_A^t and H_B^t given by:\n\nH_A^t := (A_S^t)^âŠ¤ (A_S^t (A_S^t)^âŠ¤)^â€  A_S^t  and  H_B^t := B_S^t ((B_S^t)^âŠ¤ B_S^t)^â€  (B_S^t)^âŠ¤, (53)\n\nwhere â€  denotes the Moore-Penrose pseudoinverse.",
      "index": 76,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Proof. The proof follows a similar structure to that of Lemma 3, with subgradients replacing gradients throughout the analysis. We examine both sketch types separately before combining them into a unified update rule.\n\nLeft Sketch Analysis. When the Left sketch is selected, the update takes the form:\n\nW^{t+1} = W^t + (Î±/r) B_S^t Ã‚^t. (54)\n\nThe matrix Ã‚^t is defined as the solution to the optimization problem:\n\nÃ‚^t := arg min_A { f(W^t) + (Î±/r) âŸ¨âˆ‚f(W^t), B_S^t AâŸ©_F + (Î±^2/(2Î³_t r^2)) ||B_S^t A||_F^2 }. (55)\n\nBy computing the gradient of the objective with respect to A and setting it to zero, we obtain:\n\n(Î±/r) (B_S^t)^âŠ¤ âˆ‚f(W^t) + (Î±^2/(Î³_t r^2)) (B_S^t)^âŠ¤ B_S^t Ã‚^t = 0;\n\nÃ‚^t = âˆ’ (Î³_t r/Î±) ((B_S^t)^âŠ¤ B_S^t)^â€  (B_S^t)^âŠ¤ âˆ‚f(W^t). (56)",
      "index": 77,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Substituting this expression back into the update equation yields the Left Sketch update:\n\n$$W^{t+1} = W^t + \\frac{\\alpha}{r} B_S^t \\hat{A}^t$$\n\n$$= W^t - \\gamma_t B_S^t \\left((B_S^t)^\\top B_S^t\\right)^\\dagger (B_S^t)^\\top \\partial f(W^t)$$\n\n$$= W^t - \\gamma_t H_B^t \\partial f(W^t).$$\n(57)",
      "index": 78,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Right Sketch Analysis. For the Right sketch, we follow an analogous approach. The update rule takes the form:\n\n$$W^{t+1} = W^t + \\frac{\\alpha}{r} \\hat{B}^t A_S^t.$$\n(58)\n\nApplying similar optimization steps but now with respect to matrix $B$, we obtain:\n\n$$\\hat{B}^t = -\\frac{\\gamma_t r}{\\alpha} \\partial f(W^t) (A_S^t)^\\top \\left(A_S^t (A_S^t)^\\top\\right)^\\dagger,$$\n(59)\n\nwhich leads to the Right sketch update:\n\n$$W^{t+1} = W^t + \\frac{\\alpha}{r} \\hat{B}^t A_S^t$$\n\n$$= W^t - \\gamma_t \\partial f(W^t) (A_S^t)^\\top \\left(A_S^t (A_S^t)^\\top\\right)^\\dagger A_S^t$$\n\n$$= W^t - \\gamma_t \\partial f(W^t) H_A^t.$$\n(60)",
      "index": 79,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Combined Update Rule. By combining equations (57) and (60), we arrive at the unified update rule:\n\n$$W^{t+1} = W^t - \\gamma_t G^t,$$\n(61)\n\nwhere $G^t$ takes the form specified in the lemma statement, thus completing the proof. â–¡",
      "index": 80,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Assumption 10. Consider a projection matrix $H$ generated through either Left Sketch (Definition 1) or Right Sketch (Definition 2). For the sampling distributions $\\mathcal{D}_S^B$ and $\\mathcal{D}_S^A$, the expected projection matrix $H$ satisfies\n\n$$\\mathbb{E}[H] = \\alpha I,$$\n(62)\n\nwhere $\\alpha$ is a constant > 0.",
      "index": 81,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Theorem 10. Let Assumptions 1, 7, 8, 9, and 10 hold. Let us define the following quantities: $\\overline{W}^T := \\frac{1}{T} \\sum_{t=0}^{T-1} W^t$ as the averaged iterate, $R_0^2 := \\|W^0 - W^*\\|_F^2$ as the initial distance to optimum. Consider the sequence $\\{W^t\\}$ produced by Bernoulli-LoRA (Algorithm 3) with updates of $\\hat{A}^t$ and $\\hat{B}^t$ computed according to Lemma 4.\n\n1. (Constant stepsize). If the stepsize is constant, i.e., $\\gamma_t := \\gamma > 0$, then\n\n$$\\mathbb{E}\\left[f(\\overline{W}^T) - f(W^*)\\right] \\leq \\frac{R_0^2}{2\\gamma \\alpha T} + \\frac{\\gamma L_0^2}{2}.$$\n(63)\n\nMoreover, with the optimal stepsize $\\gamma_* = \\sqrt{\\frac{R_0^2}{T \\alpha L_0^2}}$, we obtain:\n\n$$\\mathbb{E}\\left[f(\\overline{W}^T) - f(W^*)\\right] \\leq \\frac{R_0 L_0}{\\sqrt{\\alpha T}}.$$\n(64)",
      "index": 82,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "2. (Polyak stepsize). If the stepsize is chosen adaptively as\n$$\\gamma_t = \\frac{f(W^t) - f(W^*)}{\\|\\partial f(W^t)\\|_F^2}, \\tag{65}$$\nthen\n$$\\mathbb{E}\\left[f(\\overline{W}^T) - f(W^*)\\right] \\le \\frac{R^0 L_0}{\\sqrt{\\alpha T}}. \\tag{66}$$",
      "index": 83,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Proof. From Lemma 4, we know that Bernoulli-LoRA updates in the non-smooth setting can be expressed as\n$$W^{t+1} = W^t - \\gamma_t G^t, \\tag{67}$$\nwhere $G^t$ takes the form\n$$\nG^t = \\begin{cases}\nH_B^t \\partial f(W^t), & \\text{with probability } p,\\\\\n\\partial f(W^t) H_A^t, & \\text{with probability } 1-p,\n\\end{cases} \\tag{68}\n$$\nwith projection matrices $H_A^t$ and $H_B^t$ as defined in the lemma.\n\nTo analyze the convergence, we first compute the conditional expectation and second moment of $G^t$:\n$$\\mathbb{E}\\left[G^t \\mid W^t, H^t\\right] = p\\, H_B^t \\partial f(W^t) + (1-p)\\, \\partial f(W^t) H_A^t, \\tag{69}$$\n$$\\mathbb{E}\\left[\\|G^t\\|_F^2 \\mid W^t, H^t\\right] = p \\left\\|H_B^t \\partial f(W^t)\\right\\|_F^2 + (1-p) \\left\\|\\partial f(W^t) H_A^t\\right\\|_F^2, \\tag{70}$$\nwhere we defined $H^t := \\{H_A^t, H_B^t\\}$.\n\nBy the definition of subgradient, we have:\n$$f(W^*) \\ge f(W^t) + \\langle \\partial f(W^t), W^* - W^t \\rangle_F, \\tag{71}$$\nwhich implies:\n$$\\langle \\partial f(W^t), W^t - W^* \\rangle_F \\ge f(W^t) - f(W^*). \\tag{72}$$\n\nLet us establish key auxiliary bounds. First, for the inner product terms:\n\\begin{align}\n-2\\gamma_t \\mathbb{E}\\left[\\langle G^t, W^t - W^* \\rangle_F \\mid W^t, H^t\\right]\n&\\stackrel{(69)}{=} -2\\gamma_t p \\langle H_B^t \\partial f(W^t), W^t - W^* \\rangle_F \\\\\n&\\quad - 2\\gamma_t(1-p) \\langle \\partial f(W^t) H_A^t, W^t - W^* \\rangle_F. \\tag{73}\n\\end{align}\n\nFor projection matrices, we have the following properties:\n\\begin{align}\n\\|\\partial f(W^t) H_A^t\\|_F^2\n&= \\langle \\partial f(W^t) H_A^t, \\partial f(W^t) H_A^t \\rangle_F \\\\\n&= \\mathrm{Tr}\\!\\left((H_A^t)^\\top (\\partial f(W^t))^\\top \\partial f(W^t) H_A^t\\right) \\\\\n&= \\mathrm{Tr}\\!\\left((\\partial f(W^t))^\\top \\partial f(W^t) H_A^t (H_A^t)^\\top\\right) \\\\\n&= \\mathrm{Tr}\\!\\left((\\partial f(W^t))^\\top \\partial f(W^t) H_A^t\\right) \\\\\n&= \\langle \\partial f(W^t), \\partial f(W^t) H_A^t \\rangle_F, \\tag{74}\n\\end{align}\nand similarly, one can show that\n$$\\|H_B^t \\partial f(W^t)\\|_F^2 = \\langle \\partial f(W^t), H_B^t \\partial f(W^t) \\rangle_F. \\tag{75}$$",
      "index": 84,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "This allows us to express the second moment term as:\n$$\\gamma_t^2 \\mathbb{E}\\left[\\left\\|G^t\\right\\|_{\\mathrm{F}}^2 \\mid W^t, H^t\\right] \\stackrel{(70)}{=} \\gamma_t^2 p \\left\\|H_B^t \\partial f(W^t)\\right\\|_{\\mathrm{F}}^2 + \\gamma_t^2(1-p) \\left\\|\\partial f(W^t) H_A^t\\right\\|_{\\mathrm{F}}^2$$\n$$\\stackrel{(74),(75)}{=} \\gamma_t^2 p \\left\\langle \\partial f(W^t), H_B^t \\partial f(W^t)\\right\\rangle_{\\mathrm{F}} + \\gamma_t^2(1-p) \\left\\langle \\partial f(W^t), \\partial f(W^t) H_A^t\\right\\rangle_{\\mathrm{F}}.$$\n(76)\n\nCombining these bounds, we can analyze the distance to the optimal solution:\n$$\\mathbb{E}\\left[\\left\\|W^{t+1} - W^*\\right\\|_{\\mathrm{F}}^2 \\mid W^t, H^t\\right] = \\mathbb{E}\\left[\\left\\|W^t - \\gamma_t G^t - W^*\\right\\|_{\\mathrm{F}}^2 \\mid W^t, H^t\\right]$$\n$$= \\left\\|W^t - W^*\\right\\|_{\\mathrm{F}}^2 - 2\\gamma_t \\mathbb{E}\\left[\\left\\langle G^t, W^t - W^*\\right\\rangle_{\\mathrm{F}} \\mid W^t, H^t\\right]$$\n$$+ \\gamma_t^2 \\mathbb{E}\\left[\\left\\|G^t\\right\\|_{\\mathrm{F}}^2 \\mid W^t, H^t\\right]$$\n$$\\stackrel{(73),(76)}{=} \\left\\|W^t - W^*\\right\\|_{\\mathrm{F}}^2 - 2\\gamma_t p \\left\\langle H_B^t \\partial f(W^t), W^t - W^*\\right\\rangle_{\\mathrm{F}}$$\n$$- 2\\gamma_t(1-p) \\left\\langle \\partial f(W^t) H_A^t, W^t - W^*\\right\\rangle_{\\mathrm{F}} + \\gamma_t^2 p \\left\\langle \\partial f(W^t), H_B^t \\partial f(W^t)\\right\\rangle_{\\mathrm{F}}$$\n$$+ \\gamma_t^2(1-p) \\left\\langle \\partial f(W^t), \\partial f(W^t) H_A^t\\right\\rangle_{\\mathrm{F}}.$$\n(77)",
      "index": 85,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "For the expected projection matrices (see Assumption 10), we have:\n$$\\left\\langle \\partial f(W^t), \\mathbb{E}\\left[H_B^t\\right] \\partial f(W^t)\\right\\rangle_{\\mathrm{F}} = \\mathrm{Tr}\\left(\\left(\\partial f(W^t)\\right)^\\top \\mathbb{E}\\left[H_B^t\\right] \\partial f(W^t)\\right)$$\n$$= \\alpha \\mathrm{Tr}\\left(\\left(\\partial f(W^t)\\right)^\\top \\partial f(W^t)\\right)$$\n$$= \\alpha \\left\\|\\partial f(W^t)\\right\\|_{\\mathrm{F}}^2,$$\n(78)\nand similarly,\n$$\\left\\langle \\partial f(W^t), \\partial f(W^t) \\mathbb{E}\\left[H_A^t\\right]\\right\\rangle_{\\mathrm{F}} = \\alpha \\left\\|\\partial f(W^t)\\right\\|_{\\mathrm{F}}^2.$$\n(79)\n\nTaking expectation of both sides of (77) again, we get\n$$\\mathbb{E}\\left[\\left\\|W^{t+1} - W^*\\right\\|_{\\mathrm{F}}^2 \\mid W^t\\right] = \\mathbb{E}\\left[\\mathbb{E}\\left[\\left\\|W^{t+1} - W^*\\right\\|_{\\mathrm{F}}^2 \\mid W^t, H^t\\right] \\mid W^t\\right]$$\n(80)\n$$= \\left\\|W^t - W^*\\right\\|_{\\mathrm{F}}^2 - 2\\gamma_t p \\left\\langle \\mathbb{E}\\left[H_B^t\\right] \\partial f\\left(W^t\\right), W^t - W^*\\right\\rangle_{\\mathrm{F}}$$\n(81)\n$$- 2\\gamma_t(1-p) \\left\\langle \\partial f\\left(W^t\\right) \\mathbb{E}\\left[H_A^t\\right], W^t - W^*\\right\\rangle_{\\mathrm{F}}$$\n$$+ \\gamma_t^2 p \\left\\langle \\partial f\\left(W^t\\right), \\mathbb{E}\\left[H_B^t\\right] \\partial f\\left(W^t\\right)\\right\\rangle_{\\mathrm{F}} + \\gamma_t^2(1-p) \\left\\langle \\partial f\\left(W^t\\right), \\partial f\\left(W^t\\right) \\mathbb{E}\\left[H_A^t\\right]\\right\\rangle_{\\mathrm{F}}$$\n$$\\stackrel{(78),(79)}{=} \\left\\|W^t - W^*\\right\\|_{\\mathrm{F}}^2 - 2\\gamma_t p \\alpha \\left\\langle \\partial f\\left(W^t\\right), W^t - W^*\\right\\rangle_{\\mathrm{F}}$$\n(82)\n$$- 2\\gamma_t(1-p)\\alpha \\left\\langle \\partial f\\left(W^t\\right), W^t - W^*\\right\\rangle_{\\mathrm{F}} + \\gamma_t^2 \\alpha \\left\\|\\partial f\\left(W^t\\right)\\right\\|_{\\mathrm{F}}^2$$\n$$= \\left\\|W^t - W^*\\right\\|_{\\mathrm{F}}^2 - 2\\gamma_t \\alpha \\left\\langle \\partial f\\left(W^t\\right), W^t - W^*\\right\\rangle_{\\mathrm{F}} + \\gamma_t^2 \\alpha \\left\\|\\partial f\\left(W^t\\right)\\right\\|_{\\mathrm{F}}^2$$\n$$\\stackrel{(72)}{=} \\left\\|W^t - W^*\\right\\|_{\\mathrm{F}}^2 - 2\\gamma_t \\alpha \\left(f(W^t) - f(W^*)\\right) + \\gamma_t^2 \\alpha \\left\\|\\partial f\\left(W^t\\right)\\right\\|_{\\mathrm{F}}^2.$$\n(83)\n\nBy Assumption 9, subgradients are uniformly bounded (see [Beck, 2017]):\n$$\\|\\partial f(W)\\|_{\\mathrm{F}} \\leq L_0 \\quad \\forall W \\in \\mathbb{R}^{m \\times n}.$$\n(84)",
      "index": 86,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Now we analyze both stepsize strategies separately.\n\n1. (Constant stepsize). Let us first consider using a fixed stepsize $\\gamma_t := \\gamma > 0$. Taking expectation of both sides of (80) again, applying tower property (13) and using the bound (84), we obtain:\n$$\\mathbb{E}\\left[\\left\\|W^{t+1} - W^*\\right\\|_{\\mathrm{F}}^2\\right] \\leq \\mathbb{E}\\left[\\left\\|W^t - W^*\\right\\|_{\\mathrm{F}}^2\\right] - 2\\gamma \\alpha \\mathbb{E}\\left[f(W^t) - f(W^*)\\right] + \\gamma^2 \\alpha L_0^2.$$\n(85)",
      "index": 87,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Rearranging terms in (85):\n\n$$2\\gamma\\alpha\\,\\mathbb{E}\\!\\left[f(W^t)-f(W^*)\\right]\\leq \\mathbb{E}\\!\\left[\\|W^t-W^*\\|_F^2\\right]-\\mathbb{E}\\!\\left[\\|W^{t+1}-W^*\\|_F^2\\right]+\\gamma^2\\alpha L_0^2.$$\n(86)\n\nSumming inequality (86) for $t=0,\\ldots,T-1$:\n\n$$2\\gamma\\alpha \\sum_{t=0}^{T-1}\\mathbb{E}\\!\\left[f(W^t)-f(W^*)\\right]\\leq \\sum_{t=0}^{T-1}\\left(\\mathbb{E}\\!\\left[\\|W^t-W^*\\|_F^2\\right]-\\mathbb{E}\\!\\left[\\|W^{t+1}-W^*\\|_F^2\\right]\\right)+T\\gamma^2\\alpha L_0^2$$\n$$=\\mathbb{E}\\!\\left[\\|W^0-W^*\\|_F^2\\right]-\\mathbb{E}\\!\\left[\\|W^T-W^*\\|_F^2\\right]+T\\gamma^2\\alpha L_0^2$$\n$$\\leq \\|W^0-W^*\\|_F^2+T\\gamma^2\\alpha L_0^2,$$\n(87)\n\nwhere the last inequality follows from the non-negativity of $\\|W^T-W^*\\|_F^2$.\n\nFor the averaged iterate $\\overline{W}^{\\,T}:=\\frac{1}{T}\\sum_{t=0}^{T-1}W^t$, by convexity of $f$ we have:\n\n$$\\mathbb{E}\\!\\left[f(\\overline{W}^{\\,T})-f(W^*)\\right]\\leq \\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\!\\left[f(W^t)-f(W^*)\\right]$$\n$$\\stackrel{(87)}{\\leq}\\frac{\\|W^0-W^*\\|_F^2}{2\\gamma\\alpha T}+\\frac{\\gamma L_0^2}{2}$$\n$$=\\frac{(R^0)^2}{2\\gamma\\alpha T}+\\frac{\\gamma L_0^2}{2},$$\n(88)\n\nwhere we denoted $(R^0)^2:=\\|W^0-W^*\\|_F^2$.\n\nTo optimize this bound, we minimize it with respect to $\\gamma$. The optimal stepsize $\\gamma_*$ solves:\n\n$$\\gamma_*=\\arg\\min_{\\gamma>0}\\left(\\frac{(R^0)^2}{2\\gamma\\alpha T}+\\frac{\\gamma L_0^2}{2}\\right)\n=\\sqrt{\\frac{(R^0)^2}{T\\alpha L_0^2}}.$$\n(89)\n\nSubstituting $\\gamma_*$ back into (88), we obtain the optimal convergence rate:\n\n$$\\mathbb{E}\\!\\left[f(\\overline{W}^{\\,T})-f(W^*)\\right]\\leq \\frac{R^0L_0}{\\sqrt{\\alpha T}}.$$\n(90)",
      "index": 88,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "2. (Polyak stepsize). For this strategy, we choose the stepsize adaptively based on the current function value:\n\n$$\\gamma_t=\\arg\\min_{\\gamma>0}\\left\\{\\|W^t-W^*\\|_F^2-2\\gamma\\alpha\\big(f(W^t)-f(W^*)\\big)+\\gamma^2\\alpha\\|\\partial f(W^t)\\|_F^2\\right\\}\n=\\frac{f(W^t)-f(W^*)}{\\|\\partial f(W^t)\\|_F^2}.$$\n(91)\n\nSubstituting this stepsize into inequality (80):\n\n$$\\mathbb{E}\\!\\left[\\|W^{t+1}-W^*\\|_F^2\\,\\big|\\,W^t\\right]=\\mathbb{E}\\!\\left[\\mathbb{E}\\!\\left[\\|W^{t+1}-W^*\\|_F^2\\,\\big|\\,W^t,H^t\\right]\\big|\\,W^t\\right]$$\n$$\\leq \\|W^t-W^*\\|_F^2-2\\gamma_t\\alpha\\big(f(W^t)-f(W^*)\\big)+\\gamma_t^2\\alpha\\|\\partial f(W^t)\\|_F^2$$\n$$\\stackrel{(91)}{=}\\|W^t-W^*\\|_F^2-\\frac{\\alpha\\big(f(W^t)-f(W^*)\\big)^2}{\\|\\partial f(W^t)\\|_F^2}$$\n$$\\stackrel{(84)}{\\leq}\\|W^t-W^*\\|_F^2-\\frac{\\alpha\\big(f(W^t)-f(W^*)\\big)^2}{L_0^2}.$$\n(92)",
      "index": 89,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Taking expectation of both sides of (92) again and applying the tower property\n\n$$\\mathbb{E}\\big[\\|W^{t+1}-W^*\\|_{\\mathrm{F}}^2\\big]\\leq \\mathbb{E}\\big[\\|W^t-W^*\\|_{\\mathrm{F}}^2\\big]-\\frac{\\alpha\\,\\mathbb{E}\\big[(f(W^t)-f(W^*))^2\\big]}{L_0^2} \\tag{93}$$\n\nSince $f$ is convex, by Jensenâ€™s inequality (16) and the Cauchyâ€“Bunyakovskyâ€“Schwarz inequality (14) with $X:=f(W^t)-f(W^*)$ and $Y:=1$, we have\n\n$$\\mathbb{E}\\big[f(\\overline{W}^T)-f(W^*)\\big]\\stackrel{(16)}{\\leq}\\mathbb{E}\\left[\\frac{1}{T}\\sum_{t=0}^{T-1} f(W^t)-f(W^*)\\right]$$\n$$\\leq \\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\big[f(W^t)-f(W^*)\\big]$$\n$$\\stackrel{(14)}{\\leq}\\frac{1}{T}\\sum_{t=0}^{T-1}\\sqrt{\\mathbb{E}\\big[(f(W^t)-f(W^*))^2\\big]}$$\n$$\\leq \\sqrt{\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\big[(f(W^t)-f(W^*))^2\\big]}$$\n$$\\stackrel{(93)}{\\leq}\\frac{R^0 L_0}{\\sqrt{\\alpha T}}, \\tag{94}$$\n\nwhich matches the optimal rate achieved by the constant stepsize strategy with optimal tuning. â–¡",
      "index": 90,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "C.2 Analysis of Bernoulli-LoRA-SGD\n\nAlgorithm 4 Bernoulli-LoRA-SGD\n\n1: Parameters: pre-trained model W^0 âˆˆ R^{mÃ—n}, rank r â‰ª min{m,n}, scaling factor Î± > 0, chain length T, sketch distribution ð’Ÿ_S^B or ð’Ÿ_S^A, Bernoulli probability p\n2: for t = 0, 1, ..., T - 1 do\n3: Sample c^t âˆ¼ Be(p)\n\nEarlier findings were derived utilizing full gradient computations. Nonetheless, this method proves impractical in deep learning applications, where obtaining full gradients is rarely feasible. Our focus moves to a framework that employs Stochastic Gradient Descent (SGD) while incorporating a more flexible and generalized data sampling strategy, enabling greater adaptability in the selection and utilization of data throughout the training process. General sampling techniques for strongly convex functions have been thoroughly examined in [Gower et al., 2019]. For broader convex optimization problems, Khaled et al. [2023] provide a comprehensive study of how SGD performs under different sampling strategies. In non-convex scenarios, the works of Khaled and RichtÃ¡rik [2023] and [Demidovich et al., 2023b] investigate the effects of generalized sampling methods on SGD's convergence and efficiency, offering valuable insights into its adaptability for diverse machine learning applications. In this section we focus on Bernoulli-LoRA-SGD, a method designed in the scope of the Bernoulli-LoRA framework, based on the classical SGD algorithm.\n\nFor convergence analysis, we notice the gradient step in Algorithm 4 is equivalent to the following update\nW^{t+1} = W^t - Î³ Äœ^t, where Äœ^t = { H_B^t G^t, with probability p; G^t H_A^t, with probability 1 - p }, (95)\nwhere G^t = g(W^t) is an unbiased stochastic gradient, which satisfies Assumption 4.\n\nTheorem 11. Let Assumptions 2, 3, and 4 hold, and stepsize satisfies\n0 < Î³ â‰¤ min { 1 / âˆš(L A_1 Î»_max^p T), 1 / (L B_1) (Î»_max^p / Î»_min^p)^{-1} }.\nThen iterates generated by Bernoulli-LoRA-SGD (Algorithm 4) satisfy\nE[ ||âˆ‡f(áº†^T)||_F^2 ] â‰¤ 6 ( f(W^0) - f* ) / ( Î³ Î»_min^p T ) + Î³ L C_1 Î»_max^p / Î»_min^p.",
      "index": 91,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "where $\\lambda_{\\min}^p := p\\lambda_{\\min}^{H_B} + (1-p)\\lambda_{\\min}^{H_A}$, $\\lambda_{\\max}^p := p\\lambda_{\\max}^{H_B} + (1-p)\\lambda_{\\max}^{H_A}$, and $\\widetilde{W}^T$ is chosen at random from $\\{W^0, W^1, \\ldots, W^{T-1}\\}$ with probabilities $\\left\\{\\frac{w_t}{\\mathcal{W}_{T-1}}\\right\\}_{t=0}^{T-1}$, where $w_t = \\frac{w_{t-1}}{(1+\\gamma^2 L A_1 \\lambda_{\\max}^p)}$, $\\mathcal{W}_{T-1} = \\sum_{t=0}^{T-1} w_t$, and $w_{-1} > 0$.",
      "index": 92,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Proof. We start with smoothness of function $f$:\n$$f(W^{t+1}) \\leq f(W^t) + \\langle \\nabla f(W^t), W^{t+1} - W^t \\rangle + \\frac{L}{2} \\|W^{t+1} - W^t\\|_F^2$$\n$$\\stackrel{(95)}{=} f(W^t) - \\gamma \\langle \\nabla f(W^t), \\hat{G}^t \\rangle + \\frac{\\gamma^2 L}{2} \\|\\hat{G}^t\\|_F^2. \\qquad (96)$$\n\nTaking a conditional expectation with respect to $W^t$, we bound the second and the third terms from inequality (96):\n$$\\mathbb{E}\\left[\\langle \\nabla f(W^t), \\hat{G}^t \\rangle \\mid W^t\\right] = \\langle \\nabla f(W^t), \\mathbb{E}[\\hat{G}^t \\mid W^t] \\rangle$$\n$$\\stackrel{(95)}{=} p\\langle \\nabla f(W^t), \\mathbb{E}[H_B^t G^t \\mid W^t] \\rangle + (1-p)\\langle \\nabla f(W^t), \\mathbb{E}[G^t H_A^t \\mid W^t] \\rangle$$\n$$\\stackrel{(*)}{=} p\\langle \\nabla f(W^t), \\mathbb{E}[H_B^t \\mid W^t]\\,\\mathbb{E}[G^t \\mid W^t] \\rangle + (1-p)\\langle \\nabla f(W^t), \\mathbb{E}[G^t \\mid W^t]\\,\\mathbb{E}[H_A^t \\mid W^t] \\rangle$$\n$$= p\\langle \\nabla f(W^t), \\mathbb{E}[H_B^t \\mid W^t]\\,\\nabla f(W^t) \\rangle + (1-p)\\langle \\nabla f(W^t), \\nabla f(W^t)\\,\\mathbb{E}[H_A^t \\mid W^t] \\rangle$$\n$$\\geq \\underbrace{\\left(p\\,\\lambda_{\\min}(\\mathbb{E}[H_B^t]) + (1-p)\\,\\lambda_{\\min}(\\mathbb{E}[H_A^t])\\right)}_{:=\\lambda_{\\min}^p}\\,\\|\\nabla f(W^t)\\|_F^2$$\n$$= \\lambda_{\\min}^p \\|\\nabla f(W^t)\\|_F^2. \\qquad (97)$$\nwhere in $(*)$ we used that $H_B^t$, $H_A^t$ and $G^t$ are independent.\n\nNow we bound the third term:\n$$\\mathbb{E}\\left[\\|\\hat{G}^t\\|_F^2 \\mid W^t\\right] \\stackrel{(95)}{=} p\\,\\mathbb{E}\\left[\\|H_B^t G^t\\|_F^2 \\mid W^t\\right] + (1-p)\\,\\mathbb{E}\\left[\\|G^t H_A^t\\|_F^2 \\mid W^t\\right]$$\n$$= p\\,\\mathbb{E}\\left[\\langle H_B^t G^t, H_B^t G^t \\rangle \\mid W^t\\right] + (1-p)\\,\\mathbb{E}\\left[\\langle G^t H_A^t, G^t H_A^t \\rangle \\mid W^t\\right]$$\n$$\\stackrel{(**)}{=} p\\,\\mathbb{E}\\left[\\langle G^t, H_B^t G^t \\rangle \\mid W^t\\right] + (1-p)\\,\\mathbb{E}\\left[\\langle G^t, G^t H_A^t \\rangle \\mid W^t\\right],$$\nwhere in $(**)$ we used the property of projection matrices $H_B^t$, $H_A^t$. By the independence of $H_B^t$, $H_A^t$, and $G^t$, we obtain\n$$\\mathbb{E}\\left[\\|\\hat{G}^t\\|_F^2 \\mid W^t\\right] = p\\,\\mathbb{E}\\left[\\left\\langle G^t, \\mathbb{E}[H_B^t \\mid W^t]\\,G^t \\right\\rangle \\mid W^t\\right] + (1-p)\\,\\mathbb{E}\\left[\\left\\langle G^t, G^t\\,\\mathbb{E}[H_A^t \\mid W^t] \\right\\rangle \\mid W^t\\right]$$\n$$\\leq p\\,\\lambda_{\\max}\\!\\left(\\mathbb{E}[H_B^t \\mid W^t]\\right)\\,\\mathbb{E}\\left[\\|G^t\\|_F^2 \\mid W^t\\right] + (1-p)\\,\\lambda_{\\max}\\!\\left(\\mathbb{E}[H_A^t \\mid W^t]\\right)\\,\\mathbb{E}\\left[\\|G^t\\|_F^2 \\mid W^t\\right]$$\n$$= \\underbrace{\\left(p\\,\\lambda_{\\max}(\\mathbb{E}[H_B^t \\mid W^t]) + (1-p)\\,\\lambda_{\\max}(\\mathbb{E}[H_A^t \\mid W^t])\\right)}_{:=\\lambda_{\\max}^p}\\,\\mathbb{E}\\left[\\|G^t\\|_F^2 \\mid W^t\\right]$$\n$$= \\lambda_{\\max}^p\\,\\mathbb{E}\\left[\\|G^t\\|_F^2 \\mid W^t\\right]. \\qquad (98)$$\n\nPlugging (97) and (98) into (96), we obtain\n$$\\mathbb{E}\\left[f(W^{t+1}) \\mid W^t\\right] \\leq f(W^t) - \\gamma\\,\\mathbb{E}\\left[\\langle \\nabla f(W^t), \\hat{G}^t \\rangle \\mid W^t\\right] + \\frac{\\gamma^2 L}{2}\\,\\mathbb{E}\\left[\\|\\hat{G}^t\\|_F^2 \\mid W^t\\right]$$\n$$\\leq f(W^t) - \\gamma\\,\\lambda_{\\min}^p \\|\\nabla f(W^t)\\|_F^2 + \\frac{\\gamma^2 \\lambda_{\\max}^p L}{2}\\,\\mathbb{E}\\left[\\|G^t\\|_F^2 \\mid W^t\\right].$$",
      "index": 93,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "By Assumption 4,\n$$\\mathbb{E}\\!\\left[f(W^{t+1})-f^*\\mid W^t\\right]\\le f(W^t)-\\gamma\\,\\mathbb{E}\\!\\left[\\langle\\nabla f(W^t),\\hat G^t\\rangle\\mid W^t\\right]+\\frac{\\gamma^2L}{2}\\,\\mathbb{E}\\!\\left[\\|\\hat G^t\\|_F^2\\mid W^t\\right]$$\n$$\\le f(W^t)-f^*-\\gamma\\lambda_{\\min}^p\\|\\nabla f(W^t)\\|_F^2+\\frac{\\gamma^2\\lambda_{\\max}^pL}{2}\\Big(2A_1(f(W^t)-f^*)+B_1\\|\\nabla f(W^t)\\|_F^2+C_1\\Big)$$\n$$\\le\\left(1+\\gamma^2\\lambda_{\\max}^pLA_1\\right)(f(W^t)-f^*)-\\gamma\\lambda_{\\min}^p\\!\\left(1-\\frac{\\gamma LB_1\\lambda_{\\max}^p}{2\\lambda_{\\min}^p}\\right)\\!\\|\\nabla f(W^t)\\|_F^2+\\frac{\\gamma^2\\lambda_{\\max}^pLC_1}{2}.$$\n\nTaking mathematical expectation and selecting a stepsize as $0<\\gamma\\le\\frac{1}{LB_1}\\!\\left(\\frac{\\lambda_{\\max}^p}{\\lambda_{\\min}^p}\\right)^{-1}$, we get\n$$\\mathbb{E}\\!\\left[f(W^{t+1})-f^*\\right]\\le\\left(1+\\gamma^2\\lambda_{\\max}^pLA_1\\right)\\mathbb{E}\\!\\left[f(W^t)-f^*\\right]-\\frac{\\gamma\\lambda_{\\min}^p}{2}\\mathbb{E}\\!\\left[\\|\\nabla f(W^t)\\|_F^2\\right]+\\frac{\\gamma^2\\lambda_{\\max}^pLC_1}{2}.\\tag{99}$$\n\nDefining $\\delta^t:=\\mathbb{E}\\!\\left[f(W^t)-f^*\\right]$, $r^t:=\\mathbb{E}\\!\\left[\\|\\nabla f(W^t)\\|_F^2\\right]$ for every $t\\ge0$, we have\n$$\\delta^{t+1}\\le\\left(1+\\gamma^2\\lambda_{\\max}^pLA_1\\right)\\delta^t-\\frac{\\gamma\\lambda_{\\min}^p}{2}r^t+\\frac{\\gamma^2\\lambda_{\\max}^pLC_1}{2}.$$\n\nFixing $w_{-1}>0$ and defining $w_t=\\dfrac{w_{t-1}}{1+\\gamma^2LA_1\\lambda_{\\max}^p}$ for all $t\\ge0$, we have\n$$\\frac{1}{2}\\lambda_{\\min}^p w_tr^t\\le\\frac{w_t}{\\gamma}\\left(1+\\gamma^2\\lambda_{\\max}^pLA_1\\right)\\delta^t-\\frac{w_t}{\\gamma}\\delta^{t+1}+\\frac{1}{2}\\gamma LC_1\\lambda_{\\max}^p w_t\n=\\frac{w_{t-1}\\delta^t}{\\gamma}-\\frac{w_t\\delta^{t+1}}{\\gamma}+\\frac{1}{2}\\gamma LC_1\\lambda_{\\max}^p w_t.$$\n\nSumming over $t$ from $0$ to $T-1$, we have\n$$\\sum_{t=0}^{T-1}w_tr^t\\le\\frac{2w_{-1}\\delta^0}{\\gamma\\lambda_{\\min}^p}-\\frac{2w_{T-1}\\delta^T}{\\gamma\\lambda_{\\min}^p}+\\gamma LC_1\\frac{\\lambda_{\\max}^p}{\\lambda_{\\min}^p}\\sum_{t=0}^{T-1}w_t.$$\n\nDefining $\\mathcal{W}_{T-1}=\\sum_{t=0}^{T-1}w_t$, we acquire\n$$\\sum_{t=0}^{T-1}\\frac{w_t}{\\mathcal{W}_{T-1}}\\,r^t\\le\\frac{2w_{-1}\\delta^0}{\\gamma\\lambda_{\\min}^p\\mathcal{W}_{T-1}}+\\gamma LC_1\\frac{\\lambda_{\\max}^p}{\\lambda_{\\min}^p}.$$\n\nUsing the next chain of inequalities\n$$\\mathcal{W}_{T-1}=\\sum_{t=0}^{T-1}w_t\\ge T\\min_{0\\le t\\le T-1}w_t=Tw_{T-1}=\\frac{Tw_{-1}}{(1+\\gamma^2\\lambda_{\\max}^pLA_1)^T},$$\nwe have\n$$\\sum_{t=0}^{T-1}\\frac{w_t}{\\mathcal{W}_{T-1}}\\,r^t\\le\\frac{2(1+\\gamma^2\\lambda_{\\max}^pLA_1)^T}{\\gamma T\\lambda_{\\min}^p}\\bigl(f(W^0)-f^*\\bigr)+\\gamma LC_1\\frac{\\lambda_{\\max}^p}{\\lambda_{\\min}^p}.$$",
      "index": 94,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Selecting 0 < Î³ â‰¤ 1/âˆš(L A_1 Î»_max^p T), and using (1 + Î³^2 Î»_max^p L A_1)^T â‰¤ exp(Î³^2 Î»_max^p L A_1 T) â‰¤ exp(1) â‰¤ 3, we obtain\n\nâˆ‘_{t=0}^{T-1} (w_t/ð“¦^{T-1}) r^t â‰¤ 6Î´^0/(Î³ T Î»_min^p) + Î³ L C_1 (Î»_max^p/Î»_min^p).\n\nâ–¡\n\nNext we show convergence of Bernoulli-LoRA-SGD under additional Assumption 6.",
      "index": 95,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "C.2.2 Convergence under Polyak-Åojasiewicz Condition\n\nTheorem 12. Let Assumptions 2, 3, 4, and 6 hold, and stepsize satisfy\n\n0 < Î³ â‰¤ min{ Î¼ Î»_min^p/(2 L A_1 Î»_max^p), 2/(Î¼ Î»_min^p), 1/(L B_1) (Î»_max^p/Î»_min^p)^{-1} }.\n\nThen iterates generated by Bernoulli-LoRA-SGD (Algorithm 4) satisfy\n\nE[f(W^T) âˆ’ f^*] â‰¤ (1 âˆ’ Â½ Î³ Î¼ Î»_min^p)^T (f(W^0) âˆ’ f^*) + (Î³ L C_1/Î¼) Â· (Î»_max^p/Î»_min^p),\n\nwhere Î»_min^p := p Î»_min^{H_B} + (1 âˆ’ p) Î»_min^{H_A}, Î»_max^p := p Î»_max^{H_B} + (1 âˆ’ p) Î»_max^{H_A}.\n\nProof. We start our proof with inequality (99). Using PL-inequality (see Assumption 6), we have\n\nE[f(W^{t+1}) âˆ’ f^*] â‰¤ (1 + Î³^2 Î»_max^p L A_1) E[f(W^t) âˆ’ f^*] âˆ’ (Î³ Î»_min^p/2) E[â€–âˆ‡f(W^t)â€–_F^2] + (Î³^2 Î»_max^p L C_1/2)\n\nâ‰¤ (1 âˆ’ Î³ Î¼ Î»_min^p + Î³^2 Î»_max^p L A_1) E[f(W^t) âˆ’ f^*] + (Î³^2 Î»_max^p L C_1/2).\n\nTaking the stepsize as 0 < Î³ â‰¤ min{ Î¼ Î»_min^p/(2 L A_1 Î»_max^p), 2/(Î¼ Î»_min^p) }, we obtain\n\nE[f(W^{t+1}) âˆ’ f^*] â‰¤ (1 âˆ’ Â½ Î³ Î¼ Î»_min^p) E[f(W^t) âˆ’ f^*] + (Î³^2 Î»_max^p L C_1/2)\n\nâ‰¤ (1 âˆ’ Â½ Î³ Î¼ Î»_min^p)^{t+1} E[f(W^0) âˆ’ f^*] + (Î³^2 Î»_max^p L C_1/2) âˆ‘_{Ï„=0}^{t} (1 âˆ’ Â½ Î³ Î¼ Î»_min^p)^{tâˆ’Ï„}\n\nâ‰¤ (1 âˆ’ Â½ Î³ Î¼ Î»_min^p)^{t+1} E[f(W^0) âˆ’ f^*] + (Î³^2 Î»_max^p L C_1/2) âˆ‘_{Ï„=0}^{âˆž} (1 âˆ’ Â½ Î³ Î¼ Î»_min^p)^{Ï„}\n\n= (1 âˆ’ Â½ Î³ Î¼ Î»_min^p)^{t+1} E[f(W^0) âˆ’ f^*] + (Î³^2 Î»_max^p L C_1)/(Î³ Î¼ Î»_min^p),\n\nwhere in the last equation we use the formula of the sum of geometric progression.\n\nâ–¡",
      "index": 96,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "C.3 Analysis of Bernoulli-LoRA-MVR\n\nAlgorithm 5 Bernoulli-LoRA-MVR\n\n1: Parameters: pre-trained model W^0 âˆˆ â„^{mÃ—n}, G^0 âˆˆ â„^{mÃ—n}, rank r â‰ª min{m,n}, scaling factor Î± > 0, chain length T, sketch distribution ð’Ÿ_S^B or ð’Ÿ_S^A, Bernoulli probability p, momentum parameter b âˆˆ [0,1]\n2: for t = 0, 1, â€¦, T âˆ’ 1 do\n3: Sample c^t âˆ¼ Be(p) (Bernoulli random variable)\n4: if c^t = 1 then\n5: Sample B_S^t âˆ¼ ð’Ÿ_S^B (Left sketch)\n6: Ä¤A^t = âˆ’Î· ((B_S^t)^âŠ¤ B_S^t)^â€  (B_S^t)^âŠ¤ G^t\n7: W^{t+1} = W^t + (Î±/r) B_S^t Ä¤A^t\n8: else\n9: Sample A_S^t âˆ¼ ð’Ÿ_S^A (Right sketch)\n10: Ä¤B^t = âˆ’Î· G^t (A_S^t)^âŠ¤ (A_S^t (A_S^t)^âŠ¤)^â€ \n11: W^{t+1} = W^t + (Î±/r) Ä¤B^t A_S^t\n12: end if\n13: Sample Î¾^{t+1} âˆ¼ ð’Ÿ\n14: G^{t+1} = âˆ‡f_{Î¾^{t+1}}(W^{t+1}) + (1 âˆ’ b)(G^t âˆ’ âˆ‡f_{Î¾^{t+1}}(W^t))\n15: end for\n\nRecently, there has been a significant surge of interest in variance-reduced methods for addressing finite-sum problems [J. Reddi et al., 2015; Shang et al., 2018; Malinovsky et al., 2022; RichtÃ¡rik et al., 2024]. It has gained prominence as a formidable alternative to stochastic gradient descent (SGD) in tackling non-convex optimization problems. Notably, it has been pivotal in introducing the first algorithms capable of surpassing SGDâ€™s convergence rate for locating first-order critical points. Despite these advancements, variance reduction methods often come with challenges, including the necessity for meticulously tuned learning rates and the reliance on overly large batch sizes to realize their benefits. To address some of these limitations, Momentum Variance Reduction (MVR) was proposed specifically for server-only stochastic non-convex optimization [Cutkosky and Orabona, 2019]. This approach leverages a modified form of momentum to achieve variance reduction while eliminating the dependence on large batch sizes. A proof on the MVR technique with better dependence on the momentum parameter was obtained by Tyurin and RichtÃ¡rik [2023]. In the context of Federated Learning, Karagulyan et al. [2024] proposed the SPAM method. On the server side, MVR is utilized to enhance optimization efficiency, while the client side incorporates the Stochastic Proximal Point Method updates. This section is devoted to Bernoulli-LoRA-MVR, a method designed in the scope of the Bernoulli-LoRA framework, based on the MVR technique.\n\nTo show convergence guarantees for Bernoulli-LoRA-MVR, the iterates of the method can be rewritten in the following way:\n\nW^{t+1} = W^t âˆ’ Î³ Äœ^t, where Äœ^t = { H_B^t G^t, with probability p; G^t H_A^t, with probability 1 âˆ’ p } (100)\n\nG^{t+1} = âˆ‡f_{Î¾^{t+1}}(W^{t+1}) + (1 âˆ’ b)(G^t âˆ’ âˆ‡f_{Î¾^{t+1}}(W^t)). (101)\n\nFirst of all, we reprove the descent lemma from the paper of Li et al. [2021] for the generic gradient step (100).",
      "index": 97,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Lemma 5. Let Assumptions 1, 3 hold. Then, iterates defined as (100) satisfy\n$$\n\\mathbb{E}\\left[f(W^{t+1})-f^*\\mid W^t\\right]\\le f(W^t)-f^*-\\frac{\\gamma\\lambda_{\\min}^p}{2}\\|\\nabla f(W^t)\\|_{\\mathrm{F}}^2+\\frac{\\gamma\\lambda_{\\max}^p}{2}\\|G^t-\\nabla f(W^t)\\|_{\\mathrm{F}}^2-\\left(\\frac{1}{2\\gamma}-\\frac{L}{2}\\right)\\mathbb{E}\\left[\\|W^{t+1}-W^t\\|_{\\mathrm{F}}^2\\mid W^t\\right].\n$$\n\nProof. By Assumption 3, we have\n$$\n\\begin{aligned}\nf(W^{t+1})&\\le f(W^t)+\\langle\\nabla f(W^t),W^{t+1}-W^t\\rangle_F+\\frac{L}{2}\\|W^{t+1}-W^t\\|_{\\mathrm{F}}^2\\\\\n&=f(W^t)-\\gamma\\langle\\nabla f(W^t),\\hat G^t\\rangle_F+\\frac{L}{2}\\|W^{t+1}-W^t\\|_{\\mathrm{F}}^2.\n\\end{aligned}\\tag{102}\n$$\nTo continue our proof, we need to bound the second term from (102). Taking conditional expectation by $H^t,W^t$, we obtain\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\langle\\nabla f(W^t),\\hat G^t\\rangle_F\\mid H^t,W^t\\right]\n&\\stackrel{(100)}{=}\np\\langle\\nabla f(W^t),H_B^{t\\top}G^t\\rangle_F+(1-p)\\langle\\nabla f(W^t),G^tH_A^t\\rangle_F\\\\\n&=p\\langle H_B^t\\nabla f(W^t),H_B^tG^t\\rangle_F+(1-p)\\langle\\nabla f(W^t)H_A^t,G^tH_A^t\\rangle_F\\\\\n&=\\frac{p}{2}\\Big(\\|H_B^t\\nabla f(W^t)\\|_{\\mathrm{F}}^2+\\|H_B^tG^t\\|_{\\mathrm{F}}^2-\\|H_B^tG^t-H_B^t\\nabla f(W^t)\\|_{\\mathrm{F}}^2\\Big)\\\\\n&\\quad+\\frac{1-p}{2}\\Big(\\|\\nabla f(W^t)H_A^t\\|_{\\mathrm{F}}^2+\\|G^tH_A^t\\|_{\\mathrm{F}}^2-\\|G^tH_A^t-\\nabla f(W^t)H_A^t\\|_{\\mathrm{F}}^2\\Big)\\\\\n&\\ge \\frac{1}{2}\\Big(p\\|H_B^t\\nabla f(W^t)\\|_{\\mathrm{F}}^2+(1-p)\\|\\nabla f(W^t)H_A^t\\|_{\\mathrm{F}}^2\\Big)\n+\\frac{1}{2}\\mathbb{E}\\left[\\|\\hat G^t\\|_{\\mathrm{F}}^2\\mid H^t,W^t\\right]\\\\\n&\\quad-\\frac{1}{2}\\Big(p\\|H_B^tG^t-H_B^t\\nabla f(W^t)\\|_{\\mathrm{F}}^2+(1-p)\\|G^tH_A^t-\\nabla f(W^t)H_A^t\\|_{\\mathrm{F}}^2\\Big).\n\\end{aligned}\n$$\nTaking conditional expectation by $W^t$, we have\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\langle\\nabla f(W^t),\\hat G^t\\rangle_F\\mid W^t\\right]\n&\\ge \\frac{1}{2}\\Big(p\\,\\mathbb{E}\\left[\\|H_B^t\\nabla f(W^t)\\|_{\\mathrm{F}}^2\\mid W^t\\right]+(1-p)\\,\\mathbb{E}\\left[\\|\\nabla f(W^t)H_A^t\\|_{\\mathrm{F}}^2\\mid W^t\\right]\\Big)\\\\\n&\\quad+\\frac{1}{2}\\mathbb{E}\\left[\\|\\hat G^t\\|_{\\mathrm{F}}^2\\mid W^t\\right]\\\\\n&\\quad-\\frac{1}{2}\\Big(p\\,\\mathbb{E}\\left[\\|H_B^tG^t-H_B^t\\nabla f(W^t)\\|_{\\mathrm{F}}^2\\mid W^t\\right]+(1-p)\\,\\mathbb{E}\\left[\\|G^tH_A^t-\\nabla f(W^t)H_A^t\\|_{\\mathrm{F}}^2\\mid W^t\\right]\\Big)\\\\\n&\\stackrel{(*)}{\\ge} \\frac{1}{2}\\underbrace{\\big(p\\,\\lambda_{\\min}(\\mathbb{E}[H_B^{t\\top}])+(1-p)\\,\\lambda_{\\min}(\\mathbb{E}[H_A^t])\\big)}_{:=\\lambda_{\\min}^p}\\|\\nabla f(W^t)\\|_{\\mathrm{F}}^2\n+\\frac{1}{2}\\mathbb{E}\\left[\\|\\hat G^t\\|_{\\mathrm{F}}^2\\mid W^t\\right]\\\\\n&\\quad-\\frac{1}{2}\\underbrace{\\big(p\\,\\lambda_{\\max}(\\mathbb{E}[H_B^{t\\top}])+(1-p)\\,\\lambda_{\\max}(\\mathbb{E}[H_A^t])\\big)}_{:=\\lambda_{\\max}^p}\\|G^t-\\nabla f(W^t)\\|_{\\mathrm{F}}^2\\\\\n&\\stackrel{(100)}{=} \\frac{\\lambda_{\\min}^p}{2}\\|\\nabla f(W^t)\\|_{\\mathrm{F}}^2+\\frac{1}{2\\gamma^2}\\mathbb{E}\\left[\\|W^{t+1}-W^t\\|_{\\mathrm{F}}^2\\mid W^t\\right]-\\frac{\\lambda_{\\max}^p}{2}\\|G^t-\\nabla f(W^t)\\|_{\\mathrm{F}}^2.\n\\end{aligned}\\tag{103}\n$$\nwhere in (*) we used the following inequalities for any matrix $V\\in\\mathbb{R}^{m\\times n}$:\n$\n\\begin{aligned}\n\\mathbb{E}\\left[\\|H_B^{t\\top}V\\|_{\\mathrm{F}}^2\\right]\n&=\\mathbb{E}\\left[\\langle H_B^{t\\top}V,H_B^{t\\top}V\\rangle_F\\right]\n=\\langle \\mathbb{E}[H_B^{t\\top}]V, V\\rangle_F \\ge \\lambda_{\\min}\\big(\\mathbb{E}[H_B^{t\\top}]\\big)\\|V\\|_{\\mathrm{F}}^2,\\\\\n\\mathbb{E}\\left[\\|H_B^{t\\top}V\\|_{\\mathrm{F}}^2\\right]\n&\\le \\lambda_{\\max}\\big(\\mathbb{E}[H_B^{t\\top}]\\big)\\|V\\|_{\\mathrm{F}}^2,\\\\\n\\mathbb{E}\\left[\\|V H_A^t\\|_{\\mathrm{F}}^2\\right]\n&=\\mathbb{E}\\left[\\langle V H_A^t, V H_A^t\\rangle_F\\right]\n=\\langle V\\,\\mathbb{E}[H_A^t], V\\rangle_F \\ge \\lambda_{\\min}\\big(\\mathbb{E}[H_A^t]\\big)\\|V\\|_{\\mathrm{F}}^2,\\\\\n\\mathbb{E}\\left[\\|V H_A^t\\|_{\\mathrm{F}}^2\\right]\n&\\le \\lambda_{\\max}\\big(\\mathbb{E}[H_A^t]\\big)\\|V\\|_{\\mathrm{F}}^2.\n\\end{aligned}\n$",
      "index": 98,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Plugging in (103) into (102), we get\n$$\n\\mathbb{E}\\left[f(W^{t+1}) \\mid W^t\\right] \\leq f(W^t) - \\frac{\\gamma\\lambda_{\\min}^p}{2}\\|\\nabla f(W^t)\\|_{\\mathrm{F}}^2 - \\frac{1}{2\\gamma}\\mathbb{E}\\left[\\|W^{t+1} - W^t\\|_{\\mathrm{F}}^2 \\mid W^t\\right] + \\frac{\\gamma\\lambda_{\\max}^p}{2}\\|G^t - \\nabla f(W^t)\\|_{\\mathrm{F}}^2 + \\frac{L}{2}\\mathbb{E}\\left[\\|W^{t+1} - W^t\\|_{\\mathrm{F}}^2 \\mid W^t\\right].\n$$\nâ–¡\n\nLemma 6. Let Assumptions 3, 5 hold. Then, iterates generated by Bernoulli-LoRA-MVR (Algorithm 5) satisfy\n$$\n\\mathbb{E}\\left[\\|G^{t+1} - \\nabla f(W^{t+1})\\|_{\\mathrm{F}}^2\\right] \\leq (1-b)^2\\mathbb{E}\\left[\\|G^t - \\nabla f(W^t)\\|_{\\mathrm{F}}^2\\right] + 2(1-b)^2L^2\\mathbb{E}\\left[\\|W^{t+1} - W^t\\|_{\\mathrm{F}}^2\\right] + 2b^2\\sigma^2.\n$$\n(104)\n\nProof. Taking conditional expectation by $\\mathcal{F}^{t+1}=\\{W^{t+1},G^t\\}$, we obtain\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\|G^{t+1} - \\nabla f(W^{t+1})\\|_{\\mathrm{F}}^2 \\mid \\mathcal{F}^{t+1}\\right]\n&\\stackrel{(101)}{=} \\mathbb{E}\\left[\\|\\nabla f_{\\xi_{t+1}}(W^{t+1}) - \\nabla f(W^{t+1}) + (1-b)(G^t - \\nabla f_{\\xi_{t+1}}(W^t))\\|_{\\mathrm{F}}^2 \\mid \\mathcal{F}^{t+1}\\right] \\\\\n&\\stackrel{(15)}{=} (1-b)^2\\|G^t - \\nabla f(W^t)\\|_{\\mathrm{F}}^2 \\\\\n&\\quad + \\mathbb{E}\\left[\\|\\nabla f_{\\xi_{t+1}}(W^{t+1}) - \\nabla f(W^{t+1}) + (1-b)(\\nabla f(W^t) - \\nabla f_{\\xi_{t+1}}(W^t))\\|_{\\mathrm{F}}^2 \\mid \\mathcal{F}^{t+1}\\right] \\\\\n&\\leq (1-b)^2\\|G^t - \\nabla f(W^t)\\|_{\\mathrm{F}}^2 + 2b^2\\mathbb{E}\\left[\\|\\nabla f_{\\xi_{t+1}}(W^{t+1}) - \\nabla f(W^{t+1})\\|_{\\mathrm{F}}^2 \\mid \\mathcal{F}^{t+1}\\right] \\\\\n&\\quad + 2(1-b)^2\\mathbb{E}\\left[\\|\\nabla f_{\\xi_{t+1}}(W^{t+1}) - \\nabla f_{\\xi_{t+1}}(W^t) - \\nabla f(W^{t+1}) + \\nabla f(W^t)\\|_{\\mathrm{F}}^2 \\mid \\mathcal{F}^{t+1}\\right] \\\\\n&\\leq (1-b)^2\\|G^t - \\nabla f(W^t)\\|_{\\mathrm{F}}^2 + 2b^2\\mathbb{E}\\left[\\|\\nabla f_{\\xi_{t+1}}(W^{t+1}) - \\nabla f(W^{t+1})\\|_{\\mathrm{F}}^2 \\mid \\mathcal{F}^{t+1}\\right] \\\\\n&\\quad + 2(1-b)^2\\mathbb{E}\\left[\\|\\nabla f_{\\xi_{t+1}}(W^{t+1}) - \\nabla f_{\\xi_{t+1}}(W^t)\\|_{\\mathrm{F}}^2 \\mid \\mathcal{F}^{t+1}\\right] \\\\\n&\\leq (1-b)^2\\|G^t - \\nabla f(W^t)\\|_{\\mathrm{F}}^2 + 2(1-b)^2L^2\\|W^{t+1} - W^t\\|_{\\mathrm{F}}^2 + 2b^2\\sigma^2,\n\\end{aligned}\n$$\nwhere in the last inequality we used smoothness of $f_\\xi$ and bounded variance assumption. Taking expectation, we conclude the proof. â–¡",
      "index": 99,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "C.3.1 Convergence for Smooth Non-Convex Functions\n\nTheorem 13. Let Assumptions 1, 2, 3, and 5 hold, and let the stepsize satisfy\n$0 < \\gamma \\leq \\frac{1}{L\\left(1+\\sqrt{\\frac{2\\lambda_{\\max}^p(1-b)^2}{b}}\\right)}$.\nThen the iterates of Bernoulli-LoRA-MVR (Algorithm 5) satisfy\n$$\n\\mathbb{E}\\left[\\left\\|\\nabla f(\\widetilde{W}^T)\\right\\|_{\\mathrm{F}}^2\\right] \\leq \\frac{2(f(W^0) - f^*)}{\\lambda_{\\min}^p\\gamma T} + \\frac{\\|G^0 - \\nabla f(W^0)\\|_{\\mathrm{F}}^2}{b(2-b)T} \\cdot \\frac{\\lambda_{\\max}^p}{\\lambda_{\\min}^p} + \\frac{2b\\sigma^2}{2-b} \\cdot \\frac{\\lambda_{\\max}^p}{\\lambda_{\\min}^p},\n$$\n(105)\nwhere $\\lambda_{\\min}^p := p\\lambda_{\\min}^{H_B} + (1-p)\\lambda_{\\min}^{H_A}$, $\\lambda_{\\max}^p := p\\lambda_{\\max}^{H_B} + (1-p)\\lambda_{\\max}^{H_A}$, and $\\widetilde{W}^T$ is drawn uniformly at random from the iterate sequence $\\{W^0, W^1, \\ldots, W^{T-1}\\}$.\n\nProof. Denote the Lyapunov function $\\Phi_t$ as follows\n$$\n\\Phi_t = f(W^t) - f^* + \\frac{\\gamma\\lambda_{\\max}^p}{2b(2-b)}\\|G^t - \\nabla f(W^t)\\|_{\\mathrm{F}}^2.\n$$\n(106)",
      "index": 100,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "By Lemma 5 and Lemma 6, we have\n$$\n\\mathbb{E}[\\Phi_{t+1}] \\leq \\mathbb{E}\\left[f(W^t)\\right] - f^* - \\frac{\\gamma\\lambda_{\\min}^p}{2}\\mathbb{E}\\left[\\|\\nabla f(W^t)\\|_F^2\\right] - \\left(\\frac{1}{2\\gamma} - \\frac{L}{2}\\right)\\mathbb{E}\\left[\\|W^{t+1} - W^t\\|_F^2\\right]\n$$\n$$\n+ \\frac{\\gamma\\lambda_{\\max}^p}{2}\\mathbb{E}\\left[\\|G^t - \\nabla f(W^t)\\|_F^2\\right] + \\frac{\\gamma(1-b)^2\\lambda_{\\max}^p}{2b(2-b)}\\mathbb{E}\\left[\\|G^t - \\nabla f(W^t)\\|_F^2\\right]\n$$\n$$\n+ \\frac{\\gamma(1-b)^2L^2\\lambda_{\\max}^p}{2b(2-b)}\\mathbb{E}\\left[\\|W^{t+1} - W^t\\|_F^2\\right] + \\frac{\\gamma\\lambda_{\\max}^p b\\sigma^2}{2-b}\n$$\n$$\n\\leq \\mathbb{E}[\\Phi_t] - \\frac{\\gamma\\lambda_{\\min}^p}{2}\\mathbb{E}\\left[\\|\\nabla f(W^t)\\|_F^2\\right] + \\frac{\\gamma\\lambda_{\\max}^p b\\sigma^2}{2-b}\n$$\n$$\n- \\left(\\frac{1}{2\\gamma} - \\frac{L}{2} - \\frac{\\gamma(1-b)^2L^2\\lambda_{\\max}^p}{2b(2-b)}\\right)\\mathbb{E}\\left[\\|W^{t+1} - W^t\\|_F^2\\right].\n$$\nSelecting $0 < \\gamma \\leq \\frac{1}{L\\left(1 + \\sqrt{\\frac{(1-b)^2}{b(2-b)}\\lambda_{\\max}^p}\\right)}$, we obtain\n$$\n\\mathbb{E}[\\Phi_{t+1}] \\leq \\mathbb{E}[\\Phi_t] - \\frac{\\gamma\\lambda_{\\min}^p}{2}\\mathbb{E}\\left[\\|\\nabla f(W^t)\\|_F^2\\right] + \\frac{\\gamma\\lambda_{\\max}^p b\\sigma^2}{2-b}.\n$$\nSumming over $t$ from 0 to $T - 1$, we get\n$$\n\\frac{\\gamma\\lambda_{\\min}^p}{2}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\|\\nabla f(W^t)\\|_F^2\\right] \\leq \\mathbb{E}[\\Phi_0] - \\mathbb{E}[\\Phi_T] + \\frac{\\gamma\\lambda_{\\max}^p b\\sigma^2}{2-b}T.\n$$\nFinally, dividing both sides by $\\frac{\\gamma\\lambda_{\\min}^p}{2}$ yields\n$$\n\\mathbb{E}\\left[\\|\\nabla f(\\widetilde{W}^T)\\|_F^2\\right] \\leq \\frac{2\\Phi_0}{\\lambda_{\\min}^p \\gamma T} + \\frac{2b\\sigma^2}{2-b} \\cdot \\frac{\\lambda_{\\max}^p}{\\lambda_{\\min}^p},\n$$\nwhere $\\widetilde{W}^T$ is drawn uniformly at random from the iterate sequence $\\{W^0, W^1, \\ldots, W^{T-1}\\}$. â–¡\nNext we show convergence guarantee for Bernoulli-LoRA-MVR, supposing additionally Assumption 6 holds.",
      "index": 101,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "C.3.2 Convergence under Polyak-Åojasiewicz Condition\nTheorem 14. Let Assumptions 1, 2, 3, 5, and 6 hold, and let the stepsize satisfy\n$$\n0 < \\gamma \\leq \\min\\left\\{\\frac{1}{L\\left(1 + \\sqrt{\\frac{2(1-b)^2}{b(2-b)}\\lambda_{\\max}^p}\\right)}, \\frac{b}{2\\mu\\lambda_{\\min}^p}\\right\\}.\n$$\nThen the iterates of Bernoulli-LoRA-MVR (Algorithm 5) satisfy\n$$\n\\mathbb{E}\\left[f(W^T) - f^*\\right] \\leq \\left(1 - \\gamma\\mu\\lambda_{\\min}^p\\right)^T \\Phi_0 + \\frac{b\\sigma^2}{(2-b)\\mu} \\cdot \\frac{\\lambda_{\\max}^p}{\\lambda_{\\min}^p}, \\tag{107}\n$$\nwhere $\\lambda_{\\min}^p := p\\lambda_{\\min}^{H_B} + (1-p)\\lambda_{\\min}^{H_A}$, $\\lambda_{\\max}^p := p\\lambda_{\\max}^{H_B} + (1-p)\\lambda_{\\max}^{H_A}$, and $\\Phi_0 = f(W^0) - f^* + \\frac{\\gamma\\lambda_{\\max}^p}{b(2-b)}\\|G^0 - \\nabla f(W^0)\\|_F^2$.",
      "index": 102,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Proof. Denote Lyapunov function Î¦_t as follows\n\nÎ¦_t = f(W^t) âˆ’ f* + Î³ Î»_max^p/(b(2 âˆ’ b)) ||G^t âˆ’ âˆ‡f(W^t)||_F^2. (108)\n\nBy Lemma 5 and Lemma 6, we have\n\nE[Î¦_{t+1}] â‰¤ E[f(W^t)] âˆ’ f* âˆ’ (Î³ Î»_min^p/2) E[||âˆ‡f(W^t)||_F^2] âˆ’ (1/(2Î³) âˆ’ L/2) E[||W^{t+1} âˆ’ W^t||_F^2] + (Î³ Î»_max^p/2) E[||G^t âˆ’ âˆ‡f(W^t)||_F^2] + Î³(1 âˆ’ b)^2 Î»_max^p/(b(2 âˆ’ b)) E[||G^t âˆ’ âˆ‡f(W^t)||_F^2] + Î³(1 âˆ’ b)^2 L^2 Î»_max^p/(b(2 âˆ’ b)) E[||W^{t+1} âˆ’ W^t||_F^2] + Î³ Î»_max^p b Ïƒ^2/(2 âˆ’ b)\n\nâ‰¤ max{1 âˆ’ Î³ Î¼ Î»_min^p, 1 âˆ’ b/2} E[Î¦_t] + Î³ Î»_max^p b Ïƒ^2/(2 âˆ’ b) âˆ’ (1/(2Î³) âˆ’ L/2 âˆ’ Î³(1 âˆ’ b)^2 L^2 Î»_max^p/(b(2 âˆ’ b))) E[||W^{t+1} âˆ’ W^t||_F^2],\n\nwhere in the last inequality we used Assumption 6. Selecting positive stepsize Î³ satisfying the upper bound assumed in the theorem statement, we obtain\n\nE[Î¦_{t+1}] â‰¤ (1 âˆ’ Î³ Î¼ Î»_min^p) E[Î¦_t] + Î³ Î»_max^p b Ïƒ^2/(2 âˆ’ b)\n\nâ‰¤ (1 âˆ’ Î³ Î¼ Î»_min^p)^{t+1} E[Î¦_0] + Î³ Î»_max^p b Ïƒ^2/(2 âˆ’ b) âˆ‘_{Ï„=0}^t (1 âˆ’ Î³ Î¼ Î»_min^p)^{tâˆ’Ï„}\n\nâ‰¤ (1 âˆ’ Î³ Î¼ Î»_min^p)^{t+1} E[Î¦_0] + Î³ Î»_max^p b Ïƒ^2/(2 âˆ’ b) âˆ‘_{Ï„=0}^âˆž (1 âˆ’ Î³ Î¼ Î»_min^p)^Ï„\n\n= (1 âˆ’ Î³ Î¼ Î»_min^p)^{t+1} E[Î¦_0] + Î³ Î»_max^p b Ïƒ^2/((2 âˆ’ b) Î³ Î¼ Î»_min^p),\n\nwhere, in the last equation, we used the formula for the sum of a geometric progression.\n\nâ–¡",
      "index": 103,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "C.4 Analysis of Bernoulli-LoRA-PAGE\n\nAlgorithm 6 Bernoulli-LoRA-PAGE\n1: Parameters: pre-trained model $W^0 \\in \\mathbb{R}^{m \\times n}$, a matrix $G^0 \\in \\mathbb{R}^{m \\times n}$, rank $r \\ll \\min\\{m,n\\}$, scaling factor $\\alpha > 0$, step size $\\eta > 0$, chain length $T$, sketch distribution $\\mathcal{D}_S^B$ or $\\mathcal{D}_S^A$, Bernoulli probability $p$, probability $q$\n2: for $t = 0, 1, \\ldots, T-1$ do\n3: Sample $c^t \\sim \\mathrm{Be}(p)$ (Bernoulli random variable)\n4: if $c^t = 1$ then\n5: Sample $B_S^t \\sim \\mathcal{D}_S^B$ (Left sketch)\n6: $\\hat{A}^t = -\\eta\\left((B_S^t)^\\top B_S^t\\right)^{\\dagger}(B_S^t)^\\top G^t$\n7: $W^{t+1} = W^t + \\frac{\\alpha}{r}\\, B_S^t \\hat{A}^t$\n8: else\n9: Sample $A_S^t \\sim \\mathcal{D}_S^A$ (Right sketch)\n10: $\\hat{B}^t = -\\eta\\, G^t (A_S^t)^\\top \\left(A_S^t (A_S^t)^\\top\\right)^{\\dagger} A_S^t$\n11: $W^{t+1} = W^t + \\frac{\\alpha}{r}\\, \\hat{B}^t A_S^t$\n12: end if\n13: Sample $i_{t+1}$ uniformly at random from $[n]$\n14: $G^{t+1} = \\begin{cases}\n\\nabla f(W^{t+1}), & \\text{with probability } q \\\\\nG^t + \\left(\\nabla f_{i_{t+1}}(W^{t+1}) - \\nabla f_{i_{t+1}}(W^t)\\right), & \\text{with probability } 1 - q\n\\end{cases}$\n15: end for\n\nThere exist several optimal methods for solving a general non-convex optimization problem, e.g., SPIDER [Fang et al., 2018] and SARAH [Pham et al., 2020]. However, the known lower bound used to establish their optimality works only in the small-data regime. Probabilistic Gradient Estimator (PAGE) [Li et al., 2021] is a very simple and easy-to-implement algorithm, known for achieving optimal convergence results in non-convex optimization. PAGE uses the full gradient update with probability $q_t$, or reuses the previous gradient with a small adjustment (at a low computational cost) with probability $1 - q_t$. A general version of PAGE on Riemannian manifolds is considered in [Demidovich et al., 2024a]. In this section we present Bernoulli-LoRA-PAGE, a new method within the Bernoulli-LoRA framework, based on the PAGE algorithm.\n\nNotice that the iterates of Bernoulli-LoRA-PAGE (Algorithm 6) can be rewritten in the following simple way\n$$\nW^{t+1} = W^t - \\gamma \\hat{G}^t, \\quad \\text{where } \\hat{G}^t =\n\\begin{cases}\nH_B^t G^t, & \\text{with probability } p \\\\\nG^t H_A^t, & \\text{with probability } 1 - p\n\\end{cases}\n\\tag{109}\n$$\n$$\nG^{t+1} =\n\\begin{cases}\n\\nabla f(W^{t+1}), & \\text{with probability } q \\\\\nG^t + \\left(\\nabla f_{i_{t+1}}(W^{t+1}) - \\nabla f_{i_{t+1}}(W^t)\\right), & \\text{with probability } 1 - q\n\\end{cases}\n\\tag{110}\n$$\n\nLemma 7. Let Assumption 3 hold. Then, the iterates generated by Bernoulli-LoRA-PAGE satisfy\n$$\n\\mathbb{E}\\!\\left[\\|G^{t+1} - \\nabla f(W^{t+1})\\|_F^2\\right]\n\\le (1-q)\\,\\mathbb{E}\\!\\left[\\|G^t - \\nabla f(W^t)\\|_F^2\\right]\n+ (1-q)L^2 \\mathbb{E}\\!\\left[\\|W^{t+1} - W^t\\|_F^2\\right].\n\\tag{111}\n$$",
      "index": 104,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Proof. Taking the full mathematical expectation, we obtain\n$$\n\\mathbb{E}\\left[\\left\\|G^{t+1}-\\nabla f(W^{t+1})\\right\\|_F^2\\right]\\stackrel{(110)}{=}(1-q)\\,\\mathbb{E}\\left[\\left\\|G^t-\\nabla f(W^{t+1})+\\left(\\nabla f_{i_{t+1}}(W^{t+1})-\\nabla f_{i_{t+1}}(W^t)\\right)\\right\\|_F^2\\right]\n$$\n$$\n\\stackrel{(15)}{=}(1-q)\\,\\mathbb{E}\\left[\\left\\|G^t-\\nabla f(W^t)\\right\\|_F^2\\right]\n+(1-q)\\,\\mathbb{E}\\left[\\left\\|\\left(\\nabla f_{i_{t+1}}(W^{t+1})-\\nabla f_{i_{t+1}}(W^t)\\right)-\\left(\\nabla f(W^{t+1})-\\nabla f(W^t)\\right)\\right\\|_F^2\\right]\n$$\n$$\n\\le (1-q)\\,\\mathbb{E}\\left[\\left\\|G^t-\\nabla f(W^t)\\right\\|_F^2\\right]\n+(1-q)\\,\\mathbb{E}\\left[\\left\\|\\nabla f_{i_{t+1}}(W^{t+1})-\\nabla f_{i_{t+1}}(W^t)\\right\\|_F^2\\right]\n$$\n$$\n\\le (1-q)\\,\\mathbb{E}\\left[\\left\\|G^t-\\nabla f(W^t)\\right\\|_F^2\\right]+(1-q)L^2\\,\\mathbb{E}\\left[\\left\\|W^{t+1}-W^t\\right\\|_F^2\\right],\n$$\nwhere in the last inequality we used smoothness of each $f_i$. â–¡",
      "index": 105,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "C.4.1 Convergence for Smooth Non-Convex Functions\n\nTheorem 15. Let Assumptions 1, 2, and 3 hold, and let the stepsize satisfy\n$$\n0<\\gamma\\le \\frac{1}{L\\left(1+\\sqrt{\\frac{1-q}{q}\\,\\lambda_{\\max}^p}\\right)}.\n$$\nThen the iterates of PAGE-Bernoulli-LoRA (Algorithm 6) satisfy\n$$\n\\mathbb{E}\\left[\\left\\|\\nabla f(\\widetilde{W}^T)\\right\\|_F^2\\right]\\le \\frac{2\\big(f(W^0)-f^*\\big)}{\\lambda_{\\min}^p\\,\\gamma\\,T}\n+q\\,\\frac{\\left\\|G^0-\\nabla f(W^0)\\right\\|_F^2}{T}\\cdot\\frac{\\lambda_{\\max}^p}{\\lambda_{\\min}^p}, \\tag{112}\n$$\nwhere $\\lambda_{\\min}^p:=p\\lambda_{\\min}^{H_B}+(1-p)\\lambda_{\\min}^{H_A}$, $\\lambda_{\\max}^p:=p\\lambda_{\\max}^{H_B}+(1-p)\\lambda_{\\max}^{H_A}$, and $\\widetilde{W}^T$ is drawn uniformly at random from the iterate sequence $\\{W^0,W^1,\\ldots,W^{T-1}\\}$.\n\nProof. Denote Lyapunov function $\\Phi_t$ as follows\n$$\n\\Phi_t=f(W^t)-f^*+\\frac{\\gamma\\lambda_{\\max}^p}{2q}\\left\\|G^t-\\nabla f(W^t)\\right\\|_F^2. \\tag{113}\n$$\nBy Lemma 5 and Lemma 7, we have\n$$\n\\mathbb{E}[\\Phi_{t+1}]\\le \\mathbb{E}[f(W^t)]-f^*-\\frac{\\gamma\\lambda_{\\min}^p}{2}\\mathbb{E}\\left[\\left\\|\\nabla f(W^t)\\right\\|_F^2\\right]\n-\\left(\\frac{1}{2\\gamma}-\\frac{L}{2}\\right)\\mathbb{E}\\left[\\left\\|W^{t+1}-W^t\\right\\|_F^2\\right]\n$$\n$$\n+\\frac{\\gamma\\lambda_{\\max}^p}{2}\\mathbb{E}\\left[\\left\\|G^t-\\nabla f(W^t)\\right\\|_F^2\\right]\n+\\frac{\\gamma\\lambda_{\\max}^p(1-q)}{2q}\\mathbb{E}\\left[\\left\\|G^t-\\nabla f(W^t)\\right\\|_F^2\\right]\n+\\frac{\\gamma\\lambda_{\\max}^p(1-q)L^2}{2q}\\mathbb{E}\\left[\\left\\|W^{t+1}-W^t\\right\\|_F^2\\right]\n$$\n$$\n\\le \\mathbb{E}[\\Phi_t]-\\frac{\\gamma\\lambda_{\\min}^p}{2}\\mathbb{E}\\left[\\left\\|\\nabla f(W^t)\\right\\|_F^2\\right]\n-\\left(\\frac{1}{2\\gamma}-\\frac{L}{2}-\\frac{\\gamma(1-q)L^2\\lambda_{\\max}^p}{2q}\\right)\\mathbb{E}\\left[\\left\\|W^{t+1}-W^t\\right\\|_F^2\\right].\n$$\nSelecting $0<\\gamma\\le \\frac{1}{L\\left(1+\\sqrt{\\frac{1-q}{q}\\,\\lambda_{\\max}^p}\\right)}$, we obtain\n$$\n\\mathbb{E}[\\Phi_{t+1}]\\le \\mathbb{E}[\\Phi_t]-\\frac{\\gamma\\lambda_{\\min}^p}{2}\\mathbb{E}\\left[\\left\\|\\nabla f(W^t)\\right\\|_F^2\\right].\n$$",
      "index": 106,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Summing over t from 0 to T - 1, we get\n$$\\frac{\\gamma\\lambda_{\\min}^p}{2}\\sum_{t=0}^{T-1}\\mathbb{E}\\!\\left[\\left\\|\\nabla f(W^t)\\right\\|_F^2\\right]\\leq \\mathbb{E}[\\Phi_0]-\\mathbb{E}[\\Phi_T].$$\n\nFinally, dividing both sides by $\\frac{\\gamma\\lambda_{\\min}^p}{2}$ yields\n$$\\mathbb{E}\\!\\left[\\left\\|\\nabla f(\\widetilde{W}^T)\\right\\|_F^2\\right]\\leq \\frac{2\\Phi_0}{\\gamma\\lambda_{\\min}^p\\,T},$$\nwhere $\\widetilde{W}^T$ is drawn uniformly at random from the iterate sequence $\\{W^0,W^1,\\ldots,W^{T-1}\\}$. â–¡",
      "index": 107,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "## C.4.2 Convergence under Polyak-Åojasiewicz Condition\n\nTheorem 16. Let Assumptions 1, 2, 3, and 6 hold, and let the stepsize satisfy\n$$0<\\gamma\\leq \\min\\left\\{\\frac{1}{L\\!\\left(1+2\\sqrt{\\frac{1-q}{q}\\lambda_{\\max}^p}\\right)},\\ \\frac{q}{2\\mu\\lambda_{\\min}^p}\\right\\}.$$\nThen the iterates of Bernoulli-LoRA-PAGE (Algorithm 6) satisfy\n$$\\mathbb{E}\\!\\left[f(W^T)-f^*\\right]\\leq (1-\\gamma\\mu\\lambda_{\\min}^p)^T\\Phi_0, \\tag{114}$$\nwhere $\\lambda_{\\min}^p:=p\\lambda_{\\min}^{H_B}+(1-p)\\lambda_{\\min}^{H_A}$, and $\\Phi_0=f(W^0)-f^*+\\frac{\\gamma\\lambda_{\\max}^p}{q}\\left\\|G^0-\\nabla f(W^0)\\right\\|_F^2$.\n\nProof. Denote Lyapunov function $\\Phi_t$ as follows:\n$$\\Phi_t=f(W^t)-f^*+\\frac{\\gamma\\lambda_{\\max}^p}{q}\\left\\|G^t-\\nabla f(W^t)\\right\\|_F^2. \\tag{115}$$\nBy Lemma 5 and Lemma 7, we have\n\\begin{align*}\n\\mathbb{E}[\\Phi_{t+1}]&\\leq \\mathbb{E}[f(W^t)]-f^*-\\frac{\\gamma\\lambda_{\\min}^p}{2}\\mathbb{E}\\!\\left[\\left\\|\\nabla f(W^t)\\right\\|_F^2\\right]-\\left(\\frac{1}{2\\gamma}-\\frac{L}{2}\\right)\\mathbb{E}\\!\\left[\\left\\|W^{t+1}-W^t\\right\\|_F^2\\right]\\\\\n&\\quad+\\frac{\\gamma\\lambda_{\\max}^p}{2}\\mathbb{E}\\!\\left[\\left\\|G^t-\\nabla f(W^t)\\right\\|_F^2\\right]+\\frac{\\gamma(1-q)\\lambda_{\\max}^p}{q}\\mathbb{E}\\!\\left[\\left\\|G^t-\\nabla f(W^t)\\right\\|_F^2\\right]\\\\\n&\\quad+\\frac{\\gamma(1-q)L^2\\lambda_{\\max}^p}{q}\\mathbb{E}\\!\\left[\\left\\|W^{t+1}-W^t\\right\\|_F^2\\right]\\\\\n&\\leq (1-\\gamma\\mu\\lambda_{\\min}^p)\\mathbb{E}\\!\\left[f(W^t)-f^*\\right]+\\left(1-\\frac{q}{2}\\right)\\frac{\\gamma\\lambda_{\\max}^p}{q}\\mathbb{E}\\!\\left[\\left\\|G^t-\\nabla f(W^t)\\right\\|_F^2\\right]\\\\\n&\\quad-\\left(\\frac{1}{2\\gamma}-\\frac{L}{2}-\\frac{\\gamma(1-q)L^2\\lambda_{\\max}^p}{q}\\right)\\mathbb{E}\\!\\left[\\left\\|W^{t+1}-W^t\\right\\|_F^2\\right],\n\\end{align*}\nwhere in the last inequality we used Assumption 6. Selecting $0<\\gamma\\leq \\min\\left\\{\\frac{1}{L\\!\\left(1+2\\sqrt{\\frac{1-q}{q}\\lambda_{\\max}^p}\\right)},\\frac{q}{2\\mu\\lambda_{\\min}^p}\\right\\}$, we obtain\n$$\\mathbb{E}[\\Phi_{t+1}]\\leq (1-\\gamma\\mu\\lambda_{\\min}^p)\\mathbb{E}[\\Phi_t].$$\nUnrolling the recursion, we obtain\n$$\\mathbb{E}[\\Phi_T]\\leq (1-\\gamma\\mu\\lambda_{\\min}^p)^T\\Phi_0.$$\nâ–¡",
      "index": 108,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "D Proofs for Federated Learning Extensions\n\nIn recent years, distributed optimization problems and algorithms have become a focal point in the Machine Learning (ML) community. This surge in interest is driven by the need to train modern deep neural networks, which often involve billions of parameters and massive datasets [Brown et al., 2020; Kolesnikov et al., 2020]. To achieve practical training times [Li, 2020], parallelizing computations, such as stochastic gradient evaluations, has emerged as a natural solution, leading to the widespread adoption of distributed training algorithms [Goyal et al., 2017; You et al., 2019; Le Scao et al., 2023]. Additionally, distributed methods are crucial when data is inherently distributed across multiple devices or clients, often accompanied by privacy constraintsâ€”a common scenario in Federated Learning (FL) [KoneÄnÃ½ et al., 2016; McMahan et al., 2016; Kairouz et al., 2019; Demidovich et al., 2024b; Sadiev et al., 2024; Yi et al., 2024].\n\nParallel implementations of SGD have become a prominent area of study due to their impressive scalability. However, one of the primary challenges in parallelizing SGD lies in the substantial communication overhead required to exchange gradient updates across nodes. To address this, numerous lossy compression techniques have been developed, enabling nodes to transmit quantized gradients instead of full gradients. While these methods often work well in practice, they are not universally reliable and may fail to ensure convergence.\n\nWe develop several FL methods within the Bernoulli-LoRA framework and provide a convergence analysis for them.",
      "index": 109,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "D.1 Analysis of Fed-Bernoulli-LoRA-QGD\n\nAlgorithm 7 Fed-Bernoulli-LoRA-QGD\n\n1: Parameters: pre-trained model W^0 âˆˆ R^{mÃ—n}, rank r â‰ª min{m,n}, scaling factor Î± > 0, chain length T, sketch distribution D_S^B or D_S^A, Bernoulli probabilities p and q.\n2: for t = 0, 1, â€¦, T âˆ’ 1 do\n3:     for any client l âˆˆ [M] in parallel do\n4:         Compute gradient âˆ‡f_l(W^t) and send compressed version G_l^t = Q_l^t(âˆ‡f_l(W^t)) to the server.\n5:     end for\n6:     G^t = (1/M) âˆ‘_{l=1}^M G_l^t.\n7:     Sample c^t âˆ¼ Be(p) (Bernoulli random variable).",
      "index": 110,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "To overcome these limitations, Quantized SGD (QSGD) by Alistarh et al. [2017] introduces a family of compression techniques that provide both theoretical convergence guarantees and strong empirical performance. QSGD offers a flexible mechanism for balancing communication bandwidth and convergence speed. By adjusting the number of bits transmitted per iteration, nodes can reduce bandwidth usage, albeit at the potential cost of increased variance in the gradient estimates. Different variants of QSGD were considered by HorvÃ¡th et al. [2022], Wen et al. [2017], Panferov et al. [2024].\n\nWe consider the following distributed optimization problem:\n$$\\min_{W \\in \\mathbb{R}^{m \\times n}} \\frac{1}{M} \\sum_{l=1}^{M} f_l(W),$$\nwhere $M$ represents the number of clients. In Federated Learning, a primary bottleneck is the communication overhead between clients and the central server. A common approach to mitigate this issue is communication compression.",
      "index": 111,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Definition 3. A randomized operator $\\mathcal{Q} : \\mathbb{R}^{m \\times n} \\to \\mathbb{R}^{m \\times n}$ is called an unbiased compression operator (or compressor) if there exists a constant $\\omega > 0$ such that, for any matrix $W \\in \\mathbb{R}^{m \\times n}$, the following conditions hold:\n$$\\mathbb{E}[\\mathcal{Q}(W)] = W, \\quad \\text{and} \\quad \\mathbb{E}\\!\\left[\\|\\mathcal{Q}(W) - W\\|_F^2\\right] \\le \\omega \\|W\\|_F^2.$$\n(116)",
      "index": 112,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "To analyze the optimization process, we introduce the following assumption regarding function dissimilarity:\n\nAssumption 11. Let $f^* := \\inf_W f(W)$ and $f_l^* := \\inf_W f_l$ for each $l \\in [M]$. In the non-convex case, the difference at the optimum is defined as:\n$$\\Delta^* := f^* - \\frac{1}{M} \\sum_{l=1}^{M} f_l^* \\ge 0.$$\n(117)\n\nThis assumption quantifies the discrepancy between the global optimal function value and the average of the local optimal function values between the clients.",
      "index": 113,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "To start convergence analysis, we rewrite the updates for $W^t$ and $G^t$ generated by Fed-Bernoulli-LoRA-QGD (Algorithm 7) as follows:\n$$G^t = \\frac{1}{M} \\sum_{l=1}^{M} \\mathcal{Q}_l^t(\\nabla f_l(W^t));$$\n(118)\n$$W^{t+1} = W^t - \\gamma \\hat{G}^t, \\quad \\text{where} \\quad \\hat{G}^t =\n\\begin{cases}\nH_B^t G^t, & \\text{with probability } p, \\\\\nG^t H_A^t, & \\text{with probability } 1 - p.\n\\end{cases}$$\n(119)",
      "index": 114,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "To establish the convergence guarantee for Fed-Bernoulli-LoRA-QGD (Algorithm 7), we first demonstrate that the gradient estimator $G^t$ satisfies Assumption 4. Once this is verified, the convergence rate follows directly using the same reasoning as in the proof of Theorem 11.\n\nLemma 8. Let Assumptions 2, 3, and 11 hold. Then, $G^t$ defined in Algorithm 7 (see (118)) satisfies Assumption 4 with the following constants:\n$$A_1 = \\frac{L \\omega}{M}, \\quad B_1 = 1, \\quad C_1 = 2\\frac{L \\omega \\Delta^*}{M}.$$",
      "index": 115,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Proof. First, we show \\(G^t\\) is an unbiased estimator of \\(\\nabla f(W^t)\\):\n$$\\mathbb{E}\\left[G^t\\mid W^t\\right]=\\frac{1}{M}\\sum_{l=1}^M\\mathbb{E}\\left[\\mathcal{Q}_l^t\\!\\left(\\nabla f_l(W^t)\\right)\\mid W^t\\right]\\stackrel{(116)}{=}\\frac{1}{M}\\sum_{l=1}^M\\nabla f_l(W^t)=\\nabla f(W^t).$$\n\nNow we establish that \\(G^t\\) satisfies Assumption 4. Taking the conditional expectation with respect to \\(W^t\\), we have\n$$\\mathbb{E}\\!\\left[\\|G^t\\|_F^2\\mid W^t\\right]=\\mathbb{E}\\!\\left[\\left\\|\\frac{1}{M}\\sum_{l=1}^M\\mathcal{Q}_l^t\\!\\left(\\nabla f_l(W^t)\\right)-\\nabla f(W^t)+\\nabla f(W^t)\\right\\|_F^2\\mid W^t\\right]$$\n$$\\stackrel{(15)}{=} \\mathbb{E}\\!\\left[\\left\\|\\frac{1}{M}\\sum_{l=1}^M\\mathcal{Q}_l^t\\!\\left(\\nabla f_l(W^t)\\right)-\\nabla f(W^t)\\right\\|_F^2\\mid W^t\\right]+\\|\\nabla f(W^t)\\|_F^2$$\n$$=\\frac{1}{M^2}\\sum_{l=1}^M\\mathbb{E}\\!\\left[\\left\\|\\mathcal{Q}_l^t\\!\\left(\\nabla f_l(W^t)\\right)-\\nabla f_l(W^t)\\right\\|_F^2\\mid W^t\\right]+\\|\\nabla f(W^t)\\|_F^2$$\n$$\\stackrel{(116)}{\\le}\\frac{\\omega}{M^2}\\sum_{l=1}^M\\|\\nabla f_l(W^t)\\|_F^2+\\|\\nabla f(W^t)\\|_F^2$$\n$$\\stackrel{(*)}{\\le}\\frac{2L\\omega}{M^2}\\sum_{l=1}^M\\big(f_l(W^t)-f_l^*\\big)+\\|\\nabla f(W^t)\\|_F^2$$\n$$=\\frac{2L\\omega}{M}\\big(f(W^t)-f^*\\big)+\\|\\nabla f(W^t)\\|_F^2+\\frac{2L\\omega}{M}\\left(f^*-\\frac{1}{M}\\sum_{l=1}^M f_l^*\\right)=:\\Delta^*,$$\nwhere in (*) we used smoothness of each \\(f_l\\). Thus, we have shown that \\(G^t\\) satisfies Assumption 4 with the following constants\n$$A_1=\\frac{L\\omega}{M},\\quad B_1=1,\\quad C_1=\\frac{2L\\omega\\Delta^*}{M}.$$\nâ–¡",
      "index": 116,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "D.1.1 Convergence for Smooth Non-Convex Functions\n\nTheorem 17. Let Assumptions 1, 2, and 3 hold, and stepsize satisfy\n$$0<\\gamma\\le\\min\\left\\{\\frac{1}{L\\sqrt{\\frac{\\omega}{M}\\lambda_{\\max}^p T}},\\ \\frac{1}{L}\\left(\\frac{\\lambda_{\\max}^p}{\\lambda_{\\min}^p}\\right)^{-1}\\right\\}.$$\nThen iterates generated by Fed-Bernoulli-LoRA-QGD (Algorithm 7) satisfy\n$$\\mathbb{E}\\!\\left[\\left\\|\\nabla f(\\widetilde{W}^T)\\right\\|_F^2\\right]\\le\\frac{6\\big(f(W^0)-f^*\\big)}{\\gamma\\lambda_{\\min}^p T}+\\frac{2\\gamma L\\omega\\Delta^*}{M}\\frac{\\lambda_{\\max}^p}{\\lambda_{\\min}^p},$$\nwhere \\(\\lambda_{\\min}^p:=p\\lambda_{\\min}^{H_B}+(1-p)\\lambda_{\\min}^{H_A}\\), \\(\\lambda_{\\max}^p:=p\\lambda_{\\max}^{H_B}+(1-p)\\lambda_{\\max}^{H_A}\\), and \\(\\widetilde{W}^T\\) is chosen at random from \\(\\{W^0,W^1,\\ldots,W^{T-1}\\}\\) with probabilities \\(\\left\\{\\frac{w_t}{\\mathcal{W}_{T-1}}\\right\\}_{t=0}^{T-1}\\), where \\(w_t=\\frac{w_{t-1}}{(1+\\gamma^2L^2\\lambda_{\\max}^p\\omega/M)}\\), \\(\\mathcal{W}_{T-1}=\\sum_{t=0}^{T-1}w_t\\), and \\(w_{-1}>0\\).\n\nProof. By Lemma 8, and Theorem 11, we directly obtain the statement of the theorem.",
      "index": 117,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "D.1.2 Convergence under Polyak-Åojasiewicz Condition\n\nTheorem 18. Let Assumptions 1, 2, 3, and 6 hold, and the stepsize satisfies\n$$0 < \\gamma \\leq \\min\\left\\{ \\frac{\\mu}{2L^2\\,\\omega/M}\\left(\\frac{\\lambda_{\\max}^p}{\\lambda_{\\min}^p}\\right)^{-1},\\ \\frac{2}{\\mu\\,\\lambda_{\\min}^p},\\ \\frac{1}{L}\\left(\\frac{\\lambda_{\\max}^p}{\\lambda_{\\min}^p}\\right)^{-1} \\right\\}.$$\n\nThen iterates generated by Fed-Bernoulli-LoRA-QGD (Algorithm 7) satisfy\n$$\\mathbb{E}\\big[f(W^T) - f^*\\big] \\leq \\left(1 - \\frac{1}{2}\\gamma\\mu\\lambda_{\\min}^p\\right)^T \\big(f(W^0) - f^*\\big) + \\frac{2\\gamma L^2}{\\mu}\\cdot\\frac{\\omega}{M}\\cdot\\frac{\\lambda_{\\max}^p}{\\lambda_{\\min}^p},$$\nwhere $\\lambda_{\\min}^p := p\\,\\lambda_{\\min}^{H_B} + (1-p)\\,\\lambda_{\\min}^{H_A}$, $\\lambda_{\\max}^p := p\\,\\lambda_{\\max}^{H_B} + (1-p)\\,\\lambda_{\\max}^{H_A}$.\n\nProof. By Lemma 8 and Theorem 12, we directly obtain the statement of the theorem. â–¡",
      "index": 118,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "## D.2 Analysis of Fed-Bernoulli-LoRA-MARINA\n\nAlgorithm 8 Fed-Bernoulli-LoRA-MARINA\n\n1: Parameters: pre-trained model $W^0 \\in \\mathbb{R}^{m \\times n}$, $\\{G_l^0\\}_{l \\in [M]} \\in \\mathbb{R}^{m \\times n}$, rank $r \\ll \\min\\{m,n\\}$, scaling factor $\\alpha > 0$, chain length $T$, sketch distribution $\\mathcal{D}_S^B$ or $\\mathcal{D}_S^A$, Bernoulli probabilities $p$ and $q$\n\n2: for $t = 0, 1, \\ldots, T - 1$ do\n\n3: Sample $c^t \\sim \\mathrm{Be}(p)$ (Bernoulli random variable)\n\n4: if $c^t = 1$ then\n\n5: Sample $B_S^t \\sim \\mathcal{D}_S^B$ (left sketch)\n\n6: $\\hat{A}^t = -\\eta \\left((B_S^t)^\\top B_S^t\\right)^\\dagger (B_S^t)^\\top G^t$\n\n7: $W^{t+1} = W^t + \\dfrac{\\alpha}{r} B_S^t \\hat{A}^t$\n\n8: else\n\n9: Sample $A_S^t \\sim \\mathcal{D}_S^A$ (right sketch)\n\n10: $\\hat{B}^t = -\\eta\\, G^t (A_S^t)^\\top \\left(A_S^t (A_S^t)^\\top\\right)^\\dagger$\n\n11: $W^{t+1} = W^t + \\dfrac{\\alpha}{r} \\hat{B}^t A_S^t$\n\n12: end if\n\n13: Broadcast $W^{t+1}$ to each client $l \\in [M]$\n\n14: Sample $s^t \\sim \\mathrm{Be}(q)$\n\n15: for any client $l \\in [M]$ in parallel do\n\n16: Compute gradient $\\nabla f_l(W^{t+1})$\n\n17: $G_l^{t+1} = \\begin{cases}\n\\nabla f_l(W^{t+1}), & \\text{with probability } q \\\\\nG_l^t + \\mathcal{Q}_l^t\\left(\\nabla f_l(W^{t+1}) - \\nabla f_l(W^t)\\right), & \\text{with probability } 1 - q\n\\end{cases}$\n\n18: Send $G_l^{t+1}$ to the server\n\n19: end for\n\n20: $G^{t+1} = \\dfrac{1}{M} \\sum_{l=1}^M G_l^{t+1}$\n\n21: end for\n\nMARINA [Gorbunov et al., 2021] is an advanced method that significantly enhances communication efficiency in non-convex distributed learning across heterogeneous datasets. Its core innovation lies in a communication reduction mechanism that compresses the differences between gradients. The communication complexity bounds for MARINA are known to be better than those of all previous first-order methods. Non-smooth convex analysis of MARINA with different stepsize strategies can be found in [Sokolov and RichtÃ¡rik, 2024]. This section is devoted to Fed-Bernoulli-LoRA-MARINA (Algorithm 8), a method within the Bernoulli-LoRA framework, based on the MARINA algorithm.\n\nIn order to start convergence analysis, we rewrite the updates $W^t$, $G^t$ generated by Fed-Bernoulli-LoRA-MARINA (Algorithm 8):\n\n$W^{t+1} = W^t - \\gamma \\hat{G}^t,\\quad \\text{where } \\hat{G}^t =\n\\begin{cases}\nH_B^t G^t, & \\text{with probability } p \\\\\nG^t H_A^t, & \\text{with probability } 1 - p\n\\end{cases}$ (120)\n\n$G_l^{t+1} =\n\\begin{cases}\n\\nabla f_l(W^{t+1}), & \\text{with probability } q \\\\\nG_l^t + \\mathcal{Q}_l^t\\left(\\nabla f_l(W^{t+1}) - \\nabla f_l(W^t)\\right), & \\text{with probability } 1 - q\n\\end{cases}$ (121)\n\n$G^{t+1} = \\frac{1}{M} \\sum_{l=1}^M G_l^{t+1}.$ (122)",
      "index": 119,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Lemma 9. Let Assumption 3 hold. Then iterates generated by Fed-Bernoulli-LoRA-MARINA satisfy\n$$\n\\mathbb{E}\\left[\\left\\|G^{t+1} - \\nabla f(W^{t+1})\\right\\|_{\\mathrm{F}}^2\\right] \\leq (1-q)\\mathbb{E}\\left[\\left\\|G^t - \\nabla f(W^t)\\right\\|_{\\mathrm{F}}^2\\right] + (1-q)\\frac{\\omega L^2}{M}\\mathbb{E}\\left[\\left\\|W^{t+1} - W^t\\right\\|_{\\mathrm{F}}^2\\right].\n$$\n(123)\n\nProof. Taking the conditional expectation with respect to $W^{t+1}$ and defining $D_l^{t+1} := \\nabla f_l(W^{t+1}) - \\nabla f_l(W^t)$, $D^{t+1} = \\frac{1}{M}\\sum_{l=1}^M D_l^{t+1}$, we obtain\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\left\\|G^{t+1} - \\nabla f(W^{t+1})\\right\\|_{\\mathrm{F}}^2 \\mid W^{t+1}\\right]\n&= (1-q)\\mathbb{E}\\left[\\left\\|G^t - \\nabla f(W^t) + \\frac{1}{M}\\sum_{l=1}^M \\mathcal{Q}_l^t\\left(\\nabla f_l(W^{t+1}) - \\nabla f_l(W^t)\\right)\\right\\|_{\\mathrm{F}}^2 \\mid W^{t+1}\\right] \\\\\n&\\stackrel{(15)}{=} (1-q)\\left\\|G^t - \\nabla f(W^t)\\right\\|_{\\mathrm{F}}^2 + (1-q)\\mathbb{E}\\left[\\left\\|\\frac{1}{M}\\sum_{l=1}^M \\mathcal{Q}_l^t\\left(D_l^{t+1}\\right) - D^{t+1}\\right\\|_{\\mathrm{F}}^2 \\mid W^{t+1}\\right] \\\\\n&= (1-q)\\left\\|G^t - \\nabla f(W^t)\\right\\|_{\\mathrm{F}}^2 + \\frac{1-q}{M^2}\\sum_{l=1}^M \\mathbb{E}\\left[\\left\\|\\mathcal{Q}_l^t\\left(D_l^{t+1}\\right) - D_l^{t+1}\\right\\|_{\\mathrm{F}}^2 \\mid W^{t+1}\\right] \\\\\n&\\stackrel{(116)}{\\leq} (1-q)\\left\\|G^t - \\nabla f(W^t)\\right\\|_{\\mathrm{F}}^2 + \\frac{(1-q)\\omega}{M^2}\\sum_{l=1}^M \\left\\|\\nabla f_l(W^{t+1}) - \\nabla f_l(W^t)\\right\\|_{\\mathrm{F}}^2 \\\\\n&\\leq (1-q)\\left\\|G^t - \\nabla f(W^t)\\right\\|_{\\mathrm{F}}^2 + \\frac{(1-q)\\omega L^2}{M}\\left\\|W^{t+1} - W^t\\right\\|_{\\mathrm{F}}^2,\n\\end{aligned}\n$$\nwhere in the last inequality we used that the gradient of each $f_l$ is Lipschitz continuous. â–¡",
      "index": 120,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "D.2.1 Convergence for Smooth Non-Convex Functions\n\nTheorem 19. Let Assumptions 1, 2, 3, and 4 hold, and let the stepsize satisfy\n$$\n0 < \\gamma \\leq \\frac{1}{L\\left(1 + \\sqrt{\\lambda_{\\max}^p \\cdot \\frac{1-q}{q} \\cdot \\frac{\\omega}{M}}\\right)}.\n$$\nThen the iterates of Fed-Bernoulli-LoRA-MARINA (Algorithm 8) satisfy\n$$\n\\mathbb{E}\\left[\\left\\|\\nabla f(\\widetilde{W}^T)\\right\\|_{\\mathrm{F}}^2\\right] \\leq \\frac{2\\left(f(W^0) - f^*\\right)}{\\gamma \\lambda_{\\min}^p T} + \\frac{\\left\\|G^0 - \\nabla f(W^0)\\right\\|_{\\mathrm{F}}^2}{qT} \\cdot \\frac{\\lambda_{\\max}^p}{\\lambda_{\\min}^p}.\n$$\n(124)\nwhere $\\lambda_{\\min}^p := p\\lambda_{\\min}^{H_B} + (1-p)\\lambda_{\\min}^{H_A}$, $\\lambda_{\\max}^p := p\\lambda_{\\max}^{H_B} + (1-p)\\lambda_{\\max}^{H_A}$, and $\\widetilde{W}^T$ is drawn uniformly at random from the iterate sequence $\\{W^0, W^1, \\ldots, W^{T-1}\\}$.\n\nProof. Denote Lyapunov function $\\Phi_t$ as follows\n$$\n\\Phi_t = f(W^t) - f^* + \\frac{\\gamma \\lambda_{\\max}^p}{2q}\\left\\|G^t - \\nabla f(W^t)\\right\\|_{\\mathrm{F}}^2.\n$$\n(125)\nBy Lemma 5 and Lemma 9, we have\n$$\n\\begin{aligned}\n\\mathbb{E}[\\Phi_{t+1}] &\\leq \\mathbb{E}\\left[f(W^t)\\right] - f^* - \\frac{\\gamma \\lambda_{\\min}^p}{2}\\mathbb{E}\\left[\\left\\|\\nabla f(W^t)\\right\\|_{\\mathrm{F}}^2\\right] - \\left(\\frac{1}{2\\gamma} - \\frac{L}{2}\\right)\\mathbb{E}\\left[\\left\\|W^{t+1} - W^t\\right\\|_{\\mathrm{F}}^2\\right] \\\\\n&\\quad + \\frac{\\gamma \\lambda_{\\max}^p}{2}\\mathbb{E}\\left[\\left\\|G^t - \\nabla f(W^t)\\right\\|_{\\mathrm{F}}^2\\right] + \\frac{\\gamma(1-q)\\lambda_{\\max}^p}{2q}\\mathbb{E}\\left[\\left\\|G^t - \\nabla f(W^t)\\right\\|_{\\mathrm{F}}^2\\right] \\\\\n&\\quad + \\frac{\\gamma(1-q)L^2\\omega\\lambda_{\\max}^p}{2qM}\\mathbb{E}\\left[\\left\\|W^{t+1} - W^t\\right\\|_{\\mathrm{F}}^2\\right] \\\\\n&\\leq \\mathbb{E}[\\Phi_t] - \\frac{\\gamma \\lambda_{\\min}^p}{2}\\mathbb{E}\\left[\\left\\|\\nabla f(W^t)\\right\\|_{\\mathrm{F}}^2\\right] - \\left(\\frac{1}{2\\gamma} - \\frac{L}{2} - \\frac{\\gamma(1-q)L^2\\omega\\lambda_{\\max}^p}{2qM}\\right)\\mathbb{E}\\left[\\left\\|W^{t+1} - W^t\\right\\|_{\\mathrm{F}}^2\\right].\n$$",
      "index": 121,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Selecting 0 < Î³ â‰¤ 1/(L(1 + âˆš(Î»_max^p Â· (1âˆ’q)/q Â· Ï‰/M))), we obtain\nE[Î¦_{t+1}] â‰¤ E[Î¦_t] âˆ’ (Î³ Î»_min^p / 2) E[â€–âˆ‡f(W^t)â€–_F^2].\n\nSumming over, we get\n(Î³ Î»_min^p / 2) âˆ‘_{t=0}^{Tâˆ’1} E[â€–âˆ‡f(W^t)â€–_F^2] â‰¤ E[Î¦_0] âˆ’ E[Î¦_T].\n\nFinally, we derive\nE[â€–âˆ‡f(áº†^T)â€–_F^2] â‰¤ 2 Î¦_0/(Î»_min^p Î³ T),\nwhere áº†^T is drawn uniformly at random from the iterate sequence {W^0, W^1, â€¦, W^{Tâˆ’1}}. â–¡",
      "index": 122,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "D.2.2 Convergence under Polyak-Åojasiewicz Condition\n\nTheorem 20. Let Assumptions 1, 2, 3, and 6 hold, and let the stepsize satisfy\n0 < Î³ â‰¤ min{ 1/(L(1 + âˆš(2 Î»_max^p Â· (1âˆ’q)/q Â· Ï‰/M))), q/(2 Î¼ Î»_min^p) }.\nThen the iterates of Fed-Bernoulli-LoRA-MARINA (Algorithm 8) satisfy\nE[f(W^T) âˆ’ f^*] â‰¤ (1 âˆ’ Î³ Î¼ Î»_min^p)^T Î¦_0, (126)\nwhere Î»_min^p := p Î»_min^{H_B} + (1âˆ’p) Î»_min^{H_A}, Î»_max^p := p Î»_max^{H_B} + (1âˆ’p) Î»_max^{H_A}, and Î¦_0 = f(W^0) âˆ’ f^* + (Î³ Î»_max^p / q) â€–G^0 âˆ’ âˆ‡f(W^0)â€–_F^2.\n\nProof. Denote Lyapunov function Î¦_t as follows\nÎ¦_t = f(W^t) âˆ’ f^* + (Î³ Î»_max^p / q) â€–G^t âˆ’ âˆ‡f(W^t)â€–_F^2. (127)\n\nBy Lemma 5 and Lemma 7, we have\nE[Î¦_{t+1}] â‰¤ E[f(W^t)] âˆ’ f^* âˆ’ (Î³ Î»_min^p / 2) E[â€–âˆ‡f(W^t)â€–_F^2] âˆ’ (1/(2Î³) âˆ’ L/2) E[â€–W^{t+1} âˆ’ W^tâ€–_F^2]\n+ (Î³ Î»_max^p / 2) E[â€–G^t âˆ’ âˆ‡f(W^t)â€–_F^2] + (Î³ (1âˆ’q) Î»_max^p / q) E[â€–G^t âˆ’ âˆ‡f(W^t)â€–_F^2]\n+ (Î³ (1âˆ’q) L^2 Î»_max^p / q) Â· (Ï‰/M) E[â€–W^{t+1} âˆ’ W^tâ€–_F^2]\nâ‰¤ (1 âˆ’ Î³ Î¼ Î»_min^p) E[f(W^t) âˆ’ f^*] + (1 âˆ’ q/2) (Î³ Î»_max^p / q) E[â€–G^t âˆ’ âˆ‡f(W^t)â€–_F^2]\nâˆ’ (1/(2Î³) âˆ’ L/2 âˆ’ Î³ (1âˆ’q) L^2 Î»_max^p / q Â· Ï‰/M) E[â€–W^{t+1} âˆ’ W^tâ€–_F^2].",
      "index": 123,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "where in the last inequality we used Assumption 6. Selecting\n$$0<\\gamma\\leq \\min\\left\\{\\frac{1}{L\\left(1+\\sqrt{\\frac{2(1-q)\\omega}{qM}}\\,\\lambda_{\\max}^p\\right)},\\ \\frac{q}{2\\mu\\,\\lambda_{\\min}^p}\\right\\},$$\nwe obtain\n$$\\mathbb{E}[\\Phi_{t+1}] \\le (1-\\gamma\\mu\\,\\lambda_{\\min}^p)\\,\\mathbb{E}[\\Phi_t].$$\nTaking recursion, we have\n$$\\mathbb{E}[\\Phi_T] \\le (1-\\gamma\\mu\\,\\lambda_{\\min}^p)^T\\,\\Phi_0.$$\nâ–¡",
      "index": 124,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "D.3 Analysis of Fed-Bernoulli-LoRA-EF21",
      "index": 125,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Algorithm 9 Fed-Bernoulli-LoRA-EF21\n\n1: Parameters: pre-trained model W^0 âˆˆ R^{mÃ—n}, {G_l^0}_{lâˆˆ[M]} âˆˆ R^{mÃ—n}, rank r â‰ª min{m,n}, scaling factor Î± > 0, chain length T, sketch distribution ð’Ÿ_S^B or ð’Ÿ_S^A, Bernoulli probability p\n2: for t = 0, 1, â€¦, T âˆ’ 1 do\n3:    Sample c^t âˆ¼ Be(p)\n4:    if c^t = 1 then\n5:       Sample B_S^t âˆ¼ ð’Ÿ_S^B\n6:       Ã‚^t = âˆ’Î· ((B_S^t)^âŠ¤ B_S^t)^â€  (B_S^t)^âŠ¤ G^t\n7:       W^{t+1} = W^t + (Î±/r) B_S^t Ã‚^t\n8:    else\n9:       Sample A_S^t âˆ¼ ð’Ÿ_S^A\n10:      BÌ‚^t = âˆ’Î· G^t (A_S^t)^âŠ¤ (A_S^t (A_S^t)^âŠ¤)^â€ \n11:      W^{t+1} = W^t + (Î±/r) BÌ‚^t A_S^t\n12:   end if\n13:   Broadcast W^{t+1} to each client l âˆˆ [M]\n14:   for each client l âˆˆ [M] do\n15:      Compute gradient âˆ‡f_l(W^{t+1})\n16:      G_l^{t+1} = G_l^t + C_l^t(âˆ‡f_l(W^{t+1}) âˆ’ G_l^t)\n17:   end for\n18:   Aggregate on the server\n19:   G^{t+1} = (1/M) âˆ‘_{l=1}^M G_l^{t+1}\n20: end for",
      "index": 126,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Compared to Fed-Bernoulli-LoRA-MARINA, in this section we work with the wider class of compression operators called contractive.\n\nDefinition 4. A randomized operator C : R^{mÃ—n} â†’ R^{mÃ—n} is called a contractive compression operator (compressor) if it satisfies the following condition: there exists a constant 0 < Î² < 1 such that E[â€–C(W) âˆ’ Wâ€–_F^2] â‰¤ (1 âˆ’ Î²)â€–Wâ€–_F^2, âˆ€ W âˆˆ R^{mÃ—n}. (128)\n\nW^{t+1} = W^t âˆ’ Î³ Äœ^t, where\nÄœ^t = { H_B^t G^t, with probability p\n        G^t H_A^t, with probability 1 âˆ’ p. } (129)\n\nThe iterates of Fed-Bernoulli-LoRA-EF21 can be rewritten as follows:\nG_l^{t+1} = G_l^t + C_l^t(âˆ‡f_l(W^{t+1}) âˆ’ G_l^t), âˆ€ l âˆˆ [M]. (130)",
      "index": 127,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Lemma 10. Let Assumption 3 hold. Then the iterates generated by Fed-Bernoulli-LoRA-EF21 (Algorithm 9) satisfy\n$$\n\\mathbb{E}\\left[\\left\\|G_l^{t+1} - \\nabla f_l(W^{t+1})\\right\\|_{\\mathrm{F}}^2\\right] \\leq \\sqrt{1-\\beta}\\,\\mathbb{E}\\left[\\left\\|G_l^t - \\nabla f_l(W^t)\\right\\|_{\\mathrm{F}}^2\\right] + \\frac{(1-\\beta)L^2}{1-\\sqrt{1-\\beta}}\\,\\mathbb{E}\\left[\\left\\|W^{t+1} - W^t\\right\\|_{\\mathrm{F}}^2\\right].\n$$\n\nProof. For each $l \\in [M]$ we have\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\left\\|G_l^{t+1} - \\nabla f_l(W^{t+1})\\right\\|_{\\mathrm{F}}^2\\right] &\\stackrel{(130),(131)}{=} \\mathbb{E}\\left[\\mathbb{E}\\left[\\left\\|\\mathcal{C}_l^t\\left(\\nabla f_l(W^{t+1}) - G_l^t\\right) - \\left(\\nabla f_l(W^{t+1}) - G_l^t\\right)\\right\\|_{\\mathrm{F}}^2 \\,\\middle|\\, G_l^t, W^{t+1}\\right]\\right] \\\\\n&\\stackrel{(128)}{\\leq} (1-\\beta)\\,\\mathbb{E}\\left[\\left\\|G_l^t - \\nabla f_l(W^{t+1})\\right\\|_{\\mathrm{F}}^2\\right] \\\\\n&\\leq (1-\\beta)(1+\\theta)\\,\\mathbb{E}\\left[\\left\\|G_l^t - \\nabla f_l(W^t)\\right\\|_{\\mathrm{F}}^2\\right] \\\\\n&\\quad + (1-\\beta)\\left(1+\\frac{1}{\\theta}\\right)\\,\\mathbb{E}\\left[\\left\\|\\nabla f_l(W^{t+1}) - \\nabla f_l(W^t)\\right\\|_{\\mathrm{F}}^2\\right],\n\\end{aligned}\n$$\nwhere in the last inequality we used $\\|U + V\\|_{\\mathrm{F}}^2 \\leq (1+\\theta)\\|U\\|_{\\mathrm{F}}^2 + \\left(1+\\frac{1}{\\theta}\\right)\\|V\\|_{\\mathrm{F}}^2$ for any constant $\\theta > 0$, and matrices $U,V \\in \\mathbb{R}^{m \\times n}$. Taking $\\theta = \\frac{1}{\\sqrt{1-\\beta}} - 1$, we acquire\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\left\\|G_l^{t+1} - \\nabla f_l(W^{t+1})\\right\\|_{\\mathrm{F}}^2\\right] &\\leq \\sqrt{1-\\beta}\\,\\mathbb{E}\\left[\\left\\|G_l^t - \\nabla f_l(W^t)\\right\\|_{\\mathrm{F}}^2\\right] + \\frac{1-\\beta}{1-\\sqrt{1-\\beta}}\\,\\mathbb{E}\\left[\\left\\|\\nabla f_l(W^{t+1}) - \\nabla f_l(W^t)\\right\\|_{\\mathrm{F}}^2\\right] \\\\\n&\\leq \\sqrt{1-\\beta}\\,\\mathbb{E}\\left[\\left\\|G_l^t - \\nabla f_l(W^t)\\right\\|_{\\mathrm{F}}^2\\right] + \\frac{(1-\\beta)L^2}{1-\\sqrt{1-\\beta}}\\,\\mathbb{E}\\left[\\left\\|W^{t+1} - W^t\\right\\|_{\\mathrm{F}}^2\\right],\n\\end{aligned}\n$$\nwhere in the last inequality we used that the gradient of each $f_l$ is Lipschitz continuous. Summing over $l$ from $1$ to $M$, we finish the proof. â–¡",
      "index": 128,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "D.3.1 Convergence for Smooth Non-Convex Functions\n\nTheorem 21. Let Assumptions 1, 2, and 3 hold, and let the stepsize satisfy\n$$\n0 < \\gamma \\leq \\frac{1}{L\\left(1 + \\sqrt{\\frac{\\lambda_{\\max}^p(1-\\beta)}{1-\\sqrt{1-\\beta}}}\\right)}.\n$$\nThen the iterates of Fed-Bernoulli-LoRA-EF21 (Algorithm 9) satisfy\n$$\n\\mathbb{E}\\left[\\left\\|\\nabla f(\\widetilde{W}^T)\\right\\|_{\\mathrm{F}}^2\\right] \\leq \\frac{2\\big(f(W^0) - f^*\\big)}{\\gamma\\,\\lambda_{\\min}^p\\,T} + \\frac{\\mathcal{G}^0}{(1-\\sqrt{1-\\beta})\\,T}\\cdot \\frac{\\lambda_{\\max}^p}{\\lambda_{\\min}^p}, \\tag{132}\n$$\nwhere $\\lambda_{\\min}^p := p\\,\\lambda_{\\min}^{H_B} + (1-p)\\,\\lambda_{\\min}^{H_A}$, and $\\lambda_{\\max}^p := p\\,\\lambda_{\\max}^{H_B} + (1-p)\\,\\lambda_{\\max}^{H_A}$. $\\widetilde{W}^T$ is drawn uniformly at random from the iterate sequence $\\{W^0, W^1, \\ldots, W^{T-1}\\}$, and $\\mathcal{G}^0 := \\frac{1}{M}\\sum_{l=1}^M \\left\\|G_l^0 - \\nabla f_l(W^0)\\right\\|_{\\mathrm{F}}^2$.\n\nProof. Denote Lyapunov function $\\Phi_t$ as follows\n$$\n\\Phi_t = f(W^t) - f^* + \\frac{\\gamma\\,\\lambda_{\\max}^p}{2\\left(1 - \\sqrt{1-\\beta}\\right)} \\cdot \\frac{1}{M}\\sum_{l=1}^M \\left\\|G_l^t - \\nabla f_l(W^t)\\right\\|_{\\mathrm{F}}^2. \\tag{133}\n$$",
      "index": 129,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "By Lemma 5 and Lemma 10, we have\n$$\n\\mathbb{E}[\\Phi_{t+1}] \\leq \\mathbb{E}\\left[f(W^t)\\right] - f^* - \\frac{\\gamma\\lambda_{\\min}^p}{2}\\mathbb{E}\\left[\\left\\|\\nabla f(W^t)\\right\\|_{\\mathrm{F}}^2\\right] - \\left(\\frac{1}{2\\gamma} - \\frac{L}{2}\\right)\\mathbb{E}\\left[\\left\\|W^{t+1} - W^t\\right\\|_{\\mathrm{F}}^2\\right]\n$$\n$$\n+ \\frac{\\gamma\\lambda_{\\max}^p}{2}\\mathbb{E}\\left[\\left\\|G^t - \\nabla f(W^t)\\right\\|_{\\mathrm{F}}^2\\right] + \\frac{\\gamma\\lambda_{\\max}^p\\sqrt{1-\\beta}}{2(1-\\sqrt{1-\\beta})} \\cdot \\frac{1}{M}\\sum_{l=1}^M\\mathbb{E}\\left[\\left\\|G_l^t - \\nabla f_l(W^t)\\right\\|_{\\mathrm{F}}^2\\right]\n$$\n$$\n+ \\frac{\\gamma\\lambda_{\\max}^p L^2(1-\\beta)}{2(1-\\sqrt{1-\\beta})^2}\\mathbb{E}\\left[\\left\\|W^{t+1} - W^t\\right\\|_{\\mathrm{F}}^2\\right]\n$$\n$$\n\\leq \\mathbb{E}[\\Phi_t] - \\frac{\\gamma\\lambda_{\\min}^p}{2}\\mathbb{E}\\left[\\left\\|\\nabla f(W^t)\\right\\|_{\\mathrm{F}}^2\\right] - \\left(\\frac{1}{2\\gamma} - \\frac{L}{2} - \\frac{\\gamma\\lambda_{\\max}^p L^2(1-\\beta)}{2(1-\\sqrt{1-\\beta})^2}\\right)\\mathbb{E}\\left[\\left\\|W^{t+1} - W^t\\right\\|_{\\mathrm{F}}^2\\right].\n$$\nSelecting $0 < \\gamma \\leq \\frac{1}{L\\left(1 + \\frac{\\sqrt{2\\lambda_{\\max}^p(1-\\beta)}}{1-\\sqrt{1-\\beta}}\\right)}$, we obtain\n$$\n\\mathbb{E}[\\Phi_{t+1}] \\leq \\mathbb{E}[\\Phi_t] - \\frac{\\gamma\\lambda_{\\min}^p}{2}\\mathbb{E}\\left[\\left\\|\\nabla f(W^t)\\right\\|_{\\mathrm{F}}^2\\right].\n$$\nSumming over $t$ from $0$ to $T-1$, we get\n$$\n\\frac{\\gamma\\lambda_{\\min}^p}{2}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla f(W^t)\\right\\|_{\\mathrm{F}}^2\\right] \\leq \\mathbb{E}[\\Phi_0] - \\mathbb{E}[\\Phi_T].\n$$\nFinally, dividing both sides by $\\frac{\\gamma\\lambda_{\\min}^p}{2}$ yields\n$$\n\\mathbb{E}\\left[\\left\\|\\nabla f(\\widetilde{W}^T)\\right\\|_{\\mathrm{F}}^2\\right] \\leq \\frac{2\\Phi_0}{\\gamma\\lambda_{\\min}^p T},\n$$\nwhere $\\widetilde{W}^T$ is drawn uniformly at random from the iterate sequence $\\{W^0, W^1, \\ldots, W^{T-1}\\}$. â–¡",
      "index": 130,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "D.3.2 Convergence under Polyak-Åojasiewicz Condition\n\nTheorem 22. Let Assumptions 1, 2, 3, and 6 hold, and let the stepsize satisfy\n$$\n0 < \\gamma \\leq \\min\\left\\{\\frac{1}{L\\left(1 + \\frac{\\sqrt{2\\lambda_{\\max}^p(1-\\beta)}}{1-\\sqrt{1-\\beta}}\\right)}, \\frac{1 + \\sqrt{1-\\beta}}{2\\mu\\lambda_{\\min}^p}\\right\\}.\n$$\nThen the iterates of Fed-Bernoulli-LoRA-EF21 (Algorithm 9) satisfy\n$$\n\\mathbb{E}\\left[f(W^T) - f^*\\right] \\leq (1 - \\gamma\\mu\\lambda_{\\min}^p)^T\\Phi_0, \\tag{134}\n$$\nwhere $\\lambda_{\\min}^p := p\\lambda_{\\min}^{H_B} + (1-p)\\lambda_{\\min}^{H_A}$, $\\lambda_{\\max}^p := p\\lambda_{\\max}^{H_B} + (1-p)\\lambda_{\\max}^{H_A}$, and $\\Phi_0 = f(W^0) - f^* + \\frac{\\gamma\\lambda_{\\max}^p}{1-\\sqrt{1-\\beta}} \\cdot \\frac{1}{M}\\sum_{l=1}^M\\left\\|G_l^0 - \\nabla f_l(W^0)\\right\\|_{\\mathrm{F}}^2$.\n\nProof. Denote Lyapunov function $\\Phi_t$ as follows:\n$\n\\Phi_t = f(W^t) - f^* + \\frac{\\gamma\\lambda_{\\max}^p}{1-\\sqrt{1-\\beta}} \\cdot \\frac{1}{M}\\sum_{l=1}^M\\left\\|G_l^t - \\nabla f_l(W^t)\\right\\|_{\\mathrm{F}}^2. \\tag{135}\n$",
      "index": 131,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "By Lemma 5 and Lemma 10, we have\n$$\\mathbb{E}\\!\\left[\\Phi_{t+1}\\right] \\le \\mathbb{E}\\!\\left[f(W^t)\\right]-f^*-\\frac{\\gamma\\lambda_{\\min}^p}{2}\\,\\mathbb{E}\\!\\left[\\|\\nabla f(W^t)\\|_F^2\\right]-\\left(\\frac{1}{2\\gamma}-\\frac{L}{2}\\right)\\mathbb{E}\\!\\left[\\|W^{t+1}-W^t\\|_F^2\\right]$$\n$$+\\frac{\\gamma\\lambda_{\\max}^p}{2}\\,\\mathbb{E}\\!\\left[\\|G^t-\\nabla f(W^t)\\|_F^2\\right]+\\frac{\\gamma\\lambda_{\\max}^p\\sqrt{1-\\beta}}{1-\\sqrt{1-\\beta}}\\cdot\\frac{1}{M}\\sum_{l=1}^M\\mathbb{E}\\!\\left[\\|G_l^t-\\nabla f_l(W^t)\\|_F^2\\right]$$\n$$+\\frac{\\gamma\\lambda_{\\max}^p(1-\\beta)L^2}{(1-\\sqrt{1-\\beta})^2}\\,\\mathbb{E}\\!\\left[\\|W^{t+1}-W^t\\|_F^2\\right]$$\n$$\\le \\left(1-\\gamma\\mu\\lambda_{\\min}^p\\right)\\mathbb{E}\\!\\left[f(W^t)-f^*\\right]+\\frac{\\gamma\\lambda_{\\max}^p(1+\\sqrt{1-\\beta})}{2(1-\\sqrt{1-\\beta})}\\cdot\\frac{1}{M}\\sum_{l=1}^M\\mathbb{E}\\!\\left[\\|G_l^t-\\nabla f_l(W^t)\\|_F^2\\right]$$\n$$-\\left(\\frac{1}{2\\gamma}-\\frac{L}{2}-\\frac{\\gamma\\lambda_{\\max}^p(1-\\beta)L^2}{(1-\\sqrt{1-\\beta})^2}\\right)\\mathbb{E}\\!\\left[\\|W^{t+1}-W^t\\|_F^2\\right],$$\nwhere in the last inequality we used Assumption 6. Selecting\n$$0<\\gamma\\le \\min\\left\\{\\frac{1}{L\\!\\left(1+\\frac{\\sqrt{2\\lambda_{\\max}^p(1-\\beta)}}{1-\\sqrt{1-\\beta}}\\right)},\\ \\frac{1+\\sqrt{1-\\beta}}{2\\mu\\lambda_{\\min}^p}\\right\\},$$\nwe obtain\n$$\\mathbb{E}\\!\\left[\\Phi_{t+1}\\right]\\le\\left(1-\\gamma\\mu\\lambda_{\\min}^p\\right)\\mathbb{E}\\!\\left[\\Phi_t\\right].$$\nTaking the recursion, we have\n$$\\mathbb{E}\\!\\left[\\Phi_T\\right]\\le\\left(1-\\gamma\\mu\\lambda_{\\min}^p\\right)^T\\Phi_0.$$\nâ–¡",
      "index": 132,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "E Experiments: Missing Details\nIn this section, we provide additional details regarding the experimental setting from Section 8.",
      "index": 133,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "E.1 Linear Regression with Non-convex Regularization\nFull gradient setting. We begin by evaluating these methods in a standard optimization setting where full gradients are computed at each iteration. In this regime, we compare Bernoulli-LoRA-GD and RAC-LoRA-GD.\n(a) Rank r = 1.\n(b) Rank r = 2.\nFigure 2: Comparison of RAC-LoRA-GD and Bernoulli-LoRA-GD on linear regression fine-tuning. Curves with p = 0.01, 0.2, â€¦ indicate Bernoulli-LoRA-GD sampling parameters. RAC-LoRA-GD(A) trains B after resampling A, while RAC-LoRA-GD(B) does the reverse. All methods use Î³ = c/LÌ‚ with c âˆˆ {1, 2} tuned individually.\nFigure 2 shows that, across all tested probabilities, Bernoulli-LoRA-GD and both variants of RAC-LoRA-GD exhibit similar convergence on the linear regression task. This numerical stability suggests that the ratio of updates between A and B has little effect on the performance for this problem. We also observe that higher ranks r produce faster convergence, which aligns with the theoretical r/n factor in our analysis.",
      "index": 134,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Hardware and Software. All algorithms were implemented in Python 3.10 and executed on three different CPU cluster node types:\n1. AMD EPYC 7702 64-Core,\n2. Intel(R) Xeon(R) Gold 6148 CPU @ 2.40 GHz,\n3. Intel(R) Xeon(R) Gold 6248 CPU @ 2.50 GHz.",
      "index": 135,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Implementation Details. For each method, we set the stepsize to Î³ = c/LÌ‚, where c is a constant multiplier tuned individually for every algorithm. Convergence was monitored by computing the squared norm of the full gradient at each iteration. The algorithms terminated when either a maximum iteration limit was reached or the criterion ||âˆ‡f(x^t)||_2^2 â‰¤ 5 Ã— 10^-16 was satisfied. To ensure reliability, each method was run 20 times using different random seeds, and all figures show the median performance over these trials.",
      "index": 136,
      "section": "Appendix"
    },
    {
      "type": "image_url",
      "image_url": {
        "url": "data:image/jpeg;base64,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"
      },
      "index": 137,
      "section": "Appendix"
    },
    {
      "type": "image_url",
      "image_url": {
        "url": "data:image/jpeg;base64,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"
      },
      "index": 138,
      "section": "Appendix"
    },
    {
      "type": "text",
      "text": "Datasets. The synthetic pre-training dataset $(\\tilde{D}, \\tilde{b})$ was generated using\n\n```\nsklearn.datasets.make_regression\n```\n\nwith moderate noise and a controlled rank structure:\n\n```\nwt_D, wt_b = make_regression(n_samples=90000, n_features=4096,\n                             n_informative=4096, noise=20.0,\n                             bias=0.0, tail_strength=0.8,\n                             effective_rank=64, random_state=42)\n```\n\nfollowed by standard scaling. The fine-tuning dataset $(\\hat{D}, \\hat{b})$ was produced similarly:\n\n```\nh_D, h_b = make_regression(n_samples=10000, n_features=4096,\n                           n_informative=4096//2, noise=50.0,\n                           bias=10.0, tail_strength=0.9,\n                           effective_rank=32, random_state=84)\n```\n\nand subsequently adjusted with a biased scaling (mean 1, standard deviation 2).",
      "index": 139,
      "section": "Appendix"
    }
  ],
  "images": [
    "images/page_15_picture_1.jpg",
    "images/page_63_picture_1.jpg",
    "images/page_63_picture_2.jpg"
  ],
  "section_labels": {
    "labels": [
      {
        "content_index": "1-2",
        "section": "Abstract"
      },
      {
        "content_index": "3-6",
        "section": "Introduction"
      },
      {
        "content_index": "7-8",
        "section": "Preliminaries"
      },
      {
        "content_index": "9-10",
        "section": "Motivation"
      },
      {
        "content_index": "11-16",
        "section": "Introduction"
      },
      {
        "content_index": "17",
        "section": "Preliminaries"
      },
      {
        "content_index": "18-39",
        "section": "Method"
      },
      {
        "content_index": "40-43",
        "section": "Experiments"
      },
      {
        "content_index": "44-45",
        "section": "Conclusion"
      },
      {
        "content_index": "46",
        "section": "Appendix"
      },
      {
        "content_index": "47-52",
        "section": "References"
      },
      {
        "content_index": "53-139",
        "section": "Appendix"
      }
    ],
    "model_used": "post_clean"
  }
}