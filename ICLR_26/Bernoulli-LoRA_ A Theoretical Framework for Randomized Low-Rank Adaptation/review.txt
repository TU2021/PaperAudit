### Summary

This submission proposes **Bernoulli-LoRA**, a theoretical framework for **parameter-efficient fine-tuning (PEFT)** that introduces a **Bernoulli random mechanism** to decide, at each iteration, whether to update one of the two LoRA low-rank factors (updating one while keeping the other fixed). The paper positions this as a unifying template for multiple LoRA-style update strategies and develops a broad suite of variants (GD/SGD, variance-reduced methods such as PAGE/MVR, and federated/compressed variants such as QGD/MARINA/EF21). Under standard assumptions from nonconvex optimization (smoothness, PŁ, and a “positive expected projection” type condition), it provides convergence guarantees, and extends to convex non-smooth settings (including Polyak-type step sizes). Experiments are presented on small-scale tasks (linear regression and MNIST) to illustrate convergence behavior.

Overall, reviewers view the work as **sound in terms of proofs and analysis**, with at least one reviewer explicitly confirming correctness of proofs, and another considering the stochastic masking idea original and appealing. However, several reviewers are **skeptical about the practical and/or theoretical novelty**, emphasizing strong similarity to prior **RAC-LoRA/COLA-style** asymmetric updates, limited empirical validation on foundation-model settings, and idealized assumptions whose realism for deep LoRA fine-tuning is not convincingly argued or empirically checked. Presentation and framing are also criticized by some reviewers, including missing related work on LoRA optimization dynamics and unclear early exposition/notation in the initial version.

### Strengths

* **Broad and rigorous convergence framework:** The paper provides a comprehensive set of convergence results spanning multiple optimization regimes (deterministic, stochastic, variance-reduced, and federated with compression/error feedback), which is valued by multiple reviewers.
* **Technically sound proofs:** At least one reviewer reports checking the proofs carefully and finding them correct; others consider the theoretical results reasonable as extensions of standard optimization tools.
* **Conceptually simple mechanism:** The Bernoulli choice of updating one factor per step is straightforward to implement and lends itself to a clean projected-gradient interpretation, which helps theoretical tractability.
* **Some empirical alignment with theory:** Reviewers note that the small-scale experiments are consistent with the predicted convergence behavior, even if they are not viewed as compelling evidence of superiority.

### Weaknesses

* **Novelty concerns relative to RAC-LoRA / existing asymmetric LoRA analyses:** Multiple reviewers argue Bernoulli-LoRA looks like a **minor stochastic modification** of RAC-LoRA (randomizing the left/right alternation), and that many results appear to be direct extensions/adaptations rather than fundamentally new insights. Reviewers request clearer, explicit comparison and tighter claims about what is genuinely new.
* **Simplified modeling of practical LoRA usage:** One reviewer criticizes that the theory appears to focus on LoRA applied to a single matrix, whereas practical LoRA is applied across many matrices and layers. Relatedly, reviewers say the optimization variable / problem statement was unclear in the initial writeup, risking confusion with full fine-tuning.
* **Assumptions may be unrealistic for deep networks / LoRA parameterization:** Reviewers point out reliance on smoothness/Lipschitz-type conditions and a “positive expected projection” assumption that may not hold in real deep LoRA training, especially given the non-smooth / non-Lipschitz nature of certain composed objectives. There is little empirical evidence that these assumptions hold (even approximately) in practical settings.
* **Weak empirical section for a PEFT paper:** Multiple reviewers find experiments too toy (linear regression, MNIST; small MLPs), and not representative of real PEFT usage (LLMs, ViTs, diffusion models, multimodal). The experiments do not convincingly demonstrate practical advantage over existing baselines, raising “why study this” concerns for some reviewers.
* **Presentation / related work gaps:** One reviewer rates presentation poorly, noting awkward structure/flow and missing discussion of recent LoRA optimization theory (NTK regime, gradient flow/dynamics, convergence-to-low-rank minima vs failure modes, lazy vs feature learning). There are also complaints about missing definitions of key symbols in theorem statements/equations.
* **Insufficient discussion of the role of pretraining:** A reviewer emphasizes that optimization in fine-tuning is inseparable from the existence of a pretrained model and the relation between pretraining and fine-tuning distributions; they argue the paper’s convergence perspective does not meaningfully address this uniqueness, weakening interpretability for “fine-tuning” as opposed to generic constrained optimization on a fixed objective.
