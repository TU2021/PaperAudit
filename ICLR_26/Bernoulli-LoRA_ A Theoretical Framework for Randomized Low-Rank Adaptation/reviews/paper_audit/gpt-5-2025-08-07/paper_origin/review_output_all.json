{
  "baseline_review": "Summary\n- The paper proposes Bernoulli-LoRA, a unified theoretical framework for low-rank adaptation in parameter-efficient fine-tuning that randomly selects which of the two LoRA factors (A or B) to optimize at each step (Algorithm 1, Section 6; Definitions 1‚Äì2). The update is reformulated as a projected gradient step in the full parameter space (Section 6.1; Equations (7)‚Äì(8), Lemma 3), enabling analysis under standard non-convex smooth, P≈Å, and non-smooth convex settings. Multiple algorithmic variants are analyzed: GD, SGD, MVR, PAGE, and federated extensions with quantization (QGD), gradient-difference compression (MARINA), and error feedback (EF21) (Section 6.2‚Äì6.3; Table 2). Convergence rates are established and summarized (Table 1; Theorems 1‚Äì7, 10, 12‚Äì22). Experiments on a linear regression with non-convex regularization and on MNIST with an MLP provide empirical validation, including a variance-reduction advantage for Bernoulli-LoRA-PAGE (Figure 1) and competitive accuracy on MNIST (Table 3; Appendix E).Strengths\n- Bold unified projected-gradient formulation\n  ‚Ä¢ Reformulating left/right sketch updates as projected gradient steps in the full parameter space (Section 6.1; Equations (7)‚Äì(8), Lemma 3; Equations (24), (30)) demonstrates technical soundness and avoids smoothness issues of the LoRA parameterization (Section 7, ‚ÄúA significant challenge...‚Äù; Section 6.1), enabling rigorous analysis.\n  ‚Ä¢ The definition and use of projection matrices H_B^t and H_A^t (Section 6.1; H_B^t and H_A^t definitions) provide a clean analytical handle via spectral properties, which is critical for non-convex convergence proofs (Theorem 1; inequalities (40)‚Äì(42)).\n  ‚Ä¢ The probability-weighted eigenvalue Œª_min^p and Œª_max^p unify left/right sketches and the Bernoulli mixing (Section 7; Table 1), aiding clarity and generality.- Broad algorithmic coverage with convergence\n  ‚Ä¢ The framework instantiates GD, SGD, variance-reduced (MVR, PAGE), and federated extensions (QGD, MARINA, EF21) with projected estimators (Section 6.2; Table 2), showing breadth and practical relevance.\n  ‚Ä¢ Convergence is provided under smooth non-convex and P≈Å conditions for all core variants (Theorems 1‚Äì7, 12‚Äì22; Table 1), evidencing technical completeness.\n  ‚Ä¢ Non-smooth convex analysis with both constant and Polyak-type stepsizes (Algorithm 3; Assumptions 7‚Äì10; Theorem 10) broadens applicability beyond differentiable objectives.- Sound treatment of projection assumptions\n  ‚Ä¢ Assumption 1 (positive expected projection; Section 7) is justified through Gaussian sketches, with proof that E[H]= (r/n) I (Appendix B; Lemma 2; Remark 1), aiding tractability and providing explicit Œª_min bounds.\n  ‚Ä¢ The use of rotational invariance and Schur-type arguments (Appendix B; Lemma 1 and Lemma 2) is technically correct and strengthens the framework‚Äôs foundations.\n  ‚Ä¢ Assumption 10 (E[H]=Œ± I; Section C.1.3) enables clean non-smooth analysis (Theorem 10), highlighting the value of symmetric sketching distributions.- Federated learning extensions with compression and EF\n  ‚Ä¢ The paper adapts quantization (QGD), gradient-difference compression (MARINA), and error feedback (EF21) to LoRA-style projected updates (Section 6.3; Algorithms 7‚Äì9), filling a gap between PEFT and communication-efficient FL.\n  ‚Ä¢ Convergence results explicitly quantify the impact of compression parameters œâ and Œ≤ (Table 1; Theorems 5‚Äì7; Assumption 11; Definition 3‚Äì4), demonstrating technical depth and practical relevance for distributed systems.\n  ‚Ä¢ MARINA and EF21 analyses reuse PAGE/MVR Lyapunov techniques adapted to projected estimators (Appendix D; Lemmas 9‚Äì10; Theorems 19‚Äì22), evidencing methodological maturity.- Empirical evidence consistent with theory\n  ‚Ä¢ Bernoulli-LoRA-PAGE reduces variance and converges to tight tolerances in the stochastic linear regression setting (Figure 1), matching the theoretical advantage of variance-reduced estimators (Theorem 4; Table 1).\n  ‚Ä¢ In MNIST PEFT, Bernoulli-LoRA achieves competitive accuracy vis-√†-vis RAC-LoRA while training one matrix per block (Table 3; discussion), aligning with the framework‚Äôs resource-efficiency goals (Section 4).\n  ‚Ä¢ Appendix E provides implementation and dataset details (Appendix E.1; hardware, stepsize policy Œ≥=c/ƒ§L), contributing to reproducibility and clarity.- Clear notation and organization\n  ‚Ä¢ Notation is defined upfront (Section 5) with consistent use of Frobenius norms, inner products, and pseudoinverses, helping technical clarity across proofs (e.g., Lemma 3; Lemmas 5‚Äì7).\n  ‚Ä¢ The content map (Section 1, Contents) and summary table of rates (Table 1) aid navigability and comprehension.Weaknesses\n- Limited empirical scope and mismatch with large-scale/FL scenarios\n  ‚Ä¢ Experiments are restricted to a synthetic linear regression and an MNIST MLP (Section 8.1‚Äì8.2), which may not reflect behavior on large models central to PEFT (impact; generalization).\n  ‚Ä¢ No federated experiments are reported for QGD/MARINA/EF21 variants despite extensive theory (Section 6.3; Algorithms 7‚Äì9; Theorems 5‚Äì7); No direct evidence found in the manuscript, limiting practical validation of communication savings and robustness.\n  ‚Ä¢ MNIST experiments use AdamW (Section 8.2) rather than the theorized update forms (e.g., GD/SGD/PAGE with W^{t+1}=W^t‚àíŒ≥ƒú^t in Equations (7)‚Äì(8)), introducing potential mismatch between theoretical guarantees and empirical setup (technical soundness).- Unstated independence assumptions in proofs\n  ‚Ä¢ The SGD analysis explicitly uses independence between H_B^t/H_A^t and G^t (Appendix C.2, step (97): ‚Äúin (*) we used that H_B^t, H_A^t and G^t are independent‚Äù), but such independence is not formalized as an assumption (clarity; rigor).\n  ‚Ä¢ Algorithm 1 (Section 6) does not state whether the Bernoulli variable c^t, sketches A_S^t/B_S^t, and stochastic gradients are mutually independent across time and sources (technical soundness).\n  ‚Ä¢ Federated analyses implicitly assume independence between compressors, local gradients, and sketches (Appendix D, Lemmas 8‚Äì10), but this is not stated explicitly; No direct evidence found in the manuscript (rigor).- Strong projection assumptions and practical alignment\n  ‚Ä¢ Assumption 1 and Lemma 2 hinge on Gaussian sketches to obtain E[H]= (r/n) I (Appendix B; Remark 1), while in practice LoRA often uses zero-initialized B or structured A (Section 6; Table 3 footnote ^1 indicates deterministic assignment), potentially misaligning assumptions and deployments (impact; realism).\n  ‚Ä¢ Assumption 10 (E[H]=Œ± I; Section C.1.3) may be restrictive beyond Gaussian/rotationally invariant designs; the manuscript does not discuss non-isotropic sketches (clarity; generality).\n  ‚Ä¢ Stepsize conditions depend on Œª_min^p, Œª_max^p (Table 1; Theorems 2‚Äì7, 10, 12‚Äì22), but practical estimation strategies for these quantities are not provided (reproducibility; guidance).- Limited theoretical/empirical comparison to RAC-LoRA/COLA\n  ‚Ä¢ While the framework claims to unify and extend LoRA approaches (Section 4), there is no formal complexity comparison to RAC-LoRA/COLA (e.g., tighter bounds or oracle complexity), beyond qualitative discussion; No direct evidence found in the manuscript (novelty positioning).\n  ‚Ä¢ Empirically, Bernoulli-LoRA-GD and RAC-LoRA-GD perform similarly in linear regression (Appendix E, Figure 2a‚Äìb), which does not substantiate clear advantages in this setting (impact).\n  ‚Ä¢ COLA is included in MNIST accuracy comparison (Table 3), but without analysis of chaining vs Bernoulli updates or convergence guarantees; No direct evidence found in the manuscript (completeness).- Sparse guidance on choosing p, sketch distributions, and T\n  ‚Ä¢ The probability p influences Œª_min^p and Œª_max^p (Table 1; Section 7), yet the manuscript offers no principled strategy for selecting p (clarity; practical utility).\n  ‚Ä¢ Sketch distributions ùíü_S^A/ùíü_S^B are left general (Algorithm 1), and Gaussian is discussed (Appendix B), but alternative distributions (e.g., orthonormal, data-informed) are not analyzed empirically (impact).\n  ‚Ä¢ Chain length T appears in rates (Table 1) but its interaction with p, variance, and convergence speed is not studied beyond Figure 1; No direct evidence found in the manuscript (practical guidance).- Minor clarity and editorial issues\n  ‚Ä¢ Appendix C.1 contains a residual mention of ‚ÄúRAC-LoRA‚Äù in the heading text (‚Äú‚Ä¶ main convergence result for RAC-LoRA with Gradient Descent updates.‚Äù), which seems unintended (Appendix C.1, before C.1.1) (clarity).\n  ‚Ä¢ Notational inconsistencies around hats in Definitions 1‚Äì2 (Section 6, Equations (5)‚Äì(6)) can momentarily confuse what is sampled vs optimized (clarity).\n  ‚Ä¢ Table 3 footnote ^1 states a deterministic assignment at initialization ‚Äúyielded better performance,‚Äù which deviates from the Bernoulli prescription and could be clarified as an implementation detail vs methodological change (Section 8.2; Table 3) (clarity).Suggestions for Improvement\n- Broaden empirical validation and align with theory\n  ‚Ä¢ Add federated experiments for QGD/MARINA/EF21 to demonstrate communication savings and robustness under data heterogeneity (Section 6.3; Algorithms 7‚Äì9; Theorems 5‚Äì7), measuring gradient bytes, compression parameters œâ/Œ≤, and convergence traces.\n  ‚Ä¢ Include larger-scale PEFT tasks (e.g., higher-dimensional layers or more complex datasets) and assess variance-reduction benefits consistent with Figure 1 and Theorem 4 (Section 8.1; Table 1).\n  ‚Ä¢ For MNIST, run ablations using SGD and PAGE (Equations (7)‚Äì(8)) in addition to AdamW to align with theoretical updates and report accuracy vs convergence diagnostics (Section 8.2).- Formalize and relax independence assumptions\n  ‚Ä¢ Introduce an explicit assumption detailing independence between c^t, A_S^t/B_S^t and stochastic gradients g(W^t), and between client compressors and sketches (Appendix C.2 step (97); Algorithm 1; Appendix D).\n  ‚Ä¢ Discuss and, where possible, extend proofs to handle mild dependencies (e.g., conditioning on sketches), noting where bounds change (Appendix C.2; Lemmas 5‚Äì7).\n  ‚Ä¢ For federated variants, specify independence between client-side randomness and server-side sketching to make Lemmas 8‚Äì10 fully rigorous (Appendix D).- Strengthen practicality of projection assumptions and parameter estimation\n  ‚Ä¢ Empirically verify Assumption 1 and Assumption 10 under commonly used LoRA initializations (e.g., zero-initialized B, Gaussian A) by estimating E[H] and eigenvalues on real layers; compare against Lemma 2 (Appendix B; Section 7).\n  ‚Ä¢ Provide guidance or bounds to estimate Œª_min^p and Œª_max^p in practice (Table 1; Theorems 2‚Äì7, 10, 12‚Äì22), perhaps via Monte Carlo sketching diagnostics over ùíü_S^A/ùíü_S^B.\n  ‚Ä¢ Discuss non-isotropic sketches and offer relaxed results that assume only lower-bounds on the minimal eigenvalue of E[H] (Section C.1.3; Theorem 10), improving generality beyond Œ± I.- Deepen comparisons to RAC-LoRA and COLA\n  ‚Ä¢ Add theoretical discussion contrasting rates or constants under common assumptions, indicating when Bernoulli mixing improves Œª_min^p or variance control relative to RAC-LoRA/COLA (Section 4; Table 1).\n  ‚Ä¢ Expand experiments beyond Appendix E Figure 2 to scenarios where variance-reduction and Bernoulli mixing should shine (e.g., higher noise), quantifying advantages vs RAC-LoRA (Appendix E).\n  ‚Ä¢ Include analyses or ablations that explain MNIST results vis-√†-vis COLA (Table 3), e.g., parameter counts, chaining length vs Bernoulli probability p, and convergence behavior.- Provide practical tuning guidance for p, sketches, and T\n  ‚Ä¢ Derive or suggest heuristics to choose p (e.g., maximizing Œª_min^p subject to Œª_max^p constraints; Section 7; Table 1), and validate experimentally across a grid, extending Figure 1.\n  ‚Ä¢ Evaluate alternative sketch distributions (orthonormal, data-dependent) beyond Gaussian (Appendix B), adding empirical results to show their effect on Œª_min^p/Œª_max^p and performance.\n  ‚Ä¢ Study the interaction of chain length T with p and variance (Table 1), reporting diminishing returns or optimal regimes on both tasks and in federated settings.- Resolve clarity and editorial issues\n  ‚Ä¢ Correct the ‚ÄúRAC-LoRA‚Äù mention in Appendix C.1 to ‚ÄúBernoulli-LoRA‚Äù and check for similar residual references.\n  ‚Ä¢ Standardize hat notation in Definitions 1‚Äì2 to clearly distinguish sampled vs optimized matrices (Section 6; Equations (5)‚Äì(6)).\n  ‚Ä¢ Clarify Table 3 footnote ^1 to indicate whether deterministic assignment at initialization modifies the method or is an implementation detail, and report results with fully Bernoulli selection for completeness (Section 8.2).Score\n- Overall (10): 7 ‚Äî Comprehensive theoretical framework with projected-gradient unification and convergence across GD/SGD/VR/FL (Section 6.1; Table 1; Theorems 1‚Äì7, 10), but limited empirical scope (Section 8; No direct federated experiments).\n- Novelty (10): 7 ‚Äî Randomized Bernoulli selection that unifies left/right sketches and extends to VR and FL settings (Algorithms 1, 5‚Äì9; Table 2) is a meaningful advance, though formal advantages over RAC-LoRA/COLA are not fully quantified (No direct evidence found).\n- Technical Quality (10): 8 ‚Äî Proofs are careful and leverage spectral properties (Lemma 3; Lemmas 5‚Äì7; Theorems 1‚Äì7, 10, 12‚Äì22), with well-justified assumptions (Assumption 1; Lemma 2), albeit independence assumptions are not explicitly stated (Appendix C.2, step (97)).\n- Clarity (10): 7 ‚Äî Well-structured with clear notation and summaries (Section 5; Table 1; Table 2), but minor editorial issues and limited practical guidance for selecting p and sketches (Section 8.2; Appendix C.1; Assumption 10).\n- Confidence (5): 4 ‚Äî The theoretical sections are detailed and internally consistent (Table 1; Appendices C‚ÄìD), but some practical claims and empirical validations (especially FL) are missing, warranting moderate confidence.",
  "final_review": "Summary\n- The paper proposes Bernoulli-LoRA, a unified theoretical framework for parameter-efficient fine-tuning that randomly selects which of the two LoRA factors (A or B) to optimize at each step (Algorithm 1, Section 6; Definitions 1‚Äì2). The update is reformulated as a projected gradient step in the full parameter space (Section 6.1; Equations (7)‚Äì(8), Lemma 3), enabling analysis under standard non-convex smooth, P≈Å, and non-smooth convex settings. Multiple algorithmic variants are analyzed: GD, SGD, MVR, PAGE, and federated extensions with quantization (QGD), gradient-difference compression (MARINA), and error feedback (EF21) (Section 6.2‚Äì6.3; Table 2). Convergence rates are established and summarized (Table 1; Theorems 1‚Äì7, 10, 12‚Äì22). Experiments on a linear regression with non-convex regularization and on MNIST with an MLP provide empirical validation, including a variance-reduction advantage for Bernoulli-LoRA-PAGE (Figure 1) and competitive accuracy on MNIST (Table 3; Appendix E).Strengths\n- Bold unified projected-gradient formulation\n  ‚Ä¢ Reformulating left/right sketch updates as projected gradient steps in the full parameter space (Section 6.1; Equations (7)‚Äì(8), Lemma 3; Equations (24), (30)) demonstrates technical soundness and avoids smoothness issues of the LoRA parameterization (Section 7, ‚ÄúA significant challenge...‚Äù; Section 6.1), enabling rigorous analysis.\n  ‚Ä¢ The definition and use of projection matrices H_B^t and H_A^t (Section 6.1; H_B^t and H_A^t definitions) provide a clean analytical handle via spectral properties, which is critical for non-convex convergence proofs (Theorem 1; inequalities (40)‚Äì(42)).\n  ‚Ä¢ The probability-weighted eigenvalue Œª_min^p and Œª_max^p unify left/right sketches and the Bernoulli mixing (Section 7; Table 1), aiding clarity and generality.\n- Broad algorithmic coverage with convergence\n  ‚Ä¢ The framework instantiates GD, SGD, variance-reduced (MVR, PAGE), and federated extensions (QGD, MARINA, EF21) with projected estimators (Section 6.2; Table 2), showing breadth and practical relevance.\n  ‚Ä¢ Convergence is provided under smooth non-convex and P≈Å conditions for all core variants (Theorems 1‚Äì7, 12‚Äì22; Table 1), evidencing technical completeness.\n  ‚Ä¢ Non-smooth convex analysis with both constant and Polyak-type stepsizes (Algorithm 3; Assumptions 7‚Äì10; Theorem 10) broadens applicability beyond differentiable objectives.\n- Sound treatment of projection assumptions\n  ‚Ä¢ Assumption 1 (positive expected projection; Section 7) is justified through Gaussian sketches, with proof that E[H]= (r/n) I (Appendix B; Lemma 2; Remark 1), aiding tractability and providing explicit Œª_min bounds.\n  ‚Ä¢ The use of rotational invariance and Schur-type arguments (Appendix B; Lemma 1 and Lemma 2) is technically correct and strengthens the framework‚Äôs foundations.\n  ‚Ä¢ Assumption 10 (E[H]=Œ± I; Section C.1.3) enables clean non-smooth analysis (Theorem 10), highlighting the value of symmetric sketching distributions.\n- Federated learning extensions with compression and EF\n  ‚Ä¢ The paper adapts quantization (QGD), gradient-difference compression (MARINA), and error feedback (EF21) to LoRA-style projected updates (Section 6.3; Algorithms 7‚Äì9), filling a gap between PEFT and communication-efficient FL.\n  ‚Ä¢ Convergence results explicitly quantify the impact of compression parameters œâ and Œ≤ (Table 1; Theorems 5‚Äì7; Assumption 11; Definition 3‚Äì4), demonstrating technical depth and practical relevance for distributed systems.\n  ‚Ä¢ MARINA and EF21 analyses reuse PAGE/MVR Lyapunov techniques adapted to projected estimators (Appendix D; Lemmas 9‚Äì10; Theorems 19‚Äì22), evidencing methodological maturity.\n- Empirical evidence consistent with theory\n  ‚Ä¢ Bernoulli-LoRA-PAGE reduces variance and converges to tight tolerances in the stochastic linear regression setting (Figure 1), matching the theoretical advantage of variance-reduced estimators (Theorem 4; Table 1).\n  ‚Ä¢ In MNIST PEFT, Bernoulli-LoRA achieves competitive accuracy vis-√†-vis RAC-LoRA while training one matrix per block (Table 3; discussion), aligning with the framework‚Äôs resource-efficiency goals (Section 4).\n  ‚Ä¢ Appendix E.1 provides implementation details for the linear regression setting (Appendix E.1; hardware; stepsize policy Œ≥=c/ƒ§L), contributing to reproducibility for that experiment.\n- Clear notation and organization\n  ‚Ä¢ Notation is defined upfront (Section 5) with consistent use of Frobenius norms, inner products, and pseudoinverses, helping technical clarity across proofs (e.g., Lemma 3; Lemmas 5‚Äì7).\n  ‚Ä¢ The content map (Section 1, Contents) and summary table of rates (Table 1) aid navigability and comprehension.Weaknesses\n- Limited empirical scope and mismatch with large-scale/FL scenarios\n  ‚Ä¢ Experiments are restricted to a synthetic linear regression and an MNIST MLP (Section 8.1‚Äì8.2), which may not reflect behavior on large models central to PEFT (impact; generalization).\n  ‚Ä¢ No federated experiments are reported for QGD/MARINA/EF21 variants despite extensive theory (Section 6.3; Algorithms 7‚Äì9; Theorems 5‚Äì7); No direct evidence found in the manuscript, limiting practical validation of communication savings and robustness.\n  ‚Ä¢ MNIST experiments use AdamW (Section 8.2) rather than the theorized update forms (e.g., GD/SGD/PAGE with W^{t+1}=W^t‚àíŒ≥ƒú^t in Equations (7)‚Äì(8)), introducing potential mismatch between theoretical guarantees and empirical setup (technical soundness).\n- Unstated independence assumptions in proofs\n  ‚Ä¢ The SGD analysis explicitly uses independence between H_B^t/H_A^t and G^t (Appendix C.2, step (97): ‚Äúin (*) we used that H_B^t, H_A^t and G^t are independent‚Äù), but such independence is not formalized as an assumption (clarity; rigor).\n  ‚Ä¢ Algorithm 1 (Section 6) does not state whether the Bernoulli variable c^t, sketches A_S^t/B_S^t, and stochastic gradients are mutually independent across time and sources (technical soundness).\n  ‚Ä¢ Federated analyses implicitly assume independence between compressors, local gradients, and sketches (Appendix D, Lemmas 8‚Äì10), but this is not stated explicitly; No direct evidence found in the manuscript (rigor).\n- Strong projection assumptions and practical alignment\n  ‚Ä¢ Assumption 1 and Lemma 2 hinge on Gaussian sketches to obtain E[H]= (r/n) I (Appendix B; Remark 1), while in practice LoRA often uses zero-initialized B or structured A (Section 6; Table 3 footnote ^1 indicates deterministic assignment), potentially misaligning assumptions and deployments (impact; realism).\n  ‚Ä¢ Assumption 10 (E[H]=Œ± I; Section C.1.3) may be restrictive beyond Gaussian/rotationally invariant designs; the manuscript does not discuss non-isotropic sketches (clarity; generality).\n  ‚Ä¢ Stepsize conditions depend on Œª_min^p, Œª_max^p (Table 1; Theorems 2‚Äì7, 10, 12‚Äì22), but practical estimation strategies for these quantities are not provided (reproducibility; guidance).\n- Limited theoretical/empirical comparison to RAC-LoRA/COLA\n  ‚Ä¢ While the framework claims to unify and extend LoRA approaches (Section 4), there is no formal complexity comparison to RAC-LoRA/COLA (e.g., tighter bounds or oracle complexity), beyond qualitative discussion; No direct evidence found in the manuscript (novelty positioning).\n  ‚Ä¢ Empirically, Bernoulli-LoRA-GD and RAC-LoRA-GD perform similarly in linear regression (Appendix E, Figure 2a‚Äìb), which does not substantiate clear advantages in this setting (impact).\n  ‚Ä¢ COLA is included in MNIST accuracy comparison (Table 3), but without analysis of chaining vs Bernoulli updates or convergence guarantees; No direct evidence found in the manuscript (completeness).\n- Sparse guidance on choosing p, sketch distributions, and T\n  ‚Ä¢ The probability p influences Œª_min^p and Œª_max^p (Table 1; Section 7), yet the manuscript offers no principled strategy for selecting p (clarity; practical utility).\n  ‚Ä¢ Sketch distributions ùíü_S^A/ùíü_S^B are left general (Algorithm 1), and Gaussian is discussed (Appendix B), but alternative distributions (e.g., orthonormal, data-informed) are not analyzed empirically (impact).\n  ‚Ä¢ Chain length T appears in rates (Table 1) but its interaction with p, variance, and convergence speed is not studied beyond Figure 1; No direct evidence found in the manuscript (practical guidance).\n- Minor clarity and editorial issues\n  ‚Ä¢ Appendix C.1 contains a residual mention of ‚ÄúRAC-LoRA‚Äù in the heading text (‚Äú‚Ä¶ main convergence result for RAC-LoRA with Gradient Descent updates.‚Äù), which seems unintended (Appendix C.1, before C.1.1) (clarity).\n  ‚Ä¢ Notational inconsistencies around hats in Definitions 1‚Äì2 (Section 6, Equations (5)‚Äì(6)) can momentarily confuse what is sampled vs optimized (clarity).\n  ‚Ä¢ Table 3 footnote ^1 states a deterministic assignment at initialization ‚Äúyielded better performance,‚Äù which deviates from the Bernoulli prescription and could be clarified as an implementation detail vs methodological change (Section 8.2; Table 3) (clarity).\n- Internal inconsistencies in stepsize bounds and summary rates\n  ‚Ä¢ For Bernoulli-LoRA-SGD, the main-text stepsize bound (Theorem 2, Section 7; ‚Äú0 < Œ≥ ‚â§ min{ 1/(‚àö(L A1 Œª_max^p) T), ‚Ä¶ }‚Äù) differs materially from the appendix bound (Theorem 11, Appendix C.2: ‚Äú0 < Œ≥ ‚â§ min{ 1/‚àö(L A1 Œª_max^p T), ‚Ä¶ }‚Äù), changing the scaling with T (anchors: Section 7, Theorem 2; Appendix C.2, Theorem 11).\n  ‚Ä¢ For Fed-Bernoulli-LoRA-QGD, the main-text stepsize bound (Theorem 5, Section 7: ‚Äú1/(L‚àö(œâ/M) Œª_max^p T)‚Äù) does not match the appendix (Theorem 17, Appendix D.1: ‚Äú1/(L ‚àö((œâ/M) Œª_max^p T))‚Äù) (anchors: Section 7, Theorem 5; Appendix D.1, Theorem 17).\n  ‚Ä¢ For Fed-Bernoulli-LoRA-MARINA, the main-text bound (Theorem 6, Section 7) places Œª_max^p inside an inverse under the square root (‚Äú(1/Œª_max^p) ¬∑ ((1‚àíq)/q) ¬∑ (œâ/M)‚Äù), while the appendix uses Œª_max^p without inversion (Theorem 19, Appendix D.2) (anchors: Section 7, Theorem 6; Appendix D.2, Theorem 19).\n  ‚Ä¢ For Fed-Bernoulli-LoRA-EF21, the stepsize form in the main text (Theorem 7, Section 7) differs from the constants and factors used in the detailed proof (Theorem 21, Appendix D.3; e.g., additional ‚àö2-type terms in bounds) (anchors: Section 7, Theorem 7; Appendix D.3, Theorem 21).\n  ‚Ä¢ Table 1 omits Œª_max^p/Œª_min^p dependence in the variance terms for PAGE/MVR/EF21 compared to the detailed theorems (Table 1 vs Theorem 4, Section 7; Theorem 3, Section 7; Theorem 21, Appendix D.3), which can mislead readers regarding rate constants (anchors: Table 1; Section 7, Theorems 3‚Äì4; Appendix D.3, Theorem 21).\n- Algorithmic specification errors and missing details\n  ‚Ä¢ Algorithm 3 (non-smooth setting) flips the branch semantics (‚ÄúIf c^t = 1 then (Right sketch)‚Äù) relative to Algorithm 1 (‚Äúif c^t = 1 then Left sketch‚Äù), creating an internal contradiction (anchors: Algorithm 1, Section 6; Algorithm 3, Appendix C.1.3).\n  ‚Ä¢ Algorithm 6 (Bernoulli-LoRA-PAGE) samples i_{t+1} from [n], whereas the finite-sum formulation defines N samples (Equation (2), Section 2), causing a definition mismatch (anchors: Algorithm 6, Appendix C.4; Equation (2), Section 2).\n  ‚Ä¢ Lemma 10 (EF21) references an equation (131) that does not appear in that vicinity, undermining traceability of the proof (anchors: Appendix D.3, Lemma 10).\n  ‚Ä¢ The MNIST section states that detailed configurations are provided in Appendix E (Section 8.2), but Appendix E includes details only for the linear regression experiments (Appendix E.1), not MNIST, impeding reproducibility (anchors: Section 8.2; Appendix E.1).\n  ‚Ä¢ Minor presentational and bibliography issues: Figure 1 caption mentions curves for multiple p values while the legend exemplifies p=0.5 for Bernoulli variants (Section 8.1; Figure 1/legend), and Appendix B cites ‚ÄúSchur, 2024‚Äù whereas the references list includes Schur (1905) (anchors: Appendix B; References).Suggestions for Improvement\n- Broaden empirical validation and align with theory\n  ‚Ä¢ Add federated experiments for QGD/MARINA/EF21 to demonstrate communication savings and robustness under data heterogeneity (Section 6.3; Algorithms 7‚Äì9; Theorems 5‚Äì7), measuring gradient bytes, compression parameters œâ/Œ≤, and convergence traces.\n  ‚Ä¢ Include larger-scale PEFT tasks (e.g., higher-dimensional layers or more complex datasets) and assess variance-reduction benefits consistent with Figure 1 and Theorem 4 (Section 8.1; Table 1).\n  ‚Ä¢ For MNIST, run ablations using SGD and PAGE (Equations (7)‚Äì(8)) in addition to AdamW to align with theoretical updates and report accuracy vs convergence diagnostics (Section 8.2).\n- Formalize and relax independence assumptions\n  ‚Ä¢ Introduce an explicit assumption detailing independence between c^t, A_S^t/B_S^t and stochastic gradients g(W^t), and between client compressors and sketches (Appendix C.2 step (97); Algorithm 1; Appendix D).\n  ‚Ä¢ Discuss and, where possible, extend proofs to handle mild dependencies (e.g., conditioning on sketches), noting where bounds change (Appendix C.2; Lemmas 5‚Äì7).\n  ‚Ä¢ For federated variants, specify independence between client-side randomness and server-side sketching to make Lemmas 8‚Äì10 fully rigorous (Appendix D).\n- Strengthen practicality of projection assumptions and parameter estimation\n  ‚Ä¢ Empirically verify Assumption 1 and Assumption 10 under commonly used LoRA initializations (e.g., zero-initialized B, Gaussian A) by estimating E[H] and eigenvalues on real layers; compare against Lemma 2 (Appendix B; Section 7).\n  ‚Ä¢ Provide guidance or bounds to estimate Œª_min^p and Œª_max^p in practice (Table 1; Theorems 2‚Äì7, 10, 12‚Äì22), perhaps via Monte Carlo sketching diagnostics over ùíü_S^A/ùíü_S^B.\n  ‚Ä¢ Discuss non-isotropic sketches and offer relaxed results that assume only lower-bounds on the minimal eigenvalue of E[H] (Section C.1.3; Theorem 10), improving generality beyond Œ± I.\n- Deepen comparisons to RAC-LoRA and COLA\n  ‚Ä¢ Add theoretical discussion contrasting rates or constants under common assumptions, indicating when Bernoulli mixing improves Œª_min^p or variance control relative to RAC-LoRA/COLA (Section 4; Table 1).\n  ‚Ä¢ Expand experiments beyond Appendix E Figure 2 to scenarios where variance-reduction and Bernoulli mixing should shine (e.g., higher noise), quantifying advantages vs RAC-LoRA (Appendix E).\n  ‚Ä¢ Include analyses or ablations that explain MNIST results vis-√†-vis COLA (Table 3), e.g., parameter counts, chaining length vs Bernoulli probability p, and convergence behavior.\n- Provide practical tuning guidance for p, sketches, and T\n  ‚Ä¢ Derive or suggest heuristics to choose p (e.g., maximizing Œª_min^p subject to Œª_max^p constraints; Section 7; Table 1), and validate experimentally across a grid, extending Figure 1.\n  ‚Ä¢ Evaluate alternative sketch distributions (orthonormal, data-dependent) beyond Gaussian (Appendix B), adding empirical results to show their effect on Œª_min^p/Œª_max^p and performance.\n  ‚Ä¢ Study the interaction of chain length T with p and variance (Table 1), reporting diminishing returns or optimal regimes on both tasks and in federated settings.\n- Resolve clarity and editorial issues\n  ‚Ä¢ Correct the ‚ÄúRAC-LoRA‚Äù mention in Appendix C.1 to ‚ÄúBernoulli-LoRA‚Äù and check for similar residual references.\n  ‚Ä¢ Standardize hat notation in Definitions 1‚Äì2 to clearly distinguish sampled vs optimized matrices (Section 6; Equations (5)‚Äì(6)).\n  ‚Ä¢ Clarify Table 3 footnote ^1 to indicate whether deterministic assignment at initialization modifies the method or is an implementation detail, and report results with fully Bernoulli selection for completeness (Section 8.2).\n- Align stepsize bounds and Table 1 with detailed theorems\n  ‚Ä¢ Reconcile the stepsize bound in Theorem 2 (Section 7) with Theorem 11 (Appendix C.2), ensuring consistent T-scaling; update the main text or appendix accordingly.\n  ‚Ä¢ Make Theorem 5 (Section 7) consistent with Theorem 17 (Appendix D.1) by placing œâ/M, Œª_max^p, and T inside the square root as in the detailed proof.\n  ‚Ä¢ Fix Theorem 6 (Section 7) to match Theorem 19 (Appendix D.2) regarding Œª_max^p placement (remove the inversion inside the square root if unintended).\n  ‚Ä¢ Harmonize Theorem 7 (Section 7) with Theorem 21 (Appendix D.3), clarifying constants (e.g., ‚àö2-type factors) used in sufficient conditions for Œ≥.\n  ‚Ä¢ Update Table 1 to include the Œª_max^p/Œª_min^p dependence in the variance terms for PAGE, MVR, and EF21 (compare Table 1 against Theorem 4, Section 7; Theorem 3, Section 7; Theorem 21, Appendix D.3).\n- Fix algorithmic specifications and add missing experimental details\n  ‚Ä¢ Make the c^t branch condition consistent across Algorithm 1 (Section 6) and Algorithm 3 (Appendix C.1.3), preserving left/right sketch semantics.\n  ‚Ä¢ Correct the sampling in Algorithm 6 to draw i_{t+1} from [N] per Equation (2) (Section 2) for finite-sum; adjust notation accordingly.\n  ‚Ä¢ Repair Lemma 10 (Appendix D.3) to reference existing equations (e.g., (130)) and remove or define (131) as needed for traceability.\n  ‚Ä¢ Align Figure 1 caption and legend regarding p values, or add a panel/legend showing curves for the stated p grid (Section 8.1).\n  ‚Ä¢ Correct ‚ÄúSchur, 2024‚Äù in Appendix B to match the references list (Schur, 1905), or add the missing reference entry.\n  ‚Ä¢ Provide MNIST experiment configurations in Appendix E (architecture, preprocessing, hyperparameter ranges, seeds) to match the statement in Section 8.2.Score\n- Overall (10): 6 ‚Äî Strong theoretical unification and breadth (Section 6.1; Table 2; Theorems 1‚Äì7, 10, 12‚Äì22), but multiple internal inconsistencies in stepsize bounds and summary rates (Section 7 Theorems 2, 5‚Äì7 vs Appendix C.2/D.1‚ÄìD.3) and missing MNIST details in Appendix E (Section 8.2; Appendix E.1).\n- Novelty (10): 7 ‚Äî Randomized Bernoulli selection unifying left/right sketches with VR and FL extensions (Algorithms 1, 5‚Äì9; Table 2) constitutes a meaningful synthesis, though formal advantages vs RAC-LoRA/COLA remain mostly qualitative (Table 3; No direct complexity comparison).\n- Technical Quality (10): 7 ‚Äî Projected-gradient analysis and convergence proofs are substantial (Lemma 3; Lemmas 5‚Äì7; Theorems 1‚Äì7, 10, 12‚Äì22), but unaligned stepsize conditions and Table 1 discrepancies with detailed theorems reduce rigor (Section 7; Appendix C/D).\n- Clarity (10): 6 ‚Äî Generally clear notation and organization (Section 5; Table 1; Table 2), yet branch inconsistency between Algorithms 1 and 3, PAGE index mismatch (Appendix C.4), EF21 lemma equation reference, and editorial artifacts (Appendix C.1; Appendix B citation) hinder readability (anchors: Algorithms; Appendix C/D; References).\n- Confidence (5): 4 ‚Äî High confidence in the overarching framework and many proofs (Table 1; Appendices C‚ÄìD); moderate confidence reduced by internal inconsistencies and missing experimental details (Section 7; Appendix E).",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 7,
        "novelty": 7,
        "technical_quality": 8,
        "clarity": 7,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 6,
        "novelty": 7,
        "technical_quality": 7,
        "clarity": 6,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "Summary\n- The paper proposes Bernoulli-LoRA, a unified theoretical framework for parameter-efficient fine-tuning that randomly selects which of the two LoRA factors (A or B) to optimize at each step (Algorithm 1, Section 6; Definitions 1‚Äì2). The update is reformulated as a projected gradient step in the full parameter space (Section 6.1; Equations (7)‚Äì(8), Lemma 3), enabling analysis under standard non-convex smooth, P≈Å, and non-smooth convex settings. Multiple algorithmic variants are analyzed: GD, SGD, MVR, PAGE, and federated extensions with quantization (QGD), gradient-difference compression (MARINA), and error feedback (EF21) (Section 6.2‚Äì6.3; Table 2). Convergence rates are established and summarized (Table 1; Theorems 1‚Äì7, 10, 12‚Äì22). Experiments on a linear regression with non-convex regularization and on MNIST with an MLP provide empirical validation, including a variance-reduction advantage for Bernoulli-LoRA-PAGE (Figure 1) and competitive accuracy on MNIST (Table 3; Appendix E).Strengths\n- Bold unified projected-gradient formulation\n  ‚Ä¢ Reformulating left/right sketch updates as projected gradient steps in the full parameter space (Section 6.1; Equations (7)‚Äì(8), Lemma 3; Equations (24), (30)) demonstrates technical soundness and avoids smoothness issues of the LoRA parameterization (Section 7, ‚ÄúA significant challenge...‚Äù; Section 6.1), enabling rigorous analysis.\n  ‚Ä¢ The definition and use of projection matrices H_B^t and H_A^t (Section 6.1; H_B^t and H_A^t definitions) provide a clean analytical handle via spectral properties, which is critical for non-convex convergence proofs (Theorem 1; inequalities (40)‚Äì(42)).\n  ‚Ä¢ The probability-weighted eigenvalue Œª_min^p and Œª_max^p unify left/right sketches and the Bernoulli mixing (Section 7; Table 1), aiding clarity and generality.\n- Broad algorithmic coverage with convergence\n  ‚Ä¢ The framework instantiates GD, SGD, variance-reduced (MVR, PAGE), and federated extensions (QGD, MARINA, EF21) with projected estimators (Section 6.2; Table 2), showing breadth and practical relevance.\n  ‚Ä¢ Convergence is provided under smooth non-convex and P≈Å conditions for all core variants (Theorems 1‚Äì7, 12‚Äì22; Table 1), evidencing technical completeness.\n  ‚Ä¢ Non-smooth convex analysis with both constant and Polyak-type stepsizes (Algorithm 3; Assumptions 7‚Äì10; Theorem 10) broadens applicability beyond differentiable objectives.\n- Sound treatment of projection assumptions\n  ‚Ä¢ Assumption 1 (positive expected projection; Section 7) is justified through Gaussian sketches, with proof that E[H]= (r/n) I (Appendix B; Lemma 2; Remark 1), aiding tractability and providing explicit Œª_min bounds.\n  ‚Ä¢ The use of rotational invariance and Schur-type arguments (Appendix B; Lemma 1 and Lemma 2) is technically correct and strengthens the framework‚Äôs foundations.\n  ‚Ä¢ Assumption 10 (E[H]=Œ± I; Section C.1.3) enables clean non-smooth analysis (Theorem 10), highlighting the value of symmetric sketching distributions.\n- Federated learning extensions with compression and EF\n  ‚Ä¢ The paper adapts quantization (QGD), gradient-difference compression (MARINA), and error feedback (EF21) to LoRA-style projected updates (Section 6.3; Algorithms 7‚Äì9), filling a gap between PEFT and communication-efficient FL.\n  ‚Ä¢ Convergence results explicitly quantify the impact of compression parameters œâ and Œ≤ (Table 1; Theorems 5‚Äì7; Assumption 11; Definition 3‚Äì4), demonstrating technical depth and practical relevance for distributed systems.\n  ‚Ä¢ MARINA and EF21 analyses reuse PAGE/MVR Lyapunov techniques adapted to projected estimators (Appendix D; Lemmas 9‚Äì10; Theorems 19‚Äì22), evidencing methodological maturity.\n- Empirical evidence consistent with theory\n  ‚Ä¢ Bernoulli-LoRA-PAGE reduces variance and converges to tight tolerances in the stochastic linear regression setting (Figure 1), matching the theoretical advantage of variance-reduced estimators (Theorem 4; Table 1).\n  ‚Ä¢ In MNIST PEFT, Bernoulli-LoRA achieves competitive accuracy vis-√†-vis RAC-LoRA while training one matrix per block (Table 3; discussion), aligning with the framework‚Äôs resource-efficiency goals (Section 4).\n  ‚Ä¢ Appendix E.1 provides implementation details for the linear regression setting (Appendix E.1; hardware; stepsize policy Œ≥=c/ƒ§L), contributing to reproducibility for that experiment.\n- Clear notation and organization\n  ‚Ä¢ Notation is defined upfront (Section 5) with consistent use of Frobenius norms, inner products, and pseudoinverses, helping technical clarity across proofs (e.g., Lemma 3; Lemmas 5‚Äì7).\n  ‚Ä¢ The content map (Section 1, Contents) and summary table of rates (Table 1) aid navigability and comprehension.Weaknesses\n- Limited empirical scope and mismatch with large-scale/FL scenarios\n  ‚Ä¢ Experiments are restricted to a synthetic linear regression and an MNIST MLP (Section 8.1‚Äì8.2), which may not reflect behavior on large models central to PEFT (impact; generalization).\n  ‚Ä¢ No federated experiments are reported for QGD/MARINA/EF21 variants despite extensive theory (Section 6.3; Algorithms 7‚Äì9; Theorems 5‚Äì7); No direct evidence found in the manuscript, limiting practical validation of communication savings and robustness.\n  ‚Ä¢ MNIST experiments use AdamW (Section 8.2) rather than the theorized update forms (e.g., GD/SGD/PAGE with W^{t+1}=W^t‚àíŒ≥ƒú^t in Equations (7)‚Äì(8)), introducing potential mismatch between theoretical guarantees and empirical setup (technical soundness).\n- Unstated independence assumptions in proofs\n  ‚Ä¢ The SGD analysis explicitly uses independence between H_B^t/H_A^t and G^t (Appendix C.2, step (97): ‚Äúin (*) we used that H_B^t, H_A^t and G^t are independent‚Äù), but such independence is not formalized as an assumption (clarity; rigor).\n  ‚Ä¢ Algorithm 1 (Section 6) does not state whether the Bernoulli variable c^t, sketches A_S^t/B_S^t, and stochastic gradients are mutually independent across time and sources (technical soundness).\n  ‚Ä¢ Federated analyses implicitly assume independence between compressors, local gradients, and sketches (Appendix D, Lemmas 8‚Äì10), but this is not stated explicitly; No direct evidence found in the manuscript (rigor).\n- Strong projection assumptions and practical alignment\n  ‚Ä¢ Assumption 1 and Lemma 2 hinge on Gaussian sketches to obtain E[H]= (r/n) I (Appendix B; Remark 1), while in practice LoRA often uses zero-initialized B or structured A (Section 6; Table 3 footnote ^1 indicates deterministic assignment), potentially misaligning assumptions and deployments (impact; realism).\n  ‚Ä¢ Assumption 10 (E[H]=Œ± I; Section C.1.3) may be restrictive beyond Gaussian/rotationally invariant designs; the manuscript does not discuss non-isotropic sketches (clarity; generality).\n  ‚Ä¢ Stepsize conditions depend on Œª_min^p, Œª_max^p (Table 1; Theorems 2‚Äì7, 10, 12‚Äì22), but practical estimation strategies for these quantities are not provided (reproducibility; guidance).\n- Limited theoretical/empirical comparison to RAC-LoRA/COLA\n  ‚Ä¢ While the framework claims to unify and extend LoRA approaches (Section 4), there is no formal complexity comparison to RAC-LoRA/COLA (e.g., tighter bounds or oracle complexity), beyond qualitative discussion; No direct evidence found in the manuscript (novelty positioning).\n  ‚Ä¢ Empirically, Bernoulli-LoRA-GD and RAC-LoRA-GD perform similarly in linear regression (Appendix E, Figure 2a‚Äìb), which does not substantiate clear advantages in this setting (impact).\n  ‚Ä¢ COLA is included in MNIST accuracy comparison (Table 3), but without analysis of chaining vs Bernoulli updates or convergence guarantees; No direct evidence found in the manuscript (completeness).\n- Sparse guidance on choosing p, sketch distributions, and T\n  ‚Ä¢ The probability p influences Œª_min^p and Œª_max^p (Table 1; Section 7), yet the manuscript offers no principled strategy for selecting p (clarity; practical utility).\n  ‚Ä¢ Sketch distributions ùíü_S^A/ùíü_S^B are left general (Algorithm 1), and Gaussian is discussed (Appendix B), but alternative distributions (e.g., orthonormal, data-informed) are not analyzed empirically (impact).\n  ‚Ä¢ Chain length T appears in rates (Table 1) but its interaction with p, variance, and convergence speed is not studied beyond Figure 1; No direct evidence found in the manuscript (practical guidance).\n- Minor clarity and editorial issues\n  ‚Ä¢ Appendix C.1 contains a residual mention of ‚ÄúRAC-LoRA‚Äù in the heading text (‚Äú‚Ä¶ main convergence result for RAC-LoRA with Gradient Descent updates.‚Äù), which seems unintended (Appendix C.1, before C.1.1) (clarity).\n  ‚Ä¢ Notational inconsistencies around hats in Definitions 1‚Äì2 (Section 6, Equations (5)‚Äì(6)) can momentarily confuse what is sampled vs optimized (clarity).\n  ‚Ä¢ Table 3 footnote ^1 states a deterministic assignment at initialization ‚Äúyielded better performance,‚Äù which deviates from the Bernoulli prescription and could be clarified as an implementation detail vs methodological change (Section 8.2; Table 3) (clarity).\n- Internal inconsistencies in stepsize bounds and summary rates\n  ‚Ä¢ For Bernoulli-LoRA-SGD, the main-text stepsize bound (Theorem 2, Section 7; ‚Äú0 < Œ≥ ‚â§ min{ 1/(‚àö(L A1 Œª_max^p) T), ‚Ä¶ }‚Äù) differs materially from the appendix bound (Theorem 11, Appendix C.2: ‚Äú0 < Œ≥ ‚â§ min{ 1/‚àö(L A1 Œª_max^p T), ‚Ä¶ }‚Äù), changing the scaling with T (anchors: Section 7, Theorem 2; Appendix C.2, Theorem 11).\n  ‚Ä¢ For Fed-Bernoulli-LoRA-QGD, the main-text stepsize bound (Theorem 5, Section 7: ‚Äú1/(L‚àö(œâ/M) Œª_max^p T)‚Äù) does not match the appendix (Theorem 17, Appendix D.1: ‚Äú1/(L ‚àö((œâ/M) Œª_max^p T))‚Äù) (anchors: Section 7, Theorem 5; Appendix D.1, Theorem 17).\n  ‚Ä¢ For Fed-Bernoulli-LoRA-MARINA, the main-text bound (Theorem 6, Section 7) places Œª_max^p inside an inverse under the square root (‚Äú(1/Œª_max^p) ¬∑ ((1‚àíq)/q) ¬∑ (œâ/M)‚Äù), while the appendix uses Œª_max^p without inversion (Theorem 19, Appendix D.2) (anchors: Section 7, Theorem 6; Appendix D.2, Theorem 19).\n  ‚Ä¢ For Fed-Bernoulli-LoRA-EF21, the stepsize form in the main text (Theorem 7, Section 7) differs from the constants and factors used in the detailed proof (Theorem 21, Appendix D.3; e.g., additional ‚àö2-type terms in bounds) (anchors: Section 7, Theorem 7; Appendix D.3, Theorem 21).\n  ‚Ä¢ Table 1 omits Œª_max^p/Œª_min^p dependence in the variance terms for PAGE/MVR/EF21 compared to the detailed theorems (Table 1 vs Theorem 4, Section 7; Theorem 3, Section 7; Theorem 21, Appendix D.3), which can mislead readers regarding rate constants (anchors: Table 1; Section 7, Theorems 3‚Äì4; Appendix D.3, Theorem 21).\n- Algorithmic specification errors and missing details\n  ‚Ä¢ Algorithm 3 (non-smooth setting) flips the branch semantics (‚ÄúIf c^t = 1 then (Right sketch)‚Äù) relative to Algorithm 1 (‚Äúif c^t = 1 then Left sketch‚Äù), creating an internal contradiction (anchors: Algorithm 1, Section 6; Algorithm 3, Appendix C.1.3).\n  ‚Ä¢ Algorithm 6 (Bernoulli-LoRA-PAGE) samples i_{t+1} from [n], whereas the finite-sum formulation defines N samples (Equation (2), Section 2), causing a definition mismatch (anchors: Algorithm 6, Appendix C.4; Equation (2), Section 2).\n  ‚Ä¢ Lemma 10 (EF21) references an equation (131) that does not appear in that vicinity, undermining traceability of the proof (anchors: Appendix D.3, Lemma 10).\n  ‚Ä¢ The MNIST section states that detailed configurations are provided in Appendix E (Section 8.2), but Appendix E includes details only for the linear regression experiments (Appendix E.1), not MNIST, impeding reproducibility (anchors: Section 8.2; Appendix E.1).\n  ‚Ä¢ Minor presentational and bibliography issues: Figure 1 caption mentions curves for multiple p values while the legend exemplifies p=0.5 for Bernoulli variants (Section 8.1; Figure 1/legend), and Appendix B cites ‚ÄúSchur, 2024‚Äù whereas the references list includes Schur (1905) (anchors: Appendix B; References).Suggestions for Improvement\n- Broaden empirical validation and align with theory\n  ‚Ä¢ Add federated experiments for QGD/MARINA/EF21 to demonstrate communication savings and robustness under data heterogeneity (Section 6.3; Algorithms 7‚Äì9; Theorems 5‚Äì7), measuring gradient bytes, compression parameters œâ/Œ≤, and convergence traces.\n  ‚Ä¢ Include larger-scale PEFT tasks (e.g., higher-dimensional layers or more complex datasets) and assess variance-reduction benefits consistent with Figure 1 and Theorem 4 (Section 8.1; Table 1).\n  ‚Ä¢ For MNIST, run ablations using SGD and PAGE (Equations (7)‚Äì(8)) in addition to AdamW to align with theoretical updates and report accuracy vs convergence diagnostics (Section 8.2).\n- Formalize and relax independence assumptions\n  ‚Ä¢ Introduce an explicit assumption detailing independence between c^t, A_S^t/B_S^t and stochastic gradients g(W^t), and between client compressors and sketches (Appendix C.2 step (97); Algorithm 1; Appendix D).\n  ‚Ä¢ Discuss and, where possible, extend proofs to handle mild dependencies (e.g., conditioning on sketches), noting where bounds change (Appendix C.2; Lemmas 5‚Äì7).\n  ‚Ä¢ For federated variants, specify independence between client-side randomness and server-side sketching to make Lemmas 8‚Äì10 fully rigorous (Appendix D).\n- Strengthen practicality of projection assumptions and parameter estimation\n  ‚Ä¢ Empirically verify Assumption 1 and Assumption 10 under commonly used LoRA initializations (e.g., zero-initialized B, Gaussian A) by estimating E[H] and eigenvalues on real layers; compare against Lemma 2 (Appendix B; Section 7).\n  ‚Ä¢ Provide guidance or bounds to estimate Œª_min^p and Œª_max^p in practice (Table 1; Theorems 2‚Äì7, 10, 12‚Äì22), perhaps via Monte Carlo sketching diagnostics over ùíü_S^A/ùíü_S^B.\n  ‚Ä¢ Discuss non-isotropic sketches and offer relaxed results that assume only lower-bounds on the minimal eigenvalue of E[H] (Section C.1.3; Theorem 10), improving generality beyond Œ± I.\n- Deepen comparisons to RAC-LoRA and COLA\n  ‚Ä¢ Add theoretical discussion contrasting rates or constants under common assumptions, indicating when Bernoulli mixing improves Œª_min^p or variance control relative to RAC-LoRA/COLA (Section 4; Table 1).\n  ‚Ä¢ Expand experiments beyond Appendix E Figure 2 to scenarios where variance-reduction and Bernoulli mixing should shine (e.g., higher noise), quantifying advantages vs RAC-LoRA (Appendix E).\n  ‚Ä¢ Include analyses or ablations that explain MNIST results vis-√†-vis COLA (Table 3), e.g., parameter counts, chaining length vs Bernoulli probability p, and convergence behavior.\n- Provide practical tuning guidance for p, sketches, and T\n  ‚Ä¢ Derive or suggest heuristics to choose p (e.g., maximizing Œª_min^p subject to Œª_max^p constraints; Section 7; Table 1), and validate experimentally across a grid, extending Figure 1.\n  ‚Ä¢ Evaluate alternative sketch distributions (orthonormal, data-dependent) beyond Gaussian (Appendix B), adding empirical results to show their effect on Œª_min^p/Œª_max^p and performance.\n  ‚Ä¢ Study the interaction of chain length T with p and variance (Table 1), reporting diminishing returns or optimal regimes on both tasks and in federated settings.\n- Resolve clarity and editorial issues\n  ‚Ä¢ Correct the ‚ÄúRAC-LoRA‚Äù mention in Appendix C.1 to ‚ÄúBernoulli-LoRA‚Äù and check for similar residual references.\n  ‚Ä¢ Standardize hat notation in Definitions 1‚Äì2 to clearly distinguish sampled vs optimized matrices (Section 6; Equations (5)‚Äì(6)).\n  ‚Ä¢ Clarify Table 3 footnote ^1 to indicate whether deterministic assignment at initialization modifies the method or is an implementation detail, and report results with fully Bernoulli selection for completeness (Section 8.2).\n- Align stepsize bounds and Table 1 with detailed theorems\n  ‚Ä¢ Reconcile the stepsize bound in Theorem 2 (Section 7) with Theorem 11 (Appendix C.2), ensuring consistent T-scaling; update the main text or appendix accordingly.\n  ‚Ä¢ Make Theorem 5 (Section 7) consistent with Theorem 17 (Appendix D.1) by placing œâ/M, Œª_max^p, and T inside the square root as in the detailed proof.\n  ‚Ä¢ Fix Theorem 6 (Section 7) to match Theorem 19 (Appendix D.2) regarding Œª_max^p placement (remove the inversion inside the square root if unintended).\n  ‚Ä¢ Harmonize Theorem 7 (Section 7) with Theorem 21 (Appendix D.3), clarifying constants (e.g., ‚àö2-type factors) used in sufficient conditions for Œ≥.\n  ‚Ä¢ Update Table 1 to include the Œª_max^p/Œª_min^p dependence in the variance terms for PAGE, MVR, and EF21 (compare Table 1 against Theorem 4, Section 7; Theorem 3, Section 7; Theorem 21, Appendix D.3).\n- Fix algorithmic specifications and add missing experimental details\n  ‚Ä¢ Make the c^t branch condition consistent across Algorithm 1 (Section 6) and Algorithm 3 (Appendix C.1.3), preserving left/right sketch semantics.\n  ‚Ä¢ Correct the sampling in Algorithm 6 to draw i_{t+1} from [N] per Equation (2) (Section 2) for finite-sum; adjust notation accordingly.\n  ‚Ä¢ Repair Lemma 10 (Appendix D.3) to reference existing equations (e.g., (130)) and remove or define (131) as needed for traceability.\n  ‚Ä¢ Align Figure 1 caption and legend regarding p values, or add a panel/legend showing curves for the stated p grid (Section 8.1).\n  ‚Ä¢ Correct ‚ÄúSchur, 2024‚Äù in Appendix B to match the references list (Schur, 1905), or add the missing reference entry.\n  ‚Ä¢ Provide MNIST experiment configurations in Appendix E (architecture, preprocessing, hyperparameter ranges, seeds) to match the statement in Section 8.2.Score\n- Overall (10): 6 ‚Äî Strong theoretical unification and breadth (Section 6.1; Table 2; Theorems 1‚Äì7, 10, 12‚Äì22), but multiple internal inconsistencies in stepsize bounds and summary rates (Section 7 Theorems 2, 5‚Äì7 vs Appendix C.2/D.1‚ÄìD.3) and missing MNIST details in Appendix E (Section 8.2; Appendix E.1).\n- Novelty (10): 7 ‚Äî Randomized Bernoulli selection unifying left/right sketches with VR and FL extensions (Algorithms 1, 5‚Äì9; Table 2) constitutes a meaningful synthesis, though formal advantages vs RAC-LoRA/COLA remain mostly qualitative (Table 3; No direct complexity comparison).\n- Technical Quality (10): 7 ‚Äî Projected-gradient analysis and convergence proofs are substantial (Lemma 3; Lemmas 5‚Äì7; Theorems 1‚Äì7, 10, 12‚Äì22), but unaligned stepsize conditions and Table 1 discrepancies with detailed theorems reduce rigor (Section 7; Appendix C/D).\n- Clarity (10): 6 ‚Äî Generally clear notation and organization (Section 5; Table 1; Table 2), yet branch inconsistency between Algorithms 1 and 3, PAGE index mismatch (Appendix C.4), EF21 lemma equation reference, and editorial artifacts (Appendix C.1; Appendix B citation) hinder readability (anchors: Algorithms; Appendix C/D; References).\n- Confidence (5): 4 ‚Äî High confidence in the overarching framework and many proofs (Table 1; Appendices C‚ÄìD); moderate confidence reduced by internal inconsistencies and missing experimental details (Section 7; Appendix E)."
}