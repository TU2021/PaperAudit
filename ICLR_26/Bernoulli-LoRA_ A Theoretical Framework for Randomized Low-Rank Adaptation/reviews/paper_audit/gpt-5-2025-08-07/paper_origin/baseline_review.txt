Summary
- The paper proposes Bernoulli-LoRA, a unified theoretical framework for low-rank adaptation in parameter-efficient fine-tuning that randomly selects which of the two LoRA factors (A or B) to optimize at each step (Algorithm 1, Section 6; Definitions 1‚Äì2). The update is reformulated as a projected gradient step in the full parameter space (Section 6.1; Equations (7)‚Äì(8), Lemma 3), enabling analysis under standard non-convex smooth, P≈Å, and non-smooth convex settings. Multiple algorithmic variants are analyzed: GD, SGD, MVR, PAGE, and federated extensions with quantization (QGD), gradient-difference compression (MARINA), and error feedback (EF21) (Section 6.2‚Äì6.3; Table 2). Convergence rates are established and summarized (Table 1; Theorems 1‚Äì7, 10, 12‚Äì22). Experiments on a linear regression with non-convex regularization and on MNIST with an MLP provide empirical validation, including a variance-reduction advantage for Bernoulli-LoRA-PAGE (Figure 1) and competitive accuracy on MNIST (Table 3; Appendix E).Strengths
- Bold unified projected-gradient formulation
  ‚Ä¢ Reformulating left/right sketch updates as projected gradient steps in the full parameter space (Section 6.1; Equations (7)‚Äì(8), Lemma 3; Equations (24), (30)) demonstrates technical soundness and avoids smoothness issues of the LoRA parameterization (Section 7, ‚ÄúA significant challenge...‚Äù; Section 6.1), enabling rigorous analysis.
  ‚Ä¢ The definition and use of projection matrices H_B^t and H_A^t (Section 6.1; H_B^t and H_A^t definitions) provide a clean analytical handle via spectral properties, which is critical for non-convex convergence proofs (Theorem 1; inequalities (40)‚Äì(42)).
  ‚Ä¢ The probability-weighted eigenvalue Œª_min^p and Œª_max^p unify left/right sketches and the Bernoulli mixing (Section 7; Table 1), aiding clarity and generality.- Broad algorithmic coverage with convergence
  ‚Ä¢ The framework instantiates GD, SGD, variance-reduced (MVR, PAGE), and federated extensions (QGD, MARINA, EF21) with projected estimators (Section 6.2; Table 2), showing breadth and practical relevance.
  ‚Ä¢ Convergence is provided under smooth non-convex and P≈Å conditions for all core variants (Theorems 1‚Äì7, 12‚Äì22; Table 1), evidencing technical completeness.
  ‚Ä¢ Non-smooth convex analysis with both constant and Polyak-type stepsizes (Algorithm 3; Assumptions 7‚Äì10; Theorem 10) broadens applicability beyond differentiable objectives.- Sound treatment of projection assumptions
  ‚Ä¢ Assumption 1 (positive expected projection; Section 7) is justified through Gaussian sketches, with proof that E[H]= (r/n) I (Appendix B; Lemma 2; Remark 1), aiding tractability and providing explicit Œª_min bounds.
  ‚Ä¢ The use of rotational invariance and Schur-type arguments (Appendix B; Lemma 1 and Lemma 2) is technically correct and strengthens the framework‚Äôs foundations.
  ‚Ä¢ Assumption 10 (E[H]=Œ± I; Section C.1.3) enables clean non-smooth analysis (Theorem 10), highlighting the value of symmetric sketching distributions.- Federated learning extensions with compression and EF
  ‚Ä¢ The paper adapts quantization (QGD), gradient-difference compression (MARINA), and error feedback (EF21) to LoRA-style projected updates (Section 6.3; Algorithms 7‚Äì9), filling a gap between PEFT and communication-efficient FL.
  ‚Ä¢ Convergence results explicitly quantify the impact of compression parameters œâ and Œ≤ (Table 1; Theorems 5‚Äì7; Assumption 11; Definition 3‚Äì4), demonstrating technical depth and practical relevance for distributed systems.
  ‚Ä¢ MARINA and EF21 analyses reuse PAGE/MVR Lyapunov techniques adapted to projected estimators (Appendix D; Lemmas 9‚Äì10; Theorems 19‚Äì22), evidencing methodological maturity.- Empirical evidence consistent with theory
  ‚Ä¢ Bernoulli-LoRA-PAGE reduces variance and converges to tight tolerances in the stochastic linear regression setting (Figure 1), matching the theoretical advantage of variance-reduced estimators (Theorem 4; Table 1).
  ‚Ä¢ In MNIST PEFT, Bernoulli-LoRA achieves competitive accuracy vis-√†-vis RAC-LoRA while training one matrix per block (Table 3; discussion), aligning with the framework‚Äôs resource-efficiency goals (Section 4).
  ‚Ä¢ Appendix E provides implementation and dataset details (Appendix E.1; hardware, stepsize policy Œ≥=c/ƒ§L), contributing to reproducibility and clarity.- Clear notation and organization
  ‚Ä¢ Notation is defined upfront (Section 5) with consistent use of Frobenius norms, inner products, and pseudoinverses, helping technical clarity across proofs (e.g., Lemma 3; Lemmas 5‚Äì7).
  ‚Ä¢ The content map (Section 1, Contents) and summary table of rates (Table 1) aid navigability and comprehension.Weaknesses
- Limited empirical scope and mismatch with large-scale/FL scenarios
  ‚Ä¢ Experiments are restricted to a synthetic linear regression and an MNIST MLP (Section 8.1‚Äì8.2), which may not reflect behavior on large models central to PEFT (impact; generalization).
  ‚Ä¢ No federated experiments are reported for QGD/MARINA/EF21 variants despite extensive theory (Section 6.3; Algorithms 7‚Äì9; Theorems 5‚Äì7); No direct evidence found in the manuscript, limiting practical validation of communication savings and robustness.
  ‚Ä¢ MNIST experiments use AdamW (Section 8.2) rather than the theorized update forms (e.g., GD/SGD/PAGE with W^{t+1}=W^t‚àíŒ≥ƒú^t in Equations (7)‚Äì(8)), introducing potential mismatch between theoretical guarantees and empirical setup (technical soundness).- Unstated independence assumptions in proofs
  ‚Ä¢ The SGD analysis explicitly uses independence between H_B^t/H_A^t and G^t (Appendix C.2, step (97): ‚Äúin (*) we used that H_B^t, H_A^t and G^t are independent‚Äù), but such independence is not formalized as an assumption (clarity; rigor).
  ‚Ä¢ Algorithm 1 (Section 6) does not state whether the Bernoulli variable c^t, sketches A_S^t/B_S^t, and stochastic gradients are mutually independent across time and sources (technical soundness).
  ‚Ä¢ Federated analyses implicitly assume independence between compressors, local gradients, and sketches (Appendix D, Lemmas 8‚Äì10), but this is not stated explicitly; No direct evidence found in the manuscript (rigor).- Strong projection assumptions and practical alignment
  ‚Ä¢ Assumption 1 and Lemma 2 hinge on Gaussian sketches to obtain E[H]= (r/n) I (Appendix B; Remark 1), while in practice LoRA often uses zero-initialized B or structured A (Section 6; Table 3 footnote ^1 indicates deterministic assignment), potentially misaligning assumptions and deployments (impact; realism).
  ‚Ä¢ Assumption 10 (E[H]=Œ± I; Section C.1.3) may be restrictive beyond Gaussian/rotationally invariant designs; the manuscript does not discuss non-isotropic sketches (clarity; generality).
  ‚Ä¢ Stepsize conditions depend on Œª_min^p, Œª_max^p (Table 1; Theorems 2‚Äì7, 10, 12‚Äì22), but practical estimation strategies for these quantities are not provided (reproducibility; guidance).- Limited theoretical/empirical comparison to RAC-LoRA/COLA
  ‚Ä¢ While the framework claims to unify and extend LoRA approaches (Section 4), there is no formal complexity comparison to RAC-LoRA/COLA (e.g., tighter bounds or oracle complexity), beyond qualitative discussion; No direct evidence found in the manuscript (novelty positioning).
  ‚Ä¢ Empirically, Bernoulli-LoRA-GD and RAC-LoRA-GD perform similarly in linear regression (Appendix E, Figure 2a‚Äìb), which does not substantiate clear advantages in this setting (impact).
  ‚Ä¢ COLA is included in MNIST accuracy comparison (Table 3), but without analysis of chaining vs Bernoulli updates or convergence guarantees; No direct evidence found in the manuscript (completeness).- Sparse guidance on choosing p, sketch distributions, and T
  ‚Ä¢ The probability p influences Œª_min^p and Œª_max^p (Table 1; Section 7), yet the manuscript offers no principled strategy for selecting p (clarity; practical utility).
  ‚Ä¢ Sketch distributions ùíü_S^A/ùíü_S^B are left general (Algorithm 1), and Gaussian is discussed (Appendix B), but alternative distributions (e.g., orthonormal, data-informed) are not analyzed empirically (impact).
  ‚Ä¢ Chain length T appears in rates (Table 1) but its interaction with p, variance, and convergence speed is not studied beyond Figure 1; No direct evidence found in the manuscript (practical guidance).- Minor clarity and editorial issues
  ‚Ä¢ Appendix C.1 contains a residual mention of ‚ÄúRAC-LoRA‚Äù in the heading text (‚Äú‚Ä¶ main convergence result for RAC-LoRA with Gradient Descent updates.‚Äù), which seems unintended (Appendix C.1, before C.1.1) (clarity).
  ‚Ä¢ Notational inconsistencies around hats in Definitions 1‚Äì2 (Section 6, Equations (5)‚Äì(6)) can momentarily confuse what is sampled vs optimized (clarity).
  ‚Ä¢ Table 3 footnote ^1 states a deterministic assignment at initialization ‚Äúyielded better performance,‚Äù which deviates from the Bernoulli prescription and could be clarified as an implementation detail vs methodological change (Section 8.2; Table 3) (clarity).Suggestions for Improvement
- Broaden empirical validation and align with theory
  ‚Ä¢ Add federated experiments for QGD/MARINA/EF21 to demonstrate communication savings and robustness under data heterogeneity (Section 6.3; Algorithms 7‚Äì9; Theorems 5‚Äì7), measuring gradient bytes, compression parameters œâ/Œ≤, and convergence traces.
  ‚Ä¢ Include larger-scale PEFT tasks (e.g., higher-dimensional layers or more complex datasets) and assess variance-reduction benefits consistent with Figure 1 and Theorem 4 (Section 8.1; Table 1).
  ‚Ä¢ For MNIST, run ablations using SGD and PAGE (Equations (7)‚Äì(8)) in addition to AdamW to align with theoretical updates and report accuracy vs convergence diagnostics (Section 8.2).- Formalize and relax independence assumptions
  ‚Ä¢ Introduce an explicit assumption detailing independence between c^t, A_S^t/B_S^t and stochastic gradients g(W^t), and between client compressors and sketches (Appendix C.2 step (97); Algorithm 1; Appendix D).
  ‚Ä¢ Discuss and, where possible, extend proofs to handle mild dependencies (e.g., conditioning on sketches), noting where bounds change (Appendix C.2; Lemmas 5‚Äì7).
  ‚Ä¢ For federated variants, specify independence between client-side randomness and server-side sketching to make Lemmas 8‚Äì10 fully rigorous (Appendix D).- Strengthen practicality of projection assumptions and parameter estimation
  ‚Ä¢ Empirically verify Assumption 1 and Assumption 10 under commonly used LoRA initializations (e.g., zero-initialized B, Gaussian A) by estimating E[H] and eigenvalues on real layers; compare against Lemma 2 (Appendix B; Section 7).
  ‚Ä¢ Provide guidance or bounds to estimate Œª_min^p and Œª_max^p in practice (Table 1; Theorems 2‚Äì7, 10, 12‚Äì22), perhaps via Monte Carlo sketching diagnostics over ùíü_S^A/ùíü_S^B.
  ‚Ä¢ Discuss non-isotropic sketches and offer relaxed results that assume only lower-bounds on the minimal eigenvalue of E[H] (Section C.1.3; Theorem 10), improving generality beyond Œ± I.- Deepen comparisons to RAC-LoRA and COLA
  ‚Ä¢ Add theoretical discussion contrasting rates or constants under common assumptions, indicating when Bernoulli mixing improves Œª_min^p or variance control relative to RAC-LoRA/COLA (Section 4; Table 1).
  ‚Ä¢ Expand experiments beyond Appendix E Figure 2 to scenarios where variance-reduction and Bernoulli mixing should shine (e.g., higher noise), quantifying advantages vs RAC-LoRA (Appendix E).
  ‚Ä¢ Include analyses or ablations that explain MNIST results vis-√†-vis COLA (Table 3), e.g., parameter counts, chaining length vs Bernoulli probability p, and convergence behavior.- Provide practical tuning guidance for p, sketches, and T
  ‚Ä¢ Derive or suggest heuristics to choose p (e.g., maximizing Œª_min^p subject to Œª_max^p constraints; Section 7; Table 1), and validate experimentally across a grid, extending Figure 1.
  ‚Ä¢ Evaluate alternative sketch distributions (orthonormal, data-dependent) beyond Gaussian (Appendix B), adding empirical results to show their effect on Œª_min^p/Œª_max^p and performance.
  ‚Ä¢ Study the interaction of chain length T with p and variance (Table 1), reporting diminishing returns or optimal regimes on both tasks and in federated settings.- Resolve clarity and editorial issues
  ‚Ä¢ Correct the ‚ÄúRAC-LoRA‚Äù mention in Appendix C.1 to ‚ÄúBernoulli-LoRA‚Äù and check for similar residual references.
  ‚Ä¢ Standardize hat notation in Definitions 1‚Äì2 to clearly distinguish sampled vs optimized matrices (Section 6; Equations (5)‚Äì(6)).
  ‚Ä¢ Clarify Table 3 footnote ^1 to indicate whether deterministic assignment at initialization modifies the method or is an implementation detail, and report results with fully Bernoulli selection for completeness (Section 8.2).Score
- Overall (10): 7 ‚Äî Comprehensive theoretical framework with projected-gradient unification and convergence across GD/SGD/VR/FL (Section 6.1; Table 1; Theorems 1‚Äì7, 10), but limited empirical scope (Section 8; No direct federated experiments).
- Novelty (10): 7 ‚Äî Randomized Bernoulli selection that unifies left/right sketches and extends to VR and FL settings (Algorithms 1, 5‚Äì9; Table 2) is a meaningful advance, though formal advantages over RAC-LoRA/COLA are not fully quantified (No direct evidence found).
- Technical Quality (10): 8 ‚Äî Proofs are careful and leverage spectral properties (Lemma 3; Lemmas 5‚Äì7; Theorems 1‚Äì7, 10, 12‚Äì22), with well-justified assumptions (Assumption 1; Lemma 2), albeit independence assumptions are not explicitly stated (Appendix C.2, step (97)).
- Clarity (10): 7 ‚Äî Well-structured with clear notation and summaries (Section 5; Table 1; Table 2), but minor editorial issues and limited practical guidance for selecting p and sketches (Section 8.2; Appendix C.1; Assumption 10).
- Confidence (5): 4 ‚Äî The theoretical sections are detailed and internally consistent (Table 1; Appendices C‚ÄìD), but some practical claims and empirical validations (especially FL) are missing, warranting moderate confidence.