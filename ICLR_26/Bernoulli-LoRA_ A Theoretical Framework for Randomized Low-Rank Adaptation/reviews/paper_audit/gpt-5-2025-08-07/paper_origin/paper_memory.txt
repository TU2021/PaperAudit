# Global Summary
- Problem: Provide a unified, theoretically grounded framework for parameter-efficient fine-tuning (PEFT) via low-rank adaptation (LoRA), addressing gaps in convergence theory, stochastic variance reduction, and federated learning (FL) with communication compression.
- Core approach: Bernoulli-LoRA randomly chooses, at each step, to optimize either A or B in a rank-r LoRA update using a Bernoulli trial with probability p. The fixed counterpart is sampled from a specified distribution (Left/Right sketches). Each update equals a projected gradient step in the full parameter space: W^{t+1} = W^t âˆ’ Î³ Äœ^t with Äœ^t = H_B^t G^t w.p. p, or G^t H_A^t w.p. 1âˆ’p. The framework instantiates multiple base estimators G^t (full, stochastic, VR, or FL-compressed).
- Theoretical scope: Convergence guarantees under L-smooth non-convex objectives, PL condition, and non-smooth convex objectives (with both constant and Polyak stepsizes). Assumption 1 (positive expected projection) holds for common Gaussian sketches with E[H] = (r/n) I and Î»_min = r/n.
- Algorithms analyzed: Bernoulli-LoRA-GD, -SGD, -MVR, -PAGE; and FL variants Fed-Bernoulli-LoRA-QGD, -MARINA, -EF21. Convergence rates summarized (notation Î”^0 = f(W^0) âˆ’ f*, ğ’¢^0 = ||G^0 âˆ’ âˆ‡f(W^0)||_F^2, Äœ^0 = (1/M)âˆ‘||G_l^0 âˆ’ âˆ‡f_l(W^0)||_F^2, Î»_min = p Î»_min^{HB} + (1âˆ’p) Î»_min^{HA}, Î»_max analogously):
  - GD (non-convex): Î” bound in gradients: E||âˆ‡f(áº†^T)||_F^2 â‰¤ 2Î”^0/(Î³ Î»_min T).
  - SGD (non-convex): â‰¤ Î” term 6Î”^0/(Î³ Î»_min T) + variance floor Î³ L C1 (Î»_max/Î»_min). Under PL: linear rate (1 âˆ’ Î³ Î¼ Î»_min/2)^T Î”^0 + (Î³ L C1/Î¼) (Î»_max/Î»_min).
  - MVR (non-convex): â‰¤ 2Î”^0/(Î³ Î»_min T) + (ğ’¢^0/(bT) + 2bÏƒ^2/(2âˆ’b)) (Î»_max/Î»_min).
  - PAGE (non-convex): â‰¤ 2Î”^0/(Î³ Î»_min T) + (ğ’¢^0/(qT)) (Î»_max/Î»_min).
  - Fed-QGD (non-convex): â‰¤ 6Î”^0/(Î³ Î»_min T) + (2Î³ L Ï‰ Î”*/M) (Î»_max/Î»_min); PL summary table states additive term (Î³ L^2 Ï‰ Î»_max)/(M Î¼ Î»_min).
  - Fed-MARINA (non-convex): â‰¤ 2Î”^0/(Î³ Î»_min T) + (ğ’¢^0/(qT)) (Î»_max/Î»_min).
  - Fed-EF21 (non-convex): â‰¤ 2Î”^0/(Î³ Î»_min T) + (2 Äœ^0/(Î²T)) (Î»_max/Î»_min).
  - Non-smooth convex (GD): with E[H] = Î± I, constant stepsize: E[f(Å´) âˆ’ f(W*)] â‰¤ R_0^2/(2 Î³ Î± T) + Î³ L_0^2/2; optimal Î³* = âˆš(R_0^2/(T Î± L_0^2)) yields R_0 L_0/âˆš(Î± T); Polyak stepsize achieves â‰¤ R^0 L_0/âˆš(Î± T).
- Empirical scope and highlights:
  - Linear regression with non-convex regularization (n = 4096; pretrain mÌƒ = 90,000; fine-tune mÌ‚ = 10,000; LÌ‚ â‰ˆ 12,305.3): In the stochastic setting (batch = 100 = 1% of data), Bernoulli-LoRA-PAGE markedly reduces variance and converges to target tolerance (gradient norm squared drops from ~10^3 to <10^âˆ’15 over data passes), while SGD variants (RAC-LoRA-SGD A/B and Bernoulli-LoRA-SGD) stall; step sizes Î³ = c/LÌ‚ with c tuned per method.
  - MNIST MLP transfer (pretrain on digits 0â€“4, adapt to 5â€“9), r = 1, 50 epochs, AdamW (Î²1 = 0.9, Î²2 = 0.999, Îµ = 1eâˆ’8), lr = 2eâˆ’4, batch 128, 20 seeds, medianÂ±std:
    - FPFT: 99.5 (54,700 params).
    - LoRA (Gaussian/Zero): 85.69 Â± 1.60 (1K), LoRA (Zero/Gaussian): 89.82 Â± 0.90 (1K).
    - COLA: 93.32 Â± 0.50 (G/Z) and 96.55 Â± 0.20 (Z/G), 1K.
    - AsymmLoRA: 64.04 Â± 6.90 (G/Z, 133) and 74.52 Â± 7.20 (Z/G, 912).
    - RAC-LoRA: 93.02 Â± 0.50 (G/Z, 133) and 96.49 Â± 0.20 (Z/G, 912).
    - Bernoulli-LoRA: 96.46 Â± 0.17 with p = 0.99, expected trainable â‰ˆ pÂ·912 + (1âˆ’p)Â·133 â‰ˆ 904; used deterministic A/B assignment at initialization despite the probabilistic prescription.
- Caveats explicitly stated: Full gradient methods are impractical at large scale (used for theory). Some stepsize constants and bounds rely on assumptions (Expected Smoothness, bounded variance, PL). Practicality of Assumption 1 is argued via Gaussian sketches (E[H] = (r/n) I). Detailed hyperparameter sweep ranges or computational budgets outside those reported are not specified.

# Abstract
- Motivation: PEFT is vital for adapting large models efficiently; LoRA is effective but lacks comprehensive theory. RAC-LoRA offered early analysis.
- Proposal: Bernoulli-LoRA, a probabilistic framework that flips a Bernoulli coin each step to decide whether to update A or B; other matrix is sampled and fixed.
- Scope: Variants include Bernoulli-LoRA-GD, -SGD, -PAGE, -MVR, and FL variants -QGD, -MARINA, -EF21.
- Theory: Convergence guarantees for each variant under standard non-convex assumptions; extended to convex, non-smooth with constant and Polyak-type stepsizes.
- Empirics: Extensive experiments validating theory; demonstrates practical efficacy.

# Introduction
- Background: PEFT updates a small subset of parameters; LoRA parameterizes Î”W = (Î±/r) B A added to W^0. Typically A is Gaussian-initialized and B zero-initialized; r â‰ª min{m, n}; Î± acts as stepsize. LoRA reduces parameters and mitigates forgetting.
- COLA: Sequentially trains chains of low-rank modules to approximate higher-rank updates W = W^0 + (Î±/r) âˆ‘_{t=0}^{Tâˆ’1} B^t A^t, improving on non-low-rank adaptations without direct high-rank fitting.
- Contributions:
  - Bernoulli-LoRA: Randomized, generic framework choosing A or B to train via a Bernoulli trial; other factor drawn from a distribution and fixed that step. Generalizes existing LoRA update strategies and operates sequentially over T low-rank updates.
  - Foundational variants: Bernoulli-LoRA-GD (full gradients) and -SGD (unbiased stochastic gradient).
  - Variance reduction in non-convex settings: Bernoulli-LoRA-PAGE (finite-sum; PAGE estimator) and -MVR (expectation; STORM-inspired momentum VR).
  - Federated Learning extensions: Fed-Bernoulli-LoRA-QGD (QSGD-style quantization), -MARINA (compressed gradient differences), -EF21 (error feedback with contractive compressors). Claimed as first comprehensive theoretical analyses of LoRA-type methods with communication-efficient FL techniques.
  - Non-smooth convex analysis: First theoretical analysis for LoRA-type methods in this class; rates for constant and Polyak stepsizes.
- Summary table of convergence (non-convex â€œNCâ€ and PL â€œPÅâ€ rates; constants omitted):
  - Bernoulli-LoRA-GD: NC Î”^0/(Î³ Î»_min T); PL (1 âˆ’ Î³ Î¼ Î»_min)^T Î”^0.
  - Bernoulli-LoRA-SGD: NC Î”^0/(Î³ Î»_min T) + Î³ L C1 Î»_max/Î»_min; PL (1 âˆ’ Î³ Î¼ Î»_min)^T Î”^0 + Î³ L C1/(Î¼) Â· Î»_max/Î»_min.
  - Bernoulli-LoRA-MVR: NC Î¦1/(Î³ Î»_min T) + [b Ïƒ^2 Î»_max]/[(2âˆ’b) Î»_min]; PL (1 âˆ’ Î³ Î¼ Î»_min)^T Î¦1 + [b Ïƒ^2 Î»_max]/[(2âˆ’b) Î¼ Î»_min]; Î¦1 := Î”^0 + (b Î³/(2âˆ’b)) ğ’¢^0.
  - Bernoulli-LoRA-PAGE: NC Î¦2/(Î³ Î»_min T); PL (1 âˆ’ Î³ Î¼ Î»_min)^T Î¦2; Î¦2 := Î”^0 + (Î³/q) ğ’¢^0.
  - Fed-Bernoulli-LoRA-QGD: NC Î”^0/(Î³ Î»_min T) + Î³ L Ï‰ Î”* Î»_max/(M Î»_min); PL (1 âˆ’ Î³ Î¼ Î»_min)^T Î”^0 + Î³ L^2 Ï‰ Î»_max/(M Î¼ Î»_min).
  - Fed-Bernoulli-LoRA-MARINA: NC Î¦2/(Î³ Î»_min T); PL (1 âˆ’ Î³ Î¼ Î»_min)^T Î¦2.
  - Fed-Bernoulli-LoRA-EF21: NC Î¦3/(Î³ Î»_min T); PL (1 âˆ’ Î³ Î¼ Î»_min)^T Î¦3; Î¦3 := Î”^0 + [Î³/(1âˆ’âˆš(1âˆ’Î²))] ğ’¢^0.
  - Notation: Î»_min = p Î»_min^{HB} + (1âˆ’p) Î»_min^{HA}, Î»_max analogously; q is PAGE frequency; Î² contractive compressor parameter; b momentum; Ï‰ compression; Î”* heterogeneity gap; T chain length.

# Preliminaries
- Problem: Minimize f(W^0 + Î”W) over Î”W âˆˆ â„^{mÃ—n}.
- Stochastic settings:
  - Finite-sum: f(W^0 + Î”W) = (1/N) âˆ‘_{i=1}^N f_i(W^0 + Î”W).
  - Expectation: f(W^0 + Î”W) = E_{Î¾âˆ¼ğ”»}[f_Î¾(W^0 + Î”W)].
- Federated objective: f(W^0 + Î”W) = (1/M) âˆ‘_{l=1}^M f_l(W^0 + Î”W).
- Notation:
  - Frobenius norm and trace inner product; Î”^0 := f(W^0) âˆ’ f*, ğ’¢^0 := ||G^0 âˆ’ âˆ‡f(W^0)||_F^2, Äœ^0 := (1/M)âˆ‘ ||G_l^0 âˆ’ âˆ‡f_l(W^0)||_F^2.
  - Pseudoinverse â€ .

# Motivation
- Gaps: LoRA reparameterization can render smooth f non-smooth in (A, B), complicating analysis. COLAâ€™s analysis optimizes Î”W in full rank, not modeling the low-rank core. Many LoRA methods remain heuristic and sensitive to hyperparameters; divergence examples exist (e.g., COLA).
- RAC-LoRA initiated theory but lacks optimal VR analyses for non-convex settings and coverage of advanced FL setups with compression and error feedback.
- Need: A unified, theoretically justified framework that handles stochastic training, VR, and FL communications constraints for low-rank updates.

# Method
- Framework (Algorithm 1):
  - At step t, sample c^t âˆ¼ Be(p).
  - Left sketch: sample B_S^t âˆ¼ ğ’Ÿ_B, optimize Ã‚^t in Î”W = (Î±/r) B_S^t Ã‚^t.
  - Right sketch: sample A_S^t âˆ¼ ğ’Ÿ_A, optimize Ì‚B^t in Î”W = (Î±/r) Ì‚B^t A_S^t.
- Projected update equivalence:
  - With one GD step on the subproblem, Left sketch yields W^{t+1} = W^t âˆ’ Î³ H_B^t âˆ‡f(W^t), with H_B^t = B_S^t ((B_S^t)^âŠ¤ B_S^t)^â€  (B_S^t)^âŠ¤.
  - Right sketch yields W^{t+1} = W^t âˆ’ Î³ âˆ‡f(W^t) H_A^t, with H_A^t = (A_S^t)^âŠ¤ (A_S^t (A_S^t)^âŠ¤)^â€  A_S^t.
  - Unified form: W^{t+1} = W^t âˆ’ Î³ Äœ^t with Äœ^t = H_B^t G^t (w.p. p) or G^t H_A^t (w.p. 1âˆ’p), G^t a base estimator.
- Variants and base estimators (Table 2):
  - GD: G^t = âˆ‡f(W^t).
  - SGD: G^t = unbiased stochastic gradient g(W^t).
  - MVR: G^{t} = âˆ‡f_{Î¾^t}(W^t) + (1âˆ’b)(G^{tâˆ’1} âˆ’ âˆ‡f_{Î¾^t}(W^{tâˆ’1})).
  - PAGE (finite-sum): G^t = âˆ‡f(W^t) w.p. q; else G^{tâˆ’1} + âˆ‡f_{i_t}(W^t) âˆ’ âˆ‡f_{i_t}(W^{tâˆ’1}).
  - Fed-QGD: G^t = (1/M) âˆ‘_l ğ’¬_l^t(âˆ‡f_l(W^t)) (unbiased compressor with variance parameter Ï‰).
  - Fed-MARINA: per-client G_l^t uses full gradient w.p. q or adds compressed gradient difference; server averages.
  - Fed-EF21: per-client G_l^t = G_l^{tâˆ’1} + ğ’_l^t(âˆ‡f_l(W^t) âˆ’ G_l^{tâˆ’1}) (contractive compressor with parameter Î²); server averages.
- Assumptions:
  - Assumption 1 (Positive Expected Projection): Î»_min(E[H]) > 0. Gaussian sketches imply E[H] = (r/n) I (Appendix B), so Î»_min = r/n.
  - Assumption 2: f is lower bounded.
  - Assumption 3: L-Lipschitz gradient (smoothness).
  - Assumption 4 (Expected Smoothness for SGD): E||g(W)||^2 â‰¤ 2A1(f(W) âˆ’ f*) + B1||âˆ‡f(W)||^2 + C1.
  - Assumption 5 (Bounded Variance): E||âˆ‡f_Î¾(W) âˆ’ âˆ‡f(W)||^2 â‰¤ Ïƒ^2.
  - Assumption 6 (PÅ): (1/2)||âˆ‡f(W)||^2 â‰¥ Î¼(f(W) âˆ’ f*).
  - Non-smooth convex regime: convex, L_0-Lipschitz; and E[H] = Î± I.
- Convergence highlights (step-size conditions provided in-section; here are bounds):
  - Theorem 1 (GD, non-convex): E||âˆ‡f(áº†^T)||^2 â‰¤ 2Î”^0/(Î³ Î»_min^p T) with 0 < Î³ â‰¤ 1/L.
  - Theorem 2 (SGD, non-convex): E||âˆ‡f(áº†^T)||^2 â‰¤ 6Î”^0/(Î³ Î»_min^p T) + Î³ L C1 (Î»_max^p/Î»_min^p), with Î³ bounded by problem constants (min of terms depending on L, A1, B1, Î»_min^p, Î»_max^p, T).
  - Theorem 3 (MVR, non-convex): E||âˆ‡f(áº†^T)||^2 â‰¤ 2Î”^0/(Î³ Î»_min^p T) + (ğ’¢^0/(bT) + 2bÏƒ^2/(2âˆ’b)) (Î»_max^p/Î»_min^p).
  - Theorem 4 (PAGE, non-convex): E||âˆ‡f(áº†^T)||^2 â‰¤ 2Î”^0/(Î³ Î»_min^p T) + (ğ’¢^0/(qT)) (Î»_max^p/Î»_min^p).
  - Theorem 5 (Fed-QGD, non-convex): E||âˆ‡f(áº†^T)||^2 â‰¤ 6Î”^0/(Î³ Î»_min^p T) + (2Î³ L Ï‰ Î”*/M) (Î»_max^p/Î»_min^p).
  - Theorem 6 (Fed-MARINA, non-convex): E||âˆ‡f(áº†^T)||^2 â‰¤ 2Î”^0/(Î³ Î»_min^p T) + (ğ’¢^0/(qT)) (Î»_max^p/Î»_min^p).
  - Theorem 7 (Fed-EF21, non-convex): E||âˆ‡f(áº†^T)||^2 â‰¤ 2Î”^0/(Î³ Î»_min^p T) + (2Äœ^0/(Î²T)) (Î»_max^p/Î»_min^p).
  - Theorem 8 (SGD under PÅ): E[f(W^T) âˆ’ f*] â‰¤ (1 âˆ’ Î³ Î¼ Î»_min^p/2)^T Î”^0 + (Î³ L C1/Î¼) (Î»_max^p/Î»_min^p).
  - Non-smooth convex (Theorem 10): constant stepsize rate R_0^2/(2Î³ Î± T) + Î³ L_0^2/2; with Î³* gives R_0 L_0/âˆš(Î± T); Polyak stepsize matches R^0 L_0/âˆš(Î± T).

# Experiments
- 8.1 Linear Regression with Non-convex Regularization:
  - Pretraining problem (n = 4096, mÌƒ = 9Ã—10^4): minimize 1/(2 mÌƒ) ||DÌƒ x âˆ’ bÌƒ||^2 + Î»Ìƒ âˆ‘ x_j^2/(1 + x_j^2); Î»Ìƒ = ||DÌƒ||_2 â‰ˆ 18.2; LÌƒ â‰ˆ 54.7; optimize to ||âˆ‡fÌƒ(xÌƒ*)||^2 â‰¤ 10^âˆ’8.
  - Fine-tuning problem (n = 4096, mÌ‚ = 10^4): same objective form; Î»Ì‚ = ||DÌ‚||_2 â‰ˆ 4101.7; LÌ‚ â‰ˆ 12305.3; initialization xÌƒ*.
  - Stochastic setting: batch size 100 (1% of data). Compared RAC-LoRA-SGD (A trains B after resampling A; B trains A analogously), Bernoulli-LoRA-SGD, Bernoulli-LoRA-PAGE; all methods used Î³ = c/LÌ‚ with c tuned per method.
  - Result (Figure 1): Bernoulli-LoRA-PAGE (e.g., p = 0.5) reduces ||âˆ‡f(x)||^2 from ~10^3 to below 10^âˆ’15 within ~100â€“200 data passes; all SGD variants flatten at higher error levels. Claimed evidence of effective variance reduction and convergence.
- 8.2 MLP on MNIST:
  - Setup: Pretrain 3-layer MLP on digits 0â€“4; fine-tune to classify digits 5â€“9; rank r = 1; epochs = 50; optimizer AdamW (Î²1 = 0.9, Î²2 = 0.999, Îµ = 10^âˆ’8); lr = 2Ã—10^âˆ’4; batch = 128; 20 independent seeds; report median accuracy Â± std; select best median for Bernoulli-LoRA. Code: https://github.com/IgorSokoloff/Bernoulli-LoRA_experiments.
  - Results (Table 3):
    - FPFT: 99.5 (54,700 trainable).
    - LoRA: Gaussian/Zero 85.69 Â± 1.60 (1K); Zero/Gaussian 89.82 Â± 0.90 (1K).
    - COLA: Gaussian/Zero 93.32 Â± 0.50 (1K); Zero/Gaussian 96.55 Â± 0.20 (1K).
    - AsymmLoRA: Gaussian/Zero 64.04 Â± 6.90 (133); Zero/Gaussian 74.52 Â± 7.20 (912).
    - RAC-LoRA: Gaussian/Zero 93.02 Â± 0.50 (133); Zero/Gaussian 96.49 Â± 0.20 (912).
    - Bernoulli-LoRA: 96.46 Â± 0.17 with ğ’Ÿ_A = Zero (deterministically fixed at initialization for better performance), ğ’Ÿ_B = Gaussian; p = 0.99 â‡’ expected trainable parameters â‰ˆ 904 (pÂ·912 + (1âˆ’p)Â·133).
  - Discussion (as stated): Bernoulli-LoRA matches RAC-LoRA test accuracy with comparable parameter efficiency and theoretical guarantees; both train one matrix per LoRA block; COLA trains two.
- Additional experimental details (Appendix E):
  - Full gradient setting (Figures 2a/b): Bernoulli-LoRA-GD and RAC-LoRA-GD exhibit similar convergence across p, for r = 1 and r = 2; Î³ = c/LÌ‚ with c âˆˆ {1, 2}; higher r accelerates convergence (consistent with r/n factor).
  - Hardware: Python 3.10 on CPUs (AMD EPYC 7702 64-Core; Intel Xeon Gold 6148 2.40 GHz; Intel Xeon Gold 6248 2.50 GHz).
  - Stopping: median over 20 seeds; termination when ||âˆ‡f(x^t)||_2^2 â‰¤ 5Ã—10^âˆ’16 or max iterations.

# Conclusion
- Impact statement provided: goal is to advance ML; no specific societal consequences highlighted beyond general potential impacts. No separate concluding claims on limitations or future work provided in this section.

# Appendix
- A: Basic inequalities (tower property, CBS, variance decomposition, Jensen) used in proofs.
- B: Assumption 1 discussion. Lemma 1 (rotational invariance implies scalar matrix). Lemma 2: For Gaussian sketches, E[H_B] = E[H_A] = (r/n) I_n, hence Î»_min = r/n.
- C: Detailed proofs for core variants:
  - C.1 Bernoulli-LoRA-GD: lemma showing projected GD form; Theorems for non-convex (rate 2Î”^0/(Î³ Î»_min^p T)), PL (linear rate (1 âˆ’ Î³ Î¼ Î»_min^p)^T Î”^0), and non-smooth convex (Algorithm 3 with subgradient form and rates in Theorem 10).
  - C.2 Bernoulli-LoRA-SGD: Theorems 11 (non-convex) and 12 (PL) with weighted sampling analysis; explicit stepsize conditions and weighted iterate selection.
  - C.3 Bernoulli-LoRA-MVR: Lemmas 5â€“6 for descent and estimator recursion; Theorems 13 (non-convex) and 14 (PL) with Lyapunov function Î¦_t incorporating estimator error; stepsize depends on L, b, Î»_max^p.
  - C.4 Bernoulli-LoRA-PAGE: Lemma 7; Theorems 15 (non-convex) and 16 (PL) with Î¦_t including 1/q factor; stepsize depends on L, q, Î»_max^p.
- D: Federated extensions:
  - D.1 Fed-Bernoulli-LoRA-QGD: Unbiased compressor definition (E[ğ’¬(W)] = W, E||ğ’¬(W) âˆ’ W||^2 â‰¤ Ï‰||W||^2); assumption on client dissimilarity (Î”* â‰¥ 0); Lemma 8 shows expected smoothness constants A1 = L Ï‰/M, B1 = 1, C1 = 2L Ï‰ Î”*/M; Theorems 17 (non-convex) and 18 (PL) with rates and stepsize ranges in terms of Ï‰/M and Î»_min^p, Î»_max^p.
  - D.2 Fed-Bernoulli-LoRA-MARINA: Lemma 9 bounds estimator error using Ï‰ and L; Theorems 19 (non-convex) and 20 (PL) with stepsize depending on âˆš(Î»_max^pÂ·(1âˆ’q)/qÂ·Ï‰/M).
  - D.3 Fed-Bernoulli-LoRA-EF21: Contractive compressor definition with parameter Î² (E||ğ’(W) âˆ’ W||^2 â‰¤ (1âˆ’Î²)||W||^2); Lemma 10 per-client recursion with âˆš(1âˆ’Î²); Theorems 21 (non-convex) and 22 (PL) with stepsize depending on âˆš(Î»_max^p(1âˆ’Î²)/(1âˆ’âˆš(1âˆ’Î²))).
- E: Experimental details (data generation via sklearn make_regression with specified parameters; scaling; stopping criteria; hardware; figures).

# References
- Cited works include LoRA [Hu et al., 2022], PAGE [Li et al., 2021], MVR/STORM [Cutkosky and Orabona, 2019], MARINA [Gorbunov et al., 2021], EF21 [RichtÃ¡rik et al., 2021], RAC-LoRA [Malinovsky et al., 2024], COLA [Xia et al., 2024], surveys and optimization texts (e.g., Beck, Nesterov), compression literature (QSGD, TernGrad, Natural Compression), and supporting theory on smoothness and PL conditions. Quantitative constants and assumptions are specified in the main text (e.g., Ï‰, Î², b, q, L, Î¼).