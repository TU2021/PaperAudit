# Global Summary
This paper introduces Bernoulli-LoRA, a theoretical framework for Low-Rank Adaptation (LoRA) that aims to provide rigorous convergence guarantees for this class of parameter-efficient fine-tuning (PEFT) methods. The core idea is a probabilistic mechanism where a Bernoulli trial at each step determines which of the two low-rank factor matrices (A or B) is updated. This approach unifies and generalizes existing LoRA update strategies. The authors reformulate the update as a projected gradient step, which circumvents the non-smoothness issue inherent in the LoRA reparameterization.

The paper provides a comprehensive theoretical analysis under standard non-convex optimization assumptions (e.g., L-smoothness). It establishes convergence guarantees for seven algorithmic variants: Bernoulli-LoRA-GD, -SGD, -PAGE, -MVR, and three federated learning extensions, Fed-Bernoulli-LoRA-QGD, -MARINA, and -EF21, which incorporate techniques like quantization and error feedback. The analysis is also extended to non-smooth convex functions and functions satisfying the Polyak-Łojasiewicz (PŁ) condition, providing linear convergence rates.

Experiments on a non-convex linear regression task show that the variance-reduced Bernoulli-LoRA-PAGE variant converges to a high-precision solution where SGD-based methods stall. On an MLP fine-tuning task on MNIST, Bernoulli-LoRA achieves a median test accuracy of 96.46 ± 0.17%, matching the performance of the state-of-the-art RAC-LoRA baseline while offering a more flexible and general theoretical framework.

# Abstract
The paper addresses the limited theoretical understanding of Low-Rank Adaptation (LoRA), a popular parameter-efficient fine-tuning (PEFT) method. It introduces Bernoulli-LoRA, a novel theoretical framework that uses a probabilistic Bernoulli mechanism to select which of the two low-rank matrices to update at each step. This framework is shown to unify and generalize existing LoRA approaches. The authors provide convergence guarantees for several variants of their framework, including those based on GD, SGD, PAGE, MVR, QGD, MARINA, and EF21, under standard non-convex optimization assumptions. The analysis is also extended to convex, non-smooth functions with both constant and adaptive stepsizes. Experimental results are presented to validate the theoretical findings and demonstrate the practical effectiveness of the proposed methods.

# Introduction
- Fine-tuning large pre-trained models is computationally expensive, motivating Parameter-Efficient Fine-Tuning (PEFT) methods. LoRA is a prominent PEFT technique that updates a weight matrix $W^0$ by adding a low-rank product: $W = W^0 + \frac{\alpha}{r}BA$, where only $A$ and $B$ are trainable.
- Chain of LoRA (COLA) is an iterative extension that applies successive LoRA updates: $W = W^0 + \frac{\alpha}{r} \sum_{t=0}^{T-1} B^t A^t$, allowing for higher-rank adaptations over time.
- **Contributions:**
    - **Bernoulli-LoRA Framework:** A generic low-rank adaptation framework where a Bernoulli trial at each step probabilistically determines whether to update matrix A or B. This unifies existing update strategies.
    - **Foundational Variants:** Provides convergence analysis for Bernoulli-LoRA-GD and Bernoulli-LoRA-SGD.
    - **Advanced Variance Reduction (VR) Variants:** Introduces and analyzes Bernoulli-LoRA-PAGE and Bernoulli-LoRA-MVR, claiming to be the first theoretical analysis of LoRA-type methods with these advanced VR schemes in non-convex settings.
    - **Federated Learning (FL) Extensions:** Develops and analyzes Fed-Bernoulli-LoRA-QGD, -MARINA, and -EF21, claiming to be the first comprehensive theoretical analysis of LoRA-type methods integrated with communication-efficient FL techniques like quantization, gradient difference compression, and error feedback.
    - **Non-Smooth Convex Analysis:** Presents the first theoretical analysis of LoRA-type methods for non-smooth convex optimization problems, including convergence for constant and Polyak-type stepsizes.
- A summary table (Table 1) presents convergence rates for all proposed methods in non-convex (NC) and Polyak-Łojasiewicz (PŁ) settings. For example, Bernoulli-LoRA-GD has an NC rate of $\mathcal{O}(\frac{\Delta^0}{T})$ and a PŁ rate of $\mathcal{O}((1 - \gamma \mu \lambda_{\min})^T)$. Rates for stochastic and federated methods include additional terms related to variance, compression, or data heterogeneity.

# Preliminaries
- The core problem is formulated as minimizing an objective function over an adaptation term $\Delta W$: $\min_{\Delta W \in \mathbb{R}^{m \times n}} f(W^0 + \Delta W)$.
- The paper considers three specific problem structures:
    - **Finite-Sum:** $f(W) = \frac{1}{N} \sum_{i=1}^{N} f_i(W)$, for methods like PAGE.
    - **Expectation:** $f(W) = \mathbb{E}_{\xi \sim \mathcal{D}} [f_{\xi}(W)]$, for methods like MVR.
    - **Federated Learning:** $f(W) = \frac{1}{M} \sum_{l=1}^{M} f_l(W)$, for the distributed methods.
- Notation includes the Frobenius norm $\|\cdot\|_F$, trace inner product $\langle A, B \rangle$, and definitions for initial suboptimality $\Delta^0 := f(W^0) - f^*$ and initial gradient estimator error $\mathcal{G}^0 := \|G^0 - \nabla f(W^0)\|_F^2$.

# Motivation
- A significant gap exists in the theoretical understanding of LoRA and its variants like COLA.
- The LoRA re-parameterization transforms a smooth loss into a non-smooth one, complicating analysis.
- Existing theoretical work on COLA analyzes full-rank updates, failing to capture the essence of LoRA's low-rank structure.
- Most LoRA-based methods are heuristics, sensitive to hyperparameters, and can even diverge, as shown for COLA by Malinovsky et al. [2024].
- RAC-LoRA provided the first rigorous framework but did not cover optimal variance-reduced techniques for non-convex optimization or advanced Federated Learning settings with communication compression and error feedback.
- This work is motivated by the need to extend a theoretically sound LoRA framework to these advanced and practical optimization scenarios.

# Method
- **Bernoulli-LoRA Framework:** At each iteration $t$, a Bernoulli random variable $c^t \sim Be(p)$ is sampled. If $c^t=1$ (with probability $p$), a "Left sketch" is performed where $B_S^t$ is sampled and fixed, and matrix $\hat{A}^t$ is optimized. If $c^t=0$ (with probability $1-p$), a "Right sketch" is performed where $A_S^t$ is fixed and $\hat{B}^t$ is optimized.
- **Reformulation as Projected Gradient Step:** The update is shown to be equivalent to a projected gradient step: $W^{t+1} = W^t - \gamma \hat{G}^t$. The projected gradient estimator is $\hat{G}^t = H_B^t G^t$ with probability $p$, and $\hat{G}^t = G^t H_A^t$ with probability $1-p$, where $H_A^t$ and $H_B^t$ are projection matrices. This reformulation avoids the non-smoothness issue of direct LoRA optimization.
- **Algorithmic Variants:** The paper defines several methods based on the choice of the base gradient estimator $G^t$:
    - **Bernoulli-LoRA-GD:** $G^t = \nabla f(W^t)$.
    - **Bernoulli-LoRA-SGD:** $G^t$ is an unbiased stochastic gradient.
    - **Bernoulli-LoRA-PAGE:** $G^t$ is updated using the PAGE rule (full gradient with probability $q$, variance-reduced update otherwise).
    - **Bernoulli-LoRA-MVR:** $G^t$ is updated using a momentum variance reduction rule.
- **Federated Learning Extensions:**
    - **Fed-Bernoulli-LoRA-QGD:** Clients send quantized local gradients to the server.
    - **Fed-Bernoulli-LoRA-MARINA:** Clients use gradient difference compression.
    - **Fed-Bernoulli-LoRA-EF21:** Clients use the EF21 error feedback mechanism with contractive compressors.
- **Convergence Results:**
    - Analysis relies on key assumptions like L-smoothness (Asm. 3) and Positive Expected Projection (Asm. 1), which states $\lambda_{\min}[\mathbb{E}[H]] > 0$. This is shown to hold for Gaussian sampling where $\mathbb{E}[H] = (r/n)I_n$.
    - For smooth non-convex functions, the goal is to find an $\varepsilon$-stationary point ($\mathbb{E}[\|\nabla f(\hat{W})\|^2] \leq \varepsilon^2$).
    - **Theorem 1 (Bernoulli-LoRA-GD):** Under L-smoothness, converges with rate $\mathbb{E}[\|\nabla f(\widetilde{W}^T)\|^2] \leq \frac{2\Delta^0}{\gamma \lambda_{\min}^p T}$.
    - **Theorem 2 (Bernoulli-LoRA-SGD):** Under expected smoothness, converges with rate $\mathcal{O}(\frac{\Delta^0}{T}) + \mathcal{O}(\gamma)$.
    - Theorems 3-7 provide convergence rates for MVR, PAGE, QGD, MARINA, and EF21 variants, respectively.
    - Under the Polyak-Łojasiewicz (PŁ) condition (Asm. 6), linear convergence rates are established. For example, **Theorem 8 (Bernoulli-LoRA-SGD)** shows a rate of $\mathbb{E}[f(W^T) - f^*] \leq (1 - \gamma\mu\lambda_{\min}^p/2)^T \Delta^0 + \text{noise term}$.

# Experiments
- **Linear Regression with Non-convex Regularization:**
    - A synthetic task with pre-training ($\tilde{m}=9 \times 10^4, n=4096$) and fine-tuning ($\hat{m}=10^4, n=4096$) phases.
    - In the stochastic setting (batch size 100), Bernoulli-LoRA-PAGE converges to a high-precision solution ($\|\nabla f(x)\|^2 < 10^{-15}$), while Bernoulli-LoRA-SGD and RAC-LoRA-SGD stall at a much higher error ($\approx 10^0 - 10^3$), demonstrating the effectiveness of variance reduction.
    - In the full-gradient setting (Appendix), Bernoulli-LoRA-GD and RAC-LoRA-GD show similar convergence behavior.
- **MLP on MNIST:**
    - A three-layer MLP is pre-trained on digits 0-4 and fine-tuned on digits 5-9.
    - Setup: rank $r=1$, 50 epochs, AdamW optimizer, LR $2 \times 10^{-4}$, batch size 128. Results are median of 20 runs.
    - **Results (Table 3):**
        - Full fine-tuning (FPFT): 99.5% accuracy.
        - COLA: 96.55 ± 0.20%.
        - RAC-LoRA: 96.49 ± 0.20%.
        - **Bernoulli-LoRA (p=0.99): 96.46 ± 0.17% accuracy.**
    - The paper claims Bernoulli-LoRA matches the performance of RAC-LoRA while being theoretically grounded and more flexible. With $p=0.99$, it uses an expected parameter count of $\approx 904$, compared to 912 for the best RAC-LoRA variant.

# Conclusion
The paper states its goal is to advance the field of Machine Learning and does not highlight any specific societal consequences.

# Appendix
- **Appendix B:** Provides a detailed justification for Assumption 1 (Positive Expected Projection). It proves that for random matrices with i.i.d. Gaussian entries, the expected projection matrix is a scaled identity matrix, $\mathbb{E}[H] = (r/n)I_n$, thus its minimum eigenvalue is $r/n > 0$.
- **Appendix C & D:** Contain detailed mathematical proofs for the convergence theorems of all core and federated learning algorithmic variants, respectively. This includes analysis for smooth non-convex, PŁ, and non-smooth convex settings.
- **Appendix E:** Provides additional experimental details.
    - **Linear Regression:** Includes results for the full-gradient setting, showing that Bernoulli-LoRA-GD and RAC-LoRA-GD perform similarly, and that a higher rank ($r=2$ vs $r=1$) leads to faster convergence.
    - **Hardware:** Experiments were run on AMD EPYC 7702 and Intel Xeon Gold CPUs.
    - **Implementation:** Implemented in Python 3.10. Stepsize was set to $\gamma = c/\hat{L}$ with $c$ tuned. Termination criterion was $\|\nabla f(x^t)\|_2^2 \leq 5 \times 10^{-16}$. Median of 20 runs reported.
    - **Datasets:** Details the `sklearn.datasets.make_regression` parameters used to generate the synthetic data for the linear regression task.

# References
The paper includes an extensive list of references covering PEFT, LoRA, optimization theory (stochastic, non-convex, federated), and related machine learning concepts. Key cited works include Hu et al. [2022] for LoRA, Malinovsky et al. [2024] for RAC-LoRA, and various papers on optimization algorithms like PAGE, MVR, MARINA, and EF21.