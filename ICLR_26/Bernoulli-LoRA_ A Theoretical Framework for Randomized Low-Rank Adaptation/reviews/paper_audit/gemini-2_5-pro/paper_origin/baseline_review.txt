1) Summary
This paper introduces Bernoulli-LoRA, a theoretical framework for Low-Rank Adaptation (PEFT) methods. The core idea is a probabilistic mechanism where a Bernoulli trial at each step determines which of the two low-rank matrices (A or B) is updated. This approach is presented as a generalization of existing LoRA update strategies. The authors reformulate the update as a projected gradient step, enabling a unified and rigorous convergence analysis. They develop and analyze numerous variants, including foundational methods (GD, SGD), advanced variance-reduced methods (PAGE, MVR), and communication-efficient federated learning algorithms (QGD, MARINA, EF21). For these variants, the paper establishes convergence guarantees for smooth non-convex, Polyak-Łojasiewicz, and non-smooth convex optimization settings. Experimental results on linear regression and MNIST classification are provided to validate the theoretical findings.2) Strengths
*   **Comprehensive and Unified Theoretical Framework:** The paper proposes a general framework that successfully unifies different LoRA-style update schemes under a single probabilistic model. The reformulation of the update as a projected gradient step is elegant and provides a solid foundation for rigorous analysis.
    *   The core update is expressed as a projected gradient estimator (Section 6.1, Equations 7 and 8), which allows leveraging a wide range of existing optimization theory.
    *   The framework is shown to encompass various update strategies by simply changing the base gradient estimator $G^t$ (Table 2), demonstrating its generality.
    *   The convergence analysis is extensive, covering multiple settings (smooth non-convex, PŁ, non-smooth convex) and providing detailed proofs for each variant (Appendices C and D). This significantly advances the theoretical understanding of LoRA beyond existing heuristic approaches.*   **Broad Scope of Algorithmic Variants and Settings:** The work demonstrates the versatility of the Bernoulli-LoRA framework by instantiating it with a large family of modern and practical optimization algorithms.
    *   It covers foundational methods like GD (Algorithm 2) and SGD (Algorithm 4), providing a baseline understanding.
    *   It incorporates state-of-the-art variance reduction techniques like PAGE (Algorithm 6) and MVR (Algorithm 5), which are crucial for efficient training on large datasets.
    *   It extends the framework to the challenging federated learning setting, proposing variants with communication compression (Fed-Bernoulli-LoRA-QGD, Algorithm 7; Fed-Bernoulli-LoRA-MARINA, Algorithm 8) and error feedback (Fed-Bernoulli-LoRA-EF21, Algorithm 9).
    *   The analysis is extended to non-smooth convex functions (Section 4, Algorithm 3), broadening the applicability of the framework to a wider class of optimization problems.*   **Novel Theoretical Contributions for LoRA:** The paper presents what appear to be the first theoretical analyses of LoRA-type methods in several important and practical contexts.
    *   The authors claim to provide the first convergence guarantees for LoRA-type methods that incorporate advanced variance-reduction schemes like PAGE and MVR in non-convex settings (Section 4, Contributions).
    *   The work is presented as the first to theoretically analyze LoRA-type methods integrated with established communication-efficient federated learning techniques like quantization, gradient difference compression, and error feedback (Section 4, Contributions).
    *   The paper also claims to be the first to provide a theoretical analysis of LoRA-type methods for non-smooth convex optimization problems (Section 4, Contributions). These contributions fill significant gaps in the literature.*   **High Clarity and Quality of Presentation:** The paper is exceptionally well-written and structured, making complex theoretical material accessible.
    *   The motivation is clearly articulated (Section 3), positioning the work against the limitations of prior art like COLA and RAC-LoRA.
    *   Table 1 provides a concise and highly effective summary of the main convergence results, allowing for easy comparison between the different algorithmic variants.
    *   Table 2 clearly defines the base gradient estimator for each proposed method, reinforcing the unified nature of the framework.
    *   The appendices are detailed and appear to be self-contained, providing full proofs for all theoretical claims (Appendices C, D).3) Weaknesses
*   **Limited Empirical Validation:** The experiments, while illustrative of the theory, are conducted on small-scale, synthetic, or toy problems that do not reflect the typical use cases of LoRA, which are predominantly large-scale models.
    *   The primary experiments are on a linear regression problem with non-convex regularization (Section 8.1) and a three-layer MLP on MNIST (Section 8.2). These settings are far removed from the large language and vision models where PEFT methods like LoRA are most impactful.
    *   The results do not demonstrate a clear empirical advantage of the proposed Bernoulli-LoRA framework over the deterministic RAC-LoRA baseline. In the full-gradient setting, their performance is identical (Appendix E.1, Figure 2). In the MLP experiment, the final accuracy is nearly the same (Table 3: 96.46% for Bernoulli-LoRA vs. 96.49% for RAC-LoRA).
    *   The main empirical win is for Bernoulli-LoRA-PAGE over SGD-based methods (Figure 1). While this validates the benefit of variance reduction, it is an expected outcome (VR > SGD) and does not specifically demonstrate an advantage of the Bernoulli randomization itself over a deterministic alternative like RAC-LoRA-PAGE.*   **Insufficient Justification for the Benefit of Randomization:** The paper's core novelty is the Bernoulli randomization, but its practical or theoretical advantage over a simpler deterministic update scheme is not clearly established.
    *   The theoretical rates presented in Table 1 depend on `lambda_min^p` and `lambda_max^p`, which are convex combinations of the eigenvalues corresponding to the deterministic choices. It is not shown whether a mixed strategy (Bernoulli probability `p` between 0 and 1) can ever yield a better convergence rate than a pure, deterministic strategy (`p=0` or `p=1`).
    *   The motivation for randomization is framed as a way to unify and generalize existing methods (Section 4). However, the paper does not articulate any other potential benefits, such as improved optimization dynamics or better exploration of the parameter space, that would make this generalization superior in practice.
    *   The empirical results (Figure 2, Table 3) support this concern, as they show no performance gain from using a probabilistic update schedule compared to a deterministic one.*   **Clarity and Implications of Theoretical Assumptions:** The analysis relies on key assumptions whose practical implications and effects on the final convergence rates could be discussed more prominently.
    *   The analysis hinges on Assumption 1 (Positive Expected Projection). While Appendix B provides a good justification for i.i.d. Gaussian sampling, this is a specific case. The convergence rates in Table 1 depend on the ratio `lambda_max^p / lambda_min^p`. For Gaussian sampling, this ratio is 1 (Appendix B, Lemma 2), but the sensitivity of the rates to this ratio under other sampling schemes is not discussed in the main paper.
    *   The paper correctly notes that the LoRA reparameterization induces non-smoothness and circumvents this by analyzing projected updates in the full parameter space (Section 7, Block 33). While this is a valid and powerful analytical technique, the connection between this theoretical model and the practical reality of applying optimizers like AdamW directly to the low-rank factors A and B could be made more explicit.
    *   The analysis for non-smooth convex functions relies on Assumption 10, which states `E[H] = alpha * I`. This is a stronger condition than Assumption 1 and is only shown to hold for i.i.d. Gaussian sampling (Appendix B, Lemma 2). The implications of this for other practical initialization/sampling schemes are not explored.4) Suggestions for Improvement
*   **Strengthen and Expand Empirical Validation:**
    *   To better demonstrate the practical relevance of the extensive theory, please consider adding experiments on a more representative task, such as fine-tuning a pre-trained transformer model (e.g., RoBERTa or a small T5) on a standard benchmark from the GLUE dataset.
    *   Design experiments to specifically isolate the contribution of the Bernoulli randomization. For example, a direct comparison between Bernoulli-LoRA-PAGE and a newly implemented RAC-LoRA-PAGE (which uses a deterministic update) on the linear regression task would clarify whether the randomization offers any benefit beyond what variance reduction provides.
    *   To validate the novel contributions in federated learning, please include at least one experiment comparing one of the proposed FL algorithms (e.g., Fed-Bernoulli-LoRA-MARINA) against a suitable baseline (e.g., FedAvg with LoRA, or Fed-RAC-LoRA from Malinovsky et al. [2024]) to demonstrate its effectiveness in a communication-constrained setting.*   **Provide a Clearer Rationale for Randomization:**
    *   In the theoretical discussion, please add a paragraph analyzing the derived convergence rates (Table 1) with respect to the Bernoulli probability `p`. Is there a theoretical regime where a mixed strategy (`0 < p < 1`) is provably superior to a deterministic one? If the benefit is primarily in creating a unified analytical tool, this should be stated more explicitly as the main motivation.
    *   In the introduction or motivation sections (e.g., Section 4), please expand on why a probabilistic update might be beneficial beyond theoretical unification. For instance, could it act as a form of regularization or help escape certain optimization traps? If such benefits are hypothesized, they could be explored empirically, even on a simple landscape.*   **Improve Discussion of Theoretical Assumptions:**
    *   Please consider moving the key insights from Appendix B regarding Assumption 1 into the main paper (e.g., Section 7). Specifically, a discussion on how the `lambda_max^p / lambda_min^p` ratio influences the rates in Table 1 and under which sampling distributions this ratio is well-behaved would significantly improve the paper's self-containedness and clarity.
    *   It would be beneficial to add a brief remark discussing the relationship between the projected gradient analysis and the direct optimization of LoRA factors. Acknowledging this as an analytical model of the true process and briefly touching on potential differences would add valuable nuance.
    *   For the non-smooth convex analysis, please clarify in the main text that Assumption 10 is stronger than Assumption 1 and discuss the types of practical sampling strategies (beyond i.i.d. Gaussian) for which it might hold.5) Score
- Overall (10): 7 — The paper presents a strong and comprehensive theoretical framework for LoRA, but the empirical validation is limited and does not fully demonstrate the practical advantages of the core randomization idea.
- Novelty (10): 8 — The Bernoulli-LoRA framework is a novel generalization, and the theoretical analyses for variance-reduced and federated settings are significant, claimed firsts (Section 4).
- Technical Quality (10): 9 — The theoretical analysis is extensive, rigorous, and detailed, with comprehensive proofs provided in the appendices (Appendices C, D).
- Clarity (10): 9 — The paper is exceptionally well-written and structured, with clear explanations and helpful summaries of the methods and results (Table 1, Table 2).
- Confidence (5): 4 — I am confident in my assessment, having reviewed the theoretical claims, the structure of the proofs, and the experimental setup.