{
  "baseline_review": "1) Summary\nThis paper introduces Bernoulli-LoRA, a theoretical framework for Low-Rank Adaptation (PEFT) methods. The core idea is a probabilistic mechanism where a Bernoulli trial at each step determines which of the two low-rank matrices (A or B) is updated. This approach is presented as a generalization of existing LoRA update strategies. The authors reformulate the update as a projected gradient step, enabling a unified and rigorous convergence analysis. They develop and analyze numerous variants, including foundational methods (GD, SGD), advanced variance-reduced methods (PAGE, MVR), and communication-efficient federated learning algorithms (QGD, MARINA, EF21). For these variants, the paper establishes convergence guarantees for smooth non-convex, Polyak-Łojasiewicz, and non-smooth convex optimization settings. Experimental results on linear regression and MNIST classification are provided to validate the theoretical findings.2) Strengths\n*   **Comprehensive and Unified Theoretical Framework:** The paper proposes a general framework that successfully unifies different LoRA-style update schemes under a single probabilistic model. The reformulation of the update as a projected gradient step is elegant and provides a solid foundation for rigorous analysis.\n    *   The core update is expressed as a projected gradient estimator (Section 6.1, Equations 7 and 8), which allows leveraging a wide range of existing optimization theory.\n    *   The framework is shown to encompass various update strategies by simply changing the base gradient estimator $G^t$ (Table 2), demonstrating its generality.\n    *   The convergence analysis is extensive, covering multiple settings (smooth non-convex, PŁ, non-smooth convex) and providing detailed proofs for each variant (Appendices C and D). This significantly advances the theoretical understanding of LoRA beyond existing heuristic approaches.*   **Broad Scope of Algorithmic Variants and Settings:** The work demonstrates the versatility of the Bernoulli-LoRA framework by instantiating it with a large family of modern and practical optimization algorithms.\n    *   It covers foundational methods like GD (Algorithm 2) and SGD (Algorithm 4), providing a baseline understanding.\n    *   It incorporates state-of-the-art variance reduction techniques like PAGE (Algorithm 6) and MVR (Algorithm 5), which are crucial for efficient training on large datasets.\n    *   It extends the framework to the challenging federated learning setting, proposing variants with communication compression (Fed-Bernoulli-LoRA-QGD, Algorithm 7; Fed-Bernoulli-LoRA-MARINA, Algorithm 8) and error feedback (Fed-Bernoulli-LoRA-EF21, Algorithm 9).\n    *   The analysis is extended to non-smooth convex functions (Section 4, Algorithm 3), broadening the applicability of the framework to a wider class of optimization problems.*   **Novel Theoretical Contributions for LoRA:** The paper presents what appear to be the first theoretical analyses of LoRA-type methods in several important and practical contexts.\n    *   The authors claim to provide the first convergence guarantees for LoRA-type methods that incorporate advanced variance-reduction schemes like PAGE and MVR in non-convex settings (Section 4, Contributions).\n    *   The work is presented as the first to theoretically analyze LoRA-type methods integrated with established communication-efficient federated learning techniques like quantization, gradient difference compression, and error feedback (Section 4, Contributions).\n    *   The paper also claims to be the first to provide a theoretical analysis of LoRA-type methods for non-smooth convex optimization problems (Section 4, Contributions). These contributions fill significant gaps in the literature.*   **High Clarity and Quality of Presentation:** The paper is exceptionally well-written and structured, making complex theoretical material accessible.\n    *   The motivation is clearly articulated (Section 3), positioning the work against the limitations of prior art like COLA and RAC-LoRA.\n    *   Table 1 provides a concise and highly effective summary of the main convergence results, allowing for easy comparison between the different algorithmic variants.\n    *   Table 2 clearly defines the base gradient estimator for each proposed method, reinforcing the unified nature of the framework.\n    *   The appendices are detailed and appear to be self-contained, providing full proofs for all theoretical claims (Appendices C, D).3) Weaknesses\n*   **Limited Empirical Validation:** The experiments, while illustrative of the theory, are conducted on small-scale, synthetic, or toy problems that do not reflect the typical use cases of LoRA, which are predominantly large-scale models.\n    *   The primary experiments are on a linear regression problem with non-convex regularization (Section 8.1) and a three-layer MLP on MNIST (Section 8.2). These settings are far removed from the large language and vision models where PEFT methods like LoRA are most impactful.\n    *   The results do not demonstrate a clear empirical advantage of the proposed Bernoulli-LoRA framework over the deterministic RAC-LoRA baseline. In the full-gradient setting, their performance is identical (Appendix E.1, Figure 2). In the MLP experiment, the final accuracy is nearly the same (Table 3: 96.46% for Bernoulli-LoRA vs. 96.49% for RAC-LoRA).\n    *   The main empirical win is for Bernoulli-LoRA-PAGE over SGD-based methods (Figure 1). While this validates the benefit of variance reduction, it is an expected outcome (VR > SGD) and does not specifically demonstrate an advantage of the Bernoulli randomization itself over a deterministic alternative like RAC-LoRA-PAGE.*   **Insufficient Justification for the Benefit of Randomization:** The paper's core novelty is the Bernoulli randomization, but its practical or theoretical advantage over a simpler deterministic update scheme is not clearly established.\n    *   The theoretical rates presented in Table 1 depend on `lambda_min^p` and `lambda_max^p`, which are convex combinations of the eigenvalues corresponding to the deterministic choices. It is not shown whether a mixed strategy (Bernoulli probability `p` between 0 and 1) can ever yield a better convergence rate than a pure, deterministic strategy (`p=0` or `p=1`).\n    *   The motivation for randomization is framed as a way to unify and generalize existing methods (Section 4). However, the paper does not articulate any other potential benefits, such as improved optimization dynamics or better exploration of the parameter space, that would make this generalization superior in practice.\n    *   The empirical results (Figure 2, Table 3) support this concern, as they show no performance gain from using a probabilistic update schedule compared to a deterministic one.*   **Clarity and Implications of Theoretical Assumptions:** The analysis relies on key assumptions whose practical implications and effects on the final convergence rates could be discussed more prominently.\n    *   The analysis hinges on Assumption 1 (Positive Expected Projection). While Appendix B provides a good justification for i.i.d. Gaussian sampling, this is a specific case. The convergence rates in Table 1 depend on the ratio `lambda_max^p / lambda_min^p`. For Gaussian sampling, this ratio is 1 (Appendix B, Lemma 2), but the sensitivity of the rates to this ratio under other sampling schemes is not discussed in the main paper.\n    *   The paper correctly notes that the LoRA reparameterization induces non-smoothness and circumvents this by analyzing projected updates in the full parameter space (Section 7, Block 33). While this is a valid and powerful analytical technique, the connection between this theoretical model and the practical reality of applying optimizers like AdamW directly to the low-rank factors A and B could be made more explicit.\n    *   The analysis for non-smooth convex functions relies on Assumption 10, which states `E[H] = alpha * I`. This is a stronger condition than Assumption 1 and is only shown to hold for i.i.d. Gaussian sampling (Appendix B, Lemma 2). The implications of this for other practical initialization/sampling schemes are not explored.4) Suggestions for Improvement\n*   **Strengthen and Expand Empirical Validation:**\n    *   To better demonstrate the practical relevance of the extensive theory, please consider adding experiments on a more representative task, such as fine-tuning a pre-trained transformer model (e.g., RoBERTa or a small T5) on a standard benchmark from the GLUE dataset.\n    *   Design experiments to specifically isolate the contribution of the Bernoulli randomization. For example, a direct comparison between Bernoulli-LoRA-PAGE and a newly implemented RAC-LoRA-PAGE (which uses a deterministic update) on the linear regression task would clarify whether the randomization offers any benefit beyond what variance reduction provides.\n    *   To validate the novel contributions in federated learning, please include at least one experiment comparing one of the proposed FL algorithms (e.g., Fed-Bernoulli-LoRA-MARINA) against a suitable baseline (e.g., FedAvg with LoRA, or Fed-RAC-LoRA from Malinovsky et al. [2024]) to demonstrate its effectiveness in a communication-constrained setting.*   **Provide a Clearer Rationale for Randomization:**\n    *   In the theoretical discussion, please add a paragraph analyzing the derived convergence rates (Table 1) with respect to the Bernoulli probability `p`. Is there a theoretical regime where a mixed strategy (`0 < p < 1`) is provably superior to a deterministic one? If the benefit is primarily in creating a unified analytical tool, this should be stated more explicitly as the main motivation.\n    *   In the introduction or motivation sections (e.g., Section 4), please expand on why a probabilistic update might be beneficial beyond theoretical unification. For instance, could it act as a form of regularization or help escape certain optimization traps? If such benefits are hypothesized, they could be explored empirically, even on a simple landscape.*   **Improve Discussion of Theoretical Assumptions:**\n    *   Please consider moving the key insights from Appendix B regarding Assumption 1 into the main paper (e.g., Section 7). Specifically, a discussion on how the `lambda_max^p / lambda_min^p` ratio influences the rates in Table 1 and under which sampling distributions this ratio is well-behaved would significantly improve the paper's self-containedness and clarity.\n    *   It would be beneficial to add a brief remark discussing the relationship between the projected gradient analysis and the direct optimization of LoRA factors. Acknowledging this as an analytical model of the true process and briefly touching on potential differences would add valuable nuance.\n    *   For the non-smooth convex analysis, please clarify in the main text that Assumption 10 is stronger than Assumption 1 and discuss the types of practical sampling strategies (beyond i.i.d. Gaussian) for which it might hold.5) Score\n- Overall (10): 7 — The paper presents a strong and comprehensive theoretical framework for LoRA, but the empirical validation is limited and does not fully demonstrate the practical advantages of the core randomization idea.\n- Novelty (10): 8 — The Bernoulli-LoRA framework is a novel generalization, and the theoretical analyses for variance-reduced and federated settings are significant, claimed firsts (Section 4).\n- Technical Quality (10): 9 — The theoretical analysis is extensive, rigorous, and detailed, with comprehensive proofs provided in the appendices (Appendices C, D).\n- Clarity (10): 9 — The paper is exceptionally well-written and structured, with clear explanations and helpful summaries of the methods and results (Table 1, Table 2).\n- Confidence (5): 4 — I am confident in my assessment, having reviewed the theoretical claims, the structure of the proofs, and the experimental setup.",
  "final_review": "1) Summary\nThis paper introduces Bernoulli-LoRA, a theoretical framework for Low-Rank Adaptation (PEFT) methods. The core idea is a probabilistic mechanism where a Bernoulli trial at each step determines which of the two low-rank matrices (A or B) is updated. This approach is presented as a generalization of existing LoRA update strategies. The authors reformulate the update as a projected gradient step, enabling a unified and rigorous convergence analysis. They develop and analyze numerous variants, including foundational methods (GD, SGD), advanced variance-reduced methods (PAGE, MVR), and communication-efficient federated learning algorithms (QGD, MARINA, EF21). For these variants, the paper establishes convergence guarantees for smooth non-convex, Polyak-Łojasiewicz, and non-smooth convex optimization settings. Experimental results on linear regression and MNIST classification are provided to validate the theoretical findings.2) Strengths\n*   **Comprehensive and Unified Theoretical Framework:** The paper proposes a general framework that successfully unifies different LoRA-style update schemes under a single probabilistic model. The reformulation of the update as a projected gradient step is elegant and provides a solid foundation for rigorous analysis.\n    *   The core update is expressed as a projected gradient estimator (Section 6.1, Equations 7 and 8), which allows leveraging a wide range of existing optimization theory.\n    *   The framework is shown to encompass various update strategies by simply changing the base gradient estimator $G^t$ (Table 2), demonstrating its generality.\n    *   The convergence analysis is extensive in scope, covering multiple settings (smooth non-convex, PŁ, non-smooth convex) and providing detailed proofs for each variant (Appendices C and D). This significantly advances the theoretical understanding of LoRA beyond existing heuristic approaches.*   **Broad Scope of Algorithmic Variants and Settings:** The work demonstrates the versatility of the Bernoulli-LoRA framework by instantiating it with a large family of modern and practical optimization algorithms.\n    *   It covers foundational methods like GD (Algorithm 2) and SGD (Algorithm 4), providing a baseline understanding.\n    *   It incorporates state-of-the-art variance reduction techniques like PAGE (Algorithm 6) and MVR (Algorithm 5), which are crucial for efficient training on large datasets.\n    *   It extends the framework to the challenging federated learning setting, proposing variants with communication compression (Fed-Bernoulli-LoRA-QGD, Algorithm 7; Fed-Bernoulli-LoRA-MARINA, Algorithm 8) and error feedback (Fed-Bernoulli-LoRA-EF21, Algorithm 9).\n    *   The analysis is extended to non-smooth convex functions (Section C.1.3, Algorithm 3), broadening the applicability of the framework to a wider class of optimization problems.*   **Novel Theoretical Contributions for LoRA:** The paper presents what appear to be the first theoretical analyses of LoRA-type methods in several important and practical contexts.\n    *   The authors claim to provide the first convergence guarantees for LoRA-type methods that incorporate advanced variance-reduction schemes like PAGE and MVR in non-convex settings (Section 4, Contributions).\n    *   The work is presented as the first to theoretically analyze LoRA-type methods integrated with established communication-efficient federated learning techniques like quantization, gradient difference compression, and error feedback (Section 4, Contributions).\n    *   The paper also claims to be the first to provide a theoretical analysis of LoRA-type methods for non-smooth convex optimization problems (Section 4, Contributions). These contributions fill significant gaps in the literature.*   **High Clarity and Quality of Presentation:** The paper is exceptionally well-written and structured, making complex theoretical material accessible.\n    *   The motivation is clearly articulated (Section 3), positioning the work against the limitations of prior art like COLA and RAC-LoRA.\n    *   Table 1 provides a concise and highly effective summary of the main convergence results, allowing for easy comparison between the different algorithmic variants.\n    *   Table 2 clearly defines the base gradient estimator for each proposed method, reinforcing the unified nature of the framework.\n    *   The appendices are detailed and appear to be self-contained, providing full proofs for all theoretical claims (Appendices C, D).3) Weaknesses\n*   **Inconsistent and Contradictory Theoretical Results:** The paper's central theoretical contributions appear to be undermined by systemic inconsistencies in the stated convergence rates. The rates presented in the main summary table, the formal theorem statements, and the appendix derivations often conflict with one another.\n    *   The convergence rates summarized in Table 1 frequently do not match the formal statements in Section 7 or the final results derived in the appendices (Appendices C and D).\n    *   For example, the rate for Bernoulli-LoRA-MVR is presented with different and conflicting terms in three separate locations: Table 1, Theorem 3, and the appendix proof in Theorem 13.\n    *   Similar contradictions are present for Bernoulli-LoRA-PAGE (Table 1 vs. Theorem 4 vs. Theorem 15) and Fed-Bernoulli-LoRA-EF21 (Table 1 vs. Theorem 7 vs. Theorem 21).\n    *   For Fed-Bernoulli-LoRA-QGD, the leading term of the rate in Table 1 is missing a factor of 6 that is present in the formal statement (Theorem 5) and its proof (Theorem 17). These discrepancies make it impossible to verify the paper's core theoretical claims.*   **Limited Empirical Validation:** The experiments, while illustrative of the theory, are conducted on small-scale problems that do not reflect the typical use cases of LoRA, which are predominantly large-scale models. Furthermore, the experimental setup raises questions about the validation of the proposed method.\n    *   The primary experiments are on a linear regression problem (Section 8.1) and a three-layer MLP on MNIST (Section 8.2). These settings are far removed from the large language and vision models where PEFT methods like LoRA are most impactful.\n    *   The experiments may not faithfully implement the analyzed algorithm. For the MLP experiment, the authors report that a \"deterministic assignment...at initialization yielded better performance\" (Table 3, Footnote 1), which is a deviation from the fully probabilistic method analyzed in the theory (Algorithm 1).\n    *   The experimental reporting contains misleading information. The captions for the plots in Figure 1 and Figure 2 (Appendix E.1) state that they show results for a wide range of `p` values, but the legends only display a small subset of these, which could be misinterpreted by the reader.\n    *   The results do not demonstrate a clear empirical advantage of the Bernoulli-LoRA framework over the deterministic RAC-LoRA baseline. In the full-gradient setting, their performance is identical (Appendix E.1, Figure 2). In the MLP experiment, the final accuracy is nearly the same (Table 3: 96.46% for Bernoulli-LoRA vs. 96.49% for RAC-LoRA).*   **Insufficient Justification for the Benefit of Randomization:** The paper's core novelty is the Bernoulli randomization, but its practical or theoretical advantage over a simpler deterministic update scheme is not clearly established.\n    *   The theoretical rates presented in Table 1 depend on `lambda_min^p` and `lambda_max^p`, which are convex combinations of the eigenvalues corresponding to the deterministic choices. It is not shown whether a mixed strategy (Bernoulli probability `p` between 0 and 1) can ever yield a better convergence rate than a pure, deterministic strategy (`p=0` or `p=1`).\n    *   The motivation for randomization is framed as a way to unify and generalize existing methods (Section 4). However, the paper does not articulate any other potential benefits, such as improved optimization dynamics or better exploration of the parameter space, that would make this generalization superior in practice.\n    *   The empirical results (Figure 2, Table 3) support this concern, as they show no performance gain from using a probabilistic update schedule compared to a deterministic one.*   **Clarity and Implications of Theoretical Assumptions:** The analysis relies on key assumptions whose practical implications and effects on the final convergence rates could be discussed more prominently.\n    *   The analysis hinges on Assumption 1 (Positive Expected Projection). While Appendix B provides a good justification for i.i.d. Gaussian sampling, this is a specific case. The convergence rates in Table 1 depend on the ratio `lambda_max^p / lambda_min^p`. For Gaussian sampling, this ratio is 1 (Appendix B, Lemma 2), but the sensitivity of the rates to this ratio under other sampling schemes is not discussed in the main paper.\n    *   The paper correctly notes that the LoRA reparameterization induces non-smoothness and circumvents this by analyzing projected updates in the full parameter space (Section 7). While this is a valid analytical technique, the connection between this theoretical model and the practical reality of applying optimizers like AdamW directly to the low-rank factors A and B could be made more explicit.\n    *   The analysis for non-smooth convex functions relies on Assumption 10, which states `E[H] = alpha * I`. This is a stronger condition than Assumption 1 and is only shown to hold for i.i.d. Gaussian sampling (Appendix B, Lemma 2). The implications of this for other practical initialization/sampling schemes are not explored.4) Suggestions for Improvement\n*   **Systematically Correct and Verify Theoretical Results:**\n    *   Please perform a thorough audit of all theoretical results to resolve the contradictions between the summary in Table 1, the main theorems in Section 7, and the final derivations in the appendices.\n    *   Specifically, please provide a single, correct, and consistently derived convergence rate for Bernoulli-LoRA-MVR.\n    *   Please do the same for Bernoulli-LoRA-PAGE, Fed-Bernoulli-LoRA-EF21, and other affected algorithms.\n    *   Please ensure the rate for Fed-Bernoulli-LoRA-QGD is stated consistently across all parts of the manuscript. A single, verifiable set of results is essential for the paper's contribution to be sound.*   **Strengthen and Expand Empirical Validation:**\n    *   To better demonstrate practical relevance, please consider adding experiments on a more representative task, such as fine-tuning a pre-trained transformer model (e.g., RoBERTa or a small T5) on a standard benchmark from the GLUE dataset.\n    *   Please ensure that the algorithms implemented in the experiments strictly follow the definitions (e.g., Algorithm 1) for which theoretical guarantees are provided. Any modifications made for performance should be clearly justified and their potential impact on the theory discussed.\n    *   Please correct the captions of Figure 1 and Figure 2 to accurately reflect the data presented in the plots.\n    *   Design experiments to specifically isolate the contribution of the Bernoulli randomization. For example, a direct comparison between Bernoulli-LoRA-PAGE and a newly implemented RAC-LoRA-PAGE (which uses a deterministic update) would clarify whether randomization offers any benefit beyond what variance reduction provides.*   **Provide a Clearer Rationale for Randomization:**\n    *   In the theoretical discussion, please add a paragraph analyzing the derived convergence rates (Table 1) with respect to the Bernoulli probability `p`. Is there a theoretical regime where a mixed strategy (`0 < p < 1`) is provably superior to a deterministic one? If the benefit is primarily in creating a unified analytical tool, this should be stated more explicitly as the main motivation.\n    *   In the introduction or motivation sections (e.g., Section 4), please expand on why a probabilistic update might be beneficial beyond theoretical unification. For instance, could it act as a form of regularization or help escape certain optimization traps? If such benefits are hypothesized, they could be explored empirically.*   **Improve Discussion of Theoretical Assumptions:**\n    *   Please consider moving the key insights from Appendix B regarding Assumption 1 into the main paper (e.g., Section 7). Specifically, a discussion on how the `lambda_max^p / lambda_min^p` ratio influences the rates in Table 1 and under which sampling distributions this ratio is well-behaved would improve clarity.\n    *   It would be beneficial to add a brief remark discussing the relationship between the projected gradient analysis and the direct optimization of LoRA factors. Acknowledging this as an analytical model of the true process would add valuable nuance.\n    *   For the non-smooth convex analysis, please clarify in the main text that Assumption 10 is stronger than Assumption 1 and discuss the types of practical sampling strategies (beyond i.i.d. Gaussian) for which it might hold.5) Score\n- Overall (10): 4 — The paper proposes an interesting and ambitious theoretical framework, but its value is severely diminished by systemic inconsistencies in the main theoretical results and limited, potentially flawed, empirical validation.\n- Novelty (10): 7 — The unified Bernoulli-LoRA framework and its extension to advanced VR and FL settings are novel contributions, though it builds directly on the core analytical technique of prior work (Section 3).\n- Technical Quality (10): 3 — The technical quality is low due to numerous and significant contradictions between the stated convergence rates in the summary table, main theorems, and appendix proofs (Table 1, Section 7, Appendix C/D).\n- Clarity (10): 6 — While the paper is well-written, the presentation of contradictory theoretical results makes it fundamentally unclear what the paper's actual findings are, which is a major clarity issue (Table 1 vs. Theorems 3-7).\n- Confidence (5): 5 — I am highly confident in my assessment, having cross-referenced the theoretical claims in the table, main text, and appendices, and examined the experimental setup in detail.",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 7,
        "novelty": 8,
        "technical_quality": 9,
        "clarity": 9,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 4,
        "novelty": 7,
        "technical_quality": 3,
        "clarity": 6,
        "confidence": 5
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "1) Summary\nThis paper introduces Bernoulli-LoRA, a theoretical framework for Low-Rank Adaptation (PEFT) methods. The core idea is a probabilistic mechanism where a Bernoulli trial at each step determines which of the two low-rank matrices (A or B) is updated. This approach is presented as a generalization of existing LoRA update strategies. The authors reformulate the update as a projected gradient step, enabling a unified and rigorous convergence analysis. They develop and analyze numerous variants, including foundational methods (GD, SGD), advanced variance-reduced methods (PAGE, MVR), and communication-efficient federated learning algorithms (QGD, MARINA, EF21). For these variants, the paper establishes convergence guarantees for smooth non-convex, Polyak-Łojasiewicz, and non-smooth convex optimization settings. Experimental results on linear regression and MNIST classification are provided to validate the theoretical findings.2) Strengths\n*   **Comprehensive and Unified Theoretical Framework:** The paper proposes a general framework that successfully unifies different LoRA-style update schemes under a single probabilistic model. The reformulation of the update as a projected gradient step is elegant and provides a solid foundation for rigorous analysis.\n    *   The core update is expressed as a projected gradient estimator (Section 6.1, Equations 7 and 8), which allows leveraging a wide range of existing optimization theory.\n    *   The framework is shown to encompass various update strategies by simply changing the base gradient estimator $G^t$ (Table 2), demonstrating its generality.\n    *   The convergence analysis is extensive in scope, covering multiple settings (smooth non-convex, PŁ, non-smooth convex) and providing detailed proofs for each variant (Appendices C and D). This significantly advances the theoretical understanding of LoRA beyond existing heuristic approaches.*   **Broad Scope of Algorithmic Variants and Settings:** The work demonstrates the versatility of the Bernoulli-LoRA framework by instantiating it with a large family of modern and practical optimization algorithms.\n    *   It covers foundational methods like GD (Algorithm 2) and SGD (Algorithm 4), providing a baseline understanding.\n    *   It incorporates state-of-the-art variance reduction techniques like PAGE (Algorithm 6) and MVR (Algorithm 5), which are crucial for efficient training on large datasets.\n    *   It extends the framework to the challenging federated learning setting, proposing variants with communication compression (Fed-Bernoulli-LoRA-QGD, Algorithm 7; Fed-Bernoulli-LoRA-MARINA, Algorithm 8) and error feedback (Fed-Bernoulli-LoRA-EF21, Algorithm 9).\n    *   The analysis is extended to non-smooth convex functions (Section C.1.3, Algorithm 3), broadening the applicability of the framework to a wider class of optimization problems.*   **Novel Theoretical Contributions for LoRA:** The paper presents what appear to be the first theoretical analyses of LoRA-type methods in several important and practical contexts.\n    *   The authors claim to provide the first convergence guarantees for LoRA-type methods that incorporate advanced variance-reduction schemes like PAGE and MVR in non-convex settings (Section 4, Contributions).\n    *   The work is presented as the first to theoretically analyze LoRA-type methods integrated with established communication-efficient federated learning techniques like quantization, gradient difference compression, and error feedback (Section 4, Contributions).\n    *   The paper also claims to be the first to provide a theoretical analysis of LoRA-type methods for non-smooth convex optimization problems (Section 4, Contributions). These contributions fill significant gaps in the literature.*   **High Clarity and Quality of Presentation:** The paper is exceptionally well-written and structured, making complex theoretical material accessible.\n    *   The motivation is clearly articulated (Section 3), positioning the work against the limitations of prior art like COLA and RAC-LoRA.\n    *   Table 1 provides a concise and highly effective summary of the main convergence results, allowing for easy comparison between the different algorithmic variants.\n    *   Table 2 clearly defines the base gradient estimator for each proposed method, reinforcing the unified nature of the framework.\n    *   The appendices are detailed and appear to be self-contained, providing full proofs for all theoretical claims (Appendices C, D).3) Weaknesses\n*   **Inconsistent and Contradictory Theoretical Results:** The paper's central theoretical contributions appear to be undermined by systemic inconsistencies in the stated convergence rates. The rates presented in the main summary table, the formal theorem statements, and the appendix derivations often conflict with one another.\n    *   The convergence rates summarized in Table 1 frequently do not match the formal statements in Section 7 or the final results derived in the appendices (Appendices C and D).\n    *   For example, the rate for Bernoulli-LoRA-MVR is presented with different and conflicting terms in three separate locations: Table 1, Theorem 3, and the appendix proof in Theorem 13.\n    *   Similar contradictions are present for Bernoulli-LoRA-PAGE (Table 1 vs. Theorem 4 vs. Theorem 15) and Fed-Bernoulli-LoRA-EF21 (Table 1 vs. Theorem 7 vs. Theorem 21).\n    *   For Fed-Bernoulli-LoRA-QGD, the leading term of the rate in Table 1 is missing a factor of 6 that is present in the formal statement (Theorem 5) and its proof (Theorem 17). These discrepancies make it impossible to verify the paper's core theoretical claims.*   **Limited Empirical Validation:** The experiments, while illustrative of the theory, are conducted on small-scale problems that do not reflect the typical use cases of LoRA, which are predominantly large-scale models. Furthermore, the experimental setup raises questions about the validation of the proposed method.\n    *   The primary experiments are on a linear regression problem (Section 8.1) and a three-layer MLP on MNIST (Section 8.2). These settings are far removed from the large language and vision models where PEFT methods like LoRA are most impactful.\n    *   The experiments may not faithfully implement the analyzed algorithm. For the MLP experiment, the authors report that a \"deterministic assignment...at initialization yielded better performance\" (Table 3, Footnote 1), which is a deviation from the fully probabilistic method analyzed in the theory (Algorithm 1).\n    *   The experimental reporting contains misleading information. The captions for the plots in Figure 1 and Figure 2 (Appendix E.1) state that they show results for a wide range of `p` values, but the legends only display a small subset of these, which could be misinterpreted by the reader.\n    *   The results do not demonstrate a clear empirical advantage of the Bernoulli-LoRA framework over the deterministic RAC-LoRA baseline. In the full-gradient setting, their performance is identical (Appendix E.1, Figure 2). In the MLP experiment, the final accuracy is nearly the same (Table 3: 96.46% for Bernoulli-LoRA vs. 96.49% for RAC-LoRA).*   **Insufficient Justification for the Benefit of Randomization:** The paper's core novelty is the Bernoulli randomization, but its practical or theoretical advantage over a simpler deterministic update scheme is not clearly established.\n    *   The theoretical rates presented in Table 1 depend on `lambda_min^p` and `lambda_max^p`, which are convex combinations of the eigenvalues corresponding to the deterministic choices. It is not shown whether a mixed strategy (Bernoulli probability `p` between 0 and 1) can ever yield a better convergence rate than a pure, deterministic strategy (`p=0` or `p=1`).\n    *   The motivation for randomization is framed as a way to unify and generalize existing methods (Section 4). However, the paper does not articulate any other potential benefits, such as improved optimization dynamics or better exploration of the parameter space, that would make this generalization superior in practice.\n    *   The empirical results (Figure 2, Table 3) support this concern, as they show no performance gain from using a probabilistic update schedule compared to a deterministic one.*   **Clarity and Implications of Theoretical Assumptions:** The analysis relies on key assumptions whose practical implications and effects on the final convergence rates could be discussed more prominently.\n    *   The analysis hinges on Assumption 1 (Positive Expected Projection). While Appendix B provides a good justification for i.i.d. Gaussian sampling, this is a specific case. The convergence rates in Table 1 depend on the ratio `lambda_max^p / lambda_min^p`. For Gaussian sampling, this ratio is 1 (Appendix B, Lemma 2), but the sensitivity of the rates to this ratio under other sampling schemes is not discussed in the main paper.\n    *   The paper correctly notes that the LoRA reparameterization induces non-smoothness and circumvents this by analyzing projected updates in the full parameter space (Section 7). While this is a valid analytical technique, the connection between this theoretical model and the practical reality of applying optimizers like AdamW directly to the low-rank factors A and B could be made more explicit.\n    *   The analysis for non-smooth convex functions relies on Assumption 10, which states `E[H] = alpha * I`. This is a stronger condition than Assumption 1 and is only shown to hold for i.i.d. Gaussian sampling (Appendix B, Lemma 2). The implications of this for other practical initialization/sampling schemes are not explored.4) Suggestions for Improvement\n*   **Systematically Correct and Verify Theoretical Results:**\n    *   Please perform a thorough audit of all theoretical results to resolve the contradictions between the summary in Table 1, the main theorems in Section 7, and the final derivations in the appendices.\n    *   Specifically, please provide a single, correct, and consistently derived convergence rate for Bernoulli-LoRA-MVR.\n    *   Please do the same for Bernoulli-LoRA-PAGE, Fed-Bernoulli-LoRA-EF21, and other affected algorithms.\n    *   Please ensure the rate for Fed-Bernoulli-LoRA-QGD is stated consistently across all parts of the manuscript. A single, verifiable set of results is essential for the paper's contribution to be sound.*   **Strengthen and Expand Empirical Validation:**\n    *   To better demonstrate practical relevance, please consider adding experiments on a more representative task, such as fine-tuning a pre-trained transformer model (e.g., RoBERTa or a small T5) on a standard benchmark from the GLUE dataset.\n    *   Please ensure that the algorithms implemented in the experiments strictly follow the definitions (e.g., Algorithm 1) for which theoretical guarantees are provided. Any modifications made for performance should be clearly justified and their potential impact on the theory discussed.\n    *   Please correct the captions of Figure 1 and Figure 2 to accurately reflect the data presented in the plots.\n    *   Design experiments to specifically isolate the contribution of the Bernoulli randomization. For example, a direct comparison between Bernoulli-LoRA-PAGE and a newly implemented RAC-LoRA-PAGE (which uses a deterministic update) would clarify whether randomization offers any benefit beyond what variance reduction provides.*   **Provide a Clearer Rationale for Randomization:**\n    *   In the theoretical discussion, please add a paragraph analyzing the derived convergence rates (Table 1) with respect to the Bernoulli probability `p`. Is there a theoretical regime where a mixed strategy (`0 < p < 1`) is provably superior to a deterministic one? If the benefit is primarily in creating a unified analytical tool, this should be stated more explicitly as the main motivation.\n    *   In the introduction or motivation sections (e.g., Section 4), please expand on why a probabilistic update might be beneficial beyond theoretical unification. For instance, could it act as a form of regularization or help escape certain optimization traps? If such benefits are hypothesized, they could be explored empirically.*   **Improve Discussion of Theoretical Assumptions:**\n    *   Please consider moving the key insights from Appendix B regarding Assumption 1 into the main paper (e.g., Section 7). Specifically, a discussion on how the `lambda_max^p / lambda_min^p` ratio influences the rates in Table 1 and under which sampling distributions this ratio is well-behaved would improve clarity.\n    *   It would be beneficial to add a brief remark discussing the relationship between the projected gradient analysis and the direct optimization of LoRA factors. Acknowledging this as an analytical model of the true process would add valuable nuance.\n    *   For the non-smooth convex analysis, please clarify in the main text that Assumption 10 is stronger than Assumption 1 and discuss the types of practical sampling strategies (beyond i.i.d. Gaussian) for which it might hold.5) Score\n- Overall (10): 4 — The paper proposes an interesting and ambitious theoretical framework, but its value is severely diminished by systemic inconsistencies in the main theoretical results and limited, potentially flawed, empirical validation.\n- Novelty (10): 7 — The unified Bernoulli-LoRA framework and its extension to advanced VR and FL settings are novel contributions, though it builds directly on the core analytical technique of prior work (Section 3).\n- Technical Quality (10): 3 — The technical quality is low due to numerous and significant contradictions between the stated convergence rates in the summary table, main theorems, and appendix proofs (Table 1, Section 7, Appendix C/D).\n- Clarity (10): 6 — While the paper is well-written, the presentation of contradictory theoretical results makes it fundamentally unclear what the paper's actual findings are, which is a major clarity issue (Table 1 vs. Theorems 3-7).\n- Confidence (5): 5 — I am highly confident in my assessment, having cross-referenced the theoretical claims in the table, main text, and appendices, and examined the experimental setup in detail."
}