Here are four distinct reviews of the paper "Bernoulli-LoRA: A Theoretical Framework for Randomized Low-Rank Adaptation".

***

### **Review 1**

**Summary**

This paper introduces Bernoulli-LoRA, a theoretical framework for Low-Rank Adaptation (LoRA) that generalizes existing methods. The core idea is to use a Bernoulli random variable at each step to probabilistically decide whether to update the 'A' or 'B' matrix in the low-rank decomposition, while the other is sampled from a fixed distribution. The authors reformulate this update as a projected gradient step and provide a comprehensive theoretical analysis. They instantiate this framework with seven different optimization algorithms—including GD, SGD, variance-reduced methods (PAGE, MVR), and communication-efficient federated learning methods (QGD, MARINA, EF21)—and establish convergence guarantees for each in smooth non-convex and Polyak-Łojasiewicz settings. The analysis is also extended to non-smooth convex functions. Experiments on a synthetic linear regression task and an MLP on MNIST are presented to validate the theory.

**Soundness**

The theoretical part of the paper appears to be sound. The methodology of reformulating the LoRA update as a projected gradient step (Section 6.1), following Malinovsky et al. [2024], provides a solid foundation for the analysis. The introduction of the Bernoulli choice is a natural extension, and the subsequent analysis correctly incorporates the expectation over this choice. The assumptions made (e.g., L-smoothness, Positive Expected Projection) are standard in the non-convex optimization literature. The derivation of the expected projection matrix for Gaussian sampling in Appendix B is a crucial piece of justification for Assumption 1.

The convergence proofs for the various algorithmic variants (Appendices C and D) seem rigorous and follow established patterns for analyzing their respective base optimizers (PAGE, MVR, EF21, etc.). The Lyapunov function-based analyses are standard and appear correct. The derived convergence rates, summarized in Table 1, correctly reflect the properties of the underlying optimizers, modulated by the spectral properties (`λ_min`, `λ_max`) of the expected projection matrices. The extension to non-smooth convex optimization (Section C.1.3) is a valuable addition and appears technically correct.

**Presentation**

The paper is exceptionally well-structured and clearly written. The motivation is well-articulated in Section 3, clearly positioning the work relative to prior art like COLA and RAC-LoRA. The "Contributions" section (Section 4) is detailed and provides an excellent roadmap. The core framework is introduced logically in Section 6, with the projected gradient reformulation (Section 6.1) being a key clarifying step. Tables 1 and 2 are extremely helpful for summarizing the numerous algorithmic variants and their theoretical guarantees, making the dense material much more digestible. The appendices are extensive and provide the necessary details for verifying the claims. The writing is precise and professional.

**Contribution**

The main contribution is the development and extensive theoretical analysis of the Bernoulli-LoRA framework. This work significantly advances the theoretical understanding of LoRA-style methods. While building on RAC-LoRA, the probabilistic generalization is elegant and allows for a unified analysis of a much broader class of update strategies. The key contributions are:
1.  A novel, unifying probabilistic framework for LoRA updates.
2.  Rigorous convergence guarantees for seven different optimization settings, including, for the first time in a LoRA context, advanced variance-reduced (PAGE, MVR) and communication-efficient federated learning (MARINA, EF21) algorithms.
3.  The first convergence analysis for a LoRA-type method in the non-smooth convex setting.

This paper serves as a theoretical toolkit that connects the practical heuristic of LoRA to the rich literature on non-convex optimization.

**Strengths**

1.  **Theoretical Breadth and Depth:** The sheer volume and rigor of the theoretical analysis are the paper's greatest strength. Analyzing seven distinct algorithms under multiple settings (non-convex, PŁ, non-smooth convex) is a monumental effort and provides a strong theoretical foundation for future work.
2.  **Unifying Framework:** The Bernoulli-LoRA concept is an elegant generalization that unifies deterministic and probabilistic update schemes within a single, analyzable model.
3.  **Clarity and Organization:** The paper is exceptionally well-written and organized, making complex theoretical concepts accessible. The use of summary tables (Table 1, Table 2) is exemplary.
4.  **Novel Connections:** The paper is the first to formally analyze the combination of LoRA-style updates with modern optimization techniques like PAGE, MVR, MARINA, and EF21, bridging a significant gap between PEFT and optimization theory.

**Weaknesses**

1.  **Limited Experimental Validation:** The experiments, while sufficient to demonstrate the basic viability of the methods, are limited to small-scale problems (synthetic linear regression, MLP on MNIST). This is a significant disconnect from the motivation of fine-tuning "large foundational models" (Abstract).
2.  **Practical Implications of `p`:** The role of the Bernoulli probability `p` is central to the framework, but its practical impact is not deeply explored. The experiments show similar performance for different `p` values in the GD case (Figure 2, Appendix E) and only report the best-performing `p` in the main experiment (Table 3), which offers little guidance on how to set this hyperparameter in practice.

**Questions**

1.  The convergence rates in Table 1 depend on `λ_min` and `λ_max` of the expected projection matrices. Appendix B shows that for Gaussian sampling, these are `r/n`. How would these rates be affected by other, perhaps more structured or sparse, sampling distributions for the fixed matrices `A_S` and `B_S`?
2.  In the MNIST experiment (Table 3), Bernoulli-LoRA with `p=0.99` performs almost identically to RAC-LoRA (which corresponds to `p=1`). Does this suggest that the probabilistic choice offers little empirical benefit over a well-chosen deterministic one, at least for this task? Have you observed scenarios where a non-trivial `p` (e.g., `p=0.5`) significantly outperforms the deterministic `p=0` or `p=1` cases?
3.  The analysis for non-smooth convex functions (Theorem 10) uses an assumption (Assumption 10) that `E[H] = αI`, which is stronger than Assumption 1. While this is satisfied by Gaussian sampling, could the analysis be extended to the more general Assumption 1?

**Rating**

- Overall (10): 9 — The paper presents a comprehensive and rigorous theoretical framework that significantly advances the understanding of LoRA-style methods, though the experimental validation could be stronger.
- Novelty (10): 8 — The core idea is a clever generalization of recent work (RAC-LoRA), and the novelty lies in its application to a very broad and previously unanalyzed set of advanced optimization algorithms.
- Technical Quality (10): 10 — The theoretical analysis is extensive, deep, and appears to be technically flawless, with detailed proofs provided for all claims.
- Clarity (10): 10 — The paper is exceptionally well-written and organized, making a large volume of complex technical material easy to follow.
- Confidence (5): 5 — I am an expert in optimization theory and am highly confident in my assessment of the paper's theoretical contributions.

***

### **Review 2**

**Summary**

This paper proposes Bernoulli-LoRA, a new method for parameter-efficient fine-tuning. It randomizes the LoRA update process by using a coin flip (a Bernoulli trial) to decide which of the two low-rank matrices to train at each step. The authors provide a large body of theoretical work, proving convergence for this framework when combined with various optimizers like SGD, PAGE, and several federated learning algorithms. They test their methods on a synthetic linear regression problem and a small multi-layer perceptron (MLP) for MNIST digit classification, comparing against other LoRA variants like COLA and RAC-LoRA.

**Soundness**

The theoretical claims seem sound, based on a quick check of the main theorems and the structure of the proofs in the appendix. The assumptions are standard for this kind of analysis. However, the soundness of the experimental validation is questionable in terms of its relevance to the paper's stated goals. The experiments are conducted on toy problems (Section 8.1, 8.2) that are not representative of the scenarios where PEFT methods like LoRA are actually used (i.e., fine-tuning large language or vision models). The experimental setup for the MLP on MNIST (Section 8.2) follows a prior paper, which is good for reproducibility, but the use of AdamW as the optimizer is not covered by the theoretical analysis, which focuses on variants of GD/SGD. This creates a disconnect between the theory and the main practical experiment.

**Presentation**

The paper is well-written and easy to follow. The structure is logical, moving from motivation to the proposed framework, theoretical results, and finally experiments. The tables summarizing the algorithms (Table 2) and convergence rates (Table 1) are very effective. The figures are clear and well-labeled. The appendix is massive but appears well-organized, which is necessary given the number of proofs.

**Contribution**

The paper's main contribution is theoretical: it provides a unifying framework and convergence proofs for a randomized version of LoRA. The sheer number of algorithms analyzed is impressive. However, the practical contribution is less clear. The proposed Bernoulli-LoRA performs almost identically to the existing RAC-LoRA in the main experiment (Table 3, `96.46%` vs `96.49%`). The paper claims benefits for resource-constrained settings but provides no experiments to back this up. While the variance-reduction results (Figure 1) are interesting, showing that Bernoulli-LoRA-PAGE converges where SGD-based methods stall is an expected outcome of using a variance-reduced optimizer, rather than a specific benefit of the Bernoulli-LoRA framework itself.

**Strengths**

1.  **Comprehensive Theory:** The paper provides a very thorough theoretical treatment of its proposed framework, which is valuable for a field often driven by heuristics.
2.  **Clear Exposition:** The core idea is explained very clearly, and the paper is well-organized, making it easy to understand the contributions.
3.  **Unifying Perspective:** The Bernoulli framework is a nice way to think about different LoRA update strategies under one roof.

**Weaknesses**

1.  **Weak Experimental Evidence:** The experiments are the biggest weakness. Using a synthetic task and a small MLP on MNIST (Section 8) is insufficient to demonstrate the practical utility of Bernoulli-LoRA for the "large foundational models" mentioned in the abstract. Without experiments on at least a medium-sized transformer model (e.g., BERT, T5, or a small GPT-2 on a GLUE task), the practical relevance of the work is unproven.
2.  **No Clear Empirical Advantage:** The results in Table 3 show no meaningful performance gain over the baseline RAC-LoRA. This makes it hard to argue for the adoption of Bernoulli-LoRA in practice, as it introduces an additional hyperparameter (`p`) without a clear benefit.
3.  **Theory-Practice Gap:** The main practical experiment uses the AdamW optimizer (Section 8.2), for which no theory is provided. The theory is developed for SGD-style methods, which are shown to perform poorly in the synthetic experiment (Figure 1). This gap undermines the paper's goal of developing "theoretically grounded yet practically effective" methods.

**Questions**

1.  Why were the experiments not conducted on a large language model, which is the primary use case for LoRA and the motivation cited in your introduction? Do you expect the conclusions from the MNIST experiment to transfer to multi-billion parameter models?
2.  The paper claims that the probabilistic parameter count of Bernoulli-LoRA is "especially valuable in resource-constrained settings" (Section 8.2). Could you provide an example of such a setting and elaborate on why an *expected* parameter count is more valuable than a fixed, deterministic one (as in RAC-LoRA)? Have you run any experiments to measure this benefit (e.g., memory usage or wall-clock time)?
3.  Given that Bernoulli-LoRA performs on par with RAC-LoRA in your experiments, what is the primary motivation for a practitioner to choose your method over the simpler, deterministic RAC-LoRA?

**Rating**

- Overall (10): 5 — A theoretically impressive paper with a significant gap between its ambitious claims and its weak empirical validation, showing no clear practical advantage.
- Novelty (10): 6 — The idea is a straightforward probabilistic extension of a prior framework (RAC-LoRA), and the application of various optimizers feels somewhat mechanical.
- Technical Quality (10): 8 — The theory is strong and appears correct, but the experimental design is weak and does not adequately support the paper's claims about practical efficacy.
- Clarity (10): 9 — The paper is very well-written and structured.
- Confidence (5): 4 — I am confident in my assessment, particularly regarding the mismatch between the paper's scope and its experimental evaluation.

***

### **Review 3**

**Summary**

This paper introduces Bernoulli-LoRA, a novel and general framework for the low-rank adaptation (LoRA) of pre-trained models. The method introduces a probabilistic mechanism, governed by a Bernoulli distribution, to select which of the two low-rank factor matrices to update at each iteration. The authors show this framework can be viewed as a projected gradient method and leverage this insight to provide a sweeping theoretical analysis. They derive convergence guarantees for seven different instantiations of their framework, covering standard, variance-reduced, and federated learning settings. The theoretical results are validated with experiments on a synthetic regression problem and an MLP fine-tuning task on MNIST.

**Soundness**

The paper appears to be very sound from a methodological and theoretical standpoint. The core idea of randomizing the choice of matrix to update is a natural generalization of prior work, and the authors execute the analysis flawlessly. The reformulation as a projected gradient step in Section 6.1 is a powerful analytic tool that allows the authors to bring a vast array of standard optimization theory to bear on the problem of LoRA. The assumptions are clearly stated and are standard in the field. The proofs provided in the appendix are detailed and appear correct. The experiments, though limited in scale, are well-designed to verify the theoretical claims, such as the superior convergence of the variance-reduced variant (Bernoulli-LoRA-PAGE) over the stochastic one (Figure 1).

**Presentation**

The presentation is excellent. The paper is clearly structured, with a strong narrative that flows from the motivation (lack of theory for LoRA) to the proposed solution and its comprehensive analysis. The authors do a fantastic job of managing a large amount of technical content. The introductory sections are accessible, and the contributions are laid out explicitly in Section 4. The use of tables to summarize the different algorithms (Table 2) and their convergence rates (Table 1) is highly effective and serves as a great reference. The writing is of high quality, and the paper is a pleasure to read despite its technical density.

**Contribution**

This paper makes a significant contribution to the theoretical understanding of parameter-efficient fine-tuning. It successfully bridges the gap between the heuristic practice of LoRA and the formal theory of non-convex optimization.
The key contributions are:
1.  **A General and Elegant Framework:** Bernoulli-LoRA provides a unifying lens through which to view and analyze various LoRA update schemes.
2.  **Extensive Theoretical Guarantees:** The paper provides the first convergence proofs for LoRA-style methods combined with modern optimizers like PAGE, MVR, MARINA, and EF21. This is a major step forward.
3.  **Broadening the Scope:** The analysis for non-smooth convex functions and for various federated learning settings greatly expands the applicability of theoretically-grounded LoRA.

Overall, this work provides a much-needed theoretical foundation for a widely used class of methods and will likely become a key reference in this area.

**Strengths**

1.  **Theoretical Rigor:** The paper is a tour de force of optimization theory applied to PEFT. The analysis is comprehensive and robust.
2.  **Novelty and Generality:** The Bernoulli-LoRA framework is a simple but powerful idea that generalizes prior work and enables a unified analysis.
3.  **Clarity of Exposition:** The paper excels at presenting a large and complex set of results in a clear, organized, and accessible manner.
4.  **Foundational Work:** By connecting LoRA to established optimization algorithms and providing convergence guarantees, this work lays a solid foundation for future research into more principled PEFT methods.

**Weaknesses**

1.  **Limited Scale of Experiments:** The primary weakness is that the experiments are not performed on the large-scale models that motivate the use of LoRA. While the current experiments effectively validate the theory on a small scale, showing the practical effectiveness on a large language model would make the paper much more impactful.
2.  **Guidance on Hyperparameter `p`:** The paper introduces the Bernoulli probability `p` as a key element of its framework, but offers little practical guidance on how to set it. More discussion on the role of `p` and its effect on optimization dynamics would be beneficial.

**Questions**

1.  Could you provide more intuition on the role of the Bernoulli probability `p`? For instance, how does it interact with the rank `r` or the learning rate `γ`? Does a value of `p=0.5` represent a "most random" or "most robust" choice in some sense?
2.  The footnote in Table 3 mentions that a deterministic initialization (fixing which matrix is trainable at the start) worked better than a probabilistic one. This seems to slightly contradict the spirit of the fully probabilistic framework. Could you elaborate on this finding?
3.  While the federated learning algorithms are a great contribution, they are analyzed in a setting with full client participation and synchronized updates. How do you think the analysis would change in more practical FL settings with client sampling and asynchronous updates?

**Rating**

- Overall (10): 8 — An excellent and important theoretical paper that provides a strong foundation for principled PEFT, with the main room for improvement being more extensive, large-scale experiments.
- Novelty (10): 8 — The framework is a novel and elegant generalization, and its application to a wide range of optimizers is highly original.
- Technical Quality (10): 10 — The technical quality of the theoretical work is outstanding.
- Clarity (10): 10 — The paper is a model of clarity in presenting complex technical work.
- Confidence (5): 5 — I am very confident in my evaluation of this paper's strengths and contributions.

***

### **Review 4**

**Summary**

The paper proposes "Bernoulli-LoRA," a framework for low-rank adaptation (LoRA) where the choice of which factor matrix to update is randomized via a Bernoulli trial. The authors present this as a generalization of prior work and provide an extensive theoretical analysis, deriving convergence rates for seven different algorithmic instantiations, including SGD, variance-reduced, and federated learning methods. The paper includes experiments on a synthetic dataset and MNIST to support its theoretical claims.

**Soundness**

The theoretical derivations appear to be correct, following standard analytical techniques. The core argument relies on reformulating the update as a projected gradient step (Section 6.1) and then analyzing the expectation of this step over the randomness of the Bernoulli choice and the optimizer. However, the paper's overall argument for significance is on shaky ground. There is a major disconnect between the theory and practice presented. The theory is for SGD-type methods, but the main "practical" experiment in Section 8.2 uses AdamW. Furthermore, the experiments themselves are on toy problems and do not provide convincing evidence that the proposed framework is useful for the large models it claims to target.

**Presentation**

The paper is well-organized and clearly written. The structure is easy to follow. However, the sheer volume of content, particularly the seven algorithmic variants and the massive appendix, feels like an attempt to impress with quantity over quality of insight. While Tables 1 and 2 are helpful summaries, they also highlight the "plug-and-play" nature of the contributions, where existing optimizers are simply slotted into the new framework.

**Contribution**

The contribution of this paper is questionable. The core idea is a minor, almost trivial, modification of the RAC-LoRA framework [Malinovsky et al., 2024], which already proposed updating one matrix at a time. Simply adding a coin flip to decide which matrix to update does not feel like a significant conceptual leap. The subsequent analysis of seven different optimizers seems like a mechanical exercise. The authors take well-known optimizers (PAGE, MARINA, EF21) and apply their standard analysis to the Bernoulli-LoRA update rule. While technically demanding, this process does not seem to generate new fundamental insights into either LoRA or the optimizers themselves; it merely confirms that they work when composed with a random projection. The claim of providing the "first meaningful theoretical framework" (quoting the RAC-LoRA paper title in their own reference) feels like an overstatement, as this work is an incremental extension of that very framework.

**Strengths**

1.  **Technically Correct Analysis:** The mathematical derivations in the paper and appendix appear to be correct and are executed with care.
2.  **Broad Scope:** The paper covers a wide range of optimization settings (stochastic, variance-reduced, federated), which is ambitious.

**Weaknesses**

1.  **Incremental Novelty:** The core idea is a very small step beyond RAC-LoRA. The paper feels more like a long technical report extending RAC-LoRA than a standalone novel contribution.
2.  **Lack of New Insight:** The paper does not offer deep new insights. The conclusion is essentially that standard optimizers continue to work when their updates are randomly projected, with convergence rates scaled by the projection's spectral properties. This is not a surprising result.
3.  **Grossly Inadequate Experiments:** The experiments are completely insufficient. The motivation is fine-tuning "large foundational models," but the experiments are on linear regression and a tiny MLP on MNIST. This is a classic bait-and-switch. The results themselves are not compelling; Table 3 shows the method performs the same as the baseline it aims to generalize.
4.  **Overstated Contributions:** The paper presents itself as a major theoretical breakthrough, but the contribution feels more incremental and mechanical. The title "A Theoretical Framework" seems to aggrandize what is essentially a minor modification of a pre-existing framework.

**Questions**

1.  Can you articulate a single, clear, and non-obvious insight that was gained from this work, beyond the fact that composing standard optimizers with random projections works as expected?
2.  The paper's novelty rests on the Bernoulli randomization. However, the experiments show that `p=0.99` (Table 3) is optimal, which is nearly deterministic. In the GD case (Appendix Figure 2), performance is identical for all `p`. This suggests the randomization adds no value. Why should the community care about a framework whose central mechanism appears to be empirically irrelevant?
3.  How do you justify the claim of providing a framework for adapting "large foundational models" (Abstract) when the largest model you test has ~55k parameters (Section 8.2) and the task is MNIST classification? This seems to be a fundamental misrepresentation of the work's demonstrated applicability.

**Rating**

- Overall (10): 3 — The paper is technically correct but lacks novelty, insight, and meaningful empirical support. The contribution is incremental and overstated.
- Novelty (10): 2 — The core idea is a trivial extension of prior work, and the rest is a mechanical application of existing analyses.
- Technical Quality (10): 7 — The theory is executed correctly, but the experimental quality is very poor and does not support the claims.
- Clarity (10): 8 — The paper is clearly written, but the structure inflates the perceived contribution.
- Confidence (5): 5 — I am highly confident that the paper's contributions are overstated and its empirical support is critically lacking.