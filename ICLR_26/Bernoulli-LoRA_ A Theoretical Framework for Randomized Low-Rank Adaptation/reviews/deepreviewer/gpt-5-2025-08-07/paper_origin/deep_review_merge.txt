Summary
The paper introduces Bernoulli-LoRA, a parameter-efficient fine-tuning framework that randomly selects which low-rank factor (A or B) to update at each iteration via a Bernoulli trial, thereby unifying A-only, B-only, and alternating strategies. A key contribution is reformulating LoRA-style updates as projected gradient steps in the full parameter space, enabling a clean, unified analysis of optimization algorithms. The authors provide convergence guarantees for deterministic and stochastic methods (GD, SGD), variance-reduced estimators (PAGE, MVR), and federated learning extensions with compression and error-feedback (QGD, MARINA, EF21) under smooth non-convex and Polyak-≈Åojasiewicz conditions. They further present the first analysis of LoRA-type updates for non-smooth convex objectives with constant and Polyak stepsizes. The theoretical rates are expressed via spectral properties of expected projection operators, with Gaussian sketches yielding simple constants. Empirical results include synthetic linear regression with non-convex regularization and an MNIST MLP transfer setting, demonstrating variance reduction benefits and competitive accuracy compared to RAC-LoRA and COLA. Code is provided.

Strengths
- Elegant reformulation of LoRA updates as projected gradient steps on the full parameter matrix, which enables unified, tractable analysis across methods.
- Comprehensive theoretical coverage, including convergence guarantees for GD, SGD, and modern variance-reduced methods (PAGE, MVR), as well as federated variants with quantization, gradient-difference compression, and EF21.
- Clear spectral characterization of rates through eigenvalues of expected projection operators; for Gaussian sketches, these reduce to simple constants that make assumptions and bounds transparent.
- First analysis of non-smooth convex objectives for LoRA-type updates, including constant and Polyak stepsizes, expanding the theoretical scope beyond existing work.
- Empirical demonstrations that variance reduction improves optimization behavior and that Bernoulli-LoRA achieves competitive accuracy on a parameter-efficient MNIST transfer task.
- Generally clear organization with summarized rates and estimator definitions, enumerated algorithms, detailed proofs in appendices, and a code release that supports reproducibility.

Weaknesses
- Empirical validation is limited to small-scale settings (synthetic regression and MNIST MLP), without experiments on larger transformer/LLM tasks or federated benchmarks, leaving practical impact at scale and FL variants underexplored.
- The Bernoulli mechanism is relaxed in practice (deterministic initial assignment and a single high p setting), reducing the distinction from non-randomized baselines and limiting insight into the sensitivity to p or sketch distributions; ablations are missing.
- The analysis repeatedly uses independence between random sketches (and associated projections) and the current iterate/gradients, but this is not explicitly formalized as an assumption; making this explicit would strengthen rigor and clarify applicability to data-dependent or structured sketches.
- The non-smooth convex analysis relies on a stronger assumption that the expected projection equals a scaled identity, which is justified for Gaussian sketches but may be restrictive for non-Gaussian or structured sketching; broader characterization and discussion are lacking.
- Minor presentation issues (notation inconsistencies such as HB vs H_B, overlapping theorem numbering, occasional stray references to RAC-LoRA in Bernoulli-LoRA sections) slightly hinder readability.
- Practical considerations around computing pseudoinverses and implementing projections efficiently at small ranks in large models are not discussed, which may affect numerical stability and scalability.
