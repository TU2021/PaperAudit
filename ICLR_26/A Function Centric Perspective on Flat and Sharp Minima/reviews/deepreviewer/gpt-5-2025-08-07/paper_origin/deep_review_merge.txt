Summary
The paper argues that sharpness should be treated as a function-dependent property rather than a universal proxy for generalization. It supports this view with two complementary strands of evidence: (i) toy single-objective optimization problems where optimal minima can be sharp or flat depending on function geometry, demonstrating that optimality does not imply flatness; and (ii) extensive deep image classification experiments (CIFAR-10/100, TinyImageNet) with ResNet-18, VGG-19, and ViT, showing that common regularization strategies (SAM, weight decay, data augmentation) often produce solutions that are sharper according to reparameterization-aware metrics (Fisher–Rao, Relative Flatness) and a SAM-inspired metric, yet yield improved accuracy, calibration, corruption robustness, and higher prediction agreement. The paper further reconciles SAM’s behavior by framing it as promoting local robustness rather than global flatness, explaining why global sharpness metrics may increase even as reliability improves. The experimental design employs matched seeds and includes ablations over batch size, learning rate, and SAM radius.

Strengths
- Clear conceptual reframing: Positions sharpness as a function-dependent attribute tied to task/architecture and inductive biases, challenging the “flatness implies generalization” narrative and offering a coherent explanation of SAM as enforcing local robustness.
- Broad empirical scope: Multiple datasets (CIFAR-10/100, TinyImageNet) and architectures (ResNet-18, VGG-19, ViT), with matched-seed training and controlled ablations (batch size, learning rate, SAM radius).
- Multiple sharpness metrics: Uses reparameterization-aware measures (Fisher–Rao, Relative Flatness) alongside a SAM-inspired sharpness metric, and discusses their limitations.
- Consistent empirical pattern: Across many settings, regularization leads to sharper-but-better solutions, improving accuracy, calibration (ECE), corruption robustness (CIFAR-C), and prediction agreement, while baselines are often flatter yet worse.
- Careful reporting and organization: Clear tables/figures, mean ± SEM over seeds, and detailed appendices. Acknowledges limitations of loss landscape visualizations and avoids overinterpreting 2D projections.
- Safety-relevant perspective: Extends analysis beyond accuracy to calibration, corruption robustness, and model agreement, enriching the reliability narrative.

Weaknesses
- Data dependence of sharpness metrics: Sharpness is sometimes computed on augmented data and sometimes on standard data, leading to large numerical differences (especially for the SAM-based sharpness). This undermines direct comparability of sharpness values across training conditions and conflicts with claims that augmented and standard computations are nearly identical.
- SAM-sharpness conflates training and evaluation scales: The evaluation radius is tied to the training hyperparameter ρ; metric values co-vary with ρ, complicating cross-condition interpretation and making the metric partially circular with respect to the training setup.
- Fisher–Rao’s coupling to training loss: FR appears strongly correlated with residual training loss across conditions, raising the possibility that observed “increases in sharpness” partly reflect higher loss under stronger regularization rather than differences in curvature or functional complexity.
- Incomplete and uneven metric coverage: Relative Flatness is computed on a subset of data for some settings and is missing for TinyImageNet, limiting comprehensive cross-dataset/architecture comparisons and weakening claims that rely on multiple metrics in all settings.
- Robustness scope mismatch: The paper claims improved adversarial robustness, but evaluations focus on corruption robustness (CIFAR-C). Without adversarial attack results, the robustness claim is overstated.
- Potential compute unfairness: SAM introduces an additional optimization step per update, potentially yielding unequal effective compute or update counts across conditions; the extent to which this is controlled is not fully addressed.
- Limited causal linkage from toy to deep settings: While the toy examples are instructive, the connection to high-dimensional deep networks remains qualitative; no formal bridge is established between data-induced function complexity and measured sharpness beyond empirical correlations.
- Terminology and metric interpretation: “Functional diversity” is operationalized as lower prediction disagreement (i.e., higher agreement), which is at odds with common usage and may confuse interpretation. In some places, generalization gap definitions and negative values are not fully clarified, and formal statistical testing beyond SEM would strengthen claims.
