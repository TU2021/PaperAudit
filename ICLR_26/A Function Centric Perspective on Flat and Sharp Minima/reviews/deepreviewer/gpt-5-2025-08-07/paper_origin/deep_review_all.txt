Summary
The paper argues that sharpness should be interpreted as a function-dependent property rather than a universal proxy for generalisation. It presents two lines of evidence: (i) toy single‑objective optimisation functions where optimal minima can be sharp or flat depending on function geometry (Sec. 4; Fig. 3; Tables 1–2), and (ii) high‑dimensional image classification experiments on CIFAR‑10/100 and TinyImageNet using ResNet‑18, VGG‑19, and ViT, showing that regularisers (SAM, weight decay, data augmentation) frequently lead to sharper minima (by Fisher–Rao, Relative Flatness, and SAM‑sharpness) while improving accuracy, calibration, robustness to corruptions, and prediction agreement (Sec. 5–6; Tables 3–7; Appx F–H). The authors also reconcile SAM’s objective as promoting local robustness rather than global flatness (Sec. 6, “Reconciling SAM’s Objective…”).

Soundness
- The matched‑seed experimental design is thoughtful (Sec. 5.1), and multiple architectures/datasets and sharpness metrics are considered (Sec. 3; Appx B), which strengthens the empirical case. The toy-function section convincingly demonstrates that “optimality ≠ flatness” in low dimensions (Fig. 3; Tables 1–2).
- However, several confounds weaken causal interpretation in the deep-learning experiments:
  - Sharpness measurements depend strongly on the data used to compute them; computing sharpness on augmented vs. standard data produces large numerical differences (Appx F.1, Tables 9 vs. 10), contradicting the claim that “augmented is a superset” with “nearly identical” values. The trend direction is similar, but the magnitudes differ by an order of magnitude for SAM‑sharpness, suggesting the metric is data‑dependent.
  - SAM‑sharpness definition ties the measurement radius to the training hyperparameter ρ (Sec. B; Appx E), making comparisons across controls sensitive to a training choice; the sweep (Appx E, Table 8; Fig. 10) shows metric values co-vary with ρ itself.
  - Fisher–Rao norm appears tightly correlated with training loss across controls (Intro Fig. 1a; Appx F–H figures), raising the possibility that reported “increased sharpness” partly reflects higher residual loss under stronger regularisation, not necessarily global curvature.
  - Relative Flatness for CIFAR‑100 is computed on 20% of training data (Appx C–D), and is unavailable for TinyImageNet (Sec. 6; Table 7), limiting cross‑condition comparability.
  - The claim of improved “adversarial robustness” is not supported by adversarial evaluations; only corruption robustness (CIFAR‑C) is measured (Sec. 3 “Robustness”).
- The conceptual bridge from toy functions to deep networks is plausible but remains qualitative; no formal link is established between data-induced function complexity and measured sharpness beyond correlations.

Presentation
- The manuscript is generally clear and well-structured, with helpful tables and consistent reporting of mean ± SEM (Sec. 6; Tables 3–7; Appx). The matched‑seed design and ablations (batch size, learning rate, ρ sweeps) are described (Sec. 5.1; Appx E–F–G–H).
- Some terminology is confusing: “functional diversity” is defined yet interpreted via lower “prediction disagreement” as “functional similarity” (Sec. 3), which conflicts with ensemble-diversity literature; consider renaming to “functional agreement.”
- A few claims are broader than the provided evidence (e.g., “adversarial robustness” in Abstract vs. CIFAR‑C only in Sec. 3). Minor duplications/formatting in tables and figure captions exist in the appendices, but do not impede understanding.

Contribution
- Conceptual: A function‑centric reframing that decouples sharpness from a priori desirability and emphasises task/architecture dependence (Sec. 1, 6, 7).
- Empirical: A sizable set of controlled experiments showing regularisation often coincides with sharper minima and improved accuracy, calibration, corruption robustness, and agreement (Tables 3–7; Appx F–H). The reconciliation of SAM’s local objective with globally sharper minima is insightful (Sec. 6).
- Novelty is moderate: prior work has questioned flatness as a universal proxy and SAM’s flattening (e.g., Dinh et al., 2017; Wen et al., 2023), but this paper extends the empirical scope to safety‑relevant metrics and provides a unified function‑centric narrative.

Strengths
- Broad empirical coverage: multiple architectures (ResNet/VGG/ViT) and datasets (CIFAR‑10/100, TinyImageNet) with matched seeds (Sec. 5.1; Sec. 6; Appx F–H).
- Multiple reparameterisation‑aware sharpness measures and ablations (Fisher–Rao; Relative Flatness; SAM‑sharpness; Appx B, E).
- Safety‑relevant metrics beyond accuracy (ECE; CIFAR‑C corruption accuracy; prediction disagreement) systematically reported (Sec. 3; Tables 3–7).
- Clear demonstration that “baseline is flatter yet worse,” while regularisation yields sharper-but-better solutions (Tables 3–5, 7; Figs. 1, 10; Appx F–H).

Weaknesses
- Data dependence of sharpness metrics undermines comparability across conditions: augmented vs. standard data produces large numeric differences (Appx F.1, Tables 9 vs. 10), conflicting with claims of near equivalence.
- SAM‑sharpness measurement tied to ρ conflates training and evaluation scales (Sec. B; Appx E), complicating cross‑condition interpretation.
- Lack of adversarial robustness evaluations despite claims (Abstract; Sec. 3 uses CIFAR‑C only).
- Potential confounding by unequal compute: SAM adds an extra optimisation step per epoch (Sec. 5.1), but the number of effective updates differs; fairness to baselines is not fully addressed.
- Fisher–Rao’s strong correlation with training loss (Fig. 1a; Appx plots) suggests interpretation of “sharpness increases” may partly reflect higher residual loss under regularisation rather than curvature.
- Relative Flatness missing or downsampled for some settings (Table 7; Appx D–F–G), reducing metric completeness; cross‑architecture comparisons are discouraged but still claimed broadly (Sec. 6, item 4).

Questions
1. How are augmentation seeds handled under the “matched‑seed” setup (Sec. 5.1)? Are augmentations deterministic per example to preserve identical data order across controls?
2. Can you report sharpness using only the original (non‑augmented) dataset for all conditions to avoid data‑dependence, and add confidence intervals or statistical tests to support differences (Appx F.1)?
3. SAM‑sharpness: Why scale the probe radius by the training ρ (Sec. B)? Would a fixed evaluation radius across controls improve comparability? Can you add a calibration figure showing metric monotonicity vs. a fixed probe scale?
4. Fisher–Rao: Since FR correlates with training loss (Fig. 1a), can you control for loss (e.g., match losses across conditions or use late‑training checkpoints with comparable loss) to isolate curvature effects?
5. Can you add adversarial robustness (e.g., PGD/AutoAttack) to substantiate claims in the Abstract?
6. How exactly is “prediction disagreement” computed (pairing, averaging across seeds)? Could you also report a representation-similarity measure (e.g., CKA) and ensemble gains to contextualise “agreement vs. diversity” trade‑offs?

Rating
- Overall (10): 7 — Broad, careful empirical study and a useful function‑centric narrative, but measurement confounds (Appx F.1; Appx E) and missing adversarial evaluations limit strength.
- Novelty (10): 7 — Extends prior critiques of flatness with safety metrics and reconciles SAM’s local robustness with global sharpness (Sec. 6; Tables 3–7).
- Technical Quality (10): 6 — Solid experimental protocol, yet data‑dependent sharpness computation and ρ‑linked measurement scale weaken causal claims (Appx F.1; Appx E).
- Clarity (10): 7 — Clear structure and comprehensive tables, though terminology around “functional diversity/agreement” and Abstract robustness claims need tightening (Sec. 3; Abstract).
- Confidence (5): 4 — Read all sections and appendices; strong familiarity with sharpness metrics, but no access to code and some metrics are sensitive to setup.


Summary
This manuscript proposes a function‑centric reading of sharpness, arguing that sharper minima can correspond to more appropriate inductive biases and better reliability. Evidence spans toy optimisation (Sec. 4; Fig. 3; Tables 1–2) and deep vision tasks with regularisers (SAM, augmentation, weight decay) that often increase sharpness while improving accuracy, calibration, corruption robustness, and prediction agreement (Sec. 6; Tables 3–7; Appx F–H). The paper further explains SAM’s behaviour as promoting local robustness even when global sharpness metrics increase (Sec. 6).

Soundness
- The toy section is sound and illustrates the core intuition well (Sec. 4). The high‑dimensional experiments use matched seeds and multiple metrics, strengthening internal validity (Sec. 5.1–6).
- Key validity threats:
  - The calculation of sharpness on augmented data changes the effective data distribution used in the metric, leading to large value differences vs. standard data (Appx F.1, Tables 9 vs. 10), so cross‑condition numeric comparisons are not on a common footing.
  - SAM‑sharpness uses a perturbation radius proportional to training ρ (Sec. B), and the ρ sweep shows metric values track ρ (Appx E, Table 8), conflating training and evaluation scales.
  - FR is known to incorporate gradients (Sec. B, Eq. 9) and appears strongly coupled to the training loss (Fig. 1a), which may explain the systematic ordering of FR by condition; this challenges interpreting FR increases solely as “sharper minima.”
  - Generalisation gap is not formally defined (Sec. D mentions using test accuracy as proxy; Tables show negative gaps for ViT Aug+SAM, Table 25), complicating its use as a comparable metric across controls with different training losses.
  - The robustness evaluation uses CIFAR‑C corruptions (Sec. 3), not adversarial attacks; claims in the Abstract overstate the scope.

Presentation
- The writing is clear with extensive tables, plots, and appendices (Tables 3–7; Appx F–H). The limitations of 2D loss plots are acknowledged (Sec. 6).
- Some inconsistencies exist: the Abstract says “adversarial robustness,” but Sec. 3 evaluates corruption robustness only; “functional diversity” is operationalised as lower “prediction disagreement,” which is better described as “functional agreement” (Sec. 3).
- Reporting mean ± SEM over 10 seeds is good; formal statistical tests would help.

Contribution
- The contribution is a well‑articulated empirical challenge to the “flatness ⇒ generalisation” narrative, expanded to calibration and corruption robustness, and a conceptual reconciliation of SAM’s local objective with global sharpness metrics (Sec. 6).
- The breadth across architectures/datasets and matched‑seed controls is valuable. The novelty lies in tying regularisation‑induced sharpness to safety‑relevant metrics, despite similar observations existing regarding SAM not always flattening (Wen et al., 2023).

Strengths
- Strong empirical breadth with matched‑seed controls (Sec. 5.1; Tables 3–7).
- Multiple sharpness metrics and careful discussion of projection vs. metric limitations (Sec. 6; Appx B).
- Consistent finding that baseline is flatter yet worse across several metrics (Tables 3–5, 7).
- Useful synthesis explaining SAM’s local robustness objective (Sec. 6, “Reconciling SAM…”).

Weaknesses
- Sharpness metric computation is data‑dependent and not harmonised across conditions (Appx F.1), confounding comparisons.
- SAM‑sharpness ties evaluation radius to training ρ (Sec. B; Appx E), reducing interpretability across controls.
- FR vs. train loss correlation (Fig. 1a; Appx plots) makes FR increases ambiguous.
- Robustness claims overreach (Abstract) relative to CIFAR‑C evaluations (Sec. 3).
- Relative Flatness missing for TinyImageNet and downsampled for CIFAR‑100 (Table 7; Appx D–F), limiting completeness.

Questions
1. Can you recompute all sharpness metrics on the same non‑augmented dataset for every condition, and report both sets to demonstrate robustness?
2. Would fixing the SAM‑sharpness probe radius (independent of ρ) alter conclusions? Could you include such an ablation?
3. Please define the generalisation gap precisely and explain negative values (e.g., ViT Aug+SAM, Table 25).
4. Could you add adversarial robustness (e.g., PGD, AutoAttack) to align with the Abstract?
5. FR: can you perform matched‑loss comparisons or late‑epoch checkpoints to decouple FR from training loss?

Rating
- Overall (10): 6 — Persuasive qualitative story and broad empirical work, but measurement confounds and overstated robustness claims weaken the conclusions (Appx F.1; Sec. 3; Fig. 1a).
- Novelty (10): 6 — Builds on known critiques of flatness/SAM with expanded safety metrics and a function‑centric framing (Sec. 1, 6).
- Technical Quality (10): 5 — Solid setup but key metrics are data‑ and ρ‑dependent, and FR tracks loss, limiting causal strength (Appx F.1; Appx E; Fig. 1a).
- Clarity (10): 7 — Clear exposition and thorough appendices, with some terminology/claim scope issues (Sec. 3; Abstract).
- Confidence (5): 4 — Careful reading across main text and appendices; experienced with sharpness metrics; no code access.


Summary
The paper challenges the standard intuition that flatter minima necessarily generalise better, arguing instead for a function‑centric view where sharpness reflects the complexity of the learned function and inductive biases. It supports this with (i) toy optimisation functions exhibiting both flat and sharp global minima (Sec. 4; Fig. 3; Tables 1–2) and (ii) deep vision experiments showing regularisation often increases sharpness while improving accuracy, calibration, CIFAR‑C robustness, and functional agreement (Sec. 6; Tables 3–7; Appx F–H). It also explains why SAM can yield globally sharper minima while improving performance by focusing on local robustness (Sec. 6).

Soundness
- Methodologically, the matched‑seed setup and breadth of evaluations are commendable (Sec. 5.1; Tables 3–7). The authors use reparametrisation‑aware metrics (Fisher–Rao, Relative Flatness) and a custom SAM‑sharpness (Sec. 3; Appx B), and ablate batch size/learning rate and SAM ρ (Appx E–F–G).
- The main causal claim—regularisation increases sharpness yet improves reliability—holds across many tables (e.g., ResNet‑18 CIFAR‑10/100: Tables 3–5; TinyImageNet: Table 7; VGG/ViT: Appx G–H). However:
  - Sharpness measurement is not consistently controlled for data distribution (augmented vs. standard); the authors acknowledge trends are similar but values differ substantially (Appx F.1), which introduces confounds.
  - SAM‑sharpness depends on ρ used in training, as evidenced by the sweep (Appx E, Table 8), making it partly a function of the training hyperparameter.
  - FR correlates with training loss (Intro Fig. 1a; Appx plots), so FR increases may reflect higher residual loss rather than curvature alone.
  - Robustness evaluations focus on corruption robustness (Sec. 3), not adversarial attacks, so claims of “adversarial robustness” are not demonstrated.
- Despite these caveats, the empirical story is compelling and repeatedly observed across diverse settings.

Presentation
- Clear narrative, comprehensive experimental reporting, and informative figures/table summaries (Sec. 6; Tables 3–7; Appx). The authors candidly discuss the limitations of loss landscape visualisations (Sec. 6).
- Some terminology could be tightened (functional “diversity” vs. “agreement,” Sec. 3); the Abstract overclaims on adversarial robustness relative to evaluations.

Contribution
- The paper contributes a consolidated empirical case and a conceptual reframing that encourages practitioners to think in terms of task‑dependent function complexity rather than a universal flatness desideratum (Sec. 1, 6, 7).
- The reconciliation of SAM’s local robustness objective with observed global sharpness increases is particularly useful (Sec. 6).

Strengths
- Breadth across models/datasets and safety‑relevant metrics (Tables 3–7; Appx F–H).
- Matched‑seed design increases comparability (Sec. 5.1).
- Reparameterisation‑aware metrics and explicit discussion of their limits (Sec. 3; Sec. 6).
- Consistency of findings across architectures and hyperparameters (Appx F–G–H).

Weaknesses
- Data‑dependence in sharpness computation (Appx F.1) and ρ‑dependence of SAM‑sharpness (Appx E) complicate sharpness comparisons across controls.
- FR’s correlation with training loss (Fig. 1a) challenges direct interpretation as “sharpness increases” rather than “loss not fully minimised under regularisation.”
- Robustness claims are broader than measured; no adversarial evaluations (Sec. 3 vs. Abstract).
- Relative Flatness missing for TinyImageNet and downsampled for CIFAR‑100 (Table 7; Appx D–F), limiting completeness.

Questions
1. Would fixing the evaluation dataset (non‑augmented) for sharpness across all conditions materially change conclusions? Can you provide those numbers for FR/RF/SAM‑sharpness?
2. Can SAM‑sharpness be decoupled from ρ by using a fixed probe radius across all controls? If so, how do results change?
3. Please add adversarial robustness experiments (PGD/AutoAttack) to align with your Abstract and strengthen safety claims.
4. Can you report loss‑matched checkpoints (or late‑training snapshots) to evaluate FR/RF independent of residual loss?
5. Could you provide an analysis on whether “functional agreement” correlates with calibration/robustness to justify its desirability?

Rating
- Overall (10): 8 — Strong empirical breadth and a coherent function‑centric perspective, with some measurement confounds that are fixable (Appx F.1; Appx E).
- Novelty (10): 7 — Adds safety metrics and a unifying narrative to a debated topic; reconciles SAM behaviour (Sec. 6; Tables 3–7).
- Technical Quality (10): 7 — Careful design and many controls, tempered by data‑ and ρ‑dependent sharpness measurement and FR‑loss coupling (Appx F.1; Appx E; Fig. 1a).
- Clarity (10): 8 — Well-organised, with explicit limitations and extensive appendices; minor terminology/claim‑scope issues (Sec. 3; Abstract).
- Confidence (5): 4 — Comprehensive reading of main/appendix; domain familiarity; no code review.


Summary
The paper revisits flat vs. sharp minima by arguing sharpness reflects function complexity and inductive biases. It demonstrates (a) toy optimisation where global minima differ in sharpness (Sec. 4; Fig. 3; Tables 1–2), and (b) deep vision experiments where regularisation (SAM, augmentation, weight decay) yields sharper minima on reparameterisation‑aware metrics while improving test accuracy, ECE, CIFAR‑C robustness, and prediction agreement (Sec. 6; Tables 3–7; Appx F–H). The authors explain increased sharpness under SAM via local robustness objectives (Sec. 6).

Soundness
- The empirical setup is broad and uses matched seeds (Sec. 5.1), and the toy examples are appropriate for motivating the thesis (Sec. 4).
- Notable issues reduce the strength of claims:
  - The Abstract claims “adversarial robustness,” but only corruption robustness is assessed (Sec. 3), which is a different regime; this is a scope mismatch.
  - Sharpness metrics are computed on different datasets across conditions (augmented vs. standard), yielding large value shifts (Appx F.1, Tables 9 vs. 10); the claim that augmented and standard are “nearly identical” is not numerically borne out for SAM‑sharpness.
  - SAM‑sharpness depends on the training ρ (Sec. B; Appx E), making the metric partially circular with respect to the training setup.
  - FR’s dependence on loss (Fig. 1a; Appx plots) suggests that increases in FR could be driven by higher residual training loss under regularisation.
  - Compute fairness: SAM adds extra updates per epoch (Sec. 5.1); the paper does not control for equal gradient steps across conditions, which could affect generalisation.

Presentation
- Overall well-organised, with helpful tables and figures. Limitations of landscape visualisations are acknowledged (Sec. 6).
- Terminology around “functional diversity” vs. “agreement” (Sec. 3) is confusing; consider revising to avoid conflating higher agreement with “diversity.”
- Some appendices contain duplicated captions/formatting oddities but the data are readable.

Contribution
- The paper contributes an empirically supported, function‑centric framing, spanning multiple architectures/datasets and safety metrics, and offers a clear reconciliation of SAM’s local‑robustness objective with globally sharper minima (Sec. 6).
- While not entirely novel conceptually relative to prior critiques, the breadth and safety‑metric emphasis are valuable.

Strengths
- Extensive empirical coverage with matched seeds and multiple sharpness measures (Sec. 5–6; Tables 3–7; Appx).
- Safety‑relevant metrics (ECE, CIFAR‑C, agreement) systematically included (Sec. 3; Tables 3–7).
- Clear articulation of SAM’s objective vs. global sharpness (Sec. 6).

Weaknesses
- Robustness claims overstate evidence (Abstract vs. Sec. 3).
- Sharpness measurement confounded by data choice and ρ‑dependence (Appx F.1; Appx E).
- FR‑loss correlation complicates interpreting FR as curvature (Fig. 1a).
- Missing Relative Flatness on TinyImageNet and partial computation on CIFAR‑100 limit metric completeness (Table 7; Appx D–F).

Questions
1. Please evaluate adversarial robustness (e.g., PGD/AutoAttack) to substantiate claims in the Abstract.
2. Can sharpness be recomputed across all conditions using the same (non‑augmented) dataset, and with a fixed probe radius for SAM‑sharpness, to remove data/ρ confounds?
3. How is prediction disagreement aggregated (pairwise across seeds, averaged)? Could you contrast “agreement” with ensemble performance to clarify its desirability?
4. Can you perform loss‑matched comparisons for FR/RF to decouple sharpness metrics from training loss?

Rating
- Overall (10): 5 — Interesting perspective and broad experiments, but measurement confounds and overstated robustness claims reduce confidence (Appx F.1; Appx E; Sec. 3).
- Novelty (10): 6 — Extends existing debate with safety metrics and a function‑centric lens (Sec. 1, 6).
- Technical Quality (10): 5 — Solid setup; confounds in metric computation and scope mismatch on robustness weaken conclusions (Appx F.1; Appx E; Abstract vs. Sec. 3).
- Clarity (10): 7 — Clear writing/figures; terminology and claim scope need adjustment (Sec. 3; Abstract).
- Confidence (5): 4 — Detailed reading; domain familiarity; no code access; results largely consistent but sensitive to setup.