{
  "baseline_review": "1) Summary\nThis paper challenges the conventional wisdom that flatter minima in the loss landscape of neural networks lead to better generalization. The authors propose a \"function-centric\" perspective, hypothesizing that the geometry of a solution (i.e., its sharpness) reflects the complexity of the learned function rather than being an intrinsic indicator of quality. Through extensive experiments on single-objective optimization problems and high-dimensional image classification tasks (CIFAR, TinyImageNet) with various architectures (ResNet, VGG, ViT), the paper demonstrates that common regularization techniques—such as SAM, weight decay, and data augmentation—often lead to sharper minima. Crucially, these sharper minima frequently coincide with superior performance not only in generalization but also in safety-critical metrics like calibration and robustness. The findings suggest that sharpness should be re-evaluated as a potential signature of a well-regularized, complex function with appropriate inductive biases.2) Strengths\n*   **Compelling and Coherent Thesis:** The paper introduces a \"function-centric\" interpretation of sharpness that provides a clear and intuitive framework for reconciling conflicting results in the literature. This perspective elegantly reframes sharpness not as a flaw to be avoided, but as a characteristic of the function being learned.\n    *   The introduction clearly articulates this thesis, contrasting it with the traditional \"flatness is better\" view (Section 1).\n    *   The single-objective optimization experiments provide a simple yet powerful illustration of the core idea: functions like Sphere have inherently flat optima, while others like Rosenbrock have inherently sharp optima, demonstrating that optimal geometry is function-dependent (Section 4, Figure 3, Table 2).\n    *   The conclusion effectively summarizes how the results support this function-centric view, arguing that solution geometry is shaped by task-specific demands (Section 7).*   **Extensive and Rigorous Empirical Evaluation:** The experimental design is thorough, well-controlled, and spans a wide range of settings, lending strong support to the paper's claims.\n    *   The study uses multiple modern, reparameterization-invariant sharpness metrics (Fisher-Rao Norm, Relative Flatness, SAM-Sharpness), ensuring the findings are not artifacts of a single measurement choice (Section 3, Appendix B).\n    *   The evaluation extends beyond standard accuracy to include important safety-relevant metrics like Expected Calibration Error (ECE), corruption robustness, and functional agreement, providing a more holistic view of model quality (Section 3, Tables 3, 5, 7).\n    *   Experiments are conducted across multiple datasets (CIFAR-10, CIFAR-100, TinyImageNet) and architectures (ResNet-18, VGG-19, ViT), demonstrating the generality of the findings (Section 6, Appendices G, H).\n    *   The use of a matched-seed setup for comparing different regularization controls is a methodologically sound choice that isolates the effects of each regularizer (Section 5.1).\n    *   The appendices provide substantial additional results, including hyperparameter sweeps for learning rate and batch size, which further validate the robustness of the main conclusions (Appendix F.2, G.1).*   **Significant and Counter-intuitive Findings:** The paper presents several key results that directly challenge established beliefs in the field, particularly regarding the effects of regularization and Sharpness-Aware Minimization (SAM).\n    *   The central finding that regularizers like data augmentation and weight decay consistently lead to sharper, yet better-performing, minima is a significant departure from the idea that regularization promotes flatness (Tables 3, 5, 7). For instance, in Table 3, the \"Augmentation + SAM\" model is orders of magnitude sharper than the \"Baseline\" across all metrics but achieves the best performance on every evaluation metric.\n    *   The paper provides strong evidence that SAM, despite its motivation, often *increases* global sharpness, rather than decreasing it (Section 6, \"SAM Does Not Always Flatten\"). This is shown consistently across datasets and architectures (e.g., \"Baseline\" vs. \"Baseline + SAM\" in Tables 3, 5).\n    *   The authors offer a valuable reconciliation of SAM's behavior by distinguishing between its objective of promoting *local* robustness and the measurement of *global* sharpness, providing a more nuanced understanding of how SAM works (Section 6, \"Reconciling SAM’s Objective...\").*   **High Clarity and Quality of Presentation:** The manuscript is exceptionally well-written, well-structured, and easy to follow.\n    *   The motivation and contributions are clearly laid out in the introduction (Section 1).\n    *   The experimental setup and metrics are described in sufficient detail for reproducibility (Section 5.1, Appendices B, C, D).\n    *   Results are presented in clear, well-organized tables that facilitate easy comparison between different conditions (e.g., Tables 3, 5, 7).\n    *   The paper effectively uses visualizations, from the simple 2D function plots (Figure 3) to the high-dimensional loss landscape projections (Tables 4, 6), to build intuition, while also correctly pointing out the limitations of the latter (Section 6, \"Limitations of Loss Landscape Visualisations\").3) Weaknesses\n*   **\"Function Complexity\" Remains a Qualitative Concept:** The paper's central thesis hinges on the idea that sharpness reflects \"function complexity,\" but this term is never formally defined or measured for the learned neural network functions. The link is established correlatively: regularizers are applied, sharpness increases, and this is attributed to an increase in function complexity.\n    *   The term is used throughout the paper (e.g., Abstract, Section 1, Section 4) as the primary explanatory mechanism, but there is no attempt to quantify it for the trained models.\n    *   The argument relies on the intuition that regularizers enable the learning of \"more complex functions\" (Section 2), but without a direct measure of this complexity, the core causal claim remains an assertion rather than a demonstrated fact.*   **Fixed Hyperparameters Across Different Regularization Schemes:** The experimental protocol holds hyperparameters like learning rate and number of epochs constant across all \"controls\" (Baseline, SAM, Augmentation, etc.) to ensure a controlled comparison (Section 5.1). However, different regularizers can significantly alter the optimization dynamics, meaning that the optimal hyperparameters for the baseline model may be suboptimal for the regularized models.\n    *   As stated in Section 5.1, \"all other training details (optimiser, schedule, epochs, etc.) are held fixed across controls.\" This could potentially disadvantage certain methods that might perform even better with tuned hyperparameters.\n    *   While the appendix includes sweeps over learning rate and batch size (Appendix F.2, G.1), the main results in Tables 3, 5, and 7 are based on a single, fixed configuration, which may not represent the best possible performance for each control.*   **Inconsistent Interpretation of \"Prediction Disagreement\":** The paper frames lower prediction disagreement as a universally desirable property, interpreting it as \"stability in the learned function and robustness to training stochasticity\" (Section 3). This interpretation is not consistently supported by the results and overlooks alternative perspectives.\n    *   The paper cites literature suggesting functional diversity (i.e., higher disagreement) can be beneficial for ensembles (Fort et al., 2020), but then proceeds to interpret lower disagreement as strictly better.\n    *   The results for TinyImageNet (Table 7) contradict this interpretation. The \"Augmentation\" and \"Augmentation + SAM\" models, which achieve the best generalization gap and ECE, also exhibit the *highest* prediction disagreement, more than the baseline. This is contrary to the trends observed on CIFAR-10 (Table 3) and CIFAR-100 (Table 5). This inconsistency is not addressed.4) Suggestions for Improvement\n*   **Provide Quantitative Measures of Function Complexity:** To strengthen the central thesis, the authors should attempt to operationalize the concept of \"function complexity.\"\n    *   Consider measuring and reporting metrics that have been proposed in the literature to quantify the complexity of a learned function, such as the number of linear regions, the spectral norm of weight matrices, or path norms. Correlating these quantitative measures with the observed sharpness metrics would provide more direct and compelling evidence for the proposed function-centric view.*   **Acknowledge and Discuss the Impact of Fixed Hyperparameters:** The authors should explicitly acknowledge the potential confounding effect of using fixed hyperparameters for all regularization schemes.\n    *   In the experimental setup (Section 5.1), add a sentence discussing this limitation and justifying the choice as a trade-off for controlled comparison. Briefly mention that while the appendix sweeps explore some of this space, a full hyperparameter search for each condition was outside the scope but could be a direction for future work to find the true performance ceiling of each method.*   **Refine the Interpretation of Prediction Disagreement:** The discussion of functional diversity/prediction disagreement should be made more nuanced to reflect the experimental results and existing literature more accurately.\n    *   In Section 3, the authors should acknowledge the dual role of functional diversity, noting that while low disagreement can indicate stability, high diversity can be beneficial for ensembling.\n    *   When discussing the results, the authors should explicitly address the contradictory findings on TinyImageNet (Table 7). They could hypothesize why the trend might be different for this dataset (e.g., due to pre-training or task complexity) or simply state that the relationship between disagreement and performance appears to be task-dependent, which would align well with the paper's overall message about the lack of universal geometric rules.5) Score\n*   Overall (10): 9 — The paper presents a novel and compelling perspective on a fundamental topic, supported by extensive and rigorous experiments that yield significant, counter-intuitive findings.\n*   Novelty (10): 9 — The function-centric view is an insightful reframing of the sharpness debate, and the empirical discovery that SAM and other regularizers often increase sharpness is a highly novel contribution.\n*   Technical Quality (10): 8 — The experimental methodology is very strong (matched seeds, multiple metrics/architectures/datasets), though potentially limited by fixed hyperparameters and a qualitative treatment of \"function complexity.\"\n*   Clarity (10): 10 — The paper is exceptionally well-written, logically structured, and presents its arguments and results with outstanding clarity.\n*   Confidence (5): 5 — I am highly confident in my assessment; the paper's claims are well-supported by the provided evidence and the methodology is transparent and sound.",
  "final_review": "1) Summary\nThis paper challenges the conventional wisdom that flatter minima in the loss landscape of neural networks lead to better generalization. The authors propose a \"function-centric\" perspective, hypothesizing that the geometry of a solution (i.e., its sharpness) reflects the complexity of the learned function rather than being an intrinsic indicator of quality. Through extensive experiments on single-objective optimization problems and high-dimensional image classification tasks (CIFAR, TinyImageNet) with various architectures (ResNet, VGG, ViT), the paper demonstrates that common regularization techniques—such as SAM, weight decay, and data augmentation—often lead to sharper minima. Crucially, these sharper minima frequently coincide with superior performance not only in generalization but also in safety-critical metrics like calibration and robustness. The findings suggest that sharpness should be re-evaluated as a potential signature of a well-regularized, complex function with appropriate inductive biases.2) Strengths\n*   **Compelling and Coherent Thesis:** The paper introduces a \"function-centric\" interpretation of sharpness that provides a clear and intuitive framework for reconciling conflicting results in the literature. This perspective elegantly reframes sharpness not as a flaw to be avoided, but as a characteristic of the function being learned.\n    *   The introduction clearly articulates this thesis, contrasting it with the traditional \"flatness is better\" view (Section 1).\n    *   The single-objective optimization experiments provide a simple yet powerful illustration of the core idea: functions like Sphere have inherently flat optima, while others like Rosenbrock have inherently sharp optima, demonstrating that optimal geometry is function-dependent (Section 4, Figure 3, Table 2).\n    *   The conclusion effectively summarizes how the results support this function-centric view, arguing that solution geometry is shaped by task-specific demands (Section 7).*   **Extensive and Rigorous Empirical Evaluation:** The experimental design is thorough, well-controlled, and spans a wide range of settings, lending strong support to the paper's claims.\n    *   The study uses multiple modern, reparameterization-invariant sharpness metrics (Fisher-Rao Norm, Relative Flatness, SAM-Sharpness), ensuring the findings are not artifacts of a single measurement choice (Section 3, Appendix B).\n    *   The evaluation extends beyond standard accuracy to include important safety-relevant metrics like Expected Calibration Error (ECE), corruption robustness, and functional agreement, providing a more holistic view of model quality (Section 3, Tables 3, 5, 7).\n    *   Experiments are conducted across multiple datasets (CIFAR-10, CIFAR-100, TinyImageNet) and architectures (ResNet-18, VGG-19, ViT), demonstrating the generality of the findings (Section 6, Appendices G, H).\n    *   The use of a matched-seed setup for comparing different regularization controls is a methodologically sound choice that isolates the effects of each regularizer (Section 5.1).\n    *   The appendices provide substantial additional results, including hyperparameter sweeps for learning rate and batch size, which further validate the robustness of the main conclusions (Appendix F.2, G.1).*   **Significant and Counter-intuitive Findings:** The paper presents several key results that directly challenge established beliefs in the field, particularly regarding the effects of regularization and Sharpness-Aware Minimization (SAM).\n    *   The central finding that regularizers like data augmentation and weight decay consistently lead to sharper, yet better-performing, minima is a significant departure from the idea that regularization promotes flatness (Tables 3, 5, 7). For instance, in Table 3, the \"Augmentation + SAM\" model is orders of magnitude sharper than the \"Baseline\" across all metrics but achieves the best performance on every evaluation metric.\n    *   The paper provides strong evidence that SAM, despite its motivation, often *increases* global sharpness, rather than decreasing it (Section 6, \"SAM Does Not Always Flatten\"). This is shown consistently across datasets and architectures (e.g., \"Baseline\" vs. \"Baseline + SAM\" in Tables 3, 5).\n    *   The authors offer a valuable reconciliation of SAM's behavior by distinguishing between its objective of promoting *local* robustness and the measurement of *global* sharpness, providing a more nuanced understanding of how SAM works (Section 6, \"Reconciling SAM’s Objective...\").*   **High Clarity and Quality of Presentation:** The manuscript is well-written, well-structured, and easy to follow for the most part.\n    *   The motivation and contributions are clearly laid out in the introduction (Section 1).\n    *   The experimental setup and metrics are described in sufficient detail for reproducibility (Section 5.1, Appendices B, C, D).\n    *   Results are presented in clear, well-organized tables that facilitate easy comparison between different conditions (e.g., Tables 3, 5, 7).\n    *   The paper effectively uses visualizations, from the simple 2D function plots (Figure 3) to the high-dimensional loss landscape projections (Tables 4, 6), to build intuition, while also correctly pointing out the limitations of the latter (Section 6, \"Limitations of Loss Landscape Visualisations\").3) Weaknesses\n*   **\"Function Complexity\" Remains a Qualitative Concept:** The paper's central thesis hinges on the idea that sharpness reflects \"function complexity,\" but this term is never formally defined or measured for the learned neural network functions. The link is established correlatively: regularizers are applied, sharpness increases, and this is attributed to an increase in function complexity.\n    *   The term is used throughout the paper (e.g., Abstract, Section 1, Section 4) as the primary explanatory mechanism, but there is no attempt to quantify it for the trained models.\n    *   The argument relies on the intuition that regularizers enable the learning of \"more complex functions\" (Section 2), but without a direct measure of this complexity, the core causal claim remains an assertion rather than a demonstrated fact.*   **Fixed Hyperparameters Across Different Regularization Schemes:** The experimental protocol holds hyperparameters like learning rate and number of epochs constant across all \"controls\" (Baseline, SAM, Augmentation, etc.) to ensure a controlled comparison (Section 5.1). However, different regularizers can significantly alter the optimization dynamics, meaning that the optimal hyperparameters for the baseline model may be suboptimal for the regularized models.\n    *   As stated in Section 5.1, \"all other training details (optimiser, schedule, epochs, etc.) are held fixed across controls.\" This could potentially disadvantage certain methods that might perform even better with tuned hyperparameters.\n    *   While the appendix includes sweeps over learning rate and batch size (Appendix F.2, G.1), the main results in Tables 3, 5, and 7 are based on a single, fixed configuration, which may not represent the best possible performance for each control.*   **Inconsistencies Between Textual Claims and Tabulated Results:** Several key claims in the text are not fully supported by or are directly contradicted by the data presented in the tables, which weakens the overall narrative and raises concerns about the analysis.\n    *   The summary claim that the \"Baseline condition yields the flattest minima\" (Section 6, para 1) is contradicted by the TinyImageNet results (Table 7). In that table, the \"Baseline + SAM\" model is flatter than the Baseline on the Fisher-Rao Norm, and the \"Weight Decay\" model is flatter on SAM Sharpness.\n    *   The paper frames lower prediction disagreement as a universally desirable property (Section 3), but on TinyImageNet (Table 7), the best-performing regularized models (\"Augmentation\" and \"Augmentation + SAM\") exhibit the *highest* prediction disagreement, a contradiction that is not discussed.\n    *   The \"Generalisation Gap\" metric is used inconsistently. A negative value is reported for this metric in Table 25, which is impossible if it is an absolute value as labeled in figure captions (e.g., Figure 4, Figure 5). The metric also lacks a formal definition.4) Suggestions for Improvement\n*   **Provide Quantitative Measures of Function Complexity:** To strengthen the central thesis, the authors should attempt to operationalize the concept of \"function complexity.\"\n    *   Consider measuring and reporting metrics that have been proposed in the literature to quantify the complexity of a learned function, such as the number of linear regions, the spectral norm of weight matrices, or path norms. Correlating these quantitative measures with the observed sharpness metrics would provide more direct and compelling evidence for the proposed function-centric view.*   **Acknowledge and Discuss the Impact of Fixed Hyperparameters:** The authors should explicitly acknowledge the potential confounding effect of using fixed hyperparameters for all regularization schemes.\n    *   In the experimental setup (Section 5.1), add a sentence discussing this limitation and justifying the choice as a trade-off for controlled comparison. Briefly mention that while the appendix sweeps explore some of this space, a full hyperparameter search for each condition was outside the scope but could be a direction for future work to find the true performance ceiling of each method.*   **Reconcile Contradictory Results and Clarify Metrics:** The authors should carefully review their claims against their data and revise the text to accurately reflect the results, addressing any inconsistencies.\n    *   The summary claims in Section 6 should be revised to accurately reflect the data in Table 7, acknowledging cases where the baseline is not the flattest model and where SAM can lead to flatter solutions.\n    *   The discussion of prediction disagreement should be made more nuanced to account for the contradictory findings on TinyImageNet (Table 7). The authors could hypothesize why the trend might be different or simply state that the relationship appears to be task-dependent.\n    *   The \"Generalisation Gap\" metric should be formally defined in the methods section, and the negative value in Table 25 should be corrected or explained.5) Score\n*   Overall (10): 8 — The paper presents a novel and compelling perspective on a fundamental topic, supported by extensive experiments, though its main conclusions are weakened by inconsistencies between the text and the reported data.\n*   Novelty (10): 9 — The function-centric view is an insightful reframing of the sharpness debate, and the empirical discovery that SAM and other regularizers often increase sharpness is a highly novel contribution.\n*   Technical Quality (10): 7 — The experimental methodology is generally strong, but is weakened by a qualitative treatment of \"function complexity,\" fixed hyperparameters, and several inconsistencies in the reported results (Table 7, Table 25).\n*   Clarity (10): 8 — The paper is well-written and structured, but its clarity is significantly impacted by contradictions between the summary claims in the text and the data in the tables.\n*   Confidence (5): 5 — I am highly confident in my assessment; the paper's claims and the identified weaknesses are well-supported by direct evidence in the manuscript.",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 9,
        "novelty": 9,
        "technical_quality": 8,
        "clarity": 10,
        "confidence": 5
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 8,
        "novelty": 9,
        "technical_quality": 7,
        "clarity": 8,
        "confidence": 5
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "1) Summary\nThis paper challenges the conventional wisdom that flatter minima in the loss landscape of neural networks lead to better generalization. The authors propose a \"function-centric\" perspective, hypothesizing that the geometry of a solution (i.e., its sharpness) reflects the complexity of the learned function rather than being an intrinsic indicator of quality. Through extensive experiments on single-objective optimization problems and high-dimensional image classification tasks (CIFAR, TinyImageNet) with various architectures (ResNet, VGG, ViT), the paper demonstrates that common regularization techniques—such as SAM, weight decay, and data augmentation—often lead to sharper minima. Crucially, these sharper minima frequently coincide with superior performance not only in generalization but also in safety-critical metrics like calibration and robustness. The findings suggest that sharpness should be re-evaluated as a potential signature of a well-regularized, complex function with appropriate inductive biases.2) Strengths\n*   **Compelling and Coherent Thesis:** The paper introduces a \"function-centric\" interpretation of sharpness that provides a clear and intuitive framework for reconciling conflicting results in the literature. This perspective elegantly reframes sharpness not as a flaw to be avoided, but as a characteristic of the function being learned.\n    *   The introduction clearly articulates this thesis, contrasting it with the traditional \"flatness is better\" view (Section 1).\n    *   The single-objective optimization experiments provide a simple yet powerful illustration of the core idea: functions like Sphere have inherently flat optima, while others like Rosenbrock have inherently sharp optima, demonstrating that optimal geometry is function-dependent (Section 4, Figure 3, Table 2).\n    *   The conclusion effectively summarizes how the results support this function-centric view, arguing that solution geometry is shaped by task-specific demands (Section 7).*   **Extensive and Rigorous Empirical Evaluation:** The experimental design is thorough, well-controlled, and spans a wide range of settings, lending strong support to the paper's claims.\n    *   The study uses multiple modern, reparameterization-invariant sharpness metrics (Fisher-Rao Norm, Relative Flatness, SAM-Sharpness), ensuring the findings are not artifacts of a single measurement choice (Section 3, Appendix B).\n    *   The evaluation extends beyond standard accuracy to include important safety-relevant metrics like Expected Calibration Error (ECE), corruption robustness, and functional agreement, providing a more holistic view of model quality (Section 3, Tables 3, 5, 7).\n    *   Experiments are conducted across multiple datasets (CIFAR-10, CIFAR-100, TinyImageNet) and architectures (ResNet-18, VGG-19, ViT), demonstrating the generality of the findings (Section 6, Appendices G, H).\n    *   The use of a matched-seed setup for comparing different regularization controls is a methodologically sound choice that isolates the effects of each regularizer (Section 5.1).\n    *   The appendices provide substantial additional results, including hyperparameter sweeps for learning rate and batch size, which further validate the robustness of the main conclusions (Appendix F.2, G.1).*   **Significant and Counter-intuitive Findings:** The paper presents several key results that directly challenge established beliefs in the field, particularly regarding the effects of regularization and Sharpness-Aware Minimization (SAM).\n    *   The central finding that regularizers like data augmentation and weight decay consistently lead to sharper, yet better-performing, minima is a significant departure from the idea that regularization promotes flatness (Tables 3, 5, 7). For instance, in Table 3, the \"Augmentation + SAM\" model is orders of magnitude sharper than the \"Baseline\" across all metrics but achieves the best performance on every evaluation metric.\n    *   The paper provides strong evidence that SAM, despite its motivation, often *increases* global sharpness, rather than decreasing it (Section 6, \"SAM Does Not Always Flatten\"). This is shown consistently across datasets and architectures (e.g., \"Baseline\" vs. \"Baseline + SAM\" in Tables 3, 5).\n    *   The authors offer a valuable reconciliation of SAM's behavior by distinguishing between its objective of promoting *local* robustness and the measurement of *global* sharpness, providing a more nuanced understanding of how SAM works (Section 6, \"Reconciling SAM’s Objective...\").*   **High Clarity and Quality of Presentation:** The manuscript is well-written, well-structured, and easy to follow for the most part.\n    *   The motivation and contributions are clearly laid out in the introduction (Section 1).\n    *   The experimental setup and metrics are described in sufficient detail for reproducibility (Section 5.1, Appendices B, C, D).\n    *   Results are presented in clear, well-organized tables that facilitate easy comparison between different conditions (e.g., Tables 3, 5, 7).\n    *   The paper effectively uses visualizations, from the simple 2D function plots (Figure 3) to the high-dimensional loss landscape projections (Tables 4, 6), to build intuition, while also correctly pointing out the limitations of the latter (Section 6, \"Limitations of Loss Landscape Visualisations\").3) Weaknesses\n*   **\"Function Complexity\" Remains a Qualitative Concept:** The paper's central thesis hinges on the idea that sharpness reflects \"function complexity,\" but this term is never formally defined or measured for the learned neural network functions. The link is established correlatively: regularizers are applied, sharpness increases, and this is attributed to an increase in function complexity.\n    *   The term is used throughout the paper (e.g., Abstract, Section 1, Section 4) as the primary explanatory mechanism, but there is no attempt to quantify it for the trained models.\n    *   The argument relies on the intuition that regularizers enable the learning of \"more complex functions\" (Section 2), but without a direct measure of this complexity, the core causal claim remains an assertion rather than a demonstrated fact.*   **Fixed Hyperparameters Across Different Regularization Schemes:** The experimental protocol holds hyperparameters like learning rate and number of epochs constant across all \"controls\" (Baseline, SAM, Augmentation, etc.) to ensure a controlled comparison (Section 5.1). However, different regularizers can significantly alter the optimization dynamics, meaning that the optimal hyperparameters for the baseline model may be suboptimal for the regularized models.\n    *   As stated in Section 5.1, \"all other training details (optimiser, schedule, epochs, etc.) are held fixed across controls.\" This could potentially disadvantage certain methods that might perform even better with tuned hyperparameters.\n    *   While the appendix includes sweeps over learning rate and batch size (Appendix F.2, G.1), the main results in Tables 3, 5, and 7 are based on a single, fixed configuration, which may not represent the best possible performance for each control.*   **Inconsistencies Between Textual Claims and Tabulated Results:** Several key claims in the text are not fully supported by or are directly contradicted by the data presented in the tables, which weakens the overall narrative and raises concerns about the analysis.\n    *   The summary claim that the \"Baseline condition yields the flattest minima\" (Section 6, para 1) is contradicted by the TinyImageNet results (Table 7). In that table, the \"Baseline + SAM\" model is flatter than the Baseline on the Fisher-Rao Norm, and the \"Weight Decay\" model is flatter on SAM Sharpness.\n    *   The paper frames lower prediction disagreement as a universally desirable property (Section 3), but on TinyImageNet (Table 7), the best-performing regularized models (\"Augmentation\" and \"Augmentation + SAM\") exhibit the *highest* prediction disagreement, a contradiction that is not discussed.\n    *   The \"Generalisation Gap\" metric is used inconsistently. A negative value is reported for this metric in Table 25, which is impossible if it is an absolute value as labeled in figure captions (e.g., Figure 4, Figure 5). The metric also lacks a formal definition.4) Suggestions for Improvement\n*   **Provide Quantitative Measures of Function Complexity:** To strengthen the central thesis, the authors should attempt to operationalize the concept of \"function complexity.\"\n    *   Consider measuring and reporting metrics that have been proposed in the literature to quantify the complexity of a learned function, such as the number of linear regions, the spectral norm of weight matrices, or path norms. Correlating these quantitative measures with the observed sharpness metrics would provide more direct and compelling evidence for the proposed function-centric view.*   **Acknowledge and Discuss the Impact of Fixed Hyperparameters:** The authors should explicitly acknowledge the potential confounding effect of using fixed hyperparameters for all regularization schemes.\n    *   In the experimental setup (Section 5.1), add a sentence discussing this limitation and justifying the choice as a trade-off for controlled comparison. Briefly mention that while the appendix sweeps explore some of this space, a full hyperparameter search for each condition was outside the scope but could be a direction for future work to find the true performance ceiling of each method.*   **Reconcile Contradictory Results and Clarify Metrics:** The authors should carefully review their claims against their data and revise the text to accurately reflect the results, addressing any inconsistencies.\n    *   The summary claims in Section 6 should be revised to accurately reflect the data in Table 7, acknowledging cases where the baseline is not the flattest model and where SAM can lead to flatter solutions.\n    *   The discussion of prediction disagreement should be made more nuanced to account for the contradictory findings on TinyImageNet (Table 7). The authors could hypothesize why the trend might be different or simply state that the relationship appears to be task-dependent.\n    *   The \"Generalisation Gap\" metric should be formally defined in the methods section, and the negative value in Table 25 should be corrected or explained.5) Score\n*   Overall (10): 8 — The paper presents a novel and compelling perspective on a fundamental topic, supported by extensive experiments, though its main conclusions are weakened by inconsistencies between the text and the reported data.\n*   Novelty (10): 9 — The function-centric view is an insightful reframing of the sharpness debate, and the empirical discovery that SAM and other regularizers often increase sharpness is a highly novel contribution.\n*   Technical Quality (10): 7 — The experimental methodology is generally strong, but is weakened by a qualitative treatment of \"function complexity,\" fixed hyperparameters, and several inconsistencies in the reported results (Table 7, Table 25).\n*   Clarity (10): 8 — The paper is well-written and structured, but its clarity is significantly impacted by contradictions between the summary claims in the text and the data in the tables.\n*   Confidence (5): 5 — I am highly confident in my assessment; the paper's claims and the identified weaknesses are well-supported by direct evidence in the manuscript."
}