# Global Summary
- Problem: The relationship between loss-landscape sharpness/flatness and model quality (generalisation and safety properties) is commonly asserted but disputed. The paper argues sharpness is a function-dependent property tied to the complexity of the learned function, not a universal proxy for generalisation.
- Core approach: A function-centric analysis spanning (i) classical single-objective optimisation surfaces with known minima geometry and (ii) modern image classification with standard architectures (ResNet-18, VGG-19, ViT) under matched-seed controls. Sharpness is measured with reparameterisation-invariant metrics (Fisher–Rao norm, Relative Flatness) and SAM-sharpness; safety evaluations include ECE calibration, robustness (CIFAR10-C/CIFAR100-C), and functional agreement (prediction disagreement).
- Evaluation scope: 7 toy functions (e.g., Sphere, Rosenbrock, Himmelblau’s) with geometry quantified analytically; deep models trained on CIFAR-10, CIFAR-100, and Tiny ImageNet across six training controls: Baseline, Baseline+SAM, Augmentation, Augmentation+SAM, Weight Decay (5e−4), Weight Decay+SAM. Ten matched seeds per control; identical initial weights and data order; 100-epoch training budget.
- Key findings (quantitative highlights):
  - Regularised models often converge to sharper minima and simultaneously improve test accuracy, calibration, robustness, and functional agreement vs. flatter baselines.
  - ResNet-18 on CIFAR-10 (batch 256, LR 1e−3): Aug+SAM vs Baseline—Accuracy 0.908 ± 0.000 vs 0.720 ± 0.002; ECE 0.014 ± 0.001 vs 0.186 ± 0.001; Corruption Acc 71.419 ± 0.283 vs 58.614 ± 0.201; Disagreement 0.069 ± 0.000 vs 0.282 ± 0.001; FR 5.571 ± 0.035 vs 0.032 ± 0.001; RF 4970.972 ± 30.139 vs 34.607 ± 0.757.
  - ResNet-18 on CIFAR-100: Aug+SAM vs Baseline—Accuracy 0.705 ± 0.001 vs 0.530 ± 0.002; ECE 0.145 ± 0.001 vs 0.220 ± 0.001; Corruption Acc 45.428 ± 0.217 vs 38.760 ± 0.085; FR 4.179 ± 0.032 vs 0.294 ± 0.028; RF 4196.832 ± 52.606 vs 32.085 ± 0.313. Weight Decay gives best ECE (0.099 ± 0.005) but remains sharper than Baseline on most sharpness metrics.
  - ResNet-18 on Tiny ImageNet: Aug+SAM achieves best generalisation gap 16.777 ± 0.084 and ECE 0.044 ± 0.001; Weight Decay+SAM attains top test accuracy 0.539 ± 0.001 and lowest disagreement 0.339 ± 0.000. Both are sharper than Baseline on Fisher–Rao norm.
  - SAM hyperparameter sweep (ρ ∈ {0.5, 0.25, 0.05, 0.025, 0.005, 0.0025}) on CIFAR-10 shows increasing sharpness with larger ρ and strong performance at ρ = 0.25 (Accuracy 0.835 ± 0.002; ECE 0.026 ± 0.003; FR 8.712 ± 0.623; RF 4876.348 ± 314.164).
- Claimed implications: Sharpness reflects function complexity and inductive bias; regularisation can increase function complexity, yielding sharper but better solutions. SAM promotes local robustness rather than global flatness and can increase global sharpness metrics.
- Caveats explicitly stated:
  - Loss-landscape 2D visualisations can be misleading relative to metric-based sharpness.
  - Sharpness is not meaningfully comparable across architectures or tasks without accounting for function complexity/implicit bias; aggregation can exhibit Simpson’s paradox.
  - No universal “Goldilocks zone” of sharpness; task- and architecture-dependent.
  - Computational constraints: Relative Flatness not computed on Tiny ImageNet; SAM-sharpness sometimes computed on subsets.
  - The study prioritises reproducibility over SOTA optimisation.

# Abstract
- Claim: The paper reframes sharpness as a function-dependent property. Empirically, sharper minima often appear with regularisation (SAM, weight decay, data augmentation) and can accompany better generalisation, calibration, robustness, and functional agreement.
- Observation: Unregularised baselines converge to flatter minima but perform worse on safety metrics.
- Conclusion: Function complexity, not flatness alone, determines solution geometry; sharper minima can encode more appropriate inductive biases under regularisation.

# Introduction
- Context: Flat minima historically linked to smaller generalisation gaps and robustness (Occam’s Razor), with support from visualisations and SAM. Counterpoint: reparameterisation can change sharpness without function change (Dinh et al., 2017), motivating invariant metrics (Fisher–Rao, Relative Flatness).
- Gap: Relationship between flatness and safety-critical properties (adversarial robustness, calibration, functional diversity/consistency) remains underexplored.
- Hypothesis: Geometry reflects the complexity of the learned function; sharper minima need not indicate overfitting and may appear in better-regularised, high-dimensional tasks.
- Plan:
  - Toy single-objective problems (e.g., Sphere vs Rosenbrock) show optimal points can be intrinsically flat or sharp; geometry tied to function complexity.
  - Scale up to CIFAR and Tiny ImageNet with ResNet, VGG, ViT; compare Baseline vs regularised (SAM, weight decay, augmentation) using invariant sharpness metrics and safety evaluations (ECE, corruption robustness, functional agreement).
- Contributions:
  - Function-centric interpretation of sharpness; sharper minima can coincide with stronger generalisation, calibration, robustness.
  - Regularisation often induces sharper minima, challenging the assumption that regularisation promotes flatness.
  - Sharpness comparisons require accounting for function complexity and implicit bias; no universal sharpness target across tasks/architectures.
- Figure 1: 240 minima (ResNet-18, CIFAR-10) showing relationships between FR norm, Relative Flatness, generalisation gap vs train loss; full results in Appendix F.2.

# Related Work
- Early empirical link between flat minima and generalisation (Hochreiter & Schmidhuber, 1994; 1997). Landscape visualisation tied residual connections to reduced non-convexity (Li et al., 2018).
- SAM (Foret et al., 2021) proposed to reduce sharpness and improve generalisation; later work questioned whether SAM truly finds flatter minima (Wen et al., 2023).
- Reparameterisation critique: sharpness manipulable (Dinh et al., 2017), prompting invariant measures (Fisher–Rao, Relative Flatness) that reasserted a correlation with generalisation.
- This work argues regularisation improves by enabling learning of more complex functions, often associated with sharper minima; sharper solutions can emerge when models generalise better.

# Method
- Sharpness metrics:
  - Fisher–Rao norm (Liang et al., 2019), reparameterisation-invariant; analytical formula for cross-entropy (Eq. 9).
  - Relative Flatness (Petzka et al., 2021), reparameterisation-invariant; computed between feature extractor and classifier; high computational cost.
  - SAM-sharpness (Foret et al., 2021; Mason-Williams et al., 2024a): average-case loss increase at K = 100 perturbations 0.005ρ away (Eq. 10).
- Safety evaluations:
  - Calibration: Expected Calibration Error (ECE; lower is better).
  - Functional diversity/similarity: Prediction Disagreement on test set (lower means more agreement/functional consistency).
  - Robustness: Corruption Accuracy (mean) on CIFAR10-C/CIFAR100-C (higher is better).
- Formal setup for high-dimensional experiments:
  - Controls C = {Baseline, Baseline+SAM, Aug, Aug+SAM, WD, WD+SAM}.
  - Sharpness metrics M = {FR, RF, SAM}.
  - Evaluations R = {Acc_clean, Acc_corr, ECE, Disagree}.
  - Ten matched seeds per control; identical initialisation and data order; report mean ± SEM across seeds.
- Hypothesis: Regularisation increases sharpness (higher S) while improving evaluations (higher accuracy/corruption accuracy; lower ECE/disagreement).
- Additional definitions and protocols in Appendices B and C.

# Experiments
- Single-objective optimisation (Section 4):
  - Himmelblau’s function has four global minima with differing local geometry; all achieve zero loss, showing that optimality does not uniquely determine flatness/sharpness.
    - At minima (3.0, 2.0): Condition Number 3.200; Hessian Trace 108.000; Determinant 2116.000; Max Eigenvalue 82.284.
    - (−2.805118, 3.131312): 1.242; 145.390; 5222.890; 80.550.
    - (−3.77931, −3.283186): 1.892; 204.500; 9460.560; 133.786.
    - (3.584428, −1.848126): 3.674; 134.110; 3024.540; 105.419.
  - Six single-minimum functions show intrinsic geometry differences at the global optimum (Table 2):
    - Sphere: Condition Number 1.000; Trace 4.000; Det 4.000; Max Eig 2.000 (flattest).
    - Rosenbrock: 2508.010; Trace 1002.000; Det 400.000; Max Eig 1001.600 (sharp).
    - Rastrigin: 1.000; Trace 793.568; Det 157438.000; Max Eig 396.784.
    - Beale: 162.473; Trace 49.281; Det 14.766; Max Eig 48.980.
    - Booth: 9.000; Trace 20.000; Det 36.000; Max Eig 18.000.
    - Three-hump Camel: 2.784; Trace 6.000; Det 7.000; Max Eig 4.414.
  - MLP fitting setup (Appendix A.1):
    - 3-layer ReLU MLP (input 2, hidden 64, output 1), Adam lr 1e−3.
    - Train/test datasets: each 10,000 (X,Y) pairs, X,Y ∼ U(−3.5, 3.5). Seven datasets (one per function), identical input distribution, different targets.
    - Trained for up to 10^6 epochs; metrics recorded at epochs 10^0 to 10^6; 10 runs per function/initialisation.
  - Findings:
    - The sharpness encountered during training mirrors the target function’s intrinsic sharpness; sharper objectives (e.g., Rosenbrock) lead to sharper learned minima.
    - Training to equivalent losses (e.g., target train loss 300, 150, 100, 10, 1) preserves differences in sharpness across functions; generalisation gaps can be similar while sharpness differs.
    - Some functions (Beale, Rosenbrock) do not reach stricter target losses within 10^6 epochs, consistent with higher complexity.

- High-dimensional optimisation (Sections 5–6):
  - Controls, metrics, seeds as defined; matched initialisation and data order across controls for fair geometric comparisons.
  - Training setups (Appendix D):
    - CIFAR-10: SGD momentum 0.9, 100 epochs, batch 256, LR 0.001. SAM with ρ = 0.05. Aug: RandomCrop(pad 4, fill 128) + RandomHorizontalFlip(p=0.5). WD = 5e−4.
    - CIFAR-100: Same but LR 1e−2 with cosine annealing (T_max 100).
    - Tiny ImageNet: Pretrained ResNet-18 and VGG19-BN; replace final layer with 200 outputs; SGD LR 0.001, batch 256, 100 epochs; Aug: RandomResizedCrop(64) + RandomHorizontalFlip(0.5); WD = 5e−4.
  - Sharpness computation (Appendix D):
    - CIFAR-10: FR, SAM-sharpness, RF on full training data; for Aug conditions, use augmented data (Appendix F.1 shows trends unchanged if computed on standard data).
    - CIFAR-100: FR and SAM on full train; RF on 20% due to cost.
    - Tiny ImageNet: FR on full train; SAM on 20%; RF not computed (memory).
  - ResNet-18 results (means ± SEM; Tables 3, 5, 7):
    - CIFAR-10:
      - Baseline: Gap 28.050 ± 0.175; Acc 0.720 ± 0.002; ECE 0.186 ± 0.001; cACC 58.614 ± 0.201; Disagree 0.282 ± 0.001; FR 0.032 ± 0.001; SAM 1.366E−05 ± 1.206E−06; RF 34.607 ± 0.757.
      - Baseline+SAM: Acc 0.794 ± 0.001; ECE 0.108 ± 0.001; FR 0.107 ± 0.006; SAM 5.823E−05 ± 9.056E−06; RF 75.093 ± 1.693.
      - Aug: Gap 10.399 ± 0.067; Acc 0.886 ± 0.001; ECE 0.077 ± 0.001; cACC 68.755 ± 0.219; Disagree 0.121 ± 0.001; FR 3.940 ± 0.207; SAM 1.905E−01 ± 2.203E−02; RF 2903.220 ± 89.243.
      - Aug+SAM: Gap 6.864 ± 0.038; Acc 0.908 ± 0.000; ECE 0.014 ± 0.001; cACC 71.419 ± 0.283; Disagree 0.069 ± 0.000; FR 5.571 ± 0.035; SAM 1.303E−01 ± 1.547E−02; RF 4970.972 ± 30.139.
      - WD: Acc 0.721 ± 0.002; ECE 0.174 ± 0.002; FR 0.065 ± 0.004; RF 59.767 ± 3.009.
      - WD+SAM: Acc 0.802 ± 0.001; ECE 0.096 ± 0.001; FR 0.127 ± 0.006; RF 88.807 ± 2.336.
    - CIFAR-100:
      - Baseline: Gap 47.010 ± 0.166; Acc 0.530 ± 0.002; ECE 0.220 ± 0.001; cACC 38.760 ± 0.085; Disagree 0.452 ± 0.000; FR 0.294 ± 0.028; SAM 2.607E−04 ± 3.147E−05; RF 32.085 ± 0.313.
      - Baseline+SAM: Acc 0.556 ± 0.002; ECE 0.191 ± 0.002; cACC 41.888 ± 0.098; FR 0.399 ± 0.014; RF 123.791 ± 4.185.
      - Aug: Gap 29.642 ± 0.133; Acc 0.697 ± 0.002; ECE 0.185 ± 0.001; cACC 44.613 ± 0.169; Disagree 0.288 ± 0.001; FR 3.587 ± 0.150; SAM 1.110E−01 ± 9.173E−03; RF 2766.925 ± 178.669.
      - Aug+SAM: Gap 28.999 ± 0.092; Acc 0.705 ± 0.001; ECE 0.145 ± 0.001; cACC 45.428 ± 0.217; Disagree 0.269 ± 0.000; FR 4.179 ± 0.032; SAM 1.081E−01 ± 1.636E−02; RF 4196.832 ± 52.606.
      - WD: Acc 0.521 ± 0.003; ECE 0.099 ± 0.005 (best ECE); FR 0.861 ± 0.116; RF 136.969 ± 7.484.
      - WD+SAM: Acc 0.543 ± 0.001; ECE 0.106 ± 0.002; FR 1.788 ± 0.069; RF 360.271 ± 16.190.
    - Tiny ImageNet (pretrained ResNet-18):
      - Baseline: Gap 49.643 ± 0.103; Acc 0.503 ± 0.001; ECE 0.257 ± 0.001; Disagree 0.385 ± 0.000; FR 0.479 ± 0.002; SAM 3.202E−04 ± 9.872E−06.
      - Baseline+SAM: Gap 46.255 ± 0.128; Acc 0.537 ± 0.001; ECE 0.223 ± 0.001; Disagree 0.344 ± 0.000; FR 0.427 ± 0.004; SAM 3.080E−04 ± 8.424E−06.
      - Aug: Gap 19.993 ± 0.091; Acc 0.508 ± 0.001; ECE 0.102 ± 0.001; Disagree 0.544 ± 0.000; FR 25.887 ± 0.098; SAM 1.680E+00 ± 8.776E−02.
      - Aug+SAM: Gap 16.777 ± 0.084 (best); Acc 0.520 ± 0.001; ECE 0.044 ± 0.001 (best); Disagree 0.514 ± 0.000; FR 25.193 ± 0.034; SAM 1.446E+00 ± 6.332E−02.
      - WD: Acc 0.503 ± 0.001; ECE 0.202 ± 0.001; Disagree 0.384 ± 0.000; FR 0.998 ± 0.002; SAM 2.297E−04 ± 9.718E−06.
      - WD+SAM: Acc 0.539 ± 0.001 (best); ECE 0.177 ± 0.001; Disagree 0.339 ± 0.000 (best); FR 0.736 ± 0.004; SAM 3.784E−04 ± 9.996E−06.
  - Additional architectures (Appendices G, H):
    - VGG-19 on CIFAR-10 (batch 256, LR 1e−3): Aug+SAM—Acc 0.903 ± 0.001; ECE 0.019 ± 0.001; cACC 71.268 ± 0.196; FR 4.278 ± 0.027; RF 1609.212 ± 22.719; sharper than Baseline (FR 0.022 ± 0.003; RF 7.374 ± 0.470).
    - VGG-19 on CIFAR-100: Aug+SAM—Acc 0.656 ± 0.001; cACC 41.276 ± 0.089; ECE 0.157 ± 0.001; FR 5.653 ± 0.073; RF 2085.080 ± 31.648. Weight Decay has best ECE 0.138 ± 0.000 but is sharper than Baseline on FR and SAM metrics.
    - VGG19-BN on Tiny ImageNet: WD+SAM—Acc 0.641 ± 0.001 (best); Aug+SAM—ECE 0.056 ± 0.002 (best). FR greatly increases under augmentation.
    - ViT on CIFAR-10: Aug—Acc 0.724 ± 0.001; ECE 0.019 ± 0.001; FR 22.809 ± 0.117; RF 38465.647 ± 139.905 (all sharply higher than Baseline’s sharpness). Aug+SAM yields negative generalisation gap (−1.199 ± 0.097), ECE 0.030 ± 0.001, Disagreement 0.201 ± 0.001.
    - ViT on CIFAR-100: Aug+SAM—Acc 0.523 ± 0.001; ECE 0.146 ± 0.002; FR 19.664 ± 0.127; RF 17812.985 ± 55.523 vs Baseline Acc 0.309 ± 0.002; ECE 0.402 ± 0.002; FR 0.646 ± 0.061; RF 112.185 ± 4.246.
  - Landscape visualisations (Tables 4, 6, 20, 23): qualitatively show geometry changes under regularisation and SAM; however, 2D projections can appear broader even when invariant sharpness metrics increase.
  - SAM behaviour: Often increases FR, RF, and SAM-sharpness, especially when combined with augmentation; aligns with SAM’s objective of local robustness rather than guaranteed global flatness.
  - Safety at sharper minima: Across CIFAR datasets, the best safety metrics occur at solutions sharper than the Baseline.
  - No geometric Goldilocks zone: The sharpest model is not always best; optimal sharpness level depends on task/architecture. Aggregating across architectures can invert trends (Simpson’s paradox).

# Conclusion
- Empirical evidence across toy and high-dimensional settings shows that regularisation (SAM, weight decay, augmentation) often leads to sharper solutions that improve generalisation and safety metrics (calibration, corruption robustness, prediction agreement).
- Sharpness should be interpreted through a function-centric lens: geometry reflects the complexity of the learned function and inductive biases.
- SAM promotes local robustness and may increase global sharpness metrics; improved performance can coincide with sharper minima.
- There is no universal target sharpness; it is task- and architecture-dependent. The study encourages re-evaluating geometric intuitions and explicitly linking training controls, geometry, and reliability.

# Appendix
- Acknowledgements: Computations on King’s College London HPC (2025) and Sulis Tier 2 HPC; UKRI support (EP/S023356/1).
- Single-objective functions: Sphere (Eq. 2), Rosenbrock (Eq. 3, a=1, b=100), Rastrigin (Eq. 4, a=10), Beale (Eq. 5), Booth (Eq. 6), Three-hump Camel (Eq. 7), Himmelblau’s (Eq. 8). Global minima and sharpness statistics reported (Tables 1–2).
- Toy training details: MLP (3 layers, hidden 64) with Adam lr 1e−3; datasets of 10,000 train and 10,000 test points (U(−3.5, 3.5)); 10 runs per function; up to 10^6 epochs; analyses also at fixed target train losses (300, 150, 100, 10, 1). For Beale and Rosenbrock, some targets not reached within 10^6 epochs.
- Sharpness metrics (Appendix B): FR formula (Eq. 9); SAM-sharpness (Eq. 10, K=100, distance 0.005ρ); Relative Flatness definition and computational burden; layer counts for FR: VGG19=18, ResNet18=21, ViT=26.
- Safety metrics (Appendix C): ECE via TorchMetrics Multiclass Calibration Error; Prediction Disagreement (top-1); Corruption Accuracy averages over CIFAR10-C/CIFAR100-C severity levels.
- Experimental settings (Appendix D): Full training hyperparameters per dataset; sharpness computation subsets (CIFAR-100 RF on 20%; Tiny ImageNet RF not computed; Tiny ImageNet SAM-sharpness on 20%); 100-epoch training budget for comparability.
- SAM ρ sweep (Appendix E): ρ ∈ {0.5, 0.25, 0.05, 0.025, 0.005, 0.0025}. Best overall trade-off at ρ = 0.25 (Acc 0.835 ± 0.002; ECE 0.026 ± 0.003; cACC 68.479 ± 0.302; FR 8.712 ± 0.623; RF 4876.348 ± 314.164). Sharpest (ρ = 0.5) is not best performing.
- Robustness of augmentation sharpness computation (Appendix F.1, G.2): Using augmented vs standard training data for sharpness yields similar trends (e.g., ResNet-18 CIFAR-10 Aug FR 3.940 ± 0.207 vs 3.962 ± 0.292).
- Hyperparameter sweeps (Appendix F.2, G.1): Across batch sizes {128, 256} and LRs {1e−3, 1e−2}, Aug+SAM typically best on evaluations and sharper than Baseline; learning rate can alter absolute sharpness levels while trends persist.
- Additional results:
  - VGG-19 on CIFAR-10/100 and Tiny ImageNet (Tables 16–19, 22, 24): Aug+SAM often best; Weight Decay sometimes best ECE.
  - ViT on CIFAR-10/100 (Tables 25–26): Aug and Aug+SAM best; sharpness metrics dramatically increase under augmentation.
- Limitations discussed: Loss landscape projections vs invariant metrics; computational constraints for Relative Flatness on Tiny ImageNet; caution against cross-architecture sharpness comparisons; Simpson’s paradox in aggregated analyses.

# References
- Cited works include foundational and recent studies on flat/sharp minima, SAM, invariant sharpness metrics (e.g., Hochreiter & Schmidhuber 1994/1997; Dinh et al. 2017; Li et al. 2018; Foret et al. 2021; Petzka et al. 2021; Wen et al. 2023; Liang et al. 2019), datasets (Krizhevsky & Hinton 2009; Le & Yang 2015; Hendrycks & Dietterich 2019), calibration (Guo et al. 2017; Kumar et al. 2019), and related perspectives on diversity, ensembles, and inductive bias. Numerical results and methodology are attributed throughout to the paper’s tables and appendices.