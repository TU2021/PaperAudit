{
  "baseline_review": "Summary\n- The paper revisits the relationship between loss-landscape sharpness and model performance, arguing that sharpness is a function-dependent property tied to the complexity of the learned function rather than a universal proxy for generalisation. The authors support a “function-centric” view via two empirical tracks: (1) toy single-objective optimisation problems where global minima exhibit varying sharpness despite equal optimal loss (Section 4; Table 1; Figure 2; Figure 3; Table 2), and (2) high-dimensional vision tasks on CIFAR-10/100 and TinyImageNet across ResNet-18, VGG-19, and ViT with standard regularisers (SAM, weight decay, augmentation), measured using reparameterisation-invariant sharpness metrics (Fisher–Rao Norm, Relative Flatness) and safety metrics (ECE, corruption accuracy, prediction disagreement) (Sections 3, 5–6; Tables 3, 5, 7; Appendices G–H). Core findings: regularised models typically converge to sharper minima yet improve generalisation and safety metrics; SAM often increases sharpness; and loss landscape visualisations can mislead relative to metric-based sharpness (Sections 6; Tables 3–7; Figures 1, 4, 6–7).Strengths\n- Bold function-centric hypothesis, consistently evaluated across settings\n  • The central claim—that sharpness reflects function complexity rather than being a universal proxy for generalisation—is clearly stated and carried through both toy and high-dimensional studies (Section 1; Section 4; Section 5; Section 6). This is impactful because it reframes a pervasive intuition about flat minima and grounds it in experiments spanning different regimes.\n  • Toy optimisation demonstrates identical optimal loss with varying sharpness across global minima (Himmelblau; Figure 2; Table 1), and varying sharpness across distinct single-objective functions (Sphere vs Rosenbrock; Figure 3; Table 2). This supports the hypothesis by showing geometry depends on the target function.\n  • High-dimensional results consistently show that controls producing better test performance and safety metrics tend to be sharper (e.g., CIFAR-10 ResNet-18: Aug+SAM vs Baseline in Table 3), strengthening the claim’s relevance to modern practice.- Thorough empirical coverage with matched-seed controls and multiple architectures\n  • Matched seeds across controls ensure the same initial weights and data order for fair geometric comparisons (Section 5.1). This improves experimental rigor by reducing confounding due to random initialisation.\n  • Multiple datasets and architectures (ResNet-18, VGG-19, ViT) are covered, with primary results in the main text and extensive confirmation in Appendices G–H (Tables 16–26; Figures 11). This breadth increases the external validity of observed trends.\n  • Systematic sweeps over batch size and learning rate (Appendix F.2; Tables 12–15; Figures 1, 11) and over SAM’s ρ hyperparameter (Appendix E; Figure 10; Table 8) show trends are robust across training regimes, enhancing the reliability of conclusions.- Use of reparameterisation-invariant sharpness metrics and multiple safety evaluations\n  • Fisher–Rao Norm and Relative Flatness are explicitly used (Section 3; Appendix B), addressing concerns about parameterisation dependence raised in prior work. This improves technical soundness of geometric measurement.\n  • Safety evaluations extend beyond accuracy: Expected Calibration Error, corruption robustness (CIFAR-C), and prediction disagreement are defined and used (Section 3; Appendix C). This broadens impact by connecting geometry to safety-critical properties.\n  • Clear reporting of mean ± SEM across 10 matched seeds (Sections 5–6; Tables 3, 5, 7; Appendices G–H) maintains transparency and supports statistical reliability.- Nuanced analysis of SAM’s objective versus global sharpness\n  • The paper reconciles SAM’s local robustness objective with increased global sharpness under broader metrics (Section 6, “SAM Does Not Always Flatten”; “Reconciling SAM’s Objective with Increased Sharpness”; Tables 3, 5, 7; Table 8). This is a valuable technical clarification that helps interpret widely used methods.\n  • Empirical evidence shows SAM often increases Fisher–Rao, Relative Flatness, and SAM-sharpness while improving evaluations (e.g., CIFAR-10, Baseline+SAM vs Baseline in Table 3), refining practical understanding of SAM’s effects.\n  • The discussion on the limitations of low-dimensional loss visualisations (Section 6; Tables 4, 6; Figures 6–7) strengthens the interpretive framework by advocating metric-based assessments.- Clear experimental descriptions and reproducibility-oriented framing\n  • Training details, metric choices, and dataset usage are provided per dataset and condition (Appendix D; Appendices F–H), aiding reproducibility.\n  • The paper emphasizes controlled comparisons over SOTA optimisation (Section 5.1), aligning with good scientific practice (Appendix D; references to Herrmann et al., 2024).\n  • Hyperparameter selections (e.g., SAM’s ρ=0.05 for main runs; Appendix E’s sweep) and calculation subsets (Appendices C–D) are stated, improving experimental transparency.Weaknesses\n- Ambiguity and lack of formal measurement of “function complexity”\n  • The paper repeatedly attributes sharpness differences to “function complexity” (Section 1; Sections 4–6), but provides no formal definition or quantitative measure in the high-dimensional setting (No direct evidence found in the manuscript). This undermines the core causal narrative.\n  • While toy functions provide intuitive geometric differences (Figure 3; Table 2), the translation of “complexity” to CIFAR/TinyImageNet tasks is asserted rather than measured (Sections 5–6). This affects novelty and technical soundness of the main claim.\n  • Statements such as “regularisation increases the complexity of the learned function” (Section 1; Section 6) are not empirically substantiated via margins, Lipschitz bounds, path norms, or representational measures (No direct evidence found in the manuscript).- Generalisation gap definition and comparability are unclear\n  • The definition of “generalisation gap” is not formalised (Section 3; Appendix D) and negative values appear (ViT, CIFAR-10: Aug+SAM — generalisation gap −1.199 ± 0.097; Appendix H.1, Table 25), suggesting non-standard computation or train/test comparability issues.\n  • Augmentation conditions “approximately converge” on the training set (Appendix D), which can alter train accuracy relative to test accuracy and inflate or flip the gap, complicating cross-condition comparisons (Appendix D; Tables 3, 16–19, 25–26).\n  • The text links “high test accuracy” to “small generalisation gap” (Appendix D) without defining whether the gap is train minus test accuracy, test minus train, or loss-based; this affects clarity and the validity of conclusions drawn from gap values (No direct evidence found in the manuscript for a formal definition).- Robustness claims conflate adversarial and corruption robustness\n  • The Introduction cites “robustness to adversarial perturbations (Hendrycks & Dietterich, 2019)” (Section 1), but the actual robustness evaluation uses CIFAR-C corruptions (Section 3; Appendix C). Hendrycks & Dietterich (2019) is not an adversarial benchmark, but common corruptions.\n  • The Abstract claims improved “robustness” broadly; however, no adversarial robustness experiments (e.g., PGD/FGSM) are presented (No direct evidence found in the manuscript), which weakens the scope of the safety claims.\n  • Safety-related conclusions (Section 6; “Safety Properties Can Exist at Sharper Minima”) rely on corruption robustness, not adversarial robustness, limiting generality.- Sharpness metric computation choices limit comparability and introduce dataset dependence\n  • Relative Flatness is computed on only 20% of CIFAR-100 and is unavailable for TinyImageNet due to memory constraints (Appendix C–D; Appendix A.2; Appendix TINYIMAGENET Sharpness). This hinders cross-dataset comparability.\n  • SAM-sharpness is computed with subsets for TinyImageNet (20%) and depends on the loss over the dataset, making it sensitive to whether augmented or standard data are used; Table 9 vs Table 10 show large numeric differences in SAM-sharpness despite similar trends (Appendix F.1).\n  • The claim that “sharpness calculation depends more on model weights than on the data” (Appendix F.1; Table 11) conflicts with the sizable differences in SAM-sharpness magnitudes between augmented vs standard data (Table 9 vs Table 10), affecting technical clarity.- Overstatement of novelty regarding SAM increasing multiple sharpness metrics\n  • The statement “To our knowledge, this is the first work to systematically document that SAM can increase multiple sharpness metrics” (Section 6) is not substantiated with a literature analysis beyond citing Wen et al. (2023) (No direct evidence found in the manuscript). This risks overclaiming novelty.\n  • The evidence presented (Tables 3, 5, 7; Table 8) is compelling empirically, but whether this is truly “first” is not demonstrated via a survey or meta-analysis (No direct evidence found in the manuscript).\n  • The framing could be softened to avoid novelty overreach, improving credibility.- Inconsistency in augmentation description and potential confounds\n  • Section 5.1 states augmentation uses “random rotation and crop,” while Appendix D specifies “Random Crop (pad=4, fill=128) + Random Horizontal Flip (p=0.5)” for CIFAR (Section 5.1; Appendix D). This inconsistency affects reproducibility and clarity.\n  • For TinyImageNet, the augmentation is “Random Resized Crop to 64 + Random Horizontal Flip” (Appendix D), which differs from CIFAR augmentation; yet the paper occasionally aggregates trends across datasets (Figures 1, 11) without emphasising these differences, risking confounding.\n  • Augmentation can reduce train accuracy (“approximately converge” in Appendix D), potentially shrinking the generalisation gap mechanically—this is not discussed when interpreting gap reductions (Sections 6; Tables 3, 5, 7).- Functional diversity definition and interpretation are confusing\n  • Section 3 frames “Functional Diversity” but measures “Prediction Disagreement” and interprets lower disagreement as more desirable “functional similarity.” This naming (diversity vs similarity) is inconsistent and may mislead.\n  • The computation details (e.g., whether pairwise across all seeds or relative to a reference model) are not specified (Section 3; Tables 3, 5, 7; Appendices G–H), affecting reproducibility.\n  • The rationale for why agreement (lower disagreement) is a safety property is asserted rather than tied to ensemble diversity or stability metrics with quantitative links (No direct evidence found in the manuscript).- Causal interpretation that regularisation increases learned function complexity is untested\n  • The paper repeatedly states or implies that regularisation yields “more complex” learned functions, driving sharper minima (Section 1; Section 6), but no direct complexity measure (e.g., margin distributions, Lipschitz constants, path norms, representational similarity/entropy) is reported (No direct evidence found in the manuscript).\n  • Loss landscape visualisations (Tables 4, 6; Figures 6–7) are acknowledged as limited (Section 6), yet are used qualitatively to support increased “complexity,” which weakens causal claims.\n  • Without direct complexity quantification, the conclusion risks being correlational: regularisers co-occur with sharpness increases and performance improvements, but the mechanism remains unestablished.Suggestions for Improvement\n- Clarify and operationalise “function complexity” with quantitative measures\n  • Provide a formal definition tailored to high-dimensional tasks and quantify it (e.g., margin distributions, Lipschitz constants/gradient norms, path norms, network Jacobian spectra) (Sections 4–6). This will directly support the causal claim.\n  • Relate measured complexity to sharpness metrics within and across conditions (Tables 3, 5, 7; Appendices G–H) to test whether complexity mediates the observed sharpness–performance relationship.\n  • Include ablations manipulating complexity (e.g., architectural changes influencing capacity or explicit regularisers altering margins) and report how measured complexity shifts correlate with sharpness (Section 5.1).- Define the generalisation gap precisely and ensure fair comparability\n  • Provide a formal definition (train vs test accuracy difference? loss difference?), units, and sign convention; explain negative gaps (Appendix H.1; Table 25) and how augmentation affects train accuracy (Appendix D).\n  • Report per-condition train metrics explicitly alongside test metrics to contextualise gap changes (Tables 3, 5, 7; Appendices G–H), and consider gap normalisation to handle non-converged training under augmentation.\n  • If augmentation affects convergence (“approximately converge” in Appendix D), add a control where training is extended or regularised to achieve comparable train accuracy for fair cross-condition gap comparisons.- Align robustness claims with the evaluated protocol and extend to adversarial tests\n  • Revise the Introduction/Abstract to reflect that robustness is measured on CIFAR-C (corruptions), not adversarial perturbations (Section 1; Section 3; Appendix C).\n  • Add adversarial robustness experiments (e.g., FGSM/PGD with standard budgets) for a subset of conditions (Baseline, SAM, Aug+SAM) on CIFAR-10/100 to test whether conclusions generalise (Tables 3, 5).\n  • Report both corruption and adversarial robustness side-by-side and discuss potential trade-offs with sharpness across metrics (Section 6), strengthening safety claims.- Standardise sharpness metric computation and address dataset dependence\n  • Where feasible, compute Relative Flatness on full datasets or apply unbiased subsampling strategies; otherwise, report sensitivity analyses showing stability of RF estimates across subsample sizes (Appendix D).\n  • For SAM-sharpness, demonstrate the effect of using augmented vs standard training data on absolute values (Appendix F.1; Table 9 vs Table 10) and consider normalising or reporting both to avoid misinterpretation; clarify why large magnitude differences occur despite similar trends.\n  • Provide a consolidated table summarising which metrics were computed on full vs partial data per dataset (Appendix D) and discuss implications for cross-dataset comparisons (Section 6).- Temper novelty claims and situate findings within related work more precisely\n  • Rephrase the “first to document” claim (Section 6) to “we systematically show,” unless a literature review confirms priority; add a brief related-work paragraph analysing existing evidence (e.g., Wen et al., 2023) and how this work differs.\n  • Clearly state the added value: reparameterisation-invariant metrics, breadth of architectures/datasets, matched-seed design, and inclusion of safety metrics (Sections 3, 5–6; Figures 1, 11).\n  • Link claims to specific empirical anchors (Tables 3, 5, 7; Table 8) to keep evidence-first phrasing.- Make augmentation protocols consistent and discuss their impact on convergence\n  • Harmonise descriptions: ensure Section 5.1 and Appendix D match for CIFAR augmentation (Random Crop + Horizontal Flip vs “rotation and crop”), and clarify TinyImageNet differences (Appendix D).\n  • Discuss how augmentation alters training loss/accuracy and consequently the generalisation gap (Appendix D), and consider reporting train accuracy for augmented conditions explicitly (Tables 3, 5, 7).\n  • If augmentation reduces train convergence (“approximately converge”), add supplementary runs or calibrate training schedules to balance convergence across conditions; alternatively, introduce a normalised gap metric.- Clarify the “Functional Diversity” metric and its interpretation\n  • Rename the section/metric to “Functional Agreement” or explicitly state that “functional diversity” here is operationalised via disagreement where lower values are interpreted as stability (Section 3).\n  • Specify computation: pairwise disagreement across all seed pairs vs reference model, and the aggregation method to obtain the single table value (Tables 3, 5, 7; Appendices G–H).\n  • Add analyses linking prediction disagreement to calibration/robustness (Section 6) to justify why agreement is a safety property; if possible, compare with ensemble diversity metrics from Fort et al. (2020) or Lu et al. (2024) as referenced.- Test the causal link between regularisation and learned function complexity\n  • Measure complexity proxies (margins, Lipschitz constants, path norms, representation entropy/convergence) per condition and test whether regularisation shifts these (Sections 5–6; Tables 3, 5, 7).\n  • Use controlled ablations (e.g., vary weight decay strength, augmentation intensity, SAM ρ in Appendix E) and report whether the measured complexity tracks sharpness and safety metrics (Table 8; Figure 10).\n  • Where feasible, perform mediation analysis: does complexity mediate the effect of regularisation on sharpness and safety metrics? This strengthens the causal interpretation (Section 6).Score\n- Overall (10): 6 — Strong empirical breadth with matched seeds and multiple architectures/datasets (Sections 5–6; Tables 3, 5, 7; Appendices G–H), but key claims hinge on unmeasured “function complexity,” unclear generalisation gap definition, and robustness conflation (Section 1; Section 3; Appendix D; Appendix H.1).\n- Novelty (10): 6 — Function-centric framing and systematic evidence that regularisation can increase sharpness while improving performance (Tables 3, 5, 7; Table 8) are useful, but the “first to document” claim (Section 6) is overstated and the conceptual link to complexity is asserted rather than formalised.\n- Technical Quality (10): 6 — Reparameterisation-invariant metrics, SEM over matched seeds, and hyperparameter sweeps (Sections 3, 5–6; Appendix E; Appendix F.2) are solid, yet metric computation inconsistencies (Appendix D; Appendix F.1), lack of adversarial robustness, and unclear generalisation gap reduce rigor.\n- Clarity (10): 7 — The paper is generally well written with detailed tables and appendices (Sections 5–6; Appendices D–H), but augmentation inconsistencies (Section 5.1 vs Appendix D), ambiguous gap definition, and functional diversity terminology (Section 3) hinder clarity.\n- Confidence (5): 4 — High confidence in the empirical observations due to extensive tabulated results and matched-seed design (Tables 3, 5, 7; Appendix E; Appendices G–H), lower confidence in causal and robustness claims due to missing formal complexity measures and adversarial tests (Section 1; Section 3; Appendix D).",
  "final_review": "Summary\n- The paper revisits the relationship between loss-landscape sharpness and model performance, arguing that sharpness is a function-dependent property tied to the complexity of the learned function rather than a universal proxy for generalisation. The authors support a “function-centric” view via two empirical tracks: (1) toy single-objective optimisation problems where global minima exhibit varying sharpness despite equal optimal loss (Section 4; Table 1; Figure 2; Figure 3; Table 2), and (2) high-dimensional vision tasks on CIFAR-10/100 and TinyImageNet across ResNet-18, VGG-19, and ViT with standard regularisers (SAM, weight decay, augmentation), measured using reparameterisation-invariant sharpness metrics (Fisher–Rao Norm, Relative Flatness) and safety metrics (ECE, corruption accuracy, prediction disagreement) (Sections 3, 5–6; Tables 3, 5, 7; Appendices G–H). Core findings: regularised models typically converge to sharper minima yet improve generalisation and safety metrics; SAM often increases sharpness; and loss landscape visualisations can mislead relative to metric-based sharpness (Sections 6; Tables 3–7; Figures 1, 4, 6–7).Strengths\n- Bold function-centric hypothesis, consistently evaluated across settings\n  • The central claim—that sharpness reflects function complexity rather than being a universal proxy for generalisation—is clearly stated and carried through both toy and high-dimensional studies (Section 1; Section 4; Section 5; Section 6). This is impactful because it reframes a pervasive intuition about flat minima and grounds it in experiments spanning different regimes.\n  • Toy optimisation demonstrates identical optimal loss with varying sharpness across global minima (Himmelblau; Figure 2; Table 1), and varying sharpness across distinct single-objective functions (Sphere vs Rosenbrock; Figure 3; Table 2). This supports the hypothesis by showing geometry depends on the target function.\n  • High-dimensional results consistently show that controls producing better test performance and safety metrics tend to be sharper (e.g., CIFAR-10 ResNet-18: Aug+SAM vs Baseline in Table 3), strengthening the claim’s relevance to modern practice.\n- Thorough empirical coverage with matched-seed controls and multiple architectures\n  • Matched seeds across controls ensure the same initial weights and data order for fair geometric comparisons (Section 5.1). This improves experimental rigor by reducing confounding due to random initialisation, though under augmentation the effective training distribution differs (Appendix D), which the paper acknowledges via “approximately converge.”\n  • Multiple datasets and architectures (ResNet-18, VGG-19, ViT) are covered, with primary results in the main text and extensive confirmation in Appendices G–H (Tables 16–26; Figures 11). This breadth increases the external validity of observed trends.\n  • Systematic sweeps over batch size and learning rate (Appendix F.2; Tables 12–15; Figures 1, 11) and over SAM’s ρ hyperparameter (Appendix E; Figure 10; Table 8) show trends are robust across training regimes, enhancing the reliability of conclusions.\n- Use of reparameterisation-invariant sharpness metrics and multiple safety evaluations\n  • Fisher–Rao Norm and Relative Flatness are explicitly used (Section 3; Appendix B), addressing concerns about parameterisation dependence raised in prior work. This improves technical soundness of geometric measurement.\n  • Safety evaluations extend beyond accuracy: Expected Calibration Error, corruption robustness (CIFAR-C), and prediction disagreement are defined and used (Section 3; Appendix C). This broadens impact by connecting geometry to safety-critical properties.\n  • Clear reporting of mean ± SEM across 10 matched seeds (Sections 5–6; Tables 3, 5, 7; Appendices G–H) maintains transparency and supports statistical reliability.\n- Nuanced analysis of SAM’s objective versus global sharpness\n  • The paper reconciles SAM’s local robustness objective with increased global sharpness under broader metrics (Section 6, “SAM Does Not Always Flatten”; “Reconciling SAM’s Objective with Increased Sharpness”; Tables 3, 5, 7; Table 8). This is a valuable technical clarification that helps interpret widely used methods.\n  • Empirical evidence shows SAM often increases Fisher–Rao, Relative Flatness, and SAM-sharpness while improving evaluations (e.g., CIFAR-10, Baseline+SAM vs Baseline in Table 3), refining practical understanding of SAM’s effects.\n  • The discussion on the limitations of low-dimensional loss visualisations (Section 6; Tables 4, 6; Figures 6–7) strengthens the interpretive framework by advocating metric-based assessments.\n- Clear experimental descriptions and reproducibility-oriented framing\n  • Training details, metric choices, and dataset usage are provided per dataset and condition (Appendix D; Appendices F–H), aiding reproducibility.\n  • The paper emphasizes controlled comparisons over SOTA optimisation (Section 5.1), aligning with good scientific practice (Appendix D; references to Herrmann et al., 2024).\n  • Hyperparameter selections (e.g., SAM’s ρ=0.05 for main runs; Appendix E’s sweep) and calculation subsets (Appendices C–D) are stated, improving experimental transparency.Weaknesses\n- Ambiguity and lack of formal measurement of “function complexity”\n  • The paper repeatedly attributes sharpness differences to “function complexity” (Section 1; Sections 4–6), but provides no formal definition or quantitative measure in the high-dimensional setting (No direct evidence found in the manuscript). This undermines the core causal narrative.\n  • While toy functions provide intuitive geometric differences (Figure 3; Table 2), the translation of “complexity” to CIFAR/TinyImageNet tasks is asserted rather than measured (Sections 5–6). This affects novelty and technical soundness of the main claim.\n  • Statements such as “regularisation increases the complexity of the learned function” (Section 1; Section 6) are not empirically substantiated via margins, Lipschitz bounds, path norms, or representational measures (No direct evidence found in the manuscript).\n- Generalisation gap definition and comparability are unclear\n  • The definition of “generalisation gap” is not formalised (Section 3; Appendix D) and negative values appear (ViT, CIFAR-10: Aug+SAM — generalisation gap −1.199 ± 0.097; Appendix H.1, Table 25), suggesting non-standard computation or train/test comparability issues.\n  • Augmentation conditions “approximately converge” on the training set (Appendix D), which can alter train accuracy relative to test accuracy and inflate or flip the gap, complicating cross-condition comparisons (Appendix D; Tables 3, 16–19, 25–26).\n  • The text links “high test accuracy” to “small generalisation gap” (Appendix D) without defining whether the gap is train minus test accuracy, test minus train, or loss-based; this affects clarity and the validity of conclusions drawn from gap values (No direct evidence found in the manuscript for a formal definition).\n- Robustness claims conflate adversarial and corruption robustness\n  • The Introduction cites “robustness to adversarial perturbations (Hendrycks & Dietterich, 2019)” (Section 1), but the actual robustness evaluation uses CIFAR-C corruptions (Section 3; Appendix C). Hendrycks & Dietterich (2019) is not an adversarial benchmark, but common corruptions.\n  • The Abstract claims improved “robustness” broadly; however, no adversarial robustness experiments (e.g., PGD/FGSM) are presented (No direct evidence found in the manuscript), which weakens the scope of the safety claims.\n  • Safety-related conclusions (Section 6; “Safety Properties Can Exist at Sharper Minima”) rely on corruption robustness, not adversarial robustness, limiting generality.\n- Sharpness metric computation choices limit comparability and introduce dataset dependence\n  • Relative Flatness is computed on only 20% of CIFAR-100 and is unavailable for TinyImageNet due to memory constraints (Appendix C–D; Appendix A.2; Appendix TINYIMAGENET Sharpness). This hinders cross-dataset comparability.\n  • SAM-sharpness is computed with subsets for TinyImageNet (20%) and depends on the loss over the dataset, making it sensitive to whether augmented or standard data are used; Table 9 vs Table 10 show large numeric differences in SAM-sharpness despite similar trends (Appendix F.1).\n  • The claim that “sharpness calculation depends more on model weights than on the data” (Appendix F.1; Table 11) conflicts with the sizable differences in SAM-sharpness magnitudes between augmented vs standard data (Table 9 vs Table 10), affecting technical clarity.\n- Overstatement of novelty regarding SAM increasing multiple sharpness metrics\n  • The statement “To our knowledge, this is the first work to systematically document that SAM can increase multiple sharpness metrics” (Section 6) is not substantiated with a literature analysis beyond citing Wen et al. (2023) (No direct evidence found in the manuscript). This risks overclaiming novelty.\n  • The evidence presented (Tables 3, 5, 7; Table 8) is compelling empirically, but whether this is truly “first” is not demonstrated via a survey or meta-analysis (No direct evidence found in the manuscript).\n  • The framing could be softened to avoid novelty overreach, improving credibility.\n- Inconsistency in augmentation description and potential confounds\n  • Section 5.1 states augmentation uses “random rotation and crop,” while Appendix D specifies “Random Crop (pad=4, fill=128) + Random Horizontal Flip (p=0.5)” for CIFAR (Section 5.1; Appendix D). This inconsistency affects reproducibility and clarity.\n  • For TinyImageNet, the augmentation is “Random Resized Crop to 64 + Random Horizontal Flip” (Appendix D), which differs from CIFAR augmentation; yet the paper occasionally aggregates trends across datasets (Figures 1, 11) without emphasising these differences, risking confounding.\n  • Augmentation can reduce train accuracy (“approximately converge” in Appendix D), potentially shrinking the generalisation gap mechanically—this is not discussed when interpreting gap reductions (Sections 6; Tables 3, 5, 7).\n- Functional diversity definition and interpretation are confusing\n  • Section 3 frames “Functional Diversity” but measures “Prediction Disagreement” and interprets lower disagreement as more desirable “functional similarity.” This naming (diversity vs similarity) is inconsistent and may mislead.\n  • The computation details (e.g., whether pairwise across all seeds or relative to a reference model) are not specified (Section 3; Tables 3, 5, 7; Appendices G–H), affecting reproducibility.\n  • The rationale for why agreement (lower disagreement) is a safety property is asserted rather than tied to ensemble diversity or stability metrics with quantitative links (No direct evidence found in the manuscript).\n- Causal interpretation that regularisation increases learned function complexity is untested\n  • The paper repeatedly states or implies that regularisation yields “more complex” learned functions, driving sharper minima (Section 1; Section 6), but no direct complexity measure (e.g., margin distributions, Lipschitz constants, path norms, representational similarity/entropy) is reported (No direct evidence found in the manuscript).\n  • Loss landscape visualisations (Tables 4, 6; Figures 6–7) are acknowledged as limited (Section 6), yet are used qualitatively to support increased “complexity,” which weakens causal claims.\n  • Without direct complexity quantification, the conclusion risks being correlational: regularisers co-occur with sharpness increases and performance improvements, but the mechanism remains unestablished.\n- Mislabeling and inconsistent bolding in TinyImageNet results (Table 7)\n  • Table 7 caption states “Bolded values indicate the best performance per metric,” but Augmentation+SAM is bolded for Test Accuracy and Prediction Disagreement while Weight Decay+SAM has higher accuracy (0.539 ± 0.001) and lower disagreement (0.339 ± 0.000) (Table 7). This mislabeling affects interpretation.\n  • The table does not consistently apply bolding to sharpness minima despite the caption indicating best per metric; consistency is important for clarity (Table 7).\n  • Such inconsistencies weaken the evidentiary presentation of which controls are “best”, potentially biasing conclusions (Table 7).\n- Ambiguity in SAM-sharpness metric hyperparameter ρ across controls\n  • Appendix B defines SAM-sharpness using offsets “0.005ρ” and division by ρ (Eq. 10), but the manuscript does not specify which ρ is used for the metric when a model was trained without SAM or with a different ρ. No direct evidence found in the manuscript clarifying this.\n  • Nevertheless, Baseline models report finite SAM-sharpness values (e.g., Table 3: 1.366E−05 ± 1.206E−06; Table 5; Table 7), implying a nonzero ρ was used for measurement; without explicit specification, cross-condition comparability is unclear (Appendix B; Tables 3, 5, 7; Table 8).\n  • The ρ sweep (Appendix E; Table 8) lists training ρ values, but it does not state whether the same ρ is used in SAM-sharpness computation for all controls, including non-SAM models. No direct evidence found in the manuscript.Suggestions for Improvement\n- Clarify and operationalise “function complexity” with quantitative measures\n  • Provide a formal definition tailored to high-dimensional tasks and quantify it (e.g., margin distributions, Lipschitz constants/gradient norms, path norms, network Jacobian spectra) (Sections 4–6). This will directly support the causal claim.\n  • Relate measured complexity to sharpness metrics within and across conditions (Tables 3, 5, 7; Appendices G–H) to test whether complexity mediates the observed sharpness–performance relationship.\n  • Include ablations manipulating complexity (e.g., architectural changes influencing capacity or explicit regularisers altering margins) and report how measured complexity shifts correlate with sharpness (Section 5.1).\n- Define the generalisation gap precisely and ensure fair comparability\n  • Provide a formal definition (train vs test accuracy difference? loss difference?), units, and sign convention; explain negative gaps (Appendix H.1; Table 25) and how augmentation affects train accuracy (Appendix D).\n  • Report per-condition train metrics explicitly alongside test metrics to contextualise gap changes (Tables 3, 5, 7; Appendices G–H), and consider gap normalisation to handle non-converged training under augmentation.\n  • If augmentation affects convergence (“approximately converge” in Appendix D), add a control where training is extended or regularised to achieve comparable train accuracy for fair cross-condition gap comparisons.\n- Align robustness claims with the evaluated protocol and extend to adversarial tests\n  • Revise the Introduction/Abstract to reflect that robustness is measured on CIFAR-C (corruptions), not adversarial perturbations (Section 1; Section 3; Appendix C).\n  • Add adversarial robustness experiments (e.g., FGSM/PGD with standard budgets) for a subset of conditions (Baseline, SAM, Aug+SAM) on CIFAR-10/100 to test whether conclusions generalise (Tables 3, 5).\n  • Report both corruption and adversarial robustness side-by-side and discuss potential trade-offs with sharpness across metrics (Section 6), strengthening safety claims.\n- Standardise sharpness metric computation and address dataset dependence\n  • Where feasible, compute Relative Flatness on full datasets or apply unbiased subsampling strategies; otherwise, report sensitivity analyses showing stability of RF estimates across subsample sizes (Appendix D).\n  • For SAM-sharpness, demonstrate the effect of using augmented vs standard training data on absolute values (Appendix F.1; Table 9 vs Table 10) and consider normalising or reporting both to avoid misinterpretation; clarify why large magnitude differences occur despite similar trends.\n  • Provide a consolidated table summarising which metrics were computed on full vs partial data per dataset (Appendix D) and discuss implications for cross-dataset comparisons (Section 6).\n- Temper novelty claims and situate findings within related work more precisely\n  • Rephrase the “first to document” claim (Section 6) to “we systematically show,” unless a literature review confirms priority; add a brief related-work paragraph analysing existing evidence (e.g., Wen et al., 2023) and how this work differs.\n  • Clearly state the added value: reparameterisation-invariant metrics, breadth of architectures/datasets, matched-seed design, and inclusion of safety metrics (Sections 3, 5–6; Figures 1, 11).\n  • Link claims to specific empirical anchors (Tables 3, 5, 7; Table 8) to keep evidence-first phrasing.\n- Make augmentation protocols consistent and discuss their impact on convergence\n  • Harmonise descriptions: ensure Section 5.1 and Appendix D match for CIFAR augmentation (Random Crop + Horizontal Flip vs “rotation and crop”), and clarify TinyImageNet differences (Appendix D).\n  • Discuss how augmentation alters training loss/accuracy and consequently the generalisation gap (Appendix D), and consider reporting train accuracy for augmented conditions explicitly (Tables 3, 5, 7).\n  • If augmentation reduces train convergence (“approximately converge”), add supplementary runs or calibrate training schedules to balance convergence across conditions; alternatively, introduce a normalised gap metric.\n- Clarify the “Functional Diversity” metric and its interpretation\n  • Rename the section/metric to “Functional Agreement” or explicitly state that “functional diversity” here is operationalised via disagreement where lower values are interpreted as stability (Section 3).\n  • Specify computation: pairwise disagreement across all seed pairs vs reference model, and the aggregation method to obtain the single table value (Tables 3, 5, 7; Appendices G–H).\n  • Add analyses linking prediction disagreement to calibration/robustness (Section 6) to justify why agreement is a safety property; if possible, compare with ensemble diversity metrics from Fort et al. (2020) or Lu et al. (2024) as referenced.\n- Test the causal link between regularisation and learned function complexity\n  • Measure complexity proxies (margins, Lipschitz constants, path norms, representation entropy/convergence) per condition and test whether regularisation shifts these (Sections 5–6; Tables 3, 5, 7).\n  • Use controlled ablations (e.g., vary weight decay strength, augmentation intensity, SAM ρ in Appendix E) and report whether the measured complexity tracks sharpness and safety metrics (Table 8; Figure 10).\n  • Where feasible, perform mediation analysis: does complexity mediate the effect of regularisation on sharpness and safety metrics? This strengthens the causal interpretation (Section 6).\n- Correct mislabeling and ensure consistent highlighting of best values in Table 7\n  • Update Table 7 to bold Weight Decay+SAM for Test Accuracy (0.539 ± 0.001) and Prediction Disagreement (0.339 ± 0.000) and ensure consistency with the caption’s “best per metric” instruction (Table 7).\n  • Apply consistent bolding to sharpness minima (Fisher–Rao Norm, SAM-sharpness) if the caption indicates best per metric; clarify whether “best” for sharpness refers to minima (Table 7).\n  • Re-check cross-text references to TinyImageNet “best” results to avoid misinterpretation based on the current misbolding (Section 6; Table 7).\n- Specify the SAM-sharpness ρ used for metric computation across all controls\n  • Explicitly state the ρ used to compute SAM-sharpness for each control, including models trained without SAM (Appendix B; Tables 3, 5, 7; Table 8).\n  • Indicate whether a single metric ρ (e.g., 0.05) is used uniformly for SAM-sharpness evaluation across all controls to ensure comparability, independent of training ρ (Appendix B; Appendix E).\n  • If different metric ρ values are used, provide a rationale and a sensitivity analysis showing the effect on SAM-sharpness across controls (Appendix B; Appendix E).Score\n- Overall (10): 6 — Strong empirical breadth with matched seeds and multiple architectures/datasets (Sections 5–6; Tables 3, 5, 7; Appendices G–H), but key claims hinge on unmeasured “function complexity,” unclear generalisation gap definition, and robustness conflation (Section 1; Section 3; Appendix D; Appendix H.1).\n- Novelty (10): 6 — Function-centric framing and systematic evidence that regularisation can increase sharpness while improving performance (Tables 3, 5, 7; Table 8) are useful, but the “first to document” claim (Section 6) is overstated and the conceptual link to complexity is asserted rather than formalised.\n- Technical Quality (10): 6 — Reparameterisation-invariant metrics, SEM over matched seeds, and hyperparameter sweeps (Sections 3, 5–6; Appendix E; Appendix F.2) are solid, yet metric computation inconsistencies (Appendix D; Appendix F.1), lack of adversarial robustness, and unclear generalisation gap reduce rigor.\n- Clarity (10): 7 — The paper is generally well written with detailed tables and appendices (Sections 5–6; Appendices D–H), but augmentation inconsistencies (Section 5.1 vs Appendix D), ambiguous gap definition, and functional diversity terminology (Section 3) hinder clarity.\n- Confidence (5): 4 — High confidence in the empirical observations due to extensive tabulated results and matched-seed design (Tables 3, 5, 7; Appendix E; Appendices G–H), lower confidence in causal and robustness claims due to missing formal complexity measures and adversarial tests (Section 1; Section 3; Appendix D).",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 6,
        "novelty": 6,
        "technical_quality": 6,
        "clarity": 7,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 6,
        "novelty": 6,
        "technical_quality": 6,
        "clarity": 7,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "Summary\n- The paper revisits the relationship between loss-landscape sharpness and model performance, arguing that sharpness is a function-dependent property tied to the complexity of the learned function rather than a universal proxy for generalisation. The authors support a “function-centric” view via two empirical tracks: (1) toy single-objective optimisation problems where global minima exhibit varying sharpness despite equal optimal loss (Section 4; Table 1; Figure 2; Figure 3; Table 2), and (2) high-dimensional vision tasks on CIFAR-10/100 and TinyImageNet across ResNet-18, VGG-19, and ViT with standard regularisers (SAM, weight decay, augmentation), measured using reparameterisation-invariant sharpness metrics (Fisher–Rao Norm, Relative Flatness) and safety metrics (ECE, corruption accuracy, prediction disagreement) (Sections 3, 5–6; Tables 3, 5, 7; Appendices G–H). Core findings: regularised models typically converge to sharper minima yet improve generalisation and safety metrics; SAM often increases sharpness; and loss landscape visualisations can mislead relative to metric-based sharpness (Sections 6; Tables 3–7; Figures 1, 4, 6–7).Strengths\n- Bold function-centric hypothesis, consistently evaluated across settings\n  • The central claim—that sharpness reflects function complexity rather than being a universal proxy for generalisation—is clearly stated and carried through both toy and high-dimensional studies (Section 1; Section 4; Section 5; Section 6). This is impactful because it reframes a pervasive intuition about flat minima and grounds it in experiments spanning different regimes.\n  • Toy optimisation demonstrates identical optimal loss with varying sharpness across global minima (Himmelblau; Figure 2; Table 1), and varying sharpness across distinct single-objective functions (Sphere vs Rosenbrock; Figure 3; Table 2). This supports the hypothesis by showing geometry depends on the target function.\n  • High-dimensional results consistently show that controls producing better test performance and safety metrics tend to be sharper (e.g., CIFAR-10 ResNet-18: Aug+SAM vs Baseline in Table 3), strengthening the claim’s relevance to modern practice.\n- Thorough empirical coverage with matched-seed controls and multiple architectures\n  • Matched seeds across controls ensure the same initial weights and data order for fair geometric comparisons (Section 5.1). This improves experimental rigor by reducing confounding due to random initialisation, though under augmentation the effective training distribution differs (Appendix D), which the paper acknowledges via “approximately converge.”\n  • Multiple datasets and architectures (ResNet-18, VGG-19, ViT) are covered, with primary results in the main text and extensive confirmation in Appendices G–H (Tables 16–26; Figures 11). This breadth increases the external validity of observed trends.\n  • Systematic sweeps over batch size and learning rate (Appendix F.2; Tables 12–15; Figures 1, 11) and over SAM’s ρ hyperparameter (Appendix E; Figure 10; Table 8) show trends are robust across training regimes, enhancing the reliability of conclusions.\n- Use of reparameterisation-invariant sharpness metrics and multiple safety evaluations\n  • Fisher–Rao Norm and Relative Flatness are explicitly used (Section 3; Appendix B), addressing concerns about parameterisation dependence raised in prior work. This improves technical soundness of geometric measurement.\n  • Safety evaluations extend beyond accuracy: Expected Calibration Error, corruption robustness (CIFAR-C), and prediction disagreement are defined and used (Section 3; Appendix C). This broadens impact by connecting geometry to safety-critical properties.\n  • Clear reporting of mean ± SEM across 10 matched seeds (Sections 5–6; Tables 3, 5, 7; Appendices G–H) maintains transparency and supports statistical reliability.\n- Nuanced analysis of SAM’s objective versus global sharpness\n  • The paper reconciles SAM’s local robustness objective with increased global sharpness under broader metrics (Section 6, “SAM Does Not Always Flatten”; “Reconciling SAM’s Objective with Increased Sharpness”; Tables 3, 5, 7; Table 8). This is a valuable technical clarification that helps interpret widely used methods.\n  • Empirical evidence shows SAM often increases Fisher–Rao, Relative Flatness, and SAM-sharpness while improving evaluations (e.g., CIFAR-10, Baseline+SAM vs Baseline in Table 3), refining practical understanding of SAM’s effects.\n  • The discussion on the limitations of low-dimensional loss visualisations (Section 6; Tables 4, 6; Figures 6–7) strengthens the interpretive framework by advocating metric-based assessments.\n- Clear experimental descriptions and reproducibility-oriented framing\n  • Training details, metric choices, and dataset usage are provided per dataset and condition (Appendix D; Appendices F–H), aiding reproducibility.\n  • The paper emphasizes controlled comparisons over SOTA optimisation (Section 5.1), aligning with good scientific practice (Appendix D; references to Herrmann et al., 2024).\n  • Hyperparameter selections (e.g., SAM’s ρ=0.05 for main runs; Appendix E’s sweep) and calculation subsets (Appendices C–D) are stated, improving experimental transparency.Weaknesses\n- Ambiguity and lack of formal measurement of “function complexity”\n  • The paper repeatedly attributes sharpness differences to “function complexity” (Section 1; Sections 4–6), but provides no formal definition or quantitative measure in the high-dimensional setting (No direct evidence found in the manuscript). This undermines the core causal narrative.\n  • While toy functions provide intuitive geometric differences (Figure 3; Table 2), the translation of “complexity” to CIFAR/TinyImageNet tasks is asserted rather than measured (Sections 5–6). This affects novelty and technical soundness of the main claim.\n  • Statements such as “regularisation increases the complexity of the learned function” (Section 1; Section 6) are not empirically substantiated via margins, Lipschitz bounds, path norms, or representational measures (No direct evidence found in the manuscript).\n- Generalisation gap definition and comparability are unclear\n  • The definition of “generalisation gap” is not formalised (Section 3; Appendix D) and negative values appear (ViT, CIFAR-10: Aug+SAM — generalisation gap −1.199 ± 0.097; Appendix H.1, Table 25), suggesting non-standard computation or train/test comparability issues.\n  • Augmentation conditions “approximately converge” on the training set (Appendix D), which can alter train accuracy relative to test accuracy and inflate or flip the gap, complicating cross-condition comparisons (Appendix D; Tables 3, 16–19, 25–26).\n  • The text links “high test accuracy” to “small generalisation gap” (Appendix D) without defining whether the gap is train minus test accuracy, test minus train, or loss-based; this affects clarity and the validity of conclusions drawn from gap values (No direct evidence found in the manuscript for a formal definition).\n- Robustness claims conflate adversarial and corruption robustness\n  • The Introduction cites “robustness to adversarial perturbations (Hendrycks & Dietterich, 2019)” (Section 1), but the actual robustness evaluation uses CIFAR-C corruptions (Section 3; Appendix C). Hendrycks & Dietterich (2019) is not an adversarial benchmark, but common corruptions.\n  • The Abstract claims improved “robustness” broadly; however, no adversarial robustness experiments (e.g., PGD/FGSM) are presented (No direct evidence found in the manuscript), which weakens the scope of the safety claims.\n  • Safety-related conclusions (Section 6; “Safety Properties Can Exist at Sharper Minima”) rely on corruption robustness, not adversarial robustness, limiting generality.\n- Sharpness metric computation choices limit comparability and introduce dataset dependence\n  • Relative Flatness is computed on only 20% of CIFAR-100 and is unavailable for TinyImageNet due to memory constraints (Appendix C–D; Appendix A.2; Appendix TINYIMAGENET Sharpness). This hinders cross-dataset comparability.\n  • SAM-sharpness is computed with subsets for TinyImageNet (20%) and depends on the loss over the dataset, making it sensitive to whether augmented or standard data are used; Table 9 vs Table 10 show large numeric differences in SAM-sharpness despite similar trends (Appendix F.1).\n  • The claim that “sharpness calculation depends more on model weights than on the data” (Appendix F.1; Table 11) conflicts with the sizable differences in SAM-sharpness magnitudes between augmented vs standard data (Table 9 vs Table 10), affecting technical clarity.\n- Overstatement of novelty regarding SAM increasing multiple sharpness metrics\n  • The statement “To our knowledge, this is the first work to systematically document that SAM can increase multiple sharpness metrics” (Section 6) is not substantiated with a literature analysis beyond citing Wen et al. (2023) (No direct evidence found in the manuscript). This risks overclaiming novelty.\n  • The evidence presented (Tables 3, 5, 7; Table 8) is compelling empirically, but whether this is truly “first” is not demonstrated via a survey or meta-analysis (No direct evidence found in the manuscript).\n  • The framing could be softened to avoid novelty overreach, improving credibility.\n- Inconsistency in augmentation description and potential confounds\n  • Section 5.1 states augmentation uses “random rotation and crop,” while Appendix D specifies “Random Crop (pad=4, fill=128) + Random Horizontal Flip (p=0.5)” for CIFAR (Section 5.1; Appendix D). This inconsistency affects reproducibility and clarity.\n  • For TinyImageNet, the augmentation is “Random Resized Crop to 64 + Random Horizontal Flip” (Appendix D), which differs from CIFAR augmentation; yet the paper occasionally aggregates trends across datasets (Figures 1, 11) without emphasising these differences, risking confounding.\n  • Augmentation can reduce train accuracy (“approximately converge” in Appendix D), potentially shrinking the generalisation gap mechanically—this is not discussed when interpreting gap reductions (Sections 6; Tables 3, 5, 7).\n- Functional diversity definition and interpretation are confusing\n  • Section 3 frames “Functional Diversity” but measures “Prediction Disagreement” and interprets lower disagreement as more desirable “functional similarity.” This naming (diversity vs similarity) is inconsistent and may mislead.\n  • The computation details (e.g., whether pairwise across all seeds or relative to a reference model) are not specified (Section 3; Tables 3, 5, 7; Appendices G–H), affecting reproducibility.\n  • The rationale for why agreement (lower disagreement) is a safety property is asserted rather than tied to ensemble diversity or stability metrics with quantitative links (No direct evidence found in the manuscript).\n- Causal interpretation that regularisation increases learned function complexity is untested\n  • The paper repeatedly states or implies that regularisation yields “more complex” learned functions, driving sharper minima (Section 1; Section 6), but no direct complexity measure (e.g., margin distributions, Lipschitz constants, path norms, representational similarity/entropy) is reported (No direct evidence found in the manuscript).\n  • Loss landscape visualisations (Tables 4, 6; Figures 6–7) are acknowledged as limited (Section 6), yet are used qualitatively to support increased “complexity,” which weakens causal claims.\n  • Without direct complexity quantification, the conclusion risks being correlational: regularisers co-occur with sharpness increases and performance improvements, but the mechanism remains unestablished.\n- Mislabeling and inconsistent bolding in TinyImageNet results (Table 7)\n  • Table 7 caption states “Bolded values indicate the best performance per metric,” but Augmentation+SAM is bolded for Test Accuracy and Prediction Disagreement while Weight Decay+SAM has higher accuracy (0.539 ± 0.001) and lower disagreement (0.339 ± 0.000) (Table 7). This mislabeling affects interpretation.\n  • The table does not consistently apply bolding to sharpness minima despite the caption indicating best per metric; consistency is important for clarity (Table 7).\n  • Such inconsistencies weaken the evidentiary presentation of which controls are “best”, potentially biasing conclusions (Table 7).\n- Ambiguity in SAM-sharpness metric hyperparameter ρ across controls\n  • Appendix B defines SAM-sharpness using offsets “0.005ρ” and division by ρ (Eq. 10), but the manuscript does not specify which ρ is used for the metric when a model was trained without SAM or with a different ρ. No direct evidence found in the manuscript clarifying this.\n  • Nevertheless, Baseline models report finite SAM-sharpness values (e.g., Table 3: 1.366E−05 ± 1.206E−06; Table 5; Table 7), implying a nonzero ρ was used for measurement; without explicit specification, cross-condition comparability is unclear (Appendix B; Tables 3, 5, 7; Table 8).\n  • The ρ sweep (Appendix E; Table 8) lists training ρ values, but it does not state whether the same ρ is used in SAM-sharpness computation for all controls, including non-SAM models. No direct evidence found in the manuscript.Suggestions for Improvement\n- Clarify and operationalise “function complexity” with quantitative measures\n  • Provide a formal definition tailored to high-dimensional tasks and quantify it (e.g., margin distributions, Lipschitz constants/gradient norms, path norms, network Jacobian spectra) (Sections 4–6). This will directly support the causal claim.\n  • Relate measured complexity to sharpness metrics within and across conditions (Tables 3, 5, 7; Appendices G–H) to test whether complexity mediates the observed sharpness–performance relationship.\n  • Include ablations manipulating complexity (e.g., architectural changes influencing capacity or explicit regularisers altering margins) and report how measured complexity shifts correlate with sharpness (Section 5.1).\n- Define the generalisation gap precisely and ensure fair comparability\n  • Provide a formal definition (train vs test accuracy difference? loss difference?), units, and sign convention; explain negative gaps (Appendix H.1; Table 25) and how augmentation affects train accuracy (Appendix D).\n  • Report per-condition train metrics explicitly alongside test metrics to contextualise gap changes (Tables 3, 5, 7; Appendices G–H), and consider gap normalisation to handle non-converged training under augmentation.\n  • If augmentation affects convergence (“approximately converge” in Appendix D), add a control where training is extended or regularised to achieve comparable train accuracy for fair cross-condition gap comparisons.\n- Align robustness claims with the evaluated protocol and extend to adversarial tests\n  • Revise the Introduction/Abstract to reflect that robustness is measured on CIFAR-C (corruptions), not adversarial perturbations (Section 1; Section 3; Appendix C).\n  • Add adversarial robustness experiments (e.g., FGSM/PGD with standard budgets) for a subset of conditions (Baseline, SAM, Aug+SAM) on CIFAR-10/100 to test whether conclusions generalise (Tables 3, 5).\n  • Report both corruption and adversarial robustness side-by-side and discuss potential trade-offs with sharpness across metrics (Section 6), strengthening safety claims.\n- Standardise sharpness metric computation and address dataset dependence\n  • Where feasible, compute Relative Flatness on full datasets or apply unbiased subsampling strategies; otherwise, report sensitivity analyses showing stability of RF estimates across subsample sizes (Appendix D).\n  • For SAM-sharpness, demonstrate the effect of using augmented vs standard training data on absolute values (Appendix F.1; Table 9 vs Table 10) and consider normalising or reporting both to avoid misinterpretation; clarify why large magnitude differences occur despite similar trends.\n  • Provide a consolidated table summarising which metrics were computed on full vs partial data per dataset (Appendix D) and discuss implications for cross-dataset comparisons (Section 6).\n- Temper novelty claims and situate findings within related work more precisely\n  • Rephrase the “first to document” claim (Section 6) to “we systematically show,” unless a literature review confirms priority; add a brief related-work paragraph analysing existing evidence (e.g., Wen et al., 2023) and how this work differs.\n  • Clearly state the added value: reparameterisation-invariant metrics, breadth of architectures/datasets, matched-seed design, and inclusion of safety metrics (Sections 3, 5–6; Figures 1, 11).\n  • Link claims to specific empirical anchors (Tables 3, 5, 7; Table 8) to keep evidence-first phrasing.\n- Make augmentation protocols consistent and discuss their impact on convergence\n  • Harmonise descriptions: ensure Section 5.1 and Appendix D match for CIFAR augmentation (Random Crop + Horizontal Flip vs “rotation and crop”), and clarify TinyImageNet differences (Appendix D).\n  • Discuss how augmentation alters training loss/accuracy and consequently the generalisation gap (Appendix D), and consider reporting train accuracy for augmented conditions explicitly (Tables 3, 5, 7).\n  • If augmentation reduces train convergence (“approximately converge”), add supplementary runs or calibrate training schedules to balance convergence across conditions; alternatively, introduce a normalised gap metric.\n- Clarify the “Functional Diversity” metric and its interpretation\n  • Rename the section/metric to “Functional Agreement” or explicitly state that “functional diversity” here is operationalised via disagreement where lower values are interpreted as stability (Section 3).\n  • Specify computation: pairwise disagreement across all seed pairs vs reference model, and the aggregation method to obtain the single table value (Tables 3, 5, 7; Appendices G–H).\n  • Add analyses linking prediction disagreement to calibration/robustness (Section 6) to justify why agreement is a safety property; if possible, compare with ensemble diversity metrics from Fort et al. (2020) or Lu et al. (2024) as referenced.\n- Test the causal link between regularisation and learned function complexity\n  • Measure complexity proxies (margins, Lipschitz constants, path norms, representation entropy/convergence) per condition and test whether regularisation shifts these (Sections 5–6; Tables 3, 5, 7).\n  • Use controlled ablations (e.g., vary weight decay strength, augmentation intensity, SAM ρ in Appendix E) and report whether the measured complexity tracks sharpness and safety metrics (Table 8; Figure 10).\n  • Where feasible, perform mediation analysis: does complexity mediate the effect of regularisation on sharpness and safety metrics? This strengthens the causal interpretation (Section 6).\n- Correct mislabeling and ensure consistent highlighting of best values in Table 7\n  • Update Table 7 to bold Weight Decay+SAM for Test Accuracy (0.539 ± 0.001) and Prediction Disagreement (0.339 ± 0.000) and ensure consistency with the caption’s “best per metric” instruction (Table 7).\n  • Apply consistent bolding to sharpness minima (Fisher–Rao Norm, SAM-sharpness) if the caption indicates best per metric; clarify whether “best” for sharpness refers to minima (Table 7).\n  • Re-check cross-text references to TinyImageNet “best” results to avoid misinterpretation based on the current misbolding (Section 6; Table 7).\n- Specify the SAM-sharpness ρ used for metric computation across all controls\n  • Explicitly state the ρ used to compute SAM-sharpness for each control, including models trained without SAM (Appendix B; Tables 3, 5, 7; Table 8).\n  • Indicate whether a single metric ρ (e.g., 0.05) is used uniformly for SAM-sharpness evaluation across all controls to ensure comparability, independent of training ρ (Appendix B; Appendix E).\n  • If different metric ρ values are used, provide a rationale and a sensitivity analysis showing the effect on SAM-sharpness across controls (Appendix B; Appendix E).Score\n- Overall (10): 6 — Strong empirical breadth with matched seeds and multiple architectures/datasets (Sections 5–6; Tables 3, 5, 7; Appendices G–H), but key claims hinge on unmeasured “function complexity,” unclear generalisation gap definition, and robustness conflation (Section 1; Section 3; Appendix D; Appendix H.1).\n- Novelty (10): 6 — Function-centric framing and systematic evidence that regularisation can increase sharpness while improving performance (Tables 3, 5, 7; Table 8) are useful, but the “first to document” claim (Section 6) is overstated and the conceptual link to complexity is asserted rather than formalised.\n- Technical Quality (10): 6 — Reparameterisation-invariant metrics, SEM over matched seeds, and hyperparameter sweeps (Sections 3, 5–6; Appendix E; Appendix F.2) are solid, yet metric computation inconsistencies (Appendix D; Appendix F.1), lack of adversarial robustness, and unclear generalisation gap reduce rigor.\n- Clarity (10): 7 — The paper is generally well written with detailed tables and appendices (Sections 5–6; Appendices D–H), but augmentation inconsistencies (Section 5.1 vs Appendix D), ambiguous gap definition, and functional diversity terminology (Section 3) hinder clarity.\n- Confidence (5): 4 — High confidence in the empirical observations due to extensive tabulated results and matched-seed design (Tables 3, 5, 7; Appendix E; Appendices G–H), lower confidence in causal and robustness claims due to missing formal complexity measures and adversarial tests (Section 1; Section 3; Appendix D)."
}