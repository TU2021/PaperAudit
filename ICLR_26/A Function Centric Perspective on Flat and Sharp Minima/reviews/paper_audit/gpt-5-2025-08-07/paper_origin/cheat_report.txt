Academic integrity and internal consistency risk report

Summary of high-impact issues identified

1) Undefined and inconsistently used “generalisation gap”
- Evidence: The paper reports a “Generalisation Gap” throughout results tables (e.g., Table 3, Table 5, Table 7, Table 12–15, Table 16–19, Table 22, Table 25–26), but no formal definition is provided in Section 3 (Method) or anywhere else. Section 3 defines sharpness, calibration, functional diversity, and robustness, but not generalisation gap. No direct evidence found in the manuscript of a precise formula or unit.
- Consequence: Reported values include negative gaps (e.g., ViT on CIFAR-10: Table 25 shows −1.199 ± 0.097), which is unusual under common definitions (train performance surpassing test) and prevents verification and consistent interpretation of claims (e.g., Figures 1 and 10 plot “generalisation gap” without a formal definition).
- Impact: Central to the paper’s conclusions; the lack of definition materially affects correctness and interpretability of all “gap”-related analyses and plots.

2) Contradiction between augmentation description and actual implementation
- Evidence: Section 5.1 (Experimental Setup) states augmentation is “random rotation and crop” (Block #24). Training details in Appendix D specify Random Crop + Random Horizontal Flip for CIFAR-10 and CIFAR-100 (Blocks #70–#72), and Random Resized Crop + Random Horizontal Flip for TinyImageNet (Block #73), with no rotation.
- Consequence: The implemented augmentation differs from the described protocol, undermining reproducibility and the comparability claims tied to the augmentation control.
- Impact: Affects interpretation of “augmentation increases sharpness and improves evaluations” since the augmentation type is not consistently or correctly reported.

3) Sharpness calculation on augmented vs standard datasets: claimed equivalence contradicted by SAM-sharpness results and trend reversal
- Evidence: Section F.1 claims “calculating sharpness with augmented data is nearly identical to using the standard dataset” and that trends are preserved (Block #78). However, Tables 9 and 10 show large discrepancies for SAM Sharpness:
  - CIFAR-10, ResNet-18, Augmentation: 1.905E−01 ± 2.203E−02 (Table 9) vs 1.591E−02 ± 1.609E−03 (Table 10).
  - Augmentation+SAM: 1.303E−01 ± 1.547E−02 (Table 9) vs 2.035E−02 ± 1.203E−03 (Table 10).
  These differ by roughly an order of magnitude. Moreover, the relative ordering flips: with augmented data, Aug+SAM has lower SAM-sharpness than Aug; with standard data, Aug+SAM has higher SAM-sharpness than Aug.
- Consequence: The assertion that dataset choice does not materially impact sharpness estimates is not supported for SAM-sharpness; trend reversals suggest comparisons across controls may be confounded by the dataset used in metric computation.
- Impact: Undermines key comparability claims and the robustness of sharpness conclusions under augmentation.

4) Mislabeling of “best” values and inconsistent bolding in Table 7 (TinyImageNet)
- Evidence: Table 7 caption states “Bolded values indicate the best performance per metric.” The table bolds Augmentation+SAM for Test Accuracy (0.520) and Prediction Disagreement (0.514), but Weight Decay+SAM has higher accuracy (0.539) and lower disagreement (0.339), which should be the bolded “best” values (Block #34). Additionally, sharpness columns in Table 7 are not bolded for their minima despite the caption’s instruction; other tables (e.g., Table 24) do bold minima for sharpness.
- Consequence: Mislabeling best values and inconsistent bolding create factual mismatches between table content and reported highlights.
- Impact: Directly affects readers’ interpretation of which control is “best,” weakening the evidentiary support for main claims.

5) “Matched seeds” comparability claim is invalid under augmentation
- Evidence: Section 5.1 claims all controls share identical initialisation and data order so “could traverse to (and even reach) the same minima” (Block #24). Under augmentation, the effective training data distribution changes stochastically across epochs and controls (Appendix D, Blocks #70–#74), and the paper itself computes sharpness on augmented datasets for augmentation controls (Blocks #70, #72, #74), acknowledging different sampling. Section F.1’s large differences in SAM-sharpness between augmented and standard datasets further show dataset choice matters (Tables 9–10).
- Consequence: The assumption of comparable minima across controls is not valid when the loss surfaces differ due to augmentation; this affects all geometric comparisons intended to rely on matched seeds.
- Impact: Core to the paper’s methodology; undermines claims about geometry-function relationships across controls.

6) Ambiguity in SAM-sharpness metric definition regarding ρ for models not trained with SAM
- Evidence: Appendix B defines SAM-sharpness (Eq. 10) using “0.005ρ” offsets and division by ρ (Block #65), but does not specify which ρ is used to compute the metric for non-SAM models or across differing SAM training ρ values. Yet baseline (ρ=0 in training) has finite SAM-sharpness values (e.g., Table 3: 1.366E−05 ± 1.206E−06; Table 8: 1.366E−05 ± 1.206E−06), implying a nonzero ρ was used for the metric. No direct evidence found in the manuscript clarifying this.
- Consequence: Without a clear, consistent metric ρ, SAM-sharpness comparisons across controls are ambiguous and potentially non-equivalent.
- Impact: Affects all conclusions relying on SAM-sharpness, especially those about regularisation increasing sharpness.

7) Overstated cross-condition claim about Augmentation+SAM being best “across evaluations”
- Evidence: Section 6 (“SAM Does Not Always Flatten”) states: “Notably, Augmentation+SAM achieves the best performance across evaluations while also being the sharpest model” (Block #30), referencing Tables 3, 5, 7.
  - CIFAR-10 (Table 3): Augmentation+SAM is indeed best across listed evaluations.
  - CIFAR-100 (Table 5): Weight Decay has best ECE (0.099 ± 0.005); Augmentation+SAM does not achieve best calibration.
  - TinyImageNet (Table 7): Weight Decay+SAM has best accuracy (0.539 ± 0.001) and disagreement (0.339 ± 0.000); Augmentation+SAM does not achieve best performance across evaluations.
- Consequence: The statement is not supported “across” the referenced datasets; it only holds for CIFAR-10.
- Impact: Misleading generalisation that weakens the central narrative.

Additional missing or ambiguous details that affect trustworthiness

- Prediction Disagreement aggregation: Section 3 states it is “the disagreement between the top-1 predictions of two models” (Block #69), but does not specify how it is aggregated across the 10 seeds (pairwise average? vs. baseline? across controls?). No direct evidence found in the manuscript clarifying this.
- ECE computation specifics: Appendix C states TorchMetrics Multiclass Calibration Error is used (Block #69) but omits key settings (e.g., number of bins), which materially affect ECE values. No direct evidence found in the manuscript of bin count or calibration protocol specifics.
- TinyImageNet fine-tuning scope: Appendix D (Block #73) mentions replacing the final classification layer in pre-trained models, but does not specify whether the entire network is fine-tuned or only the last layer. No direct evidence found in the manuscript clarifying parameter freezing, which affects comparability and sharpness.

Conclusion

The manuscript exhibits several substantive internal inconsistencies and missing methodological details that materially affect its scientific correctness and the trustworthiness of its conclusions. The most critical issues are the undefined generalisation gap, contradictory augmentation description vs. implementation, the non-equivalence of sharpness computations on augmented vs. standard data (particularly for SAM-sharpness, including trend reversals), mislabeling of “best” values in Table 7, and ambiguity in SAM-sharpness’s use of ρ across controls. These should be resolved to support the paper’s central claims. If addressed, the empirical narrative would be clearer and more defensible.