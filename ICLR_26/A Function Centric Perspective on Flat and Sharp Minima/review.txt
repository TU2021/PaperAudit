### Summary

This paper revisits the commonly cited connection between flat minima and generalization, arguing for a **function-centric** interpretation: sharpness is better viewed as a property induced by the **learned function / task complexity and training interventions**, rather than a universal proxy for poor generalization. The authors support this thesis with a broad empirical study spanning (i) **toy regression / single-objective function fitting** and (ii) modern vision benchmarks (**CIFAR-10/100, TinyImageNet**) across multiple architectures (e.g., ResNet/VGG/ViT). They compare standard training against widely used regularizers—**weight decay, data augmentation, and SAM**—and report that, under their chosen (largely reparameterization-aware) sharpness measures (e.g., **Relative Flatness, Fisher–Rao norm, SAM-sharpness**), **regularization often produces sharper minima while improving accuracy and “safety” metrics** such as calibration (ECE), corruption robustness, and prediction consistency. Reviewers generally agree the topic is important and the empirical scope is substantial, but they raise significant concerns about novelty, conceptual clarity, and the rigor/interpretability of cross-setting sharpness comparisons.

### Strengths

* **Timely and Relevant Research Question:** Understanding when sharpness/flatness correlates with generalization remains an active debate, and a function-centric framing is considered interesting by multiple reviewers.
* **Broad Empirical Coverage Across Tasks and Architectures:** Reviewers acknowledge the breadth of experiments (toy settings + multiple real-world datasets; multiple architectures), and some find the results largely correct.
* **Beyond Accuracy: Inclusion of Calibration/Robustness/Consistency Metrics:** The use of ECE, corruption robustness, and prediction disagreement as additional evaluation axes is viewed as meaningful for deployment-relevant behavior.
* **Potentially Insightful Empirical Observation:** Several reviewers find it noteworthy (and counterintuitive relative to some narratives) that common regularizers can lead to **increased sharpness** while simultaneously improving generalization and safety-related metrics.

### Weaknesses

* **Limited Novelty / Unclear Increment Over Prior Work:** At least one reviewer argues the headline message—flatness alone is not a reliable generalization indicator—is not new in light of reparameterization critiques and subsequent literature. The paper is seen as needing a crisper articulation of what is *truly novel* beyond reframing and scale.
* **Ambiguous Definition of “Function Complexity”:** Multiple reviewers find the notion of function-space complexity insufficiently defined and potentially conflated with downstream performance metrics (accuracy, ECE, robustness). This creates the impression that the takeaway may reduce to “adding regularization improves metrics,” rather than establishing a principled complexity–geometry relationship.
* **Experimental Clarity Issues in the Toy/Section 4 Setup:** A key critique is that the “objective complexity determines sharpness” narrative is potentially misleading: in the toy regression experiments the optimized surface is the **MSE loss landscape**, while the known functions define the *target function* to be fitted, and this distinction is not always clearly communicated.
* **Assumption Mismatches for Flatness Metrics:** Reviewers emphasize that certain metrics (e.g., Relative Flatness) rely on assumptions such as **locally constant labels**, which may not hold in regression/high-curvature toy settings; thus, conclusions drawn from those settings require careful qualification.
* **Cross-Setting Comparability and Confounding:** Reviewers question whether sharpness values are meaningfully comparable across heterogeneous interventions (augmentation vs no augmentation; weight decay affecting norms), since these change the loss surface/scale. There are explicit concerns about confounding and **Simpson’s paradox**, suggesting that aggregated trends may invert when conditioning on training method.
* **SAM Interpretation Tension:** Reviewers note an apparent contradiction with common SAM framing (e.g., reduced Hessian-based sharpness in the SAM paper) and request clearer explanation of why SAM yields “sharper” solutions under the paper’s chosen metrics, as well as clearer alignment between what SAM optimizes and what “SAM-sharpness” measures.
* **Presentation / Completeness / Proofreading:** Reported issues include missing formal definitions for some appendix metrics, inconsistent robustness terminology (adversarial vs average-case corruption/natural noise), questions about baseline configuration (potentially under-tuned), and several concrete typos/citation/link errors.
