{
  "paper": "TowerVision _ Understanding and Improving Multilinguality in Vision-Language Models",
  "runs": [
    {
      "config": {
        "judge_model": "gpt-5-2025-08-07",
        "review_agent": "paper_audit",
        "ai_model": "gpt-5-2025-08-07",
        "baseline_file": "baseline_review.txt",
        "final_file": "final_review_all.txt",
        "temperature": 0.0,
        "max_tokens": 20000,
        "timeout_s": 600,
        "metric": "regression_v2_minimal",
        "prompt_hash": "2142c04332"
      },
      "config_key": "1a46f14f5902ebb0fc2c14773cec6644d2fa5338",
      "inputs": {
        "baseline_review": "reviews/paper_audit/gpt-5-2025-08-07/paper_origin/baseline_review.txt",
        "final_review": "reviews/paper_audit/gpt-5-2025-08-07/paper_origin/final_review_all.txt"
      },
      "regression": {
        "score_delta": {
          "baseline_score": 7.0,
          "final_score": 7.0,
          "delta": 0.0,
          "scale_hint": "1-10"
        },
        "differences": [
          {
            "diff_type": "new_critique",
            "summary": "Architectural ambiguity about SigLIP2 tokens/embeddings ('size 729')",
            "paperaudit_types": [
              "METHOD_LOGIC_CONSISTENCY",
              "EXPERIMENTAL_DESIGN_PROTOCOL",
              "CONTEXT_MISALIGNMENT_INCOHERENCE"
            ],
            "why_impacts_score": "Hinders reproducibility and clarity; contributes to lower clarity assessment",
            "evidence": {
              "baseline_quote": "§2.2 describes stages but omits optimizer, batch size, LR schedules, durations, and hardware specifics.",
              "final_quote": "§2.2: SigLIP2 ‘embeddings of size 729’ with 384×384, patch 14; token size is ambiguous."
            }
          },
          {
            "diff_type": "new_critique",
            "summary": "Inconsistent CC-OCR labeling and mischaracterized TextVQA task",
            "paperaudit_types": [
              "CLAIM_RESULT_DISTORTION",
              "RHETORICAL_PRESENTATION_MANIPULATION",
              "CONTEXT_MISALIGNMENT_INCOHERENCE"
            ],
            "why_impacts_score": "Mislabels/descriptions can mislead readers and reduce perceived clarity and rigor",
            "evidence": {
              "baseline_quote": "Table 5 (‘2B En’, ‘2B Multi’) does not specify which benchmark or metric the numbers represent.",
              "final_quote": "Table 1 labels ‘English CC-OCR’; §3.1 mischaracterizes TextVQA as ‘assessing scientific understanding’."
            }
          },
          {
            "diff_type": "intensified_critique",
            "summary": "Comparative evaluation naming/omission inconsistencies (LLaVA-Next mismatch)",
            "paperaudit_types": [
              "CLAIM_RESULT_DISTORTION",
              "CONTEXT_MISALIGNMENT_INCOHERENCE",
              "EXPERIMENTAL_DESIGN_PROTOCOL"
            ],
            "why_impacts_score": "Confusing baselines and omissions undermine fairness and credibility of comparisons",
            "evidence": {
              "baseline_quote": "Baselines exclude models listed in Appendix; no rationale provided for omissions.",
              "final_quote": "‘Llama3-Llava-Next-8B’ in §3.2 corresponds to a 7B checkpoint; no rationale for naming/omissions."
            }
          },
          {
            "diff_type": "intensified_critique",
            "summary": "OCR data coverage critique strengthened with lower per-subset counts",
            "paperaudit_types": [
              "EVIDENCE_DATA_INTEGRITY",
              "EXPERIMENTAL_DESIGN_PROTOCOL"
            ],
            "why_impacts_score": "Smaller OCR data suggests weaker training coverage, worsening OCR performance concerns",
            "evidence": {
              "baseline_quote": "DocVQA 47K, ST-VQA 17K, TextVQA 16K; Table 8.",
              "final_quote": "DocVQA 9.7K, ST-VQA 17.2K, TextVQA 15.7K; Table 8."
            }
          },
          {
            "diff_type": "score_rationale_shift",
            "summary": "Clarity sub-score lowered due to added reporting/label inconsistencies",
            "paperaudit_types": [
              "RHETORICAL_PRESENTATION_MANIPULATION",
              "CONTEXT_MISALIGNMENT_INCOHERENCE",
              "METHOD_LOGIC_CONSISTENCY"
            ],
            "why_impacts_score": "Explicitly reduces the clarity score from 7 to 6",
            "evidence": {
              "baseline_quote": "Clarity (10): 7 — clear pipeline, but some tables lack definitions; per-language distributions are absent.",
              "final_quote": "Clarity (10): 6 — clear pipeline, but labels/descriptions lack precision; ‘729’ tokens ambiguity persists."
            }
          }
        ]
      },
      "generated_at": "2026-01-05T20:37:40"
    }
  ]
}