# Innovation & Motivation Report

## 1. The Authors' Claimed Contribution
- Core Problem: The paper targets the English-centric design of vision-language models (VLMs), which limits effectiveness across languages and culturally grounded tasks, and investigates where and how to inject multilinguality (data composition, encoder choice, and text backbones) in image- and video-text systems.
- Claimed Gap: “Despite significant advances in vision-language models (VLMs), most existing work follows an English-centric design process, limiting their effectiveness in multilingual settings.” (Abstract) and “VLMs predominantly built and trained in English; high-quality multilingual vision-text data scarce despite availability of text-only multilingual corpora.” (Introduction)
- Proposed Solution: A multi-stage training pipeline building TOWERVISION (image-text) and TOWERVIDEO (video-text) atop a multilingual text-only backbone (TOWER+), coupled with a curated multilingual multimodal dataset (VISIONBLOCKS). Key empirical choices include English-only caption alignment for the projector, multilingual finetuning for vision and video, high-dynamic-resolution tiling with SigLIP2, and ablations on backbones and encoders. The authors emphasize: “Our findings highlight that multilingual vision-language training data substantially improves cross-lingual generalization -- both from high-resource to underrepresented languages and vice versa -- and that instruction-tuned LLMs are not always the optimal initialization point.” (Abstract) and report that “English-only captions suffice for projector alignment” (Ablations).

## 2. Comparative Scrutiny (The "Trial")
*Here, we analyze how the paper stands against the identified similar works.*

### vs. An Exploration of Data Augmentation and Sampling Techniques for Domain-Agnostic Question Answering (Longpre et al., 2019)
- Identified Overlap: Both adopt a data-centric philosophy: translation/synthetic augmentation and careful mixture/sampling on top of a strong pretrained backbone to improve generalization under distribution shift.
- Manuscript’s Defense: The manuscript does not cite this specific work in the provided text. It differentiates its scope by focusing on multilingual and multimodal generalization rather than domain-agnostic textual QA, and by releasing a curated multimodal dataset and conducting cross-modal ablations. Relevant claims include: “By incorporating visual and cultural context during fine-tuning, our models surpass existing approaches trained on substantially larger datasets...” (Abstract) and “Multilingual vision-language training data substantially improves cross-lingual generalization...” (Abstract).
- Reviewer's Assessment: The methodological echo (augmentation and sampling atop a pretrained model) is generic and widely used. The substantive difference is the multilingual, multimodal extension (image/video), the new dataset (VISIONBLOCKS), and the staged fusion pipeline. This distinction is meaningful in application scope, but technically incremental relative to the data-centric approach.

### vs. Machine Translation Evaluation Meets Community Question Answering (Guzmán et al.)
- Identified Overlap: Repurposing MT evaluation signals outside pure MT. The manuscript uses COMETKiwi to filter translations and xCOMET to evaluate multimodal translation, and contrastive tasks (CoMMuTE), echoing B’s thesis that MT evaluation features improve decision quality.
- Manuscript’s Defense: The manuscript does not cite this specific work. It leverages MT evaluation metrics as data curation and evaluation tools in a multimodal, multilingual pipeline rather than embedding MTE features into a ranker. Explicit anchors: “Translate PixMo-Cap captions... filter with COMETKiwi score ≥ ‘0.85’.” (Dataset creation, Section 2.1) and reporting Multi30K xCOMET scores (Experiments).
- Reviewer's Assessment: The reuse of COMET-based signals for data quality and assessment is aligned but not novel by itself. The difference lies in multimodal conditioning (image/video), cultural tasks, and broad language coverage. This is an application-oriented extension with clear practical value.

### vs. Bottom-Up and Top-Down Attention for Image Captioning and VQA (Anderson et al.)
- Identified Overlap: Fine-grained visual representation and language-conditioned attention; using captioning as an alignment signal that transfers to VQA.
- Manuscript’s Defense: No explicit citation is visible in the provided text to this specific work. The manuscript operationalizes fine-grained image understanding through high-dynamic-resolution tiling and SigLIP2 embeddings, and uses caption prediction in Stage 1: “Projector pretraining: caption prediction on PixMo-Cap; vision encoder and LLM frozen; English captions only.” (Training procedure).
- Reviewer's Assessment: Conceptual alignment is strong, but the architectural choices (SigLIP2 + LLaVA-style MLP projector into an LLM) are standard in contemporary LLM-based VLMs. The novelty here is the multilingual extension, the staged English-only projector alignment, and extensive ablations; the attention/fusion paradigm itself is incremental relative to prior bottom-up/top-down approaches.

### vs. RUBi: Reducing Unimodal Biases in Visual Question Answering (Cadene et al.)
- Identified Overlap: Shared goal of mitigating reliance on language-only shortcuts by making visual evidence indispensable.
- Manuscript’s Defense: The manuscript does not cite RUBi. Its strategy is indirect: stronger visual pathways (SigLIP2), fine-grained tiling, and evaluations that require visual grounding (ALM-Bench, CoMMuTE). It also notes: “instruction-tuned LLMs are not always optimal prior to modality fusion.” (Abstract; Section 4), which implies controlling language dominance during alignment.
- Reviewer's Assessment: While philosophically aligned, the manuscript does not introduce a new anti-bias training objective akin to RUBi. Its contribution is primarily empirical—showing that multilingual multimodal finetuning and task choice increase reliance on visual signals. This is an application-level distinction, not a methodological advance over bias-reduction strategies.

### vs. Trivial Transfer Learning for Low-Resource NMT (Kocmi & Bojar)
- Identified Overlap: Parent-to-child transfer from high-resource training to low-resource targets; continued training on new corpora without specialized tricks.
- Manuscript’s Defense: Not cited. The authors’ staged pipeline closely mirrors this transfer principle: “English-only captions suffice for projector alignment” (Ablations) followed by multilingual finetuning across image/video tasks; and “expanding language coverage (10→20) yields consistent gains, especially at 2B scale.” (Ablations).
- Reviewer's Assessment: The paper effectively adapts trivial transfer to multimodal settings, demonstrating cross-script and low-resource gains (Table 11). This is a solid empirical extension but conceptually consistent with known transfer paradigms.

### vs. Second Place Solution of WSDM2023 Toloka VQA Challenge (Wu et al.)
- Identified Overlap: Staged training from a pretrained multimodal foundation, extensive synthetic data augmentation, specialization to visual grounding tasks.
- Manuscript’s Defense: Not cited. Distinctions include multilingual and video extensions, broader task coverage (cultural VQA, multimodal MT), and open releases. The manuscript’s three-stage design: projector alignment, full vision finetuning on VISIONBLOCKS, and video finetuning on translated LLaVA-Video-178k (Training procedure).
- Reviewer's Assessment: Methodological staging and synthetic augmentation are shared. Extending to multilingual and video and the systematic ablations are the principal novel aspects; the staging itself is not new.

### vs. TowerVision: Understanding and Improving Multilinguality in Vision-Language Models
- Identified Overlap: The title and abstract match the manuscript under review.
- Manuscript’s Defense: This entry appears to be the same paper; thus, it is not a distinct prior work.
- Reviewer's Assessment: No novelty inference can be drawn from this “similar work” beyond confirming the manuscript’s stated scope.

## 3. Novelty Verdict
- Innovation Type: Application-Oriented (with incremental architectural choices and substantive empirical analysis)
- Assessment:
  The manuscript’s motivation—addressing English-centric VLMs and scarce multilingual multimodal data—is clearly articulated and well-supported by empirical design and ablations. Its distinctiveness lies in:
  - A comprehensive, multilingual, multimodal pipeline (image and video) built on TOWER+, with systematic analysis of multilingual levers (data composition, encoder choice, backbone selection).
  - Release of a large curated dataset (VISIONBLOCKS) and training recipes, and empirical findings such as “English-only captions suffice for projector alignment,” “instruction-tuned LLMs are not always optimal prior to modality fusion,” and measurable gains from expanding language coverage.
  Against similar works, the architecture and training paradigm (LLaVA-style projector into a pretrained encoder/backbone, attention-based fusion, staged finetuning, augmentation/translation) are evolutionary rather than groundbreaking. The paper survives comparison by shifting the focus decisively to multilingual, culturally grounded tasks and by offering thorough ablations and open resources; however, it does not introduce new algorithms or theoretical advances.
  - Strength:
    - Clear problem framing with concrete multilingual/cultural benchmarks; strong results in cultural VQA and multimodal translation.
    - Systematic ablations isolating where multilinguality matters (backbone choice, encoder multilinguality, projector alignment language, language coverage).
    - Public release of models/data/recipes increases significance and reproducibility.
  - Weakness:
    - Core fusion design and training stages are consistent with prevailing LLaVA-style practices; limited architectural novelty.
    - Heavy reliance on translated/synthetic data; evaluation of open-ended video answers uses GPT-4o, which may complicate reproducibility and fairness.
    - Less competitive in OCR-centric tasks; hardware/budget details are not specified.

## 4. Key Evidence Anchors
- Abstract: “most existing work follows an English-centric design process, limiting their effectiveness in multilingual settings.”
- Abstract: “Our findings highlight that multilingual vision-language training data substantially improves cross-lingual generalization -- both from high-resource to underrepresented languages and vice versa -- and that instruction-tuned LLMs are not always the optimal initialization point.”
- Introduction: “VLMs predominantly built and trained in English; high-quality multilingual vision-text data scarce despite availability of text-only multilingual corpora.”
- Ablations (Section 4; Tables 4–6, 11):
  - Backbones: TOWER+ backbones outperform GEMMA2 (Table 4).
  - Vision encoders: SigLIP2 advantages in low-data regimes (Table 5).
  - Projector alignment: “English-only captions suffice for projector alignment” (Table 6).
  - Language coverage: “expanding language coverage (10→20) yields consistent gains, especially at 2B scale” with detailed per-language improvements (Table 11).
- Dataset creation (Section 2.1): Use of COMETKiwi ≥ “0.85” for translation filtering; synthetic captions via Gemini 2.5; multilingual composition of VISIONBLOCKS (Appendix Table 8).
- Training procedure (Section 2.2): Three-stage pipeline (projector pretraining on English captions; multilingual vision finetuning; multilingual video finetuning).
- Results (Section 3; Tables 1–3, 7): Competitive ALM-Bench scores; SOTA xCOMET on Multi30K; ViMUL-Bench multilingual video QA; English-only projector alignment sufficiency evidenced by ablations.