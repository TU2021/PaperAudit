Summary
- The paper proposes TowerVision, multilingual vision-language models for images and video built on the multilingual text backbone Tower+. The system combines a multilingual-aware vision encoder (SigLIP2), an MLP projector, and a multi-stage pipeline: projector pretraining on English captions, image-text finetuning on the curated multilingual VisionBlocks dataset, and video finetuning to obtain TowerVideo (§2.2). Contributions include VisionBlocks (6.3M samples; Figure 2, Table 8), ablations over backbones and instruction tuning (Table 4), vision encoders (Table 5), projector-caption language (Table 6), and language coverage (Figure 3, Table 11). Results show strong cultural understanding (ALM-Bench; Table 1), competitive multimodal translation (Multi30K, CoMMuTE; Table 2), and improved video accuracy through multilingual finetuning (ViMUL-Bench; Tables 3, 7), with 32-frame efficiency trade-offs (§3.3).Strengths
- Bolded: Comprehensive multilingual dataset curation (VisionBlocks)
  - Evidence: Figure 2 and Table 8 detail 6.3M samples spanning chart/plot, VQA, OCR, knowledge, cultural, counting/math, vision-text, and video-text, with composition tags for Public, Translated, and Synthetic; the pie charts indicate 63% English vs 37% multilingual (§2.1, Figure 2, Table 8). Why it matters: Demonstrates thoughtful, large-scale data aggregation with multilingual coverage, a key enabler for cross-lingual VLMs (novelty/impact).
  - Evidence: Translated PixMo-Cap captions filtered via COMETKiwi with a threshold of 0.85, plus diverse language-specific prompt templates and Gemini 2.5 synthetic caption augmentation (§2.1; A.5.1; A.5.2). Why it matters: Quality-controlled translations and diverse synthetic supervision support robust alignment and multilingual generalization (technical soundness).
  - Evidence: Inclusion of EuroBlocks-SFT text-only data (~20%) to preserve text capabilities during multimodal SFT (§2.1; Table 8). Why it matters: Addresses common regression in text-only skills when finetuning multimodally (experimental design/rigor).
- Bolded: Clear, modular multi-stage training pipeline
  - Evidence: Three-stage recipe—projector pretraining on PixMo-Cap (English only), image-text finetuning with high-dynamic-resolution tiling (up to 6 tiles), and video finetuning with 32 frames (§2.2). Why it matters: Transparent staging helps isolate where multilinguality is most beneficial and informs best practices (clarity/replicability).
  - Evidence: Use of SigLIP2-so400m/14@384px and LLaVA-style MLP projector, with frozen encoder/backbone during projector training (§2.2). Why it matters: Reasoned architectural choices aligned with prior high-performing VLMs while focusing on multilingual encoders (technical grounding).
  - Evidence: Custom fork of LLaVA-Next with public repository link (§2.2). Why it matters: Improves reproducibility and community uptake (impact/clarity).
- Bolded: Strong performance on culturally grounded tasks
  - Evidence: TowerVision-9B attains top ALM-Bench accuracy among listed open baselines (83.6 en; 85.2 multi), surpassing Qwen2.5-VL-7B (83.1 en; 83.6 multi) and Gemma3-12B-it (83.5 en; 84.5 multi) (Table 1); Figure 1 qualitative examples illustrate culturally grounded reasoning. Why it matters: Validates the claimed focus on cultural multilinguality (impact/experimental rigor).
  - Evidence: TowerVision models show enhanced cross-lingual generalization across 23 languages (Table 1; §3.3). Why it matters: Demonstrates effectiveness beyond English-centric settings (impact).
  - Evidence: Ablations attribute gains to multilingual backbone (Tower+), multilingual-aware vision encoder, and expanded language coverage (Table 4; Table 5; Figure 3; Table 11). Why it matters: Evidence-backed design choices (technical soundness).
- Bolded: Competitive multimodal translation performance
  - Evidence: TowerVision-9B achieves top xCOMET on Multi30K en→de (98.1), en→fr (95.6), en→cs (95.1), outperforming Qwen2.5-VL-7B (97.1/93.2/83.9) and CulturalPangea-7B (95.8/92.1/80.0) (Table 2). Why it matters: Indicates strong translation grounding with visual disambiguation (impact).
  - Evidence: CoMMuTE pairwise accuracy is competitive (e.g., en→fr 78.8 for 9B), close to Gemma3-4B-it 78.2 and Qwen2.5-VL-7B 76.9 (Table 2). Why it matters: Suggests solid multimodal disambiguation capabilities (rigor).
- Bolded: Systematic, evidence-based ablation study
  - Evidence: Backbone multilinguality and instruction tuning (Table 4) show Tower+it-9B consistently best on ALM-Bench and CC-OCR; instruction-tuned LLMs are not always optimal at smaller scales (2B). Why it matters: Nuanced insights guide initialization choices (novelty/technical soundness).
  - Evidence: Encoder comparison (SigLIP2 vs SigLIP1) across data regimes and scales (Table 5) highlights benefits of multilingual-aware encoders in low-data settings. Why it matters: Clarifies when encoder multilinguality is most impactful (novelty).
  - Evidence: Projector pretraining language study (Table 6) shows English-only captions suffice for alignment, with minimal gains from multilingual captions. Why it matters: Practical guidance on where to invest multilingual supervision (impact).
- Bolded: Extension to video with multilingual finetuning
  - Evidence: TowerVideo improves over TowerVision on video tasks, and adding multilingual video-text yields further gains (Table 3; Table 7). Why it matters: Demonstrates recipe portability and utility in video modality (impact).
  - Evidence: Clear reporting of frames (32) and efficiency trade-offs compared to VideoLLaMA3 (180 frames) (§3.3). Why it matters: Fairness and transparency in comparisons (clarity).
  - Evidence: Per-language breakdowns on ViMUL-Bench for 14 languages (Table 3; Table 7). Why it matters: Fine-grained multilingual analysis (experimental rigor).
- Bolded: Commitment to openness and reproducibility
  - Evidence: Public release of models, dataset, and training recipes is stated (Abstract; §8 Reproducibility); codebase link provided (§2.2). Why it matters: Enables community validation and reuse (impact).
  - Evidence: Appendix includes prompts used for translation and synthetic data generation (A.5.1; A.5.2). Why it matters: Transparency in data creation (clarity).
  - Evidence: Detailed dataset composition in Table 8 and baseline checkpoint references (Table 9). Why it matters: Facilitates replication of evaluation (experimental rigor).Weaknesses
- Bolded: Limited OCR performance and insufficient OCR-focused training data
  - Evidence: TowerVision-9B OCRBench 69.7 and CC-OCR 56.3 lag behind Qwen2.5-VL-7B (84.5, 78.6) (Table 1); authors acknowledge lower OCR competitiveness (§3.3). Why it matters: Undermines utility in document-heavy and literacy tasks (impact/technical coverage).
  - Evidence: VisionBlocks OCR-relevant subsets are modest relative to the overall dataset (DocVQA 9.7K, ST-VQA 17.2K, TextVQA 15.7K; Table 8), supporting the hypothesis of limited OCR-specific data (§3.3). Why it matters: Data imbalance likely contributes to deficits (experimental design).
  - Evidence: Encoder configuration ablation (Table 10) shows relatively low TextVQA/OCRBench scores under several settings, suggesting current recipe does not sufficiently address OCR. Why it matters: Indicates need for targeted architectural/training adjustments (technical soundness).
- Bolded: Dependence on proprietary systems for data creation and evaluation
  - Evidence: Synthetic captions generated via Gemini 2.5 API (A.5.2; §2.1) and use of Gemini 1.5 Pro in PangeaIns curation pipeline (cited within §2.1) introduce closed-model dependence. Why it matters: Limits strict reproducibility and may embed proprietary model biases (reproducibility/ethics).
  - Evidence: Video open-ended responses are judged with GPT-4o (Table 3; §3.3). Why it matters: Reliance on a proprietary judge may affect comparability and transparency (evaluation rigor).
  - Evidence: Data release note “Released on Hugging Face upon acceptance” (§2.1) means key synthetic/translated resources are not yet accessible for verification. Why it matters: Reproducibility is deferred, and licensing constraints may apply (clarity/reproducibility).
- Bolded: Incomplete reporting of training details and compute
  - Evidence: §2.2 describes stages but omits optimizer, batch size, learning rate schedules, training durations, and compute hardware specifics; No direct evidence found in the manuscript. Why it matters: Hinders exact reproduction and assessment of efficiency (reproducibility).
  - Evidence: §8 Reproducibility states intent to release but does not provide current detailed hyperparameter tables or training logs; No direct evidence found in the manuscript. Why it matters: Without precise settings, results may be hard to replicate (clarity).
  - Evidence: Appendix A.3 discusses encoder variants but not full training budgets, data sampling strategies per language, or seed control; No direct evidence found in the manuscript. Why it matters: Missing design details complicate interpretation of ablation results (experimental rigor).
  - Evidence: §2.2 specifies SigLIP2-so400m/14@384px “produces … embeddings of size 729” while also stating 384×384 inputs and patch size 14; this token/embedding size is ambiguous without clarifying resize/padding or whether “729” refers to tokens, and Table 10 again uses 384×384 with patch 14 (Table 10; §2.2). Why it matters: Architectural description ambiguity impairs reproducibility and clarity (technical soundness/clarity).
- Bolded: Ambiguities and inconsistencies in ablation presentation
  - Evidence: Table 5 (“2B En”, “2B Multi”, etc.) does not specify which benchmark(s) or metric the numbers represent; No direct evidence found in the manuscript. Why it matters: Readers cannot interpret the magnitude or relevance of reported values (clarity).
  - Evidence: Narrative claims SigLIP2 consistently outperforms SigLIP1 “without additional multilingual data,” but Table 5 shows a reversal at 9B En (SigLIP1-En: 78.3 vs SigLIP2-En: 77.2). Why it matters: Unexplained exceptions weaken the stated conclusion (technical soundness).
  - Evidence: Figure 3 references “more details in §A.4,” yet §A.4 contains no methodological description; instead, Table 11 provides per-language gains without linking to Figure 3 axes and units. Why it matters: Missing methodological clarity reduces confidence in the language coverage analysis (clarity/rigor).
  - Evidence: Table 1 labels “English (↑) CC-OCR,” whereas §3.1 states CC-OCR is used for multilingual OCR capabilities and Table 4 correctly labels “Multilingual CC-OCR”; additionally, §3.1 mischaracterizes TextVQA as “assessing scientific understanding,” though it is an OCR-centric VQA dataset (Table 1; §3.1; Table 4). Why it matters: Inconsistent headers and task descriptions can mislead interpretation (clarity).
- Bolded: Insufficient transparency on per-language balance and translation quality for video
  - Evidence: Claims of uniform translation into supported languages for LLaVA-Video-178k (§2.1) lack per-language counts or sampling distributions; No direct evidence found in the manuscript. Why it matters: Without distributions, it is hard to attribute per-language performance (experimental rigor).
  - Evidence: Unlike PixMo-Cap translations filtered by COMETKiwi, video translations via Tower+9B (§2.1) do not report quality filtering or scoring. Why it matters: Unverified translations may introduce noise and uneven supervision (technical soundness).
  - Evidence: Figure 2/Table 8 provide category totals but not language-wise breakdowns for image and video subsets. Why it matters: Limits auditability of multilingual coverage and potential biases (clarity).
- Bolded: Comparative evaluation gaps and overstatements for video
  - Evidence: Table 3 shows TowerVideo-9B often trails VideoLLaMA3-7B (e.g., zh: 44.8 vs 48.0) and ViMUL-7B (ar: 38.6 vs 41.5), with TowerVision-2B markedly lower (e.g., ar: 18.9 vs 41.5). Why it matters: The “competitive” claim is uneven across languages (impact/rigor).
  - Evidence: Some languages show modest or negative gains from multilingual finetuning at 2B (Table 7: en 45.2 vs 45.5; ar 23.0 vs 25.7), indicating sensitivity to data quality/quantity. Why it matters: Suggests recipe may need further tuning for low-resource languages (technical soundness).
  - Evidence: Baseline selection in main tables excludes some models listed in Appendix (e.g., Pixtral-12B-2409 appears in Table 9 but not in Tables 1–3) and the “Llama3-Llava-Next-8B” baseline named in §3.2 corresponds to an LLaVA-Next 7B checkpoint in Table 9; no rationale is provided for these choices/omissions (Table 9; §3.2; Tables 1–3). Why it matters: Could bias or confuse the comparative picture (evaluation completeness/clarity).Suggestions for Improvement
- Bolded: Strengthen OCR capabilities through targeted data and modeling
  - Action: Increase OCR-focused training data proportion (e.g., expand DocVQA/ST-VQA/TextVQA subsets or add similar public OCR corpora) and report updated performance on OCRBench/CC-OCR (Table 1; Table 8). Verification: Re-run Table 1 with added OCR data and publish deltas.
  - Action: Explore text-aware modules (e.g., recognition heads or higher-resolution pipelines) and report ablations alongside Table 10 (resolution/tiles/patches). Verification: Add a new table mirroring Table 10 with document-centric images and show OCRBench gains.
  - Action: Conduct targeted finetuning on scanned/document-style inputs and include per-category CC-OCR analyses. Verification: Release per-subtask CC-OCR results to diagnose failure modes.
- Bolded: Reduce proprietary dependence and improve evaluation transparency
  - Action: Complement Gemini 2.5 synthetic captioning with open VLM generators referenced in the paper (e.g., LLaVA (Liu et al., 2023a/b), InstructBLIP (Dai et al., 2023)) and report side-by-side quality metrics. Verification: Provide a small benchmark comparing synthetic sources with COMETKiwi/xCOMET.
  - Action: Replace or augment GPT-4o judging with open evaluation pipelines (e.g., lmms-eval, Zhang et al., 2025b) and human evaluation on a subset for calibration; disclose prompts and judge agreement. Verification: Publish inter-judge agreement and sensitivity analysis for Table 3.
  - Action: Release the synthetic/translated datasets and document licenses and generation settings beyond “upon acceptance” (§2.1, §8). Verification: Provide dataset cards with provenance, licenses, and quality metrics.
- Bolded: Provide complete training and compute details
  - Action: Add a hyperparameter table (optimizers, LR schedules, batch sizes, gradient clipping, epochs, seeds) for each stage (§2.2) and for ablations (Tables 4–6, 10). Verification: Include in Appendix with versioned configs.
  - Action: Report compute budgets (GPU/TPU types, hours, memory footprints) and throughput, especially for tiling and video frames. Verification: Add a resource table enabling fair efficiency comparisons.
  - Action: Release training curves and logs (loss/accuracy per epoch, validation metrics), and seed-controlled runs to show variance. Verification: Provide mean±std over ≥3 seeds for key benchmarks.
  - Action: Clarify the SigLIP2 resolution/patch/token mapping and what “embeddings of size 729” denotes (§2.2; Table 10), including any resize/padding policy. Verification: Add an architectural note in §2.2 and cross-reference in Table 10.
- Bolded: Clarify ablation methodologies and resolve inconsistencies
  - Action: Explicitly define the metric and benchmark for Table 5 (“2B En/Multi; 9B En/Multi”) in the caption and text, and break results per benchmark where possible. Verification: Update Table 5 with labeled columns (e.g., ALM-Bench en/multi accuracy).
  - Action: Discuss and analyze the 9B En reversal where SigLIP1 beats SigLIP2 (Table 5), including potential data or optimization effects, with error bars. Verification: Add statistical intervals and an explanatory note in §4.
  - Action: Expand §A.4 with methodological details for Figure 3 (sampling, normalization, languages included, metric definitions) and tie each bar to entries in Table 11. Verification: Provide a mapping table linking Figure 3 bars to Table 11 values.
  - Action: Correct mislabeled headers/titles and task descriptions: fix Table 1 “English CC-OCR” to “Multilingual,” align with Table 4/§3.1, and update TextVQA description in §3.1 to reflect its OCR-centric nature. Verification: Revise table captions/headers and the §3.1 description.
- Bolded: Increase transparency on per-language balance and video translation quality
  - Action: Report per-language sample counts for image-text and video-text subsets (including translated vs original), ideally in an added table next to Table 8. Verification: Publish distributions and balance indicators (Gini/entropy).
  - Action: Apply translation quality filtering/scoring for video translations (e.g., COMETKiwi/xCOMET, both cited) and report acceptance thresholds and coverage. Verification: Provide quality histograms and pre/post-filter performance on ViMUL-Bench.
  - Action: Document sampling strategies (uniform vs temperature-based) and curriculum if any, especially for low-resource languages. Verification: Include sampling configs and compare balanced vs skewed training runs.
- Bolded: Strengthen video comparisons and claims of competitiveness
  - Action: Include additional baselines listed in Table 9 (e.g., Pixtral-12B-2409) in Tables 1–3 or justify omissions; also align baseline names/scales with actual checkpoints (e.g., LLaVA-Next naming in §3.2 vs Table 9), and ensure comparable frame counts or provide efficiency-normalized metrics. Verification: Reproduce expanded comparisons and report per-language averages.
  - Action: Report statistical significance (e.g., bootstrap CIs) on ViMUL-Bench per language for TowerVideo vs baselines (Table 3). Verification: Add confidence intervals to Table 3 and discuss significance.
  - Action: Analyze data-per-language vs performance correlations and failure cases, particularly where multilingual finetuning yields negative/limited gains (Table 7), and adjust data or loss weighting accordingly. Verification: Publish correlation plots and ablation of language-specific weights.Score
- Overall (10): 7 — Strong dataset curation and multilingual ablations (Figure 2; Table 8; Tables 4–6; Figure 3; Table 11) with competitive cultural and translation results (Tables 1–2), but OCR and some video benchmarks lag and some reporting inconsistencies remain (Table 1; Table 3; §3.1; §2.2).
- Novelty (10): 7 — Systematic study of where multilinguality matters (Tables 4–6; Figure 3; Table 11) and curated VisionBlocks are valuable (Figure 2; Table 8), though many components build on established recipes (§2.2).
- Technical Quality (10): 7 — Solid experiments and ablations with clear gains (Tables 1–2; Table 4; Table 6), tempered by reliance on proprietary systems (§2.1; §3.3) and some architectural ambiguity (§2.2; Table 10).
- Clarity (10): 6 — Clear high-level pipeline (§2.2) and dataset breakdown (Figure 2; Table 8), but some tables/labels/descriptions lack precision (Table 5 metric; Table 1 CC-OCR “English”; §3.1 TextVQA description; Figure 3/§A.4 linkage; §2.2 “729” tokens).
- Confidence (5): 4 — Good empirical breadth and anchors across figures/tables, but deferred releases (§2.1; §8) and incomplete reporting limit full verification at review time.