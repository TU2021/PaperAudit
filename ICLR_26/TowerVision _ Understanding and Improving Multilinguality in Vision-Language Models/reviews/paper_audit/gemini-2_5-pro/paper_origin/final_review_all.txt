1) Summary
This paper introduces TOWERVISION, a family of open multilingual vision-language models (VLMs) for both image and video tasks, built upon the TOWER+ multilingual text model. The authors conduct a comprehensive empirical study on various design choices for multilingual VLMs, including the text backbone, vision encoder, and data composition at different training stages. The core contributions include the TOWERVISION and TOWERVIDEO models, a new curated multilingual dataset named VISIONBLOCKS, and a detailed analysis of how different components affect multilingual performance. The resulting models demonstrate competitive or state-of-the-art performance on several multilingual benchmarks, particularly in tasks requiring cultural understanding (ALM-Bench) and multimodal translation (Multi30K), outperforming larger models in some cases. The study also provides insights, such as the benefits of multilingual backbones and the finding that high-quality English captions suffice for the initial alignment stage.2) Strengths
*   **Comprehensive and Systematic Ablation Studies**
    *   The paper provides a thorough investigation into the design choices for multilingual VLMs, which offers valuable insights for the community (Section 4). This systematic approach strengthens the paper's conclusions.
    *   The authors ablate the choice of the text backbone, comparing a general-purpose model (GEMMA2) with a multilingual-focused one (TOWER+), and also analyze the impact of instruction tuning (Table 4). This demonstrates that a strong multilingual backbone is crucial for cross-modal performance.
    *   The study examines the role of the vision encoder's multilingual capacity by comparing SigLIP1 (English-centric) with SigLIP2 (multilingual), showing that a multilingual-aware encoder is particularly beneficial in low-data regimes (Table 5).
    *   The work analyzes the impact of data composition at different training stages, such as showing that high-quality English-only captions are sufficient for projector pretraining (Table 6) and that broader language coverage during fine-tuning improves cross-lingual generalization (Figure 3, Table 11).*   **Valuable Public Release of Models and Data**
    *   The authors release a suite of open-source models (TOWERVISION-2B/9B, TOWERVIDEO-2B/9B) for 20 languages and dialects, which is a significant contribution to multilingual VLM research (Abstract, Section 1, Footnote 2).
    *   The paper introduces and releases VISIONBLOCKS, a large-scale (6M samples) curated dataset for vision-language tasks (Section 2.1, Figure 2). This resource aggregates existing datasets and enriches them with new high-quality translations and synthetic data.
    *   The commitment to reproducibility is strong, with promises to release all models, data, and training recipes, including a link to the codebase used (Section 2.2, Section 8). This transparency facilitates future research and verification.*   **Strong Performance on Culturally-Aware and Translation Tasks**
    *   The TOWERVISION models achieve state-of-the-art results on the culturally diverse ALM-Bench benchmark, outperforming strong baselines like Qwen2.5-VL-7B and Gemma3-12B in the multilingual split (Table 1). This validates the effectiveness of the proposed multilingualization recipe for culturally grounded tasks.
    *   The models demonstrate excellent performance on multimodal machine translation, achieving state-of-the-art scores on Multi30K across all language pairs (Table 2). The 9B model is the top performer, and even the 2B variant is highly competitive with larger models.
    *   The extension to video, TOWERVIDEO, shows competitive performance on the ViMUL-Bench, highlighting that the benefits of the multilingual approach transfer to the video modality (Table 3).*   **Clear and Well-Structured Presentation**
    *   The paper is well-written and logically organized. The research questions are clearly articulated in the introduction (Section 1) and are systematically addressed through targeted experiments and ablations (Section 4).
    *   The multi-stage training process is explained clearly and concisely, detailing the projector pretraining, vision fine-tuning, and video fine-tuning phases (Section 2.2).
    *   Figures and tables are informative and effectively support the paper's claims. For example, Figure 1 provides a compelling overview of the model's capabilities, and Figure 2 clearly illustrates the composition of the VISIONBLOCKS dataset.3) Weaknesses
*   **Underperformance on OCR-Related Tasks**
    *   The paper candidly acknowledges that TOWERVISION is less competitive on OCR-related tasks (Section 3.3). This is a notable limitation for a general-purpose VLM.
    *   Experimental results confirm this weakness, with TOWERVISION-9B lagging significantly behind the Qwen2.5-VL models on both OCRBench and CC-OCR benchmarks (Table 1). For example, on OCRBench, TOWERVISION-9B scores 69.7 while Qwen2.5-VL-7B scores 84.5.
    *   The likely cause is the composition of the VISIONBLOCKS dataset, which appears to lack a sufficient amount of OCR-focused data compared to the training mixtures of competing models (Section 3.3).*   **Ambiguity in Dataset Curation and Filtering Details**
    *   While the paper describes the sources for VISIONBLOCKS, some key details about the curation and quality control process are sparse (Section 2.1).
    *   For the translated data, the authors mention filtering with a CometKiwi score threshold of 0.85, but do not report the volume or percentage of data that was discarded. This information would help in assessing the stringency of the quality control.
    *   For the synthetic data generated via the Gemini API, the prompts are provided (Section A.5.2), but the paper does not describe any subsequent quality control or filtering steps applied to these generated captions. The reliability of LLM-generated data can vary, making this an important omission.*   **Insufficient Analysis of Performance on Unsupported Languages**
    *   The paper evaluates models on languages marked as "unsupported" (marked with '*' in Table 3, Table 7, and Table 11), but the definition of "unsupported" is not explicitly stated. It is unclear if this means the language is absent from the fine-tuning data, the TOWER+ backbone, or both.
    *   There is a direct contradiction in the reporting of supported languages. For instance, Arabic (ar) and Swedish (sv) are not listed as supported languages (Footnote 2, Section 1) but are presented as such (i.e., without an asterisk) in the ViMUL-Bench results (Table 3). This creates confusion about the evaluation setup.
    *   The performance on some unsupported languages, particularly in the video domain, is often very low and significantly worse than baselines. For instance, on ViMUL-Bench, TOWERVIDEO-9B scores 22.1 on Bengali (bn*) and 24.1 on Tamil (ta*), while VideoLLaMA3-7B scores 36.6 and 22.8, respectively (Table 3). This performance drop warrants more detailed analysis.*   **Overstated Claims and Minor Reporting Inconsistencies**
    *   The abstract claims the models "surpass existing approaches... as demonstrated on... ViMUL-Bench (video tasks)" (Abstract, Block 2). However, the results in Table 3 do not fully support this strong claim, as TOWERVIDEO-9B is outperformed by baselines like VideoLLaMA3-7B and LLaVA-Video-7B on several languages, including English, Arabic, and Chinese. The performance is competitive, but "surpass" is an overstatement.
    *   The manuscript contains minor reporting inconsistencies that could suggest a lack of careful proofreading and may confuse the reader. For example, Table 5 reports the exact same score (83.6) for two different 9B model variants on the ALM-Bench (en) benchmark, which is statistically unlikely. Additionally, Table 6 incorrectly bolds the lower score (83.0) instead of the higher one (83.6) in the "9B En" column.4) Suggestions for Improvement
*   **Address the OCR Performance Gap**
    *   To improve the model's OCR capabilities and make it a more versatile VLM, consider augmenting the VISIONBLOCKS dataset with more OCR-centric data.
    *   This could involve incorporating public datasets focused on text in the wild, document understanding, or scene text recognition, similar to the data used for training models like Qwen2.5-VL.
    *   Ablating the effect of adding this data would further strengthen the paper by demonstrating a direct path to mitigating this identified weakness.*   **Provide More Detail on Data Curation**
    *   To enhance the reproducibility and perceived quality of the VISIONBLOCKS dataset, please provide more quantitative details on the filtering processes in the appendix (Section 2.1).
    *   Specifically, report the rejection rate for the translated captions based on the CometKiwi score threshold. This would clarify the quality of the initial translations.
    *   Describe any quality control mechanisms used for the synthetically generated captions from the Gemini API. For example, were they manually reviewed, or were automated filters applied to remove low-quality or irrelevant descriptions?*   **Clarify and Expand on Unsupported Language Evaluation**
    *   Please add a clear definition of what "unsupported" means in the context of your experiments (e.g., in the captions of Table 3, Table 7, and Table 11). Specify whether these languages are absent from the TOWER+ vocabulary, the fine-tuning data, or both.
    *   Resolve the contradiction regarding languages like Arabic and Swedish by either including them in the supported list (Footnote 2) and explaining their data source, or by correctly marking them as unsupported in Table 3 and discussing their zero-shot performance.
    *   Include a discussion in the main text (e.g., Section 3.3 or Section 4) analyzing the poor performance on some unsupported languages like Bengali and Tamil in the video tasks.*   **Revise Claims and Proofread for Accuracy**
    *   Revise the abstract to more accurately reflect the model's performance on ViMUL-Bench, using terms like "competitive" rather than "surpass" to align with the data presented in Table 3.
    *   Conduct a thorough proofread of all tables to correct inconsistencies, such as the potentially erroneous identical scores in Table 5 and the incorrect bolding in Table 6. This will improve the manuscript's credibility and prevent reader confusion.5) Score
*   Overall (10): 7 — The paper presents a thorough study on multilingual VLMs with valuable models and data, but its contributions are weakened by overstated claims and several reporting inconsistencies (Abstract, Table 3, Table 5, Table 6).
*   Novelty (10): 7 — While the architecture builds on established methods, the novelty lies in the systematic investigation of multilingualization, the creation of the VISIONBLOCKS dataset, and the specific combination of a strong multilingual backbone and vision encoder (Section 2.2, Section 4).
*   Technical Quality (10): 7 — The experimental methodology is largely rigorous, but the technical quality is diminished by overstated claims in the abstract and multiple inconsistencies in the reported results (Table 3, Table 5, Table 6, Footnote 2).
*   Clarity (10): 8 — The paper is generally well-written and structured, but clarity is impacted by contradictions regarding supported languages and several minor errors in the presentation of results (Table 3, Table 5, Table 6).
*   Confidence (5): 5 — I have read the paper and appendix thoroughly and am highly confident in my assessment of its strengths and weaknesses.