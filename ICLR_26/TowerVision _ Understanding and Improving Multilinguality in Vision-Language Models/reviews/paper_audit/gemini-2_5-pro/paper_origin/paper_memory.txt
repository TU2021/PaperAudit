# Global Summary
This paper introduces TOWERVISION, a family of open-source multilingual vision-language models (VLMs) for both image and video tasks, designed to address the English-centric bias in existing VLMs. The models are built upon the multilingual text-only model TOWER+ and are trained on VISIONBLOCKS, a new curated 6M-sample dataset combining public, translated, and synthetic data. The authors conduct a comprehensive empirical study on multilingual design choices, including the text backbone, vision encoder, and data composition at different training stages. TOWERVISION models (2B and 9B parameters) achieve competitive or state-of-the-art performance on several multilingual benchmarks, particularly excelling in culturally-aware tasks like ALM-Bench (85.2% accuracy for 9B model) and multimodal translation on Multi30K. The key findings are that: 1) multilingual vision-language training data significantly improves cross-lingual generalization, 2) starting with a strong multilingual text backbone (TOWER+) is crucial, and 3) instruction-tuned LLMs are not always the optimal starting point for VLM training. The paper also introduces TOWERVIDEO, a video-adapted model that shows competitive performance on the ViMUL-Bench. All models, data, and training recipes are publicly released.

# Abstract
The paper addresses the English-centric design of most vision-language models (VLMs), which limits their multilingual effectiveness. It presents an empirical study on multilingual design choices, resulting in TOWERVISION, a family of open multilingual VLMs for image-text and video-text tasks. TOWERVISION is built on the TOWER+ multilingual text model. It achieves competitive performance on multilingual multimodal benchmarks, showing particular strength in culturally grounded tasks (ALM-Bench, Multi30K) and multimodal translation. The video version, TOWERVIDEO, also performs well on video benchmarks like ViMUL-Bench. The authors claim their models surpass existing approaches trained on much larger datasets. They also release VISIONBLOCKS, a high-quality curated vision-language dataset. Key findings include that multilingual training data improves cross-lingual generalization (from high-resource to underrepresented languages and vice-versa) and that instruction-tuned LLMs are not always the best initialization. All models, data, and code are released publicly.

# Method
The TOWERVISION architecture consists of three main components: a multilingual text-only backbone (TOWER+ 2B/9B), a Vision Transformer encoder (SigLIP2-so400m/14@384px), and a 2-layer MLP projector to align vision and text modalities, following a LLaVA-based architecture.

- **VISIONBLOCKS Dataset:** A 6M-sample multilingual dataset created for training. It aggregates existing data (e.g., from PixMo, CulturalGround, PangeaIns), enhances it with translated and synthetic data, and includes text-only data.
    - **Data Collection:** Uses English data from PixMo, multilingual data from CulturalGround and PangeaIns.
    - **Translated Data:** PixMo-Cap captions were translated using a Tower model and filtered with a CometKiwi score threshold of 0.85.
    - **Synthetic Data:** Captions were generated using the Gemini 2.5 API with diverse prompts to improve detail.
    - **Text-only Data:** ~20% of the SFT mixture is from EuroBlocks to retain the backbone's text capabilities.
    - **Video Data:** The LLaVA-Video-178k dataset was used, with half kept in English and the other half translated uniformly into supported languages using Tower+9B.
    - **Statistics:** The dataset is 63% English and 37% multilingual. It comprises 81% public, 17% translated, and 2% synthetic data. Total size is 6,313,286 samples.

- **Training Process:** A three-stage process is used.
    1.  **Projector Pretraining:** Only the projector is trained on the English-only PixMo-Cap dataset to align modalities, with the vision encoder and LLM backbone frozen.
    2.  **Vision Finetuning:** The full model is unfrozen and trained on the image-text portion of VISIONBLOCKS. This stage uses high-dynamic resolution with a maximum of six tiles. This produces the TOWERVISION models.
    3.  **Video Finetuning:** TOWERVISION is further finetuned on the video portion of VISIONBLOCKS using 32-frame inputs to create the TOWERVIDEO models. Tiling is omitted for efficiency.

The models were trained using a custom fork of the LLaVA-Next codebase.

# Introduction
The paper highlights that despite advances in VLMs, development has been English-centric, neglecting other languages. A key challenge is the asymmetric data landscape: high-quality multilingual text corpora are available, but high-quality multilingual vision-text data is scarce. The work aims to answer what the best strategies are for extending VLMs to multiple languages. It questions at which stages and on which modules multilingualization should be applied for the greatest impact.

- **Contribution:** The paper introduces TOWERVISION, a suite of open-source multilingual VLMs for 20 languages and dialects, built on the TOWER+ models.
- **Methodology:** The authors systematically investigate VLM multilingualization through comprehensive ablations on components (projector, vision encoder, text LLM) and the impact of multilingual data across training stages.
- **Extension:** The work also includes TOWERVIDEO, extending the analysis to the video modality.
- **Dataset:** The paper releases VISIONBLOCKS, a curated dataset enriched with quality-controlled translations.

# Experiments
The models are evaluated on a suite of benchmarks for single-image, multi-image, and video tasks across multiple languages.

- **Evaluation Benchmarks:**
    - **Vision-Language:** ALM-Bench (cultural QA), OCRBench (English OCR), cc-OCR (multilingual OCR), TextVQA (scientific understanding).
    - **Multimodal Translation:** CoMMuTE (ambiguity resolution), Multi30K (image caption translation).
    - **Video:** ViMUL-Bench (culturally-diverse multilingual video QA across 14 languages).

- **Baselines:**
    - **Image Models:** Qwen2.5-VL (3B, 7B), Gemma3-it (4B, 12B), CulturalPangea-7B, Llama3-LLaVA-Next-8B, Aya-Vision-8B.
    - **Video Models:** VideoLLaMA3-7B, LLaVA-Video-7B, ViMUL-7B.

- **Main Results:**
    - **Cultural Tasks:** TOWERVISION-9B achieves state-of-the-art on ALM-Bench with 85.2% accuracy on the multilingual split, outperforming Qwen2.5-VL-7B (83.6%) and Gemma3-12B (84.5%).
    - **OCR Tasks:** TOWERVISION is less competitive on OCR-related tasks (e.g., 69.7% on OCRBench vs. 84.5% for Qwen2.5-VL-7B), attributed to less OCR-focused data in VISIONBLOCKS.
    - **Multimodal Translation:** TOWERVISION-9B achieves SOTA on Multi30K across all language pairs (e.g., 95.1 en→cs, 98.1 en→de, 95.6 en→fr).
    - **Model Scaling:** The 2B model is competitive with larger models on multilingual tasks. For instance, on ALM-Bench (multi), TOWERVISION-2B (81.1%) is competitive with Qwen2.5-VL-7B (83.6%). Scaling from 2B to 9B consistently improves performance.
    - **Video Performance:** TOWERVIDEO models are competitive on ViMUL-Bench. TOWERVIDEO-9B achieves 51.9% on English, close to LLaVA-Video-7B (53.3%). It shows strong performance in languages like German (47.1%) and Japanese (42.3%).

- **Ablation Studies:**
    - **Backbone:** Using TOWER+ consistently outperforms the base GEMMA2 model. Instruction-tuning helps with reasoning tasks (ALM-Bench), while non-instructed models perform better on raw visual extraction at smaller scales (2B). TOWER+it-9B is the best overall model.
    - **Vision Encoder:** The multilingual SigLIP2 encoder outperforms the English-centric SigLIP1 in low-data regimes. The performance gap narrows when sufficient multilingual fine-tuning data is used.
    - **Projector Pretraining:** Using only high-quality English captions for projector pretraining is sufficient and sometimes better than adding multilingual captions. On ALM-Bench (multi), the English-only pre-trained 9B model scored 85.2% vs. 84.1% for the one trained with multilingual captions.
    - **Language Coverage:** Training on more languages (20 vs. 10) consistently improves performance on core, added, and even unseen languages, with larger gains for the 2B model (+16.1% on core average).
    - **Video Finetuning:** Adding multilingual data during video fine-tuning enhances cross-lingual generalization without degrading English performance.

# Conclusion
The paper introduces TOWERVISION, a suite of multimodal models for image and video tasks with a focus on multilinguality and cultural understanding. These models show competitive or improved performance compared to existing open systems. The authors also release the VISIONBLOCKS dataset and provide a detailed training recipe with extensive ablations on key design choices. The work aims to advance research on culturally diverse, multilingual multimodal models and help close the performance gap with English-centric models.

# Appendix
- **A.1 VISIONBLOCKS Details:** Table 8 provides a full breakdown of the dataset. It contains a total of 6,313,286 samples, with 3,982,630 (63.1%) being English and 2,330,656 (36.9%) being multilingual.
- **A.2 Model Checkpoints:** Table 9 lists the HuggingFace checkpoints for all baseline models used in the evaluation.
- **A.3 Vision Encoder Variants:** An ablation on the TOWERVISION 2B model (Table 10) explored different vision encoder configurations. The chosen setup (384x384 resolution, patch size 14, 6 tiles) offered the best trade-off, achieving top scores on TextVQA (70.3), OCRBench (62.1), and CC-OCR (46.1) among the tested variants.
- **A.4 Cross-Lingual Generalization:** Table 11 provides detailed results for the language coverage experiment. Training the 2B model on more languages resulted in a +16.1% gain on the core language average and a +13.9% gain on unseen languages. For the 9B model, the gains were smaller (+1.1% on core average).
- **A.5 System Prompts:** This section lists the specific prompts used to generate translations with a Tower model and synthetic captions with the Gemini 2.5 API. The prompts were designed to elicit diverse styles and formats.

# References
This section contains the list of references cited in the manuscript.