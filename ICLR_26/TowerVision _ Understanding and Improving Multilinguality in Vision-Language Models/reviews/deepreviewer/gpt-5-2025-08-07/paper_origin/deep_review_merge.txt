Summary
The paper presents TowerVision (images) and TowerVideo (video), multilingual vision–language models built on a multilingual text backbone (TOWER+) and a SigLIP2 vision encoder with a two-layer projector. Training follows a staged recipe: projector alignment using English-only captions, multilingual image–text finetuning on a curated 6.3M-example dataset (VisionBlocks), and optional multilingual video finetuning on a translated subset of LLaVA-Video. VisionBlocks blends public, translated, and synthetic data with translation quality filtering (e.g., COMETKiwi thresholds) and documented prompt pipelines. Through targeted ablations, the authors investigate where multilingual signals are most impactful: they find that a multilingual backbone provides notable gains over English-centric ones, multilingual-aware vision encoders are particularly beneficial in low multilingual data regimes, English-only captions suffice for projector alignment, and increasing language coverage improves cross-lingual generalization when dataset size is controlled. Empirically, TowerVision achieves strong results on culturally grounded multilingual QA (ALM-Bench) and multimodal translation (Multi30K), and TowerVideo is competitive on multilingual video QA (ViMUL-Bench). The paper emphasizes reproducibility with practical training details (e.g., HDR tiling and frame counts) and plans to release models, data, and recipes.

Strengths
- Systematic, component-wise ablations that clearly isolate the contributions of the text backbone, vision encoder, projector alignment data, and language coverage, yielding actionable guidance on “where to multilingualize.”
- Strong multilingual performance across tasks and modalities at moderate scale, with notable gains on culturally grounded QA and multimodal translation, and competitive results on multilingual video QA.
- A clear and reproducible training pipeline with practical implementation details (e.g., HDR tiling, resolution choices, 32-frame video setup), aiding adoption and replication.
- Substantial resource contribution: VisionBlocks (6.3M examples) with documented curation and translation filtering, plus open checkpoints and code release plans.
- Careful controls in experiments (e.g., holding dataset size constant in language-coverage studies), strengthening causal interpretation of findings such as the benefits of broader language coverage and the sufficiency of English captions for projector alignment.

Weaknesses
- Evaluation breadth is limited: multimodal translation is reported with only xCOMET, and open-ended multilingual video QA relies on a single English-centric LLM judge (GPT‑4o), which may introduce language/script-specific biases; the absence of human or multilingual ensemble judging reduces confidence in cross-lingual fairness.
- Pronounced OCR underperformance is reported without targeted remediation or error analysis (e.g., adding OCR-focused multilingual data, assessing tiling effects), leaving an important capability gap unaddressed.
- Reporting gaps and minor presentation issues: unlabeled metrics in at least one key table (encoder comparison), mischaracterization of TextVQA’s focus, and formatting glitches detract from clarity; compute budgets, training schedules, and per-language data distributions are insufficiently detailed.
- Data provenance and licensing concerns are not fully resolved for synthetic/translated content (e.g., Gemini 2.5–generated outputs), and potential train–test overlap or indirect contamination in translation benchmarks is not explicitly ruled out.
- Limited analysis of translation quality and artifacts from synthetic data and translations beyond a threshold filter (e.g., lack of error taxonomy or human spot-checks), which weakens claims about dataset quality and its impact on model performance.
