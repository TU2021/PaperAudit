Summary
The paper introduces TowerVision, a family of multilingual vision-language models (VLMs) for images and video built on the multilingual text backbone TOWER+ and a SigLIP2 vision encoder, plus TowerVideo for video. The authors curate a 6.3M-example dataset, VisionBlocks, mixing public, translated, and synthetic data (§2.1; Table 8; Fig. 2). Training follows three stages: projector alignment on English captions, full image-text finetuning with multilingual data, and video finetuning on translated LLaVA-Video (§2.2). Main empirical findings include: (i) multilingual backbones matter (Table 4), (ii) multilingual-aware vision encoders help particularly in low-data regimes (Table 5), (iii) English-only captions suffice for projector alignment (Table 6), and (iv) expanding language coverage improves cross-lingual transfer (Fig. 3; Table 11). TowerVision achieves strong results on ALM-Bench and Multi30K (Tables 1–2) and competitive video QA on ViMUL-Bench (Table 3). Models, data, and recipes will be released.

Soundness
The empirical methodology is largely sound: the training protocol is detailed (§2.2) and the ablations are well-targeted (Tables 4–6; Table 10; Fig. 3; Table 11), isolating the contribution of backbones, encoders, and training-stage multilinguality. Dataset construction uses quality filters (COMETKiwi ≥ 0.85 for translations; §2.1), and language coverage is held constant in certain comparisons (Fig. 3 discussion; §3.3/§4). However, some evaluation choices could bias conclusions: video open-ended responses are judged by GPT‑4o (Table 3; §3.3), which may exhibit language-specific biases; including a multilingual ensemble judge or human spot-checks would strengthen claims. For multimodal translation, only xCOMET is reported (Table 2), limiting comparability to prior BLEU/chrF-based literature. The claim that TextVQA assesses “scientific understanding” (§3.1) appears inconsistent with the dataset’s focus on text-in-image reading, suggesting a minor oversight. Overall, the logic of the findings (e.g., Table 6’s result that multilingual captions during projector training do not help) is persuasive and consistent with past VLM practice, and the cross-lingual transfer study is carefully constructed.

Presentation
The paper is generally clear and well-organized, with an accessible problem setup (Fig. 1; Fig. 2) and concise training recipe (§2.2). Tables summarizing results are comprehensive (Tables 1–3) and the ablation figures are helpful (Fig. 3). A few issues detract from polish: (i) Table 5 omits explicit metric labels (likely ALM-Bench accuracy), (ii) minor mischaracterization of TextVQA (§3.1), and (iii) a few formatting glitches and footnote placements in §2.2/Appendix. The data composition breakdown (Table 8) is valuable, though per-language distributions within multilingual subsets are not shown, making it hard to judge linguistic balance.

Contribution
The work’s primary contribution is a careful, component-wise study of “where and how” to inject multilinguality into VLMs, alongside open models and a large curated dataset. Empirical takeaways (Table 4; Table 6; Fig. 3) offer actionable guidance: use a strong multilingual backbone, prefer multilingual encoders when multilingual data is scarce, and rely on high-quality English captions for projector alignment. The release of VisionBlocks and multilingual TowerVideo adds practical value. While building on established LLaVA-style stacking, the multilingual ablation agenda and cross-lingual generalization analysis are timely and impactful.

Strengths
- Thorough ablations isolating the impact of backbone, encoder, and alignment-stage data (Tables 4–6; Table 10).
- Strong multilingual performance on culturally grounded tasks (ALM-Bench; Table 1) and multimodal translation (Multi30K; Table 2), with competitive video results (Table 3).
- Clear, reproducible training pipeline with practical details (tiling, resolution, frames; §2.2; Table 10).
- Significant resource contribution: VisionBlocks (6.3M examples; Table 8), prompts, and release plans (§§7–8; A.5).

Weaknesses
- Evaluation gaps: reliance on a single automatic metric for translation (xCOMET; Table 2) and an English-centric LLM judge for multilingual video (GPT‑4o; Table 3).
- OCR underperformance (Table 1) not remedied by targeted data or analysis; lack of error breakdown on OCRBench/CC‑OCR.
- Table 5 lacks metric labeling; small presentation inconsistencies (e.g., TextVQA described as “scientific understanding”; §3.1).
- Limited reporting on per-language data distributions and compute/energy usage; dataset licensing for Gemini 2.5 outputs is not fully clarified (§2.1).
- Potential risk of train–test overlap or indirect contamination in translation benchmarks is not explicitly ruled out.

Questions
- Can you report BLEU/chrF in addition to xCOMET on Multi30K and include human or multilingual-judge spot checks for ViMUL-Bench (Table 3) to address judge bias?
- What are the per-language sample counts in VisionBlocks (beyond totals in Table 8), and do these correlate with per-language ALM-Bench gains (Fig. 3; Table 11)?
- Were Multi30K/CoMMuTE test sets or closely related training instances excluded from EuroBlocks/Tower+ data to avoid leakage?
- Could adding small amounts of OCR-heavy multilingual data (e.g., receipts, street signs) remedy the deficits seen in Table 1? Any ablations?
- Please clarify the metric reported in Table 5 and add per-language video results stratified by question type (MCQ vs open-ended) to examine judge effects.

Rating
- Overall (10): 8 — Strong empirical study with actionable insights and open artifacts, with minor evaluation and presentation gaps (Tables 1–3, 4–6; §2.1; §3.1; Fig. 3).
- Novelty (10): 7 — Builds on LLaVA-style stacks but contributes a useful, systematic multilingual ablation agenda and dataset (Tables 4–6; Table 8).
- Technical Quality (10): 8 — Sound methodology and controls; could be improved with broader metrics and judge audits (Table 2; Table 3; §4).
- Clarity (10): 7 — Generally clear, but a few labeling and description inconsistencies remain (Table 5; §3.1).
- Confidence (5): 4 — Assessment grounded in reported results and anchors, but depends on promised releases and evaluation clarifications.


Summary
This paper presents TowerVision/TowerVideo, multilingual VLMs trained via a staged recipe: English-caption projector pretraining, multilingual image-text finetuning with VisionBlocks, and optional video finetuning on a translated LLaVA-Video subset (§2.2). The authors explore where multilingual signals help most: backbone pretraining and instruction tuning (Table 4), multilingual-aware vision encoders (Table 5), and language coverage (Fig. 3; Table 11). They report gains on ALM-Bench and Multi30K (Tables 1–2) and competitive ViMUL-Bench results (Table 3). VisionBlocks aggregates public, translated, and synthetic data with translation quality control (§2.1; Table 8).

Soundness
The core claims are supported by multiple ablations with sensible controls. The backbone study (Table 4) isolates multilingual pretraining and instruction tuning; the encoder study (Table 5) contrasts SigLIP1 vs SigLIP2 under English-only vs multilingual finetuning; projector alignment experiments (Table 6) test multilingual vs English-only captions. The language-expansion study controls for dataset size (Fig. 3; Table 11), strengthening the causality argument. Nonetheless, two concerns limit rigor: (i) translation quality and potential artifacts from synthetic data (Gemini 2.5 API; §2.1) are not error-analyzed; (ii) the use of a single LLM judge for multilingual open-ended video (Table 3) can introduce language bias. The OCR weakness (Table 1) is only hypothesized, not diagnosed. Still, the findings are plausible and repeatable with the provided recipes.

Presentation
The narrative is easy to follow, with helpful diagrams (Figs. 1–2) and a clean training pipeline (§2.2). VisionBlocks stats are summarized clearly (Table 8). Areas needing improvement: (i) Table 5 does not specify what metric the numbers represent; (ii) mislabeling TextVQA’s purpose (§3.1) could confuse readers; (iii) compute budgets and training schedules (epochs, batch sizes) are not fully specified. Appendices provide helpful prompts (A.5) and extra comparisons (Table 10; Table 11).

Contribution
- A practical and thoroughly evaluated recipe for multilingualizing VLMs, with tested design choices and clear takeaways (Tables 4–6).
- An open 6.3M-example multilingual multimodal dataset with explicit curation/translation pipelines and quality thresholds (Table 8; §2.1).
- Open checkpoints and code (links in Abstract; §7–8), fostering replicability.
Compared to prior works focusing on multilingual SFT alone (e.g., Pangea), the systematic analysis across modules and stages is a valuable addition.

Strengths
- Careful component-wise ablations answering “where to multilingualize” (Tables 4–6).
- Strong cultural understanding and translation performance at moderate scale (Tables 1–2).
- Clear engineering choices: SigLIP2, HDR tiling, 32-frame video, and their trade-offs (Table 10; §2.2).
- Data quality controls (COMETKiwi 0.85 filter; prompt diversity; §2.1; A.5).

Weaknesses
- Evaluation reliance on a single judge for multilingual video (Table 3) and a single automatic metric for translation (Table 2).
- Limited OCR data yields pronounced deficits (Table 1) with no corrective ablation.
- Missing details on per-language data balance and training compute; Table 5’s metric is unlabeled.
- Potential dataset licensing and redistribution issues for Gemini 2.5–generated captions not discussed (§2.1).

Questions
- Can you report BLEU/chrF and human evaluations for Multi30K and include a multilingual ensemble judge for ViMUL-Bench to mitigate judge bias?
- Please include per-language sample counts and domain categories in VisionBlocks and correlate them with ALM-Bench outcomes (Table 11).
- Did you assess translation artifacts or hallucinations from Gemini/Tower+ via human sampling? Any error taxonomy?
- What is the exact metric for Table 5? Are those ALM-Bench-multi accuracies?
- Would adding small OCR-focused multilingual subsets during Stage 2 close the gap in Table 1?

Rating
- Overall (10): 7 — Solid and timely study with open assets and clear insights, but evaluation breadth and reporting gaps limit impact (Tables 1–3; 4–6; §2.1).
- Novelty (10): 7 — Systematic multilingual ablations across modules/stages provide fresh guidance beyond prior English-centric pipelines (Tables 4–6; Fig. 3).
- Technical Quality (10): 7 — Generally robust experiments with good controls; needs broader metrics and judge audits (Table 2; Table 3).
- Clarity (10): 7 — Mostly clear; some mislabeled/unspecified elements (Table 5; §3.1) and missing compute details.
- Confidence (5): 3 — Good evidence but some claims hinge on forthcoming releases and expanded evaluations.


Summary
The authors develop TowerVision, a multilingual image VLM, and TowerVideo, a multilingual video VLM. VisionBlocks is a 6.3M-example curated dataset blending English and multilingual vision-text and video-text data with translations filtered by COMETKiwi (§2.1; Table 8). Training proceeds in three stages: projector alignment on English captions, multilingual image-text SFT with HDR tiling, and multilingual video SFT on a translated LLaVA-Video subset (§2.2). TowerVision shows strong cultural QA (ALM-Bench) and multimodal translation (Multi30K), and TowerVideo is competitive on ViMUL-Bench (Tables 1–3). Ablations show the importance of a multilingual backbone (Table 4), multilingual-aware vision encoders (Table 5), and broader language coverage (Fig. 3; Table 11), while multilingual captions do not help projector alignment (Table 6).

Soundness
The staged training and ablations are technically well-motivated and align with contemporary VLM practice. The backbone comparison (Gemma2 vs TOWER+; it vs pt) is informative and uses matched training recipes (Table 4). The encoder comparison (SigLIP1 vs SigLIP2) systematically varies training data multilinguality (Table 5), supporting the “low-data advantage” of multilingual encoders. The language coverage study controls for dataset size (Fig. 3; §4), strengthening causal inference. However, the translation evaluation relies solely on xCOMET (Table 2), and the open-ended video assessment uses GPT‑4o (Table 3) without a multilingual judge ensemble or calibration; these choices may understate variance across languages/scripts. The OCR deficit (Table 1) is acknowledged but not further analyzed.

Presentation
The manuscript presents a coherent narrative, with helpful overviews (Figs. 1–2) and explicit dataset composition (Table 8). Minor issues: Table 5 lacks metric labels; §3.1’s description of TextVQA is inaccurate; compute/training hyperparameters are only partially reported. Despite these, the paper is readable and the ablation story is easy to follow.

Contribution
Contributions are threefold: (i) practical recipe and evidence on when and where to inject multilinguality into VLMs (Tables 4–6; Fig. 3), (ii) open multilingual models for images and video, and (iii) VisionBlocks, a sizable curated dataset with translation quality control (Table 8; A.5). The empirical findings—especially that multilingual backbones matter more than instruction-tuned LLMs as VLM initializations, and that English-only projector alignment suffices—are valuable to practitioners.

Strengths
- Comprehensive ablation suite with concrete, generalizable conclusions (Tables 4–6; Fig. 3; Table 11).
- Competitive multilingual performance at moderate scale, notably on cultural QA and multimodal translation (Tables 1–2).
- Data curation pipeline with explicit translation quality thresholds and prompt diversity (§2.1; A.5).
- Open-source orientation with code/data/model release plans (§§7–8).

Weaknesses
- Evaluation breadth: single-metric translation reporting and single LLM judge for multilingual video; lack of human validation (Tables 2–3).
- Insufficient OCR capability (Table 1) without remediation experiments.
- Reporting gaps: unlabeled metric in Table 5; incomplete per-language data distributions; missing compute/training schedules.
- Potential licensing ambiguity around Gemini 2.5–generated captions and redistribution (§2.1).

Questions
- Please add BLEU/chrF and human or multilingual-judge evaluations for Multi30K and ViMUL-Bench to complement xCOMET and GPT‑4o.
- Could you provide per-language sample counts for VisionBlocks and relate them to ALM-Bench per-language gains (Fig. 3; Table 11)?
- What filtering did you apply to ensure that translations used for training do not mirror test references in Multi30K/CoMMuTE?
- Would incorporating a small OCR-focused multilingual subset in Stage 2 improve Table 1, and how does HDR tiling affect OCRBench vs ALM-Bench?
- Clarify the metric for Table 5 and include error analyses for translation and cultural QA.

Rating
- Overall (10): 8 — Strong, practically useful study with clear insights and competitive results; some evaluation/reporting limitations remain (Tables 1–3; 4–6; §2.1).
- Novelty (10): 8 — While built on LLaVA-like architecture, the multilingual ablation agenda and data release are substantive (Tables 4–6; Fig. 3; Table 8).
- Technical Quality (10): 8 — Solid experiments and controls; would benefit from broader metrics and judge audits (Table 2; Table 3).
- Clarity (10): 8 — Generally clear and well-structured; a few minor inconsistencies (Table 5; §3.1).
- Confidence (5): 4 — Judgments based on detailed reported results; some dependence on promised releases and additional evaluations.


Summary
TowerVision (image) and TowerVideo (video) are multilingual VLMs trained on a curated 6.3M-example VisionBlocks dataset blending public, translated, and synthetic resources (§2.1; Table 8). The pipeline uses TOWER+ as text backbone, SigLIP2 as vision encoder, and a two-layer projector (§2.2). The authors report strong cultural QA and multimodal translation results (Tables 1–2) and competitive multilingual video QA (Table 3). Ablations reveal: (i) multilingual backbones outperform English-centric ones (Table 4), (ii) multilingual-aware encoders help when multilingual data is scarce (Table 5), (iii) English-only projector alignment suffices (Table 6), and (iv) adding languages boosts cross-lingual generalization (Fig. 3; Table 11). Models, data, and training recipes will be released.

Soundness
Methodology is thoughtful and largely reproducible: staged training with explicit design justifications (e.g., HDR tiling up to six tiles; 32-frame video; §2.2) and systematic ablations. Translation filtering via COMETKiwi ≥ 0.85 (§2.1) is a strong choice. The conclusions flow from the evidence: e.g., Table 6’s result aligns with prior VLM alignment practice; Table 4 demonstrates importance of a multilingual backbone; Fig. 3/ Table 11 substantiate language-coverage effects. Concerns remain: (i) evaluation of video QA with a single judge may skew results for certain languages/scripts (Table 3), (ii) only xCOMET for translation (Table 2), and (iii) OCR weaknesses (Table 1) are hypothesized but not probed with targeted ablations (e.g., adding OCR data or changing tiling). Still, the overall evidence base supports the main claims.

Presentation
The paper communicates the approach well with intuitive diagrams and clear tables (Figs. 1–2, Tables 1–3). The dataset overview (Table 8) is comprehensive. Two clarity issues stand out: Table 5’s metric is unspecified, and §3.1 mischaracterizes TextVQA. More details on compute, training hyperparameters, and per-language data distributions would enhance transparency. The appendices add useful implementation detail (Table 10; A.5 prompts).

Contribution
- A well-evidenced guide to multilingualizing VLMs, illuminating which stages and modules matter most (Tables 4–6).
- VisionBlocks, a large multilingual vision/video-language resource with filtering and prompt artifacts documented (Table 8; A.5).
- Open multilingual VLMs for both images and video, competitive against stronger baselines on key multilingual tasks (Tables 1–3).
The combination of open artifacts and principled ablations is a meaningful contribution for practitioners and researchers.

Strengths
- Careful experimental design with component-level insights (Tables 4–6; Fig. 3).
- Competitive multilingual performance across modalities with moderate parameter counts (Tables 1–3).
- Practical recipes and implementation details enabling replication (tiling, frames, encoders; §2.2; Table 10).
- Data curation quality controls and transparency (COMETKiwi threshold; translation prompts; §2.1; A.5).

Weaknesses
- Evaluation breadth is limited for translation and video (single automatic metric/judge; Tables 2–3).
- OCR lag without remedial experiments (Table 1).
- Limited visibility into per-language distributions and compute; unlabeled metrics (Table 5) and a minor dataset description error (§3.1).
- Potential licensing questions around releasing Gemini-generated captions (§2.1) and lack of explicit leakage checks for translation benchmarks.

Questions
- Please report BLEU/chrF and add a multilingual judge or human sampling for ViMUL-Bench to validate Table 3.
- Provide per-language counts and domain categories in VisionBlocks and correlate with ALM-Bench gains (Fig. 3; Table 11).
- Did you verify that Multi30K/CoMMuTE test items were not seen (or paraphrased) in EuroBlocks or translated VisionBlocks data?
- Can adding a small multilingual OCR subset during Stage 2 narrow the gap on OCRBench/CC‑OCR (Table 1)? Any preliminary tests?
- What metric does Table 5 report? Could you include per-task breakdowns to confirm the trend?

Rating
- Overall (10): 7 — Useful, well-supported study with open artifacts; evaluation/reporting refinements would strengthen it (Tables 1–3; 4–6; §2.1).
- Novelty (10): 7 — Novel in its systematic multilingual ablations and practical guidance, though the architecture is standard (Tables 4–6; Fig. 3).
- Technical Quality (10): 7 — Solid evidence and controls; needs broader evaluation metrics and judge robustness checks (Table 2; Table 3).
- Clarity (10): 8 — Clear exposition and figures; minor labeling and description issues (Table 5; §3.1).
- Confidence (5): 3 — Reasoned assessment but contingent on promised releases and additional evaluation details.