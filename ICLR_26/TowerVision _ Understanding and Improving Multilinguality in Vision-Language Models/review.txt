### Summary 

**TowerVision** introduces a series of **multilingual vision-language models (VLMs)** capable of processing both **image-text** and **video-text** tasks. This work builds upon the **Tower+ text backbone** and focuses on addressing the **English-centric limitations** of existing VLMs. The key contribution of the paper is its **empirical study** that explores how different design choices¡ªsuch as training data composition, encoder selection, and text backbones¡ªimpact the performance of multilingual VLMs. The study culminates in the development of **TowerVision**, a family of **multilingual VLMs** that performs well on various benchmarks, including those for culturally grounded tasks and multimodal translation.

**TowerVision** is based on the **Tower+ model** and incorporates a comprehensive training process that includes:

1. **Projector pretraining** (aligning visual features with text embeddings),
2. **Vision fine-tuning** (adapting vision encoders to multilingual data), and
3. **Video fine-tuning** (extending the model's capabilities to video-based tasks).

The model is evaluated on **multilingual benchmarks**, demonstrating its **competitive performance** and ability to handle **cross-lingual generalization**. Furthermore, the paper introduces **VisionBlocks**, a curated multilingual vision-language dataset containing around **6 million samples** that further contributes to the multilingual capabilities of VLMs.

---

### Strengths

1. **Multilingual and Culturally Grounded Model**:

   * **TowerVision** provides a practical, **open-source multilingual model** capable of addressing the limitations of English-centric VLMs.
   * The models demonstrate strength in **culturally grounded tasks** (e.g., **ALM-Bench**) and **multimodal translation** (e.g., **Multi30K**).

2. **Empirical Insights**:

   * The paper offers **valuable findings** on multilingual design, such as the importance of using **English-only captions** during the **projector alignment phase** and **multilingual data** in **fine-tuning** stages.
   * **Stage-specific data composition** and the **TowerPlus backbone** are highlighted as important design choices that improve **multilingual VLM performance**.

3. **New Dataset (VisionBlocks)**:

   * The authors introduce **VisionBlocks**, a **curated multilingual vision-language dataset** that includes **6 million samples**, combining **synthetic data**, **translated text**, and **high-quality image-text pairs**.

4. **Broad Evaluation**:

   * Extensive evaluation across **multiple modalities** (image and video), languages (20+), and tasks (e.g., **VQA**, **OCR**, **cultural reasoning**), which shows competitive results against state-of-the-art models like **Qwen2.5-VL**, **Gemma3**, and **Aya-Vision**.

5. **Video Capability**:

   * **TowerVideo**, an extension of **TowerVision** for video tasks, shows that the multilingual model can transfer effectively to **video-based VLMs** with minimal adaptation.

---

### Weaknesses

1. **Limited Novelty**:

   * The main concern raised by multiple reviewers is the **lack of architectural novelty**. The authors acknowledge that **TowerVision** builds upon existing VLM pipelines and **does not introduce a radically new architecture**. Instead, the novelty lies in the **empirical investigation** of multilingual design choices and the introduction of **VisionBlocks** as a multilingual dataset.

2. **Stage-Specific Performance**:

   * The finding that **multilingual captions** during the **projector alignment stage** **slightly decrease performance** is counterintuitive to the model¡¯s overall claim that multilingual data improves performance. The authors explain that this is a **stage-specific effect**, where **English-only captions** provide a more consistent signal during early stages. However, this point requires further exploration to justify why multilingual data does not perform well at this stage.

3. **Limited OCR Performance**:

   * **TowerVision** is reported to be **less competitive in OCR tasks** due to a **lack of OCR-specific data** in **VisionBlocks**. Although the authors plan to incorporate **OCR data** in future versions, the current lack of robust OCR capabilities limits the model's general multilingual use.

4. **Dataset Design and Evaluation**:

   * Some **design choices** in the creation of **VisionBlocks**, such as **translation using Tower** and **automated filtering with COMETKIWI**, are questioned by reviewers. Although these methods are standard in the field, the authors admit that automated filtering may miss **subtle cultural inaccuracies**, which could undermine the model¡¯s claim of being **culturally grounded**.
   * Additionally, reviewers suggested more **ablation studies** specific to video tasks, especially regarding parameters like **frame size** for video fine-tuning, which the authors plan to address in future work.

5. **Quantitative Results**:

   * Despite **improvements in multilingual tasks**, the **quantitative results** show **mixed performance**, particularly on **multilingual video-to-text tasks**. Reviewers suggest that this highlights the need for deeper analysis, possibly incorporating **qualitative insights** or **error analyses** to explain the performance nuances across different languages and cultural contexts.
