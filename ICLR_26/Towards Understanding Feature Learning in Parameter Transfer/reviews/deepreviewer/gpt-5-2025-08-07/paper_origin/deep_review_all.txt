Summary
The paper develops a theoretical framework for parameter transfer in transfer learning, focusing on two-layer ReLU CNNs where an α-fraction of upstream filters are inherited by a downstream model. The data model comprises two “patches” with a universal signal u shared across tasks and task-specific signals v1, v2, plus Gaussian noise. Under explicit over-parameterized conditions (Condition 4.1), the authors prove a phase transition in downstream generalization (Theorem 4.2): when dimension d lies below a threshold determined by α, N1, N2, signal strengths, and noise levels, test error approaches Bayes optimal; otherwise, it is bounded away from optimal (≥ 0.1). They compare with training from scratch (Theorem 4.3, from Kou et al., 2023), derive a sufficiency criterion Γ for successful transfer (Proposition 4.4), and identify regimes of negative transfer when universal knowledge is weak (Proposition 4.4(2); Section 4). Extensive appendices analyze gradient dynamics via a signal-noise coefficient decomposition (Lemma B.2; Lemmas A.1/A.2) and ODE-style approximations. Synthetic experiments (Figure 1, Figure 2) and real-image experiments on CIFAR-10/100 with ResNet/VGG and ViT (Table 1; Figures 3–4) empirically corroborate the theoretical trends.

Soundness
- Methodology: The training dynamics analysis is carefully constructed via a decomposition of convolutional weights into components aligned with u, v1, v2, and per-sample noise vectors (Definition B.1). The iterative coefficient updates under gradient descent (Lemma B.2) are derived explicitly for both tasks and used to bound signal learning and noise memorization (Lemmas A.1/A.2; D.8/D.9; E.9/E.10). The phase-transition statements hinge on concentration (Lemma C.1), initialization bounds (Lemma C.2), and balanced gradient contributions across samples (Lemma D.4/E.5).
- Logical consistency: The core logic—more inherited parameters and larger upstream sample size improve the universal component; weak u yields over-amplified weights that can magnify noise—is consistent with the derived inequalities (Theorem 4.2; Proposition 4.4(2)). The comparison to the no-transfer baseline (Theorem 4.3) is appropriate and clarifies when transfer strictly helps.
- Assumptions: The orthogonality u ⟂ v1, u ⟂ v2 and orthogonality of noise to signal (Section 3) simplify analysis but limit generality. The over-parameterization and small initialization/learning-rate constraints (Condition 4.1) are strong; d = ˜Ω(max{nσp−2‖u+v‖², n²}) is demanding. The random inheritance of filters (Algorithm 1) deviates from practical layer-wise transfer.
- Self-verification: The appendix provides many cross-checkable bounds; however, there are indexing and typographical inconsistencies that hamper verifiability (e.g., Theorem 4.3 cites “Condition 3.1” but only Condition 4.1 is defined; Theorem E.12’s training-loss bound momentarily uses σp,1 and N1 while bounding Task 2—Block #107—suggesting a copy-paste error).

Presentation
- Organization: The paper is clearly structured: introduction and motivation, related work, setting, algorithms, main theorems, propositions, and extensive proofs. The decomposition and iterative rules (Section B) are helpful for reproducibility of reasoning.
- Clarity: Most derivations are readable, though the heavy notation (bars/underscores, mixed indices) and occasional typos (e.g., Condition references; mismatched σp indices in Theorem E.12) introduce friction. The narrative around negative transfer is clear and tied to equations (Block #17).
- Visual aids: Synthetic plots (Figure 1, Figure 2) succinctly illustrate how N1, σp,1, and ‖u‖ affect accuracy; real-data plots (Figures 3–4) support claims. Table 1 summarizes outcomes across models and N1/N2 ratios. Some figure labels are truncated or briefly described (Figure 2b–c captions; Block #22), and α choices are not consistently reported in real experiments.

Contribution
- Significance: The paper offers, to the best of my knowledge, one of the first training-dynamics analyses for parameter transfer in nonlinear CNNs, bridging a gap left by static generalization works. The phase transition characterizations, explicit dependence on α, N1, σp, and universal signal strength, and a theoretical account of negative transfer are valuable to theory-minded practitioners.
- Novelty: Combining feature-learning-style dynamics with partial parameter reuse and proving sharp regimes versus scratch training (Theorems 4.2/4.3; Proposition 4.4) is novel. The explicit coefficient dynamics and ODE approximation approach are adapted from recent feature learning works but creatively extended to two coupled tasks with inheritance.
- Practical impact: While the model is stylized, the insights (strong u helps; noise/overtraining can hurt; α matters) align with practice. The synthesis and real experiments increase credibility, though implementation specifics are sparse.

Strengths
- First dynamic theory for parameter transfer with partial inheritance, not just generalization bounds (Section 4; Theorem 4.2).
- Clear identification of factors (α, N1, σp, ‖u‖) driving success or negative transfer (Proposition 4.4; Block #17; Figures 1–2).
- Rigorous coefficient-level analysis with explicit iterative updates and concentration lemmas (Lemma B.2; Lemma C.1/C.2; Lemma D.4/E.5).
- Empirical alignment: Synthetic and real data trends match theory; noise-robustness advantage of transfer demonstrated (Figure 3; Table 1; ViT results in Figures 4/Appendix).

Weaknesses
- Strong, stylized data assumptions: orthogonality and patch assignment limit applicability; real-world data rarely obey such structure (Definitions 3.1–3.2; Block #10).
- Stringent scaling: Condition 4.1 requires very large dimensions and small learning rates; “tightness” claim is asserted but could be more thoroughly justified with necessity proofs (Block #12; Block #3).
- Random inheritance choice (Algorithm 1) differs from practical feature selection; yet theory recommends transferring “strong shared features” (Block #11 vs Block #4), creating a mismatch between analysis and recommended practice.
- Clarity/consistency issues: referencing Condition 3.1 in Theorem 4.3 (Block #14), σp index confusion in Theorem E.12 (Block #107), mixed notation around x̄_t and x_t solutions appear in multiple places and could be streamlined.
- Empirical details missing: precise α values, layer mapping across architectures, training hyperparameters, and variance across seeds are not fully specified; negative transfer is shown convincingly in synthetic (Figure 1c at ‖u‖=0) but not explicitly on real data.

Questions
- How sensitive are the phase boundaries in Theorem 4.2 to relaxing orthogonality assumptions (noise not orthogonal to signals, u not strictly orthogonal to v1/v2), beyond the brief note in Block #10?
- Can the authors provide explicit necessity arguments for the claimed “tight” conditions (Block #3), or counterexamples that saturate them?
- How does the theory adapt if inheritance uses feature- or layer-selection heuristics (e.g., gradient-based “learngene,” Block #5), instead of random filters as in Algorithm 1?
- In real experiments, what α and layer-selection strategy were used for ResNet/VGG transfer; how were mismatched shapes handled, and were batchnorm statistics reinitialized?
- The σp confusion in Theorem E.12 (Block #107) appears to use source-task quantities while bounding target-task training loss. Can the authors correct and clarify that step?
- Can the framework extend to deeper CNNs or Transformers with skip connections, and what additional dynamics (e.g., layerwise coupling) would be needed?

Rating
- Overall (10): 8 — Strong, novel dynamic theory of parameter transfer with clear phase transitions and negative-transfer regimes, albeit under stylized assumptions and with minor presentation issues (Theorem 4.2; Proposition 4.4; Figures 1–3; Block #14/#107).
- Novelty (10): 9 — First rigorous treatment of training dynamics for partial parameter inheritance in nonlinear CNNs, beyond static bounds (Section 4; Lemma B.2; Theorem 4.2).
- Technical Quality (10): 8 — Careful coefficient dynamics, concentration, and ODE approximations; a few indexing/condition references need correction to solidify soundness (Lemma D.4/E.5; Block #14/#107).
- Clarity (10): 7 — Generally clear structure and proofs, but notation and cross-references occasionally inconsistent; empirical details could be fuller (Algorithm 1; Block #22; Table 1).
- Confidence (5): 4 — High familiarity with feature-learning theory and transfer learning; assessment based on thorough read of theory and experiments; a few typos lower absolute confidence in every detail.

---

Summary
This work analyzes when and why transferring a subset of parameters from an upstream two-layer ReLU CNN benefits a downstream task. The authors model shared universal features via a vector u and task-specific signals v1, v2, with Gaussian noise. They derive training-dynamics bounds on signal learning and noise memorization through a coefficient decomposition and prove a phase transition in downstream generalization for parameter transfer versus scratch training (Theorems 4.2–4.3). The analysis quantifies the roles of α (fraction of inherited filters), training sample sizes N1/N2, noise levels σp,1/σp,2, and universal signal strength ‖u‖. Synthetic and real experiments corroborate: increasing N1, reducing σp,1, and strengthening ‖u‖ improves transfer; weak ‖u‖ can cause negative transfer.

Soundness
- The derivations are grounded in explicit dynamics (Lemma B.2), concentration for Gaussian noise (Lemma C.1), and controlled initialization (Lemma C.2). The ODE approximations for cumulative memorization and learning rates (Lemmas A.1/A.2; D.4/D.8; E.5/E.9) are consistent with prior feature-learning analyses.
- The main phase transition (Theorem 4.2) is plausible: bounds tie error to d relative to combined signal-to-noise and sample-size terms. Comparison to Kou et al. (2023) (Theorem 4.3) is apt.
- However, several technical choices are restrictive: orthogonality structure (Definitions 3.1–3.2), very large d (Condition 4.1), and small learning rates. The negative transfer mechanism hinges on large weight norms amplifying task-specific noise (Block #4), which is convincingly argued in synthetic settings but may be weaker under feature-selection transfer.
- Minor inconsistencies—Condition 3.1 versus 4.1 (Block #14), σp misindexing in Theorem E.12 (Block #107)—should be corrected to ensure rigor.

Presentation
- The paper is densely written but logically organized. The signal-noise decomposition (Definition B.1) and gradient updates (Lemma B.2) are helpful.
- Typos and references occasionally distract (e.g., “Condition 3.1,” mismatched σp terms, some figure captions truncated in Block #22). The description of Algorithm 1 (random filter inheritance) and practical advice (Block #11) are clear but somewhat misaligned.
- Figures and tables illustrate trends well; however, empirical setup details (α, exact layer mapping, hyperparameters) are insufficient, especially for ResNet/VGG where shapes may differ.

Contribution
- The study advances transfer learning theory by analyzing nonlinear training dynamics under partial parameter reuse, quantifying both positive and negative transfer regimes.
- The explicit dependence on α, N1/N2, σp, and ‖u‖ offers actionable insight, complementing static generalization bounds in prior work.
- The empirical validations suggest practical relevance, though the stylized theory’s assumptions limit direct translation to diverse real-world scenarios.

Strengths
- Rigorous dynamic analysis, not just static bounds (Section 4; Lemmas A.1/A.2).
- Clear identification of when transfer helps/hurts, with interpretable parameters (Proposition 4.4; Block #17; synthetic Figures 1–2).
- Strong empirical support across architectures (ResNet/VGG/ViT) and noise robustness (Figure 3; Table 1; Figure 4).
- Detailed appendices enabling self-verification of key steps (Lemma C.1, D.4/E.5).

Weaknesses
- Assumptions (orthogonality; patch assignment; small initialization; large d) reduce generality (Sections 3–4).
- Random selection of filters to inherit (Algorithm 1) diverges from practice; theory for learned selection would be more impactful.
- Some proof steps and references need cleanup for accuracy (Block #14, Block #107); “tightness” claim could be better substantiated.
- Empirical protocol lacks specifics (α per experiment, layer selection/mapping across networks, seed variability, batchnorm handling).

Questions
- Can the authors relax u ⟂ v1/v2 and noise orthogonality, and indicate which lemmas break or how constants change (Block #10)?
- How does α interact with the phase transition experimentally; can you provide curves as α varies for fixed N1/N2 and σp?
- For ResNet/VGG transfer, which layers were inherited, and how were incompatible shapes handled? Were batchnorm statistics re-estimated?
- Could the negative transfer claim be demonstrated on real data by purposely misaligning upstream tasks to reduce ‖u‖ (e.g., unrelated domains)?
- Can you reconcile the σp,1/N1 terms in Theorem E.12’s training-loss bound for Task 2 (Block #107), or provide an erratum?

Rating
- Overall (10): 7 — Solid theory and empirical corroboration, but with restrictive assumptions, random inheritance mismatch, and minor inconsistencies (Theorem 4.2; Algorithm 1; Block #14/#107; Figures 1–3).
- Novelty (10): 8 — Dynamic analysis of partial parameter transfer is novel and meaningful beyond prior static bounds (Section 4; Lemma B.2).
- Technical Quality (10): 7 — Sound approach with careful lemmas; a few reference/typo issues and strong assumptions temper robustness (Condition 4.1; Block #14/#107).
- Clarity (10): 7 — Clear structure and helpful figures; notation and empirical details need tightening (Definition B.1; Table 1; Figure captions).
- Confidence (5): 4 — High confidence in the overall assessment based on derivations and figures; minor errors noted.

---

Summary
The paper studies parameter transfer where only αm filters from an upstream two-layer ReLU CNN are inherited by a downstream model. Under a stylized data model with shared universal signal u and task-specific signals (v1, v2), the authors analyze gradient dynamics via a coefficient decomposition (u, v1, v2, per-sample noises) and show that inherited parameters act as carriers of universal knowledge. They prove a phase transition in generalization (Theorem 4.2) and provide regimes where transfer outperforms scratch training (Proposition 4.4(1)) and where negative transfer occurs due to weak universal knowledge (Proposition 4.4(2)). Synthetic and real experiments (Figures 1–4, Table 1) validate that larger N1, smaller σp,1, and stronger ‖u‖ improve downstream accuracy with transfer.

Soundness
- The theoretical framework is consistent with feature-learning literature: small initialization avoids NTK “lazy” regime (Section 2; Condition 4.1(4)), and coefficient dynamics are tracked precisely (Lemma B.2).
- The phase-transition inequalities (Theorem 4.2 Parts 2–3) scale as expected with (α² N1²‖u‖⁴/σp,1⁴ + N2²‖u+v2‖⁴/σp,2⁴)/((α²σp,2²N1)/σp,1² + N2), indicating additive contributions from universal and task-specific signals.
- The negative transfer explanation (Block #4; Block #17) is internally consistent: large upstream weights derived from strong source signals can amplify target noise if ‖u‖ is small relative to ‖u+v2‖.
- Some technical inconsistencies are present: (i) Theorem 4.3 references “Condition 3.1” (Block #14), but only Condition 4.1 is defined; (ii) Theorem E.12’s training-loss proof uses σp,1/N1 quantities for Task 2 (Block #107). These appear to be editorial oversights but should be rectified.

Presentation
- Strengths: Clear algorithms (Algorithm 1–2), careful notation for coefficient decomposition (Definition B.1), and thorough appendices with supporting lemmas (C, D, E).
- Weaknesses: Occasional notational overload and typos disrupt flow (Condition indexing; σp subscripts; mixed x̄_t/x_t notation across sections). The heatmap captions in Figure 2 (Block #22) are terse; some figures seem truncated.
- Empirical reporting: Table 1 shows robust gains with scaling N1/N2; Figures 3–4 demonstrate transfer benefits under added noise and with ViTs. However, α, layer-selection, and training details are not fully specified for real experiments.

Contribution
- The paper contributes a dynamic perspective to transfer learning theory, moving beyond static generalization bounds to explicitly characterize training trajectories and phase transitions.
- The identification of concrete levers (α, N1, σp, ‖u‖) that govern effectiveness and the first rigorous proof of negative transfer under weak universal knowledge are valuable additions.
- The empirical validation breadth across synthetic and multiple architectures (ResNet/VGG/ViT) increases relevance.

Strengths
- Comprehensive theoretical analysis with explicit update rules and concentration bounds (Lemma B.2; Lemma C.1/C.2).
- Clear phase-transition theorem and comparison to scratch training (Theorems 4.2–4.3).
- Negative transfer characterized theoretically and observed in synthetic settings (Figure 1c at ‖u‖=0; Block #17).
- Empirical evidence across varied settings and noise levels (Figure 3; Table 1; Figures 4).

Weaknesses
- Stylized data assumptions reduce realism (orthogonality; patch randomization; Definitions 3.1–3.2).
- Strong over-parameterization and small-η requirements (Condition 4.1) may limit practical regimes; “tightness” is asserted but not exhaustively defended.
- Random filter inheritance (Algorithm 1) is not how practitioners select parameters; theory for learned selection would be more impactful.
- Minor but important editorial errors (Block #14; Block #107) and missing experimental details (α, layers) affect reproducibility.

Questions
- Can the authors provide an ablation over α in synthetic and real experiments to empirically confirm α’s role predicted by Theorem 4.2?
- How do results change if u and v1/v2 are not orthogonal, or if noise has nontrivial correlations with signals (Block #10)? Which lemmas are most sensitive?
- For ViT experiments (Block #29; Figure 4), why were layers 9–11 chosen; is there empirical or theoretical rationale aligned with the universal knowledge hypothesis?
- Can you provide a corrected and detailed proof for Theorem E.12’s training-loss bound with consistent σp,2/N2 quantities?

Rating
- Overall (10): 8 — Strong theoretical advance with convincing experiments; realism limited by assumptions and minor presentation issues (Theorem 4.2; Proposition 4.4; Figures 1–4; Block #14/#107).
- Novelty (10): 9 — First comprehensive training-dynamics analysis for parameter transfer in nonlinear CNNs with partial inheritance (Section 4; Lemma B.2).
- Technical Quality (10): 8 — Solid derivations and bounds; small inconsistencies and strict assumptions prevent top marks (Condition 4.1; Block #14/#107).
- Clarity (10): 7 — Well-structured but notation-heavy; rectify reference/typo issues and add empirical details (Algorithm 1; Table 1).
- Confidence (5): 4 — High confidence based on close reading and familiarity with related theory; minor errors noted.

---

Summary
The paper proposes a theory for parameter transfer in transfer learning, modeling two related binary tasks with a universal signal u and task-specific signals v1, v2. Using a two-layer ReLU CNN, the authors transfer an α-proportion of upstream filters to the downstream model and analyze training dynamics via a decomposition of weights into signal/noise components. They prove that, under Condition 4.1, transfer can drive test error near Bayes-optimal when d is below a threshold that depends on α, N1/N2, signal strengths, and noise levels (Theorem 4.2), and otherwise yields suboptimal error (≥ 0.1). They recover baseline results without transfer (Theorem 4.3) and show when transfer helps (Proposition 4.4(1)) or hurts (Proposition 4.4(2)). Synthetic and real experiments support the theory.

Soundness
- The approach is methodical: define precise update rules (Lemma B.2), obtain concentration inequalities (Lemma C.1/C.2), and approximate cumulative learning via ODEs (Lemma C.4; Lemmas A.1/A.2; D.4/D.8).
- The phase transition is justified by bounding learned signal versus memorized noise contributions, with conditions scaling in α, sample sizes, and noise. The comparison to scratch training is grounded (Theorem 4.3).
- Potential issues verified: (i) uniqueness of coefficient decomposition relies on linear independence (Block #47/#48)—reasonable under Gaussian assumptions; (ii) reference mismatches (Condition 3.1 vs 4.1, Block #14) and σp misindexing (Block #107) indicate some editorial errors; (iii) random inheritance in Algorithm 1 could bias the downstream in ways not covered by the “strong shared features” recommendation (Block #11).

Presentation
- Clarity: The main narrative is comprehensible, but proofs are long and notation dense. Some figures have limited captions (Figure 2 in Block #22) and implementation details are missing.
- Organization: Strong—clear separation of setting, algorithms, results, proofs, and experiments. The discussion (Section 7) candidly acknowledges shallow-network focus and proposes extensions.
- Typos/inconsistencies: The aforementioned condition reference and σp index mismatch; occasional mixing y_{i,1}/y_{i,2} indices in lemmas; minor formatting issues in the appendix equations.

Contribution
- The work fills an important theoretical gap by explaining transfer learning efficacy and failure through training dynamics, not just final bounds.
- It yields actionable insights: ensure strong universal features (‖u‖), sufficient upstream data (N1), and control noise to avoid negative transfer; α matters quantitatively.
- The combination of theory with empirical validations across CNNs and ViTs increases practical relevance.

Strengths
- Dynamic, coefficient-level analysis of parameter transfer (Definition B.1; Lemma B.2), beyond static generalization literature.
- Sharp phase-transition result and clear identification of negative transfer conditions (Theorem 4.2; Proposition 4.4(2)).
- Empirical alignment with theory across synthetic and real data (Figures 1–4; Table 1; Figure 3’s noise study).
- Comprehensive appendices enabling self-check of proofs and bounds.

Weaknesses
- The data and architectural assumptions are idealized (orthogonality; two-layer CNN; small initialization), possibly limiting generalization to modern deep networks.
- Random inheritance vs practical selection is a methodological gap (Algorithm 1 vs Block #11 guidance).
- Presentation issues—condition references and index mismatches—should be fixed; empirical details (α values, layer mappings, seeds, batchnorm handling) are thin.

Questions
- How would the results change if α depends on layer depth (e.g., inheriting only early or late layers)? Can you extend bounds to layer-wise α?
- Can the authors quantify the robustness of the phase transition to moderate correlations between u and v1/v2?
- Are there experiments demonstrating negative transfer on real datasets by choosing misaligned source tasks (e.g., unrelated domains)?
- What is the computational overhead of pretraining in your real experiments, and how does that relate to the theoretical benefits through N1 terms?

Rating
- Overall (10): 7 — Valuable theory and supporting experiments, with restrictive assumptions and some presentation inconsistencies (Theorem 4.2; Algorithm 1; Block #14/#107; Figures 1–4).
- Novelty (10): 8 — Dynamic analysis of partial parameter transfer is new and meaningful (Section 4; Lemma B.2).
- Technical Quality (10): 7 — Thoughtful derivations; minor errors and strong assumptions reduce the score (Condition 4.1; Block #14/#107).
- Clarity (10): 7 — Generally clear but dense; fix references and add empirical specifics (Table 1; Figure captions; α details).
- Confidence (5): 4 — High confidence in the main claims; minor inconsistencies noted.