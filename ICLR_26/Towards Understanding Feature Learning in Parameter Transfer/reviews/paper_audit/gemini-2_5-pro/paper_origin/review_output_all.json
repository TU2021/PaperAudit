{
  "baseline_review": "1) Summary\nThe paper presents a theoretical analysis of partial parameter transfer for two-layer ReLU convolutional neural networks (CNNs) within the feature learning paradigm. The authors aim to understand the conditions under which transferring a subset of parameters from an upstream model is beneficial compared to training from scratch. The core contributions include a theoretical framework that characterizes the training dynamics, identifying key factors like shared \"universal knowledge,\" upstream sample size, and noise level. The analysis reveals a sharp phase transition for generalization performance based on these factors and the data dimension. Notably, the work provides, to the best of our knowledge, the first theoretical explanation for the phenomenon of negative transfer, where inherited parameters can degrade downstream task performance. These theoretical findings are supported by both numerical simulations on synthetic data and experiments on real-world datasets with modern architectures.2) Strengths\n*   **Novel Theoretical Framework for a Practical Problem**\n    *   The paper provides what appears to be the first theoretical analysis of the *training dynamics* of partial parameter transfer (Section 1, Introduction). This moves beyond existing work that focuses on static generalization bounds (Section 2, Related Work) and addresses a more fundamental question of how knowledge is reused during training.\n    *   The problem of partial parameter transfer is highly practical, as architectural mismatches between upstream and downstream models are common (Section 1, Introduction). Providing a theoretical foundation for this practice is a significant contribution.\n    *   The analysis is situated within the feature learning regime, which avoids the \"lazy training\" assumptions of NTK theory and is considered more representative of how real neural networks learn (Section 2, Related Work).*   **Comprehensive and Sharp Theoretical Results**\n    *   The main result, Theorem 4.2, establishes a sharp phase transition for the test error of the downstream model. It provides a precise condition on the data dimension `d` that determines whether the model achieves near Bayes-optimal error or remains sub-optimal.\n    *   The theory successfully identifies and formalizes the roles of several intuitive factors: the proportion of transferred parameters (`α`), the strength of the shared signal (`||u||_2`), the upstream sample size (`N_1`), and the upstream noise level (`σ_{p,1}`) (Theorem 4.2, Proposition 4.4).\n    *   A key strength is the theoretical characterization of negative transfer (Proposition 4.4, part 2). The paper proves that when the shared signal is weak, even a well-trained upstream model can harm downstream performance by amplifying noise, providing a rigorous explanation for this empirically observed phenomenon.*   **Thorough Empirical Validation**\n    *   The numerical experiments in Section 5 are carefully designed to directly validate the theoretical claims. Figure 1 systematically shows that downstream accuracy improves with larger `N_1`, smaller `σ_{p,1}`, and stronger `||u||_2`, perfectly aligning with the theory. The case of `||u||_2 = 0` in Figure 1c provides a clear demonstration of negative transfer.\n    *   The heatmap in Figure 2 provides a compelling visualization of the phase transition predicted by Theorem 4.2, showing a clear boundary in the (`d`, `||u||_2`) space between high and low accuracy regions.\n    *   The real-data experiments in Section 6 demonstrate that the insights are not confined to the simplified theoretical setting. The results on CIFAR-10/100 with modern architectures like ResNet, VGG (Table 1, Figure 3), and ViT (Figure 4, Appendix) show that performance consistently improves with more upstream data and that parameter transfer offers a robust advantage in the presence of target task noise.3) Weaknesses\n*   **Opaque Conditions for Negative Transfer**\n    *   The formal condition for negative transfer presented in Proposition 4.4, part 2, is mathematically dense and not intuitively explained. The condition `||u + v2||2^2 / ||u||2^2 ≥ α N1 σp,2^2 / (N2 σp,1^2) ≥ C4` involves a complex interplay of multiple factors, and its direct implications are difficult to parse without significant effort.\n    *   The paper states that the key mechanism is a weak shared signal (`||u||_2`), but the formal condition is more complex than just `||u||_2` being small. The connection between this formal condition and the clear experimental result of negative transfer when `||u||_2 = 0` (Figure 1c) is not explicitly made, leaving a gap in the argument.\n    *   The text mentions that the value of `Γ` (defined in Section 4) is not a necessary condition for failure, but it does not provide a clear, alternative intuition for when failure is guaranteed, beyond the complex inequality in Proposition 4.4.*   **Significant Gap Between Theory and Practice**\n    *   The theoretical analysis is restricted to a two-layer ReLU CNN (Section 3), whereas the real-data experiments use deep, complex architectures like ResNet-101 and DeiT-Base (Section 6). The paper acknowledges this limitation (Section 7) but does not offer much discussion on why the conclusions from the shallow model should extend to deep ones.\n    *   The theoretical data model assumes a simple additive structure of a universal signal (`u`) and a task-specific signal (`v_k`) (Definitions 3.1, 3.2). It is unclear how this maps to the complex, hierarchical features learned from real-world datasets like CIFAR-100, where the \"universal knowledge\" between, for example, classes of vehicles and classes of animals is non-trivial to define.\n    *   The theoretical parameter transfer mechanism involves randomly sampling `αm` filters to transfer (Algorithm 1). In practice, transfer learning with deep models typically involves transferring entire contiguous blocks of layers (e.g., the first `k` layers or, as in the ViT experiment, layers 9-11, Section 6), which is structurally different.*   **Clarity and Presentation of Main Results**\n    *   The main theoretical results in Section 4 are presented in a highly condensed format that may be inaccessible to readers not already expert in this specific line of theoretical work. The phase transition condition in Theorem 4.2 is a formidable fraction that obscures the underlying intuition.\n    *   While the introduction of the quantity `Γ` is helpful, the paper could benefit from more high-level, qualitative explanations of its main theorems in the body of the text to aid reader comprehension.\n    *   The appendix, containing the proofs, is extremely dense. While this suggests technical depth, its structure, with many nested propositions and lemmas across more than 20 pages (Appendix A-E), makes it very challenging to follow the main thread of the proof without a significant time investment.4) Suggestions for Improvement\n*   **Elucidate the Negative Transfer Condition**\n    *   In Section 4, following Proposition 4.4, please add a paragraph dedicated to providing an intuitive breakdown of the negative transfer condition. For instance, explaining what each ratio in the inequality represents (e.g., \"signal specificity\" vs. \"upstream task strength\") would be very helpful.\n    *   Explicitly connect the theoretical condition to the experiment in Figure 1c. A brief discussion showing that the `||u||_2 = 0` setting satisfies the conditions of Proposition 4.4, part 2 would make the link between theory and experiment more concrete.\n    *   Consider adding a simplified corollary or discussing a limiting case (e.g., as `||u||_2` approaches zero) to make the result more digestible.*   **Address the Theory-Practice Gap**\n    *   In Section 6, please add a short discussion on the conceptual mapping from the theoretical model to the real-world experiments. For example, you could hypothesize that `u` corresponds to low-level features (edges, textures) shared across all CIFAR classes, while `v_k` corresponds to higher-level, class-specific features.\n    *   In the Discussion (Section 7), briefly elaborate on why the core mechanisms identified (learning shared vs. specific signals, noise memorization) are likely to be fundamental and thus may generalize from shallow to deep networks, even if the exact mathematical forms change.\n    *   Acknowledge the difference between the random filter sampling in the theory and the block-wise layer transfer in practice, and perhaps comment on whether the current theory could be seen as an approximation of the latter.*   **Enhance Presentation and Readability**\n    *   In Section 4, after stating Theorem 4.2, please add an \"Intuition\" or \"Interpretation\" paragraph. This could explain, in words, that the numerator of the threshold represents the combined signal strength from both tasks, while the denominator represents a complexity term, making the condition a signal-to-noise/complexity ratio.\n    *   To improve the accessibility of the main results, consider adding a small table in Section 4 that summarizes the key quantities (e.g., `Γ`, the phase transition threshold) and provides a brief, intuitive description for each.\n    *   At the beginning of Appendix A (\"Proof Sketch\") and Appendix D (\"The First System\"), please add a high-level roadmap of the proof. This could be a short paragraph or a bulleted list outlining the main logical steps (e.g., 1. Decompose weights, 2. Analyze coefficient dynamics, 3. Show loss balancing, 4. Derive bounds), which would greatly help readers navigate the dense technical details.5) Score\n*   Overall (10): 8 — The paper provides a novel and rigorous theoretical analysis for a practical and important problem, with strong empirical support.\n*   Novelty (10): 9 — This work appears to be the first to analyze the training dynamics of parameter transfer and provide a theoretical account of negative transfer.\n*   Technical Quality (10): 9 — The theoretical framework is sound, building on modern feature learning analysis, and the experiments are well-designed and executed.\n*   Clarity (10): 7 — While the motivation and high-level ideas are clear, the presentation of the main theoretical results is dense and could be made more accessible.\n*   Confidence (5): 4 — I am confident in my assessment of the paper's contributions and experimental results, as I am familiar with this area of research, though I have not verified every line of the proofs in the appendix.",
  "final_review": "1) Summary\nThe paper presents a theoretical analysis of partial parameter transfer for two-layer ReLU convolutional neural networks (CNNs) within the feature learning paradigm. The authors aim to understand the conditions under which transferring a subset of parameters from an upstream model is beneficial compared to training from scratch. The core contributions include a theoretical framework that characterizes the training dynamics, identifying key factors like shared \"universal knowledge,\" upstream sample size, and noise level. The analysis reveals a sharp phase transition for generalization performance based on these factors and the data dimension. The work also provides a theoretical explanation for the phenomenon of negative transfer, where inherited parameters can degrade downstream task performance. These theoretical findings are supported by both numerical simulations on synthetic data and experiments on real-world datasets with modern architectures.2) Strengths\n*   **Novel Theoretical Framework for a Practical Problem**\n    *   The paper provides what appears to be the first theoretical analysis of the *training dynamics* of partial parameter transfer (Section 1, Introduction). This moves beyond existing work that focuses on static generalization bounds (Section 2, Related Work) and addresses a more fundamental question of how knowledge is reused during training.\n    *   The problem of partial parameter transfer is highly practical, as architectural mismatches between upstream and downstream models are common (Section 1, Introduction). Providing a theoretical foundation for this practice is a significant contribution.\n    *   The analysis is situated within the feature learning regime, which avoids the \"lazy training\" assumptions of NTK theory and is considered more representative of how real neural networks learn (Section 2, Related Work).*   **Comprehensive and Sharp Theoretical Results**\n    *   The main result, Theorem 4.2, establishes a sharp phase transition for the test error of the downstream model. It provides a precise condition on the data dimension `d` that determines whether the model achieves near Bayes-optimal error or remains sub-optimal.\n    *   The theory successfully identifies and formalizes the roles of several intuitive factors: the proportion of transferred parameters (`α`), the strength of the shared signal (`||u||_2`), the upstream sample size (`N_1`), and the upstream noise level (`σ_{p,1}`) (Theorem 4.2, Proposition 4.4).\n    *   A key strength is the theoretical characterization of negative transfer (Proposition 4.4, part 2). The paper provides a formal condition under which inherited parameters can be detrimental, offering a rigorous explanation for this empirically observed phenomenon.*   **Thorough Empirical Validation**\n    *   The numerical experiments in Section 5 are carefully designed to directly validate the theoretical claims. Figure 1 systematically shows that downstream accuracy improves with larger `N_1`, smaller `σ_{p,1}`, and stronger `||u||_2`, aligning with the theory. The case of `||u||_2 = 0` in Figure 1c provides a clear demonstration of negative transfer.\n    *   The heatmap in Figure 2 provides a compelling visualization of the phase transition predicted by Theorem 4.2, showing a clear boundary in the (`d`, `||u||_2`) space between high and low accuracy regions.\n    *   The real-data experiments in Section 6 demonstrate that the insights are not confined to the simplified theoretical setting. The results on CIFAR-10/100 with modern architectures like ResNet, VGG (Table 1, Figure 3), and ViT (Figure 4, Appendix) show that performance consistently improves with more upstream data.3) Weaknesses\n*   **Opaque Conditions and Unsubstantiated Claims for Negative Transfer**\n    *   The formal condition for negative transfer presented in Proposition 4.4, part 2, is mathematically dense and not intuitively explained. The condition `||u + v2||2^2 / ||u||2^2 ≥ α N1 σp,2^2 / (N2 σp,1^2) ≥ C4` involves a complex interplay of multiple factors, and its direct implications are difficult to parse.\n    *   The paper claims to have \"theoretically proved the existence of the negative transfer\" (Section 1, Introduction). However, the main theorems (Theorem 4.2 and 4.3) only establish conditions under which each method achieves sub-optimal error (≥0.1) independently. A direct theoretical comparison proving that the error with parameter transfer can be strictly *greater* than the error from training from scratch appears to be missing from the analysis, making this strong claim unsubstantiated.\n    *   The connection between the formal condition in Proposition 4.4 and the clear experimental result of negative transfer when `||u||_2 = 0` (Figure 1c) is not explicitly made, leaving a gap in the argument.*   **Significant Gap and Contradiction Between Theory and Practice**\n    *   The theoretical analysis is restricted to a two-layer ReLU CNN (Section 3), whereas the real-data experiments use deep architectures like ResNet-101 and DeiT-Base (Section 6). The paper acknowledges this limitation (Section 7) but does not offer much discussion on why the conclusions from the shallow model should extend to deep ones.\n    *   There is an apparent contradiction between a key experimental result and the intuition from the theory. The experiment on varying downstream noise `σ_{p,2}` (Section 6, Figure 3) shows that the performance advantage of parameter transfer *widens* with more noise. However, the main theoretical condition for successful transfer relies on the quantity `Γ`, which is inversely proportional to `σ_{p,2}²` (Section 4), suggesting that transfer should become *less* beneficial with more downstream noise. The paper does not address this discrepancy.\n    *   The theoretical parameter transfer mechanism involves randomly sampling `αm` filters (Algorithm 1), while in practice, transfer with deep models typically involves entire contiguous blocks of layers (e.g., layers 9-11 for ViT in Section 6), which is structurally different.*   **Clarity and Presentation of Main Results**\n    *   The main theoretical results in Section 4 are presented in a highly condensed format. The phase transition condition in Theorem 4.2 is a formidable fraction that obscures the underlying intuition.\n    *   The appendix, containing the proofs, is extremely dense and contains inconsistencies that undermine confidence. For instance, Lemma A.2 (Appendix A) and its full version, Lemma E.9 (Appendix E), present conflicting constant terms (`c^D` vs. `c̄^D`) in the bounds for the downstream signal-learning coefficients, making the derivation difficult to verify.\n    *   There are minor but distracting errors in the text, such as inconsistent notation for training duration (`T*` in Algorithms 1 & 2 vs. `T1`, `T2` in Section 5) and an incorrect cross-reference in Theorem 4.3 (citing \"Condition 3.1\" which does not exist).4) Suggestions for Improvement\n*   **Elucidate and Substantiate the Negative Transfer Condition**\n    *   In Section 4, following Proposition 4.4, please add a paragraph dedicated to providing an intuitive breakdown of the negative transfer condition. Explaining what each ratio in the inequality represents would be helpful.\n    *   Please clarify the claim of having \"proved\" negative transfer. Either provide a direct comparative proof showing conditions where `error_PT > error_scratch`, or soften the claims in Section 1 and Proposition 4.4 to more accurately reflect that the theory identifies conditions for PT to perform poorly, not necessarily worse than the alternative.\n    *   Explicitly connect the theoretical condition to the experiment in Figure 1c. A brief discussion showing that the `||u||_2 = 0` setting satisfies the conditions of Proposition 4.4, part 2 would make the link between theory and experiment more concrete.*   **Address the Theory-Practice Gap and Contradiction**\n    *   In Section 7 (Discussion), briefly elaborate on why the core mechanisms identified (learning shared vs. specific signals, noise memorization) are likely to be fundamental and thus may generalize from shallow to deep networks.\n    *   Please discuss the apparent contradiction regarding the effect of downstream noise `σ_{p,2}`. An explanation for why the experimental results in Figure 3 diverge from the intuition provided by the theoretical quantity `Γ` would significantly strengthen the paper.\n    *   Acknowledge the difference between the random filter sampling in the theory and the block-wise layer transfer in practice, and perhaps comment on whether the current theory could be seen as an approximation of the latter.*   **Enhance Presentation and Readability**\n    *   In Section 4, after stating Theorem 4.2, please add an \"Intuition\" or \"Interpretation\" paragraph. This could explain, in words, the high-level meaning of the phase transition threshold.\n    *   Please carefully verify the proofs in the appendix for correctness and consistency, paying special attention to the bounds and constants presented in Lemmas A.2 and E.9.\n    *   Please correct the minor notational and cross-referencing errors throughout the manuscript (e.g., `T*` vs. `T1/T2`, \"Condition 3.1\" in Theorem 4.3).5) Score\n*   Overall (10): 7 — The paper tackles an important problem with a novel theoretical approach, but the technical quality is undermined by an apparent contradiction in the experiments and potential errors in the proofs.\n*   Novelty (10): 8 — The work is among the first to analyze the training dynamics of parameter transfer and formally model negative transfer, though the strength of the latter claim is questionable.\n*   Technical Quality (10): 7 — The theoretical framework is ambitious, but its credibility is weakened by an unexplained contradiction between theory and experiments (Figure 3 vs. `Γ` definition) and inconsistencies in the appendix proofs (Lemma A.2 vs. E.9).\n*   Clarity (10): 7 — While the high-level ideas are clear, the presentation of theoretical results is dense, and minor errors (Theorem 4.3) and inconsistencies (Appendix E.9) hinder comprehension and verification.\n*   Confidence (5): 4 — I am confident in my assessment of the paper's contributions and experimental results, as I am familiar with this area of research, though I have not verified every line of the proofs in the appendix.",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 8,
        "novelty": 9,
        "technical_quality": 9,
        "clarity": 7,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 7,
        "novelty": 8,
        "technical_quality": 7,
        "clarity": 7,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "1) Summary\nThe paper presents a theoretical analysis of partial parameter transfer for two-layer ReLU convolutional neural networks (CNNs) within the feature learning paradigm. The authors aim to understand the conditions under which transferring a subset of parameters from an upstream model is beneficial compared to training from scratch. The core contributions include a theoretical framework that characterizes the training dynamics, identifying key factors like shared \"universal knowledge,\" upstream sample size, and noise level. The analysis reveals a sharp phase transition for generalization performance based on these factors and the data dimension. The work also provides a theoretical explanation for the phenomenon of negative transfer, where inherited parameters can degrade downstream task performance. These theoretical findings are supported by both numerical simulations on synthetic data and experiments on real-world datasets with modern architectures.2) Strengths\n*   **Novel Theoretical Framework for a Practical Problem**\n    *   The paper provides what appears to be the first theoretical analysis of the *training dynamics* of partial parameter transfer (Section 1, Introduction). This moves beyond existing work that focuses on static generalization bounds (Section 2, Related Work) and addresses a more fundamental question of how knowledge is reused during training.\n    *   The problem of partial parameter transfer is highly practical, as architectural mismatches between upstream and downstream models are common (Section 1, Introduction). Providing a theoretical foundation for this practice is a significant contribution.\n    *   The analysis is situated within the feature learning regime, which avoids the \"lazy training\" assumptions of NTK theory and is considered more representative of how real neural networks learn (Section 2, Related Work).*   **Comprehensive and Sharp Theoretical Results**\n    *   The main result, Theorem 4.2, establishes a sharp phase transition for the test error of the downstream model. It provides a precise condition on the data dimension `d` that determines whether the model achieves near Bayes-optimal error or remains sub-optimal.\n    *   The theory successfully identifies and formalizes the roles of several intuitive factors: the proportion of transferred parameters (`α`), the strength of the shared signal (`||u||_2`), the upstream sample size (`N_1`), and the upstream noise level (`σ_{p,1}`) (Theorem 4.2, Proposition 4.4).\n    *   A key strength is the theoretical characterization of negative transfer (Proposition 4.4, part 2). The paper provides a formal condition under which inherited parameters can be detrimental, offering a rigorous explanation for this empirically observed phenomenon.*   **Thorough Empirical Validation**\n    *   The numerical experiments in Section 5 are carefully designed to directly validate the theoretical claims. Figure 1 systematically shows that downstream accuracy improves with larger `N_1`, smaller `σ_{p,1}`, and stronger `||u||_2`, aligning with the theory. The case of `||u||_2 = 0` in Figure 1c provides a clear demonstration of negative transfer.\n    *   The heatmap in Figure 2 provides a compelling visualization of the phase transition predicted by Theorem 4.2, showing a clear boundary in the (`d`, `||u||_2`) space between high and low accuracy regions.\n    *   The real-data experiments in Section 6 demonstrate that the insights are not confined to the simplified theoretical setting. The results on CIFAR-10/100 with modern architectures like ResNet, VGG (Table 1, Figure 3), and ViT (Figure 4, Appendix) show that performance consistently improves with more upstream data.3) Weaknesses\n*   **Opaque Conditions and Unsubstantiated Claims for Negative Transfer**\n    *   The formal condition for negative transfer presented in Proposition 4.4, part 2, is mathematically dense and not intuitively explained. The condition `||u + v2||2^2 / ||u||2^2 ≥ α N1 σp,2^2 / (N2 σp,1^2) ≥ C4` involves a complex interplay of multiple factors, and its direct implications are difficult to parse.\n    *   The paper claims to have \"theoretically proved the existence of the negative transfer\" (Section 1, Introduction). However, the main theorems (Theorem 4.2 and 4.3) only establish conditions under which each method achieves sub-optimal error (≥0.1) independently. A direct theoretical comparison proving that the error with parameter transfer can be strictly *greater* than the error from training from scratch appears to be missing from the analysis, making this strong claim unsubstantiated.\n    *   The connection between the formal condition in Proposition 4.4 and the clear experimental result of negative transfer when `||u||_2 = 0` (Figure 1c) is not explicitly made, leaving a gap in the argument.*   **Significant Gap and Contradiction Between Theory and Practice**\n    *   The theoretical analysis is restricted to a two-layer ReLU CNN (Section 3), whereas the real-data experiments use deep architectures like ResNet-101 and DeiT-Base (Section 6). The paper acknowledges this limitation (Section 7) but does not offer much discussion on why the conclusions from the shallow model should extend to deep ones.\n    *   There is an apparent contradiction between a key experimental result and the intuition from the theory. The experiment on varying downstream noise `σ_{p,2}` (Section 6, Figure 3) shows that the performance advantage of parameter transfer *widens* with more noise. However, the main theoretical condition for successful transfer relies on the quantity `Γ`, which is inversely proportional to `σ_{p,2}²` (Section 4), suggesting that transfer should become *less* beneficial with more downstream noise. The paper does not address this discrepancy.\n    *   The theoretical parameter transfer mechanism involves randomly sampling `αm` filters (Algorithm 1), while in practice, transfer with deep models typically involves entire contiguous blocks of layers (e.g., layers 9-11 for ViT in Section 6), which is structurally different.*   **Clarity and Presentation of Main Results**\n    *   The main theoretical results in Section 4 are presented in a highly condensed format. The phase transition condition in Theorem 4.2 is a formidable fraction that obscures the underlying intuition.\n    *   The appendix, containing the proofs, is extremely dense and contains inconsistencies that undermine confidence. For instance, Lemma A.2 (Appendix A) and its full version, Lemma E.9 (Appendix E), present conflicting constant terms (`c^D` vs. `c̄^D`) in the bounds for the downstream signal-learning coefficients, making the derivation difficult to verify.\n    *   There are minor but distracting errors in the text, such as inconsistent notation for training duration (`T*` in Algorithms 1 & 2 vs. `T1`, `T2` in Section 5) and an incorrect cross-reference in Theorem 4.3 (citing \"Condition 3.1\" which does not exist).4) Suggestions for Improvement\n*   **Elucidate and Substantiate the Negative Transfer Condition**\n    *   In Section 4, following Proposition 4.4, please add a paragraph dedicated to providing an intuitive breakdown of the negative transfer condition. Explaining what each ratio in the inequality represents would be helpful.\n    *   Please clarify the claim of having \"proved\" negative transfer. Either provide a direct comparative proof showing conditions where `error_PT > error_scratch`, or soften the claims in Section 1 and Proposition 4.4 to more accurately reflect that the theory identifies conditions for PT to perform poorly, not necessarily worse than the alternative.\n    *   Explicitly connect the theoretical condition to the experiment in Figure 1c. A brief discussion showing that the `||u||_2 = 0` setting satisfies the conditions of Proposition 4.4, part 2 would make the link between theory and experiment more concrete.*   **Address the Theory-Practice Gap and Contradiction**\n    *   In Section 7 (Discussion), briefly elaborate on why the core mechanisms identified (learning shared vs. specific signals, noise memorization) are likely to be fundamental and thus may generalize from shallow to deep networks.\n    *   Please discuss the apparent contradiction regarding the effect of downstream noise `σ_{p,2}`. An explanation for why the experimental results in Figure 3 diverge from the intuition provided by the theoretical quantity `Γ` would significantly strengthen the paper.\n    *   Acknowledge the difference between the random filter sampling in the theory and the block-wise layer transfer in practice, and perhaps comment on whether the current theory could be seen as an approximation of the latter.*   **Enhance Presentation and Readability**\n    *   In Section 4, after stating Theorem 4.2, please add an \"Intuition\" or \"Interpretation\" paragraph. This could explain, in words, the high-level meaning of the phase transition threshold.\n    *   Please carefully verify the proofs in the appendix for correctness and consistency, paying special attention to the bounds and constants presented in Lemmas A.2 and E.9.\n    *   Please correct the minor notational and cross-referencing errors throughout the manuscript (e.g., `T*` vs. `T1/T2`, \"Condition 3.1\" in Theorem 4.3).5) Score\n*   Overall (10): 7 — The paper tackles an important problem with a novel theoretical approach, but the technical quality is undermined by an apparent contradiction in the experiments and potential errors in the proofs.\n*   Novelty (10): 8 — The work is among the first to analyze the training dynamics of parameter transfer and formally model negative transfer, though the strength of the latter claim is questionable.\n*   Technical Quality (10): 7 — The theoretical framework is ambitious, but its credibility is weakened by an unexplained contradiction between theory and experiments (Figure 3 vs. `Γ` definition) and inconsistencies in the appendix proofs (Lemma A.2 vs. E.9).\n*   Clarity (10): 7 — While the high-level ideas are clear, the presentation of theoretical results is dense, and minor errors (Theorem 4.3) and inconsistencies (Appendix E.9) hinder comprehension and verification.\n*   Confidence (5): 4 — I am confident in my assessment of the paper's contributions and experimental results, as I am familiar with this area of research, though I have not verified every line of the proofs in the appendix."
}