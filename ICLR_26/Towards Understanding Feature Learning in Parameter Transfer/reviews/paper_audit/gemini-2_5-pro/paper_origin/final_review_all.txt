1) Summary
The paper presents a theoretical analysis of partial parameter transfer for two-layer ReLU convolutional neural networks (CNNs) within the feature learning paradigm. The authors aim to understand the conditions under which transferring a subset of parameters from an upstream model is beneficial compared to training from scratch. The core contributions include a theoretical framework that characterizes the training dynamics, identifying key factors like shared "universal knowledge," upstream sample size, and noise level. The analysis reveals a sharp phase transition for generalization performance based on these factors and the data dimension. The work also provides a theoretical explanation for the phenomenon of negative transfer, where inherited parameters can degrade downstream task performance. These theoretical findings are supported by both numerical simulations on synthetic data and experiments on real-world datasets with modern architectures.2) Strengths
*   **Novel Theoretical Framework for a Practical Problem**
    *   The paper provides what appears to be the first theoretical analysis of the *training dynamics* of partial parameter transfer (Section 1, Introduction). This moves beyond existing work that focuses on static generalization bounds (Section 2, Related Work) and addresses a more fundamental question of how knowledge is reused during training.
    *   The problem of partial parameter transfer is highly practical, as architectural mismatches between upstream and downstream models are common (Section 1, Introduction). Providing a theoretical foundation for this practice is a significant contribution.
    *   The analysis is situated within the feature learning regime, which avoids the "lazy training" assumptions of NTK theory and is considered more representative of how real neural networks learn (Section 2, Related Work).*   **Comprehensive and Sharp Theoretical Results**
    *   The main result, Theorem 4.2, establishes a sharp phase transition for the test error of the downstream model. It provides a precise condition on the data dimension `d` that determines whether the model achieves near Bayes-optimal error or remains sub-optimal.
    *   The theory successfully identifies and formalizes the roles of several intuitive factors: the proportion of transferred parameters (`α`), the strength of the shared signal (`||u||_2`), the upstream sample size (`N_1`), and the upstream noise level (`σ_{p,1}`) (Theorem 4.2, Proposition 4.4).
    *   A key strength is the theoretical characterization of negative transfer (Proposition 4.4, part 2). The paper provides a formal condition under which inherited parameters can be detrimental, offering a rigorous explanation for this empirically observed phenomenon.*   **Thorough Empirical Validation**
    *   The numerical experiments in Section 5 are carefully designed to directly validate the theoretical claims. Figure 1 systematically shows that downstream accuracy improves with larger `N_1`, smaller `σ_{p,1}`, and stronger `||u||_2`, aligning with the theory. The case of `||u||_2 = 0` in Figure 1c provides a clear demonstration of negative transfer.
    *   The heatmap in Figure 2 provides a compelling visualization of the phase transition predicted by Theorem 4.2, showing a clear boundary in the (`d`, `||u||_2`) space between high and low accuracy regions.
    *   The real-data experiments in Section 6 demonstrate that the insights are not confined to the simplified theoretical setting. The results on CIFAR-10/100 with modern architectures like ResNet, VGG (Table 1, Figure 3), and ViT (Figure 4, Appendix) show that performance consistently improves with more upstream data.3) Weaknesses
*   **Opaque Conditions and Unsubstantiated Claims for Negative Transfer**
    *   The formal condition for negative transfer presented in Proposition 4.4, part 2, is mathematically dense and not intuitively explained. The condition `||u + v2||2^2 / ||u||2^2 ≥ α N1 σp,2^2 / (N2 σp,1^2) ≥ C4` involves a complex interplay of multiple factors, and its direct implications are difficult to parse.
    *   The paper claims to have "theoretically proved the existence of the negative transfer" (Section 1, Introduction). However, the main theorems (Theorem 4.2 and 4.3) only establish conditions under which each method achieves sub-optimal error (≥0.1) independently. A direct theoretical comparison proving that the error with parameter transfer can be strictly *greater* than the error from training from scratch appears to be missing from the analysis, making this strong claim unsubstantiated.
    *   The connection between the formal condition in Proposition 4.4 and the clear experimental result of negative transfer when `||u||_2 = 0` (Figure 1c) is not explicitly made, leaving a gap in the argument.*   **Significant Gap and Contradiction Between Theory and Practice**
    *   The theoretical analysis is restricted to a two-layer ReLU CNN (Section 3), whereas the real-data experiments use deep architectures like ResNet-101 and DeiT-Base (Section 6). The paper acknowledges this limitation (Section 7) but does not offer much discussion on why the conclusions from the shallow model should extend to deep ones.
    *   There is an apparent contradiction between a key experimental result and the intuition from the theory. The experiment on varying downstream noise `σ_{p,2}` (Section 6, Figure 3) shows that the performance advantage of parameter transfer *widens* with more noise. However, the main theoretical condition for successful transfer relies on the quantity `Γ`, which is inversely proportional to `σ_{p,2}²` (Section 4), suggesting that transfer should become *less* beneficial with more downstream noise. The paper does not address this discrepancy.
    *   The theoretical parameter transfer mechanism involves randomly sampling `αm` filters (Algorithm 1), while in practice, transfer with deep models typically involves entire contiguous blocks of layers (e.g., layers 9-11 for ViT in Section 6), which is structurally different.*   **Clarity and Presentation of Main Results**
    *   The main theoretical results in Section 4 are presented in a highly condensed format. The phase transition condition in Theorem 4.2 is a formidable fraction that obscures the underlying intuition.
    *   The appendix, containing the proofs, is extremely dense and contains inconsistencies that undermine confidence. For instance, Lemma A.2 (Appendix A) and its full version, Lemma E.9 (Appendix E), present conflicting constant terms (`c^D` vs. `c̄^D`) in the bounds for the downstream signal-learning coefficients, making the derivation difficult to verify.
    *   There are minor but distracting errors in the text, such as inconsistent notation for training duration (`T*` in Algorithms 1 & 2 vs. `T1`, `T2` in Section 5) and an incorrect cross-reference in Theorem 4.3 (citing "Condition 3.1" which does not exist).4) Suggestions for Improvement
*   **Elucidate and Substantiate the Negative Transfer Condition**
    *   In Section 4, following Proposition 4.4, please add a paragraph dedicated to providing an intuitive breakdown of the negative transfer condition. Explaining what each ratio in the inequality represents would be helpful.
    *   Please clarify the claim of having "proved" negative transfer. Either provide a direct comparative proof showing conditions where `error_PT > error_scratch`, or soften the claims in Section 1 and Proposition 4.4 to more accurately reflect that the theory identifies conditions for PT to perform poorly, not necessarily worse than the alternative.
    *   Explicitly connect the theoretical condition to the experiment in Figure 1c. A brief discussion showing that the `||u||_2 = 0` setting satisfies the conditions of Proposition 4.4, part 2 would make the link between theory and experiment more concrete.*   **Address the Theory-Practice Gap and Contradiction**
    *   In Section 7 (Discussion), briefly elaborate on why the core mechanisms identified (learning shared vs. specific signals, noise memorization) are likely to be fundamental and thus may generalize from shallow to deep networks.
    *   Please discuss the apparent contradiction regarding the effect of downstream noise `σ_{p,2}`. An explanation for why the experimental results in Figure 3 diverge from the intuition provided by the theoretical quantity `Γ` would significantly strengthen the paper.
    *   Acknowledge the difference between the random filter sampling in the theory and the block-wise layer transfer in practice, and perhaps comment on whether the current theory could be seen as an approximation of the latter.*   **Enhance Presentation and Readability**
    *   In Section 4, after stating Theorem 4.2, please add an "Intuition" or "Interpretation" paragraph. This could explain, in words, the high-level meaning of the phase transition threshold.
    *   Please carefully verify the proofs in the appendix for correctness and consistency, paying special attention to the bounds and constants presented in Lemmas A.2 and E.9.
    *   Please correct the minor notational and cross-referencing errors throughout the manuscript (e.g., `T*` vs. `T1/T2`, "Condition 3.1" in Theorem 4.3).5) Score
*   Overall (10): 7 — The paper tackles an important problem with a novel theoretical approach, but the technical quality is undermined by an apparent contradiction in the experiments and potential errors in the proofs.
*   Novelty (10): 8 — The work is among the first to analyze the training dynamics of parameter transfer and formally model negative transfer, though the strength of the latter claim is questionable.
*   Technical Quality (10): 7 — The theoretical framework is ambitious, but its credibility is weakened by an unexplained contradiction between theory and experiments (Figure 3 vs. `Γ` definition) and inconsistencies in the appendix proofs (Lemma A.2 vs. E.9).
*   Clarity (10): 7 — While the high-level ideas are clear, the presentation of theoretical results is dense, and minor errors (Theorem 4.3) and inconsistencies (Appendix E.9) hinder comprehension and verification.
*   Confidence (5): 4 — I am confident in my assessment of the paper's contributions and experimental results, as I am familiar with this area of research, though I have not verified every line of the proofs in the appendix.