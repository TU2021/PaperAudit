# Global Summary
This paper provides a theoretical analysis of the training dynamics of partial parameter transfer in transfer learning. The study focuses on a setting where both upstream and downstream models are two-layer ReLU convolutional neural networks (CNNs). The core of the work is a theoretical framework that models the transfer of an `α`-proportion of weights from a source task (Task 1) to a target task (Task 2). The theory characterizes the conditions under which this transfer is beneficial, leading to near Bayes-optimal test error, versus when it is detrimental (negative transfer). Key factors identified are the strength of the shared signal (`universal knowledge`), the source task sample size, and the source task noise level. The main theoretical result is a phase transition theorem showing that test error depends critically on the relationship between data dimension `d` and a combination of these factors. For instance, with parameter transfer, near-optimal error is achieved if `d` is below a certain threshold that scales with `(α^2 N_1^2 ||u||_2^4 / σ_{p,1}^4 + N_2^2 ||u+v_2||_2^4 / σ_{p,2}^4) / (α^2 σ_{p,2}^2 N_1 / σ_{p,1}^2 + N_2)`. The theory is validated with numerical experiments on synthetic data and real-data experiments on CIFAR-10/100 using ResNet, VGG, and Vision Transformer architectures. A key limitation stated by the authors is that the theory is developed for shallow networks.

# Introduction
- The paper addresses the lack of theoretical understanding of the training dynamics in partial parameter transfer, where only a subset of weights is reused.
- It aims to answer two questions: (i) why parameter transfer can improve test performance over random initialization, and (ii) why it can sometimes lead to negative transfer.
- The analysis is conducted within a nonlinear dynamical system framework for two-layer ReLU CNNs.
- The framework models a shared "universal knowledge" signal between source and target tasks and assumes an `α`-proportion of weights are inherited.
- The analysis focuses on three crucial factors: (1) the universal knowledge between tasks, (2) the upstream model's training sample size, and (3) the noise level in the source task.
- Contributions are stated as:
    - First theoretical framework for parameter transfer, proving a sharp phase transition for the test error between near Bayes optimal and a constant error.
    - Theoretical explanation for when parameter transfer outperforms training from scratch, identifying the roles of universal knowledge norm, source sample size, and source noise level.
    - Theoretical proof for the existence of negative transfer, which occurs when the shared signal is weak, causing the transferred large-norm weights to amplify task-specific noise.

# Related Work
- The paper situates its work within two main areas: Transfer Learning (TL) and the analysis of neural network dynamics.
- It briefly reviews various TL application papers in heterogeneous domain adaptation, pricing, and biology.
- It distinguishes its theoretical approach from the Neural Tangent Kernel (NTK) regime, which describes "lazy training" in highly over-parameterized networks where weights stay close to initialization.
- The paper adopts a "feature learning" perspective, which uses small initializations, allows for significant weight movement, and analyzes highly nonlinear dynamics, considered closer to real neural networks.

# Preliminaries
- The problem is set up with two tasks, Task 1 (upstream) and Task 2 (downstream).
- Data Generation:
    - Data points `(x, y)` in both tasks have input `x` in `ℝ^(2d)` and label `y` in `{±1}`.
    - The input `x` is composed of two `d`-dimensional patches, one containing a signal and the other noise.
    - Task 1 signal is `y * (u + v_1)`, where `u` is a shared "universal" signal vector and `v_1` is task-specific. Noise is `ξ` from `N(0, σ_{p,1}^2 * I_proj)`.
    - Task 2 signal is `y * (u + v_2)`, with task-specific vector `v_2`. Noise is `ξ` from `N(0, σ_{p,2}^2 * I_proj)`.
    - `u`, `v_1`, and `v_2` are mutually orthogonal.
- Network Model:
    - A two-layer ReLU CNN is used for both tasks.
    - The network has `m` convolutional filters and is defined as `f(W; x) = F_{+1}(W; x) - F_{-1}(W; x)`.
    - `F_j(W; x) = (1/m) Σ_{r=1 to m} [σ(⟨w_{j,r}, x^(1)⟩) + σ(⟨w_{j,r}, x^(2)⟩)]`.
- Training:
    - The loss function is cross-entropy: `ℓ(z) = log(1 + exp(-z))`.
    - Training losses `L_Task1` and `L_Task2` are defined over sample sizes `N_1` and `N_2`, respectively.

# Method
- Algorithm 1 (Parameter Transfer):
    1. Train upstream model `f^A` on Task 1 data for `T*` iterations.
    2. Initialize downstream model `f^D`: `αm` filters get weights from `f^A`, and the remaining `(1-α)m` filters are randomly initialized from `N(0, σ_0^2)`.
    3. Train `f^D` on Task 2 data for `T*` iterations.
- Algorithm 2 (Standard Training):
    1. Initialize downstream model `f^D` with all weights from `N(0, σ_0^2)`.
    2. Train `f^D` on Task 2 data for `T*` iterations.
- Main Theoretical Results:
    - Condition 4.1 specifies assumptions on dimension `d`, sample sizes `n=max(N1, N2)`, width `m`, initialization scale `σ_0`, and learning rate `η`. For example, `d` must be large: `d = Ω̃(max{nσ_p⁻²||u+v||², n²})`, and `σ_0` must be small.
    - Theorem 4.2 (With parameter transfer): Shows a phase transition.
        - If `d ≤ C_1 * (term)`, the test error achieves Bayesian optimal. The term is `(α²N₁²||u||₂⁴/σ_{p,1}⁴ + N₂²||u+v₂||₂⁴/σ_{p,2}⁴) / (α²σ_{p,2}²N₁/σ_{p,1}² + N₂)`.
        - If `d ≥ C_3 * (term)`, the test error is sub-optimal (`≥ 0.1`).
    - Theorem 4.3 (Without parameter transfer, from Kou et al., 2023): Also shows a phase transition.
        - If `N₂||u+v₂||₂⁴ ≥ C₁'σ_{p,2}⁴d`, the test error is near Bayes-optimal.
        - If `N₂||u+v₂||₂⁴ ≤ C₃'σ_{p,2}⁴d`, the test error is sub-optimal (`≥ 0.1`).
    - Proposition 4.4 (Comparison):
        - Defines a key quantity `Γ = (α²N₁||u||₂⁴)/(σ_{p,1}²σ_{p,2}²d)`.
        - If `Γ` is large and the standard training condition for optimality is not met (`d > C₁'(N₂||u+v₂||₂⁴)/σ_{p,2}⁴`), parameter transfer improves performance from sub-optimal to near-optimal.
        - Negative transfer occurs when universal knowledge is small, specifically when `||u+v₂||₂²/||u||₂² ≥ αN₁σ_{p,2}²/(N₂σ_{p,1}²) ≥ C₄` for a large constant `C₄`.

# Experiments
- Numerical Experiments (Synthesized Data):
    - Setup: `d=2000`, `m=40` filters, `η=0.01`, `σ₀=0.01`. Upstream trained for `T₁=800` epochs, downstream for `T₂=400` epochs. Test set size is 1000.
    - Figure 1 shows that test accuracy with parameter transfer improves with:
        - (a) Larger source sample size `N₁`.
        - (b) Lower source noise level `σ_{p,1}`.
        - (c) Stronger universal signal `||u||₂`.
    - Negative transfer is observed in Figure 1(c) when `||u||₂=0`, where performance is worse than training from scratch.
    - Figure 2 shows a heatmap of test accuracy vs. `d` and `||u||₂`, empirically demonstrating the phase transition predicted by Theorem 4.2. Accuracy is high for large `||u||₂` or small `d`.
- Real Data Experiments:
    - Datasets: CIFAR-10 and CIFAR-100.
    - Architectures: ResNet-101/VGG-16 (upstream), ResNet-34/50/VGG-11/13 (downstream).
    - Varying `N₁`: Task 2 is 2 classes (CIFAR-10) or 20 classes (CIFAR-100). Task 1 uses `k` other classes. Table 1 shows that as `N₁/N₂` increases from 2 to 4, downstream accuracy consistently improves. For ResNet-101/34 on CIFAR-100, accuracy gain over baseline goes from 2.6% (`N₁/N₂=2`) to 12% (`N₁/N₂=4`).
    - Varying `σ_{p,2}`: Gaussian noise is added to Task 2 images. Figure 3 shows that parameter transfer consistently outperforms training from scratch, and the performance gap widens as noise increases from `σ_{p,2}=1` to `20`.
    - Vision Transformers: DeiT-Base pretrained on ImageNet-2012 (81.8% acc) is the upstream model. Layers 9, 10, 11 are transferred. Downstream models are fine-tuned on CIFAR-10/100. Figure 4 (in Appendix) shows parameter transfer ("VanillaLG") outperforms training from scratch.

# Discussion
- The paper summarizes its contribution as a rigorous theoretical analysis of parameter transfer in a two-layer ReLU CNN, identifying the crucial roles of universal signal strength, source sample size, and source noise level.
- The theoretical findings are stated to be supported by both numerical simulations and real-world experiments on CIFAR-10/100 with ResNet, VGG, and ViT models.
- A key limitation acknowledged is the focus on shallow neural networks. The authors argue this is a necessary first step for building a theoretical foundation.
- Future research directions proposed include:
    - Extending the analysis to deep neural networks.
    - Designing regularization techniques to guide the selection of more effective weights for transfer, instead of random selection.

# References
- This section contains the bibliography for the paper.

# Appendix
- The appendix provides the proof sketch and detailed derivations for the main theoretical results.
- The proof strategy involves decomposing the filter weights `w` into signal-learning coefficients (`γ`) and noise-memorization coefficients (`ρ`) for the universal signal `u`, task-specific signals `v₁`, `v₂`, and noise vectors `ξ`.
- The analysis tracks the dynamics of these coefficients through two systems: the upstream training (System 1) and the downstream training (System 2).
- Key technical results include Lemma A.1 and Lemma A.2, which provide bounds on the growth of the `γ` and `ρ` coefficients in each system.
- The appendix also contains additional experimental results for Vision Transformers (Figure 4), showing that a DeiT-Base model with transferred layers ("VanillaLG") outperforms a scratch-trained model on both CIFAR-10 and CIFAR-100.
- A section discusses prior work in transfer learning theory, mentioning concepts like bias-learning, representation-based transfer, and OOD robustness.