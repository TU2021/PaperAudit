1) Summary
The paper presents a theoretical analysis of partial parameter transfer for two-layer ReLU convolutional neural networks (CNNs) within the feature learning paradigm. The authors aim to understand the conditions under which transferring a subset of parameters from an upstream model is beneficial compared to training from scratch. The core contributions include a theoretical framework that characterizes the training dynamics, identifying key factors like shared "universal knowledge," upstream sample size, and noise level. The analysis reveals a sharp phase transition for generalization performance based on these factors and the data dimension. Notably, the work provides, to the best of our knowledge, the first theoretical explanation for the phenomenon of negative transfer, where inherited parameters can degrade downstream task performance. These theoretical findings are supported by both numerical simulations on synthetic data and experiments on real-world datasets with modern architectures.2) Strengths
*   **Novel Theoretical Framework for a Practical Problem**
    *   The paper provides what appears to be the first theoretical analysis of the *training dynamics* of partial parameter transfer (Section 1, Introduction). This moves beyond existing work that focuses on static generalization bounds (Section 2, Related Work) and addresses a more fundamental question of how knowledge is reused during training.
    *   The problem of partial parameter transfer is highly practical, as architectural mismatches between upstream and downstream models are common (Section 1, Introduction). Providing a theoretical foundation for this practice is a significant contribution.
    *   The analysis is situated within the feature learning regime, which avoids the "lazy training" assumptions of NTK theory and is considered more representative of how real neural networks learn (Section 2, Related Work).*   **Comprehensive and Sharp Theoretical Results**
    *   The main result, Theorem 4.2, establishes a sharp phase transition for the test error of the downstream model. It provides a precise condition on the data dimension `d` that determines whether the model achieves near Bayes-optimal error or remains sub-optimal.
    *   The theory successfully identifies and formalizes the roles of several intuitive factors: the proportion of transferred parameters (`α`), the strength of the shared signal (`||u||_2`), the upstream sample size (`N_1`), and the upstream noise level (`σ_{p,1}`) (Theorem 4.2, Proposition 4.4).
    *   A key strength is the theoretical characterization of negative transfer (Proposition 4.4, part 2). The paper proves that when the shared signal is weak, even a well-trained upstream model can harm downstream performance by amplifying noise, providing a rigorous explanation for this empirically observed phenomenon.*   **Thorough Empirical Validation**
    *   The numerical experiments in Section 5 are carefully designed to directly validate the theoretical claims. Figure 1 systematically shows that downstream accuracy improves with larger `N_1`, smaller `σ_{p,1}`, and stronger `||u||_2`, perfectly aligning with the theory. The case of `||u||_2 = 0` in Figure 1c provides a clear demonstration of negative transfer.
    *   The heatmap in Figure 2 provides a compelling visualization of the phase transition predicted by Theorem 4.2, showing a clear boundary in the (`d`, `||u||_2`) space between high and low accuracy regions.
    *   The real-data experiments in Section 6 demonstrate that the insights are not confined to the simplified theoretical setting. The results on CIFAR-10/100 with modern architectures like ResNet, VGG (Table 1, Figure 3), and ViT (Figure 4, Appendix) show that performance consistently improves with more upstream data and that parameter transfer offers a robust advantage in the presence of target task noise.3) Weaknesses
*   **Opaque Conditions for Negative Transfer**
    *   The formal condition for negative transfer presented in Proposition 4.4, part 2, is mathematically dense and not intuitively explained. The condition `||u + v2||2^2 / ||u||2^2 ≥ α N1 σp,2^2 / (N2 σp,1^2) ≥ C4` involves a complex interplay of multiple factors, and its direct implications are difficult to parse without significant effort.
    *   The paper states that the key mechanism is a weak shared signal (`||u||_2`), but the formal condition is more complex than just `||u||_2` being small. The connection between this formal condition and the clear experimental result of negative transfer when `||u||_2 = 0` (Figure 1c) is not explicitly made, leaving a gap in the argument.
    *   The text mentions that the value of `Γ` (defined in Section 4) is not a necessary condition for failure, but it does not provide a clear, alternative intuition for when failure is guaranteed, beyond the complex inequality in Proposition 4.4.*   **Significant Gap Between Theory and Practice**
    *   The theoretical analysis is restricted to a two-layer ReLU CNN (Section 3), whereas the real-data experiments use deep, complex architectures like ResNet-101 and DeiT-Base (Section 6). The paper acknowledges this limitation (Section 7) but does not offer much discussion on why the conclusions from the shallow model should extend to deep ones.
    *   The theoretical data model assumes a simple additive structure of a universal signal (`u`) and a task-specific signal (`v_k`) (Definitions 3.1, 3.2). It is unclear how this maps to the complex, hierarchical features learned from real-world datasets like CIFAR-100, where the "universal knowledge" between, for example, classes of vehicles and classes of animals is non-trivial to define.
    *   The theoretical parameter transfer mechanism involves randomly sampling `αm` filters to transfer (Algorithm 1). In practice, transfer learning with deep models typically involves transferring entire contiguous blocks of layers (e.g., the first `k` layers or, as in the ViT experiment, layers 9-11, Section 6), which is structurally different.*   **Clarity and Presentation of Main Results**
    *   The main theoretical results in Section 4 are presented in a highly condensed format that may be inaccessible to readers not already expert in this specific line of theoretical work. The phase transition condition in Theorem 4.2 is a formidable fraction that obscures the underlying intuition.
    *   While the introduction of the quantity `Γ` is helpful, the paper could benefit from more high-level, qualitative explanations of its main theorems in the body of the text to aid reader comprehension.
    *   The appendix, containing the proofs, is extremely dense. While this suggests technical depth, its structure, with many nested propositions and lemmas across more than 20 pages (Appendix A-E), makes it very challenging to follow the main thread of the proof without a significant time investment.4) Suggestions for Improvement
*   **Elucidate the Negative Transfer Condition**
    *   In Section 4, following Proposition 4.4, please add a paragraph dedicated to providing an intuitive breakdown of the negative transfer condition. For instance, explaining what each ratio in the inequality represents (e.g., "signal specificity" vs. "upstream task strength") would be very helpful.
    *   Explicitly connect the theoretical condition to the experiment in Figure 1c. A brief discussion showing that the `||u||_2 = 0` setting satisfies the conditions of Proposition 4.4, part 2 would make the link between theory and experiment more concrete.
    *   Consider adding a simplified corollary or discussing a limiting case (e.g., as `||u||_2` approaches zero) to make the result more digestible.*   **Address the Theory-Practice Gap**
    *   In Section 6, please add a short discussion on the conceptual mapping from the theoretical model to the real-world experiments. For example, you could hypothesize that `u` corresponds to low-level features (edges, textures) shared across all CIFAR classes, while `v_k` corresponds to higher-level, class-specific features.
    *   In the Discussion (Section 7), briefly elaborate on why the core mechanisms identified (learning shared vs. specific signals, noise memorization) are likely to be fundamental and thus may generalize from shallow to deep networks, even if the exact mathematical forms change.
    *   Acknowledge the difference between the random filter sampling in the theory and the block-wise layer transfer in practice, and perhaps comment on whether the current theory could be seen as an approximation of the latter.*   **Enhance Presentation and Readability**
    *   In Section 4, after stating Theorem 4.2, please add an "Intuition" or "Interpretation" paragraph. This could explain, in words, that the numerator of the threshold represents the combined signal strength from both tasks, while the denominator represents a complexity term, making the condition a signal-to-noise/complexity ratio.
    *   To improve the accessibility of the main results, consider adding a small table in Section 4 that summarizes the key quantities (e.g., `Γ`, the phase transition threshold) and provides a brief, intuitive description for each.
    *   At the beginning of Appendix A ("Proof Sketch") and Appendix D ("The First System"), please add a high-level roadmap of the proof. This could be a short paragraph or a bulleted list outlining the main logical steps (e.g., 1. Decompose weights, 2. Analyze coefficient dynamics, 3. Show loss balancing, 4. Derive bounds), which would greatly help readers navigate the dense technical details.5) Score
*   Overall (10): 8 — The paper provides a novel and rigorous theoretical analysis for a practical and important problem, with strong empirical support.
*   Novelty (10): 9 — This work appears to be the first to analyze the training dynamics of parameter transfer and provide a theoretical account of negative transfer.
*   Technical Quality (10): 9 — The theoretical framework is sound, building on modern feature learning analysis, and the experiments are well-designed and executed.
*   Clarity (10): 7 — While the motivation and high-level ideas are clear, the presentation of the main theoretical results is dense and could be made more accessible.
*   Confidence (5): 4 — I am confident in my assessment of the paper's contributions and experimental results, as I am familiar with this area of research, though I have not verified every line of the proofs in the appendix.