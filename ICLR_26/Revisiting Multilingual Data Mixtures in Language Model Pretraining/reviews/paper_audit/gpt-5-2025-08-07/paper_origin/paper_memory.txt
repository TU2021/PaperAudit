# Global Summary
- Problem: How multilingual data composition during LLM pretraining affects language coverage and performance, including whether adding English harms other languages, whether family-specific pivots help, whether curriculum learning reduces interference, and whether increasing language count triggers a curse of multilinguality.
- Core approach: Train decoder-only LLaMA-style models at 1.1B and 3B parameters on mC4 (30 languages) and FineWeb-2 (up to 1,834 languages), with token budgets from 100B to 225B, and systematically vary English proportion, pivot choices, curricula, and number of languages (25–400). Evaluate validation LM loss and aggregated downstream benchmark accuracy across 10 multilingual tasks.
- Evaluation scope: 1.1B and 3B models; 100B–225B tokens; up to 400 training languages; English fixed at 40% in some settings; distributions include natural and temperature sampling (τ = 3.3). Benchmarks include Belebele, XCodah, XCSQA, XCOPA, XStoryCloze, XWinogrande, MMMLU, INCLUDE, Exams, M3Exams.
- Key findings:
  - Increasing English data does not necessarily reduce non-English performance if sufficient multilingual tokens are present. In Fixed Multilingual Budget (90B multilingual + English to total ≤225B), English proportion up to 60% leaves multilingual loss ~2.0; English loss improves (e.g., ~3.5 to ~2.6 for 1.1B).
  - English serves as an effective pivot across families; family-specific pivots (e.g., Russian for Slavic/Cyrillic) are not consistently superior. Combining English+Russian yields the lowest losses across Slavic and Cyrillic-script sets at higher pivot allocations.
  - Curriculum learning (English-first, pivots-first, staged inclusion) shapes training dynamics but does not improve final multilingual performance; observed English gains reflect more English tokens, not curriculum order.
  - No strong curse of multilinguality at this scale when English is held at 40%: English validation loss remains ~2.678 (natural distribution) from 25 to 400 languages; benchmark accuracy stays ~49–50% (natural) and ~42–44% (temp). Degradation under temperature sampling aligns with reduced mid/high-resource allocations and increased low-resource noise.
- Major quantitative results:
  - Table 1 (3B, 100B tokens, English 40%): English LM loss (natural): 2.678–2.682; benchmark (natural): 50.13 ± 1.868 down to 49.11 ± 1.864 then 49.64 ± 1.871. English LM loss (temp): 2.675–2.707; benchmark (temp): 43.24 ± 1.874 to 42.12 ± 1.854.
  - Heatmap averages (non-English LM loss): Natural Fixed Total Budget Top-25 = 1.511; Top-200 up to 1.529. Temperature Fixed Total Budget Top-25 increases 1.482 → 1.603 when training languages rise 25 → 400; Controlled Growth temperature reduces the degradation (Top-25 1.482 → 1.592).
- Caveats explicitly stated: Models are smaller than frontier-scale (1.1B/3B vs much larger); post-training and additional sampling strategies not explored; tokenizer choice (Mistral-Nemo-Base-2407, vocab 131,000) may limit low-resource language performance.

# Abstract
- Investigates multilingual data mixtures by training 1.1B and 3B LLMs with 25–400 languages to test assumptions about English vs multilingual performance, pivot language choice, curriculum learning, and curse of multilinguality.
- Claims:
  - Mixing English and multilingual data need not degrade in-language performance of either group if each has sufficient tokens in pretraining.
  - English as a pivot benefits languages across families; family-specific pivots do not consistently improve within-family languages.
  - Curriculum learning does not alleviate negative interference.
  - No significant curse of multilinguality as training languages increase at this model scale.
- Implication: Appropriately balanced multilingual data can improve capabilities without compromising performance, including low-resource settings. Quantitative details deferred to later sections.

# Introduction
- Context: LLMs increasingly perform well across many non-English languages; typical pretraining includes >100 languages. Debate persists on trade-offs among language coverage, performance, pivot selection, and curricula.
- Prior limitations: Studies on curse of multilinguality and scaling laws often use small models (45M, 85M parameters) and fewer languages (23). Task/instruction-tuning studies may not generalize to pretraining.
- Study setup: Train 1.1B and 3B models on corpora of 100B tokens with up to 400 languages; explore language count, diversity, and token distribution effects.
- Findings summarized:
  - Finding #1: Varying English proportion/amount does not harm multilingual performance if multilingual tokens suffice; vice versa for English performance.
  - Finding #2: English as pivot provides cross-family benefits; choosing within-family pivots does not consistently enhance performance.
  - Finding #3: Curriculum learning does not reduce negative interference nor improve non-English performance.
  - Finding #4: Performance degradation is driven by finite capacity and noisy, low-resource distributions, not simply adding languages.

# Method
- Model: Decoder-only Transformer (LLaMA architecture) at 1.1B and 3B parameters. Configuration: 1.1B: 24 layers, hidden 1536, 16 attention heads; 3B: 28 layers, hidden 2496, 24 heads; RoPE θ = 500,000; vocabulary |V| = 131,000 (Mistral-Nemo-Base-2407 tokenizer).
- Pretraining data:
  - mC4 for experiments with 30 languages.
  - FineWeb-2 for experiments with up to 1,834 languages.
  - Token budgets D = 100 to D = 225 billion tokens.
- Sampling:
  - Natural sampling: sample proportional to word counts.
  - Temperature sampling: π_l ∝ ω_l^(1/τ), with τ = 3.3 used in experiments to reduce imbalance.
- Evaluation:
  - Measure language modeling loss on held-out validation sets (distinct from pretraining data).
  - Downstream suite of 10 multilingual benchmarks; aggregate by language to produce per-language scores and compare to language-specific random baselines. Aggregation details in Appendix B.

# Experiments
- Assumption #1: English hurts multilinguality.
  - Setup: Train 1.1B and 3B models on mC4 (30 languages) varying English proportion 0–100%; temperature sampling with τ = 3.3. Two regimes:
    - Fixed Total Budget: total 100B tokens; increasing English decreases non-English tokens.
    - Fixed Multilingual Budget: fix non-English at 90B; add English on top up to total 225B.
  - Results (1.1B):
    - Fixed Total Budget: English validation loss decreases (~3.5 → ~2.7) as English proportion increases to 80%. Multilingual loss stable until ~40% English, then increases (~2.0 → ~2.5 at 80%).
    - Fixed Multilingual Budget: Multilingual loss remains ~2.0 across English proportions up to 60%; English loss decreases (~3.5 → ~2.6).
    - Benchmarks: English average score rises (~40% → ~47–47.5%) with increasing English proportion; multilingual scores remain ~35–36% near random (random baselines ~35%) and do not degrade. 3B models exhibit similar trends (Appendix Figures 7–8).
  - Takeaway: Adding English does not necessarily degrade other languages if sufficient multilingual tokens remain; English performance also robust when multilingual tokens are sufficient.

- Assumption #2: “Stay in the family.”
  - Setup: 1.1B model trained on subsets of Slavic and Cyrillic-script languages; compare pivot choices:
    - English pivot.
    - Russian pivot.
    - English+Russian pivot (uniform combination).
  - Language sets: Slavic (Belarusian, Ukrainian, Macedonian, Bulgarian, Mongolian, Serbian, Polish, Czech, Slovak); Cyrillic-script (Belarusian, Ukrainian, Macedonian, Bulgarian, Kyrgyz, Tajik, Kazakh, Mongolian, Serbian, Uzbek).
  - Results:
    - As pivot proportion increases (reducing non-pivot data), non-pivot losses rise at high pivot allocations as expected.
    - Up to 50% pivot allocation, English and Russian perform comparably on non-pivot languages.
    - Beyond ≥60% pivot allocation, Russian yields slightly lower loss than English.
    - Combining English+Russian yields the lowest overall loss across both language groups (Figure 3; per-language trends in Appendix Figure 9).
  - Takeaway: English is a broadly effective pivot; at very low-resource allocations, typological similarity (Russian) can help; joint pivots (English+Russian) provide complementary benefits.

- Assumption #3: Multilingual curriculum learning reduces negative interference.
  - Curricula (3B models):
    - All-at-once: full multilingual dataset from start (baseline).
    - English-all: 0–25% tokens: English only; then full multilingual.
    - English-Pivots-all: 0–25% English; 25–50% English + Arabic + Chinese + Russian (four scripts: Latin, Arabic, Han, Cyrillic); 50–100% full multilingual.
    - Pivots-all: 0–25% four pivots; then full multilingual.
  - Results (Figure 4):
    - English loss: introducing English early (English-all or English-Pivots-all) lowers final English loss; transitions between stages cause temporary loss increases (“forgetting”) before recovery.
    - Pivot languages: English-Pivots-all has lowest average loss mid-training, but all curricula converge to similar final losses; forgetting observed at each transition.
    - Non-English weighted average loss: different trajectories but all converge to similar final loss by end of training.
    - Further analysis: English gains correlate strongly with number of English tokens in training, not curriculum structure.
  - Takeaway: Curriculum affects dynamics but does not reduce interference or improve final multilingual performance; English gains reflect exposure quantity.

- Assumption #4: The “curse of multilinguality.”
  - Setup: 3B models on 100B tokens from FineWeb-2; English fixed at 40% (40B tokens); vary non-English languages from top-25, 50, 100, 200, 400 under two distributions (natural vs temperature, τ = 3.3). Evaluate English and non-English performance; also controlled growth settings maintain data for previously-included languages while adding new ones.
  - English results (Table 1):
    - LM loss (natural): 25 → 400 languages: 2.678; 2.678; 2.682; 2.680; 2.678.
    - LM loss (temp): 2.675; 2.681; 2.687; 2.696; 2.707.
    - Benchmark (natural): 50.13 ± 1.868; 49.41 ± 1.868; 49.29 ± 1.865; 49.11 ± 1.864; 49.64 ± 1.871.
    - Benchmark (temp): 43.24 ± 1.874; 43.80 ± 1.878; 43.76 ± 1.872; 42.38 ± 1.870; 42.12 ± 1.854.
    - Observation: English performance stable under natural distribution as languages increase; temp sampling slightly worse.
  - Non-English results (Figure 5, numeric excerpts):
    - Natural Fixed Total Budget: Top-25 average validation LM loss remains ~1.511; increases marginally up to ~1.529 by 400 languages; Controlled Growth natural remains ~1.511–1.528.
    - Temperature Fixed Total Budget: Top-25 increases 1.482 → 1.603 from 25 → 400; Top-200 increases to ~1.483; Controlled Growth temperature shows smaller degradation (Top-25 1.482 → 1.592; Top-200 1.471 by 400).
  - Interpretation in-paper: Degradation stems from model capacity limits and poorer data quality/oversampling of low-resource languages under temperature sampling, rather than simply the number of languages.
  - Takeaway: Increasing language count does not inherently reduce performance when data for existing languages is maintained; under natural distribution, stability suggests a “curse of capacity” rather than “curse of multilinguality.”

# Related Work
- Data mixture optimization: Prior work on domain weighting (Doremi; importance resampling; domain reweighting; utility estimation; regression formulations) and multilingual sampling strategies (temperature sampling; UniMax) with concerns about overfitting tail languages; methods to cap repetition or balance head languages; multilingual scaling laws (He et al., 2024).
- Curse of multilinguality and negative interference: Introduced by Conneau et al. (2020); investigated with smaller/bidirectional models and modular architectures; recent cross-lingual expert LMs reduce competition; studies derive optimal sampling ratios (Chang et al., 2024); meta-learning treatments (Wang et al., 2020); interference may depend more on script than family (Alastruey et al., 2025).
- Pivot languages: Used in translation and cross-lingual instruction tuning (English as pivot effective; multi-pivot ensembling).
- Curriculum learning: Applied in machine translation and instruction tuning; shown effective in some contexts; code-switching curricula explored for cross-lingual transfer.

# Discussion
- Summary of guidance:
  - Ensure adequate absolute multilingual token volume; high English proportions do not inherently compromise non-English performance.
  - English is a strong cross-family pivot; multiple pivots (English+Russian) can help in very low-resource conditions.
  - Curriculum learning provides no final-performance benefit over well-mixed training; observed English gains are explained by data quantity.
  - “Curse” arises from capacity and data distribution/quality; do not limit language coverage arbitrarily; instead include adequate high-quality data per language.
- Limitations:
  - Scale: 1.1B/3B models and budgets below frontier models (e.g., Meta AI, DeepSeek-R1).
  - Post-training and broader sampling strategies not explored due to compute constraints.
  - Tokenizer choice may limit lower-resource language performance; training a tokenizer for 1,834 languages is impractical given vocab size/memory costs.
- Future work: Validate trade-offs at larger model scales to examine effects of increased capacity on data composition, interference, and performance.

# Appendix
- Training framework: SwissAI fork of Hugging Face’s Nanotron.
- Architectures (Table 2):
  - 1.1B: LLaMA, 24 layers, hidden 1536, 16 heads, RoPE θ = 500,000, vocab 131,000.
  - 3B: LLaMA, 28 layers, hidden 2496, 24 heads, RoPE θ = 500,000, vocab 131,000.
- Hyperparameters:
  - Learning rate: 8 × 10^{-4}; linear warmup first 4%; “1-sqrt” decay during final 20%; schedule illustrated (Figure 6; tokens B: 0→100 with LR steps: 8 at 0–80B, 4 at 85B, 2 at 90B, 0 at 100B).
  - Optimizer: AdamW, β = (0.9, 0.95).
  - Weight decay λ = 0.1.
  - Micro-batch size = 5.
- Hardware (CSCS Alps cluster):
  - Nodes: 4 NVIDIA Grace-Hopper H100 GPUs each (96 GB).
  - 1.1B models: 22 nodes (88 GPUs), ~15 h per 100B tokens, global batch size 440 examples.
  - 3B models: 64 nodes (256 GPUs), ~18 h per 100B tokens, global batch size 640 examples.
- Sampling methods:
  - Natural distribution π^{natural} ∝ ω_l (word counts).
  - Temperature sampling π_l^{temp,τ} ∝ ω_l^{1/τ}, τ = 3.3.
- Benchmarks (10 tasks): Belebele, XCodah, XCSQA, XCOPA, XStoryCloze, XWinogrande, MMMLU, INCLUDE, Exams, M3Exams. Aggregation computes per-language averages across tasks with language-specific random baselines.
- Languages:
  - Section 3 set (30 languages) listed (Arabic, Bulgarian, Bengali, Catalan, German, Greek, English, Spanish, Estonian, Basque, Persian, Finnish, French, Hindi, Haitian Creole, Indonesian, Italian, Japanese, Korean, Burmese, Portuguese, Russian, Swahili, Tamil, Telugu, Thai, Turkish, Urdu, Vietnamese, Mandarin Chinese).
  - Section 4 sets (English, Russian pivots) include Slavic and Cyrillic-script languages (Belarusian, Ukrainian, Serbian, Macedonian, Bulgarian, Polish, Czech, Slovak; Tajik, Uzbek, Kyrgyz, Kazakh, Mongolian).
- Fixed English proportion experiments (3B; Appendix Figures 7–8): replicate trends from 1.1B; Fixed Total Budget ≥50% English reduces other language performance; Fixed Multilingual Budget up to 60% English does not adversely affect others.
- Cross-lingual transfer without English:
  - 3B models trained on 1,834 languages; target 45 high/mid-resource languages vs 1,789 tail languages; vary tail proportion (6–33%).
  - English excluded during training; English validation loss still decreases when more tokens from related high-resource languages are included; increases when tail languages receive more tokens.
  - Pearson correlations between English loss and language families (Table 5): Slavic 0.853; Germanic 0.808; Romance 0.785 (positive associations); negative correlations for many other families (e.g., Oceanic −0.962, Algonquian −0.963, Quechuan −0.980).
- Data distributions (Table 8): token counts/proportions (in billions) across Top-25/50/100/200 under Natural vs Temperature and Controlled Growth variants; totals 100B for Fixed Total Budget, 90B for Controlled Growth; English excluded from these counts.
- English per-benchmark results with 25–400 languages (Table 9): e.g., Natural: Belebele 38.22 → 37.33; M3Exams 38.70 → 38.60; MMLU ~30.54–30.91; PAWS-X ~49.00–55.20; temperature variants show lower scores.
- Token allocations per language across English proportion and pivot runs (Tables 10–14): Fixed Total Budget totals 100B; Fixed Multilingual Budget totals increase from 112.0B → 224.1B when English rises 20% → 60%; pivot tables detail per-language million-token allocations as pivot proportions change.
- Acknowledgment: Supported by Swiss AI Initiative; CSCS project ID a06 on Alps; SNSF (No. 215390), Innosuisse (PFFS-21-29), EPFL Center for Imaging, Sony Group Corporation, Meta LLM Evaluation Research Grant.

# References
- Cites works on multilingual LLMs and data mixture optimization (Doremi, importance resampling), sampling strategies (UniMax), curse of multilinguality and interference (Conneau et al., Pfeiffer et al., Chang et al., Wang et al.), pivot strategies in translation and instruction tuning, and curriculum learning approaches in MT and LLMs.
- Key datasets and benchmarks referenced: mC4, FineWeb-2, Belebele, M3Exams, MMMLU, XCOPA, XWinogrande, XCSQA, XCodah, XStoryCloze, INCLUDE.
- Not specified in this section: numeric replication details beyond those tabulated in the main and appendix sections.