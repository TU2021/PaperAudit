Summary
- The paper studies how multilingual data mixtures affect LLM pretraining by training LLaMA-style decoder-only models at 1.1B and 3B parameters on 100–225B tokens, with language coverage varied from 25 up to 400 and sampling strategies varied between natural and temperature sampling (Section 2; Table 2; A.3; Table 8). It investigates four assumptions: more English hurts other languages, family-specific pivot languages are best, curriculum learning reduces negative interference, and the “curse of multilinguality.” Core findings: adding English does not degrade multilingual performance when multilingual tokens are sufficient (Figures 1–2; Appendix Figures 7–8); English as a pivot transfers broadly, with EN+RU jointly best in a Slavic/Cyrillic subset (Figure 3; Figure 9; Table 4); curriculum alters training trajectories but not final multilingual performance (Figure 4); increasing languages up to 400 does not significantly degrade performance when distributions are balanced, with degradation attributed to capacity and data distribution (Table 1; Figure 5; Table 8).Strengths
- Bold empirical scope and scale
  - Models at 1.1B and 3B, trained on 100–225B tokens with up to 400 languages (Section 2; Table 2; A.2; A.3; Table 8), which materially exceeds prior small-model studies and enables conclusions at more realistic LLM scales (novelty/impact).
  - Two corpora spanning mC4 (30 languages) and FineWeb-2 (1,834 languages), enabling experiments on both curated subsets and web-scale diversification (Section 2; footnotes 2–3; Penedo et al., 2025) (novelty/coverage).
  - Hardware and training details (A.2; A.1; Figure 6) support reproducibility and technical rigor (technical soundness).- Careful disentangling of data-budget conditions
  - Fixed Total Budget vs Fixed Multilingual Budget clearly distinguish trade-offs between proportions and absolute token counts (Section 3; Figures 1a–b; Figures 2a–b; Tables 10–11) (experimental rigor).
  - The Fixed Multilingual Budget extends total tokens to preserve multilingual coverage when adding English, showing multilingual loss/accuracy stability up to 60% English (Figures 1b and 2b) (clarity/impact).
  - Extension of the same analysis to 3B models in the appendix confirms patterns (Appendix Figure 7; Figure 8) (robustness).- Structured re-evaluation of multilingual “curse”
  - Scaling from 25→400 languages with English fixed at 40% shows English loss and benchmark performance remain stable under natural distribution, and only modest changes under temperature sampling (Table 1; Figures 5a–d; Table 8; Table 9) (technical soundness/impact).
  - Controlled growth setting keeps original languages’ token allotment fixed when adding new languages, isolating effect of language addition from data reallocation (Figures 5b, 5d; Table 8) (experimental rigor).
  - Explanation framing “curse of capacity” vs “curse of data distribution” aligns with observed differences between natural and temperature sampling (Section 5 “Takeaway”; Discussion) (clarity/insight).- Pivot-language investigation with typology and scripts
  - Pivot comparisons among English, Russian, and EN+RU on Slavic and Cyrillic-script sets show EN+RU consistently best; RU slightly stronger as pivot at very high pivot allocations (≥60%) (Figure 3; Figure 9; Table 4) (novelty/insight).
  - The analysis considers both language-family proximity and practical web-data quality for English, yielding a nuanced view (Section 4; Figure 3; Discussion) (clarity/impact).
  - Token allocation tables for pivot runs provide transparency (Tables 12–14) (technical rigor).- Curriculum learning design and analysis
  - Multiple curricula (All-at-once, English-all, English-Pivots-all, Pivots-all) designed to probe early language exposure and forgetting (Section 5; Figure 4) (novelty/rigor).
  - Observed short-term forgetting during transitions with later recovery, and final convergence of non-English losses across curricula (Figure 4b; 4c) (technical soundness).
  - Attribution of English improvements to larger English token exposure demonstrates the dominance of data distribution over curriculum structure (Section 5 “Results”; Figure 4a) (insight).- Transparent benchmarking and aggregation
  - Use of 10 multilingual benchmarks with language-specific random baselines and per-language aggregation definition (Appendix B.1–B.2) (clarity/methodological transparency).
  - Reporting English per-benchmark scores under language scaling (Table 9) and aggregated multilingual trends (Figures 2a–b; Figure 8) (experimental rigor).Weaknesses
- Limited granularity and external validity of non-English downstream evaluation
  - Group-level averages for “Multilingual” may mask per-language variations; few per-language plots are provided outside the pivot ablation (Figures 2a–b; Figure 8; “weighted average” wording in Sections 3–5; Figure 9 for per-language only in pivot) — limits visibility into whether some languages degrade or improve (experimental rigor/impact).
  - The mismatch between LM validation loss and downstream accuracy (e.g., loss rises for non-English beyond ~40–50% English in Fixed Total Budget, Figure 1a; yet downstream accuracy remains roughly flat, Figure 2a) is not analyzed, which weakens conclusions about practical impact (technical analysis).
  - Benchmarks vary in language coverage (Appendix B.1), and the paper does not quantify the distribution of languages covered in aggregated “Multilingual” scores; without per-language coverage counts, sensitivity to head vs. tail languages is unclear (No direct evidence found in the manuscript) (external validity/clarity).- Causal attribution to “curse of capacity” and “curse of data quality” lacks direct measurement
  - While natural vs. temperature sampling differences are shown (Table 1; Figures 5a–d), data quality is not measured (e.g., noise, duplication, domain bias); attributing degradation specifically to low-quality tail oversampling is hypothesized, not proven (Section 5 “Takeaway”; Discussion) (technical soundness).
  - Capacity is discussed conceptually, but there is no capacity scaling beyond 1.1B→3B, nor controlled ablations showing performance improvements with higher capacity at fixed mixtures (Limitations; Section 5) (novelty/rigor).
  - The statement of “strong correlation” between English tokens and English loss in curriculum experiments is not quantified (no coefficients or plots), limiting causal support (Section 5 “Results”) (clarity/technical rigor).- Pivot experiment generality and confounding
  - The pivot analysis is restricted to Slavic/Cyrillic sets with a 1.1B model (Section 4; Figure 3; Table 4), so claims about family boundaries not being barriers to transfer across families remain under-tested (external validity).
  - Increasing pivot share simultaneously reduces non-pivot data (Section 4; Figure 3; Tables 12–14), confounding pivot choice with the severity of low-resource conditions; the paper does not include a matched-data control keeping non-pivot tokens constant (technical soundness).
  - English’s web-data diversity is cited as a reason for broad transfer (Section 4), but no direct content-quality diagnostics are provided to distinguish diversity vs. typological proximity effects (No direct evidence found in the manuscript) (causal clarity).- Curriculum learning analysis confounded by token quantities and limited metrics
  - English improvements under curricula are attributed to more English tokens (Section 5 “Results”), but curricula are not matched on English token counts; a matched-token control is needed (technical rigor).
  - Final performance conclusions rely on validation loss (Figure 4); downstream task evaluations by curriculum regime are not reported, limiting practical interpretation (No direct evidence found in the manuscript) (external validity).
  - Forgetting is described qualitatively (Figure 4), but no quantitative forgetting/recovery metrics (e.g., delta loss per transition, retention curves) are provided (clarity/analysis depth).- Reporting consistency and clarity issues in language-scaling experiments
  - Controlled Growth setting uses a 90B total token budget while other runs use 100B (Table 8), potentially confounding comparisons across Figures 5a vs. 5b and 5c vs. 5d (clarity/technical rigor).
  - Appendix Figure 10’s caption states “Increasing token allocation for tail languages reduces validation loss in English and improves English accuracy” while Section D text states English improves when more high-resource tokens are included; this appears contradictory (Appendix Figure 10; Section D) (clarity).
  - The ± in Table 1 (e.g., “49.64 ± 1.871”) is not defined (CI vs. SE vs. std), and procedures for aggregation variability are not described (No direct evidence found in the manuscript) (clarity/statistical reporting).- Tokenization and script imbalance may bias findings
  - The tokenizer is fixed to Mistral-Nemo-Base-2407 (131k vocab) (Section 2; Limitations), potentially suboptimal for many scripts; FineWeb-2 is overwhelmingly Latin-based (Table 6: Latn 1,639 vs. Cyrl 56, Arab 30, Deva 29), risking script-induced bias in sampling and performance (external validity).
  - Sampling uses “number of words as a proxy for language frequency” (A.3), which can differ from token-level distributions across scripts, potentially affecting cross-script comparability (technical soundness).
  - The paper’s conclusions about cross-family transfer and the curse may be partly driven by this script imbalance (Limitations; Table 6), but this is not assessed experimentally (No direct evidence found in the manuscript) (causal clarity).Suggestions for Improvement
- Expand non-English evaluation granularity and analysis
  - Report per-language downstream results (not just weighted averages) for experiments in Sections 3 and 5, analogous to Appendix Figure 9 for the pivot study, to reveal language-specific winners and losers (Figures 2a–b; Figure 8).
  - Analyze the relationship between LM loss and downstream accuracy for non-English groups to explain observed discrepancies (e.g., Figures 1a vs. 2a), including correlation analyses and task-level sensitivity.
  - Quantify benchmark language coverage and the contribution of head vs. tail languages in the aggregated “Multilingual” metric (Appendix B.1–B.2), and include stratified analyses by resource level.- Directly measure data quality and capacity factors underlying the “curse”
  - Add data-quality diagnostics (e.g., duplication, readability, domain mix, contamination rates) for head vs. tail slices under natural vs. temperature sampling and relate them to performance changes (Table 8; Figures 5a–d).
  - Include capacity scaling ablations (e.g., a larger model beyond 3B, or parameter-efficient expansions) with fixed data mixtures to test the “curse of capacity” hypothesis (Limitations; Section 5).
  - Quantify correlations cited (e.g., between English tokens and English loss in Section 5) with coefficients/plots and control for confounds (e.g., pivot presence, head-language shares) to strengthen causal claims.- Strengthen and generalize pivot-language experiments
  - Extend pivot analyses to additional families/scripts (e.g., Romance, Semitic, Indic, Han) with both family-internal and cross-family pivots, and include 3B runs for comparability (Section 4; Table 4).
  - Introduce matched-data controls where non-pivot language tokens are held constant while varying pivot allocations, isolating pivot effects from low-resource severity (Figure 3; Tables 12–14).
  - Incorporate content-quality diagnostics for pivot languages (e.g., domain diversity, noise) to disentangle typological proximity from web-data richness effects (Section 4; No direct evidence found).- Tighten curriculum-learning methodology and evaluation
  - Match English token counts across curricula to isolate the effect of order from quantity, and report English loss alongside per-language multilingual outcomes (Section 5; Figure 4).
  - Add downstream benchmarks per curriculum regime (Appendix B.1), reporting language-stratified results and random baselines to complement validation loss trajectories (No direct evidence found).
  - Define and report quantitative forgetting/recovery metrics (e.g., loss deltas at transitions, time-to-recovery), enabling precise comparisons across curricula (Figure 4).- Improve reporting consistency and statistical clarity
  - Align token budgets across settings or justify differences (Table 8: 90B in Controlled Growth vs. 100B elsewhere), and annotate figures to avoid cross-setting comparisons without normalization (Figures 5a–d).
  - Correct Appendix Figure 10 caption to match Section D text (English improves with more high-resource tokens) and verify plots for label accuracy.
  - Define the ± in Table 1 (e.g., SE/CI) and describe the computation procedure, including sample sizes and aggregation variance (Appendix B.2), to clarify statistical uncertainty.- Assess tokenization/script effects and sampling proxies
  - Provide per-script loss/accuracy analyses to detect systematic biases linked to the tokenizer and script distribution (Table 6; Limitations), and consider script-aware sampling adjustments.
  - Evaluate the impact of using “number of words” as a frequency proxy versus token-level counts across scripts (A.3), and report differences; if material, adjust sampling.
  - Explore training or adapting a tokenizer for underrepresented scripts (e.g., via subword mergers or additive vocabulary) and report effects on low-resource/script-diverse languages (Limitations).Score
- Overall (10): 7 — Broad, well-executed study with clear evidence against several common assumptions using 1.1B/3B models and up to 400 languages, though causal attributions and some reporting inconsistencies limit strength (Sections 3–5; Figures 1–5; Table 8; Appendix D; Limitations).
- Novelty (10): 7 — Revisits multilingual mixture questions at higher scales and with distinct regimes (Fixed Multilingual Budget; Controlled Growth), contributing new insights on pivots and curricula (Introduction; Section 3; Section 4; Section 5; Figures 1–5).
- Technical Quality (10): 6 — Solid experimental design and transparency (A.1–A.3; Tables 10–14; Figure 6), but confounds in pivot/curriculum, limited data-quality measurement, and partial downstream granularity weaken causal claims (Figure 3; Figure 4; Discussion).
- Clarity (10): 6 — Clear takeaways and comprehensive appendices, but inconsistencies (e.g., Table 8 budgets; Appendix Figure 10 caption vs. Section D text) and missing definitions (Table 1 ±) hinder interpretability (Figures 5; Table 1; Table 8; Appendix D).
- Confidence (5): 4 — The manuscript provides extensive anchors and results, but absence of some controls and quantitative causal metrics, plus noted inconsistencies, reduce certainty in several conclusions.