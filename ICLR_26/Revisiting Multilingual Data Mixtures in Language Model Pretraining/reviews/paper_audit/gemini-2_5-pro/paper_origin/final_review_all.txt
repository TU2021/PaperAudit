1) Summary
This paper investigates the impact of multilingual data mixtures on language model pretraining by training 1.1B and 3B parameter models on corpora with up to 400 languages. The work systematically challenges four common assumptions in multilingual training. The core contributions are empirical findings showing that: (1) increasing English data does not necessarily harm multilingual performance if a sufficient number of multilingual tokens are included; (2) English serves as an effective pivot language across language families, often outperforming in-family pivots; (3) curriculum learning strategies do not mitigate negative interference or improve final performance compared to standard mixed training; and (4) the "curse of multilinguality" is more a function of model capacity and data quality/distribution rather than simply the number of languages. These results provide practical guidance for designing multilingual pretraining data mixtures.2) Strengths
*   **Comprehensive and Large-Scale Empirical Study**
    *   The study is conducted at a scale significantly larger than much of the prior work cited (e.g., Chang et al., 2024; He et al., 2024), using 1.1B and 3B parameter models (Section 2, Table 2). This adds significant weight to the findings, as scaling effects are critical in LLM pretraining.
    *   The experiments involve a large and diverse set of languages, ranging from 25 up to 400 (Sections 3, 6), and even considering up to 1,834 languages from the FineWeb2 corpus for some analyses (Section 2, Appendix D). This breadth allows for more generalizable conclusions about multilingual phenomena.
    *   The models are trained on a substantial number of tokens (100B to 225B, Section 2), ensuring that the observed effects are not artifacts of undertraining and are relevant to modern pretraining regimes.*   **Rigorous and Well-Controlled Experimental Design**
    *   The paper effectively isolates variables through clever experimental setups. The distinction between "Fixed Total Budget" and "Fixed Multilingual Budget" (Section 3, Figures 1, 2) is a prime example, allowing the authors to disentangle the effect of adding English data from the effect of removing multilingual data.
    *   The "Controlled Growth Setting" in the "curse of multilinguality" experiment (Section 6, Figure 5) is another strong design choice. By keeping the data for the original language set fixed while adding new languages, the authors directly test the impact of increased language diversity, separate from the confounder of reduced data per language.
    *   The curriculum learning experiments (Section 5, Figure 4) are systematically designed with four distinct strategies (All-at-once, English-all, etc.), providing a controlled comparison to evaluate the impact of language introduction order on learning dynamics and final performance.*   **Clear and Impactful Contributions that Challenge Prevailing Assumptions**
    *   The paper is structured around four explicit assumptions, and each is addressed with dedicated experiments and clear "Takeaway" messages. This framing makes the contributions easy to understand and digest.
    *   Finding #1 directly challenges the common belief that more English data is detrimental to other languages, providing a more nuanced view based on absolute token counts (Section 3, Figure 1b, Figure 2b).
    *   Finding #2 questions the "stay in the family" heuristic for pivot languages, showing that a high-resource, diverse language like English can be a universally strong pivot, and that a combination of pivots is often optimal (Section 4, Figure 3).
    *   Finding #4 reframes the "curse of multilinguality" not as a simple function of language count, but as a more complex interplay of model capacity, data distribution, and data quality (Section 6, Figure 5, Table 1). This is a significant refinement of a widely discussed concept.*   **Exceptional Clarity and Organization**
    *   The paper is extremely well-written and logically structured. Each section clearly states the assumption being tested, the experimental setup, the results, and a concise takeaway.
    *   The figures are generally clear, well-labeled, and effectively support the claims made in the text (e.g., Figure 1, Figure 3, Figure 5).
    *   The appendices provide substantial detail on model configurations, training hyperparameters, and benchmark setups, demonstrating transparency and enabling reproducibility (Appendix A, B, C).3) Weaknesses
*   **Significant Internal Inconsistencies and Reporting Errors**
    *   The methodology for the curriculum learning experiment (Section 5) states that the "English-all" and "English-Pivots-all" strategies train exclusively on English data for the first 25B tokens. However, the corresponding plot of English validation loss (Figure 4a) shows these models starting with an extremely high loss that only drops after 25B tokens, suggesting they were not trained on English during this initial phase. This is a critical contradiction that undermines the conclusions of this experiment.
    *   In Appendix D, the main text states that English validation loss *decreases* when more data from high-resource languages is included. Conversely, the caption for Figure 10, which visualizes this experiment, claims that increasing allocation for *tail* (low-resource) languages *reduces* English validation loss. These statements are mutually exclusive. The plot itself (Figure 10a) appears to support the main text, but the contradiction makes the finding unreliable.
    *   In the "Fixed Total Budget" experiment (Section 3), the multilingual validation loss for the 3B model clearly degrades as the English proportion increases above 50% (Figure 7a). Despite this, the caption for Figure 8a claims that downstream performance "does not hurt". However, the plot in Figure 8a shows a visible downward trend in the multilingual benchmark score, contradicting the caption and creating an inconsistency between pretraining loss and downstream evaluation claims.*   **Superficial Analysis of Downstream Task Performance**
    *   While the paper uses a comprehensive suite of 10 multilingual benchmarks (Appendix B.1), the results in the main body are presented as a single aggregated score for "Multilingual" and "English" groups (Figure 2, Figure 8). This high-level aggregation may obscure important nuances.
    *   It is unclear how different data mixtures affect performance on different *types* of tasks (e.g., commonsense reasoning vs. reading comprehension). For instance, in the "curse of multilinguality" experiments, it would be valuable to know if adding more languages disproportionately harms knowledge-intensive or reasoning-intensive tasks.
    *   The per-benchmark breakdown is only provided for English in the appendix (Table 9), leaving the impact on multilingual tasks at a granular level unanalyzed in the paper.*   **Ambiguity in the "Sufficient Tokens" Condition**
    *   A central claim is that adding English data does not harm multilingual performance "provided that languages have a sufficient number of tokens" (Abstract, Section 3 Takeaway). However, the paper does not define or quantify what constitutes a "sufficient" number of tokens.
    *   In the "Fixed Total Budget" experiment, performance degrades when English exceeds 40-50% (Figure 1a, Figure 7a), but it is difficult to translate this percentage into a generalizable, absolute token count per language that practitioners can use as a guideline.
    *   This lack of quantification makes the primary takeaway less actionable. It is unclear how this threshold would change with more languages, different language distributions, or larger model sizes.*   **Potential Confounding Effect of Data Quality**
    *   The paper concludes that the "curse of multilinguality" under temperature sampling is partly a "curse of data quality," as oversampling low-resource languages introduces more "noisy data" (Section 6 Takeaway).
    *   This is a plausible interpretation, but it is not directly supported by empirical evidence within the paper. The quality of the data for the hundreds of low-resource languages from FineWeb2 is not measured or controlled for.
    *   The observed performance degradation could be due to other factors, such as poor tokenization for these languages or simply an extreme scarcity of data that even temperature sampling cannot overcome, rather than inherent "noisiness." The claim about data quality remains an assumption.*   **Limited Generalizability to Frontier-Scale Models**
    *   The authors correctly identify this in their limitations (Section Limitations), but it remains a significant weakness. The experiments are conducted on 1.1B and 3B parameter models.
    *   The dynamics of capacity saturation, negative interference, and cross-lingual transfer may be substantially different in models that are 10x to 100x larger.
    *   While the work is a valuable contribution at its scale, the findings cannot be assumed to hold for state-of-the-art foundation models without further validation, which should be more strongly caveated in the main conclusions.4) Suggestions for Improvement
*   **Address and Resolve Internal Inconsistencies**
    *   The authors must resolve the discrepancy between the described method and the plotted results for the curriculum learning experiment (Section 5, Figure 4a). Either the plot is incorrect, or the description of the method is. This needs to be corrected and clarified for the conclusion to be valid.
    *   The contradictory statements in the text and caption of Figure 10 (Appendix D) must be reconciled. The authors should ensure the caption accurately reflects the data presented in the plot and the conclusion drawn in the text.
    *   The claim that downstream performance is unaffected in the "Fixed Total Budget" setting for the 3B model should be revisited. The authors should acknowledge the downward trend in Figure 8a and discuss its relationship with the increasing validation loss shown in Figure 7a, providing a more nuanced conclusion.*   **Incorporate Granular Downstream Task Analysis**
    *   To complement the aggregated scores in Figures 2 and 8, consider adding a table or plot in the main paper that breaks down performance by task category (e.g., reasoning, QA, classification) for a key experiment.
    *   For the "curse of multilinguality" experiment, showing how the average score changes for different task types as more languages are added would provide much richer insight than the current presentation.
    *   A per-language analysis for a representative subset of languages on key benchmarks would also strengthen the paper's claims about multilingual performance beyond a single aggregated number.*   **Attempt to Quantify the "Sufficient Tokens" Threshold**
    *   Provide a post-hoc analysis that correlates the number of training tokens for each non-English language with its final performance (either loss or downstream score) in the "Fixed Total Budget" setting. This could help identify an empirical "knee" in the performance curve, offering a more concrete estimate of what "sufficient" means at this scale.
    *   Discuss how this threshold might be expected to scale with model size or the total number of languages, even if speculatively, to make the guidance more practical for researchers working with different constraints.*   **Expand Discussion on Data Quality**
    *   In Section 6, explicitly frame the "curse of data quality" as a hypothesis rather than a direct finding.
    *   Strengthen the discussion by acknowledging data quality as a major uncontrolled variable. You could suggest how future work might address this, for example, by applying quality filtering heuristics to the low-resource data slices and re-running a subset of the experiments.
    *   Briefly discuss the potential role of the tokenizer (mentioned in Limitations) in amplifying issues with low-resource languages, as this is closely related to the data quality argument.*   **Carefully Frame Scalability Claims**
    *   In the Discussion & Conclusion (Section 8), more explicitly scope the findings to the 1-3B parameter scale.
    *   For instance, rephrase claims like "curriculum learning offer no demonstrable benefit" to "curriculum learning offer no demonstrable benefit at the 1B and 3B scales tested." This small change manages reader expectations and reinforces the need for future work at larger scales without diminishing the paper's contributions.5) Score
*   Overall (10): 6 — The paper addresses important questions with a large-scale study, but significant internal inconsistencies in key experiments undermine the reliability of the findings (e.g., Figure 4a, Figure 10).
*   Novelty (10): 8 — The scale of the investigation and the systematic, evidence-based refutation of common wisdom are highly novel and impactful (e.g., Figure 1b, Figure 3, Figure 5).
*   Technical Quality (10): 5 — While the overall experimental design is strong, critical reporting errors and contradictions between methodology and results in key sections are a major flaw (Section 5, Appendix D).
*   Clarity (10): 7 — The paper is well-written, but fundamental contradictions between text, figures, and captions make it difficult to understand what was actually done and found (e.g., Figure 4a vs Section 5 text).
*   Confidence (5): 5 — I am highly confident in my assessment; the identified weaknesses are based on direct contradictions within the manuscript itself.