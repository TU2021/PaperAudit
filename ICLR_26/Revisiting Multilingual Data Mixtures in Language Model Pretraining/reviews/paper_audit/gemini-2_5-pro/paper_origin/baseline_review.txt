1) Summary
This paper investigates the impact of multilingual data mixtures on language model pretraining by training 1.1B and 3B parameter models on corpora with up to 400 languages. The work systematically challenges four common assumptions in multilingual training. The core contributions are empirical findings showing that: (1) increasing English data does not necessarily harm multilingual performance if a sufficient number of multilingual tokens are included; (2) English serves as an effective pivot language across language families, often outperforming in-family pivots; (3) curriculum learning strategies do not mitigate negative interference or improve final performance compared to standard mixed training; and (4) the "curse of multilinguality" is more a function of model capacity and data quality/distribution rather than simply the number of languages. These results provide practical guidance for designing multilingual pretraining data mixtures.2) Strengths
*   **Comprehensive and Large-Scale Empirical Study**
    *   The study is conducted at a scale significantly larger than much of the prior work cited (e.g., Chang et al., 2024; He et al., 2024), using 1.1B and 3B parameter models (Section 2, Table 2). This adds significant weight to the findings, as scaling effects are critical in LLM pretraining.
    *   The experiments involve a large and diverse set of languages, ranging from 25 up to 400 (Sections 3, 6), and even considering up to 1,834 languages from the FineWeb2 corpus for some analyses (Section 2, Appendix D). This breadth allows for more generalizable conclusions about multilingual phenomena.
    *   The models are trained on a substantial number of tokens (100B to 225B, Section 2), ensuring that the observed effects are not artifacts of undertraining and are relevant to modern pretraining regimes.*   **Rigorous and Well-Controlled Experimental Design**
    *   The paper effectively isolates variables through clever experimental setups. The distinction between "Fixed Total Budget" and "Fixed Multilingual Budget" (Section 3, Figures 1, 2) is a prime example, allowing the authors to disentangle the effect of adding English data from the effect of removing multilingual data.
    *   The "Controlled Growth Setting" in the "curse of multilinguality" experiment (Section 6, Figure 5) is another strong design choice. By keeping the data for the original language set fixed while adding new languages, the authors directly test the impact of increased language diversity, separate from the confounder of reduced data per language.
    *   The curriculum learning experiments (Section 5, Figure 4) are systematically designed with four distinct strategies (All-at-once, English-all, etc.), providing a controlled comparison to evaluate the impact of language introduction order on learning dynamics and final performance.*   **Clear and Impactful Contributions that Challenge Prevailing Assumptions**
    *   The paper is structured around four explicit assumptions, and each is addressed with dedicated experiments and clear "Takeaway" messages. This framing makes the contributions easy to understand and digest.
    *   Finding #1 directly challenges the common belief that more English data is detrimental to other languages, providing a more nuanced view based on absolute token counts (Section 3, Figure 1b, Figure 2b).
    *   Finding #2 questions the "stay in the family" heuristic for pivot languages, showing that a high-resource, diverse language like English can be a universally strong pivot, and that a combination of pivots is often optimal (Section 4, Figure 3).
    *   Finding #4 reframes the "curse of multilinguality" not as a simple function of language count, but as a more complex interplay of model capacity, data distribution, and data quality (Section 6, Figure 5, Table 1). This is a significant refinement of a widely discussed concept.*   **Exceptional Clarity and Organization**
    *   The paper is extremely well-written and logically structured. Each section clearly states the assumption being tested, the experimental setup, the results, and a concise takeaway.
    *   The figures are generally clear, well-labeled, and effectively support the claims made in the text (e.g., Figure 1, Figure 3, Figure 5).
    *   The appendices provide substantial detail on model configurations, training hyperparameters, and benchmark setups, demonstrating transparency and enabling reproducibility (Appendix A, B, C).3) Weaknesses
*   **Superficial Analysis of Downstream Task Performance**
    *   While the paper uses a comprehensive suite of 10 multilingual benchmarks (Appendix B.1), the results in the main body are presented as a single aggregated score for "Multilingual" and "English" groups (Figure 2, Figure 8). This high-level aggregation may obscure important nuances.
    *   It is unclear how different data mixtures affect performance on different *types* of tasks (e.g., commonsense reasoning vs. reading comprehension). For instance, in the "curse of multilinguality" experiments, it would be valuable to know if adding more languages disproportionately harms knowledge-intensive or reasoning-intensive tasks.
    *   The per-benchmark breakdown is only provided for English in the appendix (Table 9), leaving the impact on multilingual tasks at a granular level unanalyzed in the paper.*   **Ambiguity in the "Sufficient Tokens" Condition**
    *   A central claim is that adding English data does not harm multilingual performance "provided that languages have a sufficient number of tokens" (Abstract, Section 3 Takeaway). However, the paper does not define or quantify what constitutes a "sufficient" number of tokens.
    *   In the "Fixed Total Budget" experiment, performance degrades when English exceeds 40-50% (Figure 1a, Figure 7a), but it is difficult to translate this percentage into a generalizable, absolute token count per language that practitioners can use as a guideline.
    *   This lack of quantification makes the primary takeaway less actionable. It is unclear how this threshold would change with more languages, different language distributions, or larger model sizes.*   **Potential Confounding Effect of Data Quality**
    *   The paper concludes that the "curse of multilinguality" under temperature sampling is partly a "curse of data quality," as oversampling low-resource languages introduces more "noisy data" (Section 6 Takeaway).
    *   This is a plausible interpretation, but it is not directly supported by empirical evidence within the paper. The quality of the data for the hundreds of low-resource languages from FineWeb2 is not measured or controlled for.
    *   The observed performance degradation could be due to other factors, such as poor tokenization for these languages or simply an extreme scarcity of data that even temperature sampling cannot overcome, rather than inherent "noisiness." The claim about data quality remains an assumption.*   **Limited Generalizability to Frontier-Scale Models**
    *   The authors correctly identify this in their limitations (Section Limitations), but it remains a significant weakness. The experiments are conducted on 1.1B and 3B parameter models.
    *   The dynamics of capacity saturation, negative interference, and cross-lingual transfer may be substantially different in models that are 10x to 100x larger.
    *   While the work is a valuable contribution at its scale, the findings cannot be assumed to hold for state-of-the-art foundation models without further validation, which should be more strongly caveated in the main conclusions.4) Suggestions for Improvement
*   **Incorporate Granular Downstream Task Analysis**
    *   To complement the aggregated scores in Figures 2 and 8, consider adding a table or plot in the main paper that breaks down performance by task category (e.g., reasoning, QA, classification) for a key experiment.
    *   For the "curse of multilinguality" experiment, showing how the average score changes for different task types as more languages are added would provide much richer insight than the current presentation.
    *   A per-language analysis for a representative subset of languages on key benchmarks would also strengthen the paper's claims about multilingual performance beyond a single aggregated number.*   **Attempt to Quantify the "Sufficient Tokens" Threshold**
    *   Provide a post-hoc analysis that correlates the number of training tokens for each non-English language with its final performance (either loss or downstream score) in the "Fixed Total Budget" setting. This could help identify an empirical "knee" in the performance curve, offering a more concrete estimate of what "sufficient" means at this scale.
    *   Discuss how this threshold might be expected to scale with model size or the total number of languages, even if speculatively, to make the guidance more practical for researchers working with different constraints.*   **Expand Discussion on Data Quality**
    *   In Section 6, explicitly frame the "curse of data quality" as a hypothesis rather than a direct finding.
    *   Strengthen the discussion by acknowledging data quality as a major uncontrolled variable. You could suggest how future work might address this, for example, by applying quality filtering heuristics to the low-resource data slices and re-running a subset of the experiments.
    *   Briefly discuss the potential role of the tokenizer (mentioned in Limitations) in amplifying issues with low-resource languages, as this is closely related to the data quality argument.*   **Carefully Frame Scalability Claims**
    *   In the Discussion & Conclusion (Section 8), more explicitly scope the findings to the 1-3B parameter scale.
    *   For instance, rephrase claims like "curriculum learning offer no demonstrable benefit" to "curriculum learning offer no demonstrable benefit at the 1B and 3B scales tested." This small change manages reader expectations and reinforces the need for future work at larger scales without diminishing the paper's contributions.5) Score
*   Overall (10): 8 — The paper presents a large-scale, methodologically sound study that provides strong evidence to refine several key assumptions in multilingual pretraining (Sections 3-6).
*   Novelty (10): 8 — While the questions are not entirely new, the scale of the investigation and the systematic, evidence-based refutation of common wisdom are highly novel and impactful (e.g., Figure 1b, Figure 3, Figure 5).
*   Technical Quality (10): 9 — The experimental design is rigorous, with excellent controls to isolate variables, such as the "Fixed Multilingual Budget" and "Controlled Growth" settings (Section 3, Section 6).
*   Clarity (10): 10 — The paper is exceptionally well-written and organized, with a clear structure, informative figures, and concise takeaways that make the complex results easy to follow (e.g., Section 3-6 Takeaways).
*   Confidence (5): 5 — I am highly confident in my assessment; the paper is clear, the experiments are well-described, and the claims are well-supported by the provided evidence.