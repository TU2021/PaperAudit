Review 1

Summary
- The paper studies how multilingual data mixtures affect LLM pretraining. Using 1.1B and 3B LLaMA-style decoder-only models, it varies (i) the English share under fixed total tokens vs fixed multilingual tokens (Sec. 3; Figs. 1–2; Tables 10–11), (ii) the identity of pivot languages on Slavic/Cyrillic-script sets (Sec. 4; Fig. 3; App. Tables 12–14), (iii) curriculum schedules for introducing languages (Sec. 5; Fig. 4), and (iv) the number of training languages from 25 to 400 under natural vs temperature sampling (Sec. 6; Table 1; Fig. 5; App. Tables 8–9). Main claims: more English does not harm other languages if enough non-English tokens are present; English is a strong cross-family pivot; curriculum ordering does not improve final multilingual performance; and the “curse of multilinguality” at this scale is largely a function of capacity and data-distribution/quality rather than language count.

Soundness
- The experimental design is largely sound and addresses relevant confounders: fixed-total-budget vs fixed-multilingual-budget disentangles composition from absolute token counts (Sec. 3). The curse-of-multilinguality analysis carefully compares natural vs temperature sampling and adds a controlled-growth setting that keeps earlier language token budgets constant (Fig. 5b,d), which strengthens causal interpretation.
- However, there are a few methodological gaps: (1) Learning-rate schedule appears specified for 100B tokens (App. A.1; Fig. 6/59), but some runs go to 225B tokens (Sec. 2; Table 11), and it is unclear how the schedule scales beyond 100B; this could influence convergence and comparisons. (2) The weighted averaging for “Multilingual” loss is not formally defined (Sec. 3; Fig. 1 captions), leaving unclear whether weights are token-proportional or uniform by language. (3) The pivot experiment mixes typology and script; moreover, the Slavic set listed in Sec. 4 includes Mongolian—which is not a Slavic language (Table 4 correctly labels Mongolian as Mongolic)—which complicates interpretation and suggests a taxonomy error.

Presentation
- The paper is generally clear, with useful figures and tables that reflect both validation loss and downstream averages (Figs. 1–2; Table 1; Fig. 5). The appendices document token allocations per configuration (Tables 10–14), which is commendable.
- Two clarity issues: (1) Appendix D’s Fig. 10 caption contradicts the text: it says increasing tail allocation reduces English loss, while the text claims the opposite (App. D: “loss decreases as more tokens from high-resource languages are included,” but Fig. 10 caption says “Increasing token allocation for tail languages reduces loss”). (2) Some figures are referenced but appear only as small placeholders in the draft (e.g., Figs. 5a–d heatmaps); values are reproduced in a table below but the visuals could be clearer.

Contribution
- The paper adds scale-appropriate evidence (1.1B/3B) challenging blanket assumptions that more English harms multilinguality and that adding languages necessarily hurts performance. The controlled-growth experiment and the clear separation of sampling regimes (natural vs temperature) are valuable for practitioners. The curriculum-learning null result helps redirect focus from ordering to data quantity/quality.

Strengths
- Careful separation of data-budget confounds (Sec. 3; Figs. 1–2).
- Controlled-growth analysis for curse-of-multilinguality with stable performance when token counts for existing languages are held fixed (Fig. 5b,d).
- Strong ablation breadth: 25→400 languages; 1.1B and 3B; two corpora (mC4 and FineWeb-2); multiple benchmarks aggregated per language with random baselines (App. B).
- Practical guidance: prioritize sufficient non-English tokens and data quality over strict balancing; English can be an effective global pivot (Sec. 4); curriculum does not help final outcomes (Sec. 5).

Weaknesses
- Reproducibility gap: LR schedule and other training hyperparameters seem tailored to 100B tokens (App. A.1; Fig. 6/59); scaling to 225B is unspecified.
- Averaging definition for “Multilingual” loss not explicit; could bias interpretations (Sec. 3; Fig. 1).
- Pivot-language experiment has a taxonomy inconsistency (Mongolian listed in “Slavic”; Sec. 4 vs Table 4), confounding conclusions about “stay in the family.”
- Some claims verge on over-generalization: Table 1 shows non-trivial degradation under temperature sampling as languages increase (e.g., English loss 2.675→2.707; English benchmarks 43.24→42.12), which suggests a conditional “curse” still exists with certain distributions. The abstract’s phrasing could be tempered.
- No analysis of language-ID noise and script/tokenizer coverage effect on lower-resource languages beyond the Limitations.

Questions
1) How was the LR schedule adapted for 225B-token runs in the fixed-multilingual-budget setting? If unchanged, could late-phase optimization differences bias results?
2) How is the “weighted average LM loss” across non-English languages computed (by validation-token counts, equal weights, or something else)?
3) Can you provide per-language trends showing that low-resource languages are not harmed when adding English in the fixed-multilingual-budget regime?
4) For the pivot experiment, please correct the Slavic set and re-run the analysis controlling script vs family (e.g., Slavic Latin vs Slavic Cyrillic).
5) What is the estimated LID error rate in FineWeb-2 across the 1,834 languages, and could misclassification affect natural vs temperature outcomes?

Rating
- Overall (10): 7 — Solid, carefully controlled study with useful practical takeaways but with reproducibility gaps and a pivot-set taxonomy error (Sec. 4; App. A.1; Table 1; Fig. 5).
- Novelty (10): 7 — Extends prior multilingual scaling/mixture work with larger models and broader language counts, particularly via controlled-growth and natural vs temperature comparisons (Sec. 6; Fig. 5).
- Technical Quality (10): 7 — Methodology is largely sound with informative ablations, but some confounds remain (LR schedule beyond 100B; weighting definitions; taxonomy inconsistency) (App. A.1; Sec. 3–4).
- Clarity (10): 7 — Generally clear, with thorough appendices; a few caption/text mismatches and figure placeholders should be fixed (App. D; Figs. 5, 10).
- Confidence (5): 4 — Based on close reading of figures/tables/appendices and consistency checks across sections, but some missing details limit full verification.


Review 2

Summary
- The paper investigates four assumptions about multilingual LLM pretraining: (1) more English hurts other languages, (2) pivot languages should come from the same family, (3) curriculum learning mitigates interference, and (4) adding more languages causes a curse of multilinguality. Using 1.1B/3B LLaMA-style models trained on mC4 (30 langs) and FineWeb-2 (up to 1,834 langs), the authors show: adding English on top of a fixed multilingual budget does not harm others (Figs. 1b, 2b), English is a strong cross-family pivot but combining multiple pivots is best (Sec. 4; Fig. 3), curricula change learning dynamics but not final multilingual loss (Sec. 5; Fig. 4), and increasing language count per se does not degrade performance under natural distributions when per-language tokens are preserved (Sec. 6; Table 1; Fig. 5).

Soundness
- The study’s core comparisons are logically motivated and the two budget regimes (Sec. 3) are crucial. The curse-of-multilinguality analysis is strengthened by controlled growth (Fig. 5b,d), which fairly tests the “count” vs “allocation/quality” hypothesis.
- Some concerns: (i) The pivot-language analysis is limited to Slavic/Cyrillic-script groups and small model size (1.1B), which may not generalize; and it confounds script and family. The reported inclusion of Mongolian in the “Slavic set” (Sec. 4) suggests a labeling error that undermines family-based conclusions. (ii) Curriculum schedules improve English largely due to more English tokens (Sec. 5), but the paper might under-emphasize the potential role of longer pretraining on English vs the order itself—more ablations equating English token counts across curricula would be helpful.

Presentation
- Organization is clear and each assumption is framed, tested, and summarized with a “Takeaway.” Extensive appendices add transparency about token counts and distributions (Tables 10–14; App. A.3).
- A few inconsistencies: Appendix D’s Fig. 10 caption contradicts the text (tail vs top languages effect on English), and references to figures in Sec. 6 include placeholder heatmaps that are hard to read; the accompanying numeric table ameliorates this.

Contribution
- Practically important insights: (a) do not fear high English fractions if absolute non-English tokens are sufficient; (b) natural distributions may be preferable to aggressive temperature sampling at this scale; (c) introducing many languages is not inherently harmful when token budgets and data quality are considered. The controlled-growth diagnostic is particularly instructive for practitioners.

Strengths
- Robust factorization of confounds (budget regimes; sampling strategies).
- Broad language coverage up to 400 for main results; multiple benchmarks with per-language aggregation and random baselines (App. B).
- Clear, actionable guidance for data mixture design and for avoiding over-interpretation of language-count effects.

Weaknesses
- Family vs script confounding and a taxonomy error in the pivot study (Sec. 4; Table 4 lists Mongolian as Mongolic, but Mongolian appears in the Slavic set description).
- Limited exploration of alternative pivots (e.g., Arabic for Semitic, Hindi for Indo-Aryan) and of pivot effects at 3B scale.
- Tokenizer choice and impact on non-Latin scripts acknowledged only in Limitations; could bias results (App. Limitations).
- Lack of generative evaluations (e.g., translation, long-form generation) where interference might manifest differently.

Questions
1) Can you correct and re-run the pivot experiment with a clean Slavic set and a separate “Cyrillic but non-Slavic” analysis to isolate family vs script?
2) How would results change if temperature were tuned per language-band to preserve head languages while capping tail repetition (e.g., UniMax-style sampling)?
3) Do per-language benchmark trends show any low-resource languages harmed in the fixed-multilingual-budget regime?
4) Could you report tokenizer coverage statistics (subword length distributions) by script/language to contextualize loss differences?

Rating
- Overall (10): 7 — Strong empirical contribution with useful diagnostics and guidance, but with pivot-set issues and limited pivot generality (Sec. 4; Fig. 3; Table 4).
- Novelty (10): 7 — New evidence at 1.1B/3B scales on data mixtures and a well-designed controlled-growth analysis for language count effects (Sec. 6; Fig. 5).
- Technical Quality (10): 7 — Careful experiments and ablations; some confounds remain (pivot taxonomy; limited pivot breadth; curriculum token counts) (Secs. 4–5).
- Clarity (10): 8 — Generally clear with thorough tables; small caption inconsistency and figure readability issues (App. D; Fig. 5).
- Confidence (5): 4 — Based on cross-checks of tables/appendices and consistency across sections; taxonomy error lowers confidence in one conclusion.


Review 3

Summary
- The work revisits whether English-heavy mixtures harm multilingual performance, whether family-internal pivots outperform cross-family pivots, whether curriculum can reduce interference, and whether adding languages inherently causes the curse of multilinguality. Using mC4 (30 langs) and FineWeb-2 (up to 400 langs in main results), with 1.1B/3B models and 100–225B tokens, the paper finds: adding English on top of sufficient non-English tokens does not hurt others (Figs. 1b, 2b); multiple pivots (EN+RU) outperform single pivots in Slavic/Cyrillic sets (Fig. 3); curricula alter trajectories but converge to similar final losses (Fig. 4); and under natural distributions, adding languages does not degrade English or non-English performance when per-language tokens are preserved (Table 1; Fig. 5).

Soundness
- The central causal interpretations are supported by sound contrasts: constant total tokens vs constant non-English tokens; natural vs temperature sampling; and the controlled-growth variant that keeps prior languages’ tokens fixed. Benchmark aggregation uses language-wise averages with random baselines (App. B), which is appropriate for multi-task heterogeneity.
- Potential validity issues: (a) The English/non-English benchmark trends in Fig. 2 show non-English accuracy near random (~35–36%) across settings—this suggests many tasks/languages remain at near-chance for 1.1B models, limiting sensitivity to detect small degradations. (b) The use of a single tokenizer (Mistral-Nemo-Base-2407; 131k) for 1,834 languages may bias against under-represented scripts; this could mask interference or transfer effects. (c) Training hyperparameters (App. A.1) appear specified for 100B tokens; behavior beyond 100B (up to 225B) needs clarification.

Presentation
- The assumptions are clearly enumerated and paired with experiments and takeaways. Token-allocation tables (10–14) are exemplary for transparency.
- Two inconsistencies: (1) App. D’s caption vs text contradiction for Fig. 10 (tail vs head allocation impact on English); (2) “Slavic set includes Mongolian” (Sec. 4) contradicts Table 4 typology and general linguistic classification.

Contribution
- The paper delivers practically important, scale-relevant evidence that (i) English need not harm others if absolute non-English tokens are adequate, (ii) aggressive temperature sampling can be counterproductive at this scale, and (iii) language count alone is not the culprit; instead, token allocations and data quality dominate. These insights can materially inform open multilingual LLM pretraining.

Strengths
- Clear experimental decompositions that practitioners can emulate (budget regimes; controlled growth).
- Breadth of language coverage and benchmarking with per-language random baselines.
- Honest limitations section acknowledging tokenizer and scale constraints.

Weaknesses
- Sensitivity of non-English downstream metrics is low in Fig. 2 (near-random), limiting the strength of “no harm” claims in downstream tasks for 1.1B.
- Pivot analysis confounded by taxonomy/script and an explicit labeling error; lacks broader pivot coverage and 3B replication.
- Unclear LR scheduling beyond 100B tokens in fixed-multilingual-budget runs.
- No exploration of generative tasks or translation; interference in such settings might differ from multiple-choice QA/RC.

Questions
1) Can you repeat key fixed-multilingual-budget results with harder downstream tasks where baselines are well above chance to ensure sensitivity?
2) Please clarify LR schedule and optimizer settings for >100B-token runs; were decay phases rescaled?
3) Could you include per-language tokenizer efficiency proxies (avg. tokens/char or BPE length) to relate loss/accuracy changes to subword fragmentation?
4) For the curse-of-multilinguality, can you report per-band (head, mid, tail) outcomes to verify that stability holds outside head languages?

Rating
- Overall (10): 6 — Useful and carefully designed study with actionable guidance, but sensitivity limits in downstream metrics and pivot taxonomy/script confounds weaken some claims (Figs. 2–3; App. D; App. A.1).
- Novelty (10): 6 — Incremental but meaningful, scaling previous insights to 1.1B/3B with better-controlled comparisons (Sec. 6).
- Technical Quality (10): 6 — Solid core methodology; some missing details and confounds (tokenizer bias; LR schedule; evaluation sensitivity) (Secs. 3–5; App. A.1).
- Clarity (10): 7 — Well structured with comprehensive tables; a few caption/text inconsistencies and figure-readability issues (App. D; Fig. 5).
- Confidence (5): 4 — Conclusions mostly supported by provided evidence, but evaluation sensitivity and taxonomy error limit full confidence.


Review 4

Summary
- This paper reevaluates multilingual pretraining mixtures using LLaMA-like 1.1B and 3B models. The authors vary English share (fixed total vs fixed multilingual tokens), pivot-language choice (EN vs RU vs EN+RU), curriculum schedules, and language count (25→400; natural vs temperature sampling; fixed vs controlled growth). Key results: sufficient absolute non-English tokens prevent degradation when adding English; English is a strong global pivot but combining with a family-proximal pivot helps at high pivot proportions; curricula change learning trajectories but not final multilingual performance; and under natural distributions, adding languages does not appreciably hurt performance when earlier language token budgets are preserved.

Soundness
- The comparisons are well-motivated and isolate key factors: token count vs composition, sampling distribution, and language count vs allocation. The controlled-growth experiment is particularly persuasive evidence against language-count as the main driver (Fig. 5b,d).
- Caveats: (i) Some improvements/degradations in Table 1 and Fig. 5 under temperature sampling do show a noticeable “curse” when low-resource languages are oversampled—so “no significant curse” needs qualification. (ii) It is unclear how validation sets are constructed and whether they reflect the same distributions as training (e.g., per-language domain balance). (iii) The pivot set definition includes an error (Mongolian in Slavic), undermining family-based interpretations.

Presentation
- Clear hypotheses, figures, and takeaways; thorough appendices with token tables. The LR schedule graphic (Fig. 6/59) is helpful.
- Issues to fix: (a) Contradictory caption in Appendix D Fig. 10; (b) Some heatmaps are too small; (c) Explicit definition of the weighting for “Multilingual” loss averages is missing.

Contribution
- The paper provides practically actionable diagnostics and guidance for large-scale multilingual pretraining: (1) protect absolute token counts for target languages; (2) prefer natural distributions or moderated sampling; (3) curriculum ordering is unlikely to fix interference; (4) adding languages is acceptable if quality and allocations are sensible. These are timely insights for open multilingual LLM efforts.

Strengths
- Systematic, compute-conscious design separating competing explanations (Sec. 3–6).
- Use of two corpora and two model scales; extensive reporting of token allocations.
- Clear practitioner takeaways grounded in empirical evidence.

Weaknesses
- Taxonomy/script confound and explicit labeling error in the pivot setup (Sec. 4), limiting the strength of conclusions about family boundaries.
- Some claims are broader than the evidence (e.g., “no significant curse”) given temperature-sampling degradations (Table 1; Fig. 5c,d).
- Reproducibility detail gaps for >100B-token runs and lack of validation-set construction details.
- Benchmarks cluster near chance for many non-English languages in 1.1B settings (Fig. 2), potentially masking small harms.

Questions
1) Please clarify validation data construction and weighting for aggregated multilingual loss: per-language domain matching and weighting scheme?
2) How were LR/decay phases adjusted for 225B-token runs, and were early/late-stage token allocations comparable across settings?
3) Would adding a script-controlled pivot study (e.g., Arabic for Arabic-script languages vs English) at 3B strengthen the “family not necessary” claim?
4) Can you include per-language plots for downstream metrics (as done for loss in App. Fig. 9) to verify there is no hidden degradation for specific low-resource languages?

Rating
- Overall (10): 7 — Valuable, careful empirical study with strong practical guidance; some over-generalization and pivot taxonomy/script issues should be addressed (Secs. 4, 6; Table 1; Fig. 5).
- Novelty (10): 7 — Offers new controlled analyses at 1.1B/3B and nuanced insights on sampling and language count (Sec. 6).
- Technical Quality (10): 7 — Good methodology with key controls; missing details on LR scheduling and validation construction, and a labeling error in pivot analysis (App. A.1; Sec. 4).
- Clarity (10): 7 — Mostly clear and well-documented; minor inconsistencies and figure readability issues (App. D Fig. 10; Fig. 5 heatmaps).
- Confidence (5): 4 — Based on careful cross-referencing of tables/figures/appendices; remaining inconsistencies and missing details temper confidence.