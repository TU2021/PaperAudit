Summary
The paper empirically re-examines common assumptions about multilingual LLM pretraining using LLaMA-style decoder-only models at 1.1B and 3B parameters trained for 100–225B tokens. It conducts four main investigations across mC4 (≈30 languages) and FineWeb-2 (up to 1,834 languages, with main experiments reported up to 400): (i) the effect of varying the English share under two regimes—fixed total tokens vs fixed multilingual (non-English) tokens; (ii) the role of pivot languages in a Slavic/Cyrillic-script setting, comparing English, Russian, and combinations; (iii) curriculum schedules for introducing languages; and (iv) the impact of increasing the number of training languages under natural distributions versus temperature sampling, including a controlled-growth setting that preserves token budgets for previously included languages. Core findings are that adding English on top of a sufficient absolute budget of non-English tokens does not harm other languages; English is a strong cross-family pivot, and combining pivots (e.g., EN+RU) can be beneficial; curriculum ordering changes training dynamics but does not improve final multilingual performance; and under natural distributions, adding languages does not inherently degrade performance when per-language token allocations for existing languages are maintained. The analysis highlights that observed degradation is more closely tied to model capacity, token allocations, sampling strategies, and data quality than to language count per se.

Strengths
- Well-controlled experimental design that isolates key confounders:
  - Clear contrasts between fixed total token budget and fixed multilingual token budget to separate composition effects from absolute token counts.
  - Natural vs temperature sampling, plus a controlled-growth variant that preserves earlier languages’ token budgets, strengthening causal interpretation of “language count” effects.
- Breadth and scale:
  - Two model sizes (1.1B and 3B), two corpora (mC4 and FineWeb-2), language counts up to 400 in main results, and multiple downstream benchmarks aggregated per language with random baselines.
- Actionable insights for practitioners:
  - Ensuring sufficient absolute non-English tokens mitigates potential harm when increasing English share.
  - English functions effectively as a global pivot; combining pivots can be advantageous.
  - Aggressive temperature sampling can be counterproductive at these scales; natural distributions or moderated schemes may be preferable.
  - Curriculum ordering does not improve final multilingual outcomes relative to token allocation and data quality.
- Transparency and reporting:
  - Extensive appendices documenting token allocations and experimental settings.
  - Clear framing of assumptions, experiments, and takeaways, with figures and tables covering both validation loss and downstream performance.

Weaknesses
- Reproducibility and methodological detail gaps:
  - Training hyperparameters, especially the learning-rate schedule, appear specified for 100B tokens, while some runs extend to 225B; it is unclear how schedules were adapted beyond 100B, potentially affecting convergence and cross-setting comparisons.
  - The weighting scheme for aggregated “Multilingual” loss is not explicitly defined (e.g., token-proportional vs uniform by language), which could bias interpretations of cross-language averages.
  - Limited detail on validation set construction and weighting (e.g., per-language domain balance), making it hard to assess representativeness.
- Pivot-language analysis concerns:
  - A taxonomy labeling error (Mongolian listed in a Slavic set) and script-family confounding weaken conclusions about family-internal versus cross-family pivots; pivot breadth is limited, and key results are primarily at 1.1B without broader replication at 3B.
- Evaluation sensitivity and scope:
  - Non-English downstream metrics for 1.1B models cluster near chance for many languages, reducing sensitivity to detect small degradations and potentially overstating “no harm” claims in downstream tasks.
  - Lack of generative evaluations (e.g., translation or long-form generation) where interference and transfer might manifest differently than in multiple-choice tasks.
  - Tokenizer choice for a very large number of languages may bias results for under-represented scripts; tokenizer coverage/efficiency statistics are not provided.
- Strength of claims vs evidence:
  - While controlled-growth under natural distributions indicates language count is not the primary driver of degradation, experiments with temperature sampling show noticeable performance drops, suggesting “no significant curse” needs qualification based on sampling regime and allocation.
- Presentation inconsistencies:
  - A caption-text contradiction in Appendix D (Fig. 10) regarding the effect of allocating tokens to tail vs head languages on English loss.
  - Some figures (e.g., heatmaps) are hard to read, and a few placeholders reduce clarity.
