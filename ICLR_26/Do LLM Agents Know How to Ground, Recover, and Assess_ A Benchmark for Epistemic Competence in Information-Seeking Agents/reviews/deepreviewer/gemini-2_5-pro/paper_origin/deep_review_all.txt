Here are four distinct reviews of the paper.

***

### **Review 1**

**Summary**
This paper introduces SeekBench, a novel benchmark designed to evaluate the epistemic competence of LLM-based information-seeking agents. Moving beyond traditional final-answer accuracy metrics, the authors propose a process-level evaluation framework that analyzes agent traces. The framework is built on three core epistemic competencies: (1) Groundedness, the ability to base reasoning on retrieved evidence; (2) Recovery, the ability to adapt search strategies when initial results are poor; and (3) Calibration, the ability to assess evidence sufficiency before answering. The authors develop a detailed annotation schema, validate it with high inter-annotator agreement, and use it to analyze thousands of traces from various state-of-the-art agents, revealing critical behavioral differences and trade-offs that accuracy-only metrics fail to capture.

**Soundness**
The methodology is exceptionally sound and rigorously executed. The authors adopt a principled, three-phase approach grounded in qualitative research methods (Content Analysis) to develop their annotation schema (Section 3.1). The schema's robustness is convincingly demonstrated through high inter-annotator agreement (human-human κ > 0.8) and strong alignment with LLM judges (κ > 0.7), which justifies the large-scale analysis (Section 3.1, Appendix 7). The formalization of the three competencies into quantitative metrics—RQI, ERF, and CE (Section 3.3)—is clear, logical, and well-motivated. The experimental design is comprehensive, covering multiple agent architectures and a diverse set of QA benchmarks, lending strong support to the paper's conclusions.

**Presentation**
The paper is exceptionally well-written, organized, and clear. The introduction effectively motivates the need for process-level evaluation and lays out the paper's contributions (Section 1). The methodology section (Section 3) is a model of clarity, systematically explaining the development from observable features to latent competencies and finally to operational metrics. The figures are highly effective and informative; for example, Figure 2 and Figure 3 provide an immediate, intuitive understanding of the core findings regarding reasoning quality and recovery dynamics. The use of appendices to provide extensive details on prompts, data, and supplementary analysis is appropriate and enhances the paper's transparency and reproducibility.

**Contribution**
The contribution of this work is significant and timely. By introducing SeekBench, the authors provide the community with the first comprehensive framework for evaluating the epistemic, procedural aspects of LLM agents. This work challenges the prevailing paradigm of accuracy-only evaluation and provides a much-needed vocabulary and toolkit for understanding *how* agents reason, not just *what* they answer. The finding that RL training can degrade evidence-grounded reasoning while improving final accuracy is a crucial insight that will likely spur new directions in agent training and alignment research. The agent synthesis experiment (Section 4.5) further underscores the value of this fine-grained analysis.

**Strengths**
1.  **Novel and Important Problem:** The paper tackles the critical and overlooked problem of evaluating the process, not just the outcome, of information-seeking agents.
2.  **Rigorous Methodology:** The development of the annotation schema and metrics is principled, transparent, and validated with strong empirical evidence of reliability (Section 3.1).
3.  **Actionable Insights:** The analysis yields clear, non-obvious findings, such as the trade-off between accuracy and reasoning quality (Section 4.2 vs. Appendix 9) and the identification of specific agent strengths (e.g., Search-R1's synthesis ability in Section 4.5).
4.  **Scalable Framework:** The successful validation of LLM-as-judge (Appendix 7) makes this sophisticated, process-level analysis scalable and practical for broader use by the research community.

**Weaknesses**
The paper is very strong, and weaknesses are minor.
1.  The discussion on the trade-offs between the different metrics (Accuracy, RQI, CE) is incredibly insightful but is currently located in the appendix (Section 13). This discussion is central to the paper's thesis and would have a greater impact if integrated into the main body.
2.  While the paper demonstrates the diagnostic power of SeekBench, it only briefly touches upon how these metrics could be used prescriptively (e.g., as reward signals for RL). Expanding on this future direction would further strengthen the paper's impact.

**Questions**
1.  The agent synthesis experiment (Section 4.5) is a fantastic proof of concept. Do the authors have plans to explore building a hybrid or mixture-of-experts agent that dynamically leverages different models for their specialized epistemic competencies?
2.  Given the finding that RL training degrades reasoning quality (RQI), do you have a hypothesis as to the mechanism? Could it be a form of reward hacking where agents learn to find shortcuts to the correct answer, bypassing sound reasoning, and could your framework be used to detect or penalize this?
3.  Do you plan to release SeekBench as a public benchmark with an associated leaderboard to encourage the community to optimize for these more nuanced epistemic competencies?

**Rating**
- Overall (10): 9 — This is a landmark paper that introduces a much-needed, rigorous framework for process-level agent evaluation, yielding significant new insights (Section 1, Section 4).
- Novelty (10): 10 — The concept of operationalizing and measuring epistemic competence for LLM agents through a multi-faceted benchmark is highly novel (Section 1, Section 3).
- Technical Quality (10): 9 — The methodology is rigorous, with strong validation for the annotation schema and clear formalization of metrics (Section 3.1, Section 3.3).
- Clarity (10): 10 — The paper is exceptionally well-written and organized, with clear explanations and excellent figures that support the narrative (Figure 2, Figure 3).
- Confidence (5): 5 — I am an expert in this area and am very confident in my assessment.

***

### **Review 2**

**Summary**
This paper proposes SeekBench, a benchmark for evaluating the "epistemic competence" of LLM search agents. The authors define three competencies—groundedness, recovery, and calibration—and introduce corresponding metrics: Reasoning Quality Index (RQI), Evidence Recovery Function (ERF), and Calibration Error (CE). They manually annotate 190 traces to develop a schema and then use an LLM-as-judge (GPT-4.1-mini) to annotate a large corpus of ~28k traces. The core claim is that these process-level metrics reveal agent failures and specializations that are missed by standard final-answer accuracy evaluations.

**Soundness**
The methodological soundness of this paper has several points of concern.
1.  **LLM-as-Judge Generalization:** The authors validate their LLM judge against human annotations on a small set of 190 traces (Section 3.1). While the agreement is "substantial" (κ ≈ 0.7-0.75), it is not perfect, and there is no guarantee that this level of performance holds across the full, diverse set of 28,493 traces used in the main experiments. Subtle distributional shifts or more complex edge cases in the larger dataset could lead to a degradation in annotation quality, potentially undermining the validity of the main findings in Section 4.
2.  **Arbitrary Metric Definitions:** The definitions of the core metrics rely on questionable simplifications. The `Evidence State` E is defined as a simple sum `C + Q` (Definition 3.1), which treats clarity and sufficiency as equally important and interchangeable; this is not well-justified. The `Evidence Recovery Function` (ERF) defines a recovery event as reaching `E=2` OR achieving a correct final answer (Eq. 7). Including `correct_i=1` conflates a process-level metric (evidence gathering) with a final outcome metric, which contradicts the paper's stated goal of separating the two.
3.  **Oversimplified Ideal Policy:** The `Calibration Error` (CE) metric is based on an "ideal policy" `π*(k) := I[k=2]` that answers *if and only if* the evidence is "good" (Definition 3.5). This is an unrealistic binary threshold. A truly calibrated agent might answer with partial evidence (`E=1`) but express low confidence, or it might need to see multiple pieces of `E=2` evidence for a complex question. The metric as defined penalizes any deviation from this rigid, unproven ideal.

**Presentation**
The paper is generally well-written, but the mathematical formalizations could be better motivated. For instance, the decomposition of RQI in Equation 4 is presented without much interpretation of what the terms mean until later. The core definitions in Section 3.3 are dense and would benefit from more running examples to build intuition before the results are presented. The reliance on appendices for critical details, such as the LLM prompts (Appendix 8) and the full IAA analysis (Appendix 7), makes it difficult to fully assess the methodology from the main text alone.

**Contribution**
The paper addresses an important problem: the inadequacy of accuracy-only evaluation for LLM agents. The creation of a human-annotated dataset and a conceptual framework for epistemic competence is a valuable starting point. However, the contribution is weakened by the methodological concerns. If the metrics are not robust or are based on flawed assumptions, then the conclusions drawn from them (e.g., that RL training degrades reasoning) may not be reliable. The paper's primary contribution is more in problem formulation than in providing a definitively sound solution.

**Strengths**
1.  **Problem Importance:** The paper correctly identifies and articulates the limitations of current agent evaluation paradigms.
2.  **Initial Dataset Creation:** The effort to create a 190-trace, expert-annotated dataset with a detailed schema is commendable and provides a valuable resource (Section 3.1).
3.  **Interesting High-Level Findings:** The conclusion that RL training may create a trade-off between accuracy and reasoning quality is provocative and warrants further investigation, even if the specific metrics used here are debatable.

**Weaknesses**
1.  **Questionable Validity of LLM-as-Judge at Scale:** The reliance on an LLM judge for the main results, with validation on only a small subset, is a significant methodological weakness.
2.  **Flawed Metric Definitions:** The definitions for `Evidence State`, `ERF`, and `CE` rely on arbitrary and oversimplified assumptions that may not reflect true epistemic competence (Section 3.3).
3.  **Conflation of Process and Outcome:** The ERF metric explicitly mixes a process goal (reaching `E=2`) with an outcome goal (`correct_i=1`), undermining the paper's central premise of process-level evaluation (Eq. 7).

**Questions**
1.  How can you be sure that the LLM judge's performance did not degrade on the larger, more diverse set of 28k traces? Have you performed any analysis on a random sample of the large-scale annotations to spot-check the LLM judge's quality?
2.  Can you provide a justification for defining the Evidence State `E` as a simple sum `C+Q`? Have you considered alternative formulations, for example, where sufficiency is weighted more heavily than clarity, or a non-linear combination?
3.  Why was the final answer correctness (`correct_i=1`) included in the definition of a recovery event (Eq. 7)? This seems to directly contradict the goal of creating a pure process-level metric. How would the ERF results in Figure 3 change if this term were removed?

**Rating**
- Overall (10): 5 — The paper tackles a vital problem but the methodological soundness of the proposed metrics and evaluation scale is questionable, casting doubt on the reliability of the conclusions.
- Novelty (10): 8 — The conceptual framework of epistemic competence is novel and important, though similar ideas of process evaluation exist.
- Technical Quality (10): 4 — The metrics are based on several poorly-justified and simplistic assumptions (Definitions 3.1, 3.4, 3.5), and the large-scale evaluation relies on an LLM judge whose generalization is unproven.
- Clarity (10): 7 — The paper is generally readable, but the core methodological definitions in Section 3.3 are dense and lack sufficient motivation and intuitive examples.
- Confidence (5): 4 — I have expertise in LLM evaluation and methodology and feel confident in my critique.

***

### **Review 3**

**Summary**
This paper introduces SeekBench, a practical framework for diagnosing the behavior of information-seeking LLM agents beyond simple answer accuracy. The authors define and measure three key skills: Groundedness (reasoning from evidence), Recovery (adapting search), and Calibration (knowing when to answer). By annotating thousands of agent interaction traces with an LLM-as-judge, they uncover actionable insights. For example, they find that RL-trained agents are often better at calibrating their answers but worse at evidence-grounded reasoning than simpler few-shot models. The paper culminates in an "agent synthesis" experiment, showing how these fine-grained insights can be used to combine agents for better performance.

**Soundness**
The methodology appears sound for its intended purpose: providing a scalable, diagnostic toolkit for agent developers. The authors' decision to use an LLM-as-judge is a pragmatic one, and they take the proper step of validating it against human experts, showing substantial agreement (Section 3.1, Appendix 7). This makes the approach feasible for large-scale analysis, which is a major advantage over purely manual annotation. The metrics (RQI, ERF, CE) are derived directly from the annotations and provide intuitive measures of the defined competencies. While one could debate the precise mathematical formulations, they serve as effective and interpretable indicators of agent behavior. The experimental setup is thorough, testing a good range of modern agents on diverse tasks.

**Presentation**
The paper is very well-presented from a practical standpoint. The structure is logical, moving from the problem to the solution and then to concrete results. The figures are a standout feature—they are clean, easy to interpret, and directly support the main takeaways. For instance, Figure 3 (Right) provides a clear, actionable comparison of different search strategies, showing that "Refine" and "Follow-up" are far more effective than "Repeat". Similarly, the tables summarizing calibration errors (Table 2) and agent synthesis results (Table 5) are concise and impactful. The paper does an excellent job of translating complex analysis into understandable, practical insights.

**Contribution**
The main contribution is a practical and scalable framework that allows researchers and engineers to look "under the hood" of LLM agents. This work moves the field from a "pass/fail" evaluation culture based on accuracy to a more diagnostic one focused on identifying specific failure modes and strengths. The most compelling part of the contribution is the demonstration that these diagnostics lead to tangible improvements, as shown in the agent synthesis experiment (Section 4.5). This proves the framework is not just an academic exercise but a useful tool for building better systems. The finding that different training methods (RL vs. few-shot) lead to different competency profiles is a key piece of practical knowledge for anyone training agents.

**Strengths**
1.  **Scalability and Practicality:** The use of a validated LLM-as-judge makes this sophisticated analysis practical to run at a scale that purely human-based evaluation could never achieve (Section 4.1).
2.  **Actionable Insights:** The analysis produces clear, useful heuristics for agent development (e.g., "RL training improves calibration but hurts planning," "focus on refine/follow-up search strategies").
3.  **Demonstrated Utility:** The agent synthesis experiment (Section 4.5) provides a strong proof-of-concept that the insights from SeekBench can be directly leveraged to improve performance.
4.  **Clear Problem Framing:** The paper effectively argues for the need for process-level evaluation in a way that resonates with the practical challenges of building reliable agents.

**Weaknesses**
1.  **Cost and Accessibility:** The paper does not discuss the computational cost of running the LLM-as-judge pipeline on ~28k traces. For labs with limited API access or budget, this could be a significant barrier to adoption. A brief discussion of cost/efficiency would improve the paper's practical guidance.
2.  **Post-Hoc Analysis:** The framework is presented as a post-hoc analysis tool. The paper would be even more impactful if it discussed the potential for integrating these metrics directly into the agent training loop, for example, by using RQI or CE as a reward signal in RL. This seems like a natural and important next step.

**Questions**
1.  The agent synthesis results in Section 4.5 are very promising. Have you considered using these competency profiles to build a dynamic mixture-of-experts model, where a router decides whether to use, for example, ASearcher for evidence gathering and Search-R1 for synthesis on a per-query basis?
2.  What was the approximate computational cost (e.g., in API calls or dollars) to run the GPT-4.1-mini annotations for the full experiment? Is this a one-time cost for the benchmark, or would a user need to re-run it to evaluate a new agent?
3.  You found that RL training degrades evidence-grounded reasoning (RQI). Do you think it would be feasible to incorporate RQI (as annotated by a judge model) as a penalty or auxiliary reward during RL training to mitigate this issue?

**Rating**
- Overall (10): 8 — A very useful and practical paper that provides a scalable framework for diagnosing and improving LLM agents, with strong, actionable findings.
- Novelty (10): 8 — The idea of a comprehensive, scalable epistemic evaluation framework is a novel and significant step beyond accuracy-only metrics.
- Technical Quality (10): 8 — The methodology is pragmatic and well-validated for its purpose, enabling a large-scale study that would otherwise be infeasible.
- Clarity (10): 9 — The paper is clearly written with excellent visualizations that make the results and their practical implications easy to understand (Figure 3, Table 5).
- Confidence (5): 5 — I have significant experience building and evaluating agentic systems and am confident in my assessment of the paper's practical value.

***

### **Review 4**

**Summary**
This paper presents SeekBench, a benchmark and evaluation framework designed to assess the "epistemic competence" of LLM-based search agents. The authors argue that conventional accuracy-based metrics are insufficient as they ignore the reasoning process. They propose a framework centered on three competencies—Groundedness, Recovery, and Calibration—which are measured using a novel set of metrics (RQI, ERF, CE) derived from fine-grained annotations of agent traces. Through a large-scale study on several SOTA agents, the paper reveals that different agents exhibit distinct competency profiles; notably, RL-trained agents excel at calibration but show deficits in evidence-grounded reasoning compared to a simple few-shot baseline.

**Soundness**
The paper's methodology is largely sound and thoughtfully constructed. The three-phase process for developing the metrics, starting with qualitative analysis and culminating in quantitative formalization, is a strong approach (Section 3). The initial schema development with human annotators and the high reported inter-annotator agreement (κ > 0.8) establish a solid foundation (Section 3.1). The subsequent validation of LLM judges against this human standard is a crucial and well-executed step, lending credibility to the large-scale annotation effort. The metrics themselves are mostly well-defined, though the ERF's inclusion of final answer correctness (Eq. 7) is a minor impurity in an otherwise process-focused framework. The experiments are extensive and the analysis is statistically careful, for instance, using Kaplan-Meier analysis for the right-censored recovery data (Section 4.3).

**Presentation**
The paper is well-structured and clearly written. The introduction provides excellent motivation for the work, and the related work section (Section 2) effectively positions the paper's contribution relative to existing evaluation methods. The core methodology in Section 3 is detailed and logical. The results in Section 4 are presented systematically, competency by competency, which aids comprehension. One area for improvement would be the placement of the "Discussion: Understanding Metric Trade-offs" section (Appendix 13). This section contains some of the most insightful, high-level synthesis in the paper, and it feels relegated to an afterthought. Integrating these points more directly into the main discussion would strengthen the paper's narrative arc and highlight the importance of the findings.

**Contribution**
This paper makes a significant and timely contribution to the field of LLM evaluation. It successfully moves beyond the simplistic "right or wrong" paradigm to a more nuanced, multi-dimensional assessment of agent behavior. The primary contribution is the conceptual framework of epistemic competence itself, operationalized into a scalable and interpretable set of metrics. The paper provides both a tool (SeekBench) and a compelling demonstration of its utility. The findings, particularly the surprising negative correlation between RL training and reasoning quality (Section 4.2), challenge common assumptions in the field and open up important new avenues for research into agent alignment and training. The work provides a much-needed language and methodology for discussing and improving the reliability and trustworthiness of autonomous agents.

**Strengths**
1.  **Strong Conceptual Framework:** The paper introduces a clear and coherent framework for thinking about agent competence (Groundedness, Recovery, Calibration) that is intuitive and powerful.
2.  **Rigorous and Transparent Development:** The methodology for creating the annotation schema and metrics is systematic and well-documented, with strong validation through inter-annotator agreement studies (Section 3.1, Appendix 7).
3.  **Significant and Surprising Findings:** The paper uncovers important, non-obvious dynamics, such as the trade-off between accuracy and reasoning quality induced by RL training (Section 4.2), which has major implications for the field.
4.  **Large-Scale Empirical Evidence:** The conclusions are supported by a large-scale experiment involving thousands of traces across multiple models and datasets, giving them substantial weight.

**Weaknesses**
1.  **Suboptimal Placement of Key Discussion:** The excellent discussion on metric trade-offs is buried in Appendix 13. This synthesis is crucial for interpreting the paper's overall message and should be in the main body.
2.  **Idealized Calibration Metric:** The Calibration Error (CE) metric is defined against a rigid "ideal policy" of answering if and only if `E=2` (Definition 3.5). This is a strong assumption, and the paper would benefit from a brief discussion of this simplification and potential alternatives.

**Questions**
1.  The discussion on metric trade-offs in Appendix 13 is excellent. Have the authors considered promoting this to a full section in the main paper? I believe it would help readers synthesize the results and better appreciate the nuances of the evaluation framework.
2.  The paper convincingly shows that current RL training methods degrade reasoning quality (RQI). Based on your analysis, do you have a hypothesis for *why* this occurs? Is the reward function for accuracy simply providing a gradient that encourages finding shortcuts, even if they are ungrounded?
3.  Regarding the Calibration Error metric, the choice of `π*(k) = I[k=2]` as the ideal policy is a hard threshold. Could you comment on how your results might change if a "softer" ideal were used, for instance, one that allows for answering with `E=1` but expects a lower probability of doing so?

**Rating**
- Overall (10): 9 — An important and well-executed paper that provides a significant contribution to LLM agent evaluation with insightful and surprising findings.
- Novelty (10): 9 — The paper introduces a novel, comprehensive framework for evaluating epistemic competence at scale, moving significantly beyond prior work.
- Technical Quality (10): 9 — The methodology is rigorous, transparent, and well-validated, with only minor points of contention in the metric definitions.
- Clarity (10): 9 — The paper is very clearly written and structured, though it could be improved by moving the key discussion from the appendix to the main text.
- Confidence (5): 5 — I am highly confident in my evaluation, based on my expertise in LLM reasoning and evaluation.