Summary
The paper introduces SeekBench, a process-level benchmark for evaluating the epistemic competence of LLM search agents along three dimensions: groundedness, recovery, and calibration. It formalizes a compact evidence state with binary clarity and sufficiency and operationalizes the competencies via three interpretable metrics: RQI for evidence-grounded reasoning, ERF for evidence recovery, and CE for answer calibration. The authors curate 190 expert-annotated traces (over 1,800 steps) with high inter-annotator agreement and scale to 28,493 traces using an LLM judge. Evidence-conditioned decompositions and survival analysis are used to diagnose agent behavior at the step and action-type level.

Empirically, few-shot prompting achieves the strongest grounded reasoning, while reinforcement learning (RL) agents improve evidence recovery and reduce overconfident answering, improving calibration. Survival curves indicate REFINE and FOLLOW-UP actions accelerate recovery relative to REPEAT. An “agent synthesis” experiment shows that combining specialized competencies—such as Search‑R1’s information synthesis and conservative answering—with stronger retrieval from other agents yields gains in end-task F1. The paper argues that process-level evaluation reveals competency-specific effects that accuracy-only metrics obscure.

Strengths
- Clear motivation for process-level evaluation beyond final-answer accuracy, with a coherent decomposition of epistemic competence into groundedness, recovery, and calibration.
- Formal, interpretable metrics tied to a compact evidence-state model, with evidence-conditioned decompositions that help localize error sources.
- High-quality human annotation pipeline with strong agreement and transparent, reproducible prompts for the LLM judge.
- Insightful empirical findings that contradict naive expectations: few-shot prompting yields better groundedness, while RL training improves recovery and calibration.
- Survival-analysis-based profiling of recovery by action type provides interpretable, actionable guidance (e.g., REFINE/FOLLOW-UP outperform REPEAT).
- Practical system-design implications via agent synthesis, showing complementary strengths across agents and gains in end-task performance.
- Broad empirical scope across multiple QA datasets and agent variants.

Weaknesses
- Metric design may bias competency estimates:
  - Grounding protocol labels plan-only steps as Not Grounded, conflating absence of factual claims with unsupported claims and likely depressing planning-type RQI irrespective of planning quality.
  - CE assumes an ideal policy that answers only when the per-turn evidence state is fully sufficient (E=2), which does not model cumulative, multi-hop evidence composition; reasonable early answers constructed from partial but jointly sufficient evidence are penalized.
  - ERF counts “first E=2” or a correct answer as recovery, blending genuine search improvement with lucky guessing and weakening the causal link to search adaptation.
  - Binary clarity/sufficiency in the evidence state is coarse and may miss gradations and interactions (e.g., high clarity but narrowly insufficient evidence).
- Reliance on a single LLM judge (GPT‑4.1‑mini) to scale annotations raises concerns about bias, style dependence, and reproducibility; cross-judge sensitivity and human-only validation at scale are limited.
- The human-annotated set is relatively small (190 traces), and the data sanitization step that removes questions answerable without search likely biases the benchmark toward retrieval-heavy queries and toward the abstention style of the chosen LLM judge; the implications of this filtering are not fully examined.
- Statistical and reporting gaps: ERF aggregates do not explicitly treat censoring or trace-length heterogeneity; survival curves would benefit from number-at-risk reporting and clearer uncertainty quantification.
- Minor presentation issues detract from polish and clarity, including figure labeling inconsistencies (E ≤ 2 vs E < 2), naming errors (“CompetenceBench”), scattered “A PREPRINT” headers, and a contradictory description of evidence sufficiency in the appendix.
