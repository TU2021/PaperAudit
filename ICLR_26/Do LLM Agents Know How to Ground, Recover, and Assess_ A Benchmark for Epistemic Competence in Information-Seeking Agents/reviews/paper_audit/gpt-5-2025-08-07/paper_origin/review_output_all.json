{
  "baseline_review": "Summary\n- The paper introduces SeekBench, a process-level benchmark to evaluate the epistemic competence of LLM search agents—specifically evidence-grounded reasoning, adaptive recovery, and calibrated answering—using expert-annotated, multi-turn traces. It formalizes an evidence state and three metrics: Reasoning Quality Index (RQI, Section 3.3.1), Evidence Recovery Function (ERF, Section 3.3.2), and Calibration Error (CE, Section 3.3.3). SeekBench includes 190 expert-annotated traces with >1,800 steps (Figure 1) and is scaled via LLM-as-judge to 28,493 traces and 283,950 steps across seven QA datasets (Section 4.1). Key findings include: RL training improves calibration (Table 2) and recovery (Figure 3) but degrades evidence-grounded reasoning (Figure 2), and different agents specialize (e.g., Search-R1 excels in synthesis; Section 4.5 and Appendix 12).Strengths\n- Bold formalization of epistemic competence\n  - Clear definition of evidence state combining clarity and sufficiency (Definition 3.1, Equation 2), which makes the latent notion of “epistemic adequacy” operational; this contributes interpretability and technical soundness.\n  - RQI, ERF, and CE are rigorously specified with trace-level decompositions and model-level aggregation (Definitions 3.2–3.5; Equations 3–6, 8–10), supporting reproducible measurement (novelty/technical soundness).\n  - Table 1 links competencies to metrics and annotated features, enabling systematic mapping from annotations to constructs (impact/clarity).- Robust annotation schema with validation\n  - The schema distinguishes functional roles (InformationSynthesis/PlanFormation/StateAssessment) and quality attributes (grounding, clarity, sufficiency) (Section 3.1; Figure 1), aiding granular analysis (clarity/impact).\n  - Iterative, kappa-based refinement from 12 to 8 features (Section 3.1) demonstrates methodological rigor (technical soundness).\n  - Strong human agreement (κ = 0.811) and substantial human–LLM alignment (GPT-4.1-mini κ = 0.731; GPT-5 κ = 0.754) are reported (Section 3.1; Appendix 7; Figure 5), supporting reliability of scaling (experimental rigor).- Large-scale, multi-dataset evaluation\n  - Seven QA datasets spanning single- and multi-hop tasks (NQ, TriviaQA, PopQA, HotpotQA, 2Wiki, MusiQue, Bamboogle) are used (Section 4.1), improving external validity (experimental rigor/impact).\n  - 28,493 traces and 283,950 steps enable pattern discovery beyond small studies (Section 4.1), strengthening statistical reliability (experimental rigor).\n  - Accuracy baselines (Table 3, Appendix 9/70) contextualize process-level metrics (clarity/impact).- Recovery analysis grounded in survival methods\n  - ERF quantifies the time-to-recovery across models (Definition 3.4; Figure 3 Left), adding a principled temporal lens (technical soundness).\n  - Use of Kaplan–Meier for right-censored trajectories (Section 4.3; Figure 3 Right) shows appropriate handling of incomplete sequences (experimental rigor).\n  - Empirical identification that REFINE and FOLLOW-UP recover fastest while REPEAT helps least (Figure 3 Right), actionable for algorithm design (impact).- Calibration findings that connect evidence state to answer behavior\n  - P(answer|E) and P(correct|answer, E) increase with evidence quality (Figure 4), validating construct validity of “evidence state” (Section 3.3 validity claim; Section 4.4) (technical soundness).\n  - RL agents reduce overconfident answering and achieve the lowest CE (Table 2), revealing training-specific competencies (impact).\n  - Failure modes are explicitly categorized (overconfident vs overcautious; Section 4.4), improving diagnostic clarity (clarity).- Process-level insights that accuracy-only metrics miss\n  - Few-shot achieves highest RQI despite not being best in F1 (Figure 2 Left; Appendix 9/70), exposing reasoning–accuracy tradeoffs (impact).\n  - Case study of correct answer via ungrounded reasoning (Appendix 10.1/76) illustrates why process evaluation is necessary (clarity/impact).\n  - Decomposition by reasoning type pinpoints synthesis vs plan/state weaknesses (Figure 2 Right; Appendix 10.2/78), guiding targeted training (impact).- Agent specialization and synthesis experiments\n  - Search-R1 shows strong InformationSynthesis (Figure 2 Right; Appendix 10.2/78) and conservative answering (Appendix 11/79), enabling complementary system design (impact).\n  - Agent synthesis improves F1 (Table 5, Appendix 12/82–86), demonstrating practical utility of competency-aware modularization (novelty/impact).\n  - Observation that Base evidence can boost other agents (Appendix 12/86) reveals hidden strengths masked by accuracy-only evaluation (impact).- Clear, reproducible prompts for LLM-as-judge\n  - Detailed judging prompts for reasoning grounding, search behavior, and evidence quality (Appendix 8.1–8.3; Blocks 63–68) aid transparency and reusability (clarity/technical soundness).\n  - Strict grounding rules (Appendix 8.1) enhance methodological discipline, e.g., superlatives require explicit spans (technical soundness).\n  - Search-type taxonomy (Initial/Refine/FollowUp/Repeat; Appendix 8.2) and quality criteria (Sufficient/Clear; Appendix 8.3) are well-defined (clarity).Weaknesses\n- Reliance on LLM-as-judge with limited human coverage\n  - Large-scale annotations rely on GPT-4.1-mini (Section 4.1), with only 190 human-annotated traces (Figure 1; Section 3.1), which may limit generalization of judgments across domains (experimental rigor).\n  - Human–LLM alignment is substantial but not perfect (κ ≤ 0.754; Appendix 7; Figure 5), leaving room for systematic bias in labels (technical soundness).\n  - Data sanitization uses GPT-4.1-mini decisions (Appendix 6/59), introducing potential circularity between the judge and selection criteria (experimental rigor).\n  - Use of proprietary, possibly non-public GPT-5 for adversarial schema validation (Section 3.1; References [OpenAI, 2025]) may hinder reproducibility (reproducibility).- Metric design choices may entrench biases and ignore sequential decision points\n  - CE’s “ideal policy” answers iff E=2 (Definition 3.5) assumes strict sufficiency/clarity, which may not be appropriate across all QA tasks (technical soundness).\n  - RQI penalizes plan-only text as “Not Grounded” (Appendix 8.1 grounding rules), potentially biasing PlanFormation scores downward by design (novelty/interpretability).\n  - Evidence state E = C + Q (Definition 3.1) conflates clarity and sufficiency linearly without weighting or relevance, possibly oversimplifying epistemic adequacy (technical soundness).\n  - CE aggregates over P(E) and P(answer|E) without explicit turn-level decision conditioning (Equation 10), potentially masking temporal calibration properties (experimental rigor).- Internal inconsistencies and numerical clarity issues\n  - Conflicting statements about Search-R1’s synthesis vs overall RQI and evidence-conditioned grounding (Figure 2 Right shows synthesis strength; Appendix 75 claims misalignment yet cites low values such as 0.10 at E=1 and 0.14 at E=2), which is hard to reconcile (clarity).\n  - The notation “P(remaining E ≤ 2)” in Figure 3 Right conflicts with narrative that recovery is reaching E=2; the intended event is likely E<2 (clarity).\n  - Mixed claims about which model leads in synthesis (e.g., Section 4.2 lists ASEARCHER 0.56; Figure 2 Right/Appendix 10.2 show Search-R1 highest at ≈0.63), creating ambiguity (clarity).\n  - Some figures show CIs but statistical testing details (e.g., bootstrap, significance) are absent; CE lacks uncertainty estimates (Figures 2–4; Table 2) (experimental rigor).- Novelty positioning and comparative baselines are underdeveloped\n  - The “first benchmark” claim is made (Abstract; Introduction) without head-to-head process-level comparisons against prior trace-level or reasoning-quality frameworks cited (Related Work, Section 2); No direct evidence found in the manuscript demonstrating superiority over specific existing process analyses (novelty).\n  - No ablation against alternative metric formulations (e.g., different evidence-state encodings or grounding criteria) to substantiate chosen metrics (Section 3.3) (technical soundness).\n  - Limited evaluation of how SeekBench’s metrics correlate with external evaluations (e.g., LLM-as-judge correctness) beyond Figure 4’s correlation of evidence with accuracy (Section 4.4), leaving construct validation incomplete (experimental rigor).\n  - Absence of user/task-level outcomes (e.g., utility or safety), which would support the practical impact of epistemic competence (impact).- Dataset scale and representativeness limitations\n  - The core expert-annotated set is only N=190 traces (>1,800 steps; Figure 1), which is small relative to the diversity of seven datasets (Section 4.1), risking sampling bias (experimental rigor).\n  - The distribution of trace sources, task types, and agent allocation across the 190 traces is not detailed (No direct evidence found in the manuscript), limiting interpretability (clarity).\n  - Data sanitization criteria might skew difficulty (Appendix 6/59), but the effect size on dataset composition is not quantified (experimental rigor).\n  - Inter-annotator agreement is aggregated; per-dataset/per-agent agreement and failure analysis are not reported (Appendix 7/60–61), limiting granularity (clarity).- Reproducibility and release details are insufficient\n  - The paper does not clearly state dataset/code/model release plans (No direct evidence found in the manuscript), hindering verification (reproducibility).\n  - Proprietary LLMs (GPT-4.1/4.1-mini/5) used for judging and validation (Section 3.1; Appendix 8) may be version-sensitive; prompts are provided, but judge configurations and hyperparameters are not fully specified (Appendix 8) (reproducibility).\n  - ERF survival analysis methodology is described conceptually (Section 4.3) but lacks implementation details (e.g., censoring rules, confidence interval computation) (experimental rigor).\n  - CE computation lacks reporting of uncertainty and sensitivity analyses (Section 4.4; Table 2), making it hard to assess robustness (technical soundness).Suggestions for Improvement\n- Strengthen human-grounded validation and mitigate judge dependence\n  - Expand human annotation beyond N=190 to cover more models/datasets, and report per-dataset/per-agent agreement (extend Appendix 7; Figure 5) to improve generalization.\n  - Quantify and analyze disagreement patterns between human and LLM judges (Appendix 7), including error taxonomy, to identify systematic biases.\n  - Reduce circularity in data sanitization by using independent human criteria rather than GPT-4.1-mini (Appendix 6/59); report how many items were removed and their characteristics.\n  - Avoid reliance on GPT-5 for validation (Section 3.1); if used, provide alternative, publicly reproducible validation with open models and release full judge prompts and settings (Appendix 8).- Refine metric definitions to better capture process and avoid built-in bias\n  - Calibrate CE against task difficulty by relaxing the “iff E=2” policy (Definition 3.5), e.g., introduce graded targets or weight sufficiency vs clarity per dataset; provide sensitivity analyses.\n  - Adjust RQI grounding rules so plan-only steps are evaluated with plan-quality criteria (e.g., relevance/consistency) instead of auto-labeling as Not Grounded (Appendix 8.1); report effects on RQI_type.\n  - Revisit evidence state E = C + Q (Definition 3.1) with alternative encodings (e.g., relevance-weighted, non-linear aggregation); include ablations comparing formulations.\n  - Introduce turn-conditional CE (Equation 10) or temporal calibration curves to reflect sequential decision-making; report uncertainty for CE and ERF.- Resolve internal inconsistencies and enhance statistical reporting\n  - Reconcile claims about Search-R1’s synthesis and evidence-conditioned groundedness (Figure 2 Right vs Appendix 75/78) by standardizing metrics and clearly annotating which figures use which datasets/splits.\n  - Correct notation in Figure 3 Right to reflect the intended event (remaining in E<2), and align the narrative with plotted quantities.\n  - Provide consistent model-wise synthesis scores (Section 4.2 vs Figure 2 Right) and explicitly label whether values are RQI_type or conditional groundedness; add table summarizing numeric values.\n  - Add statistical tests and uncertainty reporting (e.g., bootstrap CIs) for RQI, ERF, CE (Figures 2–4; Table 2) and document methods for CI computation.- Deepen novelty positioning with comparative baselines and validation\n  - Conduct head-to-head comparisons with existing process-level analyses (cited in Section 2), mapping their metrics to SeekBench and reporting where SeekBench adds unique diagnostic power.\n  - Provide metric ablations (Section 3.3), demonstrating why chosen definitions outperform alternatives on construct validity and predictive utility.\n  - Expand construct validation beyond Figure 4 by correlating SeekBench metrics with independent outcome measures (e.g., human-rated trustworthiness) where feasible.\n  - Include small user studies or application-specific evaluations to demonstrate practical utility (e.g., reduced hallucination rates), strengthening impact claims.- Improve dataset scale, transparency, and representativeness\n  - Report detailed statistics of the 190-trace set: per-dataset distribution, agent mix, step-type proportions, and sampling criteria (extend Figure 1; Appendix 6–7).\n  - Increase the size of expert-annotated traces to better cover multi-hop tasks and challenging domains; report how scaling affects κ and metric stability.\n  - Quantify the impact of sanitization (Appendix 6): how many items were removed by each criterion, and how this changed evidence-state distributions.\n  - Provide per-dataset/per-agent agreement and failure analyses (Appendix 7), highlighting where the schema is most/least reliable.- Enhance reproducibility and methodological transparency\n  - State clear plans for releasing SeekBench data, code, judge prompts, and annotation tools (No direct evidence found in the manuscript), including licenses and versions.\n  - Document judge configurations (model names/versions, temperature, max tokens) and decision thresholds (Appendix 8), enabling exact replication.\n  - Provide ERF implementation details (Section 4.3), including censoring rules, variance estimation, and CI computation; publish scripts for survival analysis.\n  - Report CE uncertainty and sensitivity analyses (Section 4.4; Table 2), and release code to compute CE given annotated traces.Score\n- Overall (10): 7 — Clear formalization and insightful process-level findings (Definitions 3.1–3.5; Figures 2–4; Table 2), tempered by LLM-judge dependence and some inconsistencies (Section 4.1; Appendix 7; Appendix 75/78).\n- Novelty (10): 7 — Introduces an evidence-state framework and three metrics (Table 1; Section 3.3), plus agent synthesis (Section 4.5; Appendix 12), though “first” claim lacks direct comparative validation (Related Work, Section 2).\n- Technical Quality (10): 7 — Solid metric formalization and survival analysis (Sections 3.3.1–3.3.3; 4.3; Figure 3), but CE/RQI design choices and reliance on LLM judges warrant further ablation and robustness checks (Section 4.1; Appendix 8).\n- Clarity (10): 6 — Generally well-structured with prompts and figures (Figure 1; Appendix 8), yet some numerical and narrative inconsistencies remain (Figure 2 vs Appendix 75/78; Figure 3 Right notation), and limited statistical reporting (Table 2).\n- Confidence (5): 4 — Assessment based on the full manuscript with extensive figures/tables (Sections 3–4; Appendix 6–12), but some reproducibility details and data release plans are missing (No direct evidence found in the manuscript).",
  "final_review": "Summary\n- The paper introduces SeekBench, a process-level benchmark to evaluate the epistemic competence of LLM search agents—specifically evidence-grounded reasoning, adaptive recovery, and calibrated answering—using expert-annotated, multi-turn traces. It formalizes an evidence state and three metrics: Reasoning Quality Index (RQI, Section 3.3.1), Evidence Recovery Function (ERF, Section 3.3.2), and Calibration Error (CE, Section 3.3.3). SeekBench includes 190 expert-annotated traces with >1,800 steps (Figure 1) and is scaled via LLM-as-judge to 28,493 traces and 283,950 steps across seven QA datasets (Section 4.1). Reported findings include: RL training improves calibration (Table 2) and recovery (Figure 3) but degrades evidence-grounded reasoning (Figure 2), and different agents specialize (e.g., Search-R1 excels in synthesis; Section 4.5 and Appendix 12).Strengths\n- Bold formalization of epistemic competence\n  - Clear definition of evidence state combining clarity and sufficiency (Definition 3.1, Equation 2), which makes the latent notion of “epistemic adequacy” operational; this contributes interpretability and technical soundness.\n  - RQI, ERF, and CE are rigorously specified with trace-level decompositions and model-level aggregation (Definitions 3.2–3.5; Equations 3–6, 8–10), supporting reproducible measurement (novelty/technical soundness).\n  - Table 1 links competencies to metrics and annotated features, enabling systematic mapping from annotations to constructs (impact/clarity).\n- Robust annotation schema with validation\n  - The schema distinguishes functional roles (InformationSynthesis/PlanFormation/StateAssessment) and quality attributes (grounding, clarity, sufficiency) (Section 3.1; Figure 1), aiding granular analysis (clarity/impact).\n  - Iterative, kappa-based refinement from 12 to 8 features (Section 3.1) demonstrates methodological rigor (technical soundness).\n  - Strong human agreement (κ = 0.811) and substantial human–LLM alignment (GPT-4.1-mini κ = 0.731; GPT-5 κ = 0.754) are reported (Section 3.1; Appendix 7; Figure 5), supporting reliability of scaling (experimental rigor).\n- Large-scale, multi-dataset evaluation\n  - Seven QA datasets spanning single- and multi-hop tasks (NQ, TriviaQA, PopQA, HotpotQA, 2Wiki, MusiQue, Bamboogle) are used (Section 4.1), improving external validity (experimental rigor/impact).\n  - 28,493 traces and 283,950 steps enable pattern discovery beyond small studies (Section 4.1), strengthening statistical reliability (experimental rigor).\n  - Accuracy baselines (Table 3, Appendix 9/70) contextualize process-level metrics (clarity/impact).\n- Recovery analysis grounded in survival methods\n  - ERF quantifies the time-to-recovery across models (Definition 3.4; Figure 3 Left), adding a principled temporal lens (technical soundness).\n  - Use of Kaplan–Meier for right-censored trajectories (Section 4.3; Figure 3 Right) shows appropriate handling of incomplete sequences (experimental rigor).\n  - Empirical identification that REFINE and FOLLOW-UP recover fastest while REPEAT helps least (Figure 3 Right), actionable for algorithm design (impact).\n- Calibration findings that connect evidence state to answer behavior\n  - P(answer|E) and P(correct|answer, E) increase with evidence quality (Figure 4), validating construct validity of “evidence state” (Section 3.3 validity claim; Section 4.4) (technical soundness).\n  - RL agents reduce overconfident answering and achieve the lowest CE (Table 2), revealing training-specific competencies (impact).\n  - Failure modes are explicitly categorized (overconfident vs overcautious; Section 4.4), improving diagnostic clarity (clarity).\n- Process-level insights that accuracy-only metrics miss\n  - Few-shot achieves highest RQI despite not being best in F1 (Figure 2 Left; Appendix 9/70), exposing reasoning–accuracy tradeoffs (impact).\n  - Case study of correct answer via ungrounded reasoning (Appendix 10.1/76) illustrates why process evaluation is necessary (clarity/impact).\n  - Decomposition by reasoning type pinpoints synthesis vs plan/state weaknesses (Figure 2 Right; Appendix 10.2/78), guiding targeted training (impact).\n- Agent specialization and synthesis experiments\n  - Search-R1 shows strong InformationSynthesis (Figure 2 Right; Appendix 10.2/78) and conservative answering (Appendix 11/79–81), enabling complementary system design (impact).\n  - Agent synthesis improves F1 (Table 5, Appendix 12/82–86), demonstrating practical utility of competency-aware modularization (novelty/impact).\n  - Observation that Base evidence can boost other agents (Appendix 12/86) reveals hidden strengths masked by accuracy-only evaluation (impact).\n- Clear, reproducible prompts for LLM-as-judge\n  - Detailed judging prompts for reasoning grounding, search behavior, and evidence quality (Appendix 8.1–8.3; 63–68) aid transparency and reusability (clarity/technical soundness).\n  - Strict grounding rules (Appendix 8.1) enhance methodological discipline, e.g., superlatives require explicit spans (technical soundness).\n  - Search-type taxonomy (Initial/Refine/FollowUp/Repeat; Appendix 8.2) and quality criteria (Sufficient/Clear; Appendix 8.3) are well-defined (clarity).Weaknesses\n- Reliance on LLM-as-judge with limited human coverage\n  - Large-scale annotations rely on GPT-4.1-mini (Section 4.1), with only 190 human-annotated traces (Figure 1; Section 3.1), which may limit generalization of judgments across domains (experimental rigor).\n  - Human–LLM alignment is substantial but not perfect (κ ≤ 0.754; Appendix 7; Figure 5), leaving room for systematic bias in labels (technical soundness).\n  - Data sanitization uses GPT-4.1-mini decisions (Appendix 6/59), introducing potential circularity between the judge and selection criteria (experimental rigor).\n  - Use of proprietary, possibly non-public GPT-5 for adversarial schema validation (Section 3.1; References [OpenAI, 2025]) may hinder reproducibility (reproducibility).\n- Metric design choices may entrench biases and ignore key definitions\n  - CE’s “ideal policy” answers iff E=2 (Definition 3.5) assumes strict sufficiency/clarity, which may not be appropriate across all QA tasks (technical soundness).\n  - RQI penalizes plan-only text as “Not Grounded” (Appendix 8.1 grounding rules), potentially biasing PlanFormation scores downward by design (novelty/interpretability).\n  - Evidence state E = C + Q (Definition 3.1) conflates clarity and sufficiency linearly without weighting or relevance, possibly oversimplifying epistemic adequacy (technical soundness).\n  - Recovery event definition mixes trace- and turn-level variables: Equation (7) sets T_recover,i to the first t where E=2 or correct_i=1, but correct_i is a trace-level constant (Section 3.3.2), which can force T_recover,i=1 on all correct traces, undermining ERF validity (technical soundness).\n- Internal inconsistencies and numerical clarity issues\n  - Conflicting statements about Search-R1’s synthesis vs overall RQI and evidence-conditioned grounding (Figure 2 Right shows synthesis strength; Appendix 10.1/75 describes “misalignment” yet cites low values such as 0.10 at E=1 and 0.14 at E=2), and ambiguity about “overcautious abstention” when the trace formalism requires a final answer at t=T (Section 1, Equation (1); Section 4.4; Table 2) (clarity).\n  - Notational issues: Figure 3 Right uses “P(remaining E ≤ 2)” while the event of interest is remaining in low evidence (E<2) (Section 4.3; Figure 3 Right). Additionally, the symbol C_{i,t} denotes clarity in Definition 3.1 but also denotes reasoning type in Section 3.3.1/Definition 3.3, overloading notation and risking confusion in RQI decompositions (Sections 3.3; Equations 2, 5–6) (clarity/technical soundness).\n  - Mixed claims about which model leads in synthesis (e.g., Section 4.2 highlights ASearcher 0.56; Figure 2 Right/Appendix 10.2 show Search-R1 highest at ≈0.63), and inconsistent agent-synthesis gains (+1.27 overall in Appendix 12/82 vs “+1.06” in the caption of Table 5/82). Additional documentation inconsistencies include Appendix 8 header referencing “CompetenceBench” (Appendix 8/62) and a fourth reasoning type “CritiqueAndCorrection” in Appendix 8.1 (63) not introduced in the main schema (Sections 3.1; Table 1; Figure 1), and Figure 1’s left panel labeling “Recovery” rather than “Repeat” (Figure 1; Table 1) (clarity).\n  - Some figures show CIs but statistical testing details (e.g., bootstrap, significance) are absent; CE lacks uncertainty estimates (Figures 2–4; Table 2) (experimental rigor).\n- Novelty positioning and comparative baselines are underdeveloped\n  - The “first benchmark” claim is made (Abstract; Introduction) without head-to-head process-level comparisons against prior trace-level or reasoning-quality frameworks cited (Related Work, Section 2); No direct evidence found in the manuscript demonstrating superiority over specific existing process analyses (novelty).\n  - No ablation against alternative metric formulations (e.g., different evidence-state encodings or grounding criteria) to substantiate chosen metrics (Section 3.3) (technical soundness).\n  - Limited evaluation of how SeekBench’s metrics correlate with external evaluations (e.g., LLM-as-judge correctness) beyond Figure 4’s correlation of evidence with accuracy (Section 4.4), leaving construct validation incomplete (experimental rigor).\n  - Absence of user/task-level outcomes (e.g., utility or safety), which would support the practical impact of epistemic competence (impact).\n- Dataset scale and representativeness limitations\n  - The core expert-annotated set is only N=190 traces (>1,800 steps; Figure 1), which is small relative to the diversity of seven datasets (Section 4.1), risking sampling bias (experimental rigor).\n  - The distribution of trace sources, task types, and agent allocation across the 190 traces is not detailed (No direct evidence found in the manuscript), limiting interpretability (clarity).\n  - Data sanitization criteria might skew difficulty (Appendix 6/59), but the effect size on dataset composition is not quantified (experimental rigor).\n  - Inter-annotator agreement is aggregated; per-dataset/per-agent agreement and failure analysis are not reported (Appendix 7/60–61), limiting granularity (clarity).\n- Reproducibility and release details are insufficient\n  - The paper does not clearly state dataset/code/model release plans (No direct evidence found in the manuscript), hindering verification (reproducibility).\n  - Proprietary LLMs (GPT-4.1/4.1-mini/5) used for judging and validation (Section 3.1; Appendix 8) may be version-sensitive; prompts are provided, but judge configurations and hyperparameters are not fully specified (Appendix 8) (reproducibility).\n  - ERF survival analysis methodology is described conceptually (Section 4.3) but lacks implementation details (e.g., censoring rules, confidence interval computation) (experimental rigor).\n  - CE computation lacks reporting of uncertainty and sensitivity analyses (Section 4.4; Table 2), making it hard to assess robustness (technical soundness).Suggestions for Improvement\n- Strengthen human-grounded validation and mitigate judge dependence\n  - Expand human annotation beyond N=190 to cover more models/datasets, and report per-dataset/per-agent agreement (extend Appendix 7; Figure 5) to improve generalization.\n  - Quantify and analyze disagreement patterns between human and LLM judges (Appendix 7), including error taxonomy, to identify systematic biases.\n  - Reduce circularity in data sanitization by using independent human criteria rather than GPT-4.1-mini (Appendix 6/59); report how many items were removed and their characteristics.\n  - Avoid reliance on GPT-5 for validation (Section 3.1); if used, provide alternative, publicly reproducible validation with open models and release full judge prompts and settings (Appendix 8).\n- Refine metric definitions to better capture process and avoid built-in bias\n  - Calibrate CE against task difficulty by relaxing the “iff E=2” policy (Definition 3.5), e.g., introduce graded targets or weight sufficiency vs clarity per dataset; provide sensitivity analyses.\n  - Adjust RQI grounding rules so plan-only steps are evaluated with plan-quality criteria (e.g., relevance/consistency) instead of auto-labeling as Not Grounded (Appendix 8.1); report effects on RQI_type.\n  - Revisit evidence state E = C + Q (Definition 3.1) with alternative encodings (e.g., relevance-weighted, non-linear aggregation); include ablations comparing formulations.\n  - Correct the recovery event definition and ERF computation: replace the trace-level correct_i in Equation (7) with a turn-based event (e.g., the first turn with E=2 or an explicit answer event at t), clarify censoring, and re-plot ERF with confidence intervals (Sections 3.3.2; 4.3).\n- Resolve internal inconsistencies and enhance statistical reporting\n  - Reconcile claims about Search-R1’s synthesis and evidence-conditioned groundedness (Figure 2 Right vs Appendix 10.1/75–78) by standardizing metrics/splits and clarify how “abstention” is encoded given τ_T = ⟨r_T, a_T⟩ (Section 1; Section 4.4; Table 2).\n  - Correct notation in Figure 3 Right to reflect the intended event (remaining in E<2) and disambiguate overloaded symbols (e.g., C_{i,t} used for clarity in Definition 3.1 vs reasoning type in Section 3.3.1/Definition 3.3).\n  - Provide consistent model-wise synthesis scores (Section 4.2 vs Figure 2 Right) and agent-synthesis gains (Appendix 12/82); also fix documentation inconsistencies (Appendix 8 title “CompetenceBench”), reconcile the appearance of “CritiqueAndCorrection” in Appendix 8.1 with the three-type schema (Table 1; Section 3.1), and align Figure 1’s left-panel “Recovery” vs Table 1 “Repeat.”\n  - Add statistical tests and uncertainty reporting (e.g., bootstrap CIs) for RQI, ERF, CE (Figures 2–4; Table 2) and document methods for CI computation.\n- Deepen novelty positioning with comparative baselines and validation\n  - Conduct head-to-head comparisons with existing process-level analyses (cited in Section 2), mapping their metrics to SeekBench and reporting where SeekBench adds unique diagnostic power.\n  - Provide metric ablations (Section 3.3), demonstrating why chosen definitions outperform alternatives on construct validity and predictive utility.\n  - Expand construct validation beyond Figure 4 by correlating SeekBench metrics with independent outcome measures (e.g., human-rated trustworthiness) where feasible.\n  - Include small user studies or application-specific evaluations to demonstrate practical utility (e.g., reduced hallucination rates), strengthening impact claims.\n- Improve dataset scale, transparency, and representativeness\n  - Report detailed statistics of the 190-trace set: per-dataset distribution, agent mix, step-type proportions, and sampling criteria (extend Figure 1; Appendix 6–7).\n  - Increase the size of expert-annotated traces to better cover multi-hop tasks and challenging domains; report how scaling affects κ and metric stability.\n  - Quantify the impact of sanitization (Appendix 6): how many items were removed by each criterion, and how this changed evidence-state distributions.\n  - Provide per-dataset/per-agent agreement and failure analyses (Appendix 7), highlighting where the schema is most/least reliable.\n- Enhance reproducibility and methodological transparency\n  - State clear plans for releasing SeekBench data, code, judge prompts, and annotation tools (No direct evidence found in the manuscript), including licenses and versions.\n  - Document judge configurations (model names/versions, temperature, max tokens) and decision thresholds (Appendix 8), enabling exact replication.\n  - Provide ERF implementation details (Section 4.3), including censoring rules, variance estimation, and CI computation; publish scripts for survival analysis.\n  - Report CE uncertainty and sensitivity analyses (Section 4.4; Table 2), and release code to compute CE given annotated traces.Score\n- Overall (10): 6 — Valuable process-level formalization and insights (Definitions 3.1–3.5; Figures 2–4; Table 2) but notable definition/consistency issues (Equation 7; Section 3.3.2; Figure 3 Right; Appendix 8/62; Appendix 10.1/75) temper confidence.\n- Novelty (10): 7 — Introduces an evidence-state framework and three metrics (Table 1; Section 3.3), plus agent synthesis (Section 4.5; Appendix 12), though “first” claim lacks direct comparative validation (Related Work, Section 2).\n- Technical Quality (10): 6 — Solid metric scaffolding and survival-analysis framing (Sections 3.3.1–3.3.3; 4.3; Figure 3), but the recovery event definition (Eq. 7) and notation overload (Sections 3.3.1–3.3.2) warrant correction and added robustness checks.\n- Clarity (10): 6 — Generally well-structured with prompts and figures (Figure 1; Appendix 8), yet multiple notation/result inconsistencies (Figure 3 Right; Definitions 3.1/3.3; Appendix 8/62; Table 5/82; Appendix 10.1/75) and limited statistical reporting (Table 2) reduce readability.\n- Confidence (5): 4 — Assessment based on the full manuscript with extensive figures/tables (Sections 3–4; Appendix 6–12); however, several specification ambiguities (Equation 7; Appendix 8 details) and missing release plans limit verifiability.",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 7,
        "novelty": 7,
        "technical_quality": 7,
        "clarity": 6,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 6,
        "novelty": 7,
        "technical_quality": 6,
        "clarity": 6,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "Summary\n- The paper introduces SeekBench, a process-level benchmark to evaluate the epistemic competence of LLM search agents—specifically evidence-grounded reasoning, adaptive recovery, and calibrated answering—using expert-annotated, multi-turn traces. It formalizes an evidence state and three metrics: Reasoning Quality Index (RQI, Section 3.3.1), Evidence Recovery Function (ERF, Section 3.3.2), and Calibration Error (CE, Section 3.3.3). SeekBench includes 190 expert-annotated traces with >1,800 steps (Figure 1) and is scaled via LLM-as-judge to 28,493 traces and 283,950 steps across seven QA datasets (Section 4.1). Reported findings include: RL training improves calibration (Table 2) and recovery (Figure 3) but degrades evidence-grounded reasoning (Figure 2), and different agents specialize (e.g., Search-R1 excels in synthesis; Section 4.5 and Appendix 12).Strengths\n- Bold formalization of epistemic competence\n  - Clear definition of evidence state combining clarity and sufficiency (Definition 3.1, Equation 2), which makes the latent notion of “epistemic adequacy” operational; this contributes interpretability and technical soundness.\n  - RQI, ERF, and CE are rigorously specified with trace-level decompositions and model-level aggregation (Definitions 3.2–3.5; Equations 3–6, 8–10), supporting reproducible measurement (novelty/technical soundness).\n  - Table 1 links competencies to metrics and annotated features, enabling systematic mapping from annotations to constructs (impact/clarity).\n- Robust annotation schema with validation\n  - The schema distinguishes functional roles (InformationSynthesis/PlanFormation/StateAssessment) and quality attributes (grounding, clarity, sufficiency) (Section 3.1; Figure 1), aiding granular analysis (clarity/impact).\n  - Iterative, kappa-based refinement from 12 to 8 features (Section 3.1) demonstrates methodological rigor (technical soundness).\n  - Strong human agreement (κ = 0.811) and substantial human–LLM alignment (GPT-4.1-mini κ = 0.731; GPT-5 κ = 0.754) are reported (Section 3.1; Appendix 7; Figure 5), supporting reliability of scaling (experimental rigor).\n- Large-scale, multi-dataset evaluation\n  - Seven QA datasets spanning single- and multi-hop tasks (NQ, TriviaQA, PopQA, HotpotQA, 2Wiki, MusiQue, Bamboogle) are used (Section 4.1), improving external validity (experimental rigor/impact).\n  - 28,493 traces and 283,950 steps enable pattern discovery beyond small studies (Section 4.1), strengthening statistical reliability (experimental rigor).\n  - Accuracy baselines (Table 3, Appendix 9/70) contextualize process-level metrics (clarity/impact).\n- Recovery analysis grounded in survival methods\n  - ERF quantifies the time-to-recovery across models (Definition 3.4; Figure 3 Left), adding a principled temporal lens (technical soundness).\n  - Use of Kaplan–Meier for right-censored trajectories (Section 4.3; Figure 3 Right) shows appropriate handling of incomplete sequences (experimental rigor).\n  - Empirical identification that REFINE and FOLLOW-UP recover fastest while REPEAT helps least (Figure 3 Right), actionable for algorithm design (impact).\n- Calibration findings that connect evidence state to answer behavior\n  - P(answer|E) and P(correct|answer, E) increase with evidence quality (Figure 4), validating construct validity of “evidence state” (Section 3.3 validity claim; Section 4.4) (technical soundness).\n  - RL agents reduce overconfident answering and achieve the lowest CE (Table 2), revealing training-specific competencies (impact).\n  - Failure modes are explicitly categorized (overconfident vs overcautious; Section 4.4), improving diagnostic clarity (clarity).\n- Process-level insights that accuracy-only metrics miss\n  - Few-shot achieves highest RQI despite not being best in F1 (Figure 2 Left; Appendix 9/70), exposing reasoning–accuracy tradeoffs (impact).\n  - Case study of correct answer via ungrounded reasoning (Appendix 10.1/76) illustrates why process evaluation is necessary (clarity/impact).\n  - Decomposition by reasoning type pinpoints synthesis vs plan/state weaknesses (Figure 2 Right; Appendix 10.2/78), guiding targeted training (impact).\n- Agent specialization and synthesis experiments\n  - Search-R1 shows strong InformationSynthesis (Figure 2 Right; Appendix 10.2/78) and conservative answering (Appendix 11/79–81), enabling complementary system design (impact).\n  - Agent synthesis improves F1 (Table 5, Appendix 12/82–86), demonstrating practical utility of competency-aware modularization (novelty/impact).\n  - Observation that Base evidence can boost other agents (Appendix 12/86) reveals hidden strengths masked by accuracy-only evaluation (impact).\n- Clear, reproducible prompts for LLM-as-judge\n  - Detailed judging prompts for reasoning grounding, search behavior, and evidence quality (Appendix 8.1–8.3; 63–68) aid transparency and reusability (clarity/technical soundness).\n  - Strict grounding rules (Appendix 8.1) enhance methodological discipline, e.g., superlatives require explicit spans (technical soundness).\n  - Search-type taxonomy (Initial/Refine/FollowUp/Repeat; Appendix 8.2) and quality criteria (Sufficient/Clear; Appendix 8.3) are well-defined (clarity).Weaknesses\n- Reliance on LLM-as-judge with limited human coverage\n  - Large-scale annotations rely on GPT-4.1-mini (Section 4.1), with only 190 human-annotated traces (Figure 1; Section 3.1), which may limit generalization of judgments across domains (experimental rigor).\n  - Human–LLM alignment is substantial but not perfect (κ ≤ 0.754; Appendix 7; Figure 5), leaving room for systematic bias in labels (technical soundness).\n  - Data sanitization uses GPT-4.1-mini decisions (Appendix 6/59), introducing potential circularity between the judge and selection criteria (experimental rigor).\n  - Use of proprietary, possibly non-public GPT-5 for adversarial schema validation (Section 3.1; References [OpenAI, 2025]) may hinder reproducibility (reproducibility).\n- Metric design choices may entrench biases and ignore key definitions\n  - CE’s “ideal policy” answers iff E=2 (Definition 3.5) assumes strict sufficiency/clarity, which may not be appropriate across all QA tasks (technical soundness).\n  - RQI penalizes plan-only text as “Not Grounded” (Appendix 8.1 grounding rules), potentially biasing PlanFormation scores downward by design (novelty/interpretability).\n  - Evidence state E = C + Q (Definition 3.1) conflates clarity and sufficiency linearly without weighting or relevance, possibly oversimplifying epistemic adequacy (technical soundness).\n  - Recovery event definition mixes trace- and turn-level variables: Equation (7) sets T_recover,i to the first t where E=2 or correct_i=1, but correct_i is a trace-level constant (Section 3.3.2), which can force T_recover,i=1 on all correct traces, undermining ERF validity (technical soundness).\n- Internal inconsistencies and numerical clarity issues\n  - Conflicting statements about Search-R1’s synthesis vs overall RQI and evidence-conditioned grounding (Figure 2 Right shows synthesis strength; Appendix 10.1/75 describes “misalignment” yet cites low values such as 0.10 at E=1 and 0.14 at E=2), and ambiguity about “overcautious abstention” when the trace formalism requires a final answer at t=T (Section 1, Equation (1); Section 4.4; Table 2) (clarity).\n  - Notational issues: Figure 3 Right uses “P(remaining E ≤ 2)” while the event of interest is remaining in low evidence (E<2) (Section 4.3; Figure 3 Right). Additionally, the symbol C_{i,t} denotes clarity in Definition 3.1 but also denotes reasoning type in Section 3.3.1/Definition 3.3, overloading notation and risking confusion in RQI decompositions (Sections 3.3; Equations 2, 5–6) (clarity/technical soundness).\n  - Mixed claims about which model leads in synthesis (e.g., Section 4.2 highlights ASearcher 0.56; Figure 2 Right/Appendix 10.2 show Search-R1 highest at ≈0.63), and inconsistent agent-synthesis gains (+1.27 overall in Appendix 12/82 vs “+1.06” in the caption of Table 5/82). Additional documentation inconsistencies include Appendix 8 header referencing “CompetenceBench” (Appendix 8/62) and a fourth reasoning type “CritiqueAndCorrection” in Appendix 8.1 (63) not introduced in the main schema (Sections 3.1; Table 1; Figure 1), and Figure 1’s left panel labeling “Recovery” rather than “Repeat” (Figure 1; Table 1) (clarity).\n  - Some figures show CIs but statistical testing details (e.g., bootstrap, significance) are absent; CE lacks uncertainty estimates (Figures 2–4; Table 2) (experimental rigor).\n- Novelty positioning and comparative baselines are underdeveloped\n  - The “first benchmark” claim is made (Abstract; Introduction) without head-to-head process-level comparisons against prior trace-level or reasoning-quality frameworks cited (Related Work, Section 2); No direct evidence found in the manuscript demonstrating superiority over specific existing process analyses (novelty).\n  - No ablation against alternative metric formulations (e.g., different evidence-state encodings or grounding criteria) to substantiate chosen metrics (Section 3.3) (technical soundness).\n  - Limited evaluation of how SeekBench’s metrics correlate with external evaluations (e.g., LLM-as-judge correctness) beyond Figure 4’s correlation of evidence with accuracy (Section 4.4), leaving construct validation incomplete (experimental rigor).\n  - Absence of user/task-level outcomes (e.g., utility or safety), which would support the practical impact of epistemic competence (impact).\n- Dataset scale and representativeness limitations\n  - The core expert-annotated set is only N=190 traces (>1,800 steps; Figure 1), which is small relative to the diversity of seven datasets (Section 4.1), risking sampling bias (experimental rigor).\n  - The distribution of trace sources, task types, and agent allocation across the 190 traces is not detailed (No direct evidence found in the manuscript), limiting interpretability (clarity).\n  - Data sanitization criteria might skew difficulty (Appendix 6/59), but the effect size on dataset composition is not quantified (experimental rigor).\n  - Inter-annotator agreement is aggregated; per-dataset/per-agent agreement and failure analysis are not reported (Appendix 7/60–61), limiting granularity (clarity).\n- Reproducibility and release details are insufficient\n  - The paper does not clearly state dataset/code/model release plans (No direct evidence found in the manuscript), hindering verification (reproducibility).\n  - Proprietary LLMs (GPT-4.1/4.1-mini/5) used for judging and validation (Section 3.1; Appendix 8) may be version-sensitive; prompts are provided, but judge configurations and hyperparameters are not fully specified (Appendix 8) (reproducibility).\n  - ERF survival analysis methodology is described conceptually (Section 4.3) but lacks implementation details (e.g., censoring rules, confidence interval computation) (experimental rigor).\n  - CE computation lacks reporting of uncertainty and sensitivity analyses (Section 4.4; Table 2), making it hard to assess robustness (technical soundness).Suggestions for Improvement\n- Strengthen human-grounded validation and mitigate judge dependence\n  - Expand human annotation beyond N=190 to cover more models/datasets, and report per-dataset/per-agent agreement (extend Appendix 7; Figure 5) to improve generalization.\n  - Quantify and analyze disagreement patterns between human and LLM judges (Appendix 7), including error taxonomy, to identify systematic biases.\n  - Reduce circularity in data sanitization by using independent human criteria rather than GPT-4.1-mini (Appendix 6/59); report how many items were removed and their characteristics.\n  - Avoid reliance on GPT-5 for validation (Section 3.1); if used, provide alternative, publicly reproducible validation with open models and release full judge prompts and settings (Appendix 8).\n- Refine metric definitions to better capture process and avoid built-in bias\n  - Calibrate CE against task difficulty by relaxing the “iff E=2” policy (Definition 3.5), e.g., introduce graded targets or weight sufficiency vs clarity per dataset; provide sensitivity analyses.\n  - Adjust RQI grounding rules so plan-only steps are evaluated with plan-quality criteria (e.g., relevance/consistency) instead of auto-labeling as Not Grounded (Appendix 8.1); report effects on RQI_type.\n  - Revisit evidence state E = C + Q (Definition 3.1) with alternative encodings (e.g., relevance-weighted, non-linear aggregation); include ablations comparing formulations.\n  - Correct the recovery event definition and ERF computation: replace the trace-level correct_i in Equation (7) with a turn-based event (e.g., the first turn with E=2 or an explicit answer event at t), clarify censoring, and re-plot ERF with confidence intervals (Sections 3.3.2; 4.3).\n- Resolve internal inconsistencies and enhance statistical reporting\n  - Reconcile claims about Search-R1’s synthesis and evidence-conditioned groundedness (Figure 2 Right vs Appendix 10.1/75–78) by standardizing metrics/splits and clarify how “abstention” is encoded given τ_T = ⟨r_T, a_T⟩ (Section 1; Section 4.4; Table 2).\n  - Correct notation in Figure 3 Right to reflect the intended event (remaining in E<2) and disambiguate overloaded symbols (e.g., C_{i,t} used for clarity in Definition 3.1 vs reasoning type in Section 3.3.1/Definition 3.3).\n  - Provide consistent model-wise synthesis scores (Section 4.2 vs Figure 2 Right) and agent-synthesis gains (Appendix 12/82); also fix documentation inconsistencies (Appendix 8 title “CompetenceBench”), reconcile the appearance of “CritiqueAndCorrection” in Appendix 8.1 with the three-type schema (Table 1; Section 3.1), and align Figure 1’s left-panel “Recovery” vs Table 1 “Repeat.”\n  - Add statistical tests and uncertainty reporting (e.g., bootstrap CIs) for RQI, ERF, CE (Figures 2–4; Table 2) and document methods for CI computation.\n- Deepen novelty positioning with comparative baselines and validation\n  - Conduct head-to-head comparisons with existing process-level analyses (cited in Section 2), mapping their metrics to SeekBench and reporting where SeekBench adds unique diagnostic power.\n  - Provide metric ablations (Section 3.3), demonstrating why chosen definitions outperform alternatives on construct validity and predictive utility.\n  - Expand construct validation beyond Figure 4 by correlating SeekBench metrics with independent outcome measures (e.g., human-rated trustworthiness) where feasible.\n  - Include small user studies or application-specific evaluations to demonstrate practical utility (e.g., reduced hallucination rates), strengthening impact claims.\n- Improve dataset scale, transparency, and representativeness\n  - Report detailed statistics of the 190-trace set: per-dataset distribution, agent mix, step-type proportions, and sampling criteria (extend Figure 1; Appendix 6–7).\n  - Increase the size of expert-annotated traces to better cover multi-hop tasks and challenging domains; report how scaling affects κ and metric stability.\n  - Quantify the impact of sanitization (Appendix 6): how many items were removed by each criterion, and how this changed evidence-state distributions.\n  - Provide per-dataset/per-agent agreement and failure analyses (Appendix 7), highlighting where the schema is most/least reliable.\n- Enhance reproducibility and methodological transparency\n  - State clear plans for releasing SeekBench data, code, judge prompts, and annotation tools (No direct evidence found in the manuscript), including licenses and versions.\n  - Document judge configurations (model names/versions, temperature, max tokens) and decision thresholds (Appendix 8), enabling exact replication.\n  - Provide ERF implementation details (Section 4.3), including censoring rules, variance estimation, and CI computation; publish scripts for survival analysis.\n  - Report CE uncertainty and sensitivity analyses (Section 4.4; Table 2), and release code to compute CE given annotated traces.Score\n- Overall (10): 6 — Valuable process-level formalization and insights (Definitions 3.1–3.5; Figures 2–4; Table 2) but notable definition/consistency issues (Equation 7; Section 3.3.2; Figure 3 Right; Appendix 8/62; Appendix 10.1/75) temper confidence.\n- Novelty (10): 7 — Introduces an evidence-state framework and three metrics (Table 1; Section 3.3), plus agent synthesis (Section 4.5; Appendix 12), though “first” claim lacks direct comparative validation (Related Work, Section 2).\n- Technical Quality (10): 6 — Solid metric scaffolding and survival-analysis framing (Sections 3.3.1–3.3.3; 4.3; Figure 3), but the recovery event definition (Eq. 7) and notation overload (Sections 3.3.1–3.3.2) warrant correction and added robustness checks.\n- Clarity (10): 6 — Generally well-structured with prompts and figures (Figure 1; Appendix 8), yet multiple notation/result inconsistencies (Figure 3 Right; Definitions 3.1/3.3; Appendix 8/62; Table 5/82; Appendix 10.1/75) and limited statistical reporting (Table 2) reduce readability.\n- Confidence (5): 4 — Assessment based on the full manuscript with extensive figures/tables (Sections 3–4; Appendix 6–12); however, several specification ambiguities (Equation 7; Appendix 8 details) and missing release plans limit verifiability."
}