# Global Summary
- Problem: Existing evaluations of LLM search agents emphasize final-answer accuracy and largely ignore how agents ground, recover, and calibrate their decisions with external evidence. The paper introduces SeekBench, a process-level benchmark to measure epistemic competence in information-seeking agents.
- Approach: Expert-annotated multi-turn traces (reasoning, search, evidence, answer) with a validated schema; formalization of an “evidence state” and three competencies—groundedness, recovery, and calibration—operationalized via metrics Reasoning Quality Index (RQI), Evidence Recovery Function (ERF), and Calibration Error (CE).
- Data and scope: SeekBench includes N=190 expert-annotated traces with 1,800+ steps. Large-scale evaluation spans 28,493 traces and 283,950 steps across seven QA datasets (NQ, TriviaQA, PopQA, HotpotQA, 2Wiki, MusiQue, Bamboogle).
- Key findings:
  - Groundedness: Few-shot prompting attains the highest overall RQI (“RQI_model = 0.27”), outperforming RL-trained agents (Base: 0.23; DeepResearcher: 0.16; ReSearch: 0.19; Search-R1: 0.08; ASearcher: 0.17).
  - Type-level strengths: Information Synthesis is strongest (e.g., Search-R1: 0.63; ASearcher: 0.56), while Plan Formation is weak across agents (consistently <0.2). Few-shot shows notable State Assessment improvement (0.28).
  - Recovery: ERF shows ASearcher recovers fastest to good evidence (E=2); Kaplan–Meier survival curves indicate REFINE and FOLLOW-UP actions most effectively escape low evidence, while REPEAT yields minimal improvement.
  - Calibration: RL training reduces overconfident answering and improves CE. Aggregated: Overconfident rate drops from 0.631 (Base) to 0.353 (RL-trained); CE reduces to 0.309 (Few-shot: 0.317; Base: 0.329). Per-agent CE: ASearcher 0.302 (lowest), DeepResearcher 0.309, ReSearch 0.305, Search-R1 0.319.
  - Evidence-condition results: RL agents achieve “31.6%” accuracy when answering with good evidence (E=2), vs “8.4%” without supporting evidence.
  - Answer F1: ASearcher (39.77) > Search-R1 (39.29) > ReSearch (38.30) > Few-shot (36.04) ≈ DeepResearcher (36.00) > Base (33.5).
  - Agent synthesis: Using one agent’s collected evidence and another’s synthesis improves F1. Search-R1 is the strongest synthesizer (+1.27 average ΔF1), with +3.50 when synthesizing over Base evidence; Base evidence leads to the highest average gains (+2.42) when other agents synthesize answers.
- Explicitly stated caveats: Evaluation depends on LLM-as-judge with validated alignment (human κ = 0.811; GPT-4.1: 0.693; GPT-4.1-mini: 0.731; GPT-5: 0.754). Test sets sanitized for ambiguous questions and data contamination. Some detailed configurations and training budgets are not specified in the manuscript.

# Abstract
- Problem: RL-trained LLM search agents are mostly evaluated by answer accuracy, overlooking step-level epistemic behaviors (grounding, recovery, calibration).
- Contribution: SeekBench—first benchmark to evaluate epistemic competence via step-level analysis of response traces.
- Dataset: 190 expert-annotated traces, 1,800+ response steps, with granular evidence annotations.
- Competencies assessed:
  1) Grounding of reasoning in observed evidence.
  2) Adaptive search reformulation to recover from low-quality results.
  3) Calibration to assess if evidence suffices for answering.
- Findings: Critical behavioral gaps among state-of-the-art agents missed by accuracy-only metrics; e.g., Search-R1’s synthesis capability. Accuracy-only misses distinct epistemic competencies important for reliable agent development.

# Introduction
- Motivation: Autonomous LLM search agents alternate between reasoning, search, evidence integration, and answering in multi-turn traces; typical exact-match/F1 evaluations miss epistemic competence—reliable acquisition, evaluation, and action upon knowledge.
- Trace formalism: T = ⟨τ₁, …, τ_T⟩; non-final turns τ_t = ⟨r_t, s_t, e_t⟩, final τ_T = ⟨r_T, a_T⟩.
- SeekBench overview:
  - 190 human-annotated traces; 1,800+ steps.
  - Step categories: Search, Reasoning, Evidence; process-level annotations to measure groundedness, recovery, and calibration.
  - Evidence distributions (Figure 1): Sufficient 46.0% vs Insufficient 54.0%; Clarity: Clear 40.8% vs Unclear 59.2%.
  - Reasoning-type distribution: StateAssessment 48.2%, PlanFormation 44.2%, InformationSynthesis 7.7%; Grounded 27.3% vs Not Grounded 72.7%.
  - Step proportions: Search 47% vs Reasoning 53%.
  - Search behavior distribution: Initial 36.1%, Repeat 24.4%, FollowUp 19.8%, Refined 19.7%.
- Contributions:
  1) Process-level benchmark with structured schema; high human agreement (Cohen’s κ > 0.8) and LLM judge alignment (κ > 0.7).
  2) Operational framework: evidence state and three competencies with metrics (RQI, ERF, CE).
  3) Large-scale evaluation across seven QA benchmarks (28,493 traces): RL agents excel at evidence gathering but struggle with reasoning; accuracy metrics fail to reveal agent-specific strengths (e.g., Search-R1’s synthesis vs Base’s reasoning), enabling complementary combinations.

# Related Work
- Reasoning quality and epistemic competence: Prior methods assess structural alignment (gold reasoning chains, causal analyses, graph-based correctness) but do not evaluate grounding in retrieved evidence or adaptive strategies under uncertainty.
- Search agent evaluation: Predominantly answer-level metrics (exact match, F1, LLM-as-Judge). Intermediate-step analyses (e.g., pass@k comparisons, ground-truth tracking, separating retrieval from synthesis) stop short of assessing grounding, recovery, or calibration.
- Process-level frameworks: Existing trace aggregation and taxonomies lack formal definitions and quantitative metrics for epistemic competencies; this work formalizes groundedness, recovery, and calibration with precise metrics and evaluation protocols.

# Method
- Phase 1 (3.1): Observable features and schema construction.
  - Functional types for reasoning: InformationSynthesis, PlanFormation, StateAssessment; search types: Initial, Repeat, FollowUp, Refined.
  - Quality attributes: grounding, evidence clarity/sufficiency.
  - Iterative refinement by three expert annotators over 190 traces; pruning/merging low-agreement features (from 12 to 8).
  - Validation with adversarial edge cases via GPT-5; final schema in Figure 1.
  - Agreement: overall human κ = 0.811; LLM judge alignment—GPT-4.1 (κ = 0.693), GPT-4.1-mini (κ = 0.731), GPT-5 (κ = 0.754).
- Phase 2 (3.2): Latent constructs and competency definitions.
  - Derived competencies: groundedness (alignment of reasoning with evidence), recovery (adaptive search after failures), calibration (answer timing aligned to evidence quality).
- Phase 3 (3.3): Metrics and operationalization.
  - Evidence state (Definition 3.1): E ∈ {0,1,2} via clarity C ∈ {0,1} and quality Q ∈ {0,1}; E = C + Q (0=poor, 1=partial, 2=good).
  - Groundedness (3.3.1): Binary grounding labels G_{i,t}; Model-level RQI (Def. 3.2) and Type-level RQI (Def. 3.3) with evidence-state decompositions.
  - Recovery (3.3.2): Recovery event T_recover,i is first turn with E=2 or correct answer (Eq. 7); ERF (Def. 3.4) as cumulative recovered proportion by turn t (Eq. 8).
  - Calibration (3.3.3): Answering propensity conditioned on evidence P(answer|E=k) (Eq. 9); Calibration Error (CE, Def. 3.5) measures deviation from ideal policy π*(k)=I[k=2]; CE=0 indicates perfect calibration.

# Experiments
- Setup (4.1):
  - Models: Qwen-2.5-7B-Instruct Base; Few-shot; RL-trained agents—Search-R1, ReSearch, ASearcher, DeepResearcher.
  - Datasets: NQ, TriviaQA, PopQA (single-hop); HotpotQA, 2Wiki, MusiQue, Bamboogle (multi-hop).
  - Sanitation: Removed ambiguous questions and data contamination (Appendix 6).
  - Scale: 28,493 traces; 283,950 steps; large-scale annotation via GPT-4.1-mini aligned to human schema.
  - Answer-level performance (Appendix 9, Table 3): ASearcher 39.77; Search-R1 39.29; ReSearch 38.30; Few-shot 36.04; DeepResearcher 36.00; Base 33.5.
- Evidence grounding (4.2):
  - RQI_model (Figure 2 Left): Few-shot 0.27 (highest); Base 0.23; DeepResearcher 0.16; ReSearch 0.19; Search-R1 0.08; ASearcher 0.17.
  - RQI_type (Figure 2 Right):
    - InformationSynthesis: Base 0.46; Few-shot 0.51; DeepResearcher 0.49; ReSearch 0.51; Search-R1 0.63; ASearcher 0.56.
    - PlanFormation: values consistently below 0.2 (e.g., Base ~0.10; Few-shot ~0.11; DeepResearcher ~0.12; ReSearch ~0.06; Search-R1 ~0.15; ASearcher ~0.12).
    - StateAssessment: Base 0.12; Few-shot 0.28; DeepResearcher 0.17; ReSearch 0.23; Search-R1 0.07; ASearcher 0.24.
  - Conclusion: RL training optimizes final answers but does not develop evidence-grounded reasoning; synthesis is a relative strength, planning and state assessment are weaknesses.
- Recovery (4.3):
  - ERF (Figure 3 Left): ASearcher exhibits superior recovery (higher ERF across turns) vs others; DeepResearcher poorest recovery among RL agents.
  - Recovery efficiency by action type (Figure 3 Right): REFINE and FOLLOW-UP most effective; REPEAT minimal improvement; grounded reasoning also aids evidence utilization. Survival analysis via Kaplan–Meier handles right-censoring.
- Calibration (4.4):
  - Evidence drives accuracy (Figure 4): RL-trained models attain “31.6%” P(correct|answer, E=2) vs “8.4%” when answering without supporting evidence.
  - Answering propensity P(answer|E): RL-trained models answer less often than base across all E, indicating greater selectivity.
  - Failures analyzed: (1) overconfident answers without E=2; (2) overcautious abstention despite E=2.
  - Aggregated calibration (Table 2): Overconfident—Base 0.631; Few-shot 0.511; RL-trained 0.353. Overcautious—Base 0.030; Few-shot 0.024; RL-trained 0.085. CE—Base 0.329; Few-shot 0.317; RL-trained 0.309.
  - Per-agent calibration (Appendix Table 4): ASearcher 0.343 (overconfident), 0.044 (overcautious), CE 0.302; DeepResearcher 0.461, 0.048, CE 0.309; ReSearch 0.406, 0.047, CE 0.305; Search-R1 0.226 (lowest overconfident) but 0.187 (highest overcautious), CE 0.319.
- Exploiting competencies (4.5):
  - Agent synthesis: Provide one agent’s evidence/trace to another for answer generation.
  - Gains: Search-R1 synthesizer +1.27 average ΔF1 (Appendix 12, Table 5), with +3.50 using Base evidence; Base evidence yields highest average gains (+2.42). Other synthesizer averages: ASearcher +0.33; DeepResearcher +0.65; ReSearch +0.99.

# Conclusion
- SeekBench delivers a process-level framework (evidence state + RQI, ERF, CE) for epistemic competence in LLM search agents.
- Main claims:
  - RL training improves accuracy and calibration (lower CE) but not evidence-grounded reasoning (lower RQI).
  - Agent-specific strengths: Search-R1 excels at information synthesis; Base demonstrates stronger reasoning capabilities than accuracy suggests; ASearcher excels at recovery/evidence acquisition and achieves highest F1.
- Future directions: Modular architectures that combine complementary competencies; training that jointly improves higher-order reasoning and answer calibration.

# References
- Cited works include datasets (NQ, TriviaQA, PopQA, HotpotQA, 2Wiki, MusiQue, Bamboogle), RL agents (Search-R1, ReSearch, ASearcher, DeepResearcher), methodological references (Content Analysis; Construct Validity; Kaplan–Meier survival analysis), and evaluations of chain-of-thought and reasoning quality.
- Notable agreement and system references: Cohen’s Kappa (1960), Krippendorff (2018), Cronbach & Meehl (1955), OpenAI o1 System Card (2024), GPT-4.1/5 announcements (2025).
- Specific references to surveys and frameworks on agentic RL and deep search agents.

# Appendix
- Contents outline: detailed sections on methodology, experiments, data sanitization, agreement analysis, LLM-as-judge prompts, accuracy, grounding analysis, calibration, agent synthesis, and metric trade-offs.
- Data sanitization (Section 6): Removed ambiguous/unanswerable questions and contamination (e.g., Pass@3 successes without search).
- Inter-annotator agreement (Section 7; Figure 5):
  - Per-field agreements: Functional Type κ > 0.8; Quality κ > 0.75.
  - Human-human κ = 0.811; LLM alignment—GPT-4.1: 0.693±0.043; GPT-4.1-mini: 0.731±0.057; GPT-5: 0.754±0.032.
- LLM-as-judge schema (Section 8): Detailed prompts for reasoning type + grounding, search behavior classification (Initial/Refined/FollowUp/Repeat), and search result quality (Sufficient/Insufficient; Clear/Unclear).
- Accuracy-level performance (Section 9; Table 3): Overall F1—ASearcher 39.77; Search-R1 39.29; ReSearch 38.30; Few-shot 36.04; DeepResearcher 36.00; Base 33.5.
- Evidence-grounded reasoning analysis (Section 10; Figures 6–7):
  - Evidence-conditioned groundedness: Examples—Base (E=1: 0.49; E=2: 0.64), ASearcher (E=1: 0.50; E=2: 0.55), ReSearch (~0.47–0.51 plateau), Search-R1 (E=1: 0.10; E=2: 0.14).
  - Type-level groundedness per evidence state (E=0/1/2) reported across IS/PF/SA (numerical values given in Figure 7 tables).
- Calibration analysis (Section 11; Figures 8–9; Table 4):
  - RL agents’ P(answer|E) increases with E but remains conservative; e.g., Search-R1 answers in ~8.5% of E=2 states; ASearcher/DeepResearcher ~19–21%.
  - Conditional accuracies P(correct|answer, E) rise with evidence; RL training reduces overconfident answering; ASearcher lowest CE (0.302).
  - Timing analysis shows minimal separation between “already high evidence” vs “not yet high,” indicating persistent overconfidence in answer timing.
- Agent synthesis (Section 12; Table 5): ΔF1 gains when synthesizing over other agents’ evidence; Search-R1 strongest synthesizer (+1.27 average), with notable +3.50 over Base evidence.
- Discussion (Section 13): Trade-offs highlighted—accuracy vs reasoning quality; calibration vs groundedness; implications for agent selection and modular designs.
- Missing details: Training budgets, hyperparameters, and dataset licenses are not specified in this section.