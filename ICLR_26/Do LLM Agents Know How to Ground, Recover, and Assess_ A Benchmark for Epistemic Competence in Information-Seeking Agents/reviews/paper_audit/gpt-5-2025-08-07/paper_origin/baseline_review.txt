Summary
- The paper introduces SeekBench, a process-level benchmark to evaluate the epistemic competence of LLM search agents—specifically evidence-grounded reasoning, adaptive recovery, and calibrated answering—using expert-annotated, multi-turn traces. It formalizes an evidence state and three metrics: Reasoning Quality Index (RQI, Section 3.3.1), Evidence Recovery Function (ERF, Section 3.3.2), and Calibration Error (CE, Section 3.3.3). SeekBench includes 190 expert-annotated traces with >1,800 steps (Figure 1) and is scaled via LLM-as-judge to 28,493 traces and 283,950 steps across seven QA datasets (Section 4.1). Key findings include: RL training improves calibration (Table 2) and recovery (Figure 3) but degrades evidence-grounded reasoning (Figure 2), and different agents specialize (e.g., Search-R1 excels in synthesis; Section 4.5 and Appendix 12).Strengths
- Bold formalization of epistemic competence
  - Clear definition of evidence state combining clarity and sufficiency (Definition 3.1, Equation 2), which makes the latent notion of “epistemic adequacy” operational; this contributes interpretability and technical soundness.
  - RQI, ERF, and CE are rigorously specified with trace-level decompositions and model-level aggregation (Definitions 3.2–3.5; Equations 3–6, 8–10), supporting reproducible measurement (novelty/technical soundness).
  - Table 1 links competencies to metrics and annotated features, enabling systematic mapping from annotations to constructs (impact/clarity).- Robust annotation schema with validation
  - The schema distinguishes functional roles (InformationSynthesis/PlanFormation/StateAssessment) and quality attributes (grounding, clarity, sufficiency) (Section 3.1; Figure 1), aiding granular analysis (clarity/impact).
  - Iterative, kappa-based refinement from 12 to 8 features (Section 3.1) demonstrates methodological rigor (technical soundness).
  - Strong human agreement (κ = 0.811) and substantial human–LLM alignment (GPT-4.1-mini κ = 0.731; GPT-5 κ = 0.754) are reported (Section 3.1; Appendix 7; Figure 5), supporting reliability of scaling (experimental rigor).- Large-scale, multi-dataset evaluation
  - Seven QA datasets spanning single- and multi-hop tasks (NQ, TriviaQA, PopQA, HotpotQA, 2Wiki, MusiQue, Bamboogle) are used (Section 4.1), improving external validity (experimental rigor/impact).
  - 28,493 traces and 283,950 steps enable pattern discovery beyond small studies (Section 4.1), strengthening statistical reliability (experimental rigor).
  - Accuracy baselines (Table 3, Appendix 9/70) contextualize process-level metrics (clarity/impact).- Recovery analysis grounded in survival methods
  - ERF quantifies the time-to-recovery across models (Definition 3.4; Figure 3 Left), adding a principled temporal lens (technical soundness).
  - Use of Kaplan–Meier for right-censored trajectories (Section 4.3; Figure 3 Right) shows appropriate handling of incomplete sequences (experimental rigor).
  - Empirical identification that REFINE and FOLLOW-UP recover fastest while REPEAT helps least (Figure 3 Right), actionable for algorithm design (impact).- Calibration findings that connect evidence state to answer behavior
  - P(answer|E) and P(correct|answer, E) increase with evidence quality (Figure 4), validating construct validity of “evidence state” (Section 3.3 validity claim; Section 4.4) (technical soundness).
  - RL agents reduce overconfident answering and achieve the lowest CE (Table 2), revealing training-specific competencies (impact).
  - Failure modes are explicitly categorized (overconfident vs overcautious; Section 4.4), improving diagnostic clarity (clarity).- Process-level insights that accuracy-only metrics miss
  - Few-shot achieves highest RQI despite not being best in F1 (Figure 2 Left; Appendix 9/70), exposing reasoning–accuracy tradeoffs (impact).
  - Case study of correct answer via ungrounded reasoning (Appendix 10.1/76) illustrates why process evaluation is necessary (clarity/impact).
  - Decomposition by reasoning type pinpoints synthesis vs plan/state weaknesses (Figure 2 Right; Appendix 10.2/78), guiding targeted training (impact).- Agent specialization and synthesis experiments
  - Search-R1 shows strong InformationSynthesis (Figure 2 Right; Appendix 10.2/78) and conservative answering (Appendix 11/79), enabling complementary system design (impact).
  - Agent synthesis improves F1 (Table 5, Appendix 12/82–86), demonstrating practical utility of competency-aware modularization (novelty/impact).
  - Observation that Base evidence can boost other agents (Appendix 12/86) reveals hidden strengths masked by accuracy-only evaluation (impact).- Clear, reproducible prompts for LLM-as-judge
  - Detailed judging prompts for reasoning grounding, search behavior, and evidence quality (Appendix 8.1–8.3; Blocks 63–68) aid transparency and reusability (clarity/technical soundness).
  - Strict grounding rules (Appendix 8.1) enhance methodological discipline, e.g., superlatives require explicit spans (technical soundness).
  - Search-type taxonomy (Initial/Refine/FollowUp/Repeat; Appendix 8.2) and quality criteria (Sufficient/Clear; Appendix 8.3) are well-defined (clarity).Weaknesses
- Reliance on LLM-as-judge with limited human coverage
  - Large-scale annotations rely on GPT-4.1-mini (Section 4.1), with only 190 human-annotated traces (Figure 1; Section 3.1), which may limit generalization of judgments across domains (experimental rigor).
  - Human–LLM alignment is substantial but not perfect (κ ≤ 0.754; Appendix 7; Figure 5), leaving room for systematic bias in labels (technical soundness).
  - Data sanitization uses GPT-4.1-mini decisions (Appendix 6/59), introducing potential circularity between the judge and selection criteria (experimental rigor).
  - Use of proprietary, possibly non-public GPT-5 for adversarial schema validation (Section 3.1; References [OpenAI, 2025]) may hinder reproducibility (reproducibility).- Metric design choices may entrench biases and ignore sequential decision points
  - CE’s “ideal policy” answers iff E=2 (Definition 3.5) assumes strict sufficiency/clarity, which may not be appropriate across all QA tasks (technical soundness).
  - RQI penalizes plan-only text as “Not Grounded” (Appendix 8.1 grounding rules), potentially biasing PlanFormation scores downward by design (novelty/interpretability).
  - Evidence state E = C + Q (Definition 3.1) conflates clarity and sufficiency linearly without weighting or relevance, possibly oversimplifying epistemic adequacy (technical soundness).
  - CE aggregates over P(E) and P(answer|E) without explicit turn-level decision conditioning (Equation 10), potentially masking temporal calibration properties (experimental rigor).- Internal inconsistencies and numerical clarity issues
  - Conflicting statements about Search-R1’s synthesis vs overall RQI and evidence-conditioned grounding (Figure 2 Right shows synthesis strength; Appendix 75 claims misalignment yet cites low values such as 0.10 at E=1 and 0.14 at E=2), which is hard to reconcile (clarity).
  - The notation “P(remaining E ≤ 2)” in Figure 3 Right conflicts with narrative that recovery is reaching E=2; the intended event is likely E<2 (clarity).
  - Mixed claims about which model leads in synthesis (e.g., Section 4.2 lists ASEARCHER 0.56; Figure 2 Right/Appendix 10.2 show Search-R1 highest at ≈0.63), creating ambiguity (clarity).
  - Some figures show CIs but statistical testing details (e.g., bootstrap, significance) are absent; CE lacks uncertainty estimates (Figures 2–4; Table 2) (experimental rigor).- Novelty positioning and comparative baselines are underdeveloped
  - The “first benchmark” claim is made (Abstract; Introduction) without head-to-head process-level comparisons against prior trace-level or reasoning-quality frameworks cited (Related Work, Section 2); No direct evidence found in the manuscript demonstrating superiority over specific existing process analyses (novelty).
  - No ablation against alternative metric formulations (e.g., different evidence-state encodings or grounding criteria) to substantiate chosen metrics (Section 3.3) (technical soundness).
  - Limited evaluation of how SeekBench’s metrics correlate with external evaluations (e.g., LLM-as-judge correctness) beyond Figure 4’s correlation of evidence with accuracy (Section 4.4), leaving construct validation incomplete (experimental rigor).
  - Absence of user/task-level outcomes (e.g., utility or safety), which would support the practical impact of epistemic competence (impact).- Dataset scale and representativeness limitations
  - The core expert-annotated set is only N=190 traces (>1,800 steps; Figure 1), which is small relative to the diversity of seven datasets (Section 4.1), risking sampling bias (experimental rigor).
  - The distribution of trace sources, task types, and agent allocation across the 190 traces is not detailed (No direct evidence found in the manuscript), limiting interpretability (clarity).
  - Data sanitization criteria might skew difficulty (Appendix 6/59), but the effect size on dataset composition is not quantified (experimental rigor).
  - Inter-annotator agreement is aggregated; per-dataset/per-agent agreement and failure analysis are not reported (Appendix 7/60–61), limiting granularity (clarity).- Reproducibility and release details are insufficient
  - The paper does not clearly state dataset/code/model release plans (No direct evidence found in the manuscript), hindering verification (reproducibility).
  - Proprietary LLMs (GPT-4.1/4.1-mini/5) used for judging and validation (Section 3.1; Appendix 8) may be version-sensitive; prompts are provided, but judge configurations and hyperparameters are not fully specified (Appendix 8) (reproducibility).
  - ERF survival analysis methodology is described conceptually (Section 4.3) but lacks implementation details (e.g., censoring rules, confidence interval computation) (experimental rigor).
  - CE computation lacks reporting of uncertainty and sensitivity analyses (Section 4.4; Table 2), making it hard to assess robustness (technical soundness).Suggestions for Improvement
- Strengthen human-grounded validation and mitigate judge dependence
  - Expand human annotation beyond N=190 to cover more models/datasets, and report per-dataset/per-agent agreement (extend Appendix 7; Figure 5) to improve generalization.
  - Quantify and analyze disagreement patterns between human and LLM judges (Appendix 7), including error taxonomy, to identify systematic biases.
  - Reduce circularity in data sanitization by using independent human criteria rather than GPT-4.1-mini (Appendix 6/59); report how many items were removed and their characteristics.
  - Avoid reliance on GPT-5 for validation (Section 3.1); if used, provide alternative, publicly reproducible validation with open models and release full judge prompts and settings (Appendix 8).- Refine metric definitions to better capture process and avoid built-in bias
  - Calibrate CE against task difficulty by relaxing the “iff E=2” policy (Definition 3.5), e.g., introduce graded targets or weight sufficiency vs clarity per dataset; provide sensitivity analyses.
  - Adjust RQI grounding rules so plan-only steps are evaluated with plan-quality criteria (e.g., relevance/consistency) instead of auto-labeling as Not Grounded (Appendix 8.1); report effects on RQI_type.
  - Revisit evidence state E = C + Q (Definition 3.1) with alternative encodings (e.g., relevance-weighted, non-linear aggregation); include ablations comparing formulations.
  - Introduce turn-conditional CE (Equation 10) or temporal calibration curves to reflect sequential decision-making; report uncertainty for CE and ERF.- Resolve internal inconsistencies and enhance statistical reporting
  - Reconcile claims about Search-R1’s synthesis and evidence-conditioned groundedness (Figure 2 Right vs Appendix 75/78) by standardizing metrics and clearly annotating which figures use which datasets/splits.
  - Correct notation in Figure 3 Right to reflect the intended event (remaining in E<2), and align the narrative with plotted quantities.
  - Provide consistent model-wise synthesis scores (Section 4.2 vs Figure 2 Right) and explicitly label whether values are RQI_type or conditional groundedness; add table summarizing numeric values.
  - Add statistical tests and uncertainty reporting (e.g., bootstrap CIs) for RQI, ERF, CE (Figures 2–4; Table 2) and document methods for CI computation.- Deepen novelty positioning with comparative baselines and validation
  - Conduct head-to-head comparisons with existing process-level analyses (cited in Section 2), mapping their metrics to SeekBench and reporting where SeekBench adds unique diagnostic power.
  - Provide metric ablations (Section 3.3), demonstrating why chosen definitions outperform alternatives on construct validity and predictive utility.
  - Expand construct validation beyond Figure 4 by correlating SeekBench metrics with independent outcome measures (e.g., human-rated trustworthiness) where feasible.
  - Include small user studies or application-specific evaluations to demonstrate practical utility (e.g., reduced hallucination rates), strengthening impact claims.- Improve dataset scale, transparency, and representativeness
  - Report detailed statistics of the 190-trace set: per-dataset distribution, agent mix, step-type proportions, and sampling criteria (extend Figure 1; Appendix 6–7).
  - Increase the size of expert-annotated traces to better cover multi-hop tasks and challenging domains; report how scaling affects κ and metric stability.
  - Quantify the impact of sanitization (Appendix 6): how many items were removed by each criterion, and how this changed evidence-state distributions.
  - Provide per-dataset/per-agent agreement and failure analyses (Appendix 7), highlighting where the schema is most/least reliable.- Enhance reproducibility and methodological transparency
  - State clear plans for releasing SeekBench data, code, judge prompts, and annotation tools (No direct evidence found in the manuscript), including licenses and versions.
  - Document judge configurations (model names/versions, temperature, max tokens) and decision thresholds (Appendix 8), enabling exact replication.
  - Provide ERF implementation details (Section 4.3), including censoring rules, variance estimation, and CI computation; publish scripts for survival analysis.
  - Report CE uncertainty and sensitivity analyses (Section 4.4; Table 2), and release code to compute CE given annotated traces.Score
- Overall (10): 7 — Clear formalization and insightful process-level findings (Definitions 3.1–3.5; Figures 2–4; Table 2), tempered by LLM-judge dependence and some inconsistencies (Section 4.1; Appendix 7; Appendix 75/78).
- Novelty (10): 7 — Introduces an evidence-state framework and three metrics (Table 1; Section 3.3), plus agent synthesis (Section 4.5; Appendix 12), though “first” claim lacks direct comparative validation (Related Work, Section 2).
- Technical Quality (10): 7 — Solid metric formalization and survival analysis (Sections 3.3.1–3.3.3; 4.3; Figure 3), but CE/RQI design choices and reliance on LLM judges warrant further ablation and robustness checks (Section 4.1; Appendix 8).
- Clarity (10): 6 — Generally well-structured with prompts and figures (Figure 1; Appendix 8), yet some numerical and narrative inconsistencies remain (Figure 2 vs Appendix 75/78; Figure 3 Right notation), and limited statistical reporting (Table 2).
- Confidence (5): 4 — Assessment based on the full manuscript with extensive figures/tables (Sections 3–4; Appendix 6–12), but some reproducibility details and data release plans are missing (No direct evidence found in the manuscript).