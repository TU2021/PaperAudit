1) Summary
The paper introduces SeekBench, a benchmark for evaluating the epistemic competence of Large Language Model (LLM) search agents. The authors argue that current evaluations, which focus on final answer accuracy, fail to assess the underlying reasoning processes. SeekBench consists of 190 expert-annotated traces and an annotation schema for analyzing agent behaviors at a step-by-step level. The framework proposes three core epistemic competencies—Groundedness, Recovery, and Calibration—and quantifies them using novel metrics: the Reasoning Quality Index (RQI), Evidence Recovery Function (ERF), and Calibration Error (CE). Experiments on several state-of-the-art RL-trained agents reveal that while RL improves final accuracy and calibration, it can degrade evidence-grounded reasoning. The benchmark also uncovers agent-specific strengths, such as synthesis capabilities, that are missed by traditional metrics.2) Strengths
*   **Important and Well-Motivated Problem**
    *   The paper compellingly argues that final-answer accuracy is an insufficient metric for evaluating autonomous agents, as it can hide critical flaws in the reasoning process (Section 1, Paragraph 4). This is a timely and significant problem as LLM agents become more autonomous.
    *   The related work section effectively positions the contribution by highlighting how existing process-level analyses lack formal definitions of epistemic competencies and quantitative metrics (Section 2, Paragraph 3).
    *   The introduction of "epistemic competence" provides a strong conceptual foundation for moving beyond simple correctness to assess whether an agent's behavior is justified and reliable (Section 1, Paragraph 2).*   **Rigorous and Systematic Methodology**
    *   The three-phase methodology (Schema Construction, Competency Definition, Metric Operationalization) is systematic and well-justified (Section 3). It grounds the work in established principles from qualitative research (Content Analysis, Section 3.1) and psychometrics (construct validity, Section 3.3).
    *   The annotation schema was developed through a rigorous iterative process involving multiple expert annotators, resulting in high inter-annotator agreement (Cohen’s Kappa κ > 0.8), which lends credibility to the dataset (Section 3.1, Paragraph 4; Appendix 7).
    *   The validation of LLM judges against human experts (κ > 0.7) provides a strong justification for using them for large-scale annotation, making the proposed evaluation framework scalable (Section 3.1, Paragraph 6; Figure 5).*   **Novel and Insightful Metrics**
    *   The paper introduces three novel and interpretable metrics (RQI, ERF, CE) that operationalize the core epistemic competencies (Table 1; Section 3.3). These metrics provide a much richer, multi-faceted view of agent performance than a single accuracy score.
    *   The Reasoning Quality Index (RQI) effectively captures the critical dimension of groundedness, and its decomposition by reasoning type and evidence state provides fine-grained insights into where reasoning fails (Section 3.3.1; Figure 2; Appendix 10).
    *   The Evidence Recovery Function (ERF) provides a clear, quantitative way to measure an agent's ability to adapt its search strategy and escape low-evidence states, which is a crucial capability for information-seeking agents (Section 3.3.2; Figure 3).
    *   The Calibration Error (CE) metric formalizes the trade-off between overconfidence and overcautiousness, providing a principled way to evaluate an agent's decision-making under uncertainty (Section 3.3.3; Table 2).*   **Comprehensive Experiments with Non-Obvious Findings**
    *   The evaluation is extensive, covering multiple models (including a base model, few-shot, and four SOTA RL-trained agents) across seven diverse QA benchmarks (Section 4.1).
    *   The key finding that RL training improves accuracy and calibration but degrades evidence-grounded reasoning is a significant and counter-intuitive result that challenges common assumptions about the benefits of RL for agent development (Section 4.2, Figure 2; Section 4.4, Table 2).
    *   The agent synthesis experiment (Section 4.5; Appendix 12) is a creative and effective demonstration of the benchmark's utility, revealing hidden strengths (e.g., the Base model's evidence collection, Search-R1's synthesis) that are completely obscured by accuracy-only metrics (Table 5).3) Weaknesses
*   **Limited Scale and Reliance on LLM Judges for Core Analysis**
    *   The human-annotated dataset, which forms the basis for validating the entire framework, is relatively small (190 traces, 1,800 steps) (Section 1, Figure 1). While the quality appears high, the small size may limit the diversity of behaviors captured.
    *   The main experimental results (Sections 4.2-4.5) are derived from a large-scale analysis (28,493 traces) annotated by an LLM judge (GPT-4.1-mini) (Section 4.1). Although the LLM judge was validated, this introduces a potential single point of failure. Systematic biases or errors from the LLM judge could affect the conclusions, and the reported human-LLM agreement (κ = 0.731) is strong but not perfect.
    *   The paper does not include an analysis of the types of disagreements between the LLM judge and human annotators, which would be helpful for understanding the potential limitations of the large-scale results. No direct evidence found in the manuscript.*   **Ambiguity in the "Evidence State" Definition**
    *   The evidence state `E_i,t` is defined as the sum of two binary variables: clarity `C_i,t` and quality (sufficiency) `Q_i,t` (Definition 3.1, Equation 2). This formulation conflates two different scenarios into the single state `E_i,t = 1`: (clear but insufficient) and (unclear but sufficient).
    *   The scenario of "unclear but sufficient" evidence seems counter-intuitive but is permitted by the definition. Treating these distinct situations as equivalent could mask important nuances in agent behavior.
    *   Analyses that rely heavily on this definition, such as the evidence-conditioned RQI (Equation 4, Figure 6) and the calibration analysis (Section 4.4, Figure 4), might be affected by this ambiguity.*   **Limited Generalizability of Experimental Findings**
    *   All evaluated models are based on the same underlying LLM, Qwen-2.5-7B-Instruct (Section 4.1). This makes it difficult to determine whether the core findings, particularly the negative impact of RL on reasoning quality, are specific to this model family or are a more general phenomenon.
    *   The paper evaluates a specific set of RL-trained agents. The conclusions drawn about "RL training" as a general technique might not hold for other RL algorithms, reward functions, or training methodologies not included in the study.
    *   The study is confined to open-domain QA tasks. While this is a major application area, the generalizability of the epistemic competencies and findings to other agent tasks (e.g., planning, tool use in different domains) is not explored. No direct evidence found in the manuscript.4) Suggestions for Improvement
*   **Acknowledge and Characterize Limitations of LLM-as-Judge**
    *   To address the reliance on a small human-annotated set, the authors should explicitly discuss the potential limitations of this scale in the main paper.
    *   To strengthen the large-scale results, the authors should add a discussion on the potential for systematic bias from the LLM judge. It would be beneficial to include a brief qualitative or quantitative error analysis on a small, random sample of the LLM-judged data, highlighting specific cases where the LLM judge agrees or disagrees with human annotators.
    *   This analysis could be added to Appendix 7 to provide readers with a clearer understanding of the LLM judge's failure modes and build more confidence in the main experimental findings.*   **Refine or Justify the "Evidence State" Definition**
    *   The authors should consider refining the evidence state to a two-dimensional vector `(C_i,t, Q_i,t)` instead of a scalar sum. This would allow for a more fine-grained analysis that distinguishes between the different types of partial evidence.
    *   Alternatively, if the authors maintain the current definition, they should provide a justification for why summing clarity and sufficiency is appropriate and perhaps show empirically that the "unclear but sufficient" state is rare or can be reasonably grouped with the "clear but insufficient" state.
    *   This would strengthen the analyses in Sections 4.2 and 4.4 by removing the ambiguity and allowing for a more precise interpretation of how agents behave in different partial-information scenarios.*   **Discuss the Scope and Generalizability of the Findings**
    *   The authors should add a paragraph in the discussion or conclusion (e.g., Section 5 or a new discussion section) explicitly acknowledging that the findings are based on a single model family (Qwen). This would appropriately scope the claims.
    *   In the same discussion, the authors should clarify that their conclusions about "RL training" apply to the specific set of methods evaluated and may not generalize to all forms of RL-based agent tuning.
    *   The authors could briefly suggest that future work should validate these findings across a wider range of model architectures (e.g., Llama, Mistral) and agent training paradigms to establish the generality of the observed trade-offs.5) Score
- Overall (10): 8 — The paper introduces a valuable, well-designed benchmark and provides novel, significant insights into agent behavior (Figure 2, Table 5).
- Novelty (10): 9 — The conceptual framework of epistemic competence and its operationalization into concrete metrics (RQI, ERF, CE) is highly novel for this domain (Section 3.3).
- Technical Quality (10): 8 — The methodology is rigorous and well-validated (Section 3.1, Appendix 7), though the evidence state definition is slightly ambiguous (Definition 3.1).
- Clarity (10): 10 — The paper is exceptionally well-written, with clear definitions, illustrative figures, and a logical structure that is easy to follow (Table 1, Figure 1).
- Confidence (5): 5 — I am highly confident in my assessment as the topic is directly within my area of expertise.