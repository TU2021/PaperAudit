### Summary

The paper proposes **SeekBench**, a process-level benchmark / evaluation framework for **LLM search agents** that focuses on *epistemic competence* rather than just final-answer accuracy. From **190 expert-annotated traces (~1,800 steps)** and a larger corpus of **28k+ agent traces** on open-domain QA benchmarks, the authors define an **annotation schema** and three core competencies:

1. **Groundedness** – whether intermediate reasoning is supported by retrieved evidence.
2. **Recovery** – whether the agent can adaptively reformulate queries to escape low-quality evidence.
3. **Calibration** – whether the agent answers or abstains in line with the sufficiency of evidence.

These are formalized into metrics such as **Reasoning Quality Index (RQI)**, **Evidence Recovery Function (ERF)**, and a **Calibration Error (CE)**. Using these metrics, the paper argues that answer-only scores hide important behavioral differences: e.g., **RL-trained agents often improve answer accuracy and calibration but show weaker evidence-grounded reasoning**, while base/few-shot models may collect strong evidence but underperform on final synthesis. SeekBench is presented as a **process-level diagnostic toolkit** for analyzing how information-seeking agents ground, recover, and assess, rather than as a simple leaderboard.

---

### Strengths

* **Timely focus on process-level evaluation for agents.**
  Reviewers agree the paper tackles an important and under-explored problem: **evaluating how search agents use evidence, recover from bad retrieval, and decide when to answer**, rather than only measuring final accuracy. This is seen as highly relevant for building reliable, transparent, and debuggable information-seeking systems.

* **Clear conceptualization of epistemic competencies.**
  The decomposition into **grounded reasoning, recovery, and calibration** is viewed as conceptually meaningful. Several reviewers find the **epistemic framing and “evidence state” notion (poor/partial/good evidence)** reasonable and helpful for structuring analysis of search agents’ behavior.

* **Well-documented framework and schema.**
  Reviewers note that, despite the framework’s complexity, the **schema, pipeline, and metrics are described in a generally clear and systematic way**. The descriptions of annotated features (reasoning types, search types, evidence clarity/sufficiency, answer correctness) and how they map to the three competencies are appreciated.

* **Granular diagnostics and empirical insights.**
  Using the proposed metrics, the benchmark yields **qualitative findings** reviewers find interesting, such as:

  * RL-trained agents **boost answer-level metrics and calibration** but can **degrade evidence-grounded reasoning**.
  * Many agents struggle with **Plan Formation and State Assessment**, performing relatively better at **Information Synthesis**.
  * Different agents exhibit **complementary strengths** (e.g., some are better at gathering evidence, others at calibrated answering), suggesting **agent composition / synthesis** as a promising direction.

* **Scale and breadth of evaluation (within the chosen setting).**
  Within the domain the paper chooses to focus on, reviewers perceive the evaluation as **substantial**: multiple LLM search agents, **seven QA benchmarks** (~28k traces), and a smaller expert-annotated subset used to define the schema and validate LLM-as-judge. This is seen as enough to generate **non-trivial empirical conclusions** about current search agents.

* **High-level methodological contribution rather than just a dataset.**
  Some reviewers emphasize that SeekBench is not only a trace collection but an **operational framework (schema + metrics + LLM-as-judge protocol)** that could, in principle, be re-used for future agents and models. This methodological orientation is considered a strength for long-term impact.

---

### Weaknesses

* **Metric design and justification feel under-motivated to several reviewers.**
  Multiple reviewers question whether the **formal metrics truly capture the intended epistemic constructs** and whether they are sufficiently justified:

  * For **RQI**, one reviewer explicitly questions **why groundedness under “insufficient evidence” (E=0)** is included, arguing that “grounded vs not grounded” may be less meaningful when evidence is clearly inadequate.
  * For **ERF**, there are concerns that the definition **ignores trace length**, making it harder to interpret “how fast” an agent escapes poor evidence across traces of different length.
  * For **CE**, a reviewer challenges the assumption that an “ideal” agent should answer iff external evidence is sufficient, pointing out that in practice an agent may legitimately rely on internal knowledge for easier questions.
    More broadly, several reviewers feel the paper **does not sufficiently contrast these metrics with existing measures (accuracy, F1, verifiability metrics, etc.) or connect them to prior cognitive / epistemic theory**, so the **novelty and necessity** of the specific formalizations feels under-argued.

* **Limited diversity and size of the human-annotated core benchmark.**
  The **190-trace expert-annotated subset** is repeatedly viewed as **small and potentially narrow**:

  * Reviewers worry it may **not cover enough question types and failure patterns** to fully validate the judges and metrics.
  * There is concern that the **annotated traces come primarily from relatively small (7B) agents**, so the schema and LLM-as-judge reliability are **not clearly validated for stronger agents**, whose errors may be subtler and harder to detect.
  * The use of **only three human annotators** is seen by some as borderline for calibrating a new benchmark, raising questions about **agreement robustness** and possible blind spots.

* **Tool and model coverage seen as too narrow for a “benchmark” claim.**
  Several reviewers feel the empirical scope is **too conservative relative to the paper’s framing**:

  * The main analysis focuses on **search-only agents** using a single web-search tool, whereas modern information-seeking agents often combine **multiple tools (web browsing, scraping, file parsing, Python, etc.)**. It is unclear whether the same schema and judge prompts remain reliable when agents use richer toolkits; currently, **judge reliability is only validated on search-only trajectories**.
  * Most core experiments are run on **Qwen-based agents around 7B scale**, and reviewers highlight the absence (in the original submission) of **larger model variants and some state-of-the-art multi-tool agents**. This makes it hard to know **how much of the observed behavior is intrinsic vs. an artifact of model size or particular training setups**.
  * Some reviewers therefore view the work as **not yet a fully general “benchmark” for epistemic competence**, but rather an important **first step grounded in a restricted agent family and tool setting**.

* **Binary / discrete labeling may miss nuance in reasoning and evidence.**
  Reviewers worry that the **binary grounded vs. ungrounded labels** and coarsely discretized evidence states (E = poor/partial/good) may **lose important distinctions**:

  * Cases with **partially supported claims or ambiguous evidence** might be oversimplified by the current labels.
  * One reviewer emphasizes that **real-world epistemic behavior often sits on a spectrum**, and the current design risks **over-binarizing subtle reasoning quality differences**, especially when applied at scale through LLM-as-judge.

* **Clarity and framing issues around what “SeekBench” actually is.**
  At least one reviewer finds the **abstract and introduction somewhat confusing**, especially the repeated emphasis that SeekBench is “a process-level benchmark of 190 traces with 1,800+ steps.” This framing initially suggests a **dataset for evaluating process-level *evaluators***, whereas in practice:

  * The 190 traces underpin the **annotation schema and judge validation**,
  * The **large-scale evaluation of agents** is conducted via an **LLM-as-judge pipeline**, not directly on that 190-trace set.
    Reviewers think the paper should **more sharply distinguish between (i) the human-annotated core used to define/validate the framework and (ii) the large-scale automatic evaluation**, to avoid misinterpretation of what the benchmark actually consists of.

* **Reliance on LLM-as-judge raises open questions.**
  While reviewers acknowledge that LLM-as-judge is a practical way to scale, they still note open concerns:

  * The paper’s main results **depend heavily on the reliability of LLM-as-judge labels**, but some reviewers feel the analysis of **per-metric reliability, cost, and judge selection** is not fully worked out in the original submission.
  * There are concerns that LLM judges might **inherit systematic biases or blind spots** from their training, especially on adversarial or subtle cases, and that these issues are **only partially probed** with the current size/diversity of human validation.

* **Limited exploration of how the metrics feed back into training or agent design.**
  Although the analysis surfaces interesting differences between agents, several reviewers feel the paper **stops short of fully closing the loop**:

  * For RL agents, reviewers explicitly ask **how process-level supervision signals (RQI/ERF/CE) could be directly incorporated into training objectives**, beyond post-hoc diagnostics.
  * The **“agent combination” insight** (composing agents with complementary strengths) is seen as promising, but reviewers note that the paper **does not deeply explore concrete orchestration strategies, conditions under which composition helps, or how to systematically design such combinations**.
