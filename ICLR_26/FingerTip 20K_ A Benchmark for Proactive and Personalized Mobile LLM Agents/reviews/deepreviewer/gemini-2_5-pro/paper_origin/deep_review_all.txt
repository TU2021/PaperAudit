### Reviewer 1

**Summary**

This paper introduces FingerTip 20K, a new benchmark and dataset designed to evaluate mobile LLM agents on proactive and personalized capabilities, which are significant limitations of current systems. The benchmark proposes two novel tracks: proactive task suggestion, where an agent predicts a user's intent from contextual cues, and personalized task execution, where an agent executes a task by mimicking a user's preferred action patterns. To support this, the authors collected a large-scale, longitudinal dataset from 91 real users, capturing their daily mobile interactions along with rich contextual information like user profiles, time, location, and historical usage. Experiments on existing generalist and specialized GUI agents demonstrate that these new tasks are challenging, while a model fine-tuned on the FingerTip data shows marked improvements, highlighting the value of the dataset and the proposed research direction.

**Soundness**

The methodology is generally sound. The motivation for creating a new benchmark is well-justified by the identified gaps in existing work—namely, the lack of proactivity and personalization (Section 1, Block #2). The data collection process is a standout feature, moving beyond sterile lab environments to capture real-world usage patterns from users' own devices over an extended period (Section 4.2, Block #17). This ecological validity is a significant improvement over prior datasets.

The formulation of the two tracks is clear and logical (Section 3, Blocks #10-13). The inputs for proactive suggestion (profile, time, history, etc.) are well-chosen to enable intent prediction. The use of historical actions as a reference for personalized execution is a clever way to frame the personalization problem. The evaluation metrics are thoughtfully designed. For task suggestion, combining semantic similarity (S1) with an LLM-based success check (SR1) is a robust approach (Section 5.1, Block #35). The personalization metric, Sim2, which compares similarity to the target user versus other user types, is novel and directly measures the desired personalization capability. The experimental setup, including the use of physical phones for evaluation, adds to the realism of the findings (Section 5.1, Block #34).

**Presentation**

The paper is very well-written, clear, and easy to follow. The introduction effectively motivates the problem and summarizes the contributions (Section 1, Block #3). The structure is logical, moving from related work to problem formulation, data collection, experiments, and conclusion. Figures and tables are used effectively to illustrate key concepts and results. Figure 1 and Figure 2 provide excellent high-level overviews of the benchmark's goals and task structures. The data collection pipeline in Figure 3 is clear and informative. The various charts in Figure 4 and the results in Tables 4, 5, and 6 are well-labeled and support the paper's claims. The inclusion of detailed prompts and data format descriptions in the appendix is very helpful for reproducibility (Appendix A.4, A.5).

**Contribution**

The contribution of this paper is significant and timely. The primary contribution is the FingerTip benchmark itself, which pushes the field of mobile agents beyond simple reactive task execution. By introducing the concepts of proactivity and personalization, backed by a high-quality, real-world dataset, the authors are setting a new and important research agenda.
1.  **Novel Tracks:** The proactive task suggestion and personalized task execution tracks are genuinely new for mobile GUI agents and address a critical aspect of creating truly helpful assistants (Section 3).
2.  **Unique Dataset:** The dataset is a major contribution. Its scale (~21k episodes), diversity (506 apps), and, most importantly, its inclusion of longitudinal user data with rich context (profiles, history) are unparalleled, as shown in Table 1.
3.  **Challenging Benchmark:** The experimental results, which show low performance from powerful SOTA models like GPT-4.1, prove that the proposed tasks are non-trivial and will spur further innovation (Table 4, Table 5). The success of the fine-tuned model validates the dataset's utility.

**Strengths**

1.  **Problem Formulation:** The paper excellently identifies and formulates the next frontier for mobile agents: moving from passive instruction-followers to proactive, personalized assistants. This is a highly relevant and impactful research direction.
2.  **Ecologically Valid Data Collection:** The collection of data from real users on their own devices over two months is a massive strength (Section 4.2, Block #17). This ensures the intents, actions, and contexts are authentic, which is a major limitation of most existing datasets.
3.  **Comprehensive Benchmark Design:** The paper provides everything needed for a strong benchmark: two well-defined tasks, a large and rich dataset, clear evaluation metrics, and baseline results from a wide range of current models.
4.  **Demonstrated Value:** The fine-tuning experiment (Section 5.4, Table 6) convincingly demonstrates that the collected data contains valuable signals for personalization and proactivity, as the fine-tuned model significantly outperforms much larger generalist models on these specific tasks.

**Weaknesses**

1.  **Limited Scope of Personalization Analysis:** The analysis of personalized actions (Section 4.4, Figure 5) is based on a simple "age group" classification. While this is a good first step, personalization is likely far more nuanced. Exploring other user clusters or a more continuous representation of user preferences could strengthen this analysis.
2.  **Sim2 Metric Interpretation:** The personalization metric `Sim2` is novel, but its interpretation could be tricky. A score around 1.0 indicates no preference, but it's not immediately clear what a "good" score is. The fine-tuned model achieves 1.21, but without a theoretical upper bound or more analysis, it's hard to gauge how much personalization this represents.
3.  **Generalizability of Fine-Tuned Model:** The fine-tuning was performed on a relatively small subset (1000 episodes) and tested on data from the same user distribution. While the results are promising, the paper could benefit from a discussion on the expected generalization to new users not seen during fine-tuning.

**Questions**

1.  In the data collection, users recorded their intent "in one sentence" (Block #17). Was there any quality control or normalization applied to these free-form intents? Could variations in how users describe the same intent affect the training and evaluation of the proactive suggestion task?
2.  The `Sim2` metric is defined as `SI/SII`, where `SI` is the similarity to the target user's golden trajectory and `SII` is the similarity to a trajectory from a different user type (Block #35). Could an agent achieve a high `Sim2` score by simply being very *dissimilar* to everyone, rather than specifically similar to the target user? Have you considered alternative formulations for this metric?
3.  The fine-tuning experiment shows a large gain in `Sim2` (personalization) but a drop in `SR2` (success rate) compared to the specialized UI-TARS model (Table 6). Do you believe there is an inherent trade-off between executing a task in a personalized way versus the most efficient/robust way?

**Rating**

- Overall (10): 9 — The paper introduces a highly novel and valuable benchmark with an excellent real-world dataset, pushing the field towards proactive and personalized agents.
- Novelty (10): 10 — The proactive suggestion and personalized execution tracks, supported by a unique longitudinal dataset, are entirely new for mobile GUI benchmarks (Table 1).
- Technical Quality (10): 9 — The data collection, task formulation, and experimental design are very strong, with only minor questions about the personalization metric's robustness.
- Clarity (10): 10 — The paper is exceptionally well-written and organized, with clear figures and tables that effectively communicate the core ideas and results (e.g., Figure 2, Table 1).
- Confidence (5): 5 — I am very confident in my assessment as I have expertise in HCI, AI agents, and benchmark design.

---
### Reviewer 2

**Summary**

The paper presents FingerTip 20K, a new benchmark for evaluating mobile GUI agents. The authors argue that existing agents are purely reactive and non-personalized. To address this, they propose two new evaluation tracks: proactive task suggestion and personalized task execution. They collected a new dataset of over 21,000 interaction episodes from 91 users in China, which includes user profiles and contextual information. The authors benchmark several existing models, showing they perform poorly on the new tasks. They also show that a model fine-tuned on their data achieves better performance, particularly in personalization.

**Soundness**

The paper's premise is sound, but there are several methodological concerns that weaken the claims.

1.  **Data Collection Bias:** The dataset is collected exclusively from 91 users in mainland China (Block #17, #48). This introduces significant geographical and cultural bias. UI interaction patterns, app ecosystems, and linguistic expressions of intent can vary drastically across regions. The paper acknowledges this in the appendix (Block #48), but such a critical limitation should be front and center in the main paper, and its implications on the generalizability of the benchmark and findings should be discussed more thoroughly.

2.  **Data Cleaning and Augmentation:** The paper states that for actions that were not correctly captured by the accessibility service, `Qwen-VL-Max` was used to "regenerate the action" (Block #17). This is a major concern. It means a non-trivial portion of the ground-truth action data is not human-generated but synthetically produced by an LLM. What percentage of actions were regenerated? How was the accuracy of this regeneration process validated? This synthetic step could introduce systematic errors or biases from the LLM into the dataset, confounding the evaluation of other models.

3.  **Evaluation Metrics:** The metrics have some weaknesses.
    *   For proactive suggestion, using `DeepSeek-V3` to generate a binary success score `SR1` (Block #35) is problematic. This means the evaluation of all models is dependent on the specific behavior and potential biases of another, un-audited LLM. An evaluation should be as objective as possible; relying on an LLM for judging correctness introduces a variable that is hard to control or reproduce.
    *   The personalization metric `Sim2 = SI/SII` (Block #35) is not fully convincing. It measures relative similarity. An agent could potentially get a high score by producing a generic path that is equally dissimilar to both the target user and the "different type" user, as long as the denominator `SII` is sufficiently small. The analysis in Section 4.4, which underpins this metric, is also based on a very coarse grouping of users by "age groups" (Block #19), which may not be the primary axis of personalization.

4.  **Experimental Rigor:** The fine-tuning experiment (Section 5.4) uses only 1,000 episodes. While it shows improvement, this is a very small fraction of the training set (16,000 episodes). It is unclear if these results would hold with more extensive training or how sensitive they are to the specific 1,000 episodes chosen. The claim that the fine-tuned model is "stronger" needs to be tempered by the fact that it was trained specifically for these tasks, whereas the generalist models were not.

**Presentation**

The paper is generally well-structured. However, some critical details are either relegated to the appendix or missing entirely. For instance, the significant limitation of the dataset's geographic scope is only mentioned in the appendix (Block #48). The use of an LLM to repair the dataset (Block #17) is mentioned briefly but without the necessary detail to assess its impact. The figures are mostly clear, but Figure 5 is confusing. It appears to be a table converted into a figure, and the text within it is hard to read and seems to contain raw data points ("78.51 / 76.29") without a clear explanation of what the two numbers represent. The actual plot is in Figure 25, but it is referenced as Figure 5. This is a confusing error. Similarly, the tables in Figure 4 and Figure 6 are just text representations of the data in the plots, which is redundant and poorly formatted.

**Contribution**

The paper's main contribution is the idea of benchmarking proactivity and personalization in mobile agents, and the creation of a new dataset for this purpose. This is a valuable direction. However, the contribution is weakened by the methodological issues mentioned above. While the dataset is large, the questions surrounding its geographic bias and synthetic data repair process limit its immediate value as a universal benchmark. The proposed tasks are novel, but the metrics used to evaluate them are not fully robust. The paper successfully highlights a problem but provides a flawed initial solution.

**Strengths**

1.  **Novel Problem Framing:** The paper correctly identifies the limitations of current mobile agents and proposes the important and novel research directions of proactivity and personalization.
2.  **Real-World Data:** Despite its biases, the dataset is collected from real users' daily activities, which is a step up from purely synthetic or lab-based datasets (Table 1).
3.  **Ambitious Goal:** The effort to build a benchmark for such complex, user-centric behaviors is commendable and sets a high bar for future work.

**Weaknesses**

1.  **Significant Data Bias:** The dataset is geographically and culturally monolithic (mainland China), severely limiting the generalizability of any findings or models trained on it.
2.  **Contaminated Ground Truth:** The use of an LLM to "regenerate" missing actions (Block #17) compromises the integrity of the dataset as a source of human ground-truth data. The extent of this contamination is not specified.
3.  **Questionable Evaluation Metrics:** The reliance on an LLM for success rate calculation (`SR1`) and the potentially gameable `Sim2` metric for personalization reduce the reliability of the experimental results.
4.  **Confusing Presentation of Figures/Tables:** Several figures are poorly formatted or referenced incorrectly (e.g., Figure 5, Figure 4, Figure 6), hindering clarity.

**Questions**

1.  What percentage of the actions in the final dataset were synthetically generated by `Qwen-VL-Max`? What steps were taken to validate the correctness of these generated actions against the actual, unrecorded human actions?
2.  For the `SR1` metric, have you considered using human evaluation or a simpler, more objective string-matching metric (like ROUGE or BLEU, in addition to the similarity scores you already have) instead of relying on `DeepSeek-V3`'s judgment? How do you ensure the `DeepSeek-V3` evaluation is consistent and unbiased?
3.  The personalization analysis in Section 4.4 is based on "age groups." Did you explore other ways of clustering users to find more meaningful axes of personalization? How robust is the `Sim2` metric if users do not fall into such neat categories?
4.  Why was the limitation of the dataset's geographic scope, a critical piece of information for a benchmark paper, placed in the appendix rather than being discussed in the main data collection or limitations section?

**Rating**

- Overall (10): 5 — A good idea for a benchmark, but significant methodological flaws in data collection and evaluation metrics cast doubt on the validity and generalizability of the results.
- Novelty (10): 8 — The core ideas of proactive and personalized agent evaluation are novel, but the execution has been seen in other domains.
- Technical Quality (10): 4 — The use of an LLM to repair ground-truth data, reliance on another LLM for evaluation, and significant unaddressed data bias are major technical weaknesses.
- Clarity (10): 6 — The paper is generally readable, but critical details are omitted or buried, and several figures are poorly presented and referenced (Figure 5).
- Confidence (5): 5 — I am highly confident in my assessment, having reviewed numerous papers on datasets, benchmarks, and agent systems.

---
### Reviewer 3

**Summary**

This paper introduces FingerTip 20K, a benchmark aimed at fostering the development of proactive and personalized mobile LLM agents. The authors identify that current agents are reactive and generic. They propose two new tasks: proactive task suggestion (predicting user intent) and personalized task execution (following user-specific action paths). A new dataset of ~21k real-world Android interaction episodes from 91 users was collected, containing user profiles and contextual data. Experiments show that SOTA models struggle with these tasks, especially in terms of success rate and personalization. A fine-tuned 7B model demonstrates the potential of leveraging the collected data to improve on these new axes of performance.

**Soundness**

The paper's approach is practical and grounded in real-world challenges. The decision to collect data from users' own phones during their daily routines (Block #17) is a crucial step towards building agents that work outside the lab. Using physical phones for evaluation (Block #34) instead of emulators, due to app restrictions, further reflects a commitment to real-world validity, even at the cost of convenience.

The formulation of the two tracks is pragmatic. Track 1 (suggestion) is a direct formalization of a "proactive assistant" feature, and the inputs provided (history, context, etc.) are what one would expect such a system to use (Block #12). Track 2 (personalization) is also well-defined, using a past trajectory as an in-context example for the agent to follow (Block #13), which is a practical way to guide an LLM towards a specific style.

The choice of metrics is reasonable from an engineering perspective. `Sim1` (text similarity) for suggestion and `SR2` (manual check) for execution success are standard and understandable. The `Sim2` metric for personalization is an interesting and creative attempt to quantify a difficult concept. While perhaps not perfect, it provides a tangible number to optimize against, which is valuable for benchmark-driven development. The fine-tuning experiment (Section 5.4) is a key part of the paper's soundness, as it provides a proof-of-concept that the data actually contains a learnable signal for these new tasks.

**Presentation**

The paper is clearly written and well-organized. The problem statement is compelling, and the proposed solution is presented logically. The tables and figures are generally effective. Table 1 provides an excellent, concise comparison to related work, immediately highlighting the novelty of FingerTip. Figure 2 clearly illustrates the inputs and outputs for the two tracks.

However, there are some presentation issues. The figures in the main body that are just text representations of plots (e.g., the tables in Figure 6, Block #38) are unnecessary and look unprofessional. The actual plots (Block #39, #40) are much better and should be the primary focus. The caption for Figure 5 ("Personalized action analysis") refers to a table-like figure, but the actual bar chart is Figure 25 in the appendix, which is confusingly not referenced in the main text. This should be corrected. The appendix is very useful, providing details on prompts (Block #58, #59) and data format (Block #51), which is great for reproducibility.

**Contribution**

The main contribution is a practical and challenging benchmark that moves the field forward. Current benchmarks (as listed in Table 1) are becoming saturated, with models achieving high success rates on reactive execution. FingerTip opens up two new, commercially relevant, and technically challenging research avenues.

The dataset itself is a significant asset. By capturing longitudinal data with user context, it enables a new class of research questions to be explored. The baselines are also a valuable contribution, as they clearly demonstrate that even powerful models like GPT-4.1 are not "proactive" or "personalized" out of the box (Tables 4 & 5), proving that this is a hard problem requiring dedicated effort. The low success rates (e.g., 38.5% for the best model on execution) show that this benchmark has a long shelf-life and will be a useful tool for measuring progress.

**Strengths**

1.  **Real-World Focus:** The entire project, from data collection on users' own phones to evaluation on physical devices, is geared towards real-world applicability, which is often missing in academic benchmarks.
2.  **Challenging and Relevant Tasks:** Proactivity and personalization are key features for the next generation of AI assistants. This benchmark provides a concrete way to measure progress towards them.
3.  **Actionable Insights:** The results clearly show that specialized, fine-tuned models outperform massive generalist models on these nuanced tasks (Table 6). This provides a clear directive for future research: focus on data and fine-tuning, not just scaling up generalist models.
4.  **Openness and Reproducibility:** The authors provide the dataset, code, and detailed prompts, which is excellent practice and will facilitate future work (Block #2, #51, #57).

**Weaknesses**

1.  **Cost and Scalability of Evaluation:** The personalized execution track requires multi-step interaction with a physical phone, and success is determined by manual checking (Block #35). This makes evaluation expensive, slow, and difficult to scale. While realistic, it may deter some researchers from using the benchmark. Have the authors considered more automated ways to check for task success?
2.  **Limited Model Diversity in Fine-Tuning:** Only one small model (Qwen-2.5-VL-7B) was fine-tuned (Section 5.4). While the results are positive, it would be more convincing to see if these gains hold across different model architectures or sizes.
3.  **Chinese-App Centric:** The dataset is heavily focused on Chinese apps (Block #17, #21). While this is a valuable contribution for that ecosystem, it means models developed on this benchmark may not perform well on UIs and apps from other parts of the world. This is a practical limitation for global development efforts.

**Questions**

1.  The evaluation for personalized execution requires a physical phone and manual success checking. Do you have plans to develop a more automated evaluation pipeline, perhaps using state-matching or programmatic checks, to make the benchmark more accessible and scalable?
2.  The fine-tuned model `Qwen-2.5-VL-7B-FT` shows a massive improvement in step ratio (from 2.16 to 1.17) and `Sim2` (from 0.95 to 1.21), but a more modest improvement in success rate `SR2` (from 1.5% to 12.5%) (Table 6). This suggests the model is getting better at *how* to do the task, but still struggles with *whether* it can do it. Do you have any insights into what the primary failure modes are for the fine-tuned model? Is it still GUI grounding, or is it higher-level planning?
3.  Given the dataset's focus on Chinese apps, what challenges do you foresee in adapting a model trained on FingerTip to a different app ecosystem, for example, one dominated by US-based apps? Would it require a complete re-collection of data, or could some form of domain adaptation be effective?

**Rating**

- Overall (10): 8 — A strong, practical benchmark that introduces important new tasks, though the evaluation methodology could be a barrier to adoption.
- Novelty (10): 9 — The combination of proactivity, personalization, and a longitudinal real-world dataset is highly novel in the mobile agent space.
- Technical Quality (10): 8 — The methodology is solid and practical, but the reliance on manual evaluation for Track 2 is a scalability bottleneck.
- Clarity (10): 8 — Very clear for the most part, but with some confusing figure references and redundant textual tables that could be cleaned up.
- Confidence (5): 5 — I am confident in my review, based on my experience building and evaluating agentic systems.

---
### Reviewer 4

**Summary**

The paper introduces FingerTip 20K, a new benchmark designed to push mobile GUI agents beyond reactive instruction following towards proactivity and personalization. The authors contribute a large-scale dataset collected from 91 users over two months, capturing real-world mobile phone interactions enriched with user profiles, contextual data, and historical usage. The benchmark features two novel tracks: (1) proactive task suggestion, which involves predicting a user's intent, and (2) personalized task execution, which requires an agent to complete a task while adhering to a user's preferred interaction style. Experimental results on current models highlight the difficulty of these new tasks, and a fine-tuning experiment demonstrates the value of the collected data for building more user-oriented agents.

**Soundness**

The paper is methodologically sound and thoughtfully constructed. The core argument—that mobile agents need to be proactive and personalized—is compelling and well-supported by a review of the limitations of existing work (Section 2.1, Table 1).

The data collection process is a key strength. By gathering longitudinal data from users' personal devices in naturalistic settings (Section 4.2), the authors have created a resource of high ecological validity, which is a significant step forward for the field. The inclusion of user profiles and historical data is the critical ingredient that enables the novel tasks.

The problem formulations for the two tracks are precise and well-scoped (Section 3). The experimental design is robust. It includes a good selection of both powerful generalist models (GPT-4.1) and specialized GUI agents (UI-TARS), providing a comprehensive baseline (Section 5.1). The metrics are well-considered; the combination of quantitative similarity scores (`Sim1`, `Sim2`) and success rates (`SR1`, `SR2`) provides a multi-faceted view of performance. The analysis of how task difficulty affects performance (Section 5.3) adds depth to the findings. The fine-tuning study (Section 5.4) serves as an effective ablation, proving that the contextual information in the dataset is indeed learnable and useful.

**Presentation**

The paper is presented with a high degree of clarity and professionalism. The narrative flows logically from motivation to solution to evaluation. The introduction does an excellent job of situating the work and stating its contributions (Block #3). The related work section is comprehensive and effectively uses Table 1 to differentiate FingerTip from prior datasets.

Visual aids are used well to convey complex information. Figure 2 provides a simple, clear diagram of the two task inputs and outputs. The data statistics in Figure 4 (the plots, not the text tables) give a good sense of the dataset's characteristics. The results tables (4, 5, 6) are easy to interpret.

A minor point of criticism is the inclusion of poorly formatted text tables within the captions or bodies of Figure 4 and Figure 6, which are redundant given the accompanying plots. Additionally, the confusion around Figure 5 (a table) and the unreferenced plot in the appendix (Figure 25) should be resolved for the final version. Overall, however, the presentation is strong.

**Contribution**

This paper makes a substantial and timely contribution to the field of AI agents.
1.  **Conceptual Shift:** Its primary contribution is shifting the goalposts for mobile agent research from mere task completion to proactive and personalized assistance. This is a crucial step towards making agents genuinely useful in everyday life.
2.  **A High-Quality, Novel Benchmark:** The FingerTip benchmark, with its two new tracks and unique dataset, is a significant contribution. It provides the infrastructure for the community to develop and measure progress on these more advanced capabilities. The fact that SOTA models perform poorly (Table 4, 5) underscores the benchmark's value and challenge.
3.  **A Rich, Real-World Dataset:** The dataset itself is a major asset. Unlike static, instruction-following datasets, this longitudinal, context-rich dataset will enable research into user modeling, intent prediction, and preference learning in the GUI domain.

This work is likely to have a high impact, inspiring a new wave of research focused on building more intelligent and user-aware mobile agents.

**Strengths**

1.  **Visionary Problem Definition:** The paper tackles a forward-looking and critical problem, moving beyond the well-trodden path of reactive agents.
2.  **High Ecological Validity:** The data collection methodology (longitudinal, in-the-wild, on personal devices) is a major strength, ensuring the dataset reflects the complexities of real-world mobile phone use.
3.  **Thorough Evaluation:** The paper provides a comprehensive evaluation, including multiple SOTA models, analysis of task difficulty, and a fine-tuning experiment that validates the core hypothesis.
4.  **Clear and Novel Tasks:** The proactive suggestion and personalized execution tracks are well-defined, innovative, and directly address the paper's stated goals.

**Weaknesses**

1.  **Limited Discussion of Limitations in Main Paper:** The primary weakness is that a significant limitation—the dataset's homogeneity (exclusively Chinese users and apps)—is only discussed in the appendix (Block #48). For a benchmark paper, this information is crucial for potential users to understand its scope and should be discussed in the main body.
2.  **Ambiguity in Data Post-Processing:** The use of `Qwen-VL-Max` to "regenerate" incomplete action data (Block #17) is a potentially confounding variable. The paper would be stronger if it quantified the extent of this regeneration and provided a more detailed analysis of its potential impact on the dataset's integrity.
3.  **Personalization Metric Simplicity:** While the `Sim2` metric is a good start, it relies on a simple ratio of Levenshtein similarities. This may not capture all the nuances of personalized interaction. For example, one user might prefer using keyboard shortcuts while another prefers clicking through menus; `Sim2` might not distinguish these stylistic differences as effectively as a more feature-based metric.

**Questions**

1.  The paper demonstrates that fine-tuning on your data improves personalization (`Sim2` > 1). Do you have any qualitative examples that show *how* the fine-tuned agent's behavior differs from the generic agent? For instance, does it choose a different path through the app that mirrors the user's history?
2.  You acknowledge the geographic limitation of the dataset in the appendix. What do you see as the biggest challenges in extending this data collection methodology to a more global and diverse user base, both technically and ethically?
3.  The proactive suggestion task currently uses up to 20 historical intents. Did you perform any analysis on how the amount of historical context affects prediction accuracy? Is there a point of diminishing returns?

**Rating**

- Overall (10): 9 — An excellent and impactful paper that introduces a novel, challenging, and much-needed benchmark for the next generation of mobile agents.
- Novelty (10): 10 — The focus on proactivity and personalization, backed by a unique longitudinal dataset, is a clear and significant step beyond existing benchmarks.
- Technical Quality (10): 8 — The methodology is strong, but would be improved by a more transparent discussion of data limitations and post-processing in the main paper.
- Clarity (10): 9 — The paper is very clearly written and structured, with only minor issues in figure presentation.
- Confidence (5): 5 — I am very confident in this evaluation. The paper is well within my area of expertise.