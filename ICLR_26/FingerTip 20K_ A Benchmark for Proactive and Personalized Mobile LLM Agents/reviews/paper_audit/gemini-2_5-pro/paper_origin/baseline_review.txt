**Summary**This paper introduces FingerTip 20K, a new benchmark designed to evaluate mobile GUI agents on proactive and personalized capabilities, addressing limitations of current reactive, instruction-following paradigms. The authors propose two novel tracks: (1) proactive task suggestion, where an agent predicts a user's intent from contextual cues, and (2) personalized task execution, where an agent executes a task by mimicking a user's preferred action patterns. To support this benchmark, the authors collected a large-scale dataset of 21,437 interaction episodes from 91 real users over two months. This dataset is distinguished by its inclusion of rich user-centric information, such as profiles, location, time, and historical interactions. Experiments on existing generalist and specialized agents reveal significant challenges in these new tasks. However, a model fine-tuned on the FingerTip dataset demonstrates marked improvements, highlighting the value of user-oriented data for building more intelligent mobile agents.**Strengths***   **Novel and Important Problem Formulation**
    *   The paper effectively identifies and addresses a critical gap in mobile agent research: the shift from purely reactive agents to proactive and personalized assistants (Section 1). This moves the field towards more practical and user-centric applications.
    *   The two proposed tracks, proactive task suggestion (Section 3.1) and personalized task execution (Section 3.2), are well-defined and represent a significant conceptual advance over existing execution-only benchmarks (Table 1).
    *   The motivation to reduce user cognitive load and serve latent needs is compelling and aligns with the long-term vision of intelligent personal assistants (Section 1).*   **High-Quality and Realistic Dataset**
    *   The core contribution is a large-scale dataset collected from 91 users during their real-world, daily mobile phone usage over two months (Section 4.2). This methodology provides a more authentic and challenging setting than benchmarks based on synthetic tasks or annotator-driven data collection (Table 1).
    *   The dataset is uniquely enriched with user-specific contextual information, including user profiles, time, location scenarios, and longitudinal data on past intents and actions (Section 3, Table 1, Appendix A.4). This rich context is essential for the proposed proactive and personalized tasks.
    *   The scale of the dataset is substantial, comprising 21,437 episodes across 506 different applications, ensuring diversity in tasks and UI interactions (Table 1, Figure 4d).*   **Thorough Experimental Evaluation and Analysis**
    *   The paper provides a comprehensive benchmark of both powerful generalist models (e.g., GPT-4.1) and specialized GUI-control agents (e.g., UI-TARS-1.5-7B), offering a clear picture of current model capabilities on the new tasks (Tables 4 and 5).
    *   The authors introduce novel and task-appropriate evaluation metrics, such as `Sim1` for textual similarity in suggestions and `Sim2` for measuring personalization in action sequences, which go beyond simple success rates (Section 5.1).
    *   The analysis of model performance under varying task difficulties (e.g., number of initial screenshots for suggestion, action sequence length for execution) provides valuable insights into model limitations (Section 5.3, Figure 6).
    *   The fine-tuning experiment (Section 5.4, Table 6) serves as a strong proof-of-concept, demonstrating that the collected data can be effectively used to improve both proactivity and personalization, even with a relatively small model.*   **Strong Empirical Motivation for Personalization**
    *   The paper provides a quantitative analysis to justify the need for the personalized task execution track (Section 4.4).
    *   By calculating the Levenshtein similarity of action sequences, the authors show that sequences for similar tasks are significantly more alike for the same user or users of the same type compared to users of different types (Figure 5). This analysis provides a solid, data-driven foundation for the benchmark's design.**Weaknesses***   **Limited Generalizability of the Dataset**
    *   The dataset was collected exclusively from 91 users in mainland China, interacting primarily with Chinese third-party apps (Appendix A.1). This narrow demographic and geographic scope may limit the generalizability of findings and models trained on this data to other regions, languages, and user populations.
    *   The prompts shown in the appendix are in English (Appendix A.5), while the app names (Figure 4d) and example screenshots (Figure 7) are in Chinese. This potential language mismatch during evaluation is not discussed and could disadvantage models not optimized for Chinese-language UIs.
    *   The user profiles include sensitive information like age, occupation, and marital status (Table 7), but the analysis of personalization is limited to a simple grouping by age (Section 4.4), leaving the impact of other profile features unexplored.*   **Potential Subjectivity and Lack of Validation in Key Metrics**
    *   The primary success metric for proactive task suggestion, `SR1`, is determined by an LLM (DeepSeek-V3) judging semantic equivalence (Section 5.1). The reliability and potential biases of this "LLM-as-a-judge" approach are not validated against human judgment, making the reported success rates difficult to interpret with confidence.
    *   The success metric for personalized task execution, `SR2`, is based on manual checking (Section 5.1). Without a detailed, public rubric or protocol for this manual evaluation, the results are difficult to reproduce and may be subject to evaluator bias.
    *   The personalization metric `Sim2` is a ratio of similarities, which is a creative approach. However, its sensitivity is not fully analyzed; for instance, it is unclear how much a `Sim2` value of 1.21 (Table 6) truly reflects a user's preference compared to a value of 1.06.*   **Ambiguity in Experimental Setup for Baselines**
    *   For the personalized execution track, the paper states that generalist models were guided by a "simple prompt" (Section 5.1). It is unclear how these non-specialized models were expected to generate precise, grounded actions (e.g., `click(coordinates=(x,y))`) from screenshots alone, as this is a known hard problem. The resulting poor performance (Table 5) is expected and may not constitute a fair or informative baseline.
    *   For the specialized GUI-control agents, the paper mentions their "original prompts were used" and that the new contextual information was "uniformly integrated" (Section 5.1). This description is vague. The exact method of integrating user profiles and historical actions into these prompts is critical for assessing why these models failed to show personalization (`Sim2` ≈ 1.0 in Table 5).
    *   The fine-tuning experiment used only 1,000 episodes, which is less than 5% of the available training data (Section 5.4). While the results are positive, this small scale raises questions about whether the performance gains are representative of the dataset's full potential.**Suggestions for Improvement***   **Address and Discuss Dataset Generalizability**
    *   The geographic and linguistic limitations should be discussed more prominently in the main paper (e.g., in Section 4 or 5), rather than being confined to the appendix, to give readers a clearer context for the results.
    *   Please clarify the language of the user-provided intents in the dataset and discuss the potential impact of evaluating models on UIs and instructions that may not be in English.
    *   Consider expanding the personalization analysis in Section 4.4 to explore the influence of other user profile attributes beyond just age, which could provide deeper insights into user behavior patterns.*   **Strengthen and Validate Evaluation Metrics**
    *   To increase confidence in the `SR1` metric, please conduct a human evaluation on a subset of the test predictions. Reporting the agreement rate (e.g., Cohen's Kappa) between the LLM judge and human annotators would substantially validate this metric.
    *   For the `SR2` metric, please provide a detailed rubric or protocol used for the manual success checks. Releasing this protocol alongside the benchmark would be invaluable for reproducibility and allow other researchers to evaluate their models consistently.
    *   For the `Sim2` metric, consider providing a qualitative analysis or examples that illustrate what a higher `Sim2` score looks like in practice. This would help readers better understand the practical significance of the reported improvements.*   **Clarify and Refine the Experimental Setup**
    *   Please provide more details on how the generalist models were prompted to produce grounded UI actions. For example, was OCR used to extract UI element coordinates and include them in the prompt? This would make the baseline setup more transparent.
    *   Please provide the exact prompt templates or a more detailed description of how user profile and historical action data were integrated into the prompts for the specialized GUI agents (Aguvis, CogAgent, UI-TARS). This is essential for understanding their failure to personalize.
    *   It would strengthen the paper to either justify the choice of using only 1,000 episodes for fine-tuning or, if feasible, present results showing how performance scales with the amount of fine-tuning data (e.g., training on 1k, 5k, and 10k episodes).**Score***   Overall (10): 8 — The paper introduces a highly novel and valuable benchmark that addresses a critical next step for mobile agents, supported by a unique real-world dataset.
*   Novelty (10): 9 — The proactive and personalized task formulations are new for this domain, and the user-centric, longitudinal data collection method is a significant and novel contribution.
*   Technical Quality (10): 7 — The data collection and initial analysis are strong, but the work is weakened by unvalidated evaluation metrics and some ambiguity in the experimental setup for baselines.
*   Clarity (10): 9 — The paper is very well-written, clearly motivated, and easy to follow, with effective use of figures and tables to present the core ideas and results (e.g., Figure 2, Table 1).
*   Confidence (5): 5 — I am highly confident in my assessment, as the paper provides sufficient detail to understand its contributions and identify its limitations.