# Global Summary
This paper introduces FingerTip 20K, a new benchmark for evaluating mobile GUI agents on proactive and personalized capabilities. The authors argue that current agents are passive, only acting on explicit instructions, and non-personalized, ignoring user context. To address this, FingerTip provides two new tracks: (1) proactive task suggestion, where an agent predicts a user's intent based on their profile, history, and current context, and (2) personalized task execution, where an agent executes a task while mimicking the user's preferred action style. The benchmark is built on a new dataset collected from 91 users over two months, comprising 21,437 episodes of real-world mobile phone usage across 506 apps, enriched with user profiles, location, time, and historical interactions. Experiments show that existing generalist and specialized GUI agents perform poorly on these new tasks, with suggestion success rates below 10% and execution success rates below 40%, while failing to show personalization. However, fine-tuning a 7B model (Qwen-2.5-VL-7B) on a small subset of the data (1,000 episodes) significantly improves performance, increasing proactive suggestion success rate from 3.1% to 9.7% and demonstrating personalized actions (Sim2 score of 1.21 vs. ~1.0 for baselines).

# Introduction
- The paper identifies two key limitations in current LLM-based mobile GUI agents: they are passive (requiring explicit instructions) and non-personalized (ignoring user context like profiles, history, and preferences).
- To address these limitations, the authors introduce the FingerTip benchmark with two new evaluation tracks:
    1.  **Proactive task suggestion:** The agent must infer a user's potential intent from their past behavior and current environment.
    2.  **Personalized task execution:** The agent must execute a given task by following the user's preferred action patterns.
- A new dataset was collected to support these tracks, featuring 21,437 episodes from 91 users over two months, covering 506 apps. The data is unique in that it was collected from users' real-life, long-term usage and includes user-related contextual information.
- Experiments on the benchmark show that existing models struggle with these new tasks. A model fine-tuned on the collected data showed significant improvement, highlighting the benchmark's value for building more user-oriented agents.

# Related Work
- Existing mobile GUI-control datasets (e.g., AndroidLab, AitW, SPHINX) are compared in Table 1. The paper claims these datasets have common drawbacks:
    - Task instructions are predefined or LLM-generated, not reflecting real user intents.
    - Demonstrations are collected by annotators in simulated environments, not from real-world usage.
    - Data instances are isolated and lack user-related contextual information.
- FingerTip is presented as the first benchmark to include user profile info, contextual info (time, place), and to feature proactive task suggestion and personalized task execution tracks.
- Existing mobile GUI agents are categorized into two types:
    1.  Prompt-engineered generalist models (e.g., GPT-4v).
    2.  Fine-tuned smaller models on GUI-specific datasets.
- The paper states that current agents are confined to passively following instructions and do not consider user preferences. While some work explores clarifying ambiguous instructions, they still require an initial instruction.

# Preliminaries
- The paper formulates the two new tasks mathematically.
- **Proactive task suggestion:** The agent's goal is to predict an intent `I` based on user profile `U`, current time `T`, scenario `S`, historical intents `I_history` (up to 20 items), and initial screen observations `O` (0 to 3 screenshots). The formulation is `I = f(U, T, S, I_history, O)`. The difficulty increases as the number of provided screenshots decreases.
- **Personalized task execution:** The agent's goal is to generate the next action `A_{t+1}` to complete a true instruction `I_true`. The agent is given the user profile `U`, the user's historical actions for a similar task `A_history` (for in-context learning), the agent's own previously executed actions `A_agent`, and the current screen state (`O_t`, `AT_t`). The formulation is `A_{t+1}, O_{t+1}, AT_{t+1} = f(U, I_true, A_history, A_agent, O_t, AT_t)`. The goal is for the final action sequence to reflect the user's preferences.

# Method
- **Data Collection:**
    - 91 users with Android phones were recruited via crowdsourcing.
    - Users installed a custom app (FingerTip APP) and used it for 1-2 months to record real intents and demonstrate the corresponding actions on their own phones.
    - The app collected user-inputted intent, time, location category, screenshots, accessibility trees (XML), and UI actions.
    - A maximum of 12 data pieces could be uploaded per user per day.
    - Post-processing was done using Qwen-VL-Max and PaddleOCR to fix non-compliant or incomplete action data.
- **Data Statistics:**
    - The dataset contains 21,437 episodes, 506 apps, with an average of 11.1 steps per episode.
    - Intent categories are diverse, with Video (12.80%), Shopping (11.19%), and Post (7.49%) being the most common.
    - Top apps include Rednote (8.16%), WeChat (4.83%), and Bilibili (3.83%).
- **Personalized Action Analysis:**
    - An analysis was conducted to verify that users have distinct action preferences.
    - Levenshtein similarity of action sequences for similar tasks was calculated between (i) the same user, (ii) users of the same type (e.g., age group), and (iii) users of different types.
    - Results (from Figure 5 bar chart) show average similarity is highest for the same user (76.51), lower for the same user type (67.46), and lowest for different user types (55.87), supporting the premise of personalized actions.
- **Data Splits:**
    - The data was split into a training set (16,000 episodes), a validation set (4,411 episodes), and two test sets: test-suggestion (1,000 episodes) and test-execution (200 episodes).
    - The test sets were created by sampling the last 20% of each user's data to ensure all users are represented.

# Experiments
- **Experimental Setup:**
    - **Models:** Generalist models (GPT-4.1, Qwen-VL-Max, DeepSeek-VL2, Qwen-2.5-VL-7B) and specialized GUI-control agents (Aguvis-7B, CogAgent-9B, UI-TARS-1.5-7B) were evaluated.
    - **Environment:** A physical phone connected via Android Debug Bridge (ADB) was used for execution tasks.
    - **Metrics for Proactive Suggestion:** `SR1` (success rate judged by DeepSeek-V3) and `Sim1` (average of cosine and Levenshtein similarity between predicted and true intent).
    - **Metrics for Personalized Execution:** `SR2` (manually checked success rate), `Sim2` (ratio of action sequence similarity to the current user vs. a different user type), Step Ratio, Time, and Token count.
- **Overall Performance:**
    - **Proactive Suggestion (Table 4):** With 0 initial screenshots, all models performed poorly. GPT-4.1 was the best generalist model with `SR1 = 7.2%` and `Sim1 = 0.35`.
    - **Personalized Execution (Table 5):** Generalist models had very low success rates (`SR2` < 6%). GUI-control models performed better, with UI-TARS-1.5-7B being the best at `SR2 = 38.5%` and a Step Ratio of 1.22. However, all models had a `Sim2` score around 1.0, indicating a lack of personalization.
- **Effect of Task Difficulty:**
    - **Suggestion (Fig 6a):** `SR1` increased for all models as more initial screenshots were provided (from 0 to 3). For GPT-4.1, `SR1` increased from ~7% to ~10%.
    - **Execution (Fig 6b):** `SR2` decreased as the required action length of the task increased. UI-TARS-1.5-7B's `SR2` dropped from ~60% for short tasks to ~10% for tasks with 15+ steps.
- **Effect of Fine-tuning:**
    - Qwen-2.5-VL-7B was fine-tuned using LoRA on 1,000 episodes from the training set.
    - **Results (Table 6):** The fine-tuned model (Qwen-2.5-VL-7B-FT) showed significant improvements.
        - In suggestion, `SR1` increased to 9.7% (+6.6), outperforming GPT-4.1. `Sim1` increased to 0.49 (+0.24).
        - In execution, `SR2` increased to 12.5% (+11.0), still below UI-TARS-1.5-7B (38.5%). However, `Sim2` increased to 1.21 (+0.26), indicating it learned personalized action preferences. The Step Ratio improved to 1.17.

# Conclusion
- The paper introduces FingerTip 20K, a benchmark for proactive and personalized mobile GUI agents, based on a new dataset of longitudinal, contextualized user interactions.
- Experiments show that existing models have significant gaps in their ability to proactively suggest tasks and execute them in a personalized manner.
- Fine-tuning Qwen-2.5-VL-7B on the FingerTip data led to improved suggestion accuracy and demonstrated the ability to align actions with user preferences, validating the utility of user-oriented training data.

# References
This section lists the references cited in the paper.

# Appendix
- **Limitations:**
    - The dataset is geographically limited, with all 91 contributors from mainland China using Chinese apps, which may affect generalizability.
    - Fine-tuning experiments were limited in scale (1000 episodes, one 7B model).
    - The paper acknowledges privacy risks, as UI traces could potentially be used for re-identification.
- **Broader Impacts:**
    - The technology could improve accessibility for elderly or motor-impaired users and automate repetitive tasks.
    - Risks include privacy invasion from continuous screen capture and potential misuse of sensitive personal data. The authors advocate for on-device processing and privacy-preserving techniques.
- **Data Collection:**
    - Participants were recruited via crowdsourcing, paid, and gave informed consent. They were instructed not to upload private information.
    - The collection app only recorded data when actively used by the participant.
- **Data Format:**
    - The data is structured by user ID and then by timestamped episodes.
    - Each episode includes screenshots, accessibility trees (XML), actions (JSON), intent description, user ID, time, and scenario.
    - An example of the user profile structure is provided (user_id, sex, age, occupation, etc.).
    - There are 12 predefined location scenario categories (e.g., residence, office, school).
- **Prompts for the LLM agents:**
    - This section provides the full text of the prompts used for the proactive task suggestion and personalized task execution tracks.