Summary
This paper introduces FingerTip 20K, a new benchmark designed to evaluate mobile GUI agents on proactive and personalized capabilities, addressing limitations of current reactive, instruction-following paradigms. The authors propose two novel tracks: (1) proactive task suggestion, where an agent predicts a user's intent from contextual cues, and (2) personalized task execution, where an agent executes a task by mimicking a user's preferred action patterns. To support this benchmark, the authors collected a large-scale dataset of 21,437 interaction episodes from real users over two months. This dataset is distinguished by its inclusion of rich user-centric information, such as profiles, location, time, and historical interactions. Experiments on existing generalist and specialized agents reveal significant challenges in these new tasks. However, a model fine-tuned on the FingerTip dataset demonstrates marked improvements, highlighting the value of user-oriented data for building more intelligent mobile agents.Strengths
*   **Novel and Important Problem Formulation**
    *   The paper effectively identifies and addresses a critical gap in mobile agent research: the shift from purely reactive agents to proactive and personalized assistants (Section 1). This moves the field towards more practical and user-centric applications.
    *   The two proposed tracks, proactive task suggestion (Section 3.1) and personalized task execution (Section 3.2), are well-defined and represent a significant conceptual advance over existing execution-only benchmarks (Table 1).
    *   The motivation to reduce user cognitive load and serve latent needs is compelling and aligns with the long-term vision of intelligent personal assistants (Section 1).*   **High-Quality and Realistic Dataset Concept**
    *   The core contribution is a large-scale dataset collected from users during their real-world, daily mobile phone usage over two months (Section 4.2). This methodology provides a more authentic and challenging setting than benchmarks based on synthetic tasks or annotator-driven data collection (Table 1).
    *   The dataset is uniquely enriched with user-specific contextual information, including user profiles, time, location scenarios, and longitudinal data on past intents and actions (Section 3, Table 1, Appendix A.4). This rich context is essential for the proposed proactive and personalized tasks.
    *   The scale of the dataset is substantial, comprising 21,437 episodes across 506 different applications, ensuring diversity in tasks and UI interactions (Table 1, Figure 4d).*   **Thorough Experimental Evaluation and Analysis**
    *   The paper provides a comprehensive benchmark of both powerful generalist models (e.g., GPT-4.1) and specialized GUI-control agents (e.g., UI-TARS-1.5-7B), offering a clear picture of current model capabilities on the new tasks (Tables 4 and 5).
    *   The authors introduce novel and task-appropriate evaluation metrics, such as `Sim1` for textual similarity in suggestions and `Sim2` for measuring personalization in action sequences, which go beyond simple success rates (Section 5.1).
    *   The analysis of model performance under varying task difficulties (e.g., number of initial screenshots for suggestion, action sequence length for execution) provides valuable insights into model limitations (Section 5.3, Figure 6).
    *   The fine-tuning experiment (Section 5.4, Table 6) serves as a strong proof-of-concept, demonstrating that the collected data can be effectively used to improve both proactivity and personalization, even with a relatively small model.*   **Strong Empirical Motivation for Personalization**
    *   The paper provides a quantitative analysis to justify the need for the personalized task execution track (Section 4.4).
    *   By calculating the Levenshtein similarity of action sequences, the authors show that sequences for similar tasks are significantly more alike for the same user or users of the same type compared to users of different types (Figure 5 bar chart). This analysis provides a solid, data-driven foundation for the benchmark's design.Weaknesses
*   **Limited Generalizability and Inconsistent Description of the Dataset**
    *   The dataset was collected exclusively from users in mainland China, interacting primarily with Chinese third-party apps (Appendix A.1). This narrow demographic and geographic scope may limit the generalizability of findings and models trained on this data to other regions, languages, and user populations.
    *   There is a direct contradiction in the number of data contributors reported. The main text states data was collected from **91 users** (Section 1, Section 4.2), while the data collection pipeline diagram explicitly states "**Users: 83**" (Figure 3). This discrepancy in a fundamental dataset statistic is concerning.
    *   The prompts shown in the appendix are in English (Appendix A.5), while the app names (Figure 4d) and example screenshots (Figure 7) are in Chinese. This potential language mismatch during evaluation is not discussed and could disadvantage models not optimized for Chinese-language UIs.*   **Potential Subjectivity and Lack of Validation in Key Metrics**
    *   The primary success metric for proactive task suggestion, `SR1`, is determined by an LLM (DeepSeek-V3) judging semantic equivalence (Section 5.1). The reliability and potential biases of this "LLM-as-a-judge" approach are not validated against human judgment, making the reported success rates difficult to interpret with confidence.
    *   The success metric for personalized task execution, `SR2`, is based on manual checking (Section 5.1). Without a detailed, public rubric or protocol for this manual evaluation, the results are difficult to reproduce and may be subject to evaluator bias.
    *   The personalization metric `Sim2` is a ratio of similarities. However, its sensitivity is not fully analyzed; for instance, it is unclear how much a `Sim2` value of 1.21 (Table 6) truly reflects a user's preference compared to a value of 1.06.*   **Ambiguity and Inconsistencies in Experimental Setup**
    *   For the personalized execution track, it is unclear how generalist models were prompted to generate precise, grounded actions (e.g., `click(coordinates=(x,y))`) from screenshots alone. The resulting poor performance (Table 5) is expected and may not constitute a fair or informative baseline.
    *   The description of how contextual information was integrated into the prompts for specialized GUI agents is vague (Section 5.1). The exact method is critical for assessing why these models failed to show personalization (`Sim2` ≈ 1.0 in Table 5).
    *   A key baseline model is referred to inconsistently as "CogAgent-7B" in one table (Table 5) and "CogAgent-9B" in another (Figure 6b), making it difficult to track the experimental setup.
    *   The fine-tuning methodology is contradictory. The text states that the authors obtained "two fine-tuned models, each suitable for one of the two tracks" (Section 5.4), but the results are presented for a single model, "Qwen-2.5-VL-7B-FT", evaluated on both tracks (Table 6).*   **Significant Data Reporting Inconsistencies**
    *   The manuscript contains conflicting numerical data between figures and their corresponding tables. In Figure 6a, the table of SR1 values for proactive suggestion is inconsistent with the plotted line chart for multiple models (e.g., Qwen-VL-Max and DeepSeek-VL2 values appear swapped and different).
    *   Similarly, in Figure 6b, the table of SR2 values for personalized execution does not match the plotted chart (e.g., values for CogAgent-9B and Aguvis-7B are inconsistent).
    *   The presentation of the personalized action analysis is also confusing, with a poorly formatted table and a bar chart presented under the same "Figure 5" label (Section 4.4) that contain different information. These discrepancies undermine the credibility of the reported results.*   **Methodological and Scholarly Practice Concerns**
    *   The data cleaning process introduces a potential for circularity. The authors state they used "Qwen-VL-Max to... regenerate the action" for non-compliant data points (Section 4.2). Since Qwen-VL-Max is also a baseline model in the evaluation (Table 4), this process may unfairly favor its performance. The paper does not quantify what portion of the dataset was modified.
    *   The bibliography contains numerous invalid references that are future-dated to the year **2025** and use what appear to be non-existent arXiv identifiers (e.g., References [14], [24], [30], [31]). This is a serious issue that raises concerns about the scholarly diligence and integrity of the manuscript.Suggestions for Improvement
*   **Address and Clarify Dataset Characteristics**
    *   The geographic and linguistic limitations should be discussed more prominently in the main paper to give readers a clearer context for the results.
    *   Please resolve the contradiction regarding the number of users (91 vs. 83) and report the correct number consistently throughout the manuscript.
    *   Please clarify the language of the user-provided intents in the dataset and discuss the potential impact of evaluating models on UIs and instructions that may not be in English.*   **Strengthen and Validate Evaluation Metrics**
    *   To increase confidence in the `SR1` metric, please conduct a human evaluation on a subset of the test predictions. Reporting the agreement rate (e.g., Cohen's Kappa) between the LLM judge and human annotators would substantially validate this metric.
    *   For the `SR2` metric, please provide a detailed rubric or protocol used for the manual success checks. Releasing this protocol would be invaluable for reproducibility.
    *   For the `Sim2` metric, consider providing a qualitative analysis or examples that illustrate what a higher `Sim2` score looks like in practice. This would help readers better understand the practical significance of the reported improvements.*   **Clarify and Refine the Experimental Setup**
    *   Please provide more details on how the generalist models were prompted to produce grounded UI actions. This would make the baseline setup more transparent.
    *   Please provide the exact prompt templates or a more detailed description of how user profile and historical action data were integrated into the prompts for the specialized GUI agents.
    *   Please correct the inconsistent naming of the CogAgent model across Table 5 and Figure 6.
    *   Please clarify the fine-tuning procedure: was it one model trained for both tasks, or two separate models as described in the text? The reporting in Table 6 should reflect the correct methodology.*   **Correct All Data Reporting Inconsistencies**
    *   Please perform a thorough check of all figures and tables to ensure complete consistency. The numerical values in the tables for Figure 6 must be corrected to match the data plotted in the charts (and the results in Tables 4 and 5).
    *   The presentation of Figure 5 should be clarified. It is recommended to remove the confusing and incomplete table and rely solely on the clearer bar chart, ensuring the text accurately refers to it.*   **Address Methodological and Scholarly Practice Issues**
    *   Please quantify the portion of the dataset that was repaired using Qwen-VL-Max and discuss the potential impact of this procedure on the evaluation fairness. A better approach would be to exclude these repaired samples from the test set for any model in the Qwen family.
    *   It is imperative to conduct a thorough review and correction of the entire bibliography. All references must be accurate, valid, and correctly dated. The use of placeholder or fabricated citations is unacceptable.Score
*   Overall (10): 5 — The paper proposes a novel and important problem, but the execution is marred by severe inconsistencies, methodological issues, and flaws in scholarly practice that require major revision.
*   Novelty (10): 9 — The proactive and personalized task formulations are new for this domain, and the user-centric, longitudinal data collection concept is a significant and novel contribution.
*   Technical Quality (10): 4 — The work is severely weakened by major data reporting inconsistencies (Figure 6), potential circularity in dataset creation (Section 4.2), and unvalidated metrics (Section 5.1).
*   Clarity (10): 6 — While the paper is generally well-motivated, its clarity is significantly undermined by numerous contradictions between the text, tables, and figures (e.g., Figure 3 vs Section 4.2, Table 6 vs Section 5.4).
*   Confidence (5): 5 — I am highly confident in my assessment, as the identified issues are based on direct and verifiable evidence within the manuscript.