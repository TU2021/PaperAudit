Summary
- The paper presents FingerTip 20K, a benchmark to evaluate mobile GUI agents along two new dimensions: (i) proactive task suggestion, where the agent predicts a user’s current intent from user profile, time, scenario, recent intents, and up to three initial screenshots; and (ii) personalized task execution, where the agent executes instructions while aligning its action sequence with a specific user’s historical action preferences (Sections 3.1–3.2; Figure 2). The dataset contains 21,437 multi-step Android episodes from 91 users over 1–2 months, with screenshots, accessibility trees, actions, and user context (Sections 4.2–4.3; Table 1; Figure 4). Experiments cover generalist MLLMs and GUI-control models, report time/token cost, and analyze task difficulty (Section 5; Tables 4–5; Figure 6). A LoRA-fine-tuned Qwen-2.5-VL-7B achieves notable gains for both tracks (Table 6). The paper releases code and data (Introduction; Appendix A.4).Strengths
- Bold contribution: Proactive and personalized benchmark design
  - Evidence: The benchmark explicitly defines two tracks—proactive intent prediction and personalized execution—beyond standard “follow instruction” settings (Sections 1, 3.1–3.2; Figure 2).
  - Why it matters: Introduces previously under-evaluated capabilities critical for user-centric agents (novelty/impact).
  - Evidence: Track 1 formalization requires f(U, T, S, I_history, O) to output an unambiguous intent with app name and final effect (Section 3.1).
  - Evidence: Track 2 formalization integrates user historical actions as in-context preference signals during multi-step control (Section 3.2).- Real-world, longitudinal data from daily usage
  - Evidence: Data collected from 91 users on their own Android phones, over 1–2 months, via a custom app capturing intents, screenshots, accessibility trees, and actions (Section 4.2; Figure 3).
  - Why it matters: Moves away from simulator-only, annotator-scripted datasets toward ecological validity (impact/rigor).
  - Evidence: Users self-report intents at time and scenario, then demonstrate actions; up to 12 episodes/day with training provided for quality (Section 4.2).
  - Evidence: Context fields include user profile, timestamp, and location category, enabling context-aware evaluation (Sections 3.1, 4.2; Appendix A.4–A.5).- Scale and diversity at the app level
  - Evidence: 21,437 episodes across 506 apps (Table 1; Section 4.3; Figure 4d).
  - Why it matters: Broad coverage of third-party apps increases generality and complexity (impact).
  - Evidence: Multi-step episodes—average 11.1 steps—with screenshots and accessibility trees (Table 1; Figure 4b).
  - Evidence: Intents span 40 categories, with distributions provided (Figure 4c; Table in Figure 4).- Clear task formalization and inputs/outputs
  - Evidence: Formal definitions for both tracks with variables and requirements (Sections 3.1–3.2).
  - Why it matters: Precise specs aid reproducibility and fair comparisons (clarity/rigor).
  - Evidence: Action space is listed with formats (Table 2), and prompts are provided for both tracks (Appendix A.5).- Personalized action analysis establishing user differences
  - Evidence: Similarity analysis across same user, same-type (age group), and different-type users shows reduced similarity across types (Section 4.4; Figure 5).
  - Why it matters: Empirically motivates the need for personalization metrics and tasks (novelty/impact).
  - Evidence: Sampled across 40 categories, normalized Levenshtein similarities plotted (Section 4.4; Figure 5).- Comprehensive evaluation across model families with cost reporting
  - Evidence: Generalist MLLMs (GPT-4.1, Qwen-VL-Max, DeepSeek-VL2, Qwen-2.5-VL-7B) and GUI-control models (Aguvis-7B, CogAgent-9B, UI-TARS-1.5-7B) are compared (Sections 5.1–5.2; Tables 4–5).
  - Why it matters: Positions FingerTip against strong baselines; cost metrics inform practicality (rigor/impact).
  - Evidence: Time and token counts per query are reported for both tracks (Tables 4–5; Section 5.1–5.2).- Difficulty analyses are informative
  - Evidence: Track 1 performance increases with more initial screenshots (0 to 3) (Section 5.3; Figure 6a).
  - Why it matters: Validates task design and the value of partial observations (soundness).
  - Evidence: Track 2 success decreases with longer action sequences, as expected (Section 5.3; Figure 6b).- Fine-tuning demonstrates the value of user-oriented data
  - Evidence: LoRA fine-tuning of Qwen-2.5-VL-7B on 1,000 episodes yields SR1 +6.6 and Sim1 +0.24; SR2 +11.0 and Sim2 +0.26; Step Ratio -0.99 (Table 6; Section 5.4).
  - Why it matters: Shows that even a smaller model benefits substantially from contextual data (impact/technical merit).
  - Evidence: The fine-tuned model surpasses GPT-4.1 in Track 1 metrics (Table 6), highlighting data’s contribution.- Ethical considerations and limitations are acknowledged
  - Evidence: Limitations on regional bias, training scale, and privacy concerns; suggestions for redaction/synthetic replay (Appendix A.1).
  - Why it matters: Demonstrates responsible framing and risk awareness (broader impact).
  - Evidence: Broader impacts and data collection governance are detailed (Appendix A.2–A.3).- Public artifacts for reproducibility
  - Evidence: Code link in the Introduction and dataset link in Appendix A.4; detailed data schema and example episode (Introduction; Appendix A.4; Figure 7).
  - Why it matters: Enables community adoption and benchmarking (impact/reproducibility).Weaknesses
- Ambiguities and potential biases in evaluation metrics
  - SR1 relies on a single LLM judge (DeepSeek-V3) to deem “same intent,” with no inter-annotator agreement or rubric (Section 5.1 Metrics). This risks bias and limits replicability (experimental rigor).
  - Sim1 combines multilingual SBERT embeddings with Levenshtein, but the suitability of the chosen model for Chinese intent matching is not validated (Section 5.1 Metrics). This may mis-score semantically equivalent but lexically divergent intents (technical soundness).
  - Sim2 normalizes a user-vs-agent action similarity by a “different-type-of-users” similarity, where “type” is defined by coarse age groups (Sections 4.4, 5.1). This heuristic may be unstable across tasks and user types (technical soundness).
  - No human evaluation or cross-judge sanity checks are reported to calibrate the automatic metrics (No direct evidence found in the manuscript), which weakens claims about personalization and suggestion success (experimental rigor).- Inconsistencies and clarity issues in the action space/specification
  - The paper mentions terminate() (Section 4.2), finish (Table 2), and finished() (Appendix A.5.2) as the terminal action; inconsistent naming can cause implementation ambiguity (clarity/reproducibility).
  - Navigation actions differ across sections: navigate_back/navigate_home/navigate_recent (Table 2) vs press_back/press_home/press_recent (Appendix A.5.2), risking mis-parsing (clarity).
  - While auto-failure is triggered at 2.5× golden steps (Section 5.1 Metrics), the interaction between wait() and this cap is not clarified (e.g., counting waits vs actions), which affects fairness across models (clarity/rigor).- Limited ablations on context contributions for Track 1
  - Difficulty analysis varies only the number of initial screenshots O (Section 5.3; Figure 6a), but not the presence/absence of user profile U, time T, scenario S, or I_history (experimental rigor).
  - There is no decomposition of which contextual elements most drive SR1 or Sim1 (No direct evidence found in the manuscript), limiting insight into where proactive capability comes from (technical soundness).
  - Overall results are reported with O=0 (Table 4; Section 5.2) but without controlled studies isolating the effect of U/T/S vs I_history on performance (experimental rigor).- Data quality and post-processing are under-quantified
  - When accessibility APIs fail, actions are regenerated by Qwen-VL-Max and coordinates checked by OCR (Section 4.2), but the proportion of regenerated actions and their error rates are not reported (experimental rigor).
  - Intent categories are assigned by DeepSeek-V3 (Section 4.3; Figure 4c) without a quality audit against human labels (experimental rigor).
  - The pipeline deletes redundant screenshots by image similarity (Figure 3) with no reported thresholding details or validation of false positives/negatives (technical soundness).
  - No error analysis is provided for noisy/missing user intents or misalignments between intents and demonstrations (No direct evidence found), which could affect both tracks (data integrity).- Reproducibility of the execution environment
  - Personalized execution is run on physical phones due to Chinese app restrictions (Section 5.1), which limits replicability outside the authors’ setup (reproducibility).
  - The dataset includes the app activity name for launching (Appendix A.4) but details on required app versions, device models, or OS are absent, complicating external evaluation (reproducibility).
  - There is no description of hardware specs, Android versions per device, or automation reliability (No direct evidence found), making results hard to replicate (reproducibility).- Statistical reporting and sample size
  - The execution test set has 200 episodes (Table 3), which is small relative to 506 apps and 40 categories, risking sampling variance (experimental rigor).
  - No confidence intervals or statistical tests accompany Tables 4–6 or Figure 6, making it hard to assess significance of improvements, especially for Sim2 near 1.0 (Section 5.2; Table 5) (experimental rigor).
  - Claims that agents act in a general (non-personalized) manner based on Sim2 ≈ 1.0 (Section 5.2) lack variance analysis or sensitivity checks (technical soundness).- Regional scope and demographic bias
  - All 91 contributors are from mainland China (Section 4.2; Appendix A.1), limiting generalization to other regions, languages, and app ecosystems (impact).
  - App distribution is dominated by Chinese apps (Figure 4d), potentially biasing UI layouts and interaction patterns (impact).
  - Prompts mandate Chinese outputs for intents and often inputs (Appendix A.5.1), restricting multilingual applicability (impact).Suggestions for Improvement
- Strengthen and validate evaluation metrics
  - Add multi-judge human evaluation for SR1 on a representative subset, report inter-annotator agreement and a rubric aligned to Section 3.1 requirements (anchors: Section 5.1 Metrics).
  - Validate Sim1 by comparing multiple Chinese-capable sentence encoders and report correlations with human judgments; include per-category breakdowns (anchors: Section 5.1 Metrics).
  - Replace or complement Sim2 with a learned personalization metric (e.g., pairwise preference between user-specific and generic traces) and test robustness to different user groupings beyond age (anchors: Sections 4.4, 5.1).
  - Provide sanity checks showing metric behavior on controlled counterfactuals (e.g., shuffled intents/actions) to demonstrate discriminative power (No direct evidence found).- Resolve action-space and protocol inconsistencies
  - Consolidate terminal action naming across Sections 4.2, Table 2, and Appendix A.5.2 (e.g., standardize on finished()) and update all prompts and parsers accordingly (anchors: Section 4.2; Table 2; Appendix A.5.2).
  - Unify navigation action names (e.g., press_back/home/recent) across Table 2 and prompts, and publish a machine-readable schema (anchors: Table 2; Appendix A.5.2).
  - Specify how wait() counts toward the 2.5× step cap and provide examples in Appendix A.5.2 to ensure fair evaluation (anchors: Section 5.1 Metrics).- Add controlled ablations on contextual inputs for Track 1
  - Report SR1/Sim1 with and without I_history, with and without U/T/S, and their combinations, keeping O fixed to isolate contributions (anchors: Section 5.3; Figure 6a).
  - Provide per-scenario (Appendix A.5; Section 3.1) and per-time-of-day analyses to quantify the value of S and T (No direct evidence found).
  - For O=0 (Table 4), break down results by availability of strong signals in I_history vs sparse history to contextualize the baseline difficulty (anchors: Section 5.2; Table 4).- Quantify data post-processing and quality
  - Report the percentage of actions regenerated by Qwen-VL-Max, their verification pass rate with OCR, and sample error cases; release a flag indicating regenerated steps (anchors: Section 4.2).
  - For intent categorization (Figure 4c), label a sample set by humans to estimate accuracy and confusion with DeepSeek-V3 labels (anchors: Section 4.3).
  - Document thresholds and methods for image-similarity deduplication (Figure 3) and provide precision/recall against a human-checked subset (anchors: Figure 3).
  - Add an error analysis of intent-demonstration alignment quality (e.g., incomplete instructions, noisy app names) and its effect on both tracks (No direct evidence found).- Improve reproducibility of the execution setup
  - Release a dockerized evaluation harness that can replay actions offline against recorded accessibility trees and screenshots, in addition to online execution (anchors: Section 5.1; Appendix A.4).
  - Document device models, Android versions, and app version constraints; provide a compatibility matrix and fallbacks when apps are unavailable (anchors: Appendix A.4).
  - Publish reliability statistics of the ADB pipeline (e.g., action success rates, latency) and guidance for calibrating click coordinates across devices (No direct evidence found).- Enhance statistical reporting and reliability
  - Expand the execution test set beyond 200 episodes or include bootstrapped confidence intervals for SR2/Sim2/Step Ratio (anchors: Table 3).
  - Add CIs and significance tests for Tables 4–6 and Figure 6; report per-category/app breakdowns to assess variance (anchors: Section 5.2; Table 5; Figure 6).
  - For Sim2-based claims about non-personalized behavior (≈1.0), provide variance analyses and sensitivity to different reference sets (anchors: Section 5.2).- Broaden scope and test multilingual generalization
  - Collect a supplemental cohort outside mainland China and add English prompts/labels to evaluate cross-lingual generalization (anchors: Section 4.2; Appendix A.1).
  - Diversify apps beyond the current Chinese-dominated list and report cross-region subset results (anchors: Figure 4d).
  - Provide bilingual prompt templates and a multilingual intent evaluation protocol (anchors: Appendix A.5.1).Score
- Overall (10): 7 — Introduces a novel and well-motivated benchmark with real-world, longitudinal user context and two challenging tracks, accompanied by diverse evaluations and fine-tuning gains (Sections 1, 3–5; Tables 4–6; Figure 6), but metric design, clarity inconsistencies, and limited statistical rigor temper confidence (Sections 5.1–5.3; Table 2; Appendix A.5.2).
- Novelty (10): 8 — First benchmark to jointly evaluate proactive intent suggestion and personalized execution with rich user context and longitudinal data (Sections 1, 3.1–3.2; Table 1; Figure 2).
- Technical Quality (10): 6 — Solid dataset and evaluations with difficulty analyses and fine-tuning (Sections 4–5; Tables 4–6), but evaluation metrics rely on LLM judging/heuristics, ablations on context are limited, and data post-processing quality is under-reported (Sections 4.2, 4.4, 5.1–5.3; Figure 5).
- Clarity (10): 7 — Problem is well formalized and prompts are provided (Sections 3; Appendix A.5), yet action-space naming inconsistencies and some missing protocol details hinder exact reproducibility (Section 4.2; Table 2; Appendix A.5.2).
- Confidence (5): 4 — High confidence in reading and assessing methods/results based on provided figures/tables and appendices (Tables 1–6; Figures 2–6), but limited access to the exact execution environment and reliance on external LLM metrics reduce certainty about some claims (Sections 5.1–5.2; Appendix A.4).