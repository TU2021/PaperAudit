Here are four distinct reviews of the research paper.

***

### **Review 1**

**Summary**
This paper investigates the capabilities of transformer architectures in the context of unsupervised learning, specifically focusing on the fundamental problem of Gaussian Mixture Model (GMM) parameter estimation. The authors propose a novel framework, TGMM, which utilizes a shared transformer backbone to solve multiple GMM tasks with varying numbers of components. Empirically, TGMM is shown to outperform the classical Expectation-Maximization (EM) algorithm and match the performance of spectral methods while offering greater flexibility. The core of the paper lies in its theoretical contributions, where the authors prove that transformers possess the expressive power to approximate both the EM algorithm and a key component of spectral methods (cubic tensor power iterations), thereby providing a mechanistic explanation for their empirical success.

**Soundness**
The methodology is sound, combining a well-designed empirical study with rigorous theoretical analysis. The TGMM architecture is thoughtfully constructed to handle variable task structures (different K) in a parameter-efficient manner. The meta-training procedure on synthetic data is appropriate for learning a general-purpose estimation algorithm. The experiments are comprehensive, covering effectiveness against classical baselines, robustness to distribution shifts, and flexibility in architecture and problem setting (anisotropic GMMs). The theoretical proofs, detailed extensively in the appendix, appear solid and build upon established results in statistical learning theory while introducing novel constructions, particularly for the tensor power iteration approximation.

**Presentation**
The paper is exceptionally well-written, clear, and logically organized. The introduction effectively motivates the research gap and outlines the contributions. The methodology section clearly describes the TGMM architecture and training process, aided by a helpful diagram (Figure 1). The experimental results are presented cogently, with figures that effectively communicate the key findings. The separation of informal theorems in the main text and detailed proofs in the appendix strikes a good balance between accessibility and rigor. The inclusion of proof ideas (Section 4.2) is particularly helpful for conveying the intuition behind the complex theoretical results.

**Contribution**
The contribution of this work is highly significant and novel. It is one of the first papers to systematically explore the mechanisms of transformers in an unsupervised learning setting, moving beyond the well-trodden ground of supervised in-context learning. The key contributions are:
1.  The proposal of TGMM, a practical and effective framework for GMM estimation.
2.  A groundbreaking theoretical result showing that transformers can approximate cubic tensor power iterations, revealing a previously unknown computational capability for high-order tensor operations.
3.  A rigorous proof that transformers can simulate the EM algorithm, providing a direct link between the attention mechanism and a classic iterative statistical procedure.
This work successfully bridges the gap between the empirical power of transformers and their theoretical understanding in the unsupervised domain.

**Strengths**
- **Novelty:** The paper tackles a novel and important problem: understanding transformers as unsupervised learning algorithms. The theoretical result on approximating tensor power iterations is particularly novel and impactful.
- **Strong Empirical Evidence:** The experiments are thorough and convincingly demonstrate that the proposed TGMM framework is not only effective but also robust and flexible, outperforming standard methods like EM in challenging scenarios.
- **Theoretical Depth:** The paper provides deep theoretical insights, proving that transformers can approximate two fundamentally different algorithms for GMMs. This dual analysis strengthens the overall conclusion.
- **Elegant Framework:** The TGMM architecture, with its shared backbone and task-specific readouts, is an elegant solution for multi-task learning in this context.

**Weaknesses**
The paper is very strong, and weaknesses are minor.
- The discussion on the poorer log-likelihood performance (found in Appendix E.2) is brief. While the authors correctly conjecture it's due to the training objective, a more detailed discussion in the main paper would be beneficial, as density estimation is a key goal of GMMs.
- The limitations section could perhaps elaborate more on the challenges of extending this analysis to other unsupervised problems beyond GMMs.

**Questions**
1. The theoretical result for approximating tensor power iterations (Theorem 2) is fascinating. Does this suggest that transformers might be inherently well-suited for other problems that rely on tensor decomposition methods, such as learning latent variable models or topic modeling?
2. The TGMM framework is shown to be flexible enough to handle anisotropic GMMs. Could the same meta-training approach be used to learn a model that can automatically determine the number of components K, rather than taking it as an input?
3. The comparison with Mamba2 (Figure 5) is interesting. Do the authors have any intuition as to why the transformer backbone outperforms Mamba2? Is it related to the global nature of attention versus the recurrent nature of SSMs for this unordered data?

**Rating**
- Overall (10): 9 — This is an excellent paper that makes a significant novel contribution by bridging theory and practice for transformers in unsupervised learning.
- Novelty (10): 10 — The investigation of transformers for unsupervised learning and the theoretical proof for tensor power iteration are highly novel.
- Technical Quality (10): 9 — The theoretical proofs are rigorous and the experiments are comprehensive and well-executed.
- Clarity (10): 9 — The paper is very well-written and easy to follow, despite the technical depth.
- Confidence (5): 5 — I am an expert in this area and have carefully reviewed the paper and its appendix.

***

### **Review 2**

**Summary**
This paper proposes using transformers for the unsupervised learning task of Gaussian Mixture Model (GMM) estimation. The authors introduce an architecture, TGMM, which is meta-trained on synthetic GMM tasks to act as a general GMM solver. Empirically, TGMM shows competitive performance against classical EM and spectral algorithms, particularly in settings where these baselines have known weaknesses. The authors support these findings with theoretical results, claiming that transformers can approximate both the EM algorithm and the cubic tensor power iteration step of spectral methods.

**Soundness**
While the paper presents interesting ideas, there are significant concerns regarding the soundness and the connection between the theoretical claims and the empirical results.

1.  **Theory-Practice Gap:** The paper provides existence proofs that a specifically constructed transformer *can* approximate certain algorithms. However, there is no evidence that the empirically trained TGMM model actually learns or implements these specific constructions. The impressive performance could be due to the transformer learning a completely different, powerful heuristic for GMM estimation from the vast amount of meta-training data.
2.  **Activation Function Mismatch:** Theorem 2, which claims transformers can *exactly* approximate cubic tensor power iterations, relies on a ReLU-activated attention mechanism (Section 4.1, Appendix D). However, the experiments use a standard GPT-2 style transformer (Section 3.1), which typically uses Softmax activation and GeLU in the MLP. This is a critical discrepancy. The authors acknowledge this is for "technical tractability" (Section 4.1), but it undermines the claim that Theorem 2 explains the empirical results.
3.  **Weak Log-Likelihood Performance:** The appendix (Figure 10c, Appendix E.2) reveals that TGMM's log-likelihood performance is significantly worse than both EM and spectral methods in higher dimensions. Since GMMs are fundamentally density models and maximizing likelihood is a primary objective, this is a major failure. A model that achieves low parameter error but assigns low likelihood to the data cannot be considered a complete "solver" for GMMs. This weakness is not adequately addressed in the main paper.
4.  **Assumptions in Theory:** The proof for EM approximation (Theorem 1) relies on Assumption (A1), which requires a good initialization close to the true parameters. This is the very problem that the paper claims TGMM mitigates for the EM algorithm ("highly sensitive to initialization", Section 1). This creates a logical tension: the theory requires a condition that the method is empirically praised for not needing.

**Presentation**
The paper is generally well-written, but it relies too heavily on the appendix. The informal theorems in Section 4 are stripped of their crucial assumptions and limitations (e.g., ReLU activation, initialization requirements, no normalization), making them potentially misleading. A reader of only the main paper would not get a full or accurate picture of the theoretical contributions. The plots in Figure 2 are too small and squashed together, making them difficult to interpret; the tabular format used in the appendix is much clearer.

**Contribution**
The paper's contribution is mixed. The empirical demonstration that a meta-trained transformer can solve GMMs is valuable. The theoretical ideas are intriguing, especially the tensor power iteration construction. However, the disconnects between the theory and the experiments, and the poor performance on a key metric (log-likelihood), temper the overall significance. The claim of "bridging the gap between practical success and theoretical understanding" (Abstract) seems overstated given these issues.

**Strengths**
- Addresses the important and under-explored topic of transformers in unsupervised learning.
- The TGMM framework is demonstrated to be empirically effective in certain regimes, particularly where classical methods fail (e.g., K > d for spectral methods).
- The robustness experiments (Section 3.2, RQ2) provide good evidence that the model is learning a generalizable algorithm.

**Weaknesses**
- **Significant theory-practice gap:** The theoretical constructions are not shown to be what the trained model learns.
- **Inconsistent assumptions:** The theory for the spectral method approximation uses ReLU attention, while experiments use a standard transformer (Section 4.1 vs 3.1).
- **Poor density estimation:** The model fails on the log-likelihood metric in high dimensions (Appendix E.2), a critical aspect of GMMs, which is not discussed in the main paper.
- **Contradictory claims:** The theory for EM approximation requires good initialization (Assumption A1), while the paper claims the method overcomes this very limitation of EM.

**Questions**
1. Can you elaborate on the mismatch between the ReLU-activated attention required for Theorem 2 and the standard transformer used in your experiments? How confident are you that the mechanism described in Theorem 2 is relevant to the performance of your empirically trained model?
2. The proof for EM approximation requires a good initialization (Assumption A1). Your experiments, however, suggest TGMM is robust and overcomes EM's initialization sensitivity. How do you reconcile this? Does the meta-training process implicitly learn to generate a good "initialization" from the data before running an EM-like procedure?
3. The poor log-likelihood performance in higher dimensions (Appendix E.2) is a serious concern for a GMM solver. Why did you not choose to optimize the log-likelihood directly in the training objective (Eq. 2)? Would this not be a more natural objective for a generative model like a GMM?

**Rating**
- Overall (10): 5 — The paper has interesting ideas but suffers from a significant gap between its theoretical claims and empirical reality, and poor performance on a key metric.
- Novelty (10): 7 — The problem area is novel, but the claims are not fully substantiated.
- Technical Quality (10): 5 — The theoretical proofs and experiments appear to be executed correctly in isolation, but their combination is flawed and key results are relegated to the appendix.
- Clarity (10): 6 — The paper is readable, but key technical details and limitations are omitted from the main text, hindering a full understanding.
- Confidence (5): 5 — I am confident in my assessment, having carefully read the main paper and the relevant sections of the appendix.

***

### **Review 3**

**Summary**
This paper presents TGMM, a transformer-based framework for estimating the parameters of Gaussian Mixture Models (GMMs). The model is trained via a meta-learning approach on a large number of synthetic GMM tasks. The authors conduct a series of experiments to evaluate TGMM's effectiveness, robustness, and flexibility. The results show that TGMM can outperform the classic EM algorithm, especially for a larger number of mixture components, and can handle cases (K > d) where the spectral method is not applicable. The paper also includes theoretical arguments suggesting that transformers can approximate the core operations of both EM and spectral algorithms.

**Soundness**
The empirical evaluation is the strongest part of this paper. The experimental design is sound, and the research questions (RQs) posed in Section 3 are well-chosen and systematically addressed. The comparison to EM and spectral methods is a reasonable starting point. The robustness checks, varying sample size (Figure 3) and perturbing the data distribution (Figure 4), are convincing and support the claim that TGMM learns a general algorithm. The flexibility experiments, swapping the backbone for Mamba2 (Figure 5) and extending to anisotropic GMMs (Figure 6), are also valuable additions.

However, the experimental section could be strengthened. The EM baseline seems weak; its performance is poor for K>2. The authors state EM is sensitive to initialization, but they do not specify how it was initialized. Using a single random start is known to be suboptimal; techniques like multiple random starts or k-means++ initialization are standard practice and would provide a much stronger baseline. Furthermore, a crucial practical aspect is missing: a comparison of computational cost. A 12-layer transformer is computationally intensive, and it's unclear how its inference time compares to running a few dozen iterations of EM.

**Presentation**
The paper is clearly written and easy to follow. The structure is logical, guiding the reader from the method to the empirical results. The figures in the main paper are generally effective, although the subplots in Figure 2 are quite small. The use of tables in Appendix E (e.g., Figure 111) is much more readable for presenting precise numerical data and could have been used to supplement the plots in the main text. A significant amount of important experimental data (e.g., results on other metrics like log-likelihood) is placed in the appendix, which feels like an omission from the main story.

**Contribution**
The primary contribution is the empirical demonstration that a transformer, when meta-trained appropriately, can serve as a powerful and flexible solver for GMMs. The TGMM framework itself is a solid contribution. It successfully overcomes known limitations of classical algorithms, such as EM's local optima and the spectral method's K < d constraint. The extension to anisotropic GMMs, where the spectral method does not apply, is a particularly strong demonstration of TGMM's practical utility. The theoretical part, while interesting, feels secondary to these strong empirical findings.

**Strengths**
- **Comprehensive Experiments:** The paper features a strong and thorough empirical evaluation, addressing effectiveness, robustness, and flexibility.
- **Practical Advantages:** TGMM is shown to overcome specific, well-known limitations of widely used GMM algorithms (EM, spectral).
- **Robustness:** The out-of-distribution generalization results (RQ2) are compelling and support the central claim that the transformer is learning an algorithm.
- **Flexibility:** The successful application to anisotropic GMMs (RQ3) highlights the versatility of the proposed framework.

**Weaknesses**
- **Weak Baseline:** The EM algorithm baseline may be unfairly represented if a simple, single initialization was used. Stronger initialization strategies (e.g., k-means++) should be considered for a fair comparison.
- **Missing Computational Cost Analysis:** The paper lacks a comparison of inference time or FLOPs between TGMM and the baselines, which is a critical factor for practical adoption.
- **Key Results in Appendix:** The poor log-likelihood performance is a major finding but is buried in the appendix (Appendix E.2). This trade-off between l2-error and likelihood should be a central point of discussion in the main paper.
- **Superficial Mamba2 Comparison:** The experiment with Mamba2 concludes that transformers are better, but provides little insight into *why*. The analysis is too shallow to be a meaningful contribution.

**Questions**
1. How was the EM algorithm initialized in your experiments? Did you use multiple random starts or a more sophisticated initialization like k-means++, which is common practice to improve EM's performance?
2. Could you provide a practical comparison of the computational cost (e.g., wall-clock inference time) of TGMM versus running the EM and spectral baselines on a representative task?
3. The results in Appendix E.2 show that TGMM has a much lower log-likelihood than the baselines in high dimensions. This suggests it is a poor density estimator. Could you comment on this apparent trade-off between parameter estimation (low l2-error) and density estimation (low log-likelihood)? For which applications would TGMM be suitable or unsuitable given this characteristic?

**Rating**
- Overall (10): 7 — A strong empirical paper with a practical contribution, but it is held back by potentially weak baselines and a lack of discussion on key trade-offs.
- Novelty (10): 7 — The application of meta-learned transformers to this problem is novel and the results are interesting.
- Technical Quality (10): 7 — The experiments are well-designed, but the choice of baselines and missing computational analysis are weaknesses.
- Clarity (10): 8 — The paper is well-written, but the organization could be improved by moving key results from the appendix to the main text.
- Confidence (5): 4 — I am confident in my assessment of the empirical aspects of the paper.

***

### **Review 4**

**Summary**
This work presents a theoretical and empirical study of transformers as algorithms for the unsupervised task of fitting Gaussian Mixture Models (GMMs). The authors propose the TGMM framework, which learns to solve GMMs via meta-training. The main thrust of the paper is to provide a theoretical justification for the empirical success of this approach. To this end, the authors prove two main results: (1) that a transformer with softmax attention can approximate iterations of the EM algorithm, and (2) that a transformer with ReLU-activated attention can exactly implement the cubic tensor power method, a core component of spectral algorithms for GMMs.

**Soundness**
The theoretical contributions of this paper are substantial and technically deep.
- **EM Approximation (Theorem 1):** The construction presented in Appendix C is intricate and clever. It correctly identifies the weighted-averaging property of softmax attention as the key mechanism for simulating the E-step and M-step of the EM algorithm. The proof framework, which combines population-level convergence guarantees with uniform concentration bounds and function approximation theory, is sound and follows a logical progression. The ability of a single model to handle varying dimensions (d) and components (K) is a non-trivial aspect of this result.
- **Tensor Power Iteration (Theorem 2):** This is the most novel theoretical result. The insight that multiple attention heads can be used to perform computations across different tensor indices (as illustrated in Figure 8 and proven in Appendix D) is a genuine contribution to our understanding of transformer capabilities. The proof of *exact* implementation for a non-normalized iteration with ReLU/linear activation is strong.

However, there are important caveats that limit the impact of these theoretical results:
1.  **Incomplete Spectral Method Approximation:** Theorem 2 proves the implementation of an *un-normalized* power iteration. The authors acknowledge this in Remark D.2. However, the robust tensor power method (Algorithm A.3, line 5) and spectral methods in general critically rely on normalization for convergence. Without it, the vector `v` would either vanish or explode. Therefore, claiming to approximate a "crucial component of spectral algorithms" is an overstatement, as the implemented component is missing a crucial piece of its own.
2.  **Contradiction in Initialization Assumption:** The proof for the EM approximation (Theorem 1) requires Assumption (A1), which posits that the algorithm is initialized in a small neighborhood of the true parameters. This is a standard but strong assumption in the EM analysis literature. Yet, the paper's motivation and empirical claims center on TGMM overcoming EM's sensitivity to poor initialization. This creates a direct conflict between the theoretical foundation and the claimed practical advantage, which is not resolved.

**Presentation**
The paper is well-structured, but the presentation of the theoretical results in the main body could be improved. The "Informal" theorems in Section 4 omit the most critical conditions and limitations (e.g., the need for good initialization in Thm 1, the lack of normalization in Thm 2). This gives a rosier picture than the reality detailed in the appendix. While the proof sketches are useful, the main paper would be stronger if the formal theorem statements (or at least their key conditions) were included, perhaps in a dedicated text box. The appendix itself is well-organized and detailed, which is commendable.

**Contribution**
The theoretical contribution is significant, despite the caveats. Theorem 2, showing that attention heads can simulate high-order tensor operations, is a beautiful and novel result that expands our understanding of what transformers can compute. It is likely to inspire future theoretical work. Theorem 1 is a solid, non-trivial construction that successfully maps a classical iterative algorithm onto the transformer architecture. Together, these results provide a strong "existence proof" for transformers' capabilities in this domain. The main weakness is the gap between these theoretical capabilities and what is claimed or observed empirically.

**Strengths**
- **Novel Theoretical Insight:** The proof that multi-head attention can implement tensor power iterations (Theorem 2) is a significant and original contribution.
- **Rigorous Analysis:** The paper provides detailed and rigorous proofs for its theoretical claims in an extensive appendix.
- **Dual-Algorithm Approach:** Providing theoretical connections to *both* EM and spectral methods is a major strength, suggesting transformers are versatile algorithmic approximators.

**Weaknesses**
- **Incomplete Approximation of Spectral Method:** The theoretical construction for the tensor power method (Theorem 2) omits the critical normalization step, making it an incomplete approximation of the actual algorithm.
- **Contradictory Assumption vs. Claim:** The theory for EM approximation relies on a good initialization (Assumption A1), which contradicts the empirical claim that the method is robust to initialization.
- **Oversimplified Theoretical Statements:** The informal theorems in the main paper hide crucial limitations, potentially misleading the reader about the scope and power of the results.

**Questions**
1. The proof of Theorem 2 omits the normalization step of the power iteration, which is essential for convergence. How does this limitation affect the claim that transformers can approximate this "crucial component" of spectral methods? Is there any theoretical path towards approximating the division required for normalization within the attention mechanism?
2. There appears to be a contradiction between the requirement of a good initialization in your EM approximation theory (Assumption A1) and the empirical claim that TGMM overcomes this very sensitivity. Could you please clarify this? Does your theory only explain the model's behavior in a "local refinement" phase, after it has somehow found a good initial guess?
3. Your construction for the EM approximation (Appendix C.2) is very specific. Does this construction provide any prescriptive insights for architecture design? For instance, does it suggest that certain types of non-linearities in the MLP or a specific number of attention heads are better suited for learning such statistical algorithms?

**Rating**
- Overall (10): 8 — A paper with outstanding theoretical novelty and depth, slightly held back by the gap between the idealized theory and the practical claims.
- Novelty (10): 9 — The theoretical result on tensor computations is highly novel and significant.
- Technical Quality (10): 9 — The proofs are rigorous and detailed, representing a high level of technical achievement, though the assumptions and limitations should be highlighted more.
- Clarity (10): 7 — The paper is generally clear, but the informal presentation of the main theorems obscures their important limitations.
- Confidence (5): 5 — I am an expert in theoretical machine learning and have carefully examined the theoretical arguments and proofs.