{
  "baseline_review": "1) Summary\nThis paper investigates the capability of transformers to solve Gaussian Mixture Model (GMM) parameter estimation, a fundamental unsupervised learning problem. The authors propose TGMM, a transformer-based framework that is meta-trained to solve GMM tasks with varying numbers of components using a shared backbone. Empirically, TGMM is shown to outperform the classical EM algorithm, particularly in more challenging scenarios, and match the performance of spectral methods while offering greater flexibility (e.g., when the number of components exceeds the data dimension). The paper provides a dual theoretical justification for these results, proving that transformers can approximate the iterative steps of the EM algorithm and can exactly implement the cubic tensor power iterations, a core component of spectral methods. This work extends the understanding of transformers as algorithmic learners into the unsupervised domain.2) Strengths\n*   **Novel and Well-Motivated Problem Formulation**\n    *   The paper pioneers the study of transformers for a canonical unsupervised learning task, GMM parameter estimation, extending the \"transformers as algorithms\" research line beyond supervised settings (Section 1). This is a timely and important direction given the prevalence of unlabeled data.\n    *   The work clearly frames the problem as learning a data-driven estimation algorithm that can handle tasks of varying complexity (e.g., different numbers of components K), which is a practical and challenging setup (Section 2.2).\n    *   The motivation for using transformers is well-grounded in the limitations of classical methods, such as the EM algorithm's sensitivity to initialization and local optima, and the spectral method's constraint that K must be less than the dimension d (Section 1).*   **Comprehensive and Rigorous Empirical Evaluation**\n    *   The experiments systematically address effectiveness, robustness, and flexibility. TGMM is shown to outperform the EM algorithm and be competitive with the spectral method on synthetic data (Figure 2).\n    *   The model demonstrates strong robustness to distribution shifts, including unseen sample sizes during testing (Figure 3) and perturbations to the data-generating distribution (Figure 4), suggesting it learns a generalizable algorithm rather than overfitting to the training distribution.\n    *   The framework's flexibility is convincingly demonstrated by its ability to handle cases where spectral methods fail (K > d, Figure 2, K=5 panel) and its successful adaptation to more complex anisotropic GMMs (Figure 6, Section 2.7).\n    *   The appendix provides extensive additional results, including evaluations with alternative metrics like clustering accuracy and log-likelihood (Appendix E.2, Figure 10), and ablation studies on the impact of sample size and model scale (Appendix E.3, E.4, Figures 15-17), which adds to the paper's thoroughness.*   **Strong and Dual Theoretical Support**\n    *   The paper provides two distinct theoretical arguments that connect the transformer architecture to classical GMM solvers. This dual justification is a significant strength.\n    *   **EM Approximation (Theorem 1)**: The authors prove that a transformer can approximate multiple steps of the EM algorithm. The construction cleverly uses softmax attention to perform the weighted averaging required in both the E-step and M-step (Section 4.1, Figure 7, Appendix C). The result holds for varying dimensions and components, aligning with the empirical setup.\n    *   **Spectral Method Component (Theorem 2)**: The paper proves that a ReLU-activated transformer can *exactly* implement cubic tensor power iterations, a key subroutine in spectral methods for GMMs. The proof idea, which leverages multiple attention heads to perform computations along different tensor dimensions, is novel and insightful (Section 4.2, Figure 8, Appendix D). This is claimed to be the first result showing transformers can perform high-order tensor calculations.*   **Elegant and Parameter-Efficient Architecture**\n    *   The proposed TGMM architecture is well-designed for the multi-task setting. It uses a shared transformer backbone with task-specific embeddings and readout heads (Section 2.2, Figure 1).\n    *   This design is parameter-efficient compared to training separate models for each task configuration (K), as the bulk of the parameters in the backbone are shared (Appendix B).\n    *   The use of an attentive-pooling mechanism in the readout module is a sensible choice for aggregating token-level information to produce task-level parameter estimates (Section 2.2).3) Weaknesses\n*   **Mismatch Between Theory and Practice for Activation Functions**\n    *   Theorem 2, which provides theoretical support for approximating the spectral method, formally requires a transformer with ReLU-activated attention (Definition 4, Appendix D.1).\n    *   However, all experiments are conducted using a standard GPT-2 style transformer, which uses softmax activation (Section 3.1).\n    *   While the authors note in an appendix remark that ReLU-based attention can perform comparably (Remark D.3), this creates a disconnect between one of the main theoretical contributions and the empirical architecture that was validated. The proof for exact implementation of tensor power iteration relies on the properties of ReLU.*   **Incomplete Theoretical Coverage of the Spectral Algorithm**\n    *   The theoretical analysis for the spectral method focuses exclusively on approximating the cubic tensor power iteration (Algorithm A.3), which is only one part of the full algorithm (Algorithm A.2).\n    *   The full spectral algorithm also involves computing empirical moments (M2, M3), performing SVD on the second moment matrix, and using the results to whiten the data and recover the final parameters. These other critical steps are not shown to be implementable by the transformer.\n    *   The paper acknowledges this as a limitation (Section 5), but it means the theoretical justification for the transformer's ability to emulate the *entire* spectral algorithm is incomplete.*   **Limited Accessibility of Theoretical Assumptions in Main Text**\n    *   The main text presents informal versions of Theorem 1 and Theorem 2 (Section 4.1), with the formal statements, assumptions, and proofs deferred to the appendices (Appendix C, D).\n    *   Crucial context, such as the requirement of a good initialization for the EM approximation (Assumption A1 in Appendix C.1), is not mentioned in the main body. This assumption is critical as it mirrors a key condition for the convergence of the classical EM algorithm itself.\n    *   Without this context, the claims in the main text might appear stronger or more general than they are, and readers must navigate the dense appendices to understand the precise conditions under which the theoretical results hold.*   **Underperformance on Log-Likelihood Metric Not Discussed in Main Body**\n    *   The main experiments report performance using an l2-error metric (Section 3.1). The training objective is a combination of squared error and cross-entropy loss (Equation 2).\n    *   Results in the appendix show that while TGMM is competitive on l2-error and clustering accuracy, it underperforms both EM and spectral methods on the log-likelihood metric, especially in higher dimensions (Appendix E.2, Figure 10c).\n    *   This is a relevant finding, as maximizing likelihood is the canonical objective for GMMs and the explicit goal of the EM algorithm. This trade-off is not discussed in the main experimental results section, which could leave readers with an incomplete picture of the method's performance characteristics.4) Suggestions for Improvement\n*   **Address the Activation Function Discrepancy**\n    *   The mismatch between the ReLU activation in Theorem 2 and the softmax in the experiments should be explicitly acknowledged and discussed in the main text (e.g., in Section 4.1 or 5), not just in an appendix remark.\n    *   To strengthen the connection, consider adding a small-scale experiment comparing the performance of a ReLU-activated TGMM against the softmax version. This would provide empirical evidence for the claim in Remark D.3 and better ground the theoretical result.*   **Clarify the Scope of the Spectral Method Analysis**\n    *   In Section 4.1, when introducing the informal Theorem 2, it would be beneficial to state more clearly that the result covers the tensor power iteration subroutine of the spectral algorithm, and briefly mention that other components of the full algorithm are not covered by the analysis. This would set clearer expectations for the reader upfront.*   **Improve the Presentation of Theoretical Results in the Main Text**\n    *   When presenting the informal theorems in Section 4.1, please briefly summarize the most critical assumptions from the appendix. For Theorem 1, mentioning that the approximation guarantee, like the classical EM algorithm, relies on a sufficiently good initialization would provide essential context for the reader.\n    *   Consider bringing key takeaways from the formal analysis, such as the final error rate achieved by the transformer (Remark C.1), into the main text to make the implications of the theory more accessible.*   **Provide a More Balanced Performance Analysis in the Main Text**\n    *   The main results section (Section 3.2) should briefly incorporate the findings on the log-likelihood metric from Appendix E.2.\n    *   A short discussion on why TGMM underperforms on this specific metric (e.g., the plausible hypothesis about the non-likelihood-based training objective mentioned in Appendix E.2) would offer a more nuanced and complete comparison against classical methods, highlighting both the strengths and potential trade-offs of the proposed approach.5) Score\n*   Overall (10): 8 — The paper presents a novel application of transformers to unsupervised learning with strong empirical results and compelling, dually-justified theoretical insights (Theorems 1 & 2, Figure 2).\n*   Novelty (10): 9 — The work is among the first to systematically study transformers for GMMs, and the theoretical connection to both EM and spectral methods is highly original (Section 1, Section 4).\n*   Technical Quality (10): 8 — The theoretical analysis is deep and the experiments are thorough, but there is a mismatch between the activation function used in theory (ReLU for Theorem 2) and practice (softmax).\n*   Clarity (10): 8 — The paper is well-written and structured, though the key assumptions for the theoretical results are relegated to the appendix, slightly hindering full comprehension from the main text alone (Section 4.1).\n*   Confidence (5): 5 — I am highly confident in my assessment, having thoroughly reviewed the main paper and the detailed appendices.",
  "final_review": "1) Summary\nThis paper investigates the capability of transformers to solve Gaussian Mixture Model (GMM) parameter estimation, a fundamental unsupervised learning problem. The authors propose TGMM, a transformer-based framework that is meta-trained to solve GMM tasks with varying numbers of components using a shared backbone. Empirically, TGMM is shown to outperform the classical EM algorithm, particularly in more challenging scenarios, and match the performance of spectral methods while offering greater flexibility. The paper provides a dual theoretical justification for these results, proving that transformers can approximate the iterative steps of the EM algorithm and can exactly implement the cubic tensor power iterations, a core component of spectral methods. This work extends the understanding of transformers as algorithmic learners into the unsupervised domain.2) Strengths\n*   **Novel and Well-Motivated Problem Formulation**\n    *   The paper pioneers the study of transformers for a canonical unsupervised learning task, GMM parameter estimation, extending the \"transformers as algorithms\" research line beyond supervised settings (Section 1). This is a timely and important direction given the prevalence of unlabeled data.\n    *   The work clearly frames the problem as learning a data-driven estimation algorithm that can handle tasks of varying complexity (e.g., different numbers of components K), which is a practical and challenging setup (Section 2.2).\n    *   The motivation for using transformers is well-grounded in the limitations of classical methods, such as the EM algorithm's sensitivity to initialization and local optima, and the spectral method's constraint that K must be less than the dimension d (Section 1).*   **Comprehensive and Rigorous Empirical Evaluation Design**\n    *   The experiments are designed to systematically address effectiveness, robustness, and flexibility. TGMM is compared against the EM algorithm and the spectral method on synthetic data (Figure 2).\n    *   The model's robustness to distribution shifts is tested, including unseen sample sizes during testing (Figure 3) and perturbations to the data-generating distribution (Figure 4), suggesting it learns a generalizable algorithm rather than overfitting to the training distribution.\n    *   The framework's flexibility is tested via adaptation to more complex anisotropic GMMs, where it outperforms the EM baseline (Figure 6, Section 2.7).\n    *   The appendix provides extensive additional results, including evaluations with alternative metrics like clustering accuracy and log-likelihood (Appendix E.2, Figure 10), and ablation studies on the impact of sample size and model scale (Appendix E.3, E.4, Figures 15-17), which adds to the paper's thoroughness.*   **Strong and Dual Theoretical Support**\n    *   The paper provides two distinct theoretical arguments that connect the transformer architecture to classical GMM solvers. This dual justification is a significant strength.\n    *   **EM Approximation (Theorem 1)**: The authors prove that a transformer can approximate multiple steps of the EM algorithm. The construction cleverly uses softmax attention to perform the weighted averaging required in both the E-step and M-step (Section 4.1, Appendix C). The result holds for varying dimensions and components, aligning with the empirical setup.\n    *   **Spectral Method Component (Theorem 2)**: The paper proves that a ReLU-activated transformer can *exactly* implement cubic tensor power iterations, a key subroutine in spectral methods for GMMs. The proof idea, which leverages multiple attention heads to perform computations along different tensor dimensions, is novel and insightful (Section 4.2, Appendix D). This is claimed to be the first result showing transformers can perform high-order tensor calculations.*   **Elegant and Parameter-Efficient Architecture**\n    *   The proposed TGMM architecture is well-designed for the multi-task setting. It uses a shared transformer backbone with task-specific embeddings and readout heads (Section 2.2, Figure 1).\n    *   This design is parameter-efficient compared to training separate models for each task configuration (K), as the bulk of the parameters in the backbone are shared (Appendix B).\n    *   The use of an attentive-pooling mechanism in the readout module is a sensible choice for aggregating token-level information to produce task-level parameter estimates (Section 2.2).3) Weaknesses\n*   **Significant Inconsistencies in Reported Empirical Results**\n    *   There are major discrepancies between results presented in plots versus tables, which undermines the reliability of the empirical evaluation. For example, for K=4 and d=2, the plot in Figure 2 shows TGMM performing worse than the spectral method, while the table in Figure 10a reports their l2-errors as identical (3.0).\n    *   These inconsistencies also affect qualitative conclusions. For clustering accuracy with K=4 and d=2, the plot in Appendix E.2 shows TGMM is comparable to the spectral method and far better than EM. However, the corresponding table in Figure 10b reports TGMM's accuracy as identical to EM (0.45) and worse than the spectral method (0.60).\n    *   Multiple such contradictions exist across different metrics and settings (e.g., l2-error for EM at K=5, d=2; clustering accuracy for K=3, d=2), suggesting a need for careful data verification.*   **Contradictory Claims Regarding Baseline Limitations**\n    *   The paper motivates its contribution by stating that the spectral method is restricted to cases where the number of components K is smaller than the data dimensionality d (Section 1, Section 3.2).\n    *   However, the main results plot (Figure 2) directly contradicts this claim by showing results for the spectral method in scenarios where K > d, such as K=4, d=2 and K=5, d=2.\n    *   This contradiction confuses the reader about the true limitations of the baseline and weakens the paper's claims about the superior flexibility of TGMM in this regard.*   **Mismatch Between Theory and Practice for Activation Functions**\n    *   Theorem 2, which provides theoretical support for approximating the spectral method, formally requires a transformer with ReLU-activated attention (Definition 4, Appendix D.1).\n    *   However, all experiments are conducted using a standard GPT-2 style transformer, which uses softmax activation (Section 3.1).\n    *   While the authors note in an appendix remark that ReLU-based attention can perform comparably (Remark D.3), this creates a disconnect between one of the main theoretical contributions and the empirical architecture that was validated.*   **Incomplete Theoretical Coverage of the Spectral Algorithm**\n    *   The theoretical analysis for the spectral method focuses exclusively on approximating the cubic tensor power iteration (Algorithm A.3), which is only one part of the full algorithm (Algorithm A.2).\n    *   The full spectral algorithm also involves computing empirical moments, performing SVD, and whitening the data. These other critical steps are not shown to be implementable by the transformer.\n    *   The paper acknowledges this as a limitation (Section 5), but it means the theoretical justification for the transformer's ability to emulate the *entire* spectral algorithm is incomplete.*   **Limited Accessibility of Theoretical Assumptions in Main Text**\n    *   The main text presents informal versions of Theorem 1 and Theorem 2 (Section 4.1), with the formal statements, assumptions, and proofs deferred to the appendices (Appendix C, D).\n    *   Crucial context, such as the requirement of a good initialization for the EM approximation (Assumption A1 in Appendix C.1), is not mentioned in the main body. This assumption is critical as it mirrors a key condition for the convergence of the classical EM algorithm itself.\n    *   The presentation of the theoretical construction in the appendix is also confusing due to ambiguous figure labeling. The text in Appendix C.2 refers to \"Figure 9\" as the formal construction, but there is both a high-level flowchart and a more detailed schematic in that section, with the labeling being unclear.*   **Underperformance on Log-Likelihood Metric Not Discussed in Main Body**\n    *   The main experiments report performance using an l2-error metric (Section 3.1). The training objective is a combination of squared error and cross-entropy loss (Equation 2).\n    *   Results in the appendix show that while TGMM is competitive on l2-error and clustering accuracy, it underperforms both EM and spectral methods on the log-likelihood metric, especially in higher dimensions (Appendix E.2, Figure 10c).\n    *   This is a relevant finding, as maximizing likelihood is the canonical objective for GMMs. This trade-off is not discussed in the main experimental results section, which could leave readers with an incomplete picture of the method's performance.4) Suggestions for Improvement\n*   **Resolve and Clarify Empirical Result Discrepancies**\n    *   The authors must thoroughly check all reported numbers and ensure that plots and tables are consistent with each other. For the K=4, d=2 case, please report the correct relative performance of TGMM and the spectral method.\n    *   For clustering accuracy, please reconcile the conflicting outcomes presented in the plots versus the tables to provide a single, verifiable set of results.\n    *   A full audit of all reported results is necessary to restore confidence in the empirical findings.*   **Correct Claims About the Spectral Method Baseline**\n    *   The claims regarding the limitations of the spectral method (K < d) should be corrected or clarified throughout the manuscript (e.g., in Section 1 and 3.2).\n    *   If a variant of the spectral method that works for K > d was used, this should be explicitly stated and cited. Otherwise, the results presented in Figure 2 for these cases should be explained or removed.\n    *   The paper's claims about TGMM's flexibility advantage should be re-evaluated and stated more precisely in light of the corrected baseline capabilities.*   **Address the Activation Function Discrepancy**\n    *   The mismatch between the ReLU activation in Theorem 2 and the softmax in the experiments should be explicitly acknowledged and discussed in the main text (e.g., in Section 4.1 or 5), not just in an appendix remark.\n    *   To strengthen the connection, consider adding a small-scale experiment comparing the performance of a ReLU-activated TGMM against the softmax version.*   **Clarify the Scope of the Spectral Method Analysis**\n    *   In Section 4.1, when introducing the informal Theorem 2, it would be beneficial to state more clearly that the result covers the tensor power iteration subroutine of the spectral algorithm, and briefly mention that other components of the full algorithm are not covered by the analysis.*   **Improve the Presentation of Theoretical Results in the Main Text**\n    *   When presenting the informal theorems in Section 4.1, please briefly summarize the most critical assumptions from the appendix. For Theorem 1, mentioning that the approximation guarantee relies on a sufficiently good initialization would provide essential context.\n    *   Please clarify the figure numbering and references in Appendix C.2 to ensure the formal construction of the transformer for EM approximation is unambiguous.*   **Provide a More Balanced Performance Analysis in the Main Text**\n    *   The main results section (Section 3.2) should briefly incorporate the findings on the log-likelihood metric from Appendix E.2.\n    *   A short discussion on why TGMM underperforms on this specific metric would offer a more nuanced and complete comparison against classical methods.5) Score\n*   Overall (10): 6 — The paper presents novel theoretical ideas, but the empirical evaluation is undermined by significant inconsistencies and contradictory claims about baselines (Figure 2 vs Section 1, Figure 10 plots vs tables).\n*   Novelty (10): 9 — The work is among the first to systematically study transformers for GMMs, and the theoretical connection to both EM and spectral methods is highly original (Section 1, Section 4).\n*   Technical Quality (10): 4 — The technical quality is severely compromised by major contradictions between plotted and tabulated results (Figure 10), and between claims and evidence for baseline methods (Figure 2), in addition to the theory-practice mismatch on activations.\n*   Clarity (10): 7 — The paper is generally well-written, but clarity is reduced by contradictory claims about baselines and confusing figure labeling in the appendix (Appendix C.2).\n*   Confidence (5): 5 — I am highly confident in my assessment, having thoroughly reviewed the main paper and the detailed appendices.",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 8,
        "novelty": 9,
        "technical_quality": 8,
        "clarity": 8,
        "confidence": 5
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 6,
        "novelty": 9,
        "technical_quality": 4,
        "clarity": 7,
        "confidence": 5
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "1) Summary\nThis paper investigates the capability of transformers to solve Gaussian Mixture Model (GMM) parameter estimation, a fundamental unsupervised learning problem. The authors propose TGMM, a transformer-based framework that is meta-trained to solve GMM tasks with varying numbers of components using a shared backbone. Empirically, TGMM is shown to outperform the classical EM algorithm, particularly in more challenging scenarios, and match the performance of spectral methods while offering greater flexibility. The paper provides a dual theoretical justification for these results, proving that transformers can approximate the iterative steps of the EM algorithm and can exactly implement the cubic tensor power iterations, a core component of spectral methods. This work extends the understanding of transformers as algorithmic learners into the unsupervised domain.2) Strengths\n*   **Novel and Well-Motivated Problem Formulation**\n    *   The paper pioneers the study of transformers for a canonical unsupervised learning task, GMM parameter estimation, extending the \"transformers as algorithms\" research line beyond supervised settings (Section 1). This is a timely and important direction given the prevalence of unlabeled data.\n    *   The work clearly frames the problem as learning a data-driven estimation algorithm that can handle tasks of varying complexity (e.g., different numbers of components K), which is a practical and challenging setup (Section 2.2).\n    *   The motivation for using transformers is well-grounded in the limitations of classical methods, such as the EM algorithm's sensitivity to initialization and local optima, and the spectral method's constraint that K must be less than the dimension d (Section 1).*   **Comprehensive and Rigorous Empirical Evaluation Design**\n    *   The experiments are designed to systematically address effectiveness, robustness, and flexibility. TGMM is compared against the EM algorithm and the spectral method on synthetic data (Figure 2).\n    *   The model's robustness to distribution shifts is tested, including unseen sample sizes during testing (Figure 3) and perturbations to the data-generating distribution (Figure 4), suggesting it learns a generalizable algorithm rather than overfitting to the training distribution.\n    *   The framework's flexibility is tested via adaptation to more complex anisotropic GMMs, where it outperforms the EM baseline (Figure 6, Section 2.7).\n    *   The appendix provides extensive additional results, including evaluations with alternative metrics like clustering accuracy and log-likelihood (Appendix E.2, Figure 10), and ablation studies on the impact of sample size and model scale (Appendix E.3, E.4, Figures 15-17), which adds to the paper's thoroughness.*   **Strong and Dual Theoretical Support**\n    *   The paper provides two distinct theoretical arguments that connect the transformer architecture to classical GMM solvers. This dual justification is a significant strength.\n    *   **EM Approximation (Theorem 1)**: The authors prove that a transformer can approximate multiple steps of the EM algorithm. The construction cleverly uses softmax attention to perform the weighted averaging required in both the E-step and M-step (Section 4.1, Appendix C). The result holds for varying dimensions and components, aligning with the empirical setup.\n    *   **Spectral Method Component (Theorem 2)**: The paper proves that a ReLU-activated transformer can *exactly* implement cubic tensor power iterations, a key subroutine in spectral methods for GMMs. The proof idea, which leverages multiple attention heads to perform computations along different tensor dimensions, is novel and insightful (Section 4.2, Appendix D). This is claimed to be the first result showing transformers can perform high-order tensor calculations.*   **Elegant and Parameter-Efficient Architecture**\n    *   The proposed TGMM architecture is well-designed for the multi-task setting. It uses a shared transformer backbone with task-specific embeddings and readout heads (Section 2.2, Figure 1).\n    *   This design is parameter-efficient compared to training separate models for each task configuration (K), as the bulk of the parameters in the backbone are shared (Appendix B).\n    *   The use of an attentive-pooling mechanism in the readout module is a sensible choice for aggregating token-level information to produce task-level parameter estimates (Section 2.2).3) Weaknesses\n*   **Significant Inconsistencies in Reported Empirical Results**\n    *   There are major discrepancies between results presented in plots versus tables, which undermines the reliability of the empirical evaluation. For example, for K=4 and d=2, the plot in Figure 2 shows TGMM performing worse than the spectral method, while the table in Figure 10a reports their l2-errors as identical (3.0).\n    *   These inconsistencies also affect qualitative conclusions. For clustering accuracy with K=4 and d=2, the plot in Appendix E.2 shows TGMM is comparable to the spectral method and far better than EM. However, the corresponding table in Figure 10b reports TGMM's accuracy as identical to EM (0.45) and worse than the spectral method (0.60).\n    *   Multiple such contradictions exist across different metrics and settings (e.g., l2-error for EM at K=5, d=2; clustering accuracy for K=3, d=2), suggesting a need for careful data verification.*   **Contradictory Claims Regarding Baseline Limitations**\n    *   The paper motivates its contribution by stating that the spectral method is restricted to cases where the number of components K is smaller than the data dimensionality d (Section 1, Section 3.2).\n    *   However, the main results plot (Figure 2) directly contradicts this claim by showing results for the spectral method in scenarios where K > d, such as K=4, d=2 and K=5, d=2.\n    *   This contradiction confuses the reader about the true limitations of the baseline and weakens the paper's claims about the superior flexibility of TGMM in this regard.*   **Mismatch Between Theory and Practice for Activation Functions**\n    *   Theorem 2, which provides theoretical support for approximating the spectral method, formally requires a transformer with ReLU-activated attention (Definition 4, Appendix D.1).\n    *   However, all experiments are conducted using a standard GPT-2 style transformer, which uses softmax activation (Section 3.1).\n    *   While the authors note in an appendix remark that ReLU-based attention can perform comparably (Remark D.3), this creates a disconnect between one of the main theoretical contributions and the empirical architecture that was validated.*   **Incomplete Theoretical Coverage of the Spectral Algorithm**\n    *   The theoretical analysis for the spectral method focuses exclusively on approximating the cubic tensor power iteration (Algorithm A.3), which is only one part of the full algorithm (Algorithm A.2).\n    *   The full spectral algorithm also involves computing empirical moments, performing SVD, and whitening the data. These other critical steps are not shown to be implementable by the transformer.\n    *   The paper acknowledges this as a limitation (Section 5), but it means the theoretical justification for the transformer's ability to emulate the *entire* spectral algorithm is incomplete.*   **Limited Accessibility of Theoretical Assumptions in Main Text**\n    *   The main text presents informal versions of Theorem 1 and Theorem 2 (Section 4.1), with the formal statements, assumptions, and proofs deferred to the appendices (Appendix C, D).\n    *   Crucial context, such as the requirement of a good initialization for the EM approximation (Assumption A1 in Appendix C.1), is not mentioned in the main body. This assumption is critical as it mirrors a key condition for the convergence of the classical EM algorithm itself.\n    *   The presentation of the theoretical construction in the appendix is also confusing due to ambiguous figure labeling. The text in Appendix C.2 refers to \"Figure 9\" as the formal construction, but there is both a high-level flowchart and a more detailed schematic in that section, with the labeling being unclear.*   **Underperformance on Log-Likelihood Metric Not Discussed in Main Body**\n    *   The main experiments report performance using an l2-error metric (Section 3.1). The training objective is a combination of squared error and cross-entropy loss (Equation 2).\n    *   Results in the appendix show that while TGMM is competitive on l2-error and clustering accuracy, it underperforms both EM and spectral methods on the log-likelihood metric, especially in higher dimensions (Appendix E.2, Figure 10c).\n    *   This is a relevant finding, as maximizing likelihood is the canonical objective for GMMs. This trade-off is not discussed in the main experimental results section, which could leave readers with an incomplete picture of the method's performance.4) Suggestions for Improvement\n*   **Resolve and Clarify Empirical Result Discrepancies**\n    *   The authors must thoroughly check all reported numbers and ensure that plots and tables are consistent with each other. For the K=4, d=2 case, please report the correct relative performance of TGMM and the spectral method.\n    *   For clustering accuracy, please reconcile the conflicting outcomes presented in the plots versus the tables to provide a single, verifiable set of results.\n    *   A full audit of all reported results is necessary to restore confidence in the empirical findings.*   **Correct Claims About the Spectral Method Baseline**\n    *   The claims regarding the limitations of the spectral method (K < d) should be corrected or clarified throughout the manuscript (e.g., in Section 1 and 3.2).\n    *   If a variant of the spectral method that works for K > d was used, this should be explicitly stated and cited. Otherwise, the results presented in Figure 2 for these cases should be explained or removed.\n    *   The paper's claims about TGMM's flexibility advantage should be re-evaluated and stated more precisely in light of the corrected baseline capabilities.*   **Address the Activation Function Discrepancy**\n    *   The mismatch between the ReLU activation in Theorem 2 and the softmax in the experiments should be explicitly acknowledged and discussed in the main text (e.g., in Section 4.1 or 5), not just in an appendix remark.\n    *   To strengthen the connection, consider adding a small-scale experiment comparing the performance of a ReLU-activated TGMM against the softmax version.*   **Clarify the Scope of the Spectral Method Analysis**\n    *   In Section 4.1, when introducing the informal Theorem 2, it would be beneficial to state more clearly that the result covers the tensor power iteration subroutine of the spectral algorithm, and briefly mention that other components of the full algorithm are not covered by the analysis.*   **Improve the Presentation of Theoretical Results in the Main Text**\n    *   When presenting the informal theorems in Section 4.1, please briefly summarize the most critical assumptions from the appendix. For Theorem 1, mentioning that the approximation guarantee relies on a sufficiently good initialization would provide essential context.\n    *   Please clarify the figure numbering and references in Appendix C.2 to ensure the formal construction of the transformer for EM approximation is unambiguous.*   **Provide a More Balanced Performance Analysis in the Main Text**\n    *   The main results section (Section 3.2) should briefly incorporate the findings on the log-likelihood metric from Appendix E.2.\n    *   A short discussion on why TGMM underperforms on this specific metric would offer a more nuanced and complete comparison against classical methods.5) Score\n*   Overall (10): 6 — The paper presents novel theoretical ideas, but the empirical evaluation is undermined by significant inconsistencies and contradictory claims about baselines (Figure 2 vs Section 1, Figure 10 plots vs tables).\n*   Novelty (10): 9 — The work is among the first to systematically study transformers for GMMs, and the theoretical connection to both EM and spectral methods is highly original (Section 1, Section 4).\n*   Technical Quality (10): 4 — The technical quality is severely compromised by major contradictions between plotted and tabulated results (Figure 10), and between claims and evidence for baseline methods (Figure 2), in addition to the theory-practice mismatch on activations.\n*   Clarity (10): 7 — The paper is generally well-written, but clarity is reduced by contradictory claims about baselines and confusing figure labeling in the appendix (Appendix C.2).\n*   Confidence (5): 5 — I am highly confident in my assessment, having thoroughly reviewed the main paper and the detailed appendices."
}