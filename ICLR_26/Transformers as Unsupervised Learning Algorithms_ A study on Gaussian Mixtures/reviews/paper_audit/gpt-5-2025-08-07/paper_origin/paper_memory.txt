# Global Summary
- Problem: Investigate whether transformers can act as unsupervised estimators, specifically learning to solve Gaussian Mixture Model (GMM) parameter estimation across varying components and dimensions without labels.
- Approach: Propose TGMM, a meta-trained transformer framework with a shared backbone and task-specific readouts that can solve multiple isotropic (and extended anisotropic) GMM tasks at inference. TGMM uses attentive pooling to decode mixture weights and means (and scales for anisotropic cases). Theoretical results show transformers can approximate EM iterations and exactly implement cubic tensor power iterations (a core step of spectral methods).
- Evaluation scope: Synthetic isotropic GMM tasks with K ∈ {2, 3, 4, 5} and dimensions d ∈ {2, 8, 32, 128}; robustness to distribution shifts (sample size and mean perturbations); flexibility via alternative backbone (Mamba2) and anisotropic GMM extension. Training uses GPT-2–style transformer (12 layers, 4 heads, hidden size 128) with AdamW, LR=1e−4, weight decay=1e−4, batch size 64, for 10^6 steps. Test averaging over 1280 tasks per K.
- Key findings:
  - Effectiveness: TGMM achieves competitive estimation, outperforming EM as K increases and matching spectral where applicable; crucially handles K > d where spectral fails. For isotropic tasks, detailed numbers (Appendix) show TGMM vs EM vs Spectral ℓ2-errors, e.g., for K=3, d ∈ {2,8,32,128}: EM={7.0, 7.0, 6.5, 6.5}, Spectral={2.5, 1.5, 1.0, 1.0}, TGMM={2.5, 2.0, 1.5, 1.5}.
  - Robustness: Under sample-size shift (train N0 ∈ {32,64,128} → test N=128), TGMM degrades gracefully; under mean perturbations with σ_p ∈ {0,…,10}, TGMM still outperforms EM when K>2 (Fig. 4; detailed trends in Appendix).
  - Flexibility: TGMM with Mamba2 backbone is viable but generally underperforms transformer at similar complexity; TGMM extends to anisotropic GMMs, outperforming EM in ℓ2-error while spectral does not directly apply.
- Theoretical claims:
  - EM approximation: A 2(L+1)-layer transformer attains error D_Θ^TF ≤ a β^L + (1/(1−β))ε(N,ε,δ,a) with probability ≥ 1−δ, uniformly over d ≤ d0 and K ≤ K0, with backbone parameters D=O(d0+K0), M=O(1). With ε=Õ(N^−3/2 d^−1/2) and L=O(log N), the error matches Õ(√(d/N)).
  - Tensor power iteration: A ReLU-activated transformer exactly implements L steps of v^{(j+1)} = T(I, v^{(j)}, v^{(j)}) with D=D′=O(d0^2), M=O(d0), log B_Θ ≤ O(1).
- Caveats explicitly stated:
  - Optimization dynamics of training are not analyzed.
  - Full spectral algorithm approximation is not addressed.
  - Study limited to classical GMM; broader unsupervised tasks are future work.

# Introduction
- Context: Transformers dominate modern LLMs and show cross-domain performance; most mechanistic studies focus on supervised settings. Unsupervised learning for transformers (without labels) is underexplored despite real-world relevance.
- Target task: Isotropic GMM parameter estimation with K components. Classical methods: EM (susceptible to local optima and initialization) and spectral moment methods (require K < d, limiting many-component low-dim cases).
- Empirical finding: Meta-trained transformers (TGMM) perform strongly on GMMs, mitigating EM’s pitfalls and being more flexible than spectral methods; TGMM can solve tasks with varying component counts simultaneously.
- Theoretical contributions:
  - Show transformers can approximate EM updates by leveraging softmax attention as weighted averaging; works across varying dimensions and K.
  - Prove ReLU-activated transformers can exactly approximate cubic tensor power iterations (core spectral step); attention heads can compute along dimensions beyond Q–K–V, enabling high-order tensor operations.
- Contributions summarized:
  - TGMM framework supports s task types K ∈ {K1,…,Ks} in one backbone; competitive and robust on synthetic GMM tasks; outperforms EM, matches spectral with greater flexibility.
  - Formal proofs of EM and cubic tensor iteration approximation; bounds polynomial in d, only require M=O(1).
  - Early study on transformers’ mechanism in unsupervised learning; insights bridging practice and theory.
- Organization: Section 2 (background), Section 3 (experiments), Section 4 (theory), Section 5 (discussions), Appendix (proofs and additional results).
- Resources: Code available at https://github.com/Rorschach1989/transformer-for-gmm. Preprint under review.

# Abstract
- Focus: Transformers’ capability to learn internal models during inference in unsupervised settings; study GMM parameter estimation.
- Proposal: TGMM to simultaneously learn multiple GMM tasks with shared transformer backbone.
- Empirical claims: TGMM mitigates EM and spectral limitations; shows robustness to distribution shifts.
- Theory: Transformers approximate EM and cubic tensor power iteration; bridges practical success and theory, positioning transformers as versatile unsupervised tools.

# Related Work
- Supervised algorithmic implementations by transformers: gradient descent for linear regression [2,45,4], UCB and RL algorithms [28], in-context Newton’s method for logistic regression [12], Robbins’ estimator and Naive Bayes [42], minimax optimality for nonparametric regression [23].
- Unsupervised theoretical comparisons:
  - [16]: Transformers implement PCA for GMM clustering, limited to two components.
  - [21]: In-context learning for mixture linear models; approximation limited to two-component GMMs; assumes ReLU activation but relies on a softmax-based lemma [38], causing inconsistency.
  - [17]: Multi-class GMM clustering; focuses on clustering not parameter estimation; bounds scale exponentially in d and require M→∞; experiments on small transformer do not validate theory.
- Authors’ claims of stronger results: Bounds polynomial in d; require only M=O(1); broader applicability beyond two-component cases; empirical validations with larger-scale transformer.

# Preliminaries
- Isotropic GMM with K components: p(x|θ) = ∑_{k=1}^{K} π_k φ(x; μ_k), φ(x; μ) = (2π)^{-d/2} exp(−½‖x−μ‖^2). Samples X_i = μ_{y_i} + Z_i with y_i ∼ π, Z_i ∼ N(0, I_d).
- Classical algorithms:
  - EM (Algorithm A.1): E-step w_k^{(j+1)}(X_i) = [π_k^{(j)} φ(X_i; μ_k^{(j)})]/[∑_{k} π_k^{(j)} φ(X_i; μ_k^{(j)})]; M-step π_k^{(j+1)} = (∑_i w_k^{(j+1)}(X_i))/N, μ_k^{(j+1)} = (∑_i w_k^{(j+1)}(X_i) X_i)/(∑_i w_k^{(j+1)}(X_i)).
  - Spectral (Algorithm A.2): Moment estimation M̂2, M̂3, top-K SVD of M̂2 and cubic tensor decomposition of Ṁ3; returns π̂_k, μ̂_k via λ_k and v_k.
- Transformer definitions:
  - Attention layer with SoftMax: ẐH = H + ∑_{m=1}^M (V_m H) SoftMax((K_m H)^⊤ (Q_m H)); per-token form given.
  - MLP: Residual token-wise ReLU MLP: Ḣ = H + W2 σ(W1 H).
  - Transformer: L layers, each attention followed by MLP.

# Method
- TGMM architecture:
  - Goal: Learn an estimation algorithm for unlabeled GMM tasks; task defined as T = (θ, X, K).
  - Input augmentation: H = [X ∥ embed(K)], then Readin projects to hidden space; shared transformer backbone produces task-aware hidden representations; task-specific Readout modules decode estimates.
  - Readout attentive pooling: O = (V_o H) SoftMax((K_o H)^⊤ Q_o) ∈ ℝ^{(d+K)×K}; first K rows’ row-wise mean-pooling yield mixture probabilities; last d rows yield mean vectors; outputs {π̂_k, μ̂_k}_{k∈[K]} = Readout_{Θ_out}(H).
  - Parameter efficiency: Extra parameters scale as O(s d D) beyond the backbone; reduction by ≈1/s vs naive multi-backbone approach (Appendix B).
  - TGMM formulation: TGMM_Θ(X; K) = Readout_{Θ_out}(TF_{Θ_TF}(Readin_{Θ_in}([X ∥ embed(K)]))).
- Task sampling and meta-training:
  - TaskSampler (Algorithm 1): Sample K ∼ p_K; μ ∼ p_μ; π ∼ p_π; N ∼ p_N; generate X i.i.d. from p(·|θ); return T=(X,θ,K).
  - Training (Algorithm 2): Initialize TGMM; for T steps, sample n tasks; compute objective L̂_n(Θ) as in (2); update Θ via AdamW; return TGMM.
  - Objective (2): L̂_n(Θ) = (1/n) ∑_{i=1}^{n} ℓ_μ(μ̂_i, μ_i) + ℓ_π(π̂_i, π_i); ℓ_μ= squared loss; ℓ_π= cross-entropy.
- Theoretical understandings (Section 4 within Method):
  - EM approximation (Theorem 1, informal; formal in Appendix C): There exists a 2L-layer transformer TF_Θ, for any d ≤ d0, K ≤ K0 and task T=(X,θ,K) under regular conditions and suitable embeddings, TF_Θ approximates L EM steps and estimates θ efficiently with one backbone across varying d and K.
  - Cubic tensor power iteration (Theorem 2, informal; formal in Appendix D): There exists a 2L-layer ReLU-activated transformer implementing L steps of v^{(j+1)} = T(I, v^{(j)}, v^{(j)}) exactly for any d ≤ d0, T ∈ ℝ^{d×d×d}, v^(0) ∈ ℝ^d; uses multi-head attention to compute high-order tensor operations.
  - Remarks: Theorem 1 supports TGMM’s shared backbone across components (M=O(1)) and robustness across sample sizes; task-specific readouts are required (Remark C.3). Proof ideas leverage softmax’s weighted averaging to mirror E/M steps; use MLP to approximate log and x^2.
- Visualizations referenced: Figure 1 (architecture), Figure 7/9 (construction for EM), Figure 8 (tensor iteration via multi-head attention).

# Experiments
- Setup:
  - Backbone: GPT-2 type transformer encoder with 12 layers, 4 heads, 128-dimensional hidden state; task embedding dimension 128.
  - Optimization: AdamW with learning rate 10^{-4} and weight decay 10^{-4}; batch size 64; train 10^6 steps.
  - Task sampling: p_K uniform over {2, 3, 4, 5}; for p_μ, μ_k sampled uniformly from [-5, 5]^d with max pairwise cosine similarity threshold 0.8; p_π uniform in [0.2, 0.8], normalized; p_N uniform in [N_0/2, N_0], default N_0=128.
  - Evaluation: Separate evaluations for K ∈ {2,3,4,5} with sample size 128; average over 1280 randomly sampled tasks.
  - Metric: ℓ2-error defined as (1/K) ∑_{i} [ (1/d)‖μ̂_{σ(i)}−μ_i‖^2 + (π̂_{σ(i)}−π_i)^2 ], with permutation σ chosen to minimize ∑‖μ̂_{σ(i)}−μ_i‖^2 via Jonker-Volgenant algorithm.
  - Additional metrics reported in Appendix E.2: cluster classification accuracy; log-likelihood.
- RQ1 Effectiveness:
  - Qualitative: For K=2, all methods reach near zero estimation error; for larger K, EM underperforms due to local minima; TGMM performs comparably to spectral but additionally handles K > d.
  - Quantitative (Appendix E.2, isotropic ℓ2-error):
    - K=2, d={2,8,32,128}: EM={0.5, 0.5, 0.5, 0.5}, Spectral={0.5, 0.5, 0.5, 0.5}, TGMM={0.5, 1.0, 1.5, 1.5}.
    - K=3, d={2,8,32,128}: EM={7.0, 7.0, 6.5, 6.5}, Spectral={2.5, 1.5, 1.0, 1.0}, TGMM={2.5, 2.0, 1.5, 1.5}.
    - K=4, d={2,8,32,128}: EM={10.5, 9.5, 8.5, 9.0}, Spectral={3.0, 2.5, 2.5, 2.0}, TGMM={3.0, 2.5, 2.0, 2.0}.
    - K=5, d={2,8,32,128}: EM={7.0, 10.5, 10.5, 10.0}, Spectral={6.0, 5.5, 5.0, 4.5}, TGMM={5.5, 5.0, 5.0, 4.5}.
  - Additional metrics (Appendix E.2):
    - Clustering accuracy examples: For K=3, TGMM=0.60 across all d; Spectral=0.90; EM=0.60. For K=4, TGMM={0.45,0.45,0.45,0.45} vs Spectral up to 0.80.
    - Log-likelihood examples: For K=4, d={32,128}: TGMM={−10.0, −12.5}, EM={−2.5, −5.0}, Spectral={−2.5, −5.0}.
- RQ2 Robustness:
  - Sample size shifts: Train N_0 ∈ {32,64,128} → test N=128; results (Figure 3, Appendix Figure 11) show graceful degradation vs in-domain (128→128).
  - Distribution shifts: Mean perturbation σ_p ∈ {0,1,…,10} with d=8; TGMM’s ℓ2-error increases with σ_p but remains better than EM for K>2 (Figures 4, Appendix Figures 12–14). Example trends (Appendix Figure 12a for ℓ2-error): At σ_p=10, K=5, ℓ2-error ≈ TGMM 95 vs EM 125 vs Spectral 75 (approximate from plots); clustering accuracy declines with σ_p yet spectral > TGMM > EM for K>2 (figures show trends).
- RQ3 Flexibility:
  - Alternative backbone: Mamba2 (12 layers, hidden size 128; other hyperparameters matched). TGMM with Mamba2 shows non-trivial estimation but generally inferior to transformer (Appendix Figures 13, 150). Example (Appendix Figure 150a): For K=4 across d, transformer’s ℓ2-error lower than Mamba2; clustering accuracy similarly higher for transformer.
  - Anisotropic extension: Add scale sampling p_σ via softplus(Uniform[-1,1]^d); Readout expands to (d+2K)×K; spectral not directly applicable; TGMM vs EM results in Figure 6 and Appendix Tables 151–153. Examples (Appendix Table 151, ℓ2-error, anisotropic):
    - K=3, d={2,8,32,128}: EM={7.5, 5.0, 5.0, 5.0}, TGMM={2.5, 2.5, 2.5, 2.5}.
    - K=4, d={2,8,32,128}: EM={10.0, 7.5, 7.5, 7.5}, TGMM={5.0, 2.5, 2.5, 2.5}.
    - K=5, d={2,8,32,128}: EM={12.5, 10.0, 10.0, 10.0}, TGMM={7.5, 5.0, 5.0, 5.0}.
- Additional evaluations (Appendix E.3, E.4):
  - Impact of inference-time N: Performance improves with larger N across ℓ2-error, clustering accuracy, and log-likelihood (Appendix Figure 15).
  - Backbone scale: Increasing layers (L ∈ {3,6,12}) or hidden dimension (D ∈ {128,256,512}) yields mild gains; even 3-layer backbone achieves non-trivial performance (Appendix Figures 16–17).

# Conclusion
- Summary: TGMM demonstrates transformers’ capability to learn GMM estimation across tasks, with theory showing EM and tensor-power approximation and experiments indicating effectiveness, robustness, and flexibility.
- Limitations and future work:
  - Optimization dynamics of training not analyzed.
  - Full spectral algorithm implementation left open.
  - Study limited to GMM; exploring other unsupervised tasks is future work.

# References
- Citations span foundational GMM/EM [1,5,9,10,29,40,50], spectral methods and tensor decomposition [3,19], transformer ICL theory and applications [2,4,11,23,27,28,45], architecture and surveys [6,15,22,39,44,46], recent unsupervised transformer analyses [16,17,21,38,41,42], and related deep learning tools [37]. Specific comparative/technical claims in Related Work reference limits of prior works (e.g., two-component focus, exponential bounds in d, inconsistency in activation assumptions).

# Appendix
- Organization:
  - A: Algorithm details for EM (A.1), spectral (A.2), and robust tensor power method (A.3).
  - B: Parameter efficiency of TGMM (extra parameters O(s d D); backbone O(L D^2); practical reduction ≈ 1/s).
  - C: Formal Theorem 1 and proofs; encoding, transformer construction for EM approximation; convergence results for population-EM and empirical-EM; approximation lemmas for log and x^2; final bound D_Θ^TF ≤ a β^L + (1/(1−β)) ε(N, ε, δ, a). Remark C.1: with ε=Õ(N^−3/2 d^−1/2), L=O(log N), obtain D_Θ^TF ≤ Õ(√(d/N)).
  - D: Formal Theorem 2 and proof; ReLU-attention definitions; exact implementation of v^{(j+1)} = T(I, v^{(j)}, v^{(j)}); parameter bounds D=D′=O(d0^2), M=O(d0), log B_Θ ≤ O(1).
  - E: Empirical details and extended results.
    - E.1: Anisotropic adjustments (output (d+2K)×K; loss adds ℓ_σ), Mamba2 configurations, software (PyTorch BSD-style; transformers Apache license), hardware (8 NVIDIA A100 GPUs, 80 GB each).
    - E.2: Complete reports for ℓ2-error, clustering accuracy, log-likelihood; explicit isotropic tables (examples listed above).
    - E.3: Effect of sample size N on performance (N ∈ {32,64,128}); consistent improvements with larger N.
    - E.4: Backbone scaling (vary L and D); mild performance gains; small backbones still effective.
- Notations introduced: π_min, ρ_π, R_min, R_max, norms and operator norms; ẐO(·) for hidden log factors; detailed encoder structures for proofs.
- Key inequalities/assumptions:
  - Separation condition for EM convergence: R_min ≥ C·√log(ρ_π K); initialization bounds on μ_i^{(0)} and π_i^{(0)}.
  - Uniform concentration lemmas (Lemma C.1–C.3) and bound forms used to control empirical-EM deviations.
- Additional figures illustrate construction steps and empirical trends; exact numerical tables provided for some comparisons and metrics.