Summary
The paper introduces Thanos, a post-training, data-aware pruning method for large language models that performs block-wise, simultaneous multi-weight updates per row. The approach decouples mask selection and weight reconstruction: it first selects pruning indices via a global residual mask based on the Wanda metric, then reconstructs the remaining weights using a closed-form solution derived from a quadratic loss with Hessian 2XXᵀ under multiple linear constraints. Thanos is instantiated across three regimes: unstructured pruning, structured pruning with row/column permutations and outlier-row preservation via a parameter α, and semi-structured n:m pruning. The authors provide algorithmic variants, complexity analysis, and practical GPU-oriented engineering (batched solves, padding, segmentation), and release code. Empirically, Thanos is evaluated on OPT and LLaMA families with perplexity and zero-shot metrics, and pruning-time comparisons; results generally show strong gains in structured and semi-structured settings and competitive outcomes for unstructured pruning.

Strengths
- Principled derivation of simultaneous multi-weight reconstruction via a multi-constraint Lagrangian, addressing limitations of sequential single-weight updates. The algebra and quadratic formulation are consistent with standard OBS/OBD principles.
- Clear algorithm design that separates global mask selection from local block-wise reconstruction, with coherent instantiations for unstructured, structured (including permutations and outlier-row handling via α), and semi-structured n:m sparsity.
- Practical engineering for scalability on GPUs: batched linear solves, padding to uniform sizes, and memory-aware vertical segmentation are described in detail, making the approach implementable at scale.
- Empirical coverage across multiple model families and sparsity regimes, with comprehensive reporting of perplexity and zero-shot metrics, and pruning-time comparisons. Gains are most pronounced in structured and n:m settings.
- Open-source release and helpful notation and appendices that document algorithms and implementation details.

Weaknesses
- Mask selection optimality is not established for multi-weight updates. The reliance on the Wanda metric, which is motivated for single-weight removal without reconstruction, lacks theoretical justification or empirical ablation for s>1. Missing comparisons against OBS-based or alternative mask strategies weaken the claim of optimality.
- Comparative fairness concerns in structured settings: Thanos employs α-based outlier-row preservation and permutations that baselines may not use. Without configuring baselines to match these design choices, it is difficult to attribute improvements specifically to the multi-weight reconstruction.
- Claims in the abstract about unstructured pruning superiority are not consistently supported; results are mixed, with baselines sometimes outperforming Thanos in unstructured regimes, especially on zero-shot tasks.
- Deployment impact is under-evaluated. Despite discussion of hardware-friendly sparsity (e.g., 2:4 on Ampere), there are no end-to-end inference throughput/latency measurements or memory footprint reductions reported for compressed models. Reported pruning-time advantages do not substitute for runtime speedups.
- Numerical and complexity clarity issues: parts of the text and pseudocode refer to inverses where solves are appropriate, including an “inverse-of-inverse” expression, which is confusing and potentially misleading. Complexity analysis could more clearly separate solve vs. inversion costs and avoid implying explicit inversion.
- Presentation shortcomings: inconsistent equation numbering between main text and appendix, occasional reference typos, crowded tables mixing regimes, and low-resolution figures hinder readability.
- Scope of ablations is limited. There is no sensitivity analysis for calibration data size and composition, limited exploration of block-size effects beyond a small model, and no systematic evaluation of α selection or layer-wise behaviors. Memory savings are not quantified per model/sparsity configuration.
- Potential interaction and robustness gaps: the impact of permutations on layers with tied structures (e.g., attention Q/K/V) and integration costs are not discussed; compatibility with common pre- or post-pruning techniques (e.g., quantization, activation scaling) is not evaluated.
