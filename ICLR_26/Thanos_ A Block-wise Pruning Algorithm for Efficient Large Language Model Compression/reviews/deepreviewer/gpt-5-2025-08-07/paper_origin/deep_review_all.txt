Summary
The paper proposes Thanos, a post-training, data-aware pruning method for LLMs that performs block-wise weight updates, removing multiple weights per row simultaneously. The authors derive a closed-form update for pruning s weights at once via Lagrange multipliers (Eq. (10)/(60)) and introduce global residual mask selection (ψX in Eq. (11)/(49)) combined with local block masks to decouple mask selection and weight reconstruction. Thanos adapts to three sparsity regimes: unstructured, structured (with row/column permutations and outlier-row preservation via α, Alg. 2), and semi-structured n:m (Alg. 8). Experiments on OPT and LLaMA families report perplexity and zero-shot metrics (Tables 2–3, Appendix D), and pruning-time comparisons (Fig. 9). The method is released as open source.

Soundness
- Derivation: The simultaneous pruning update is derived via a multi-constraint Lagrangian, yielding Δ̂ = −u R̂^{-1}R (Eqs. (7)–(10)/(55)–(61)). The math is standard and consistent with quadratic forms of the loss g(δ) = ||δX||² and Hessian H = 2XXᵀ (Sec. 2.2, Appendix F.2). The structured simplification (Eq. (13)/(72)) correctly reduces to Δ̂ = −W_{:,1:s} H_{1:s,1:s} H^{-1}_{1:s,:}, noting that (H^{-1}_{1:s,1:s})^{-1} = H_{1:s,1:s}, though explicitly writing an inverse-of-inverse is awkward and may suggest unnecessary inversion in implementation (Alg. 2 step 10).
- Mask selection: Using Wanda’s metric |W_{ij}|⋅||X_{j:}||₂ (Eq. (11)) to choose indices and then applying multi-weight reconstruction is plausible but not shown to be optimal for s>1 (Appendix G.3 argues Wanda is optimal for single-weight removal without reconstruction; it does not prove optimality for the s-constraint case). An ablation comparing OBS mask vs Wanda mask for multi-weight updates is missing.
- Complexity: The stated complexities (Table 1; Eq. (76)) match the algorithmic steps (block-wise inverse/solve, mask sorting, and updates). However, using H^{-1} repeatedly (Alg. 1 lines 12–15) presumes access to inverse rows and submatrices; the paper later suggests batched solvers and padding (Appendix H.1–H.2). The analysis could better separate “solve” vs “inverse” complexity; explicit inversion is numerically discouraged.
- Structured pruning with α: The treatment of outlier rows is sensible (Eqs. (14)–(15), Sec. 4.7.1), but fairness vs baselines is unclear because Thanos uses α>0 while SparseGPT/Wanda do not (Tables 2–3), potentially confounding comparisons. The paper acknowledges trade-offs in Appendix A.2.

Presentation
- Organization is generally clear: background (Sec. 3), problem (Sec. 2), algorithm variants (Sec. 4), experiments (Sec. 5), and implementation (Appendix H). The notation table (Appendix B) is helpful.
- Some cross-references are inconsistent (e.g., “metric from (3.2)” in Sec. 4.2 appears to refer to Sec. 3.2/OBD, likely Eq. (5)/(46)). Eq. numbering alternates between main text and Appendix duplications (e.g., (10) vs (60), (13) vs (72)).
- Tables 2–3 interleave unstructured, structured, and semi-structured entries with repeated method labels, making it hard to parse; clearer separation or multiple tables would improve readability. Several figures are low-resolution (Fig. 1 thumbnails, Fig. 2/3) and the legends are small.

Contribution
- Conceptual: Moving from single-weight updates (SparseGPT) to block-wise multi-weight updates per row with a global residual mask is a meaningful design space exploration. The structured variant with explicit row/column permutations and outlier row preservation (α) is a useful extension for practical deployment.
- Empirical: On structured and semi-structured sparsity, Thanos often improves perplexity and zero-shot metrics vs sparse baselines (Tables 2–3; Appendices D.2–D.4), and shows competitive unstructured results. Runtime comparisons (Fig. 9) indicate practical pruning-time advantages in structured regimes.
- Novelty: Prior work prunes one weight at a time (SparseGPT) or uses diagonal Hessian (Wanda). Thanos’ multi-constraint update plus mask-decoupling and α-based structured pruning present incremental but non-trivial novelty.

Strengths
- Solid mathematical grounding for multi-weight updates (Eqs. (6)–(10)/(51)–(61)), with clear construction of R, R̂, and u.
- Practical, well-described engineering to handle GPU memory constraints and batched solves (Appendix H.1–H.3), plus released code.
- Comprehensive evaluations across OPT and LLaMA families, covering perplexity and zero-shot, and multiple sparsity regimes (Tables 2–3; Appendices D–E).
- Clearly articulated structured pruning pipeline with permutations and outlier-row handling (Alg. 2; Fig. 3; Eqs. (14)–(15)).

Weaknesses
- Mask optimality: Reliance on Wanda’s metric (Eq. (11)) for selecting s indices is not theoretically justified for multi-weight updates; missing ablations vs OBS-based mask.
- Fairness: Structured/supported variants employ α and permutations unique to Thanos; baselines may not be configured comparably. The paper should explicitly report baseline variants with similar α/row-protection to isolate the effect of the multi-weight update.
- Claims: Abstract states “outperforms existing methods in unstructured pruning,” but results are mixed (Tables 2–3; Appendix D.1 show SparseGPT/Wanda sometimes win; Appendix A.2 notes SparseGPT often surpasses Thanos in zero-shot unstructured).
- Evaluation scope: No measured inference speedups or wall-clock throughput benefits from hardware-friendly sparsity (2:4) on real GPUs; Fig. 9 reports pruning time, not inference speed. Memory savings are not quantified per model/sparsity setting.
- Clarity: Table/figure resolution and mixed equation numbering (main vs appendix) reduce readability. Several editorial issues (e.g., “metric from (3.2)”) and repeated content between main text and appendix can confuse.

Questions
1. For s>1, how does an OBS-based mask (using S_kq^OBS from Eq. (4)/(45)) compare to Wanda’s metric (Eq. (11)) when paired with the Thanos reconstruction? Please add an ablation.
2. Can you report inference speedups and memory savings for 2:4 and 4:8 sparsity on Ampere GPUs (batch size, sequence length, attention/MLP breakdown), not just pruning time (Fig. 9)?
3. How sensitive is Thanos to the size and composition of calibration data (128 sequences from C4)? Please include an ablation varying d and domains.
4. In structured pruning, can you configure SparseGPT/Wanda to use α-based row exemptions and the same column-ranking (v_j in Eq. (15)) to provide apples-to-apples comparisons?
5. Implementation: Do you actually compute H^{-1}, or solve systems with H directly? Alg. 2 step 10 uses (H_{1:s,1:s}^{−1})^{−1}; please clarify the numerically stable solver used.
6. How does block size B affect wall-clock time and accuracy beyond TinyLlama (Appendix C/Table 5)? Please add curves for larger LLaMA models.
7. Robustness: Do permutations (Sec. 4.7) introduce artifacts across layers with tied/shared structures (e.g., attention Q/K/V splitting)? Any layer-wise exceptions?

Rating
- Overall (10): 7 — Solid derivation and practical engineering with clear gains in structured/semi-structured pruning, but mixed unstructured results and fairness/clarity issues (Eqs. (10), Alg. 2, Tables 2–3, Appendix H).
- Novelty (10): 7 — Multi-weight block-wise update and α-based structured pruning are incremental yet meaningful extensions over SparseGPT/Wanda (Sec. 4.1–4.7, Eq. (13), Fig. 3).
- Technical Quality (10): 7 — Correct quadratic formulation and updates; missing mask optimality ablations and inference speed evaluations; inversion vs solve clarity needed (Eqs. (6)–(10), Table 1, Fig. 9).
- Clarity (10): 6 — Helpful notation and appendices, but equation cross-references, table formatting, and low-res figures hinder readability (Tables 2–3, Fig. 1–3, Sec. 4.2).
- Confidence (5): 4 — I carefully checked derivations and experiments but lack code reproduction and hardware speed measurements; conclusions plausible but need additional ablations.

---

Summary
Thanos is a post-training pruning algorithm for LLMs that prunes multiple weights per row within blocks, reconstructing the remaining weights via a closed-form update derived from a quadratic loss with Hessian 2XXᵀ (Sec. 2, Eq. (10)). It supports three regimes: unstructured (Alg. 1), structured (Alg. 2, with row/column permutations and outlier rows α), and semi-structured n:m (Alg. 8). Experiments on OPT and LLaMA show lower perplexity and competitive zero-shot performance, especially for structured and n:m sparsity (Tables 2–3; Appendices D.2–D.4). The paper discusses complexity (Table 1; Eq. (76)) and implementation tricks (Appendix H) and releases code.

Soundness
- The core update for s simultaneous constraints is correctly derived (Sec. 4.1; Appendix G.1), consistent with quadratic minimization under linear constraints. The structured specialization (Eq. (13)) is algebraically consistent with the general form.
- The global residual mask ψX (Eq. (11)) plus local block mask scheme is coherent (Fig. 2; Appendix G.4.1/Fig. 8) and avoids fixed per-block sparsity constraints of SparseGPT (Appendix F.3/Fig. 4).
- The use of α outlier rows (Sec. 4.7.1) is well-motivated and properly integrated into column selection (v_j in Eq. (15)) and permutations (Sec. 4.7/G.4.4).
- Runtime analysis aligns with the algorithmic structure (Table 1; Eq. (76)); the Appendix shows careful engineering to mitigate GPU memory constraints via batched solves and padding (Appendix H.1–H.2).

Presentation
- The paper presents algorithms step-by-step (Alg. 1–2–7–8–9) and includes diagrams of mask construction and parameter communication (Figs. 2, 3, 7). The notation table is thorough (Appendix B).
- Some aspects are hard to follow: mixed equation numbering (main vs appendix duplicates), occasional reference typos (“metric from (3.2)”), and dense tables mixing regimes (Tables 2–3). Several figures are low-resolution, impeding interpretation (Fig. 1, Fig. 9).
- Limitations are candidly discussed (Appendix A.2), improving balance.

Contribution
- Empirical: Strong gains in structured/n:m pruning across models (Tables 2–3; Appendices D.2–D.4), with pruning-time improvements (Fig. 9c). Unstructured results are competitive but not uniformly superior (Appendix D.1).
- Methodological: A principled multi-weight reconstruction (Eq. (10)) paired with a practical global residual mask and block-wise execution; structured pruning pipeline with α is a useful addition for deployment.
- Systems: Concrete guidance on batched linear solves and memory-aware segmentation (Appendix H), making the approach practical.

Strengths
- Well-founded multi-weight update; coherent block-wise algorithm design (Sec. 4.1–4.4; Alg. 1).
- Strong structured/n:m performance gains, especially with α>0 (Tables 2–3; Fig. 3).
- Clear implementation details with pragmatic solutions to GPU constraints (Appendix H).
- Open-source release and evaluations across many LLMs.

Weaknesses
- Mask selection optimality for s>1 is not established; results hinge on Wanda’s metric (Eq. (11)) without ablation vs OBS-based masks.
- Fairness: Thanos leverages α and permutations; baselines may not. The comparative advantage might reflect different design choices rather than reconstruction alone.
- No inference speed or memory footprint results; claims about hardware benefits (2:4) are not empirically validated (Sec. 4.8, Fig. 9 focuses on pruning time).
- Tables are crowded; clarity suffers; figures lack resolution.

Questions
1. Could you add an ablation comparing ψX (Wanda) vs OBS metric for mask selection in multi-weight updates? How does s influence the optimality gap?
2. Provide end-to-end inference throughput/latency and model memory usage for 2:4/4:8 sparsity on A100/H100 (batch sizes, sequence lengths).
3. How does Thanos perform when α=0 vs α>0 across models? Is there an automated way to choose α per layer or per model?
4. Block-wise: Do you update weights only to the right of the block (off-block) or also within the block (in-block)? Clarify with a precise pseudocode or figure for Alg. 1 (Fig. 7 hints at both).
5. Can you detail the linear solver used (Cholesky/QR/triangular solves)? Do you avoid explicit H^{-1} construction?
6. How robust is ψX to activation scale changes (e.g., SmoothQuant or AWQ pre-processing)? Does it need normalization?
7. Did you try combining Thanos with post-pruning brief finetuning? Any gains?

Rating
- Overall (10): 7 — A well-motivated and practical algorithm with strong structured results, but limited mask optimality analysis and no inference speed evidence (Alg. 1–2, Eq. (10), Tables 2–3).
- Novelty (10): 7 — Block-wise multi-weight updates and α-based structured pruning offer incremental advances over SparseGPT/Wanda (Sec. 4.1–4.7; Fig. 3).
- Technical Quality (10): 7 — Correct derivations and sound engineering; needs ablations and speed/memory measurements (Eq. (11), Table 1, Appendix H, Fig. 9).
- Clarity (10): 6 — Comprehensive but marred by mixed numbering and dense tables/low-res figures (Tables 2–3; Figs. 1–3).
- Confidence (5): 4 — High-level checks done; code not executed; hardware speedups not demonstrated.

---

Summary
This work introduces Thanos, a block-wise pruning algorithm for LLMs. It (1) chooses pruning masks via a global residual selection using Wanda’s metric (Eq. (11)), (2) updates several weights per row at once using a closed-form derived from a quadratic loss with Hessian 2XXᵀ (Eq. (10)), and (3) adapts to structured/n:m sparsity via permutations and outlier row preservation (α) (Sec. 4.7; Alg. 2; Alg. 8). The authors evaluate perplexity on WikiText-2 and average zero-shot scores across LM Harness tasks (Tables 2–3), and report pruning time (Fig. 9).

Soundness
- Systems perspective: The authors correctly identify memory bottlenecks for batched solves and propose padding to uniform r_max (Appendix H.1–H.2), with vertical segmentation to stay within GPU memory. This is practical and consistent with modern PyTorch/Numpy capabilities.
- Complexity claims (Table 1; Eq. (76)) map to blockwise inversion/solves and mask update costs. The stated advantage in structured regimes is plausible due to fixed s and smaller sub-problems (Sec. 4.7–4.8).
- Hardware claims: The text mentions Ampere advantages for 2:4 (Sec. 4.8; references), but does not measure real inference speedups. Without throughput/latency results, the deployment benefit remains hypothetical.

Presentation
- The implementation details (Appendix H) are detailed and useful for practitioners (padding strategy, batched solvers, segmentation). However, the main text sometimes uses “inverse” where “solve” is intended, potentially misleading readers (Alg. 2 step 10).
- Figures of pruning time (Fig. 9) give a clear comparative picture, but axes/legends are small; stronger visualizations would help. Tables mixing sparsity regimes under different headings are confusing (Tables 2–3).
- The Impact Statement (Sec. 7) is minimal; limitations are relegated to Appendix A.

Contribution
- Practical engineering contribution enabling multi-weight updates under GPU constraints; structured/n:m adaptations are deployment-relevant.
- Empirical coverage across multiple model sizes and regimes, with consistent improvements in structured/n:m pruning quality and faster pruning in structured mode (Fig. 9c).
- The algorithm bridges the gap between purely mask-based pruning (Wanda) and sequential OBS updates (SparseGPT) by performing multi-weight reconstructions.

Strengths
- Deployment-oriented algorithmic design and engineering with clear attention to GPU practicality (Appendix H).
- Structured/n:m pruning improves quality substantially and offers pruning-time advantages (Tables 2–3, Fig. 9).
- Clear mask construction strategy with global residuals (Fig. 2; Appendix G.4.1), addressing limitations of purely local masks.

Weaknesses
- No end-to-end inference speed/memory benchmarks for compressed models; practical benefits remain inferred rather than measured (Sec. 4.8 vs Fig. 9).
- Baseline fairness concerns: Thanos uses α and permutations; baselines may not be configured equivalently (Tables 2–3).
- Unstructured performance is competitive but not consistently better; claims should reflect mixed results (Abstract vs Appendix A.2).
- Algorithmic clarity: inverse vs solve terminology; Eq. duplication between main text and appendix.

Questions
1. Can you provide actual inference speedups (tokens/sec) and memory reductions on Ampere/Hopper for 2:4 and 4:8, including end-to-end pipeline overheads?
2. How sensitive is pruning-time to block size B and r_max padding overhead (Appendix H.1)? Are there heuristics to choose B automatically?
3. Do row/column permutations (Sec. 4.7) affect cache locality or kernel fusion in deployment libraries? Any measured overhead?
4. Can SparseGPT be augmented with α and permutations to match Thanos’ structured pipeline for fair comparison?
5. Are there layers (e.g., embeddings, layer norms) excluded from pruning? How does Thanos handle non-square matrices (c≠b) or varying a?
6. Did you attempt a hybrid schedule: start with Wanda mask, refine with OBS metric after weight updates? Any gains?
7. What is the numerical conditioning of H for large models/calibration sizes; do you regularize H (e.g., λI in Alg. 5) in Thanos?

Rating
- Overall (10): 7 — Strong systems contribution for structured pruning with practical engineering; missing inference speed/memory evidence and fairness ablations (Sec. 4.7–4.8, Fig. 9, Tables 2–3).
- Novelty (10): 6 — The block-wise multi-weight update and α-based structured pipeline are useful but incremental (Alg. 2, Eq. (13), Appendix H).
- Technical Quality (10): 7 — Sound derivation and credible complexity analysis; needs deployment metrics and solver clarity (Eq. (10), Table 1, Appendix H).
- Clarity (10): 7 — Implementation details strong; some equation/table/figure clarity issues remain (Alg. 2 step 10; Tables 2–3; Fig. 9).
- Confidence (5): 4 — Reasonable confidence from careful reading; lack of reproduced speedups limits certainty.

---

Summary
The paper presents Thanos, a block-wise pruning method for LLMs that selects pruning masks via a global residual approach (ψX, Eq. (11)) and reconstructs weights by solving a quadratic least-squares with multi-constraint Lagrangian (Eq. (10)). It supports unstructured (Alg. 1), structured with row/column permutations and outlier rows (α) (Alg. 2), and semi-structured n:m (Alg. 8). Extensive experiments report perplexity and zero-shot accuracy (Tables 2–3; Appendices D.1–D.4), and pruning-time comparisons (Fig. 9). Code is released.

Soundness
- The optimization problem is well-posed (Eqs. (1)–(2)/(16)–(21)), and the reduction to per-row constrained minimization is standard (Sec. 2.2; Appendix E.4–E.5). The Hessian and OBS/OBD background are correctly recapitulated (Sec. 3.2–3.3; Appendix F.2–F.4).
- The main novelty is performing s simultaneous removals per row via Eqs. (6)–(10), avoiding the independence approximation of summing single-column changes (Sec. 4.1). This addresses a known weakness of column-wise sequential OBS updates (Sec. 4.0).
- However, the choice of mask (Wanda metric, Eq. (11)) is optimal only for single-weight removal without reconstruction (Appendix G.3). For s>1 reconstruction, optimality is unproven, and the paper relies on heuristic efficacy.

Presentation
- The narrative is comprehensive and well-referenced; appendices provide full derivations and algorithms (Appendices F–H).
- Some editorial issues reduce clarity: inconsistent equation references (e.g., “metric from (3.2)” in Sec. 4.2), duplicated equation numbers between main text and appendix (e.g., (10) vs (60)), and crowded tables mixing regimes (Tables 2–3). Several figures are low-res (Fig. 1, Fig. 9).
- The limitations are acknowledged (Appendix A.2) and the notation table is excellent (Appendix B).

Contribution
- Demonstrates that multi-weight reconstruction can materially improve structured/semi-structured pruning quality compared to baselines (Tables 2–3), with practical pruning-time advantages (Fig. 9c).
- Introduces a usable structured pipeline with outlier-row detection (α) and permutations (Sec. 4.7), helpful for real-world deployment.
- Provides implementation guidance (Appendix H) and open-source code.

Strengths
- Strong theoretical and practical framing of multi-weight updates (Sec. 4.1; Eq. (10)).
- Empirical breadth across OPT and LLaMA families with multiple sparsity regimes (Tables 2–3; Appendices D).
- Practical engineering for batched solves and GPU memory constraints (Appendix H.1–H.2).
- Clear articulation of outlier-row strategy and its impact (Sec. 4.7.1; Eq. (14)–(15); Fig. 3).

Weaknesses
- Mask optimality gap: No ablation comparing Wanda vs OBS masks for s>1; theoretical justification limited (Appendix G.3).
- Claims vs evidence: Abstract overstates unstructured superiority; results are mixed (Table 2; Appendix D.1).
- Comparative fairness: Thanos leverages α and permutations; baselines may not, obscuring attribution of gains.
- Missing deployment metrics: No inference throughput or memory measurements despite claims about hardware-friendly sparsity (Sec. 4.8).

Questions
1. Please report inference speedups (tokens/sec) and memory savings for compressed models on Ampere/Hopper for 2:4/4:8; include kernel-level utilization to support hardware claims.
2. Add an ablation comparing mask strategies: Wanda vs OBS vs magnitude, under multi-weight reconstruction; how does s affect outcomes?
3. How is α chosen? Can you provide per-layer α selection heuristics or a global sweep showing robustness?
4. For unstructured pruning, why does Thanos sometimes underperform SparseGPT on zero-shot (Appendix D.1)? Any insights on reconstructing multiple small weights vs single large ones?
5. Can you clarify the solver stack used (Cholesky/triangular solves) and whether H is regularized (λI), as SparseGPT does (Alg. 5 step 3)?
6. Do permutations introduce any cost in framework integration (e.g., parameter tying, checkpoint compatibility)?
7. Is Thanos compatible with concurrent quantization (e.g., AWQ/SmoothQuant)? Any preliminary results?

Rating
- Overall (10): 7 — A worthwhile contribution with strong structured results and practical engineering; needs deeper mask analysis and deployment metrics (Sec. 4.1–4.8, Tables 2–3, Fig. 9).
- Novelty (10): 7 — Multi-weight reconstruction and α-based structured pruning are incremental but useful innovations (Eq. (10), Alg. 2).
- Technical Quality (10): 7 — Derivations are correct; missing ablations for mask choice and missing speed/memory data (Eqs. (6)–(10), Table 1).
- Clarity (10): 7 — Generally clear but marred by reference inconsistencies and table/figure presentation (Sec. 4.2, Tables 2–3).
- Confidence (5): 4 — High-level assessment based on text and figures; code/hardware results not independently verified.