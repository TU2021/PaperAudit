{
  "baseline_review": "Summary\n- The paper proposes Thanos, a post-training pruning algorithm for large language models that updates multiple weights per row in a block-wise fashion by solving small linear systems, while constructing an adaptive “global residual” pruning mask. The formulation minimizes the change in layer outputs (Eq. (1)–(2)), extends single-weight OBS/OBD to multiple simultaneous constraints (Sec. 4.1; Eq. (6)–(10); Appendix G.1), and introduces structured pruning with “outlier rows” controlled by α (Sec. 4.7.1; Eq. (14)–(15); Alg. 2). Thanos is also adapted to n:m semi-structured sparsity (Sec. 4.8; Alg. 8). Experiments across OPT and LLaMA families evaluate perplexity on WikiText-2 and LM Harness zero-shot tasks, showing competitive/best results for unstructured pruning on several models and consistent gains for structured/semi-structured settings (Fig. 1; Table 2–3; Appendix D). Implementation details and runtime comparisons (Appendix H; Fig. 9) indicate efficiency advantages in structured pruning.Strengths\n- Bolded Title: Rigorous multi-weight update with principled derivation\n  - Evidence: The method extends single-parameter OBS/OBD to multiple simultaneous constraints via Lagrangian multipliers (Sec. 4.1; Eq. (6)–(10); Appendix G.1 with Eq. (51)–(61)). Why it matters: Provides a technically grounded and novel update rule for pruning several weights at once, improving technical soundness beyond column-wise updates.\n  - Evidence: The construction of matrices R, R̂, and vector u (Eq. (7)–(9)) plus the closed-form solution for Δ (Eq. (10)) is explicit. Why it matters: Clear algebra enhances reproducibility and credibility of the multi-constraint approach.\n  - Evidence: Complexity-aware block-wise application aligns with solvable systems per block (Sec. 4.3; Algorithm 1; Appendix G.2). Why it matters: Makes the theoretical multi-weight update computationally feasible for LLM layers.- Bolded Title: Adaptive global-residual mask enabling dynamic pruning\n  - Evidence: The mapping ψX(W, r) selects the r smallest |Wij|‖Xj:‖2 values (Eq. (11); Sec. 4.4; Fig. 2; Appendix G.4.1 Fig. 8; Eq. (69)–(71)). Why it matters: A global residual mask avoids rigid per-block or per-row sparsity, potentially improving final accuracy as weights change.\n  - Evidence: The iterative construction of M̂j and extraction of local masks Mj (Appendix G.4.1; Eq. (69)–(71)). Why it matters: Evidence shows careful design to adapt mask to evolving parameter importance (clarity and technical soundness).\n  - Evidence: Visualizations of mask updates (Fig. 2; Appendix Fig. 8, Fig. 104) demonstrate the algorithm’s dynamics. Why it matters: Improves clarity and illustrates innovation in mask management.- Bolded Title: Structured pruning with outlier-row preservation\n  - Evidence: Definition of per-row and per-column losses (Eq. (14)–(15); Sec. 4.7.1). Why it matters: A principled criterion to identify rows less suitable for pruning, addressing observed outlier phenomena.\n  - Evidence: Algorithm 2 performs row/column permutations with matrices Q and P (Sec. 4.7; Appendix G.4.4) before pruning s columns (Sec. 4.7; Eq. s=⌈pb/(1−α)⌉, Eq. (13)). Why it matters: Structured pruning is formally described with practical steps that maintain index order (clarity).\n  - Evidence: Empirical results show strong gains of α>0 in structured settings (Table 2–3: compare Thanos α=0 vs α=0.1 across LLaMA-2/3). Why it matters: Demonstrates impact of the outlier-row mechanism on accuracy (experimental rigor and impact).- Bolded Title: Extension to semi-structured n:m sparsity with practical hardware relevance\n  - Evidence: Sec. 4.8 and Algorithm 8 detail n:m integration; references to Ampere 2:4 support (Sec. 4.8; NVIDIA 2020; Lin et al., 2023). Why it matters: Aligns with industry acceleration formats, improving practical impact.\n  - Evidence: Results for 4:8 and 2:4 in Table 2–3 and Appendix D.3–D.4 consistently favor Thanos, often over SparseGPT/Wanda (e.g., LLaMA-3 8B, Table 2: Thanos α=0.1 beats baselines at 4:8/2:4). Why it matters: Demonstrates applicability and strong outcomes in semi-structured regimes.\n  - Evidence: Appendix H.3 argues computation benefits due to uniform per-row sparsity (Appendix H.3). Why it matters: Technical efficiency claims for structured formats improve practical viability.- Bolded Title: Broad empirical evaluation across model scales and tasks\n  - Evidence: Models include OPT-125M/350M and multiple LLaMA-2/3 sizes up to 70B (Sec. 5.1). Why it matters: Coverage enhances generality and impact.\n  - Evidence: Perplexity results (Table 2) show Thanos competitive/best in many unstructured cases (TinyLlama 1.3B; LLaMA-3 3B–8B) and strong dominance in structured/semi-structured (Fig. 1(b); Table 2). Why it matters: Experimental rigor and demonstrated performance.\n  - Evidence: Zero-shot results (Table 3; Appendices D.1–D.4) show Thanos competitive or best, notably in structured with α=0.1 (e.g., LLaMA-2/3 70B in structured 30%, Table 3; Appendix D.2–D.4). Why it matters: Validates generalization beyond perplexity.- Bolded Title: Efficiency analysis and implementation detail transparency\n  - Evidence: Complexity comparison (Table 1) and detailed Thanos complexity (Appendix G.6, Eq. (76)). Why it matters: Technical soundness in performance expectations.\n  - Evidence: Runtime comparisons (Appendix H; Fig. 9 a–c) show Thanos comparable or faster than SparseGPT for structured pruning, especially ≤3B (Appendix H.3). Why it matters: Practical impact and efficiency.\n  - Evidence: Implementation details (Appendix H.1–H.2), including batched solvers, padding strategy (Eq. (77)–(79)), and memory management. Why it matters: Enhances reproducibility and clarifies engineering trade-offs.- Bolded Title: Clear algorithmic presentation and supporting materials\n  - Evidence: Algorithm 1 (unstructured), Algorithm 2 (structured), Algorithm 8 (n:m); Notation table (Appendix B/Table 4). Why it matters: Clarity and reproducibility.\n  - Evidence: Generic pruning pipeline (Appendix E.1, Algorithm 3) situates Thanos within block-wise post-training practices. Why it matters: Contextual clarity for readers.\n  - Evidence: Code availability stated (Sec. 1.3(5)). Why it matters: Enables verification and adoption.Weaknesses\n- Bolded Title: Notational inconsistencies and likely errors in structured update formulas\n  - Evidence: Eq. (13) uses “(H_{1:s,1:s}^{−1})^{−1} H_{1:s,:}^{−1}”, which implies double inverse on H_{1:s,1:s} and an inverse of a non-square H_{1:s,:}; Appendix G.4.2 (Eq. (72)) instead shows “(H_{1:s,1:s})^{-1} H_{1:s,:}” without extra inverses. Why it matters: Technical correctness—ambiguity undermines confidence in structured update rule.\n  - Evidence: Algorithm 2 step 10 mirrors Eq. (13) (Sec. 4.7), perpetuating the same notational problem. Why it matters: Implementation risk—readers cannot reproduce a mathematically invalid operation as written.\n  - Evidence: Algorithm 1 step 12 references H^{-1} rows (H_{q:}^{−1}), but line 3 computes H=2XX⊤ and does not specify computing H^{-1} or its factorization; Appendix H discusses Cholesky with SparseGPT but not Thanos inversion steps here. Why it matters: Clarity and technical soundness for unstructured updates.- Bolded Title: Complexity analysis misalignment and limited empirical efficiency evidence at large scale\n  - Evidence: Table 1 omits sequence length a and assumes c=b, while Appendix G.6 provides T_Thanos = O(ab^2 + b^4/B + (cb^2/B)log(cb) + cbB^2) (Eq. (76)). Why it matters: Consistency—discrepancies complicate understanding of scaling behavior.\n  - Evidence: Fig. 9 shows pruning time primarily for OPT scales, with Thanos faster than SparseGPT for structured pruning; however, structured speedups “up to 3B” are asserted (Appendix H.3) without complete plots for ≥3B across families. Why it matters: Experimental rigor—evidence for very large models is limited.\n  - Evidence: No tabulated memory usage or explicit numeric runtime breakdowns per step (Appendix H.1–H.2 mention strategies but no quantitative data). Why it matters: Practical impact—hard to judge real-world cost-benefit.- Bolded Title: Mask metric justification lacks ablation and broader theoretical support\n  - Evidence: Thanos adopts the Wanda metric (Sec. 4.2; Eq. (11)), with Appendix G.3 arguing optimality only for single-weight removal without adjustment (Eq. (65)–(66)). No direct ablation comparing alternative metrics (e.g., OBS metric or Fisher-style criteria) is shown. Why it matters: Novelty/technical quality—using a single-weight optimal metric for multi-weight mask selection may be suboptimal; empirical validation is missing.\n  - Evidence: The claim that “Wanda provides an optimal solution for removing a single weight at a time without adjusting the remaining weights” (Sec. 4.2; Appendix G.3) is correct in its context, but extension to multi-weight selection is asserted rather than proved. Why it matters: Technical soundness—mask choice is central; broader justification would strengthen the method.\n  - Evidence: Appendix A.2 acknowledges Thanos can underperform SparseGPT on some unstructured zero-shot tasks. No ablation ties performance variation to mask choice. Why it matters: Impact—diagnosing when the metric fails would help practitioners.- Bolded Title: Experimental methodology details insufficient for strict reproducibility and fairness\n  - Evidence: Calibration data are 128 sequences sampled from C4 (Sec. 5.1), but seeds, sampling strategy, preprocessing, and exact LM Harness configurations are not fully specified. Why it matters: Reproducibility—small calibration sets are sensitive to sampling.\n  - Evidence: Baseline hyperparameters (e.g., block sizes, damping for inverses) are not fully detailed; only Thanos uses B=128/512 defaults (Sec. 5.1; Appendix C). Why it matters: Fairness—baselines may benefit from tuning parity.\n  - Evidence: No variance/CI or repeated runs are reported for perplexity/zero-shot (Table 2–3; Appendix D). Why it matters: Experimental rigor—assessing significance is difficult.- Bolded Title: Baseline coverage is limited relative to related work\n  - Evidence: The paper compares to Magnitude, SparseGPT, and Wanda (Sec. 5.1), but references mention other pruning approaches like Group Fisher (Liu et al., 2021) and AC/DC (Peste et al., 2021) without empirical comparison. Why it matters: Impact—state-of-the-art claims in structured pruning (Sec. 1.3(4); Fig. 1(b)) are stronger with broader baselines.\n  - Evidence: No fine-tuning or recovery is attempted, while some methods in literature rely on brief tuning to reach higher sparsity (Related Work Sec. 3.3; Appendix F.3 notes constraints). Why it matters: Fairness—practical comparisons often consider short FT.\n  - Evidence: Semi-structured evaluation is strong (Tables 2–3; Appendix D.3–D.4), but hardware-optimized baselines beyond Wanda/SparseGPT are not included. Why it matters: Practical impact—comparing to contemporary n:m kernels/pipelines would be informative.- Bolded Title: Evaluation breadth and hardware claims need more direct evidence\n  - Evidence: Sec. 4.8 claims speedups on NVIDIA Ampere via 2:4 (citing Lin et al., 2023; NVIDIA 2020), but no measured inference throughput or latency is reported on such hardware for Thanos-pruned models. Why it matters: Impact—users need end-to-end acceleration evidence.\n  - Evidence: Fig. 9 reports pruning time but not inference speed or memory savings; structured pruning could reduce weight size (Sec. 4.7), yet no wall-clock inference gains are shown. Why it matters: Practicality—compression goals include runtime benefits.\n  - Evidence: The paper emphasizes outlier rows α (Sec. 4.7.1) and shows accuracy gains (Table 2–3), but there is no sensitivity analysis across α values beyond 0 and 0.1. Why it matters: Clarity—guidance on α selection is limited.Suggestions for Improvement\n- Bolded Title: Fix structured update notation and clarify inversion steps\n  - Action: Replace Eq. (13) and Algorithm 2 step 10 with the corrected form consistent with Appendix G.4.2, namely Δ̂ = −W_{:,1:s}(H_{1:s,1:s})^{-1}H_{1:s,:} (Appendix Eq. (72)); ensure all inverses reference square matrices. Provide a derivation in the main text mirroring Appendix G.1/G.4.2 to avoid confusion.\n  - Action: In Algorithm 1, specify whether H^{-1} is computed via explicit inversion or factorization (e.g., Cholesky) and at which lines it is updated; clarify that H in line 3 leads to H^{-1} usages in line 12 (anchors: Algorithm 1 lines 3, 12; Appendix H.1).\n  - Action: Add shapes and dimensions for H, H^{-1}, R, R̂ in Sec. 4.1/4.3 to prevent ambiguity (anchors: Eq. (7)–(10); Algorithm 1).- Bolded Title: Align complexity analyses and extend efficiency evidence\n  - Action: Reconcile Table 1 with Appendix G.6 (Eq. (76)) by adding a note on assumptions (c=b, ignoring a) and include the full expression with a to reflect sequence length costs (anchors: Table 1; Appendix G.6).\n  - Action: Expand Fig. 9 with additional points ≥3B across model families and report numeric runtimes and memory footprints per method, ideally in a table, to substantiate claims (anchors: Appendix H Fig. 9; Sec. 4.8/H.3).\n  - Action: Provide stepwise runtime breakdowns (mask selection, linear solves, Hessian updates) for Thanos to show where gains arise (anchors: Algorithm 1; Appendix H.1–H.2).- Bolded Title: Validate mask metric choice with ablations and theoretical notes\n  - Action: Add ablations comparing ψX (Eq. (11)) to OBS-based mask (Eq. (4)/(45)) and Fisher-style criteria; measure impact on perplexity/zero-shot across at least one model (anchors: Sec. 4.2; Eq. (11), (4), (45)). \n  - Action: Provide a theoretical discussion on why the single-weight optimal metric (Appendix G.3; Eq. (65)–(66)) is suitable for multi-weight mask selection, or state practical reasons for its choice and limitations (anchors: Appendix G.3).\n  - Action: Connect underperformance cases (Appendix A.2) to mask choice via controlled experiments (e.g., LLaMA-3 8B unstructured, Table 3) to guide practitioners (anchors: Table 3; Appendix A.2).- Bolded Title: Strengthen reproducibility and fairness details\n  - Action: Specify random seeds, exact calibration sampling (C4 shard identifiers, tokenization, context length distributions), and LM Harness configuration options; publish scripts used (anchors: Sec. 5.1; Appendix D).\n  - Action: Document baseline hyperparameters and any tuning performed (e.g., SparseGPT block sizes, damping, Wanda settings) to ensure parity with Thanos choices (anchors: Sec. 5.1; Appendix F.3/F.4).\n  - Action: Report variability (mean±std over multiple runs) for key tables (Table 2–3), at least on one representative model per sparsity pattern (anchors: Table 2–3).- Bolded Title: Expand baseline coverage and practical comparison scope\n  - Action: Include comparisons to Group Fisher (Liu et al., 2021) and AC/DC (Peste et al., 2021) where applicable; if tuning is disallowed, explain rationale and consider short FT variants for parity (anchors: Sec. 3 Related Work; Sec. 5.1).\n  - Action: For semi-structured formats, compare against recent n:m pruning pipelines and report kernel-level performance where possible (anchors: Sec. 4.8; Table 2–3; Appendix D.3–D.4).\n  - Action: Clarify the scope of “state-of-the-art” claims (Sec. 1.3(4); Fig. 1) relative to the expanded baseline set to avoid overclaiming.- Bolded Title: Add end-to-end hardware evaluation and hyperparameter sensitivity\n  - Action: Measure inference throughput/latency on Ampere GPUs for 2:4 pruned models (Sec. 4.8), reporting speedups vs dense and other pruners; include memory footprint changes (anchors: Sec. 4.8; Fig. 9).\n  - Action: Provide end-to-end runtime metrics for structured pruning (Sec. 4.7), not only pruning time, to demonstrate practical benefits (anchors: Fig. 9(c); Sec. 4.7).\n  - Action: Include a sensitivity study for α (e.g., α ∈ {0, 0.05, 0.1, 0.2}) on at least one model, showing trade-offs between columns removed s=⌈pb/(1−α)⌉ and accuracy (anchors: Eq. s = ⌈pb/(1−α)⌉ in Sec. 4.7.1; Tables 2–3 show α=0 vs 0.1).Score\n- Overall (10): 7 — Strong structured/semi-structured results and clear derivations, but notation issues (Eq. (13)/Alg. 2) and limited ablations/baselines temper confidence (Fig. 1; Table 2–3; Sec. 4.7; Appendix G.4.2).\n- Novelty (10): 7 — The multi-weight block update (Sec. 4.1; Eq. (6)–(10)) and global residual mask (Sec. 4.4; Eq. (11); Appendix G.4.1) plus outlier-row mechanism (Sec. 4.7.1) constitute meaningful innovations.\n- Technical Quality (10): 6 — Solid formulation and implementation details (Appendix H), but notational inconsistencies (Eq. (13)/Alg. 2), incomplete complexity alignment (Table 1 vs Eq. (76)), and missing metric ablations reduce rigor.\n- Clarity (10): 6 — Algorithms and notation table aid readability (Alg. 1–2, 8; Appendix B), yet key equations and inversion steps are ambiguous (Sec. 4.7; Eq. (13); Algorithm 1 line 12).\n- Confidence (5): 4 — Extensive tables and appendices with code availability, but formula inconsistencies and limited baseline scope constrain full confidence (Table 2–3; Sec. 1.3(5); Sec. 4.7).",
  "final_review": "Summary\n- The paper proposes Thanos, a block-wise post-training pruning algorithm for large language models that updates multiple weights per row by solving small linear systems, while maintaining an adaptive “global residual” pruning mask. The objective minimizes the change in linear layer outputs (Eq. (1)–(2)), generalizing OBS/OBD single-weight updates to multi-constraint updates (Sec. 4.1; Eq. (6)–(10); Appendix G.1) and providing structured pruning with outlier-row preservation via α (Sec. 4.7.1; Eq. (14)–(15); Alg. 2). The approach also supports n:m semi-structured sparsity (Sec. 4.8; Alg. 8). Experiments on OPT and LLaMA families report perplexity on WikiText-2 and zero-shot LM Harness accuracy, with strong structured and semi-structured results and competitive unstructured outcomes (Fig. 1; Table 2–3; Appendices C–D). Implementation details and pruning-time comparisons (Appendix H; Fig. 9) are provided.Strengths\n- Bolded Title: Rigorous multi-weight update with principled derivation\n  - Evidence: The method extends single-parameter OBS/OBD to multiple simultaneous constraints via Lagrangian multipliers (Sec. 4.1; Eq. (6)–(10); Appendix G.1 with Eq. (51)–(61)). Why it matters: Provides a technically grounded and novel update rule for pruning several weights at once, improving technical soundness beyond column-wise updates.\n  - Evidence: The construction of matrices R, R̂, and vector u (Eq. (7)–(9)) plus the closed-form solution for Δ (Eq. (10)) is explicit. Why it matters: Clear algebra enhances reproducibility and credibility of the multi-constraint approach.\n  - Evidence: Complexity-aware block-wise application aligns with solvable systems per block (Sec. 4.3; Algorithm 1; Appendix G.2). Why it matters: Makes the theoretical multi-weight update computationally feasible for LLM layers.\n- Bolded Title: Adaptive global-residual mask enabling dynamic pruning\n  - Evidence: The mapping ψX(W, r) selects the r smallest |Wij|‖Xj:‖2 values (Eq. (11); Sec. 4.4; Fig. 2; Appendix G.4.1 Fig. 8; Eq. (69)–(71)). Why it matters: A global residual mask avoids rigid per-block or per-row sparsity, potentially improving final accuracy as weights change.\n  - Evidence: The iterative construction of M̂j and extraction of local masks Mj (Appendix G.4.1; Eq. (69)–(71)). Why it matters: Evidence shows careful design to adapt mask to evolving parameter importance (clarity and technical soundness).\n  - Evidence: Visualizations of mask updates (Fig. 2; Appendix Fig. 8, Fig. 104) demonstrate the algorithm’s dynamics. Why it matters: Improves clarity and illustrates innovation in mask management.\n- Bolded Title: Structured pruning with outlier-row preservation\n  - Evidence: Definition of per-row and per-column losses (Eq. (14)–(15); Sec. 4.7.1). Why it matters: A principled criterion to identify rows less suitable for pruning, addressing observed outlier phenomena.\n  - Evidence: Algorithm 2 performs row/column permutations with matrices Q and P (Sec. 4.7; Appendix G.4.4) before pruning s columns (Sec. 4.7; Eq. s=⌈pb/(1−α)⌉, Eq. (13)). Why it matters: Structured pruning is formally described with practical steps that maintain index order (clarity).\n  - Evidence: Empirical results show strong gains of α>0 in structured settings (Table 2–3: compare Thanos α=0 vs α=0.1 across LLaMA-2/3). Why it matters: Demonstrates impact of the outlier-row mechanism on accuracy (experimental rigor and impact).\n- Bolded Title: Extension to semi-structured n:m sparsity with practical hardware relevance\n  - Evidence: Sec. 4.8 and Algorithm 8 detail n:m integration; references to Ampere 2:4 support (Sec. 4.8; NVIDIA 2020; Lin et al., 2023). Why it matters: Aligns with industry acceleration formats, improving practical impact.\n  - Evidence: Results for 4:8 and 2:4 in Table 2–3 and Appendix D.3–D.4 consistently favor Thanos, often over SparseGPT/Wanda (e.g., LLaMA-3 8B, Table 2: Thanos α=0.1 beats baselines at 4:8/2:4). Why it matters: Demonstrates applicability and strong outcomes in semi-structured regimes.\n  - Evidence: Appendix H.3 argues computation benefits due to uniform per-row sparsity (Appendix H.3). Why it matters: Technical efficiency claims for structured formats improve practical viability.\n- Bolded Title: Broad empirical evaluation across model scales and tasks\n  - Evidence: Models include OPT-125M/350M and multiple LLaMA-2/3 sizes up to 70B (Sec. 5.1). Why it matters: Coverage enhances generality and impact.\n  - Evidence: Perplexity results (Table 2) show Thanos competitive/best in many unstructured cases (TinyLlama 1.3B; LLaMA-3 3B–8B) and strong dominance in structured/semi-structured (Fig. 1(b); Table 2). Why it matters: Experimental rigor and demonstrated performance.\n  - Evidence: Zero-shot results (Table 3; Appendices D.1–D.4) show Thanos competitive or best, notably in structured with α=0.1 (e.g., LLaMA-2/3 70B in structured 30%, Table 3; Appendix D.2–D.4). Why it matters: Validates generalization beyond perplexity.\n- Bolded Title: Efficiency analysis and implementation detail transparency\n  - Evidence: Complexity comparison (Table 1) and detailed Thanos complexity (Appendix G.6, Eq. (76)). Why it matters: Technical soundness in performance expectations.\n  - Evidence: Runtime comparisons (Appendix H; Fig. 9 a–c) show Thanos comparable or faster than SparseGPT for structured pruning, especially ≤3B (Appendix H.3). Why it matters: Practical impact and efficiency.\n  - Evidence: Implementation details (Appendix H.1–H.2), including batched solvers, padding strategy (Eq. (77)–(79)), and memory management. Why it matters: Enhances reproducibility and clarifies engineering trade-offs.\n- Bolded Title: Clear algorithmic presentation and supporting materials\n  - Evidence: Algorithm 1 (unstructured), Algorithm 2 (structured), Algorithm 8 (n:m); Notation table (Appendix B/Table 4). Why it matters: Clarity and reproducibility.\n  - Evidence: Generic pruning pipeline (Appendix E.1, Algorithm 3) situates Thanos within block-wise post-training practices. Why it matters: Contextual clarity for readers.\n  - Evidence: Code availability stated (Sec. 1.3(5)). Why it matters: Enables verification and adoption.Weaknesses\n- Bolded Title: Notational inconsistencies and likely errors in structured update formulas\n  - Evidence: Eq. (13) uses “(H_{1:s,1:s}^{−1})^{−1} H_{1:s,:}^{−1}”, which implies double inverse on H_{1:s,1:s} and an inverse of a non-square H_{1:s,:}; Appendix G.4.2 (Eq. (72)) instead shows “(H_{1:s,1:s})^{-1} H_{1:s,:}” without extra inverses. Why it matters: Technical correctness—ambiguity undermines confidence in structured update rule.\n  - Evidence: Algorithm 2 step 10 mirrors Eq. (13) (Sec. 4.7), perpetuating the same notational problem. Why it matters: Implementation risk—readers cannot reproduce a mathematically invalid operation as written.\n  - Evidence: Algorithm 1 step 12 references H^{-1} rows (H_{q:}^{−1}), but line 3 computes H=2XX⊤ and does not specify computing H^{-1} or its factorization; Appendix H discusses Cholesky with SparseGPT but not Thanos inversion steps here. Why it matters: Clarity and technical soundness for unstructured updates.\n  - Evidence: Section 2.2 defines e^q ∈ ℝ^{1×b} and uses the constraint Δ_{k:} e^q + W_{kq} = 0, whereas Appendix E.4 sets e^q ∈ ℝ^{b×1} (and Appendix B/Table 4 also uses a column vector). Why it matters: Dimensional correctness—mismatch can break derivations and implementations.\n  - Evidence: Multiple references to a “metric (3.2)” appear (e.g., Sec. 4.1, last paragraph; Sec. 3.4), while no Eq. 3.2 exists; the intended metric aligns with Eq. (5) in the main text and Eq. (46) in Appendix F.2/F.4. Why it matters: Clarity—misreferenced equations obstruct verification of the pruning metric.\n  - Evidence: In Algorithm 2 step 10, the annotation “xleftarrow{(10)}” cites Eq. (10), the general multi-weight update for unstructured pruning. The structured case has a distinct derivation (Eq. (13); Appendix Eq. (72)). Why it matters: Technical correctness—incorrect equation references impede reproducibility.\n  - Evidence: Near Fig. 3, “s = pb/(1 − α)” is written (Sec. 5.2/5.3 page), while elsewhere structured pruning uses s = ⌈pb/(1 − α)⌉ (Sec. 4.7.1; Algorithm 2 step 2; Appendix G.4.3). Why it matters: Consistency—ceil omission can change the number of columns pruned.\n- Bolded Title: Complexity analysis misalignment and limited empirical efficiency evidence at large scale\n  - Evidence: Table 1 omits sequence length a and assumes c=b, while Appendix G.6 provides T_Thanos = O(ab^2 + b^4/B + (cb^2/B)log(cb) + cbB^2) (Eq. (76)). Why it matters: Consistency—discrepancies complicate understanding of scaling behavior.\n  - Evidence: Fig. 9 shows pruning time primarily for OPT scales, with Thanos faster than SparseGPT for structured pruning; however, structured speedups “up to 3B” are asserted (Appendix H.3) without complete plots for ≥3B across families. Why it matters: Experimental rigor—evidence for very large models is limited.\n  - Evidence: No tabulated memory usage or explicit numeric runtime breakdowns per step (Appendix H.1–H.2 mention strategies but no quantitative data). Why it matters: Practical impact—hard to judge real-world cost-benefit.\n- Bolded Title: Mask metric justification lacks ablation and broader theoretical support\n  - Evidence: Thanos adopts the Wanda metric (Sec. 4.2; Eq. (11)), with Appendix G.3 arguing optimality only for single-weight removal without adjustment (Eq. (65)–(66)). No direct ablation comparing alternative metrics (e.g., OBS metric or Fisher-style criteria) is shown. Why it matters: Novelty/technical quality—using a single-weight optimal metric for multi-weight mask selection may be suboptimal; empirical validation is missing.\n  - Evidence: The claim that “Wanda provides an optimal solution for removing a single weight at a time without adjusting the remaining weights” (Sec. 4.2; Appendix G.3) is correct in its context, but extension to multi-weight selection is asserted rather than proved. Why it matters: Technical soundness—mask choice is central; broader justification would strengthen the method.\n  - Evidence: Appendix A.2 acknowledges Thanos can underperform SparseGPT on some unstructured zero-shot tasks. No ablation ties performance variation to mask choice. Why it matters: Impact—diagnosing when the metric fails would help practitioners.\n- Bolded Title: Experimental methodology details insufficient for strict reproducibility and fairness\n  - Evidence: Calibration data are 128 sequences sampled from C4 (Sec. 5.1), but seeds, sampling strategy, preprocessing, and exact LM Harness configurations are not fully specified. Why it matters: Reproducibility—small calibration sets are sensitive to sampling.\n  - Evidence: Baseline hyperparameters (e.g., block sizes, damping for inverses) are not fully detailed; only Thanos uses B=128/512 defaults (Sec. 5.1; Appendix C). Why it matters: Fairness—baselines may benefit from tuning parity.\n  - Evidence: No variance/CI or repeated runs are reported for perplexity/zero-shot (Table 2–3; Appendix D). Why it matters: Experimental rigor—assessing significance is difficult.\n  - Evidence: Semi-structured results with α=0.1 use lower effective sparsity (p decreases from 0.5 to 0.45; Sec. 5.1) but Tables 2–3 compare these alongside baselines without clearly indicating p differences within those rows. Why it matters: Fairness—non-equivalent sparsity can inflate perceived advantages.\n  - Evidence: Model naming is inconsistent: Section 5.1 lists TinyLlama 1.3B and LLaMA-2 (7B/13B/70B), while Tables 2–3 include “LLaMA-2 1.1B”; Appendix C uses “TinyLlama-1.1B” (Table 5). Why it matters: Clarity—ambiguity about which model is reported hampers verification and replication.\n  - Evidence: Section 5.2 states Thanos “outperforms … TinyLlama 1.3B and all LLaMA-3 models ranging from 3B to 70B,” but Table 2 shows a tie for LLaMA-3 70B unstructured (5.78 vs 5.78), and no comparative TinyLlama 1.3B unstructured table is provided (Appendix C reports TinyLlama-1.1B block-size results only). Why it matters: Accuracy of claims—alignment with presented evidence is necessary.\n- Bolded Title: Baseline coverage is limited relative to related work\n  - Evidence: The paper compares to Magnitude, SparseGPT, and Wanda (Sec. 5.1), but references mention other pruning approaches like Group Fisher (Liu et al., 2021) and AC/DC (Peste et al., 2021) without empirical comparison. Why it matters: Impact—state-of-the-art claims in structured pruning (Sec. 1.3(4); Fig. 1(b)) are stronger with broader baselines.\n  - Evidence: No fine-tuning or recovery is attempted, while some methods in literature rely on brief tuning to reach higher sparsity (Related Work Sec. 3.3; Appendix F.3 notes constraints). Why it matters: Fairness—practical comparisons often consider short FT.\n  - Evidence: Semi-structured evaluation is strong (Tables 2–3; Appendix D.3–D.4), but hardware-optimized baselines beyond Wanda/SparseGPT are not included. Why it matters: Practical impact—comparing to contemporary n:m kernels/pipelines would be informative.\n- Bolded Title: Evaluation breadth and hardware claims need more direct evidence\n  - Evidence: Sec. 4.8 claims speedups on NVIDIA Ampere via 2:4 (citing Lin et al., 2023; NVIDIA 2020), but no measured inference throughput or latency is reported on such hardware for Thanos-pruned models. Why it matters: Impact—users need end-to-end acceleration evidence.\n  - Evidence: Fig. 9 reports pruning time but not inference speed or memory savings; structured pruning could reduce weight size (Sec. 4.7), yet no wall-clock inference gains are shown. Why it matters: Practicality—compression goals include runtime benefits.\n  - Evidence: The paper emphasizes outlier rows α (Sec. 4.7.1) and shows accuracy gains (Table 2–3), but there is no sensitivity analysis across α values beyond 0 and 0.1. Why it matters: Clarity—guidance on α selection is limited.Suggestions for Improvement\n- Bolded Title: Fix structured update notation and clarify inversion steps\n  - Action: Replace Eq. (13) and Algorithm 2 step 10 with the corrected form consistent with Appendix G.4.2, namely W_{:,1:s} ← W_{:,1:s} − W_{:,1:s} (H_{1:s,1:s})^{-1} H_{1:s,:} (Appendix Eq. (72)); ensure all inverses reference square matrices. Provide a derivation in the main text mirroring Appendix G.1/G.4.2 to avoid confusion.\n  - Action: In Algorithm 1, specify whether H^{-1} is computed via explicit inversion or factorization (e.g., Cholesky) and at which lines it is updated; clarify that H in line 3 leads to H^{-1} usages in line 12 (anchors: Algorithm 1 lines 3, 12; Appendix H.1).\n  - Action: Add shapes and dimensions for H, H^{-1}, R, R̂ in Sec. 4.1/4.3 to prevent ambiguity (anchors: Eq. (7)–(10); Algorithm 1).\n  - Action: Make e^q a column vector consistently across the manuscript (Sec. 2.2; Appendix B/Table 4; Appendix E.4), and adjust constraints accordingly to ensure dimensional validity (Δ_{k:} e^q + W_{kq} = 0).\n  - Action: Correct equation cross-references to the pruning metric (“(3.2)”) by pointing to Eq. (5) in the main text or Eq. (46) in the appendix where appropriate (anchors: Sec. 3.4; Sec. 4.1–4.2).\n  - Action: In Algorithm 2 step 10, cite Eq. (13) or Appendix Eq. (72) rather than Eq. (10) to reflect the structured derivation (anchors: Sec. 4.7; Eq. (13); Appendix G.4.2).\n  - Action: Replace “s = pb/(1 − α)” adjacent to Fig. 3 with “s = ⌈pb/(1 − α)⌉” to match Sec. 4.7.1 and Algorithm 2 step 2 (anchors: Fig. 3; Sec. 4.7.1; Algorithm 2).\n- Bolded Title: Align complexity analyses and extend efficiency evidence\n  - Action: Reconcile Table 1 with Appendix G.6 (Eq. (76)) by adding a note on assumptions (c=b, ignoring a) and include the full expression with a to reflect sequence length costs (anchors: Table 1; Appendix G.6).\n  - Action: Expand Fig. 9 with additional points ≥3B across model families and report numeric runtimes and memory footprints per method, ideally in a table, to substantiate claims (anchors: Appendix H Fig. 9; Sec. 4.8/H.3).\n  - Action: Provide stepwise runtime breakdowns (mask selection, linear solves, Hessian updates) for Thanos to show where gains arise (anchors: Algorithm 1; Appendix H.1–H.2).\n- Bolded Title: Validate mask metric choice with ablations and theoretical notes\n  - Action: Add ablations comparing ψX (Eq. (11)) to OBS-based mask (Eq. (4)/(45)) and Fisher-style criteria; measure impact on perplexity/zero-shot across at least one model (anchors: Sec. 4.2; Eq. (11), (4), (45). \n  - Action: Provide a theoretical discussion on why the single-weight optimal metric (Appendix G.3; Eq. (65)–(66)) is suitable for multi-weight mask selection, or state practical reasons for its choice and limitations (anchors: Appendix G.3).\n  - Action: Connect underperformance cases (Appendix A.2) to mask choice via controlled experiments (e.g., LLaMA-3 8B unstructured, Table 3) to guide practitioners (anchors: Table 3; Appendix A.2).\n- Bolded Title: Strengthen reproducibility and fairness details\n  - Action: Specify random seeds, exact calibration sampling (C4 shard identifiers, tokenization, context length distributions), and LM Harness configuration options; publish scripts used (anchors: Sec. 5.1; Appendix D).\n  - Action: Document baseline hyperparameters and any tuning performed (e.g., SparseGPT block sizes, damping, Wanda settings) to ensure parity with Thanos choices (anchors: Sec. 5.1; Appendix F.3/F.4).\n  - Action: Report variability (mean±std over multiple runs) for key tables (Table 2–3), at least on one representative model per sparsity pattern (anchors: Table 2–3).\n  - Action: Ensure semi-structured comparisons are at matched effective sparsity p across methods, or annotate effective sparsity in Tables 2–3 when α=0.1 reduces p to 0.45 (anchors: Sec. 5.1; Tables 2–3).\n  - Action: Standardize model naming across the manuscript and clarify whether “LLaMA-2 1.1B” refers to TinyLlama-1.1B or a distinct model; adjust tables or text accordingly (anchors: Sec. 5.1; Tables 2–3; Appendix C/Table 5).\n  - Action: Qualify performance statements in Sec. 5.2 to reflect ties (e.g., LLaMA-3 70B unstructured in Table 2) and provide comparative TinyLlama 1.3B evidence or remove the claim (anchors: Sec. 5.2; Table 2; Appendix C/Table 5).\n- Bolded Title: Expand baseline coverage and practical comparison scope\n  - Action: Include comparisons to Group Fisher (Liu et al., 2021) and AC/DC (Peste et al., 2021) where applicable; if tuning is disallowed, explain rationale and consider short FT variants for parity (anchors: Sec. 3 Related Work; Sec. 5.1).\n  - Action: For semi-structured formats, compare against recent n:m pruning pipelines and report kernel-level performance where possible (anchors: Sec. 4.8; Table 2–3; Appendix D.3–D.4).\n  - Action: Clarify the scope of “state-of-the-art” claims (Sec. 1.3(4); Fig. 1) relative to the expanded baseline set to avoid overclaiming.\n- Bolded Title: Add end-to-end hardware evaluation and hyperparameter sensitivity\n  - Action: Measure inference throughput/latency on Ampere GPUs for 2:4 pruned models (Sec. 4.8), reporting speedups vs dense and other pruners; include memory footprint changes (anchors: Sec. 4.8; Fig. 9).\n  - Action: Provide end-to-end runtime metrics for structured pruning (Sec. 4.7), not only pruning time, to demonstrate practical benefits (anchors: Fig. 9(c); Sec. 4.7).\n  - Action: Include a sensitivity study for α (e.g., α ∈ {0, 0.05, 0.1, 0.2}) on at least one model, showing trade-offs between columns removed s=⌈pb/(1−α)⌉ and accuracy (anchors: Eq. s = ⌈pb/(1−α)⌉ in Sec. 4.7.1; Tables 2–3 show α=0 vs 0.1).Score\n- Overall (10): 7 — Strong structured/semi-structured results and clear derivations, but notation issues (Eq. (13)/Alg. 2; Sec. 2.2 e^q; misreferenced metric) and limited ablations/baselines temper confidence (Fig. 1; Table 2–3; Sec. 4.7; Appendix G.4.2; Appendix B/Table 4).\n- Novelty (10): 7 — The multi-weight block update (Sec. 4.1; Eq. (6)–(10)) and global residual mask (Sec. 4.4; Eq. (11); Appendix G.4.1) plus outlier-row mechanism (Sec. 4.7.1) constitute meaningful innovations.\n- Technical Quality (10): 6 — Solid formulation and implementation details (Appendix H), but notational inconsistencies (Eq. (13)/Alg. 2; Sec. 2.2), incomplete complexity alignment (Table 1 vs Eq. (76)), and missing metric ablations reduce rigor.\n- Clarity (10): 6 — Algorithms and notation table aid readability (Alg. 1–2, 8; Appendix B), yet key equations and cross-references are ambiguous (Sec. 4.7; Eq. (13); Algorithm 2 step 10; “(3.2)” references; Fig. 3 s-formula).\n- Confidence (5): 4 — Extensive tables and appendices with code availability, but formula inconsistencies and limited baseline scope constrain full confidence (Table 2–3; Sec. 1.3(5); Sec. 4.7; Appendix G.4.2).",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 7,
        "novelty": 7,
        "technical_quality": 6,
        "clarity": 6,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 7,
        "novelty": 7,
        "technical_quality": 6,
        "clarity": 6,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "Summary\n- The paper proposes Thanos, a block-wise post-training pruning algorithm for large language models that updates multiple weights per row by solving small linear systems, while maintaining an adaptive “global residual” pruning mask. The objective minimizes the change in linear layer outputs (Eq. (1)–(2)), generalizing OBS/OBD single-weight updates to multi-constraint updates (Sec. 4.1; Eq. (6)–(10); Appendix G.1) and providing structured pruning with outlier-row preservation via α (Sec. 4.7.1; Eq. (14)–(15); Alg. 2). The approach also supports n:m semi-structured sparsity (Sec. 4.8; Alg. 8). Experiments on OPT and LLaMA families report perplexity on WikiText-2 and zero-shot LM Harness accuracy, with strong structured and semi-structured results and competitive unstructured outcomes (Fig. 1; Table 2–3; Appendices C–D). Implementation details and pruning-time comparisons (Appendix H; Fig. 9) are provided.Strengths\n- Bolded Title: Rigorous multi-weight update with principled derivation\n  - Evidence: The method extends single-parameter OBS/OBD to multiple simultaneous constraints via Lagrangian multipliers (Sec. 4.1; Eq. (6)–(10); Appendix G.1 with Eq. (51)–(61)). Why it matters: Provides a technically grounded and novel update rule for pruning several weights at once, improving technical soundness beyond column-wise updates.\n  - Evidence: The construction of matrices R, R̂, and vector u (Eq. (7)–(9)) plus the closed-form solution for Δ (Eq. (10)) is explicit. Why it matters: Clear algebra enhances reproducibility and credibility of the multi-constraint approach.\n  - Evidence: Complexity-aware block-wise application aligns with solvable systems per block (Sec. 4.3; Algorithm 1; Appendix G.2). Why it matters: Makes the theoretical multi-weight update computationally feasible for LLM layers.\n- Bolded Title: Adaptive global-residual mask enabling dynamic pruning\n  - Evidence: The mapping ψX(W, r) selects the r smallest |Wij|‖Xj:‖2 values (Eq. (11); Sec. 4.4; Fig. 2; Appendix G.4.1 Fig. 8; Eq. (69)–(71)). Why it matters: A global residual mask avoids rigid per-block or per-row sparsity, potentially improving final accuracy as weights change.\n  - Evidence: The iterative construction of M̂j and extraction of local masks Mj (Appendix G.4.1; Eq. (69)–(71)). Why it matters: Evidence shows careful design to adapt mask to evolving parameter importance (clarity and technical soundness).\n  - Evidence: Visualizations of mask updates (Fig. 2; Appendix Fig. 8, Fig. 104) demonstrate the algorithm’s dynamics. Why it matters: Improves clarity and illustrates innovation in mask management.\n- Bolded Title: Structured pruning with outlier-row preservation\n  - Evidence: Definition of per-row and per-column losses (Eq. (14)–(15); Sec. 4.7.1). Why it matters: A principled criterion to identify rows less suitable for pruning, addressing observed outlier phenomena.\n  - Evidence: Algorithm 2 performs row/column permutations with matrices Q and P (Sec. 4.7; Appendix G.4.4) before pruning s columns (Sec. 4.7; Eq. s=⌈pb/(1−α)⌉, Eq. (13)). Why it matters: Structured pruning is formally described with practical steps that maintain index order (clarity).\n  - Evidence: Empirical results show strong gains of α>0 in structured settings (Table 2–3: compare Thanos α=0 vs α=0.1 across LLaMA-2/3). Why it matters: Demonstrates impact of the outlier-row mechanism on accuracy (experimental rigor and impact).\n- Bolded Title: Extension to semi-structured n:m sparsity with practical hardware relevance\n  - Evidence: Sec. 4.8 and Algorithm 8 detail n:m integration; references to Ampere 2:4 support (Sec. 4.8; NVIDIA 2020; Lin et al., 2023). Why it matters: Aligns with industry acceleration formats, improving practical impact.\n  - Evidence: Results for 4:8 and 2:4 in Table 2–3 and Appendix D.3–D.4 consistently favor Thanos, often over SparseGPT/Wanda (e.g., LLaMA-3 8B, Table 2: Thanos α=0.1 beats baselines at 4:8/2:4). Why it matters: Demonstrates applicability and strong outcomes in semi-structured regimes.\n  - Evidence: Appendix H.3 argues computation benefits due to uniform per-row sparsity (Appendix H.3). Why it matters: Technical efficiency claims for structured formats improve practical viability.\n- Bolded Title: Broad empirical evaluation across model scales and tasks\n  - Evidence: Models include OPT-125M/350M and multiple LLaMA-2/3 sizes up to 70B (Sec. 5.1). Why it matters: Coverage enhances generality and impact.\n  - Evidence: Perplexity results (Table 2) show Thanos competitive/best in many unstructured cases (TinyLlama 1.3B; LLaMA-3 3B–8B) and strong dominance in structured/semi-structured (Fig. 1(b); Table 2). Why it matters: Experimental rigor and demonstrated performance.\n  - Evidence: Zero-shot results (Table 3; Appendices D.1–D.4) show Thanos competitive or best, notably in structured with α=0.1 (e.g., LLaMA-2/3 70B in structured 30%, Table 3; Appendix D.2–D.4). Why it matters: Validates generalization beyond perplexity.\n- Bolded Title: Efficiency analysis and implementation detail transparency\n  - Evidence: Complexity comparison (Table 1) and detailed Thanos complexity (Appendix G.6, Eq. (76)). Why it matters: Technical soundness in performance expectations.\n  - Evidence: Runtime comparisons (Appendix H; Fig. 9 a–c) show Thanos comparable or faster than SparseGPT for structured pruning, especially ≤3B (Appendix H.3). Why it matters: Practical impact and efficiency.\n  - Evidence: Implementation details (Appendix H.1–H.2), including batched solvers, padding strategy (Eq. (77)–(79)), and memory management. Why it matters: Enhances reproducibility and clarifies engineering trade-offs.\n- Bolded Title: Clear algorithmic presentation and supporting materials\n  - Evidence: Algorithm 1 (unstructured), Algorithm 2 (structured), Algorithm 8 (n:m); Notation table (Appendix B/Table 4). Why it matters: Clarity and reproducibility.\n  - Evidence: Generic pruning pipeline (Appendix E.1, Algorithm 3) situates Thanos within block-wise post-training practices. Why it matters: Contextual clarity for readers.\n  - Evidence: Code availability stated (Sec. 1.3(5)). Why it matters: Enables verification and adoption.Weaknesses\n- Bolded Title: Notational inconsistencies and likely errors in structured update formulas\n  - Evidence: Eq. (13) uses “(H_{1:s,1:s}^{−1})^{−1} H_{1:s,:}^{−1}”, which implies double inverse on H_{1:s,1:s} and an inverse of a non-square H_{1:s,:}; Appendix G.4.2 (Eq. (72)) instead shows “(H_{1:s,1:s})^{-1} H_{1:s,:}” without extra inverses. Why it matters: Technical correctness—ambiguity undermines confidence in structured update rule.\n  - Evidence: Algorithm 2 step 10 mirrors Eq. (13) (Sec. 4.7), perpetuating the same notational problem. Why it matters: Implementation risk—readers cannot reproduce a mathematically invalid operation as written.\n  - Evidence: Algorithm 1 step 12 references H^{-1} rows (H_{q:}^{−1}), but line 3 computes H=2XX⊤ and does not specify computing H^{-1} or its factorization; Appendix H discusses Cholesky with SparseGPT but not Thanos inversion steps here. Why it matters: Clarity and technical soundness for unstructured updates.\n  - Evidence: Section 2.2 defines e^q ∈ ℝ^{1×b} and uses the constraint Δ_{k:} e^q + W_{kq} = 0, whereas Appendix E.4 sets e^q ∈ ℝ^{b×1} (and Appendix B/Table 4 also uses a column vector). Why it matters: Dimensional correctness—mismatch can break derivations and implementations.\n  - Evidence: Multiple references to a “metric (3.2)” appear (e.g., Sec. 4.1, last paragraph; Sec. 3.4), while no Eq. 3.2 exists; the intended metric aligns with Eq. (5) in the main text and Eq. (46) in Appendix F.2/F.4. Why it matters: Clarity—misreferenced equations obstruct verification of the pruning metric.\n  - Evidence: In Algorithm 2 step 10, the annotation “xleftarrow{(10)}” cites Eq. (10), the general multi-weight update for unstructured pruning. The structured case has a distinct derivation (Eq. (13); Appendix Eq. (72)). Why it matters: Technical correctness—incorrect equation references impede reproducibility.\n  - Evidence: Near Fig. 3, “s = pb/(1 − α)” is written (Sec. 5.2/5.3 page), while elsewhere structured pruning uses s = ⌈pb/(1 − α)⌉ (Sec. 4.7.1; Algorithm 2 step 2; Appendix G.4.3). Why it matters: Consistency—ceil omission can change the number of columns pruned.\n- Bolded Title: Complexity analysis misalignment and limited empirical efficiency evidence at large scale\n  - Evidence: Table 1 omits sequence length a and assumes c=b, while Appendix G.6 provides T_Thanos = O(ab^2 + b^4/B + (cb^2/B)log(cb) + cbB^2) (Eq. (76)). Why it matters: Consistency—discrepancies complicate understanding of scaling behavior.\n  - Evidence: Fig. 9 shows pruning time primarily for OPT scales, with Thanos faster than SparseGPT for structured pruning; however, structured speedups “up to 3B” are asserted (Appendix H.3) without complete plots for ≥3B across families. Why it matters: Experimental rigor—evidence for very large models is limited.\n  - Evidence: No tabulated memory usage or explicit numeric runtime breakdowns per step (Appendix H.1–H.2 mention strategies but no quantitative data). Why it matters: Practical impact—hard to judge real-world cost-benefit.\n- Bolded Title: Mask metric justification lacks ablation and broader theoretical support\n  - Evidence: Thanos adopts the Wanda metric (Sec. 4.2; Eq. (11)), with Appendix G.3 arguing optimality only for single-weight removal without adjustment (Eq. (65)–(66)). No direct ablation comparing alternative metrics (e.g., OBS metric or Fisher-style criteria) is shown. Why it matters: Novelty/technical quality—using a single-weight optimal metric for multi-weight mask selection may be suboptimal; empirical validation is missing.\n  - Evidence: The claim that “Wanda provides an optimal solution for removing a single weight at a time without adjusting the remaining weights” (Sec. 4.2; Appendix G.3) is correct in its context, but extension to multi-weight selection is asserted rather than proved. Why it matters: Technical soundness—mask choice is central; broader justification would strengthen the method.\n  - Evidence: Appendix A.2 acknowledges Thanos can underperform SparseGPT on some unstructured zero-shot tasks. No ablation ties performance variation to mask choice. Why it matters: Impact—diagnosing when the metric fails would help practitioners.\n- Bolded Title: Experimental methodology details insufficient for strict reproducibility and fairness\n  - Evidence: Calibration data are 128 sequences sampled from C4 (Sec. 5.1), but seeds, sampling strategy, preprocessing, and exact LM Harness configurations are not fully specified. Why it matters: Reproducibility—small calibration sets are sensitive to sampling.\n  - Evidence: Baseline hyperparameters (e.g., block sizes, damping for inverses) are not fully detailed; only Thanos uses B=128/512 defaults (Sec. 5.1; Appendix C). Why it matters: Fairness—baselines may benefit from tuning parity.\n  - Evidence: No variance/CI or repeated runs are reported for perplexity/zero-shot (Table 2–3; Appendix D). Why it matters: Experimental rigor—assessing significance is difficult.\n  - Evidence: Semi-structured results with α=0.1 use lower effective sparsity (p decreases from 0.5 to 0.45; Sec. 5.1) but Tables 2–3 compare these alongside baselines without clearly indicating p differences within those rows. Why it matters: Fairness—non-equivalent sparsity can inflate perceived advantages.\n  - Evidence: Model naming is inconsistent: Section 5.1 lists TinyLlama 1.3B and LLaMA-2 (7B/13B/70B), while Tables 2–3 include “LLaMA-2 1.1B”; Appendix C uses “TinyLlama-1.1B” (Table 5). Why it matters: Clarity—ambiguity about which model is reported hampers verification and replication.\n  - Evidence: Section 5.2 states Thanos “outperforms … TinyLlama 1.3B and all LLaMA-3 models ranging from 3B to 70B,” but Table 2 shows a tie for LLaMA-3 70B unstructured (5.78 vs 5.78), and no comparative TinyLlama 1.3B unstructured table is provided (Appendix C reports TinyLlama-1.1B block-size results only). Why it matters: Accuracy of claims—alignment with presented evidence is necessary.\n- Bolded Title: Baseline coverage is limited relative to related work\n  - Evidence: The paper compares to Magnitude, SparseGPT, and Wanda (Sec. 5.1), but references mention other pruning approaches like Group Fisher (Liu et al., 2021) and AC/DC (Peste et al., 2021) without empirical comparison. Why it matters: Impact—state-of-the-art claims in structured pruning (Sec. 1.3(4); Fig. 1(b)) are stronger with broader baselines.\n  - Evidence: No fine-tuning or recovery is attempted, while some methods in literature rely on brief tuning to reach higher sparsity (Related Work Sec. 3.3; Appendix F.3 notes constraints). Why it matters: Fairness—practical comparisons often consider short FT.\n  - Evidence: Semi-structured evaluation is strong (Tables 2–3; Appendix D.3–D.4), but hardware-optimized baselines beyond Wanda/SparseGPT are not included. Why it matters: Practical impact—comparing to contemporary n:m kernels/pipelines would be informative.\n- Bolded Title: Evaluation breadth and hardware claims need more direct evidence\n  - Evidence: Sec. 4.8 claims speedups on NVIDIA Ampere via 2:4 (citing Lin et al., 2023; NVIDIA 2020), but no measured inference throughput or latency is reported on such hardware for Thanos-pruned models. Why it matters: Impact—users need end-to-end acceleration evidence.\n  - Evidence: Fig. 9 reports pruning time but not inference speed or memory savings; structured pruning could reduce weight size (Sec. 4.7), yet no wall-clock inference gains are shown. Why it matters: Practicality—compression goals include runtime benefits.\n  - Evidence: The paper emphasizes outlier rows α (Sec. 4.7.1) and shows accuracy gains (Table 2–3), but there is no sensitivity analysis across α values beyond 0 and 0.1. Why it matters: Clarity—guidance on α selection is limited.Suggestions for Improvement\n- Bolded Title: Fix structured update notation and clarify inversion steps\n  - Action: Replace Eq. (13) and Algorithm 2 step 10 with the corrected form consistent with Appendix G.4.2, namely W_{:,1:s} ← W_{:,1:s} − W_{:,1:s} (H_{1:s,1:s})^{-1} H_{1:s,:} (Appendix Eq. (72)); ensure all inverses reference square matrices. Provide a derivation in the main text mirroring Appendix G.1/G.4.2 to avoid confusion.\n  - Action: In Algorithm 1, specify whether H^{-1} is computed via explicit inversion or factorization (e.g., Cholesky) and at which lines it is updated; clarify that H in line 3 leads to H^{-1} usages in line 12 (anchors: Algorithm 1 lines 3, 12; Appendix H.1).\n  - Action: Add shapes and dimensions for H, H^{-1}, R, R̂ in Sec. 4.1/4.3 to prevent ambiguity (anchors: Eq. (7)–(10); Algorithm 1).\n  - Action: Make e^q a column vector consistently across the manuscript (Sec. 2.2; Appendix B/Table 4; Appendix E.4), and adjust constraints accordingly to ensure dimensional validity (Δ_{k:} e^q + W_{kq} = 0).\n  - Action: Correct equation cross-references to the pruning metric (“(3.2)”) by pointing to Eq. (5) in the main text or Eq. (46) in the appendix where appropriate (anchors: Sec. 3.4; Sec. 4.1–4.2).\n  - Action: In Algorithm 2 step 10, cite Eq. (13) or Appendix Eq. (72) rather than Eq. (10) to reflect the structured derivation (anchors: Sec. 4.7; Eq. (13); Appendix G.4.2).\n  - Action: Replace “s = pb/(1 − α)” adjacent to Fig. 3 with “s = ⌈pb/(1 − α)⌉” to match Sec. 4.7.1 and Algorithm 2 step 2 (anchors: Fig. 3; Sec. 4.7.1; Algorithm 2).\n- Bolded Title: Align complexity analyses and extend efficiency evidence\n  - Action: Reconcile Table 1 with Appendix G.6 (Eq. (76)) by adding a note on assumptions (c=b, ignoring a) and include the full expression with a to reflect sequence length costs (anchors: Table 1; Appendix G.6).\n  - Action: Expand Fig. 9 with additional points ≥3B across model families and report numeric runtimes and memory footprints per method, ideally in a table, to substantiate claims (anchors: Appendix H Fig. 9; Sec. 4.8/H.3).\n  - Action: Provide stepwise runtime breakdowns (mask selection, linear solves, Hessian updates) for Thanos to show where gains arise (anchors: Algorithm 1; Appendix H.1–H.2).\n- Bolded Title: Validate mask metric choice with ablations and theoretical notes\n  - Action: Add ablations comparing ψX (Eq. (11)) to OBS-based mask (Eq. (4)/(45)) and Fisher-style criteria; measure impact on perplexity/zero-shot across at least one model (anchors: Sec. 4.2; Eq. (11), (4), (45). \n  - Action: Provide a theoretical discussion on why the single-weight optimal metric (Appendix G.3; Eq. (65)–(66)) is suitable for multi-weight mask selection, or state practical reasons for its choice and limitations (anchors: Appendix G.3).\n  - Action: Connect underperformance cases (Appendix A.2) to mask choice via controlled experiments (e.g., LLaMA-3 8B unstructured, Table 3) to guide practitioners (anchors: Table 3; Appendix A.2).\n- Bolded Title: Strengthen reproducibility and fairness details\n  - Action: Specify random seeds, exact calibration sampling (C4 shard identifiers, tokenization, context length distributions), and LM Harness configuration options; publish scripts used (anchors: Sec. 5.1; Appendix D).\n  - Action: Document baseline hyperparameters and any tuning performed (e.g., SparseGPT block sizes, damping, Wanda settings) to ensure parity with Thanos choices (anchors: Sec. 5.1; Appendix F.3/F.4).\n  - Action: Report variability (mean±std over multiple runs) for key tables (Table 2–3), at least on one representative model per sparsity pattern (anchors: Table 2–3).\n  - Action: Ensure semi-structured comparisons are at matched effective sparsity p across methods, or annotate effective sparsity in Tables 2–3 when α=0.1 reduces p to 0.45 (anchors: Sec. 5.1; Tables 2–3).\n  - Action: Standardize model naming across the manuscript and clarify whether “LLaMA-2 1.1B” refers to TinyLlama-1.1B or a distinct model; adjust tables or text accordingly (anchors: Sec. 5.1; Tables 2–3; Appendix C/Table 5).\n  - Action: Qualify performance statements in Sec. 5.2 to reflect ties (e.g., LLaMA-3 70B unstructured in Table 2) and provide comparative TinyLlama 1.3B evidence or remove the claim (anchors: Sec. 5.2; Table 2; Appendix C/Table 5).\n- Bolded Title: Expand baseline coverage and practical comparison scope\n  - Action: Include comparisons to Group Fisher (Liu et al., 2021) and AC/DC (Peste et al., 2021) where applicable; if tuning is disallowed, explain rationale and consider short FT variants for parity (anchors: Sec. 3 Related Work; Sec. 5.1).\n  - Action: For semi-structured formats, compare against recent n:m pruning pipelines and report kernel-level performance where possible (anchors: Sec. 4.8; Table 2–3; Appendix D.3–D.4).\n  - Action: Clarify the scope of “state-of-the-art” claims (Sec. 1.3(4); Fig. 1) relative to the expanded baseline set to avoid overclaiming.\n- Bolded Title: Add end-to-end hardware evaluation and hyperparameter sensitivity\n  - Action: Measure inference throughput/latency on Ampere GPUs for 2:4 pruned models (Sec. 4.8), reporting speedups vs dense and other pruners; include memory footprint changes (anchors: Sec. 4.8; Fig. 9).\n  - Action: Provide end-to-end runtime metrics for structured pruning (Sec. 4.7), not only pruning time, to demonstrate practical benefits (anchors: Fig. 9(c); Sec. 4.7).\n  - Action: Include a sensitivity study for α (e.g., α ∈ {0, 0.05, 0.1, 0.2}) on at least one model, showing trade-offs between columns removed s=⌈pb/(1−α)⌉ and accuracy (anchors: Eq. s = ⌈pb/(1−α)⌉ in Sec. 4.7.1; Tables 2–3 show α=0 vs 0.1).Score\n- Overall (10): 7 — Strong structured/semi-structured results and clear derivations, but notation issues (Eq. (13)/Alg. 2; Sec. 2.2 e^q; misreferenced metric) and limited ablations/baselines temper confidence (Fig. 1; Table 2–3; Sec. 4.7; Appendix G.4.2; Appendix B/Table 4).\n- Novelty (10): 7 — The multi-weight block update (Sec. 4.1; Eq. (6)–(10)) and global residual mask (Sec. 4.4; Eq. (11); Appendix G.4.1) plus outlier-row mechanism (Sec. 4.7.1) constitute meaningful innovations.\n- Technical Quality (10): 6 — Solid formulation and implementation details (Appendix H), but notational inconsistencies (Eq. (13)/Alg. 2; Sec. 2.2), incomplete complexity alignment (Table 1 vs Eq. (76)), and missing metric ablations reduce rigor.\n- Clarity (10): 6 — Algorithms and notation table aid readability (Alg. 1–2, 8; Appendix B), yet key equations and cross-references are ambiguous (Sec. 4.7; Eq. (13); Algorithm 2 step 10; “(3.2)” references; Fig. 3 s-formula).\n- Confidence (5): 4 — Extensive tables and appendices with code availability, but formula inconsistencies and limited baseline scope constrain full confidence (Table 2–3; Sec. 1.3(5); Sec. 4.7; Appendix G.4.2)."
}