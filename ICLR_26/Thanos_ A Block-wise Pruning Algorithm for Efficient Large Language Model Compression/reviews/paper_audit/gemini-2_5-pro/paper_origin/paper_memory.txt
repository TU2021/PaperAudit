# Global Summary
This paper introduces Thanos, a novel post-training, block-wise weight pruning algorithm for large language models (LLMs). The core problem is to reduce the memory and computational costs of LLMs while preserving accuracy, without retraining. Thanos prunes weights in blocks, updating multiple weights simultaneously to account for their cumulative impact, which contrasts with methods that prune weights one by one. It uses an adaptive masking strategy that is not restricted to local block or row sparsity. The method is extended to support hardware-friendly structured (column-wise) and semi-structured (n:m) sparsity. A key contribution for structured pruning is the identification and preservation of "outlier rows" to maintain model quality.

Thanos was evaluated on OPT and LLaMA model families (up to 70B parameters) against Magnitude, Wanda, and SparseGPT baselines. Performance was measured by WikiText-2 perplexity and average zero-shot accuracy on seven tasks. The paper claims that Thanos achieves state-of-the-art results in structured and semi-structured pruning, significantly outperforming baselines. For example, on LLaMA-3 8B with 30% structured sparsity, Thanos (with α=0.1) achieves a perplexity of 36.61, compared to 82.19 for SparseGPT. In unstructured pruning, its performance is competitive with SparseGPT and Wanda. A stated limitation is its higher computational complexity in unstructured settings compared to simpler methods.

# Abstract
The paper introduces Thanos, a weight-pruning algorithm for compressing large language models (LLMs). Its goal is to reduce memory footprint and improve computational efficiency by removing redundant weights without sacrificing accuracy. Thanos employs a block-wise pruning strategy with adaptive masks that adjust to weight importance. This allows for flexible unstructured sparsity as well as structured formats like n:m sparsity, which are optimized for hardware acceleration. The authors claim that experimental results show Thanos achieves state-of-the-art performance in structured pruning and surpasses existing methods in unstructured pruning, making it a practical solution for deploying LLMs in resource-constrained environments.

# Introduction
- The paper addresses the high memory and computational costs of deploying Large Language Models (LLMs), proposing post-training pruning as a solution alongside quantization.
- The work focuses on pruning weights in the linear layers of LLMs, as they constitute the majority of model parameters.
- The method operates in a post-training setting, using a pre-trained model and a small calibration dataset without requiring retraining.
- The pruning paradigm is sequential and block-wise: for each block of the LLM, inputs to linear layers are computed, each layer is pruned independently, and then a forward pass through the pruned block generates input for the next block.
- **Contributions:**
    1.  A novel block-wise pruning algorithm, Thanos, that optimally updates a block of weights.
    2.  A new structured pruning technique that detects and preserves "outlier rows" to improve performance.
    3.  An extension of Thanos to semi-structured n:m sparsity (e.g., 2:4 for NVIDIA Ampere GPUs).
    4.  Experimental evaluation showing Thanos significantly outperforms existing methods in structured pruning.
    5.  An open-source implementation of the algorithm.

# Experiments
- **Models and Datasets:** The evaluation uses OPT and LLaMA family models (TinyLlama 1.3B, LLaMA-2 7B/13B/70B, LLaMA-3 1B/3B/8B/70B).
- **Evaluation Metrics:** Performance is measured by perplexity on the WikiText-2 validation set and average zero-shot accuracy across seven tasks from the EleutherAI LM Harness (WinoGrande, OBQA, BoolQ, PiQA, HellaSwag, ARC-easy, ARC-challenge).
- **Baselines:** Thanos is compared to Magnitude pruning, SparseGPT, and Wanda.
- **Calibration Data:** All data-aware methods (Thanos, SparseGPT, Wanda) use the same calibration set of 128 sequences from the C4 training set.
- **Thanos Hyperparameters:**
    - Block size `B = 128` for unstructured sparsity.
    - Block size `B = 512` for structured (4:8, 2:4) sparsity.
    - Outlier row fraction `α = 0.1` (10% of rows) is used by default for structured/semi-structured pruning.
- **Perplexity Results (Table 2):**
    - In unstructured 50% sparsity, Thanos is competitive, outperforming baselines on TinyLlama 1.3B and LLaMA-3 models. For LLaMA-3 8B, Thanos achieves 9.32 PPL, compared to SparseGPT (9.36) and Wanda (9.83).
    - In structured pruning, Thanos shows significant improvements. For LLaMA-3 8B at 30% structured sparsity, Thanos (α=0.1) achieves 36.61 PPL, while SparseGPT gets 82.19 and Wanda gets 167.57.
    - For 2:4 sparsity on LLaMA-3 8B, Thanos (α=0.1) achieves 13.92 PPL, compared to SparseGPT (16.17) and Wanda (24.15).
- **Zero-Shot Results (Table 3):**
    - In unstructured 50% sparsity, Thanos is competitive, achieving the best results for LLaMA-2 70B and LLaMA-3 1B/3B models.
    - In structured and semi-structured sparsity, Thanos consistently outperforms other methods. For LLaMA-2 70B with 4:8 sparsity, Thanos (α=0.1) achieves 66.58% average accuracy, while SparseGPT gets 65.63%.
- **Block Size Experiments (Table 5, Appendix):** For unstructured sparsity on TinyLlama-1.1B, perplexity is stable across block sizes `B` from 8 to 4096. For structured sparsity (4:8 and 2:4), larger block sizes lead to better performance (lower perplexity).

# Preliminaries
This section states that all key notation is summarized in a table in Appendix B.

# Method
- **Problem Formulation:** The objective is to minimize the layer-wise reconstruction error `||(Ẇ − W) X||_F^2`, where `W` is the original weight matrix, `Ẇ` is the pruned matrix, and `X` is the calibration input. This is an NP-hard discrete optimization problem.
- **Updating Multiple Weights:** Unlike methods that prune one weight at a time, Thanos solves for the optimal update `widehat{Δ}_{k:}` for a row `k` when `s` weights are pruned simultaneously. The solution is given by `widehat{Δ}_{k:} = -u * widehat{R}^{-1} * R`. A brute-force search for the best `s` weights to prune is combinatorially explosive, so a pruning metric is used to pre-select them.
- **Pruning Metric:** Thanos uses the same metric as Wanda: `|W_{ij}| * ||X_{j:}||_2`, which is derived from the Optimal Brain Damage (OBD) method.
- **Block-wise Pruning:** The weight matrix `W` is processed in blocks of `B` columns. For each block, a local mask is determined, and the optimal update rule is applied to the weights within the block and all subsequent weights to its right.
- **Unstructured Sparsity (Algorithm 1):** A *global residual mask* is computed for all remaining unpruned weights to determine which weights have the lowest saliency. A *local mask* for the current block is then extracted from this global mask. This allows for a flexible, non-uniform sparsity distribution across blocks.
- **Complexity (Table 1):** The complexity of Thanos for unstructured pruning is `O(c^4/B + c^2*B^2)`, and for structured pruning is `O(c^3)`. This is compared to `O(c^3)` for SparseGPT and `O(c^2*log(c))` for Wanda.
- **Structured Sparsity (Algorithm 2):** Entire columns are pruned. This simplifies the update rule.
    - **Outlier Rows:** A novel concept where a fraction `α` of rows are identified as "outliers" and are preserved (not pruned). The importance of a row `i` is measured by `h_i = ||W_i X||_2^2`. To maintain the target sparsity `p`, the number of columns to prune is increased to `s = ⌈pb/(1−α)⌉`.
    - **Permutations:** Row and column permutations are used to group outlier rows and columns-to-be-pruned, simplifying the operation. Inverse permutations restore the original order.
- **Semi-structured n:m Sparsity:** The algorithm is extended to support n:m sparsity patterns. This setting is computationally efficient as each row has a uniform number of weights to remove, allowing for streamlined batched matrix operations.

# Related Work
- **Magnitude Pruning:** A simple, data-free method that removes weights with the smallest absolute values. It is a common baseline but less effective for LLMs compared to data-aware methods.
- **OBD and OBS:** Classical methods for pruning. Optimal Brain Surgeon (OBS) provides an optimal update rule for removing a single weight, considering the full Hessian. Optimal Brain Damage (OBD) assumes a diagonal Hessian, simplifying the pruning metric to `(|W_{kq}| * ||X_{q:}||_2)^2`.
- **SparseGPT:** A data-aware method that prunes weights sequentially (column by column). It uses the OBS update rule to adjust remaining weights to compensate for the pruned ones.
- **Wanda:** A simpler data-aware method that uses the OBD metric (`|W_{ij}| * ||X_{j:}||_2`) to select weights for pruning but does not perform any updates to the remaining weights.

# Conclusion
This section directs the reader to Appendix A for concluding comments and limitations.

# References
This section contains the bibliography for the paper.

# Appendix
- **A. Conclusion & Limitations:**
    - **Conclusion:** Re-states that Thanos is a novel block-wise pruning algorithm with an adaptive mask, outperforming other methods in structured/semi-structured sparsity and being competitive in unstructured sparsity. Its performance in structured sparsity is attributed to updating multiple weights at once.
    - **Limitations:** Thanos has high computational complexity in unstructured sparsity. SparseGPT can achieve better zero-shot results in some unstructured settings. Wanda can be more robust at very high unstructured sparsity levels for LLaMA models.
- **B. Table of Frequently Used Notation:** Provides a comprehensive table defining all mathematical symbols used in the paper.
- **C. Experiments with different block size B:** Table 5 shows results for TinyLlama-1.1B. For unstructured 50% sparsity, perplexity is stable around 11.0 for `B` from 8 to 4096. For structured 4:8 and 2:4 sparsity, perplexity improves with larger `B` (e.g., for 4:8, PPL drops from 14.44 at `B=8` to 13.85 at `B=4096`).
- **D. Full Zero-Shot evaluation on different LLM's tasks:** Contains detailed tables (Tables 6-17) of zero-shot results for all models, methods, and sparsity configurations.
- **E. Formulation of the problem:** Provides a more detailed mathematical derivation of the pruning objective function and the constrained optimization problem.
- **F. Background and related work:** Offers extended explanations of Magnitude pruning, OBD/OBS, SparseGPT, and Wanda, including algorithms and complexity analyses.
- **G. Thanos pruning:** Contains detailed derivations for updating multiple weights, the block-wise procedure, the pruning metric, and mask selection strategies for unstructured, structured, and semi-structured sparsity. It also explains the row/column permutations and presents full algorithms (e.g., Algorithm 7 for structured, Algorithm 8 for semi-structured).
- **H. Implementation details:**
    - Discusses the use of efficient batched operations (NumPy/PyTorch broadcasting) instead of loops.
    - To handle non-uniform numbers of pruned weights per row in unstructured sparsity, padding was used to create fixed-size matrices for batched linear solvers.
    - To manage GPU memory limitations, large layers are processed in vertical blocks.
    - Figure 9 shows pruning time comparisons. Thanos is faster than SparseGPT in structured sparsity across all tested model sizes. In unstructured and semi-structured sparsity, Thanos is faster for models up to ~3B parameters.