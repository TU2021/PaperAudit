OpenReview.net
Search OpenReview...
Login
back arrowGo to ICLR 2026 Conference homepage
Thanos: A Block-wise Pruning Algorithm for Efficient Large Language Model Compression
Download PDF
ICLR 2026 Conference Submission25458 Authors
20 Sept 2025 (modified: 24 Dec 2025)
ICLR 2026 Conference Submission
Everyone
Revisions
BibTeX
CC BY 4.0
Keywords: LLM Compression, Pruning, Wanda, SparseGPT, Deep Learning, AI
TL;DR: We developed a novel pruning method for LLMs that compresses matrices in a block-wise manner.
Abstract:
This paper presents Thanos, a novel weight-pruning algorithm designed to reduce the memory footprint and enhance the computational efficiency of large language models (LLMs) by removing redundant weights while maintaining accuracy. Thanos introduces a block-wise pruning strategy with adaptive masks that dynamically adjust to weight importance, enabling flexible sparsity patterns and structured formats, such as n:m sparsity, optimized for hardware acceleration. Experimental evaluations demonstrate that Thanos achieves state-of-the-art performance in structured pruning and outperforms existing methods in unstructured pruning. By providing an efficient and adaptable approach to model compression, Thanos offers a practical solution for deploying large models in resource-constrained environments. The algorithm is publicly available for further research and application.

Supplementary Material:  zip
Primary Area: foundation or frontier models, including LLMs
Submission Number: 25458
Filter by reply type...
Filter by author...
Search keywords...

Sort: Newest First
4 / 4 replies shown
Official Review of Submission25458 by Reviewer fuCo
Official Reviewby Reviewer fuCo01 Nov 2025, 01:29 (modified: 12 Nov 2025, 18:30)EveryoneRevisions
Summary:
This paper proposes Thanos, a block-wise post-training pruning algorithm for large language models. Unlike prior methods such as SparseGPT and Wanda that prune weights independently, Thanos removes multiple weights jointly within each block, leveraging local Hessian information to better preserve model performance. It introduces adaptive masking based on weight¨Cinput importance and supports unstructured, structured, and semi-structured sparsity, including an outlier row preservation mechanism for stability at high sparsity. Experiments on OPT and LLaMA series show that Thanos achieves state-of-the-art perplexity and zero-shot accuracy under both unstructured and structured pruning, while maintaining computational efficiency suitable for hardware acceleration.

Soundness: 2: fair
Presentation: 3: good
Contribution: 2: fair
Strengths:
The paper is generally well-organized. The proposed block-wise pruning approach is thoughtfully designed, offering a reasonable trade-off between pruning accuracy and computational efficiency. The inclusion of outlier-row preservation and support for structured and semi-structured sparsity makes the method adaptable to practical deployment. The experimental section is fairly comprehensive, and the presentation of algorithms and results is clear and easy to follow.

Weaknesses:
The evaluation is somewhat narrow, focusing mainly on OPT and LLaMA, leaving uncertainty about generalization to newer models. The paper lacks detailed ablation or sensitivity studies on key hyperparameters such as block size and ¦Á. Reported efficiency gains are not well supported by runtime or memory analyses.

Questions:
Could the authors evaluate Thanos on more recent or diverse model families (e.g., Qwen3) to better demonstrate its generalization across architectures?
Can the authors provide more detailed ablation or sensitivity analyses on key hyperparameters such as block size (B) and outlier ratio (¦Á), especially across different sparsity levels or model scales?
The paper claims improved efficiency. Could the authors include quantitative comparisons (e.g., runtime or throughput) to substantiate these efficiency gains?
Flag For Ethics Review: No ethics review needed.
Rating: 4: marginally below the acceptance threshold. But would not mind if paper is accepted
Confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.
Code Of Conduct: Yes
Official Review of Submission25458 by Reviewer qK9D
Official Reviewby Reviewer qK9D28 Oct 2025, 07:28 (modified: 12 Nov 2025, 18:30)EveryoneRevisions
Summary:
This paper focuses on the pruning of the transformer block to remove the redundancy but preserve the accuracy. Authors focuses on the post-training pruning and introduces a method named Thanos. Thanos dynamically constructs the global residual mask based on the number of elements already pruned and the final target. It also take a hyperparam alpha to take the outlier row into consideration for preserving the useful outlier row. Authors have conducted extensive experiemnts on multiple matrices, including perplexity analysis, zero-shot performance and also ablation with different block size, where the proposed Thanos show state-of-the-art performance for all of these metrics.

Soundness: 3: good
Presentation: 3: good
Contribution: 3: good
Strengths:
The proposed model is well described and show very promising results compared with other existing state-of-the-art methods.

Authors also provide extensive analysis for the different datasets in supplementary material, which is showing the proposed method showing a very promising results.

Weaknesses:
If authors are able to provide some analysis in addition to Llama and OPT, it would be great to show the overall generalizability of the proposed model.

Please consider include part of the results in section E to the main manuscript as these are very important numbers.

Questions:
NA

Flag For Ethics Review: No ethics review needed.
Rating: 8: accept, good paper (poster)
Confidence: 2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
Code Of Conduct: Yes
Official Review of Submission25458 by Reviewer 25Jh
Official Reviewby Reviewer 25Jh23 Oct 2025, 01:58 (modified: 12 Nov 2025, 18:30)EveryoneRevisions
Summary:
The paper introduces Thanos, a post-training pruning algorithm aimed at shrinking large language models (LLMs) without retraining. Thanos works by partitioning every linear layer into manageable column-wise blocks and, for each block, jointly selecting multiple weights to drop and analytically re-optimising the surviving weights. This is achieved by adaptive pruning mask strategy and update multiple weights simultaneously. Moreover, they devise a structured variant that permutes rows and columns so that whole columns can be excised while optionally preserving a user-defined fraction of "outlier" rows.

Soundness: 2: fair
Presentation: 3: good
Contribution: 2: fair
Strengths:
The proposed Thanos algorithm introduces a block-wise pruning algorithm with joint weight updates. This improves upon prior work (SparseGPT) that only prunes one weight per row at a time. The paper also offers well-grounded theoretical derivations and practical heuristics, such as adaptive mask updates and outlier rows compatibility method, retaining model generalization capabilities.
The Thanos supports unstructured, structured, and semi-structured (n:m) sparsity patterns. The adaptability to formats like 2:4 allows it to take advantage of hardware acceleration.
Extensive empirical evaluations across different models show that Thanos outperforms existing methods (Magnitude, Wanda, SparseGPT) on both perplexity and zero-shot tasks.
Weaknesses:
While the paper explores the effect of mask strategy and joint weight updates, it lacks a thorough ablation study to isolate the individual contributions of different algorithmic components. Such analysis would strengthen the empirical claims.
The strategy of retaining outlier rows may hinder strict structural pruning. For instance, it becomes unclear whether entire columns can always be pruned, which could compromise compatibility with hardware acceleration schemes relying on full-column removal. The paper does not sufficiently clarify how Thanos maintains structured sparsity under these conditions.
The evaluation primarily focuses on perplexity and zero-shot accuracy, but omits key practical metrics such as pruning time and inference-time memory usage. These factors are crucial when choosing among pruning methods for deployment. Including such comparisons would have significantly enhanced the practical value of the results.
Questions:
Please refer to the "Weakness" section.

Flag For Ethics Review: No ethics review needed.
Rating: 4: marginally below the acceptance threshold. But would not mind if paper is accepted
Confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
Code Of Conduct: Yes
Official Review of Submission25458 by Reviewer kKnK
Official Reviewby Reviewer kKnK17 Oct 2025, 22:10 (modified: 12 Nov 2025, 18:30)EveryoneRevisions
Summary:
This paper proposes Thanos, a block-wise pruning algorithm for large language models. It introduces adaptive masks and joint weight updates to better capture the impact of pruning multiple parameters, supporting unstructured, structured, and n:m sparsity patterns. Experiments on LLaMA and OPT show that Thanos outperforms baselines like SparseGPT, Wanda, and magnitude pruning in both perplexity and zero-shot accuracy, especially under structured sparsity.

Soundness: 2: fair
Presentation: 1: poor
Contribution: 1: poor
Strengths:
The paper addresses an important problem in LLM compression and proposes a pruning method that is relatively straightforward to implement. It covers both unstructured and structured sparsity settings, including n:m formats, which makes the method more practically relevant. The experimental section evaluates Thanos across multiple model scales and tasks, ensuring a fair comparison with widely used baselines.

Weaknesses:
The paper is poorly organized; for example, the conclusion is placed in the appendix instead of the main body, and the overall structure lacks clarity.
A large portion of the paper is devoted to re-describing prior methods, which seems unnecessary and distracts from the core contribution.
The main novelty¡ªan extension of SparseGPT with multi-element joint pruning updates¡ªappears incremental and insufficient to support a full paper.
The experimental evaluation is incomplete: it does not report model size reduction or inference speedup, and the improvements over Wanda are marginal.
There is no ablation study to disentangle the contribution of each proposed component.
The choice of models in Table 1 is unclear: the paper refers to ¡°LLaMA-2-1.1B¡± and ¡°LLaMA-3-1B,¡± which do not correspond to official Meta releases, leaving ambiguity about what models are actually used.
Questions:
Please refer to the weakness section.

Flag For Ethics Review: No ethics review needed.
Rating: 2: reject, not good enough
Confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
Code Of Conduct: Yes
About OpenReview
Hosting a Venue
All Venues
Contact
Sponsors
Donate
FAQ
Terms of Use / Privacy Policy
News
OpenReview is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the OpenReview Sponsors. ? 2025 OpenReview

Thanos: A Block-wise Pruning Algorithm for Efficient Large Language Model Compression | OpenReview