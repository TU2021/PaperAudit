Summary
The paper proposes ExPO-HM, an Explain-then-Detect training framework for hateful meme detection that aims to produce both accurate binary/fine-grained classifications and actionable natural-language rationales. ExPO-HM integrates three components: (i) SFT warmup on structured “policy manuals” derived from dataset guidelines (Sec. 3.3; Appendix B.2–B.3), (ii) GRPO with curriculum learning that first optimizes on fine-grained labels then mixes binary/fine-grained (Fig. 2; Sec. 3.3), and (iii) a Conditional Decision Entropy (CDE) reward that incentivizes confident correct decisions and penalizes confident errors (Sec. 3.2; Eq. 11). Across HatefulMemes, MAMI, and PrideMM, ExPO-HM achieves SOTA on binary detection, fine-grained classification, and explanation quality, outperforming SFT, DPO, GRPO, and prior CoT/agent systems (Table 1). Ablations show each component contributes (Table 2, Table 3). The paper further analyzes CDE as a proxy for reasoning quality, its correlation with LLM-as-a-judge, and its role in improving calibration (Sec. 4.3, Sec. 4.6; Table 12).

Soundness
The methodology is coherently motivated by the gap between human moderation practice and direct binary detection (Sec. 1). The training pipeline is internally consistent: SFT-PM aligns the model to explicit policy schemas; GRPO-CL enforces an exploration-first schedule for fine-grained reasoning; and CDE supplies a verifiable reward derived from model entropy conditioned on its own explanation (Eq. 7–11). Evidence supports the claimed benefits: large gains over GRPO/DPO baselines on fine-grained tasks (e.g., HatefulMemes Attack +14.4 F1; Target +12.7 F1, Table 1 #21 vs #20), improved reasoning judged by multiple LLMs and BERTScore (Appendix E; Table 9), and clear component-wise improvements (Table 2). The correlation between CDE and LLM-as-a-judge is quantified across 60 points with strong significance (Sec. 4.3). Calibration analysis (ECE/Brier) is carefully executed (Sec. H.1; Table 12) and theoretical bounding is provided for a binary setting (Appendix H.2).

Potential concerns: (i) CDE relies on model logits and approximation over top tokens (Appendix G.1), which may induce estimator bias; the alternative estimators (Appendix G.2–G.3) mitigate but are less precise. (ii) The r_acc “partial credit” for multi-label fine-grained tasks is described qualitatively (Sec. 3.3) but lacks exact formulae, complicating replication. (iii) The theoretical bound is derived for binary decisions; extension to multi-class/multi-label is not formalized (Appendix H.2). The study uses consistent compute and initialization across baselines (Appendix C), supporting fair comparison.

Presentation
The paper is generally clear and well-organized: motivation (Sec. 1), related work (Sec. 2), method (Sec. 3 with equations and architecture diagram Fig. 2), experiments (Sec. 4 with comprehensive tables), and appendices that document datasets, prompts, training, and evaluation. Figures/Tables directly support claims (Table 1; Fig. 3; Table 2–3). Anchoring of equations and sections is helpful. Minor presentation issues: r_acc is described but not specified mathematically; multi-label scoring and penalties would benefit from explicit formulae. Some tables are dense; but per-class breakdowns (Appendix I; Tables 13–14) greatly aid interpretation. The inclusion of licenses and ethical considerations is appreciated (Appendix B.4; Appendix Ethical Statement).

Contribution
The paper’s primary novelty is combining policy-manual SFT, curriculum-structured GRPO, and a new CDE-based reward to produce explanation-aligned detection that surpasses direct detection. Prior CoT/agent systems underperform strong SFT baselines (Table 1 #8–#9 vs #6–#7), and even GRPO alone does not improve reasoning much (Sec. 4.3). ExPO-HM is the first Explain-then-Detect system reporting consistent SOTA across binary, fine-grained, and reasoning metrics on multiple datasets (Table 1 #21). Methodologically, CDE as a reward is a fresh and pragmatic proxy for reasoning quality that correlates with human/LLM judgments and improves calibration (Sec. 3.2; Sec. 4.3; Sec. 4.6). The comprehensive evaluation, including multiple judges, prompt sensitivity, and human studies, strengthens the contribution (Appendix E–F).

Strengths
- Clear problem framing grounded in moderation practice; explanation-first paradigm justified (Sec. 1).
- Method integration is principled and empirically validated: SFT-PM, GRPO-CL, and CDE (Sec. 3.3; Table 2–3).
- Strong and consistent empirical gains across datasets, sizes, and tasks; competitive vs. RA-HMD for binary; substantial fine-grained improvements (Table 1).
- Careful evaluation of reasoning quality with multiple judges, prompt robustness, and human helpfulness/coherence; CDE-LLM correlation quantified (Sec. 4.3; Appendix E–F).
- Calibration improvements and theory provide an additional perspective on why ExPO-HM behaves better (Table 12; Appendix H.2).
- Reproducibility details: software/hardware configs, LoRA, frozen vision, inference engine, training durations (Appendix C).

Weaknesses
- Reward design details incomplete: r_acc quantitative formulation (partial credit, over-prediction penalty) is not formalized (Sec. 3.3), hindering exact reproduction and sensitivity analysis.
- CDE estimator approximates entropy over top-k tokens (Appendix G.1), which may bias estimates differently across models; multi-class/multi-label extension in theory is not provided (Appendix H.2 focuses on binary).
- Reasoning evaluation depends primarily on the Hatred corpus (Appendix B.4, Table 8), which includes only hateful examples and is limited in size; benign rationale quality remains untested.
- Policy-manual construction is manual and dataset-specific (Appendix B.2–B.3); generalization to out-of-policy domains, multilingual settings, or evolving guidelines is not empirically assessed.
- Statistical significance for the main classification improvements is not reported; while margins are large, formal tests would strengthen claims.
- Compute requirements (8×H100, GRPO training) may limit accessibility; code/data release and access control are referenced but not confirmed.

Questions
1. Please provide the exact mathematical definition of r_acc for multi-class/multi-label cases, including over-prediction penalties and partial credit (Sec. 3.3, Eq. 9).
2. How sensitive are results to the top-k approximation in CDE entropy (Appendix G.1)? Could you report k-sweeps and variance across seeds?
3. Can the theoretical calibration bounds (Appendix H.2) be extended to multi-class and multi-label decisions? How would thresholds a,b translate?
4. Did you evaluate benign rationales’ helpfulness or coherence? If not, can you include a balanced evaluation that covers benign examples?
5. What is the sample size and protocol for human helpfulness/coherence ratings (Appendix F)? Please report N, selection policy, and confidence intervals.
6. How portable is SFT-PM to unseen or evolving policies (e.g., new protected categories, languages)? Any cross-dataset or cross-policy transfer experiments?
7. For r_CDE hyperparameters set on HatefulMemes and reused elsewhere (Appendix C.3), could you report per-dataset sweeps to quantify robustness?
8. In Table 1, were improvements tested for statistical significance (e.g., bootstrap over test sets)? If so, please include p-values.

Rating
- Overall (10): 8 — Strong, well-supported method with consistent SOTA across binary/fine-grained/reasoning and solid analyses (Table 1, Table 2–3, Sec. 4.3–4.6), with minor gaps in reward formalization.
- Novelty (10): 8 — New integration of policy-manual SFT, curriculum GRPO, and CDE reward; first Explain-then-Detect to surpass direct detection at scale (Sec. 3.3; Table 1).
- Technical Quality (10): 7 — Sound pipeline and analyses (Eq. 7–11; Table 12; Appendix E–F), but r_acc lacks formal specification and theory focuses on binary.
- Clarity (10): 8 — Clear structure, strong figures/tables, thorough appendices (Fig. 2; Table 1–3; Appendix B–H), with minor formalization omissions.
- Confidence (5): 4 — High confidence based on comprehensive experiments, ablations, calibration, and multiple judges; a few unspecified implementation details warrant caution (Sec. 3.3; Appendix C).

---

Summary
This paper introduces ExPO-HM, an Explain-then-Detect framework for hateful meme moderation that prioritizes rationale generation before classification. The approach comprises policy-manual–augmented SFT, curriculum GRPO moving from fine-grained to binary detection, and a Conditional Decision Entropy reward that aligns decision confidence with reasoning quality. The authors evaluate on HatefulMemes, MAMI, and PrideMM, demonstrating improvements over SFT/DPO/GRPO baselines and prior CoT/agent systems across binary detection, fine-grained categories (attack, target, stance), and explanation quality judged by LLMs and humans (Table 1; Appendix E–F).

Soundness
The system design is logically consistent and closely aligned with moderation workflows (Sec. 1; Fig. 1). The pipeline is well-specified: training objectives (Eq. 3–6, 9–11), policy-manual conversion (Appendix B.2–B.3), curriculum scheduler (50/50/50; Sec. 3.3), and evaluation metrics (macro/micro F1; LLM-as-a-judge; CDE; calibration; Sec. 4.1). Empirical results convincingly support the claims: ExPO-HM outperforms GRPO and DPO in both binary and fine-grained tasks, and it even surpasses RA-HMD on binary (Table 1 #21 vs #7). Ablations show each component’s necessity (Table 2). The correlation analysis between CDE and LLM-as-a-judge is methodical, with multiple seeds and statistical significance (Sec. 4.3). Human helpfulness/coherence align with LLM judgments (Appendix F; Table 11).

Caveats: Some baselines (LOREHM, U-CoT+) are agentic/prompt designs that may not directly map to fine-grained tasks (Sec. 4.2), limiting head-to-head comparability on those axes. GRPO and ExPO-HM share compute budgets (Appendix C), but RA-HMD is retrieval-augmented; differences in architecture could affect fairness of reasoning comparisons. The lack of explicit r_acc formulas is a reproducibility gap. Despite these, the internal baselines (SFT/DPO/GRPO) are carefully controlled.

Presentation
The paper is readable and structured, with clear problem framing, method equations, and comprehensive experimental tables. Figures effectively summarize the pipeline (Fig. 2) and CDE distributions (Fig. 3). Appendices provide prompts, policy manuals, dataset statistics, training settings, and evaluation specifics. However, the paper would benefit from: (i) a formal definition of r_acc, (ii) a brief pseudo-code of the GRPO-CL scheduler and data mixing, (iii) clearer description of decision tokenization for CDE in multi-class settings (Appendix G.1). Tables are dense but informative; per-class breakdowns (Appendix I) demonstrate breadth.

Contribution
The work contributes substantively to explainable hateful meme detection. It provides the first explanation-first system that matches/exceeds direct detection across key metrics. The CDE reward is a novel, deployable proxy for reasoning quality that correlates with human/LLM judgments and improves calibration—crucial attributes for safety-critical moderation contexts. The evaluation breadth and depth, including multi-dataset, multiple judges, prompt robustness, and human studies, strengthen the contribution beyond incremental improvements.

Strengths
- Practical, policy-aware training design that mirrors human moderator workflows (Sec. 3.3).
- Strong empirical performance and broad evaluation across tasks and datasets (Table 1; Appendix I).
- Clear ablation demonstrating the additive value of SFT-PM, GRPO-CL, and CDE (Table 2–3).
- Rigorous reasoning evaluation using multiple judges, prompt sensitivity, and human ratings with high agreement (Appendix E–F).
- Calibration analysis and theoretical bounds indicate improved decision reliability (Table 12; Appendix H.2).
- Reproducibility details and careful reporting of software/hardware/training settings (Appendix C).

Weaknesses
- Reliance on Hatred for rationales (Appendix B.4; Table 8) limits coverage; benign rationale quality is unassessed.
- Reward accuracy term r_acc is not fully formalized; reproduction and sensitivity are difficult (Sec. 3.3).
- Policy-manual construction is manual and derived from benchmark guidelines; generalization to new domains/cultures is not empirically tested (Appendix B.2–B.3; Ethical Statement).
- Statistical significance of classification improvements is not reported; seed variation is used for correlation but not for task metrics.
- Some baselines are mismatched in capabilities (agent systems vs. fine-grained tasks), complicating claims about absolute superiority in reasoning for all settings (Sec. 4.2).

Questions
1. Can you release the exact r_acc formula, including multi-label partial credit and over-prediction penalties, and the code to compute it?
2. Do results hold when policy manuals are perturbed or partially removed (robustness to guideline incompleteness)?
3. Can you report significance tests for Table 1 improvements (e.g., bootstrap CIs) and variance across seeds for task metrics?
4. How would ExPO-HM handle multilingual memes or non-English policies? Any preliminary experiments?
5. Could you evaluate benign rationale quality and moderation usefulness, complementing Hatred’s hateful-only focus?
6. How sensitive is CDE to top-k entropy approximation and vocabulary collapsing (Appendix G.1)? Please include k sweeps.
7. What is the dataset size used for human evaluations (Appendix F)? Provide confidence intervals for coherence/helpfulness.

Rating
- Overall (10): 8 — Strong, comprehensive contribution with robust empirical gains and practical implications; some reproducibility and generalization questions remain (Table 1; Sec. 3.3).
- Novelty (10): 8 — New combination of policy-manual SFT, curriculum GRPO, and CDE reward with demonstrated benefits over prior Explain-then-Detect approaches (Sec. 3.3; Table 2–3).
- Technical Quality (10): 7 — Solid methodology and analyses; incomplete specification of r_acc and limited benign rationale evaluation temper the score (Sec. 3.3; Appendix F).
- Clarity (10): 8 — Clear exposition with strong figures/tables and extensive appendices; a few missing formulae (r_acc) (Fig. 2; Table 2–3).
- Confidence (5): 4 — High confidence in results given breadth of evaluation and ablations; some baseline comparability and formalization gaps remain (Sec. 4.2; Sec. 3.3).

---

Summary
The authors present ExPO-HM, a training framework that converts hateful meme detection into an Explain-then-Detect pipeline optimized for practical moderation. The system learns policy knowledge via SFT on structured manuals, then uses GRPO with a curriculum to first master fine-grained categories before binary decisions, and finally adds a Conditional Decision Entropy reward to align reasoning confidence with correctness. Experiments show substantial improvements over strong baselines on binary and fine-grained tasks and higher-quality explanations measured by LLM judges, CDE, and human ratings (Table 1; Sec. 4.3; Appendix F).

Soundness
Conceptual grounding is solid: moderation requires structured policies and rationales (Sec. 1). Method components are motivated and codified: SFT-PM (Appendix B.2–B.3), GRPO-CL scheduling (Sec. 3.3), and CDE reward (Sec. 3.2; Eq. 11). Empirical tests are thorough: multi-dataset, multi-size models, component ablations, multiple judges, prompt sensitivity, per-class analyses, and calibration. The reported gains are large and consistent (e.g., Qwen2.5-VL-7B ExPO-HM vs. GRPO: BF1 81.1 vs 74.5; AttackF1 75.6 vs 61.2; TargetF1 77.2 vs 64.5; LLM 6.2 vs 5.2; Table 1). The correlation analysis supports CDE as a proxy, and calibration metrics substantiate improved reliability.

Limitations include the partial specification of r_acc, potential bias from top-k entropy approximation, and theory focused on binary decisions. The human evaluation protocol is described but lacks sample sizes and CIs.

Presentation
The manuscript is well-organized and readable, with clear sectioning, equations, and visual aids. The architecture and training flow are easy to follow (Fig. 2). Tables summarize results vividly, and appendices provide extensive details on datasets, prompts, training, evaluation, and theoretical derivations. Small presentation improvements would include formally stating r_acc and offering pseudo-code for the two-stage curriculum and reward aggregation. The ethical/use policy sections are conscientious.

Contribution
This work advances explainable multimodal moderation by showing an explanation-first system can surpass direct detection without sacrificing accuracy. The CDE reward is novel for multimodal reasoning alignment and is validated empirically and theoretically. The comprehensive evaluation—including fine-grained tasks often neglected in prior work—addresses a significant gap in the field (Sec. 2; Table 1). The method’s emphasis on actionable, policy-aligned rationales is an important step toward deployable moderation tooling.

Strengths
- Clear moderation-motivated framing and policy-aware training with SFT-PM (Sec. 3.3; Appendix B.2–B.3).
- Consistent, strong performance across datasets/tasks and sizes; SOTA on binary/fine-grained/reasoning (Table 1).
- Component ablations and warmup comparisons isolate contributions (Table 2–3).
- Multi-angle reasoning evaluation: LLM judges, BERTScore, human studies, CDE-correlations, prompt robustness (Appendix E–F; Sec. 4.3).
- Decision calibration analysis and theoretical justification (Table 12; Appendix H.2).
- Reproducibility support: environment, hyperparameters, GRPO library, LoRA setup (Appendix C).

Weaknesses
- Reward accuracy term lacks explicit formula; reproducibility and sensitivity uncertain (Sec. 3.3).
- Reasoning evaluation limited by Hatred’s scope (hateful-only), leaving benign rationale quality underexplored (Appendix B.4; Table 8).
- Policy-manual creation is manual and benchmark-dependent; generalization to real-world, evolving policies or multilingual contexts is not evaluated (Appendix B.2–B.3).
- Statistical significance of Table 1 improvements is not reported; per-seed variability for task metrics is absent.
- Theoretical analysis of CDE focuses on binary; extension to multi-class/multi-label remains open (Appendix H.2).

Questions
1. Please formalize r_acc mathematically for all task settings and share its implementation details.
2. Can you add statistical significance tests for Table 1 metrics and report variance across seeds?
3. How robust is ExPO-HM to changes in policy manuals (e.g., noisy, incomplete, or evolving guidelines)?
4. Could you extend the calibration theory to multi-class/multi-label decisions and report bounds analogous to Appendix H.2?
5. What is the size and selection criteria of the human evaluation dataset? Please include confidence intervals and inter-rater reliability beyond α.
6. How does ExPO-HM perform in cross-dataset transfer (train manuals on one dataset, test on another), or in multilingual settings?
7. Can you quantify the impact of the top-k entropy approximation on CDE and calibration metrics via ablation?

Rating
- Overall (10): 8 — A substantive, well-evidenced contribution with broad empirical gains and strong alignment to practical moderation needs (Table 1; Sec. 3.3).
- Novelty (10): 8 — The CDE reward and policy-manual + curriculum RL integration for Explain-then-Detect represent meaningful innovations (Sec. 3.2–3.3).
- Technical Quality (10): 7 — Solid methodology and analyses; missing r_acc formalization and benign rationale evaluation limit the score (Sec. 3.3; Appendix F).
- Clarity (10): 8 — Clear text, helpful figures/tables, extensive appendices; would benefit from adding missing formulae (Fig. 2; Table 2–3).
- Confidence (5): 4 — High confidence due to comprehensive evaluation and ablations; some implementation details remain unspecified (Sec. 3.3; Appendix C).

---

Summary
The paper proposes ExPO-HM, a training approach for hateful meme detection that learns to explain before deciding. It augments SFT with policy manuals, schedules GRPO to emphasize fine-grained reasoning before mixing binary, and introduces a Conditional Decision Entropy reward to align confidence with correctness. Experiments on HatefulMemes, MAMI, and PrideMM show ExPO-HM dominates baselines in binary and fine-grained tasks and improves explanation quality as judged by LLMs, humans, and the CDE proxy (Table 1; Sec. 4.3–4.7).

Soundness
The methodology is both reasonable and supported by extensive empirical evidence. ExPO-HM’s components are motivated by how human annotators are trained (Sec. 1) and are implemented with clear objectives (Eq. 3–6, 9–11). The curriculum’s ordering effect is substantiated (longer, more detailed rationales and higher metrics; Sec. 4.5). The CDE reward is rigorously defined, estimated, and analyzed, including its correlation with LLM-as-a-judge and calibration gains (Sec. 3.2; Sec. 4.3; Sec. 4.6; Table 12). Ablations demonstrate necessity and additive effects (Table 2).

However, the paper leaves some aspects under-specified: exact r_acc formula, decision vocabulary mapping for multi-class entropy, and significance testing of improvements. The theoretical analysis is binary-centric; multi-class generalization is noted but not formalized. Nevertheless, the empirical coverage and controls (same compute budgets, shared warmups, per-class results) are strong.

Presentation
The paper is clearly written with good flow and logical structure. Visualizations and tables effectively convey the method and results (Fig. 2–3; Table 1–3). Appendices are rich and helpful, including datasets, prompts, policy manuals, training configurations, alternative CDE estimators, calibration analyses, and reasoning evaluations. Minor clarity gaps relate to formalizing r_acc and providing pseudo-code for curriculum and reward computations.

Contribution
The work takes a significant step toward deployable explainable moderation: it shows explanation-first detection can be both interpretable and more accurate than direct detection. CDE as a reward is a notable contribution, demonstrating measurable correlation to human/LLM judges and producing better-calibrated decisions. The comprehensive evaluation, especially on fine-grained tasks long neglected by prior work, advances the field’s benchmarks and methodology.

Strengths
- Strong empirical improvements across datasets, sizes, and tasks; substantial fine-grained gains where baselines struggle (Table 1; Appendix I).
- Clear ablation and warmup analyses validating component roles (Table 2–3).
- Multi-faceted reasoning evaluation: LLM-as-a-judge, BERTScore, human ratings, and CDE distributions (Sec. 4.3–4.7; Table 9; Fig. 3; Table 11).
- Calibration improvements and theoretical insights (Table 12; Appendix H.2).
- Detailed reproducibility information and ethical considerations (Appendix C; Ethical Statement).

Weaknesses
- Reward accuracy term r_acc lacks explicit mathematical specification; reproduction and interpretation are limited (Sec. 3.3).
- CDE estimator’s top-k approximation may vary across models/tasks; sensitivity analysis is not reported (Appendix G.1).
- Reasoning evaluation focuses on hateful-only rationales (Hatred; Appendix B.4), leaving benign explanation quality unassessed.
- Manual policy-manual construction may overfit benchmarks and may be labor-intensive; generalization to new policies or cultures is not tested (Appendix B.2–B.3).
- Statistical significance and variance across seeds for main metrics are missing.

Questions
1. Please include the exact r_acc formulae for binary and fine-grained settings, and how penalties/partial credit are computed.
2. Can you provide statistical tests and confidence intervals for Table 1 improvements across seeds?
3. How robust is CDE to top-k entropy approximation and vocabulary choices? Please add sensitivity analyses.
4. Can ExPO-HM generalize to unseen/evolving policies or multilingual memes? Any transfer experiments planned?
5. Could you evaluate benign rationale quality and moderation usefulness to complement Hatred’s focus?
6. How was the human evaluation sample chosen and how large was it? Please include CIs and inter-rater metrics beyond Krippendorff’s α.
7. What is the impact of freezing the vision module; did you test unfreezing or partial fine-tuning?

Rating
- Overall (10): 8 — A well-executed, impactful contribution with consistent SOTA and strong evaluation breadth; some formalization and generalization questions remain (Table 1; Sec. 3.3).
- Novelty (10): 8 — Introduction of CDE reward and policy-manual + curriculum RL integration for Explain-then-Detect is meaningfully novel (Sec. 3.2–3.3).
- Technical Quality (10): 7 — Strong experiments and analyses; missing r_acc specification and limited benign rationale evaluation temper the score (Sec. 3.3; Appendix F).
- Clarity (10): 8 — Clear narrative, helpful figures/tables, extensive appendices; formal reward details would improve further (Fig. 2; Table 2–3).
- Confidence (5): 4 — High confidence due to comprehensive evidence and ablations; a few unspecified implementation details warrant caution (Appendix C; Sec. 3.3).