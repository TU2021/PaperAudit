Summary
The paper introduces ExPO-HM, an Explain-then-Detect training framework for hateful meme moderation that aims to produce both accurate binary and fine-grained classifications together with actionable, policy-aligned natural-language rationales. The approach combines three components: (a) supervised fine-tuning on structured “policy manuals” distilled from dataset guidelines to align the model with moderation schemas; (b) GRPO with a curriculum that first optimizes fine-grained categories and then mixes binary and fine-grained tasks to encourage exploration and reasoning before decision; and (c) a Conditional Decision Entropy reward that encourages confident correct decisions and penalizes confident errors, using entropy conditioned on the model’s own explanation. Experiments on HatefulMemes, MAMI, and PrideMM show ExPO-HM achieves state-of-the-art results on binary detection, fine-grained classification (e.g., attack type, target, stance), and explanation quality, outperforming strong SFT, DPO, GRPO, and prior CoT/agent systems. Ablations indicate each component contributes to overall performance. The paper further analyzes CDE as a proxy for reasoning quality, reports strong correlations with LLM-as-a-judge, and demonstrates improved decision calibration (ECE/Brier), with a theoretical bound provided for the binary case. The manuscript is well organized, with extensive appendices detailing datasets, prompts, training setups, and evaluation, although some implementation details (notably the exact r_acc formulation) remain under-specified.

Strengths
- Clear, moderation-motivated problem framing and an explanation-first paradigm that mirrors how human moderators reason with policy guidelines.
- Coherent and principled integration of components: policy-manual SFT for schema alignment, curriculum GRPO to prioritize fine-grained reasoning, and a novel CDE reward that ties decision confidence to explanation-conditioned correctness.
- Strong and consistent empirical gains across multiple datasets, model sizes, and tasks, achieving state-of-the-art performance on binary detection, fine-grained classification, and explanation quality relative to SFT, DPO, GRPO, and prior CoT/agent baselines.
- Comprehensive evaluation of reasoning quality using multiple LLM judges, BERTScore, prompt robustness analyses, human helpfulness/coherence ratings, and CDE distributions; the correlation between CDE and LLM-as-a-judge is quantified with statistical significance.
- Improved decision calibration supported by quantitative metrics (ECE/Brier) and theoretical analysis for the binary setting, offering an additional safety-relevant perspective on the method’s reliability.
- Clear ablation studies demonstrating the additive benefits of each component and the importance of the curriculum ordering.
- Extensive reproducibility details, including software/hardware configurations, training durations, model adaptations (e.g., LoRA, frozen vision), prompts/manuals, and licensing/ethical considerations.

Weaknesses
- Reward design is under-specified: the accuracy term r_acc for multi-class/multi-label tasks (partial credit, over-prediction penalties) is described qualitatively but lacks exact mathematical definitions and code, hindering replication and sensitivity analysis.
- The CDE estimator relies on a top-k entropy approximation over logits, which may introduce estimator bias; sensitivity analyses (e.g., k sweeps, seed variance) are not reported, and theoretical treatment is limited to the binary case without formal multi-class/multi-label extensions or clear decision tokenization/vocabulary mapping.
- Reasoning evaluation draws heavily on a hateful-only rationale corpus, leaving benign rationale quality, balance, and real-world moderation utility underexplored; human evaluation protocols lack detail on sample sizes, selection criteria, confidence intervals, and inter-rater reliability beyond high-level agreement measures.
- Policy-manual construction is manual and benchmark-specific; portability to unseen or evolving policies, diverse cultural contexts, and multilingual settings is not empirically assessed, and there are no cross-dataset or cross-policy transfer experiments.
- Statistical significance for the main task improvements and variance across seeds are not reported; while margins appear large, formal tests and confidence intervals would strengthen claims.
- Some baseline comparability concerns remain: agentic/prompt baselines may not align with fine-grained tasks, and retrieval-augmented systems differ architecturally, complicating head-to-head fairness for reasoning claims.
- Compute requirements are substantial (e.g., multi-GPU GRPO), which may limit accessibility; code/data release and access controls are referenced but not confirmed.
- Additional robustness ablations appear missing or limited, such as per-dataset sweeps for CDE hyperparameters, sensitivity to the top-k approximation, and the impact of freezing versus fine-tuning the vision module.
