Here are 4 independent reviews of the paper "ExPO-HM: Learning to Explain-then-Detect for Hateful Meme Detection".

***

### **Review 1**

**Summary**
This paper introduces ExPO-HM, a novel framework for hateful meme detection that follows an "Explain-then-Detect" paradigm. The authors identify that previous explainable models underperform simpler direct detection baselines. To address this, ExPO-HM proposes a three-stage training process inspired by human moderator training: 1) Supervised Fine-Tuning (SFT) on structured policy manuals (SFT-PM) to instill foundational knowledge, 2) Group Relative Policy Optimization (GRPO) with a curriculum that moves from fine-grained to binary tasks, and 3) a novel Conditional Decision Entropy (CDE) metric used as a reward to encourage confident and correct reasoning. The authors demonstrate through extensive experiments on three benchmarks that ExPO-HM not only generates high-quality explanations but also achieves new state-of-the-art performance on binary and fine-grained classification tasks, significantly outperforming all baselines.

**Soundness**
The methodology is sound, logical, and well-motivated. The analogy to human moderator training provides a strong conceptual foundation for the multi-stage approach. The core technical proposal, Conditional Decision Entropy (CDE), is an intelligent proxy for reasoning quality that cleverly sidesteps the need for a dedicated reward model or expensive human preference data. The logic that good reasoning should lead to a decisive (low-entropy) and correct prediction is intuitive and well-defended. The experimental design is rigorous, featuring multiple datasets, a comprehensive set of modern baselines (including SFT, DPO, GRPO, and SOTA agentic systems), and thorough ablation studies that validate the contribution of each component of ExPO-HM. The inclusion of both automated (LLM-as-a-judge) and human evaluations for reasoning quality further strengthens the paper's claims.

**Presentation**
The paper is exceptionally well-written, clear, and easy to follow. The introduction effectively contextualizes the problem and motivates the work. Figures 1 and 2 are excellent, providing a clear and concise visual summary of the proposed method and how it differs from prior work. The structure of the paper flows logically from problem definition to solution and evaluation. The results in Table 1 are presented comprehensively and make the performance gains immediately apparent. The appendices are detailed and provide all necessary information for reproducibility.

**Contribution**
The contribution of this paper is highly significant. It presents the first "Explain-then-Detect" system for hateful meme detection that convincingly outperforms strong direct detection baselines. This marks a potential paradigm shift for the field, moving it from a focus on simple binary predictions to more practical, interpretable, and actionable moderation tools. The introduction of CDE as both a metric and a reward signal for reasoning is a novel and impactful contribution that could be applicable to other reasoning-intensive tasks beyond meme analysis. The comprehensive evaluation setup itself is a valuable contribution, setting a new standard for benchmarking in this area.

**Strengths**
- **State-of-the-Art Performance:** ExPO-HM demonstrates substantial and consistent improvements over all baselines across binary, fine-grained, and reasoning tasks on three different datasets (Table 1).
- **Novel and Effective Method:** The combination of SFT-PM, curriculum-based GRPO, and the novel CDE reward is elegant and proven highly effective through rigorous ablations (Table 2).
- **Strong Motivation and Practical Relevance:** The work is grounded in the real-world need for explainable AI in content moderation, and the results represent a significant step towards this goal.
- **Comprehensive Evaluation:** The experiments are exhaustive, including multiple strong baselines, multiple datasets, multiple tasks, and validation through both automated and human evaluation (Section 4.7, Appendix F).
- **High-Quality Presentation:** The paper is clearly written, well-organized, and effectively uses figures to communicate its core ideas.

**Weaknesses**
The paper is very strong, and weaknesses are minor. The computational cost of the GRPO-based training is non-trivial (Section C.2), which might be a consideration for real-world deployment scenarios requiring frequent retraining, though this is a common trade-off for state-of-the-art performance.

**Questions**
1. The CDE concept is very compelling. Have the authors considered its applicability to other multimodal reasoning tasks (e.g., VQA, science question answering) or even text-only reasoning tasks where models must "explain-then-answer"?
2. The SFT-PM warmup on policy manuals is a key component. How much expert effort is required to create these manuals from dataset guidelines? Is this a potential bottleneck for applying the method to new datasets with less structured documentation?
3. The performance gains on fine-grained tasks are particularly impressive (Table 1, Appendix I). Does the model show any interesting qualitative behaviors, such as being able to distinguish between subtle attack types like "mocking" vs. "contempt" more effectively than baselines?

**Rating**
- Overall (10): 10 — The paper presents a novel, highly effective, and well-validated method that achieves a significant breakthrough in explainable hateful meme detection.
- Novelty (10): 9 — The core ideas of CDE and the human-inspired training pipeline are highly novel and impactful for this domain.
- Technical Quality (10): 10 — The technical execution is flawless, with rigorous experiments, thorough ablations, and strong analytical support for all claims (Table 2, Figure 3).
- Clarity (10): 10 — The paper is exceptionally clear, well-structured, and effectively presented.
- Confidence (5): 5 — I am an expert in this area and am highly confident in my assessment.

***

### **Review 2**

**Summary**
The paper proposes ExPO-HM, a training framework for explainable hateful meme detection. The authors argue that existing "Explain-then-Detect" methods fail to match the performance of direct classification models. ExPO-HM addresses this via a three-part strategy: a supervised fine-tuning (SFT) "warmup" using prompts derived from policy manuals, followed by reinforcement learning using Group Relative Policy Optimization (GRPO) with a curriculum learning schedule, and a novel reward signal based on Conditional Decision Entropy (CDE). Experiments on three datasets show that ExPO-HM improves over baselines in binary and fine-grained classification, as well as in the quality of its generated explanations.

**Soundness**
The methodology is technically sound, but its novelty lies more in the combination and application of existing techniques rather than fundamental breakthroughs. SFT, curriculum learning, and GRPO are all established methods. The primary novel component is the Conditional Decision Entropy (CDE) metric and its use as a reward function (Eq. 11). The intuition behind CDE—that good reasoning leads to confident, correct decisions—is plausible. However, the CDE reward function itself is quite complex, involving four hyperparameters (a, b, w, ρ) that require careful tuning (Appendix C.3), which may affect its generalizability and ease of use. The empirical validation is strong, with a clear ablation study (Table 2) demonstrating that each component contributes to the final performance. The correlation found between CDE and LLM-as-a-judge scores (Section 4.3) is an important validation point for CDE as a proxy metric.

**Presentation**
The paper is well-written and structured. The methodology is explained in sufficient detail, although understanding the full picture requires reading the appendices. The CDE reward function in Equation 11 is dense; a small plot illustrating the reward shape for correct/incorrect predictions as a function of entropy would have improved clarity. Figure 2 provides a good overview of the entire pipeline. The tables are well-formatted and present a large amount of data effectively.

**Contribution**
The main contribution is an engineered solution that successfully overcomes a key challenge in the field: making explainable models competitive with, and even superior to, black-box classifiers for hateful meme detection. While not proposing a new learning algorithm from scratch, the paper's contribution is significant from a practical and application-focused standpoint. The CDE metric is a clever, novel idea for creating a reward signal for reasoning quality without relying on human-annotated rationales, which is a valuable contribution.

**Strengths**
- **Strong Empirical Results:** The method clearly outperforms a wide range of baselines, including direct detection and other explainable methods, on multiple metrics (Table 1).
- **Convincing Ablation Study:** The ablation in Table 2 systematically demonstrates the value of each proposed component (SFT-PM, GRPO-CL, CDE), justifying the complexity of the framework.
- **Novel Reward Signal:** The CDE metric is an interesting and seemingly effective way to optimize for reasoning quality in an RL framework without a separate reward model.
- **Thorough Analysis:** The paper includes useful analyses, such as the correlation between CDE and LLM judges (Section 4.3) and the CDE distribution analysis (Figure 3), which add depth to the results.

**Weaknesses**
- **Methodological Complexity:** The full ExPO-HM pipeline is complex, involving multiple training stages and a custom RL reward with several hyperparameters. This could be a barrier to adoption and replication.
- **Limited Algorithmic Novelty:** The core learning algorithms (SFT, GRPO) are not new. The novelty is concentrated in their specific combination and the design of the CDE reward.
- **Computational Cost:** The use of GRPO, an online RL algorithm, is computationally expensive (Section C.2), which may limit its practicality for rapid model iteration in a production environment.

**Questions**
1. The curriculum learning schedule uses a simple 50/50 split. How sensitive is the final performance to this schedule? For instance, what happens if you use a 25/75 or 75/25 split, or a more gradual mixing of fine-grained and binary data?
2. The CDE reward is used with GRPO. Could a similar reward be formulated for offline preference optimization methods like DPO? For example, by using CDE to help select the preferred (`y+`) response from a set of candidates.
3. Section 4.6 states that CDE does not cause policy entropy collapse. Is there quantitative data (e.g., plots of policy entropy over training steps) in the appendix or elsewhere to support this claim? This would be a valuable addition.

**Rating**
- Overall (10): 8 — A strong paper with impressive results and a novel take on training explainable models, though the method is complex and builds heavily on existing techniques.
- Novelty (10): 7 — The CDE metric/reward is novel and interesting, but the overall framework is a clever combination of existing methods rather than a fundamental new algorithm.
- Technical Quality (10): 9 — The experiments are thorough and well-executed, with strong baselines and insightful analysis.
- Clarity (10): 8 — The paper is mostly clear, but some parts of the methodology are dense and rely heavily on the appendix.
- Confidence (5): 5 — I am highly confident in my evaluation based on my expertise in multimodal learning and reinforcement learning.

***

### **Review 3**

**Summary**
This paper presents ExPO-HM, a method for hateful meme detection designed to provide explanations for its decisions without sacrificing accuracy. Motivated by the training process of human content moderators, the method first fine-tunes a large multimodal model on policy guidelines (SFT-PM), then uses reinforcement learning (GRPO) with a curriculum to refine its reasoning, and finally introduces a Conditional Decision Entropy (CDE) reward to encourage high-quality explanations. The authors show that their method achieves state-of-the-art results on three datasets, outperforming both direct detection and prior explainable models. The paper also includes a human evaluation to validate the quality of the generated explanations.

**Soundness**
The paper's central premise—mimicking human moderator training—is a compelling and intuitive motivation for the proposed technical approach. The methodology appears sound, and the results back up the claims. However, there are practical soundness questions regarding its real-world deployment. The first step, SFT-PM, requires manually converting prose guidelines into a "structured policy manual" (Appendix B.2). This manual step could be a significant bottleneck for platforms with dynamic, evolving policies or for new domains. The choice of GRPO, an online RL method, implies significant computational resources for training (Section C.2), which may not be practical for the frequent model updates needed in content moderation. The human evaluation is a good step, but the reported helpfulness scores (2.2 out of 4 for ExPO-HM, Table 11) suggest the explanations are "partially helpful" at best, indicating a gap still exists between the model's output and what a human moderator might find "highly helpful".

**Presentation**
The paper is well-written and the argument is easy to follow. The introduction does an excellent job of framing the problem and its real-world importance. The inclusion of a human evaluation (Section 4.7) and a detailed ethical statement (Section 37) are commendable and show a mature consideration of the problem space. The figures are clear and helpful. The main text is a bit light on the details of the human evaluation (e.g., number of participants/items), requiring a dive into the appendix, but this is a minor issue.

**Contribution**
The primary contribution is demonstrating that it is possible to build an explainable hateful meme detection system that is also the most accurate. This is a significant achievement and addresses a major gap in the literature where explainability often came at the cost of performance. The CDE metric is a valuable contribution as it provides a way to optimize for reasoning quality without needing human-annotated rationales for the reward signal, which is a huge practical advantage. The work provides a strong proof-of-concept for a new generation of more transparent and useful moderation tools.

**Strengths**
- **Real-world Motivation:** The analogy to human moderator training is a strong and intuitive framing for the technical approach.
- **Bridging the Performance Gap:** The paper successfully closes the performance gap between explainable models and direct-detection models, even surpassing them.
- **Human-in-the-loop Validation:** The inclusion of a human evaluation for explanation quality (Section 4.7, Appendix F) provides a crucial sanity check on the automated metrics and strengthens the paper's claims.
- **Practical Reward Design:** The CDE reward is a practical and clever solution that avoids the need for a separate, difficult-to-train reward model.
- **Ethical Consideration:** The paper includes a thoughtful ethical statement that discusses potential biases and deployment considerations.

**Weaknesses**
- **Scalability Concerns:** The manual creation of policy manuals and the high computational cost of GRPO training raise questions about the method's scalability and practicality for real-world, dynamic environments.
- **Moderate Human-Rated Helpfulness:** While better than the baseline, the human-rated helpfulness score of 2.2/4 (Table 11) indicates that the explanations are still far from perfect and may require significant editing by a human moderator, limiting their immediate utility.
- **Complexity:** The multi-stage pipeline is quite complex, which could hinder widespread adoption and further research by groups with fewer computational resources.

**Questions**
1. Could the authors elaborate on the process of creating the policy manuals? How much time and expertise did it take, and do they have ideas on how this process could be automated to improve scalability?
2. What is the inference latency of the ExPO-HM model compared to a standard Direct-SFT model? Is the generation of the explanation a significant time cost?
3. The human evaluation helpfulness score is 2.2/4. In the authors' view, what are the primary deficiencies in the generated explanations that prevent them from scoring higher (e.g., lack of specificity, incorrect reasoning steps, unnatural language), and what future work could address this?

**Rating**
- Overall (10): 8 — A very strong paper making a significant practical contribution, with some open questions about real-world scalability and the ultimate helpfulness of the explanations.
- Novelty (10): 8 — The combination of methods is uniquely tailored to the problem, and the CDE reward is a novel concept.
- Technical Quality (10): 8 — The experiments are solid, but the real-world practicality and scalability of the proposed technical solution could be further explored.
- Clarity (10): 9 — The paper is very clearly written and motivated.
- Confidence (5): 4 — I am confident in my assessment, though my expertise is more in NLP applications than core RL methods.

***

### **Review 4**

**Summary**
This paper presents ExPO-HM, a comprehensive training framework for explainable hateful meme detection. The method aims to solve the problem of prior "Explain-then-Detect" systems underperforming direct classification models. ExPO-HM consists of three stages: SFT on policy manuals (SFT-PM), GRPO with curriculum learning (GRPO-CL), and a novel reward based on Conditional Decision Entropy (CDE). The authors conduct a thorough evaluation on the HatefulMemes, MAMI, and PrideMM datasets, demonstrating state-of-the-art performance on binary classification, fine-grained classification, and explanation quality, validated by both automated metrics and human studies.

**Soundness**
The paper is technically very sound. The methodology is built on a logical progression of steps, and each step is justified through detailed ablation studies (Table 2). The introduction of CDE as a proxy for reasoning quality is well-argued, and its effectiveness is supported by multiple pieces of evidence: improved classification F1 scores, higher LLM-as-a-judge scores, a strong correlation with those scores (Section 4.3), and improved model calibration (Section 4.6, Appendix H). The experimental setup is exemplary in its thoroughness, including comparisons to a wide array of relevant baselines and SOTA models. The authors also go the extra mile in the appendix to validate their evaluation methods, for instance by using multiple LLM judges and analyzing prompt sensitivity (Appendix E), which significantly increases the reliability of their findings.

**Presentation**
The presentation is of very high quality. The paper is well-written, logically structured, and easy to read. The figures (especially Fig. 1 and 2) are highly effective at communicating the core concepts and the overall architecture. The tables are dense with information but remain clear and interpretable. The appendices are a model of thoroughness, providing extensive details on datasets, implementation, hyperparameters, and additional results that are crucial for reproducibility and for fully appreciating the work.

There are a few minor presentational inconsistencies. For example, in Table 1, the CDE metric is labeled "CDE" for the HatefulMemes dataset but "RCDE" for MAMI and PrideMM. While "R" likely stands for Reasoning, this is not explicitly defined in the caption and is inconsistent. The main text mentions the human evaluation (Section 4.7) but omits key details like the number of items evaluated, which are only found in the appendix (Appendix F); including a brief summary of these details in the main text would make it more self-contained.

**Contribution**
This paper makes a very strong and clear contribution to the field. It is, to my knowledge, the first work to demonstrate an explainable hateful content detection model that does not compromise on, but actually improves, classification accuracy compared to strong non-explainable baselines. This is a critical step forward for creating trustworthy and practical AI for content moderation. The CDE metric is a novel and generalizable idea for optimizing reasoning without direct supervision on explanations. Furthermore, the comprehensive and rigorous evaluation framework sets a high bar for future work in this area.

**Strengths**
- **Exceptional Thoroughness:** The evaluation is exhaustive and leaves little room for doubt about the method's effectiveness. The use of multiple datasets, tasks, strong baselines, and multiple evaluation modalities (automated, human, calibration) is a major strength.
- **Reproducibility and Transparency:** The detailed appendices provide a high level of transparency and contain all the necessary information for other researchers to reproduce and build upon this work.
- **Clear and Convincing Ablations:** The ablation studies in Table 2 and the analysis of warmup strategies in Table 3 are clear, well-designed, and convincingly demonstrate the importance of each part of the ExPO-HM framework.
- **High-Quality Writing:** The paper is written with a high degree of clarity and professionalism.

**Weaknesses**
The weaknesses are almost exclusively minor presentational points.
- **Minor Inconsistencies:** As noted, there are minor inconsistencies in table labeling (e.g., CDE vs. RCDE in Table 1).
- **Reliance on Appendix:** The main paper is heavily reliant on the appendix for critical experimental details (e.g., policy manual construction, human evaluation setup). While necessary due to space constraints, it slightly detracts from the self-containedness of the main 10 pages.

**Questions**
1. Could the authors clarify the meaning of "RCDE" in Table 1 and standardize the column headers for the CDE metric across datasets?
2. In Section 3.3, the paper notes that using gold human explanations (`e*`) during SFT warmup leads to worse performance. This is a very interesting finding. Could you elaborate on your hypothesis for why this occurs? Is it an issue of on-policy vs. off-policy data during the subsequent RL phase?
3. The human evaluation in Appendix F is very well-designed. For completeness, could you state the number of unique memes that were evaluated by the annotators in the main text or caption of Table 11?

**Rating**
- Overall (10): 9 — An outstanding paper with significant contributions, backed by extremely thorough and convincing experiments. A near-perfect submission.
- Novelty (10): 8 — The CDE concept and the overall pipeline design are novel and impactful, though they build upon existing algorithmic foundations like GRPO.
- Technical Quality (10): 10 — The technical quality is superb. The experiments are rigorous, the analysis is deep, and the claims are strongly supported by empirical evidence.
- Clarity (10): 9 — The paper is very clearly written, with only very minor presentational issues.
- Confidence (5): 5 — I am an expert in this field and am very confident in my assessment of this paper's quality and contribution.