{
  "baseline_review": "1) Summary\nThe paper addresses the problem of hateful meme detection, arguing that existing direct detection methods lack the necessary explanations for real-world content moderation. It identifies shortcomings in current \"Explain-then-Detect\" approaches, which often underperform simpler baselines. To overcome this, the authors propose ExPO-HM (Explain-then-Detect Policy Optimization for Hateful Memes), a framework inspired by the training process of human moderators. The method involves a Supervised Fine-Tuning (SFT) warmup on structured policy manuals, followed by Group Relative Policy Optimization (GRPO) with curriculum learning. A key contribution is the introduction of Conditional Decision Entropy (CDE) as both a metric for reasoning quality and a reward signal to encourage decisive reasoning. Experiments across three benchmarks show that ExPO-HM achieves state-of-the-art performance in binary detection, fine-grained classification, and explanation quality, significantly outperforming existing direct and explainable models.2) Strengths\n*   **Novel and Well-Motivated Framework:** The core idea of structuring the model's training process to mimic that of human moderators is both intuitive and novel. This provides a strong conceptual foundation for the proposed multi-stage approach.\n    *   The SFT warmup on structured policy manuals (SFT-PM) directly addresses the need for models to understand moderation guidelines, analogous to how human annotators are trained (Section 3.3, Figure 2).\n    *   The use of curriculum learning within GRPO (GRPO-CL), starting with fine-grained tasks before moving to binary classification, mirrors a progression from foundational understanding to final judgment (Section 3.3). This is shown to be more effective than random mixing (Table 2, #2 vs #3).\n    *   This paradigm successfully bridges the performance gap, creating the first \"Explain-then-Detect\" system reported to outperform strong direct detection baselines like RA-HMD (Table 1, #21 vs #7).*   **Introduction of a Novel and Effective Reward Signal:** The paper proposes Conditional Decision Entropy (CDE) as a proxy for reasoning quality, which is a significant technical contribution. CDE is well-defined and effectively integrated into the RL framework.\n    *   The definition of CDE is clearly formulated (Section 3.2, Eq. 7) based on the principle that good reasoning should lead to a confident and correct decision.\n    *   The CDE reward function is designed to penalize overconfident errors and reward confident correct predictions (Section 3.3, Eq. 11), which is shown to improve model calibration (Appendix H.1, Table 12).\n    *   The effectiveness of CDE as a reward is empirically validated in the ablation study, where its inclusion leads to substantial gains in classification F1 scores and LLM-as-a-judge scores (Table 2, #3 vs #4).\n    *   The paper also validates CDE as a standalone metric, showing a strong negative correlation with LLM-as-a-judge scores (Section 4.3, Pearson r = -0.78), supporting its use as a proxy for reasoning quality.*   **Comprehensive and Rigorous Experimental Evaluation:** The empirical evaluation is extensive, covering multiple datasets, tasks, and strong baselines, which robustly supports the paper's claims.\n    *   The method is tested on three distinct datasets (HatefulMemes, MAMI, PrideMM), demonstrating its generalizability (Table 1).\n    *   Evaluation spans binary classification, multiple fine-grained classification tasks (e.g., attack type, target), and reasoning quality, providing a holistic view of performance (Table 1).\n    *   The baselines are comprehensive, including zero-shot, SFT, DPO, and GRPO variants, as well as state-of-the-art direct detection (RA-HMD) and explainable systems (LOREHM, U-CoT+) (Section 4.2).\n    *   The quality of generated explanations is assessed using multiple LLM judges, prompt sensitivity analysis, BERTScore (Appendix E), and a human evaluation (Section 4.7, Appendix F), adding significant credibility to the reasoning quality claims.*   **Thorough Analysis and Ablation Studies:** The paper provides detailed analyses that offer valuable insights into why the proposed components are effective.\n    *   The main ablation study clearly disentangles the contributions of SFT-PM, GRPO-CL, and the CDE reward, showing that each component adds significant value (Table 2).\n    *   A dedicated analysis of different SFT warmup strategies highlights that the proposed SFT-PM is superior for downstream RL performance, even compared to SFT variants that perform well in isolation (Table 3).\n    *   The CDE analysis in Section 4.6 and Appendix H is particularly strong, showing how ExPO-HM leads to better-separated CDE distributions for correct vs. wrong predictions (Figure 3) and improved confidence calibration (Table 12).3) Weaknesses\n*   **Dependence on Manual Engineering for Policy Manuals:** The SFT-PM warmup stage, which is shown to be a critical component (Table 2, #1 vs #2), relies on a human expert to manually convert dataset annotation guidelines into a structured \"policy manual\" (Appendix B.2). This introduces a potential bottleneck and raises questions about the method's scalability and robustness.\n    *   The paper states this is a \"one-time process performed by a human expert\" (Appendix B.2), but the amount of effort required is not quantified. This makes it difficult to assess the practical cost of applying the method to new datasets or moderation policies.\n    *   The performance sensitivity to the specific phrasing, structure, or level of detail in these manually created manuals is not explored. It is unclear how much \"prompt engineering\" is required to create an effective manual.\n    *   This manual step contrasts with the goal of creating fully automated systems and may limit the method's adaptability in dynamic moderation environments where policies frequently change.*   **High Framework Complexity:** The full ExPO-HM framework combines several advanced techniques: SFT with custom-formatted data, online reinforcement learning (GRPO), a specific curriculum learning schedule, and a novel reward function with its own set of hyperparameters (a, b, w, ρ from Eq. 11). This complexity could be a barrier to adoption and reproducibility.\n    *   Compared to simpler and more widely used alignment methods like SFT or offline RL methods like DPO, the proposed pipeline is significantly more complex to implement and tune.\n    *   Online RL methods like GRPO are known to be less stable and more computationally expensive than offline methods, requiring on-policy sampling and careful hyperparameter tuning. While the authors provide details (Appendix C.2, C.3), the inherent complexity remains.\n    *   The paper does not provide a direct comparison of the computational cost (e.g., training time, GPU resources) between ExPO-HM and the baselines, making it hard to evaluate the performance gains against the increased complexity and cost.*   **Limited Discussion of Alternative Reasoning Quality Proxies:** The paper successfully proposes and validates CDE as a proxy for reasoning quality. However, the motivation for CDE could be strengthened by discussing it in the context of other potential automatic proxies for reasoning quality.\n    *   The paper does not explore or compare against alternative reward formulations. For example, one could reward low entropy (high confidence) in the explanation tokens themselves, or use other uncertainty metrics from the model.\n    *   While the paper shows a strong correlation between CDE and LLM-judge scores (Section 4.3), it does not provide a conceptual argument for why CDE is superior to other potential proxies, leaving the choice feeling somewhat ad-hoc, even if empirically successful.\n    *   A brief discussion of related or alternative approaches would help to better position the novelty and rationale of using CDE specifically for this task.4) Suggestions for Improvement\n*   **Analyze and Discuss the Policy Manual Creation Process:** To address the concerns about manual engineering, the authors could add a brief discussion on the practical aspects of creating the policy manuals.\n    *   In Appendix B.2, provide an estimate of the human effort required (e.g., person-hours per dataset) to create the manuals. This would help practitioners gauge the cost of adoption.\n    *   To demonstrate robustness, consider adding a small-scale ablation study in the appendix where the phrasing or structure of the policy manual is slightly altered. This would show how sensitive the SFT-PM stage is to this manual input.\n    *   Discuss potential ways to automate or semi-automate the creation of these manuals from prose guidelines in future work.*   **Provide a More Transparent Account of Framework Costs:** To help readers understand the practical trade-offs of the framework's complexity, the authors should provide more details on its computational requirements.\n    *   Add a paragraph or table in Appendix C comparing the approximate training time and computational resources (e.g., total GPU hours) required for ExPO-HM versus the key baselines (e.g., Direct-SFT, DPO, and standard GRPO) on a representative dataset.\n    *   Explicitly acknowledge the increased complexity in the main text (e.g., in the conclusion) but argue that the significant performance gains justify the cost for this critical application area.*   **Contextualize the Choice of CDE as a Reward Signal:** To strengthen the motivation for CDE, the authors could expand the discussion around its selection.\n    *   In Section 3.2, add a few sentences discussing other potential automatic proxies for reasoning quality (e.g., explanation token entropy, model uncertainty scores) and provide a brief rationale for choosing CDE (e.g., its direct link between the reasoning process and the final decision).\n    *   This does not require new experiments, but rather a conceptual framing that would help readers understand why CDE is a particularly suitable choice for an \"Explain-then-Detect\" framework.5) Score\n*   Overall (10): 9 — The paper presents a novel, well-motivated, and highly effective framework for explainable hateful meme detection, supported by exceptionally thorough experiments and analysis.\n*   Novelty (10): 9 — The core framework inspired by human moderation, the combination of SFT-PM and GRPO-CL, and the CDE metric/reward are all novel and significant contributions to the field.\n*   Technical Quality (10): 9 — The methodology is sound, the experiments are rigorous and comprehensive, and the claims are strongly supported by empirical evidence, including human evaluation and theoretical analysis (Appendix H).\n*   Clarity (10): 10 — The paper is exceptionally well-written and organized, with clear figures (Figure 1, Figure 2) and extensive appendices that make the complex methodology easy to understand.\n*   Confidence (5): 5 — I am highly confident in my assessment, as I am familiar with the literature on multimodal learning, content moderation, and RL-based model alignment.",
  "final_review": "1) Summary\nThe paper addresses the problem of hateful meme detection, arguing that existing direct detection methods lack the necessary explanations for real-world content moderation. It identifies shortcomings in current \"Explain-then-Detect\" approaches, which often underperform simpler baselines. To overcome this, the authors propose ExPO-HM (Explain-then-Detect Policy Optimization for Hateful Memes), a framework inspired by the training process of human moderators. The method involves a Supervised Fine-Tuning (SFT) warmup on structured policy manuals, followed by Group Relative Policy Optimization (GRPO) with curriculum learning. A key contribution is the introduction of Conditional Decision Entropy (CDE) as both a metric for reasoning quality and a reward signal to encourage decisive reasoning. Experiments across three benchmarks show that ExPO-HM achieves state-of-the-art performance in binary detection, fine-grained classification, and explanation quality, significantly outperforming existing direct and explainable models.2) Strengths\n*   **Novel and Well-Motivated Framework:** The core idea of structuring the model's training process to mimic that of human moderators is both intuitive and novel. This provides a strong conceptual foundation for the proposed multi-stage approach.\n    *   The SFT warmup on structured policy manuals (SFT-PM) directly addresses the need for models to understand moderation guidelines, analogous to how human annotators are trained (Section 3.3, Figure 2).\n    *   The use of curriculum learning within GRPO (GRPO-CL), starting with fine-grained tasks before moving to binary classification, mirrors a progression from foundational understanding to final judgment (Section 3.3). This is shown to be more effective than random mixing (Table 2, #2 vs #3).\n    *   This paradigm is claimed to bridge the performance gap, creating the first \"Explain-then-Detect\" system reported to outperform strong direct detection baselines like RA-HMD (Table 1, #21 vs #7).*   **Introduction of a Novel and Effective Reward Signal:** The paper proposes Conditional Decision Entropy (CDE) as a proxy for reasoning quality, which is a significant technical contribution. CDE is well-defined and effectively integrated into the RL framework.\n    *   The definition of CDE is clearly formulated (Section 3.2, Eq. 7) based on the principle that good reasoning should lead to a confident and correct decision.\n    *   The CDE reward function is designed to penalize overconfident errors and reward confident correct predictions (Section 3.3, Eq. 11), which is shown to improve model calibration (Appendix H.1, Table 12).\n    *   The effectiveness of CDE as a reward is empirically validated in the ablation study, where its inclusion leads to substantial gains in classification F1 scores and LLM-as-a-judge scores (Table 2, #3 vs #4).\n    *   The paper also validates CDE as a standalone metric, showing a strong negative correlation with LLM-as-a-judge scores (Section 4.3, Pearson r = -0.78), supporting its use as a proxy for reasoning quality.*   **Comprehensive and Rigorous Experimental Design:** The empirical evaluation is designed to be extensive, covering multiple datasets, tasks, and strong baselines, which would robustly support the paper's claims if the results were verifiable.\n    *   The method is tested on three distinct datasets (HatefulMemes, MAMI, PrideMM), demonstrating its intended generalizability (Table 1).\n    *   Evaluation spans binary classification, multiple fine-grained classification tasks (e.g., attack type, target), and reasoning quality, providing a holistic view of performance (Table 1).\n    *   The baselines are comprehensive, including zero-shot, SFT, DPO, and GRPO variants, as well as state-of-the-art direct detection (RA-HMD) and explainable systems (LOREHM, U-CoT+) (Section 4.2).\n    *   The quality of generated explanations is assessed using multiple LLM judges, prompt sensitivity analysis, BERTScore (Appendix E), and a human evaluation (Section 4.7, Appendix F), adding significant credibility to the reasoning quality claims.*   **Thorough Analysis and Ablation Studies:** The paper provides detailed analyses that offer valuable insights into why the proposed components are effective.\n    *   The main ablation study clearly disentangles the contributions of SFT-PM, GRPO-CL, and the CDE reward, showing that each component adds significant value (Table 2).\n    *   A dedicated analysis of different SFT warmup strategies highlights that the proposed SFT-PM is superior for downstream RL performance, even compared to SFT variants that perform well in isolation (Table 3).\n    *   The CDE analysis in Section 4.6 and Appendix H is particularly strong, showing how ExPO-HM leads to better-separated CDE distributions for correct vs. wrong predictions (Figure 3) and improved confidence calibration (Table 12).3) Weaknesses\n*   **Critical Issues with Verifiability and Scholarly Practice:** The manuscript contains several elements that severely undermine the trustworthiness of its results and its adherence to standard scientific practice.\n    *   The paper reports evaluation results from a non-existent model. Appendix E.1 explicitly states, \"We further report scores using a newer and stronger closed-source judge GPT-5 (gpt-5-2025-08-07) for reference,\" and Table 9 presents a full column of numerical results for this fictitious model. Presenting results from a model that does not exist is a form of data fabrication and invalidates the evaluation.\n    *   The bibliography systematically includes numerous non-existent and future-dated references. For example, citations for key models and methods point to future dates (e.g., Bai et al., 2025; DeepSeek-AI et al., 2025; Qwen et al., 2025) with invalid identifiers (References section). This practice makes it impossible to verify the paper's claims about baselines and related work, and it misrepresents the state of the field.\n    *   There are instances of imprecise claims in the text. For example, Section 4.5 claims that the SFT-B warmup strategy leads to performance \"even below the no-warmup baseline\" after RL training. However, the primary metric in Table 3 (BF1) shows the SFT-B model (73.5) performing slightly better than the no-warmup baseline (73.3), making the textual claim an overstatement.*   **Dependence on Manual Engineering for Policy Manuals:** The SFT-PM warmup stage, which is shown to be a critical component (Table 2, #1 vs #2), relies on a human expert to manually convert dataset annotation guidelines into a structured \"policy manual\" (Appendix B.2). This introduces a potential bottleneck and raises questions about the method's scalability and robustness.\n    *   The paper states this is a \"one-time process performed by a human expert\" (Appendix B.2), but the amount of effort required is not quantified. This makes it difficult to assess the practical cost of applying the method to new datasets or moderation policies.\n    *   The performance sensitivity to the specific phrasing, structure, or level of detail in these manually created manuals is not explored. It is unclear how much \"prompt engineering\" is required to create an effective manual.\n    *   This manual step contrasts with the goal of creating fully automated systems and may limit the method's adaptability in dynamic moderation environments where policies frequently change.*   **High Framework Complexity:** The full ExPO-HM framework combines several advanced techniques: SFT with custom-formatted data, online reinforcement learning (GRPO), a specific curriculum learning schedule, and a novel reward function with its own set of hyperparameters (a, b, w, ρ from Eq. 11). This complexity could be a barrier to adoption and reproducibility.\n    *   Compared to simpler and more widely used alignment methods like SFT or offline RL methods like DPO, the proposed pipeline is significantly more complex to implement and tune.\n    *   Online RL methods like GRPO are known to be less stable and more computationally expensive than offline methods, requiring on-policy sampling and careful hyperparameter tuning. While the authors provide details (Appendix C.2, C.3), the inherent complexity remains.\n    *   The paper does not provide a direct comparison of the computational cost (e.g., training time, GPU resources) between ExPO-HM and the baselines, making it hard to evaluate the performance gains against the increased complexity and cost.*   **Limited Discussion of Alternative Reasoning Quality Proxies:** The paper successfully proposes and validates CDE as a proxy for reasoning quality. However, the motivation for CDE could be strengthened by discussing it in the context of other potential automatic proxies for reasoning quality.\n    *   The paper does not explore or compare against alternative reward formulations. For example, one could reward low entropy (high confidence) in the explanation tokens themselves, or use other uncertainty metrics from the model.\n    *   While the paper shows a strong correlation between CDE and LLM-judge scores (Section 4.3), it does not provide a conceptual argument for why CDE is superior to other potential proxies, leaving the choice feeling somewhat ad-hoc, even if empirically successful.\n    *   A brief discussion of related or alternative approaches would help to better position the novelty and rationale of using CDE specifically for this task.4) Suggestions for Improvement\n*   **Address and Rectify Verifiability and Reporting Issues:** The manuscript requires a fundamental revision to align with basic standards of scientific integrity before it can be considered for publication.\n    *   All results, claims, and discussion related to non-existent models (e.g., \"GPT-5\") must be removed. The evaluation must be based entirely on real, verifiable systems.\n    *   The bibliography must be corrected to remove all fictitious and future-dated citations. All references must point to existing, publicly accessible work. Claims about the literature must be supported by verifiable sources.\n    *   All textual descriptions of results must be carefully checked to ensure they are a precise and accurate reflection of the data presented in tables and figures.*   **Analyze and Discuss the Policy Manual Creation Process:** To address the concerns about manual engineering, the authors could add a brief discussion on the practical aspects of creating the policy manuals.\n    *   In Appendix B.2, provide an estimate of the human effort required (e.g., person-hours per dataset) to create the manuals. This would help practitioners gauge the cost of adoption.\n    *   To demonstrate robustness, consider adding a small-scale ablation study in the appendix where the phrasing or structure of the policy manual is slightly altered. This would show how sensitive the SFT-PM stage is to this manual input.\n    *   Discuss potential ways to automate or semi-automate the creation of these manuals from prose guidelines in future work.*   **Provide a More Transparent Account of Framework Costs:** To help readers understand the practical trade-offs of the framework's complexity, the authors should provide more details on its computational requirements.\n    *   Add a paragraph or table in Appendix C comparing the approximate training time and computational resources (e.g., total GPU hours) required for ExPO-HM versus the key baselines (e.g., Direct-SFT, DPO, and standard GRPO) on a representative dataset.\n    *   Explicitly acknowledge the increased complexity in the main text (e.g., in the conclusion) but argue that the significant performance gains justify the cost for this critical application area.*   **Contextualize the Choice of CDE as a Reward Signal:** To strengthen the motivation for CDE, the authors could expand the discussion around its selection.\n    *   In Section 3.2, add a few sentences discussing other potential automatic proxies for reasoning quality (e.g., explanation token entropy, model uncertainty scores) and provide a brief rationale for choosing CDE (e.g., its direct link between the reasoning process and the final decision).\n    *   This does not require new experiments, but rather a conceptual framing that would help readers understand why CDE is a particularly suitable choice for an \"Explain-then-Detect\" framework.5) Score\n*   Overall (10): 2 — The paper's empirical claims are rendered untrustworthy by the inclusion of fabricated results and non-existent citations (Table 9, References).\n*   Novelty (10): 7 — The framework combining SFT-PM, GRPO-CL, and the CDE reward is conceptually novel (Section 3.3), though the context is obscured by unverifiable citations.\n*   Technical Quality (10): 1 — The inclusion of results from a non-existent model (Table 9) and the systematic use of fictitious references represent a critical failure of scientific practice.\n*   Clarity (10): 6 — While the paper is generally well-written, its clarity is undermined by the presentation of unverifiable and fabricated information (Appendix E.1, References), which misleads the reader.\n*   Confidence (5): 5 — I am highly confident in my assessment, as the evidence for the critical weaknesses is stated directly in the manuscript.",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 9,
        "novelty": 9,
        "technical_quality": 9,
        "clarity": 10,
        "confidence": 5
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 2,
        "novelty": 7,
        "technical_quality": 1,
        "clarity": 6,
        "confidence": 5
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "1) Summary\nThe paper addresses the problem of hateful meme detection, arguing that existing direct detection methods lack the necessary explanations for real-world content moderation. It identifies shortcomings in current \"Explain-then-Detect\" approaches, which often underperform simpler baselines. To overcome this, the authors propose ExPO-HM (Explain-then-Detect Policy Optimization for Hateful Memes), a framework inspired by the training process of human moderators. The method involves a Supervised Fine-Tuning (SFT) warmup on structured policy manuals, followed by Group Relative Policy Optimization (GRPO) with curriculum learning. A key contribution is the introduction of Conditional Decision Entropy (CDE) as both a metric for reasoning quality and a reward signal to encourage decisive reasoning. Experiments across three benchmarks show that ExPO-HM achieves state-of-the-art performance in binary detection, fine-grained classification, and explanation quality, significantly outperforming existing direct and explainable models.2) Strengths\n*   **Novel and Well-Motivated Framework:** The core idea of structuring the model's training process to mimic that of human moderators is both intuitive and novel. This provides a strong conceptual foundation for the proposed multi-stage approach.\n    *   The SFT warmup on structured policy manuals (SFT-PM) directly addresses the need for models to understand moderation guidelines, analogous to how human annotators are trained (Section 3.3, Figure 2).\n    *   The use of curriculum learning within GRPO (GRPO-CL), starting with fine-grained tasks before moving to binary classification, mirrors a progression from foundational understanding to final judgment (Section 3.3). This is shown to be more effective than random mixing (Table 2, #2 vs #3).\n    *   This paradigm is claimed to bridge the performance gap, creating the first \"Explain-then-Detect\" system reported to outperform strong direct detection baselines like RA-HMD (Table 1, #21 vs #7).*   **Introduction of a Novel and Effective Reward Signal:** The paper proposes Conditional Decision Entropy (CDE) as a proxy for reasoning quality, which is a significant technical contribution. CDE is well-defined and effectively integrated into the RL framework.\n    *   The definition of CDE is clearly formulated (Section 3.2, Eq. 7) based on the principle that good reasoning should lead to a confident and correct decision.\n    *   The CDE reward function is designed to penalize overconfident errors and reward confident correct predictions (Section 3.3, Eq. 11), which is shown to improve model calibration (Appendix H.1, Table 12).\n    *   The effectiveness of CDE as a reward is empirically validated in the ablation study, where its inclusion leads to substantial gains in classification F1 scores and LLM-as-a-judge scores (Table 2, #3 vs #4).\n    *   The paper also validates CDE as a standalone metric, showing a strong negative correlation with LLM-as-a-judge scores (Section 4.3, Pearson r = -0.78), supporting its use as a proxy for reasoning quality.*   **Comprehensive and Rigorous Experimental Design:** The empirical evaluation is designed to be extensive, covering multiple datasets, tasks, and strong baselines, which would robustly support the paper's claims if the results were verifiable.\n    *   The method is tested on three distinct datasets (HatefulMemes, MAMI, PrideMM), demonstrating its intended generalizability (Table 1).\n    *   Evaluation spans binary classification, multiple fine-grained classification tasks (e.g., attack type, target), and reasoning quality, providing a holistic view of performance (Table 1).\n    *   The baselines are comprehensive, including zero-shot, SFT, DPO, and GRPO variants, as well as state-of-the-art direct detection (RA-HMD) and explainable systems (LOREHM, U-CoT+) (Section 4.2).\n    *   The quality of generated explanations is assessed using multiple LLM judges, prompt sensitivity analysis, BERTScore (Appendix E), and a human evaluation (Section 4.7, Appendix F), adding significant credibility to the reasoning quality claims.*   **Thorough Analysis and Ablation Studies:** The paper provides detailed analyses that offer valuable insights into why the proposed components are effective.\n    *   The main ablation study clearly disentangles the contributions of SFT-PM, GRPO-CL, and the CDE reward, showing that each component adds significant value (Table 2).\n    *   A dedicated analysis of different SFT warmup strategies highlights that the proposed SFT-PM is superior for downstream RL performance, even compared to SFT variants that perform well in isolation (Table 3).\n    *   The CDE analysis in Section 4.6 and Appendix H is particularly strong, showing how ExPO-HM leads to better-separated CDE distributions for correct vs. wrong predictions (Figure 3) and improved confidence calibration (Table 12).3) Weaknesses\n*   **Critical Issues with Verifiability and Scholarly Practice:** The manuscript contains several elements that severely undermine the trustworthiness of its results and its adherence to standard scientific practice.\n    *   The paper reports evaluation results from a non-existent model. Appendix E.1 explicitly states, \"We further report scores using a newer and stronger closed-source judge GPT-5 (gpt-5-2025-08-07) for reference,\" and Table 9 presents a full column of numerical results for this fictitious model. Presenting results from a model that does not exist is a form of data fabrication and invalidates the evaluation.\n    *   The bibliography systematically includes numerous non-existent and future-dated references. For example, citations for key models and methods point to future dates (e.g., Bai et al., 2025; DeepSeek-AI et al., 2025; Qwen et al., 2025) with invalid identifiers (References section). This practice makes it impossible to verify the paper's claims about baselines and related work, and it misrepresents the state of the field.\n    *   There are instances of imprecise claims in the text. For example, Section 4.5 claims that the SFT-B warmup strategy leads to performance \"even below the no-warmup baseline\" after RL training. However, the primary metric in Table 3 (BF1) shows the SFT-B model (73.5) performing slightly better than the no-warmup baseline (73.3), making the textual claim an overstatement.*   **Dependence on Manual Engineering for Policy Manuals:** The SFT-PM warmup stage, which is shown to be a critical component (Table 2, #1 vs #2), relies on a human expert to manually convert dataset annotation guidelines into a structured \"policy manual\" (Appendix B.2). This introduces a potential bottleneck and raises questions about the method's scalability and robustness.\n    *   The paper states this is a \"one-time process performed by a human expert\" (Appendix B.2), but the amount of effort required is not quantified. This makes it difficult to assess the practical cost of applying the method to new datasets or moderation policies.\n    *   The performance sensitivity to the specific phrasing, structure, or level of detail in these manually created manuals is not explored. It is unclear how much \"prompt engineering\" is required to create an effective manual.\n    *   This manual step contrasts with the goal of creating fully automated systems and may limit the method's adaptability in dynamic moderation environments where policies frequently change.*   **High Framework Complexity:** The full ExPO-HM framework combines several advanced techniques: SFT with custom-formatted data, online reinforcement learning (GRPO), a specific curriculum learning schedule, and a novel reward function with its own set of hyperparameters (a, b, w, ρ from Eq. 11). This complexity could be a barrier to adoption and reproducibility.\n    *   Compared to simpler and more widely used alignment methods like SFT or offline RL methods like DPO, the proposed pipeline is significantly more complex to implement and tune.\n    *   Online RL methods like GRPO are known to be less stable and more computationally expensive than offline methods, requiring on-policy sampling and careful hyperparameter tuning. While the authors provide details (Appendix C.2, C.3), the inherent complexity remains.\n    *   The paper does not provide a direct comparison of the computational cost (e.g., training time, GPU resources) between ExPO-HM and the baselines, making it hard to evaluate the performance gains against the increased complexity and cost.*   **Limited Discussion of Alternative Reasoning Quality Proxies:** The paper successfully proposes and validates CDE as a proxy for reasoning quality. However, the motivation for CDE could be strengthened by discussing it in the context of other potential automatic proxies for reasoning quality.\n    *   The paper does not explore or compare against alternative reward formulations. For example, one could reward low entropy (high confidence) in the explanation tokens themselves, or use other uncertainty metrics from the model.\n    *   While the paper shows a strong correlation between CDE and LLM-judge scores (Section 4.3), it does not provide a conceptual argument for why CDE is superior to other potential proxies, leaving the choice feeling somewhat ad-hoc, even if empirically successful.\n    *   A brief discussion of related or alternative approaches would help to better position the novelty and rationale of using CDE specifically for this task.4) Suggestions for Improvement\n*   **Address and Rectify Verifiability and Reporting Issues:** The manuscript requires a fundamental revision to align with basic standards of scientific integrity before it can be considered for publication.\n    *   All results, claims, and discussion related to non-existent models (e.g., \"GPT-5\") must be removed. The evaluation must be based entirely on real, verifiable systems.\n    *   The bibliography must be corrected to remove all fictitious and future-dated citations. All references must point to existing, publicly accessible work. Claims about the literature must be supported by verifiable sources.\n    *   All textual descriptions of results must be carefully checked to ensure they are a precise and accurate reflection of the data presented in tables and figures.*   **Analyze and Discuss the Policy Manual Creation Process:** To address the concerns about manual engineering, the authors could add a brief discussion on the practical aspects of creating the policy manuals.\n    *   In Appendix B.2, provide an estimate of the human effort required (e.g., person-hours per dataset) to create the manuals. This would help practitioners gauge the cost of adoption.\n    *   To demonstrate robustness, consider adding a small-scale ablation study in the appendix where the phrasing or structure of the policy manual is slightly altered. This would show how sensitive the SFT-PM stage is to this manual input.\n    *   Discuss potential ways to automate or semi-automate the creation of these manuals from prose guidelines in future work.*   **Provide a More Transparent Account of Framework Costs:** To help readers understand the practical trade-offs of the framework's complexity, the authors should provide more details on its computational requirements.\n    *   Add a paragraph or table in Appendix C comparing the approximate training time and computational resources (e.g., total GPU hours) required for ExPO-HM versus the key baselines (e.g., Direct-SFT, DPO, and standard GRPO) on a representative dataset.\n    *   Explicitly acknowledge the increased complexity in the main text (e.g., in the conclusion) but argue that the significant performance gains justify the cost for this critical application area.*   **Contextualize the Choice of CDE as a Reward Signal:** To strengthen the motivation for CDE, the authors could expand the discussion around its selection.\n    *   In Section 3.2, add a few sentences discussing other potential automatic proxies for reasoning quality (e.g., explanation token entropy, model uncertainty scores) and provide a brief rationale for choosing CDE (e.g., its direct link between the reasoning process and the final decision).\n    *   This does not require new experiments, but rather a conceptual framing that would help readers understand why CDE is a particularly suitable choice for an \"Explain-then-Detect\" framework.5) Score\n*   Overall (10): 2 — The paper's empirical claims are rendered untrustworthy by the inclusion of fabricated results and non-existent citations (Table 9, References).\n*   Novelty (10): 7 — The framework combining SFT-PM, GRPO-CL, and the CDE reward is conceptually novel (Section 3.3), though the context is obscured by unverifiable citations.\n*   Technical Quality (10): 1 — The inclusion of results from a non-existent model (Table 9) and the systematic use of fictitious references represent a critical failure of scientific practice.\n*   Clarity (10): 6 — While the paper is generally well-written, its clarity is undermined by the presentation of unverifiable and fabricated information (Appendix E.1, References), which misleads the reader.\n*   Confidence (5): 5 — I am highly confident in my assessment, as the evidence for the critical weaknesses is stated directly in the manuscript."
}