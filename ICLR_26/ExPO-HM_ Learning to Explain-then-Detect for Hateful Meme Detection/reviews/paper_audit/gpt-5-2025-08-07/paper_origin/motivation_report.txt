# Innovation & Motivation Report

## 1. The Authors' Claimed Contribution
- Core Problem: Detecting hateful memes in multimodal content and providing actionable, policy-relevant explanations (targets, attack types, stance) rather than only binary labels, to better support moderation.
- Claimed Gap: “Explain-then-Detect methods using CoT or LMM agents underperform SFT, and GRPO fails to bridge the gap.” (Abstract). In the Introduction, the authors further specify: “Prior attempts: CoT prompting and agent-based frameworks perform worse than direct SFT; GRPO post-training still underperforms for hateful memes.” They diagnose two issues: “explanations fail to identify correct violated policy/target; binary rewards provide insufficient learning signal.”
- Proposed Solution: ExPO-HM, a three-part training/evaluation framework: (1) SFT warmup augmented with policy manuals derived from fine-grained labels and guidelines (SFT-PM), (2) GRPO with curriculum learning that first optimizes on fine-grained tasks then mixes binary and fine-grained (“Ordering … is crucial”), and (3) Conditional Decision Entropy (CDE) used both as a reasoning-quality metric and as a reward term to encourage calibrated confidence (“r(y_ig, d_i^*) = r_format + r_acc + w r_CDE”). The model is structured to “generate a rationale and then a decision” with output y ≡ <think> e </think> <answer> d </answer> and explicitly models πθ(d | e, x).

## 2. Comparative Scrutiny (The "Trial")
Here, we analyze how the paper stands against the identified similar works.

### vs. MemeIntel: Explainable Detection of Propagandistic and Hateful Memes
- Identified Overlap: Both advocate staged optimization for explanation-enhanced meme detection and recognize that naïvely training rationales alongside labels can degrade classification; both target multimodal memes and aim to improve explanation quality and detection.
- Manuscript's Defense:
  - Citation: The manuscript does not explicitly cite MemeIntel/MemeXplain. It does acknowledge the broader space and dataset constraints: “Explain-then-Detect: Limited research; some systems use LLM debate or reasoning agents with retrieval/reflection, targeting binary classification. Only Hatred (Hee et al., 2023) provides human-written rationales; other datasets (e.g., Arabic ArMeme) not public. Existing explainable systems underperform direct detection.” (Related Work).
  - Technical distinction: The method introduces a new confidence-aware mechanism, Conditional Decision Entropy, “rewarding confident correctness, tolerating uncertainty when wrong, penalizing confident errors” (Method, Eq. 11; default hyperparameters and stable ranges specified), and a specific curriculum schedule (“first ‘50%’ of steps on fine-grained … remaining ‘50%’ on a ‘50/50’ mix,” with ordering crucial).
- Reviewer’s Assessment: The staged learning paradigm overlaps conceptually with MemeIntel’s multi-stage optimization. However, ExPO-HM’s CDE formulation (both metric and reward), the explicit conditioning πθ(d|e,x), and a policy-manual warmup are substantive methodological differentiators within the same paradigm. The quantitative margins (e.g., HatefulMemes AttackF1 +14.4 vs GRPO; reductions in CDE from “0.26” to “0.03”) indicate the distinction is impactful. Lack of direct citation/benchmark against MemeIntel modestly weakens the positioning, but the defense—novel reward shaping and curriculum targeting policy cues—is credible.

### vs. “Towards Explainable Harmful Meme Detection through Multimodal Debate between Large Language Models”
- Identified Overlap: Both reject binary-only outcomes and produce explanations prior to decisions; the debate-plus-judge idea parallels ExPO-HM’s “rationale → decision” conditioning and calibration emphasis.
- Manuscript's Defense:
  - Citation: Debate-style systems are acknowledged generically and via baselines: “some systems use LLM debate or reasoning agents with retrieval/reflection, targeting binary classification.” (Related Work). The paper benchmarks agentic/CoT systems LOREHM and U-CoT+: “Explain-then-Detect baselines often underperform Direct SFT: e.g., U-CoT+ HatefulMemes BF1 ‘72.4’.” (Table 1).
  - Technical distinction: ExPO-HM eschews a separate judge and instead optimizes the conditional decision distribution with CDE. It formalizes decision conditioning as πθ(d|e,x) and uses CDE to “penalize confident errors” while improving calibration (ECE/Brier).
- Reviewer’s Assessment: The manuscript directly tests representative agent/debate approaches and shows they underperform. Positioning ExPO-HM as a policy-grounded, optimization-based alternative is sound. Distinction is significant and well supported by head-to-head baselines and by the strong negative CDE–LLM judge correlation (Pearson “−0.78”).

### vs. IntMeme: Leveraging LMMs for hateful meme classification with explainable decisions
- Identified Overlap: Both use LMMs to produce explanations that improve classification; both aim for interpretive, human-like analyses to support moderation decisions.
- Manuscript's Defense:
  - Citation: IntMeme is not explicitly cited. The manuscript generalizes: “Existing explainable systems underperform direct detection.” (Related Work).
  - Technical distinction: ExPO-HM formalizes explanation-conditioned decisions and introduces CDE as a learning signal and evaluation metric; includes a curriculum focused on fine-grained policy cues first and a policy-manual SFT warmup (Table 3 shows SFT-PM leads to the best post-RL performance: BF1 ‘81.1’).
- Reviewer’s Assessment: The architectural idea of separating interpretation and decision is shared. ExPO-HM’s contribution lies in a concrete RL-based conditioning and calibration mechanism (CDE) and in demonstrating SOTA across binary/fine-grained/reasoning. Absent direct comparisons, novelty is primarily in training design and metric/reward rather than in the high-level paradigm.

### vs. RA-HMD: Robust Adaptation of LMMs for Hateful Meme Detection
- Identified Overlap: Both adapt LMMs for hateful memes and discuss rationale quality; RA-HMD focuses on retrieval-augmented robustness and generalization; ExPO-HM reframes success to include explanation-first decisions and policy cues.
- Manuscript's Defense:
  - Citation and head-to-head: RA-HMD is explicitly cited as prior SOTA. The paper reports RA-HMD results and compares: “RA-HMD (Qwen2.5-VL-7B) … HatefulMemes BF1 ‘80.2’” vs ExPO-HM “BF1 ‘81.1’ … LLM ‘6.2’ … CDE ‘0.03’.” (Table 1).
  - Technical distinction: ExPO-HM replaces retrieval augmentation with policy-manual conditioning and adds curriculum learning plus CDE reward. It claims “First Explain-then-Detect hateful meme detection system that outperforms direct detection.” (Introduction, Contributions).
- Reviewer’s Assessment: The defense is strong: direct comparison against RA-HMD on identical backbones with clear margins and broader evaluation (fine-grained, reasoning, calibration). The shift from robustness-centric retrieval to calibrated explanation-first optimization is a meaningful methodological change with demonstrated benefit.

### vs. GRPO (Group Relative Policy Optimization)
- Identified Overlap: ExPO-HM builds directly on GRPO’s group-sampled, KL-regularized policy gradient; both are post-training approaches to improve reasoning/decisions.
- Manuscript's Defense:
  - Citation and failure mode: The paper cites GRPO and states: “GRPO post-training still underperforms for hateful memes.” (Introduction). Observations show “Naive post-training (SFT/DPO/GRPO) … fails to meaningfully improve reasoning; e.g., … LLM-as-a-judge ‘5.0’ (zero-shot) → ‘4.9’ (DPO) or ‘5.2’ (GRPO).” (Sec. 4.3).
  - Technical distinction: Adds a staged curriculum and a CDE-based reward to address weak binary signals and overconfident errors. Ablation demonstrates stepwise gains: CDE reduces CDE “0.263” → “0.026” and boosts LLM judge “5.2” → “6.2” (Table 2).
- Reviewer’s Assessment: The differentiation is concrete and validated by ablations and large effect sizes across datasets and model sizes. This is a substantive extension of GRPO tailored to explanation-conditioned decisions.

## 3. Novelty Verdict
- Innovation Type: Incremental
- Assessment:
  The paper’s motivation—moving from binary alarms to policy-grounded, explanation-first detection—is well articulated and consistently evidenced. While the high-level paradigm (staged optimization; explanations conditioning decisions) is present in prior works, ExPO-HM advances it with a specific and measurable innovation: Conditional Decision Entropy as both a metric and reward, coupled with a fine-grained-first curriculum and policy-manual warmup. These choices materially improve accuracy, reasoning quality, and calibration over strong baselines (including GRPO and RA-HMD), and the manuscript substantiates the claim that prior explainable approaches underperform direct detection with explicit baselines (LOREHM, U-CoT+).
  - Strength:
    • Clear gap identification and policy-centered rationale for Explain-then-Detect: “Moderators need context (attack type, targeted group) and reasons for harmfulness…” (Introduction).
    • Strong empirical defense with comprehensive evaluation and ablations; consistent gains in binary, fine-grained, and reasoning metrics; improved calibration (ECE, Brier).
    • Explicit conditioning πθ(d|e,x) and principled CDE reward that penalizes confident errors reinforce the motivation (CDE–LLM judge correlation Pearson “−0.78”).
    • Direct comparison to a leading direct-detection baseline (RA-HMD) shows SOTA improvements, supporting the “first … that outperforms direct detection” claim within Explain-then-Detect.
  - Weakness:
    • Limited direct engagement with closely related explanation-first, staged optimization works (e.g., MemeIntel, IntMeme) beyond generic acknowledgment; lack of head-to-head comparisons weakens the “first” framing in a crowded space.
    • Reliance on LLM-as-a-judge for reasoning quality introduces evaluation dependence; although mitigations and human assessments are reported, they remain proxy measures.
    • The core RL machinery (GRPO) and staged training are established; novelty primarily resides in reward shaping (CDE) and curriculum design, which reads as methodological engineering rather than new theoretical foundations.

## 4. Key Evidence Anchors
- Abstract: “Explain-then-Detect methods using CoT or LMM agents underperform SFT, and GRPO fails to bridge the gap… ExPO-HM combining SFT warmup, GRPO with curriculum learning, and Conditional Decision Entropy (CDE) as metric and reward.”
- Introduction (Contributions): “Paradigm: First Explain-then-Detect hateful meme detection system that outperforms direct detection.” “Two challenges: explanations fail to identify correct violated policy/target; binary rewards provide insufficient learning signal.”
- Related Work: “Explain-then-Detect: Limited research; some systems use LLM debate or reasoning agents… Only Hatred (Hee et al., 2023) provides human-written rationales; other datasets (e.g., Arabic ArMeme) not public. Existing explainable systems underperform direct detection… introduces ExPO-HM surpassing both explainable and direct approaches.”
- Method:
  • Output conditioning: y ≡ <think> e </think> <answer> d </answer>; πθ(d|e,x).
  • Curriculum: “Two-stage schedule (‘50/50/50’)… Ordering … is crucial.”
  • Reward: “r(y_ig, d_i^*) = r_format + r_acc + w r_CDE,” with CDE definition and piecewise reward (Eq. 11) and default hyperparameters (a, b, w, ρ).
- Experiments:
  • Main results (Table 1): ExPO-HM Qwen2.5-VL-7B HatefulMemes BF1 “81.1” vs RA-HMD “80.2”; AttackF1 “75.6”; TargetF1 “77.2”; LLM “6.2”; CDE “0.03”.
  • Baseline shortfalls: U-CoT+ HatefulMemes BF1 “72.4”; GRPO reasoning “5.2” vs ExPO-HM “6.2” (Sec. 4.3).
  • Ablation (Table 2): progressive gains from SFT-PM → GRPO-CL → CDE; CDE reduced “0.263” → “0.026”.
  • Warmup comparison (Table 3): SFT-PM yields the best post-RL performance (BF1 “81.1”).
  • Calibration: ECE/Brier improvements (e.g., 3B Brier “0.590 → 0.283”; 7B ECE “0.160”, Brier “0.214”).
  • CDE–LLM judge correlation: Pearson “−0.78”, Spearman “−0.81” (p < “0.001”).
- Observations (Sec. 4.3): “Baseline Explain-then-Detect post-training hurts classification vs Direct SFT and RA-HMD; larger agentic/CoT systems also fall short… ExPO-HM consistently outperforms across tasks and datasets.”