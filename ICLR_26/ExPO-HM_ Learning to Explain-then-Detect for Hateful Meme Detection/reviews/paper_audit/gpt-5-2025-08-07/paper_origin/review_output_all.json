{
  "baseline_review": "Summary\n   - The paper proposes ExPO-HM, an Explain-then-Detect framework for hateful meme detection that first produces a rationale and then a decision. The method combines three components: (i) supervised warmup using policy-manual–augmented prompts (SFT-PM), (ii) Group-Relative Policy Optimization with a curriculum from fine-grained to binary tasks (GRPO-CL), and (iii) Conditional Decision Entropy (CDE) used both as a metric and as a reward to shape reasoning confidence (Sec. 3.2–3.3; Eq. 7–11; Fig. 2). Experiments on HatefulMemes, MAMI, and PrideMM show state-of-the-art performance across binary detection, fine-grained classification, and reasoning quality, including consistent gains over SFT, DPO, GRPO, and prior agentic CoT systems (Table 1). Ablations attribute gains to all three components (Table 2–3), and further analyses demonstrate correlation between CDE and LLM-as-a-judge, improved calibration, and positive human evaluations (Sec. 4.3–4.7; Fig. 3; Table 11–12).Strengths\n- **Clear motivation grounded in real-world moderation needs**\n  - The introduction argues that moderators require targets, attack types, and rationales beyond binary flags (Sec. 1), aligning the task with policy-guided human annotation practice; this matters for impact and task relevance.\n  - The Explain-then-Detect formulation with <think>/<answer> explicitly structures outputs (Eq. 2), improving interpretability and actionability.\n  - The policy-manual idea directly mirrors guideline-driven moderation (Sec. 3.3; Appendix B.2), enhancing clarity and practical applicability.- **Novel CDE metric and reward for reasoning quality**\n  - CDE is formally defined as entropy over decisions conditioned on on-policy explanations (Sec. 3.2; Eq. 7–8), providing a verifiable proxy for reasoning confidence; this is technically novel.\n  - A piecewise reward function penalizes confident errors and rewards confident correctness (Eq. 10–11), improving technical soundness of RL signals.\n  - Correlation analyses show strong negative correlation with LLM-as-a-judge (Pearson r = −0.78; Spearman ρ = −0.81; p < 0.001; Sec. 4.3), supporting its validity as a metric.\n  - Theoretical discussion links CDE optimization to improved calibration with bounded Brier loss under ideal policy (Appendix H.2), strengthening rigor.- **Well-designed training pipeline that mimics human processes**\n  - SFT-PM teaches policy concepts via structured manuals built from dataset guidelines (Sec. 3.3; Appendix B.2–B.3), increasing conceptual alignment—important for generalization.\n  - GRPO-CL schedules fine-grained reasoning before binary detection (Sec. 3.3), producing longer, more detailed rationales (52 vs. 28 tokens; Sec. 4.4), indicating the curriculum benefits reasoning exploration.\n  - The architecture summary (Fig. 2) provides a clear blueprint for reproducibility and future work.- **Comprehensive, multi-dimensional evaluation**\n  - Results span binary F1, fine-grained micro F1 (attack types and targets), and reasoning (LLM-as-a-judge and CDE) across three datasets and two model sizes (Table 1), demonstrating breadth and rigor.\n  - Ablations isolate contributions of SFT-PM, GRPO-CL, and CDE (Table 2) and analyze warmup strategies (Table 3), supporting causality.\n  - Per-class fine-grained analyses (Tables 13–14) reveal class-specific gains, improving interpretability and diagnosing model behavior.- **Consistent, substantial empirical gains**\n  - On HatefulMemes with Qwen2.5-VL-7B, ExPO-HM improves AttackF1 from 61.2 (GRPO, #20) to 75.6 (#21) and TargetF1 from 64.5 (#20) to 77.2 (#21) (Table 1), evidencing strong improvements.\n  - Binary F1 surpasses RA-HMD (80.2; #7) with 81.1 (#21), indicating that Explain-then-Detect can match or exceed direct detection (Table 1).\n  - Reasoning scores improve from 5.2 (GRPO #20) to 6.2 (ExPO-HM #21) on HatefulMemes (Table 1), and CDE drops markedly (0.26 → 0.03), evidencing tighter decision confidence (Table 1).- **Calibration benefits tied to CDE**\n  - CDE reward improves calibration: ECE and Brier scores decrease across model sizes (Table 12), which matters for reliability in safety-critical moderation.\n  - CDE distributions separate correct vs. incorrect predictions (Fig. 3; Sec. 4.6), confirming intended behavior (technical soundness).- **Attention to reproducibility and implementation details**\n  - Training details include LoRA configuration, frozen vision encoders, libraries and versions, compute budget, and curriculum split (Appendix C; Sec. 4.2), facilitating replication.\n  - Hyperparameter tuning ranges and stability for CDE are documented (Appendix C.3), increasing robustness and practical usability.- **Human evaluation supporting explanation quality**\n  - Coherence reaches 100% vs. 96% (Sec. 4.7), and helpfulness improves (2.2 vs. 1.6 on 0–4 scale; Table 11), aligning with LLM-as-a-judge trends, adding external validity.Weaknesses\n- **Primary reliance on LLM-as-a-judge with limited human evaluation scale**\n  - The main reasoning metric uses a closed-source judge (GPT-4o mini; Appendix D; Table 1), which can introduce reproducibility and bias concerns; while alternative judges are reported (Appendix E.1; Table 9), the headline results emphasize GPT-4o mini, limiting robustness.\n  - Prompt sensitivity is only tested with one paraphrase (Appendix E.2; Table 10), which may not capture broader prompt variance or adversarial judge prompts—important for evaluation stability.\n  - Human evaluation reports coherence/helpfulness but does not specify the sample size of annotated memes in Sec. 4.7 (beyond “three annotators per example”), constraining statistical power; although Krippendorff’s αordinal = 0.71 is reported (Appendix F.2; Table 11), more detail on the sample would improve rigor.\n  - Potential alignment issues between judge preferences and policy manuals are not analyzed (No direct evidence found in the manuscript), risking metric-policy mismatch in downstream deployment.- **CDE design may incentivize hedging and has approximation limits**\n  - The reward explicitly “tolerates uncertainty when wrong” (Eq. 11; Sec. 3.2–3.3), which could incentivize ambiguous reasoning rather than correction—this matters for moderation usefulness under mistakes.\n  - CDE uses the full decision vocabulary (Sec. 3.2) and a top-k token approximation for entropy (Appendix G.1), which may mix label semantics with general tokens and introduce estimator bias.\n  - Monte Carlo estimation with K=16 explanations (Eq. 8) and top-10–50 token entropy approximations (Appendix G.1) can yield variance and potential instability, yet variance analyses are not provided (No direct evidence found in the manuscript).\n  - Theoretical calibration analysis (Appendix H.2) relies on an “ideal ExPO-HM policy” and thresholded entropy assumptions, but empirical linkage from reward thresholds (a, b) to observed calibration is not quantified beyond aggregate metrics (Table 12), limiting mechanistic validation.- **Fairness and group-level risk are under-examined**\n  - Group-wise false positive/false negative rates or equalized odds across protected categories are not reported (No direct evidence found in the manuscript), leaving fairness questions open for deployment.\n  - Per-class F1 in Table 14 shows large disparities (e.g., Nationality and Disability), but fairness-specific diagnostics (e.g., subgroup calibration or error parity) are absent, limiting bias assessment.\n  - MAMI fine-grained distribution is highly imbalanced (Table 6), and micro F1 can be dominated by frequent labels; complementary macro/weighted metrics or threshold analyses are not reported (No direct evidence found).\n  - PrideMM target and stance tasks (Table 7; Table 1) lack per-class fairness metrics (No direct evidence found), making it hard to evaluate differential impacts on LGBTQ+ subgroups.- **Dataset and reasoning coverage limitations**\n  - Reasoning evaluation uses Hatred (hateful-only explanations; Appendix B.4; Table 8; Sec. 4.1), so benign rationales are not assessed, potentially biasing the reasoning benchmark toward harmful cases.\n  - Cross-dataset reasoning generalization beyond HatefulMemes is not evaluated (Sec. 4.1; Table 1), leaving uncertainty about transfer to other domains and meme styles.\n  - Error analysis highlights failures on implicit or knowledge-dependent memes (Appendix J.2; Table 17; images), but the method does not incorporate retrieval or external knowledge during training/inference, limiting coverage of subtle cases.\n  - Language and cultural generalization are not addressed (No direct evidence found), and all datasets are primarily English (Appendix B), limiting broader applicability.- **Reproducibility and artifact availability constraints**\n  - The paper states strict access controls for model artifacts (Appendix: Intended use; Block #39), which may limit reproducibility for academic benchmarking beyond approved users.\n  - Policy manual construction is described (Appendix B.2), but the exact manuals/artifacts used for training are not stated as publicly released (No direct evidence found), making replication harder.\n  - While implementation details are thorough (Appendix C), code release is not promised (No direct evidence found), constraining independent verification.\n  - Seed management and statistical significance testing of improvements are not reported (Sec. 4.3 references 60 points for correlation, but main tables show point estimates), which reduces confidence in the robustness of reported gains.- **Clarity and minor inconsistencies**\n  - Sec. 3.3 says gold explanations are “off-policy and lead to worse performance,” but Table 3 shows SFT-R becomes competitive after GRPO-CL+CDE (e.g., BF1 79.2 vs. 78.9 for SFT-FG), suggesting a nuanced picture that is not clearly reconciled.\n  - Table 1 includes “RCDE ↓” for MAMI and PrideMM without explicitly defining RCDE vs. CDE in the main text (No explicit definition found), creating minor nomenclature ambiguity.\n  - The accuracy reward for multi-label fine-grained classification is referenced with “partial credit” and “penalties for over-prediction” (Sec. 3.3; r_acc) but no explicit formula is given, limiting replicability of the reward design.\n  - The curriculum scheduler rationale is described qualitatively (Sec. 3.3: “find similar performance… adopt 50/50/50”), but quantitative sensitivity analyses are limited (No direct evidence found), which would strengthen clarity about scheduler choices.Suggestions for Improvement\n- **Strengthen evaluation robustness beyond LLM-as-a-judge**\n  - Report headline reasoning results using multiple judges (e.g., open-source in Appendix E.1; Table 9) alongside GPT-4o mini in the main tables (Table 1), and include variance across judges to quantify robustness.\n  - Expand prompt sensitivity beyond a single paraphrase (Appendix E.2; Table 10) by testing diverse templates and adversarial phrasing; report the variance across prompts.\n  - Specify the number of examples used in human evaluation (Sec. 4.7) and provide confidence intervals; complement Krippendorff’s α with sample size and per-rater statistics (Appendix F.2; Table 11).\n  - Analyze potential alignment biases between policy manuals and judge preferences (No direct evidence found) by comparing scores when policy manual phrasing is varied or ablated.- **Deepen CDE validation and mitigate hedging**\n  - Add experiments that detect and penalize hedging behavior (Eq. 11; Sec. 3.2–3.3), e.g., measure entropy vs. correctness under adversarial prompts that encourage ambiguity; quantify trade-offs with r_acc.\n  - Compare full-vocabulary CDE (Sec. 3.2) to label-collapsed CDE for multi-class settings, reporting impact on both calibration and accuracy; include confusion analyses to ensure decision tokens reflect label semantics.\n  - Provide variance analyses for the Monte Carlo estimator (Eq. 8) and top-k entropy approximation (Appendix G.1), including sensitivity to K and k; report confidence intervals for CDE estimates.\n  - Link threshold settings (a, b) to observed calibration bins (Table 12) by plotting calibration curves stratified by CDE ranges, validating the theoretical ideal-policy assumptions (Appendix H.2) empirically.- **Add fairness-oriented diagnostics**\n  - Report subgroup-level FPR/FNR and equalized odds across protected categories (No direct evidence found), complementing Table 14 per-class F1 with fairness metrics relevant to moderation.\n  - Analyze disparity measures (e.g., worst-group accuracy, subgroup calibration) for protected categories where per-class F1 is low (Table 14), and discuss mitigation strategies (reweighting or targeted augmentation).\n  - For MAMI’s imbalanced labels (Table 6), include macro/weighted F1 and threshold analyses; assess how micro F1 is affected by frequent labels and report class-wise PR curves.\n  - Provide per-class metrics and fairness diagnostics for PrideMM target/stance tasks (No direct evidence found), including subgroup calibration and error parity.- **Broaden dataset and reasoning coverage**\n  - Augment reasoning evaluation with benign rationales (Appendix B.4; Table 8; Sec. 4.1) to measure false-positive explanations and their coherence/helpfulness for benign cases.\n  - Evaluate cross-dataset reasoning transfer by judging explanations on MAMI and PrideMM (Sec. 4.1; Table 1), even without gold rationales, using LLM judges and BERTScore (Appendix E.1), and report consistency.\n  - Incorporate retrieval-augmented training or inference to address implicit/knowledge-dependent memes highlighted in error analysis (Appendix J.2; Table 17), and quantify gains.\n  - Explore multilingual/cultural generalization (No direct evidence found) by testing non-English memes or culturally diverse datasets; discuss adapting policy manuals for locale-specific moderation.- **Improve reproducibility and artifact availability**\n  - Where possible, provide research-only releases of code and evaluation scripts, and document how to request model weights under the stated access controls (Appendix: Intended use; Block #39).\n  - Release the exact policy-manual artifacts used for training (Appendix B.2), including versioning and dataset-specific manuals, to ease replication.\n  - Commit to releasing training seeds, logs, and checkpoints for baselines (Appendix C), or at least report mean/variance over multiple runs for Table 1 to improve statistical confidence.\n  - Include statistical significance tests (e.g., bootstrap confidence intervals) for main results (Sec. 4.3; Table 1) to complement point estimates.- **Tighten clarity and resolve inconsistencies**\n  - Reconcile the claim that gold rationales hurt warmup (Sec. 3.3) with the observation that SFT-R performs competitively after GRPO-CL+CDE (Table 3), by clarifying at which stage and why effects differ.\n  - Define RCDE vs. CDE consistently in the main text and tables (Table 1), and state whether RCDE denotes “CDE reported for reasoning experiments” or a variant estimator.\n  - Provide the explicit formula for r_acc for multi-label fine-grained tasks (Sec. 3.3), detailing partial credit and over-prediction penalties for replicability.\n  - Add quantitative sensitivity analyses for curriculum scheduling choices (Sec. 3.3), e.g., switching criteria, mixing ratios, and plateau detection, and summarize their impact.Score\n  - Overall (10): 8 — Strong empirical improvements across binary, fine-grained, and reasoning with clear ablations and analyses (Table 1; Table 2–3; Sec. 4.3–4.6), tempered by reliance on LLM judges and limited fairness diagnostics.\n  - Novelty (10): 8 — The Conditional Decision Entropy as both metric and reward (Sec. 3.2; Eq. 7–11; Appendix H.2) and policy-manual warmup plus GRPO curriculum (Fig. 2; Sec. 3.3) present a distinctive, well-motivated approach.\n  - Technical Quality (10): 7 — Sound formulation and thorough experiments (Table 1; Fig. 3; Table 12; Appendix G–H), with remaining gaps in fairness analyses, estimator variance, and clearer reward formulae (Sec. 3.3; Table 14).\n  - Clarity (10): 7 — Generally clear with helpful figures and equations (Fig. 2; Eq. 7–11) but minor inconsistencies (Sec. 3.3 vs. Table 3) and undefined RCDE in Table 1 reduce clarity.\n  - Confidence (5): 4 — Review based on a complete manuscript with detailed appendices and results (Sec. 4; Table 1–14), though artifact access constraints (Appendix: Intended use; Block #39) limit independent verification.",
  "final_review": "Summary\n   - The paper proposes ExPO-HM, an Explain-then-Detect framework for hateful meme detection that first produces a rationale and then a decision. The method combines three components: (i) supervised warmup using policy-manual–augmented prompts (SFT-PM), (ii) Group-Relative Policy Optimization with a curriculum from fine-grained to binary tasks (GRPO-CL), and (iii) Conditional Decision Entropy (CDE) used both as a metric and as a reward to shape reasoning confidence (Sec. 3.2–3.3; Eq. 7–11; Fig. 2). Experiments on HatefulMemes, MAMI, and PrideMM show state-of-the-art performance across binary detection, fine-grained classification, and reasoning quality, including consistent gains over SFT, DPO, GRPO, and prior agentic CoT systems (Table 1). Ablations attribute gains to all three components (Table 2–3), and further analyses demonstrate correlation between CDE and LLM-as-a-judge, improved calibration, and positive human evaluations (Sec. 4.3–4.7; Fig. 3; Table 11–12).Strengths\n- **Clear motivation grounded in real-world moderation needs**\n  - The introduction argues that moderators require targets, attack types, and rationales beyond binary flags (Sec. 1), aligning the task with policy-guided human annotation practice; this matters for impact and task relevance.\n  - The Explain-then-Detect formulation with <think>/<answer> explicitly structures outputs (Eq. 2), improving interpretability and actionability.\n  - The policy-manual idea directly mirrors guideline-driven moderation (Sec. 3.3; Appendix B.2), enhancing clarity and practical applicability.\n- **Novel CDE metric and reward for reasoning quality**\n  - CDE is formally defined as entropy over decisions conditioned on on-policy explanations (Sec. 3.2; Eq. 7–8), providing a verifiable proxy for reasoning confidence; this is technically novel.\n  - A piecewise reward function penalizes confident errors and rewards confident correctness (Eq. 10–11), improving technical soundness of RL signals.\n  - Correlation analyses show strong negative correlation with LLM-as-a-judge (Pearson r = −0.78; Spearman ρ = −0.81; p < 0.001; Sec. 4.3), supporting its validity as a metric.\n  - Theoretical discussion links CDE optimization to improved calibration with bounded Brier loss under ideal policy (Appendix H.2), strengthening rigor.\n- **Well-designed training pipeline that mimics human processes**\n  - SFT-PM teaches policy concepts via structured manuals built from dataset guidelines (Sec. 3.3; Appendix B.2–B.3), increasing conceptual alignment—important for generalization.\n  - GRPO-CL schedules fine-grained reasoning before binary detection (Sec. 3.3); the claim of producing longer, more detailed rationales (52 vs. 28 tokens; Sec. 4.4) is stated but not accompanied by a reporting table or figure (No direct evidence found in the manuscript), yet the curriculum ordering is central to the method’s design.\n  - The architecture summary (Fig. 2) provides a clear blueprint for reproducibility and future work.\n- **Comprehensive, multi-dimensional evaluation**\n  - Results span binary F1, fine-grained micro F1 (attack types and targets), and reasoning (LLM-as-a-judge and CDE) across three datasets and two model sizes (Table 1), demonstrating breadth and rigor.\n  - Ablations isolate contributions of SFT-PM, GRPO-CL, and CDE (Table 2) and analyze warmup strategies (Table 3), supporting causality.\n  - Per-class fine-grained analyses (Tables 13–14) reveal class-specific gains, improving interpretability and diagnosing model behavior.\n- **Consistent, substantial empirical gains**\n  - On HatefulMemes with Qwen2.5-VL-7B, ExPO-HM improves AttackF1 from 61.2 (GRPO, #20) to 75.6 (#21) and TargetF1 from 64.5 (#20) to 77.2 (#21) (Table 1), evidencing strong improvements.\n  - Binary F1 surpasses RA-HMD (80.2; #7) with 81.1 (#21), indicating that Explain-then-Detect can match or exceed direct detection (Table 1).\n  - Reasoning scores improve from 5.2 (GRPO #20) to 6.2 (ExPO-HM #21) on HatefulMemes (Table 1), and CDE drops markedly (0.26 → 0.03), evidencing tighter decision confidence (Table 1).\n- **Calibration benefits tied to CDE**\n  - CDE reward improves calibration: ECE and Brier scores decrease across model sizes (Table 12), which matters for reliability in safety-critical moderation.\n  - CDE distributions separate correct vs. incorrect predictions (Fig. 3; Sec. 4.6), confirming intended behavior (technical soundness).\n- **Attention to reproducibility and implementation details**\n  - Training details include LoRA configuration, frozen vision encoders, libraries and versions, compute budget, and curriculum split (Appendix C; Sec. 4.2), facilitating replication.\n  - Hyperparameter tuning ranges and stability for CDE are documented (Appendix C.3), increasing robustness and practical usability.\n- **Human evaluation supporting explanation quality**\n  - Coherence reaches 100% vs. 96% (Sec. 4.7), and helpfulness improves (2.2 vs. 1.6 on 0–4 scale; Table 11), aligning with LLM-as-a-judge trends, adding external validity.Weaknesses\n- **Primary reliance on LLM-as-a-judge with limited human evaluation scale**\n  - The main reasoning metric uses a closed-source judge (GPT-4o mini; Appendix D; Table 1), which can introduce reproducibility and bias concerns; while alternative judges are reported (Appendix E.1; Table 9), the headline results emphasize GPT-4o mini, limiting robustness.\n  - Prompt sensitivity is only tested with one paraphrase (Appendix E.2; Table 10), which may not capture broader prompt variance or adversarial judge prompts—important for evaluation stability.\n  - Human evaluation reports coherence/helpfulness but does not specify the sample size of annotated memes in Sec. 4.7 (beyond “three annotators per example”), constraining statistical power; although Krippendorff’s αordinal = 0.71 is reported (Appendix F.2; Table 11), more detail on the sample would improve rigor.\n  - Potential alignment issues between judge preferences and policy manuals are not analyzed (No direct evidence found in the manuscript), risking metric-policy mismatch in downstream deployment.\n- **CDE design may incentivize hedging and has approximation limits**\n  - The reward explicitly “tolerates uncertainty when wrong” (Eq. 11; Sec. 3.2–3.3), which could incentivize ambiguous reasoning rather than correction—this matters for moderation usefulness under mistakes.\n  - CDE uses the full decision vocabulary (Sec. 3.2) and a top-k token approximation for entropy (Appendix G.1), which may mix label semantics with general tokens and introduce estimator bias.\n  - Monte Carlo estimation with K=16 explanations (Eq. 8) and top-10–50 token entropy approximations (Appendix G.1) can yield variance and potential instability, yet variance analyses are not provided (No direct evidence found in the manuscript).\n  - Theoretical calibration analysis (Appendix H.2) relies on an “ideal ExPO-HM policy” and thresholded entropy assumptions, but empirical linkage from reward thresholds (a, b) to observed calibration is not quantified beyond aggregate metrics (Table 12), limiting mechanistic validation.\n  - The CDE reward weight appears in both the overall reward r(yig, di*) (Eq. 9: “w r_CDE”) and inside the piecewise definition of r_CDE (Eq. 11: terms scaled by w), while Sec. 3.3 states “CDE rewards contribute a maximum of weight w”; this risks an unintended double weighting that affects optimization and reproducibility (technical soundness).\n  - Entropy units are not consistently specified: Eq. 7 uses “log” without base (Sec. 3.2), whereas Appendix H.2 defines binary entropy H2 in bits (Eq. 20) and derives p_a, p_b from a=0.1, b=0.5 without clarifying unit conversion; this undermines the quantitative correctness of the theoretical bounds (clarity/rigor).\n- **Fairness and group-level risk are under-examined**\n  - Group-wise false positive/false negative rates or equalized odds across protected categories are not reported (No direct evidence found in the manuscript), leaving fairness questions open for deployment.\n  - Per-class F1 in Table 14 shows large disparities (e.g., Nationality and Disability), but fairness-specific diagnostics (e.g., subgroup calibration or error parity) are absent, limiting bias assessment.\n  - MAMI fine-grained distribution is highly imbalanced (Table 6), and micro F1 can be dominated by frequent labels; complementary macro/weighted metrics or threshold analyses are not reported (No direct evidence found).\n  - PrideMM target and stance tasks (Table 7; Table 1) lack per-class fairness metrics (No direct evidence found), making it hard to evaluate differential impacts on LGBTQ+ subgroups.\n- **Dataset and reasoning coverage limitations**\n  - Reasoning evaluation uses Hatred (hateful-only explanations; Appendix B.4; Table 8; Sec. 4.1), so benign rationales are not assessed, potentially biasing the reasoning benchmark toward harmful cases.\n  - Cross-dataset reasoning generalization beyond HatefulMemes is not evaluated (Sec. 4.1; Table 1), leaving uncertainty about transfer to other domains and meme styles.\n  - Error analysis highlights failures on implicit or knowledge-dependent memes (Appendix J.2; Table 17; images), but the method does not incorporate retrieval or external knowledge during training/inference, limiting coverage of subtle cases.\n  - Language and cultural generalization are not addressed (No direct evidence found), and all datasets are primarily English (Appendix B), limiting broader applicability.\n- **Reproducibility and artifact availability constraints**\n  - The paper states strict access controls for model artifacts (Appendix: Intended use; Block #39), which may limit reproducibility for academic benchmarking beyond approved users.\n  - Policy manual construction is described (Appendix B.2), but the exact manuals/artifacts used for training are not stated as publicly released (No direct evidence found), making replication harder.\n  - While implementation details are thorough (Appendix C), code release is not promised (No direct evidence found), constraining independent verification.\n  - Seed management and statistical significance testing of improvements are not reported (Sec. 4.3 references 60 points for correlation, but main tables show point estimates), which reduces confidence in the robustness of reported gains.\n  - Evaluation split usage for fine-grained metrics is not made explicit in the main results table: Appendix B.3 notes dev_unseen for final evaluation (Table 5), but Table 1 does not annotate which splits were used for AttackF1/TargetF1 vs. BF1, complicating exact replication.\n- **Clarity and minor inconsistencies**\n  - Sec. 3.3 says gold explanations are “off-policy and lead to worse performance,” but Table 3 shows SFT-R becomes competitive after GRPO-CL+CDE (e.g., BF1 79.2 vs. 78.9 for SFT-FG), suggesting a nuanced picture that is not clearly reconciled.\n  - Table 1 includes “RCDE ↓” for MAMI and PrideMM without explicitly defining RCDE vs. CDE in the main text (No explicit definition found), creating minor nomenclature ambiguity.\n  - The accuracy reward for multi-label fine-grained classification is referenced with “partial credit” and “penalties for over-prediction” (Sec. 3.3; r_acc) but no explicit formula is given, limiting replicability of the reward design.\n  - The curriculum scheduler rationale is described qualitatively (Sec. 3.3: “find similar performance… adopt 50/50/50”), but quantitative sensitivity analyses are limited (No direct evidence found), which would strengthen clarity about scheduler choices.\n  - In Sec. 4.6, the text highlights a Brier reduction “from 0.590 to 0.283” when discussing improvements over GRPO, but 0.590 is the zero-shot value; the GRPO baseline is 0.441 (Table 12), so the correct comparison vs. GRPO should be 0.441 → 0.283 (clarity).\n  - Sec. 4.5 states SFT-B is “even below the no-warmup baseline,” yet Table 3 shows post-RL BF1 is marginally higher for SFT-B (73.5) than no warmup (73.3), while SFT-B underperforms on fine-grained metrics; the statement should be qualified by metric (clarity).Suggestions for Improvement\n- **Strengthen evaluation robustness beyond LLM-as-a-judge**\n  - Report headline reasoning results using multiple judges (e.g., open-source in Appendix E.1; Table 9) alongside GPT-4o mini in the main tables (Table 1), and include variance across judges to quantify robustness.\n  - Expand prompt sensitivity beyond a single paraphrase (Appendix E.2; Table 10) by testing diverse templates and adversarial phrasing; report the variance across prompts.\n  - Specify the number of examples used in human evaluation (Sec. 4.7) and provide confidence intervals; complement Krippendorff’s α with sample size and per-rater statistics (Appendix F.2; Table 11).\n  - Analyze potential alignment biases between policy manuals and judge preferences (No direct evidence found) by comparing scores when policy manual phrasing is varied or ablated.\n- **Deepen CDE validation and mitigate hedging**\n  - Add experiments that detect and penalize hedging behavior (Eq. 11; Sec. 3.2–3.3), e.g., measure entropy vs. correctness under adversarial prompts that encourage ambiguity; quantify trade-offs with r_acc.\n  - Compare full-vocabulary CDE (Sec. 3.2) to label-collapsed CDE for multi-class settings, reporting impact on both calibration and accuracy; include confusion analyses to ensure decision tokens reflect label semantics.\n  - Provide variance analyses for the Monte Carlo estimator (Eq. 8) and top-k entropy approximation (Appendix G.1), including sensitivity to K and k; report confidence intervals for CDE estimates.\n  - Link threshold settings (a, b) to observed calibration bins (Table 12) by plotting calibration curves stratified by CDE ranges, validating the theoretical ideal-policy assumptions (Appendix H.2) empirically.\n  - Clarify the placement of the CDE weight w by using either r = r_format + r_acc + w·r_CDE with r_CDE unscaled internally, or r = r_format + r_acc + r_CDE with w applied inside Eq. 11—ensure the implementation matches the stated “maximum of weight w” (Eq. 9; Eq. 11; Sec. 3.3).\n  - Specify the log base and entropy units used for CDE (nats vs. bits) and reconcile Appendix H.2 with training/evaluation (Eq. 7; Eq. 20), recomputing p_a, p_b and bounds with correct unit conversion.\n- **Add fairness-oriented diagnostics**\n  - Report subgroup-level FPR/FNR and equalized odds across protected categories (No direct evidence found), complementing Table 14 per-class F1 with fairness metrics relevant to moderation.\n  - Analyze disparity measures (e.g., worst-group accuracy, subgroup calibration) for protected categories where per-class F1 is low (Table 14), and discuss mitigation strategies (reweighting or targeted augmentation).\n  - For MAMI’s imbalanced labels (Table 6), include macro/weighted F1 and threshold analyses; assess how micro F1 is affected by frequent labels and report class-wise PR curves.\n  - Provide per-class metrics and fairness diagnostics for PrideMM target/stance tasks (No direct evidence found), including subgroup calibration and error parity.\n- **Broaden dataset and reasoning coverage**\n  - Augment reasoning evaluation with benign rationales (Appendix B.4; Table 8; Sec. 4.1) to measure false-positive explanations and their coherence/helpfulness for benign cases.\n  - Evaluate cross-dataset reasoning transfer by judging explanations on MAMI and PrideMM (Sec. 4.1; Table 1), even without gold rationales, using LLM judges and BERTScore (Appendix E.1), and report consistency.\n  - Incorporate retrieval-augmented training or inference to address implicit/knowledge-dependent memes highlighted in error analysis (Appendix J.2; Table 17), and quantify gains.\n  - Explore multilingual/cultural generalization (No direct evidence found) by testing non-English memes or culturally diverse datasets; discuss adapting policy manuals for locale-specific moderation.\n- **Improve reproducibility and artifact availability**\n  - Where possible, provide research-only releases of code and evaluation scripts, and document how to request model weights under the stated access controls (Appendix: Intended use; Block #39).\n  - Release the exact policy-manual artifacts used for training (Appendix B.2), including versioning and dataset-specific manuals, to ease replication.\n  - Commit to releasing training seeds, logs, and checkpoints for baselines (Appendix C), or at least report mean/variance over multiple runs for Table 1 to improve statistical confidence.\n  - Include statistical significance tests (e.g., bootstrap confidence intervals) for main results (Sec. 4.3; Table 1) to complement point estimates.\n  - Annotate Table 1 with the specific evaluation splits used for each metric (e.g., HatefulMemes dev_unseen for fine-grained; Appendix B.3; Table 5) to ensure exact reproducibility.\n- **Tighten clarity and resolve inconsistencies**\n  - Reconcile the claim that gold rationales hurt warmup (Sec. 3.3) with the observation that SFT-R performs competitively after GRPO-CL+CDE (Table 3), by clarifying at which stage and why effects differ.\n  - Define RCDE vs. CDE consistently in the main text and tables (Table 1), and state whether RCDE denotes “CDE reported for reasoning experiments” or a variant estimator.\n  - Provide the explicit formula for r_acc for multi-label fine-grained tasks (Sec. 3.3), detailing partial credit and over-prediction penalties for replicability.\n  - Add quantitative sensitivity analyses for curriculum scheduling choices (Sec. 3.3), e.g., switching criteria, mixing ratios, and plateau detection, and summarize their impact.\n  - Correct the calibration comparison in Sec. 4.6 to explicitly state the GRPO baseline Brier (0.441) vs. ExPO-HM (0.283), while optionally noting the zero-shot value (0.590) for context (Table 12).\n  - Qualify the Sec. 4.5 statement about SFT-B being “below the no-warmup baseline” by metric (BF1 vs. fine-grained), aligning the narrative with Table 3.Score\n  - Overall (10): 8 — Strong empirical improvements across binary, fine-grained, and reasoning with clear ablations and analyses (Table 1; Table 2–3; Sec. 4.3–4.6), tempered by reliance on LLM judges and several CDE-related specification issues (Eq. 7–11; Appendix H.2).\n  - Novelty (10): 8 — The Conditional Decision Entropy as both metric and reward (Sec. 3.2; Eq. 7–11; Appendix H.2) and policy-manual warmup plus GRPO curriculum (Fig. 2; Sec. 3.3) present a distinctive, well-motivated approach.\n  - Technical Quality (10): 6 — Sound formulation and thorough experiments (Table 1; Fig. 3; Table 12; Appendix G–H), with notable gaps in CDE reward scaling/units clarity (Eq. 9–11; Eq. 20), fairness analyses, estimator variance, and explicit reward formulae (Sec. 3.3; Table 14).\n  - Clarity (10): 6 — Generally clear with helpful figures and equations (Fig. 2; Eq. 7–11) but minor-to-moderate inconsistencies (RCDE undefined; Sec. 4.6 comparison wording; Sec. 4.5 SFT-B statement; evaluation split annotation) reduce clarity (Table 1; Sec. 4.5–4.6; Appendix B.3).\n  - Confidence (5): 4 — Review based on a complete manuscript with detailed appendices and results (Sec. 4; Table 1–14), though artifact access constraints (Appendix: Intended use; Block #39) and specification issues limit independent verification.",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 8,
        "novelty": 8,
        "technical_quality": 7,
        "clarity": 7,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 8,
        "novelty": 8,
        "technical_quality": 6,
        "clarity": 6,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "Summary\n   - The paper proposes ExPO-HM, an Explain-then-Detect framework for hateful meme detection that first produces a rationale and then a decision. The method combines three components: (i) supervised warmup using policy-manual–augmented prompts (SFT-PM), (ii) Group-Relative Policy Optimization with a curriculum from fine-grained to binary tasks (GRPO-CL), and (iii) Conditional Decision Entropy (CDE) used both as a metric and as a reward to shape reasoning confidence (Sec. 3.2–3.3; Eq. 7–11; Fig. 2). Experiments on HatefulMemes, MAMI, and PrideMM show state-of-the-art performance across binary detection, fine-grained classification, and reasoning quality, including consistent gains over SFT, DPO, GRPO, and prior agentic CoT systems (Table 1). Ablations attribute gains to all three components (Table 2–3), and further analyses demonstrate correlation between CDE and LLM-as-a-judge, improved calibration, and positive human evaluations (Sec. 4.3–4.7; Fig. 3; Table 11–12).Strengths\n- **Clear motivation grounded in real-world moderation needs**\n  - The introduction argues that moderators require targets, attack types, and rationales beyond binary flags (Sec. 1), aligning the task with policy-guided human annotation practice; this matters for impact and task relevance.\n  - The Explain-then-Detect formulation with <think>/<answer> explicitly structures outputs (Eq. 2), improving interpretability and actionability.\n  - The policy-manual idea directly mirrors guideline-driven moderation (Sec. 3.3; Appendix B.2), enhancing clarity and practical applicability.\n- **Novel CDE metric and reward for reasoning quality**\n  - CDE is formally defined as entropy over decisions conditioned on on-policy explanations (Sec. 3.2; Eq. 7–8), providing a verifiable proxy for reasoning confidence; this is technically novel.\n  - A piecewise reward function penalizes confident errors and rewards confident correctness (Eq. 10–11), improving technical soundness of RL signals.\n  - Correlation analyses show strong negative correlation with LLM-as-a-judge (Pearson r = −0.78; Spearman ρ = −0.81; p < 0.001; Sec. 4.3), supporting its validity as a metric.\n  - Theoretical discussion links CDE optimization to improved calibration with bounded Brier loss under ideal policy (Appendix H.2), strengthening rigor.\n- **Well-designed training pipeline that mimics human processes**\n  - SFT-PM teaches policy concepts via structured manuals built from dataset guidelines (Sec. 3.3; Appendix B.2–B.3), increasing conceptual alignment—important for generalization.\n  - GRPO-CL schedules fine-grained reasoning before binary detection (Sec. 3.3); the claim of producing longer, more detailed rationales (52 vs. 28 tokens; Sec. 4.4) is stated but not accompanied by a reporting table or figure (No direct evidence found in the manuscript), yet the curriculum ordering is central to the method’s design.\n  - The architecture summary (Fig. 2) provides a clear blueprint for reproducibility and future work.\n- **Comprehensive, multi-dimensional evaluation**\n  - Results span binary F1, fine-grained micro F1 (attack types and targets), and reasoning (LLM-as-a-judge and CDE) across three datasets and two model sizes (Table 1), demonstrating breadth and rigor.\n  - Ablations isolate contributions of SFT-PM, GRPO-CL, and CDE (Table 2) and analyze warmup strategies (Table 3), supporting causality.\n  - Per-class fine-grained analyses (Tables 13–14) reveal class-specific gains, improving interpretability and diagnosing model behavior.\n- **Consistent, substantial empirical gains**\n  - On HatefulMemes with Qwen2.5-VL-7B, ExPO-HM improves AttackF1 from 61.2 (GRPO, #20) to 75.6 (#21) and TargetF1 from 64.5 (#20) to 77.2 (#21) (Table 1), evidencing strong improvements.\n  - Binary F1 surpasses RA-HMD (80.2; #7) with 81.1 (#21), indicating that Explain-then-Detect can match or exceed direct detection (Table 1).\n  - Reasoning scores improve from 5.2 (GRPO #20) to 6.2 (ExPO-HM #21) on HatefulMemes (Table 1), and CDE drops markedly (0.26 → 0.03), evidencing tighter decision confidence (Table 1).\n- **Calibration benefits tied to CDE**\n  - CDE reward improves calibration: ECE and Brier scores decrease across model sizes (Table 12), which matters for reliability in safety-critical moderation.\n  - CDE distributions separate correct vs. incorrect predictions (Fig. 3; Sec. 4.6), confirming intended behavior (technical soundness).\n- **Attention to reproducibility and implementation details**\n  - Training details include LoRA configuration, frozen vision encoders, libraries and versions, compute budget, and curriculum split (Appendix C; Sec. 4.2), facilitating replication.\n  - Hyperparameter tuning ranges and stability for CDE are documented (Appendix C.3), increasing robustness and practical usability.\n- **Human evaluation supporting explanation quality**\n  - Coherence reaches 100% vs. 96% (Sec. 4.7), and helpfulness improves (2.2 vs. 1.6 on 0–4 scale; Table 11), aligning with LLM-as-a-judge trends, adding external validity.Weaknesses\n- **Primary reliance on LLM-as-a-judge with limited human evaluation scale**\n  - The main reasoning metric uses a closed-source judge (GPT-4o mini; Appendix D; Table 1), which can introduce reproducibility and bias concerns; while alternative judges are reported (Appendix E.1; Table 9), the headline results emphasize GPT-4o mini, limiting robustness.\n  - Prompt sensitivity is only tested with one paraphrase (Appendix E.2; Table 10), which may not capture broader prompt variance or adversarial judge prompts—important for evaluation stability.\n  - Human evaluation reports coherence/helpfulness but does not specify the sample size of annotated memes in Sec. 4.7 (beyond “three annotators per example”), constraining statistical power; although Krippendorff’s αordinal = 0.71 is reported (Appendix F.2; Table 11), more detail on the sample would improve rigor.\n  - Potential alignment issues between judge preferences and policy manuals are not analyzed (No direct evidence found in the manuscript), risking metric-policy mismatch in downstream deployment.\n- **CDE design may incentivize hedging and has approximation limits**\n  - The reward explicitly “tolerates uncertainty when wrong” (Eq. 11; Sec. 3.2–3.3), which could incentivize ambiguous reasoning rather than correction—this matters for moderation usefulness under mistakes.\n  - CDE uses the full decision vocabulary (Sec. 3.2) and a top-k token approximation for entropy (Appendix G.1), which may mix label semantics with general tokens and introduce estimator bias.\n  - Monte Carlo estimation with K=16 explanations (Eq. 8) and top-10–50 token entropy approximations (Appendix G.1) can yield variance and potential instability, yet variance analyses are not provided (No direct evidence found in the manuscript).\n  - Theoretical calibration analysis (Appendix H.2) relies on an “ideal ExPO-HM policy” and thresholded entropy assumptions, but empirical linkage from reward thresholds (a, b) to observed calibration is not quantified beyond aggregate metrics (Table 12), limiting mechanistic validation.\n  - The CDE reward weight appears in both the overall reward r(yig, di*) (Eq. 9: “w r_CDE”) and inside the piecewise definition of r_CDE (Eq. 11: terms scaled by w), while Sec. 3.3 states “CDE rewards contribute a maximum of weight w”; this risks an unintended double weighting that affects optimization and reproducibility (technical soundness).\n  - Entropy units are not consistently specified: Eq. 7 uses “log” without base (Sec. 3.2), whereas Appendix H.2 defines binary entropy H2 in bits (Eq. 20) and derives p_a, p_b from a=0.1, b=0.5 without clarifying unit conversion; this undermines the quantitative correctness of the theoretical bounds (clarity/rigor).\n- **Fairness and group-level risk are under-examined**\n  - Group-wise false positive/false negative rates or equalized odds across protected categories are not reported (No direct evidence found in the manuscript), leaving fairness questions open for deployment.\n  - Per-class F1 in Table 14 shows large disparities (e.g., Nationality and Disability), but fairness-specific diagnostics (e.g., subgroup calibration or error parity) are absent, limiting bias assessment.\n  - MAMI fine-grained distribution is highly imbalanced (Table 6), and micro F1 can be dominated by frequent labels; complementary macro/weighted metrics or threshold analyses are not reported (No direct evidence found).\n  - PrideMM target and stance tasks (Table 7; Table 1) lack per-class fairness metrics (No direct evidence found), making it hard to evaluate differential impacts on LGBTQ+ subgroups.\n- **Dataset and reasoning coverage limitations**\n  - Reasoning evaluation uses Hatred (hateful-only explanations; Appendix B.4; Table 8; Sec. 4.1), so benign rationales are not assessed, potentially biasing the reasoning benchmark toward harmful cases.\n  - Cross-dataset reasoning generalization beyond HatefulMemes is not evaluated (Sec. 4.1; Table 1), leaving uncertainty about transfer to other domains and meme styles.\n  - Error analysis highlights failures on implicit or knowledge-dependent memes (Appendix J.2; Table 17; images), but the method does not incorporate retrieval or external knowledge during training/inference, limiting coverage of subtle cases.\n  - Language and cultural generalization are not addressed (No direct evidence found), and all datasets are primarily English (Appendix B), limiting broader applicability.\n- **Reproducibility and artifact availability constraints**\n  - The paper states strict access controls for model artifacts (Appendix: Intended use; Block #39), which may limit reproducibility for academic benchmarking beyond approved users.\n  - Policy manual construction is described (Appendix B.2), but the exact manuals/artifacts used for training are not stated as publicly released (No direct evidence found), making replication harder.\n  - While implementation details are thorough (Appendix C), code release is not promised (No direct evidence found), constraining independent verification.\n  - Seed management and statistical significance testing of improvements are not reported (Sec. 4.3 references 60 points for correlation, but main tables show point estimates), which reduces confidence in the robustness of reported gains.\n  - Evaluation split usage for fine-grained metrics is not made explicit in the main results table: Appendix B.3 notes dev_unseen for final evaluation (Table 5), but Table 1 does not annotate which splits were used for AttackF1/TargetF1 vs. BF1, complicating exact replication.\n- **Clarity and minor inconsistencies**\n  - Sec. 3.3 says gold explanations are “off-policy and lead to worse performance,” but Table 3 shows SFT-R becomes competitive after GRPO-CL+CDE (e.g., BF1 79.2 vs. 78.9 for SFT-FG), suggesting a nuanced picture that is not clearly reconciled.\n  - Table 1 includes “RCDE ↓” for MAMI and PrideMM without explicitly defining RCDE vs. CDE in the main text (No explicit definition found), creating minor nomenclature ambiguity.\n  - The accuracy reward for multi-label fine-grained classification is referenced with “partial credit” and “penalties for over-prediction” (Sec. 3.3; r_acc) but no explicit formula is given, limiting replicability of the reward design.\n  - The curriculum scheduler rationale is described qualitatively (Sec. 3.3: “find similar performance… adopt 50/50/50”), but quantitative sensitivity analyses are limited (No direct evidence found), which would strengthen clarity about scheduler choices.\n  - In Sec. 4.6, the text highlights a Brier reduction “from 0.590 to 0.283” when discussing improvements over GRPO, but 0.590 is the zero-shot value; the GRPO baseline is 0.441 (Table 12), so the correct comparison vs. GRPO should be 0.441 → 0.283 (clarity).\n  - Sec. 4.5 states SFT-B is “even below the no-warmup baseline,” yet Table 3 shows post-RL BF1 is marginally higher for SFT-B (73.5) than no warmup (73.3), while SFT-B underperforms on fine-grained metrics; the statement should be qualified by metric (clarity).Suggestions for Improvement\n- **Strengthen evaluation robustness beyond LLM-as-a-judge**\n  - Report headline reasoning results using multiple judges (e.g., open-source in Appendix E.1; Table 9) alongside GPT-4o mini in the main tables (Table 1), and include variance across judges to quantify robustness.\n  - Expand prompt sensitivity beyond a single paraphrase (Appendix E.2; Table 10) by testing diverse templates and adversarial phrasing; report the variance across prompts.\n  - Specify the number of examples used in human evaluation (Sec. 4.7) and provide confidence intervals; complement Krippendorff’s α with sample size and per-rater statistics (Appendix F.2; Table 11).\n  - Analyze potential alignment biases between policy manuals and judge preferences (No direct evidence found) by comparing scores when policy manual phrasing is varied or ablated.\n- **Deepen CDE validation and mitigate hedging**\n  - Add experiments that detect and penalize hedging behavior (Eq. 11; Sec. 3.2–3.3), e.g., measure entropy vs. correctness under adversarial prompts that encourage ambiguity; quantify trade-offs with r_acc.\n  - Compare full-vocabulary CDE (Sec. 3.2) to label-collapsed CDE for multi-class settings, reporting impact on both calibration and accuracy; include confusion analyses to ensure decision tokens reflect label semantics.\n  - Provide variance analyses for the Monte Carlo estimator (Eq. 8) and top-k entropy approximation (Appendix G.1), including sensitivity to K and k; report confidence intervals for CDE estimates.\n  - Link threshold settings (a, b) to observed calibration bins (Table 12) by plotting calibration curves stratified by CDE ranges, validating the theoretical ideal-policy assumptions (Appendix H.2) empirically.\n  - Clarify the placement of the CDE weight w by using either r = r_format + r_acc + w·r_CDE with r_CDE unscaled internally, or r = r_format + r_acc + r_CDE with w applied inside Eq. 11—ensure the implementation matches the stated “maximum of weight w” (Eq. 9; Eq. 11; Sec. 3.3).\n  - Specify the log base and entropy units used for CDE (nats vs. bits) and reconcile Appendix H.2 with training/evaluation (Eq. 7; Eq. 20), recomputing p_a, p_b and bounds with correct unit conversion.\n- **Add fairness-oriented diagnostics**\n  - Report subgroup-level FPR/FNR and equalized odds across protected categories (No direct evidence found), complementing Table 14 per-class F1 with fairness metrics relevant to moderation.\n  - Analyze disparity measures (e.g., worst-group accuracy, subgroup calibration) for protected categories where per-class F1 is low (Table 14), and discuss mitigation strategies (reweighting or targeted augmentation).\n  - For MAMI’s imbalanced labels (Table 6), include macro/weighted F1 and threshold analyses; assess how micro F1 is affected by frequent labels and report class-wise PR curves.\n  - Provide per-class metrics and fairness diagnostics for PrideMM target/stance tasks (No direct evidence found), including subgroup calibration and error parity.\n- **Broaden dataset and reasoning coverage**\n  - Augment reasoning evaluation with benign rationales (Appendix B.4; Table 8; Sec. 4.1) to measure false-positive explanations and their coherence/helpfulness for benign cases.\n  - Evaluate cross-dataset reasoning transfer by judging explanations on MAMI and PrideMM (Sec. 4.1; Table 1), even without gold rationales, using LLM judges and BERTScore (Appendix E.1), and report consistency.\n  - Incorporate retrieval-augmented training or inference to address implicit/knowledge-dependent memes highlighted in error analysis (Appendix J.2; Table 17), and quantify gains.\n  - Explore multilingual/cultural generalization (No direct evidence found) by testing non-English memes or culturally diverse datasets; discuss adapting policy manuals for locale-specific moderation.\n- **Improve reproducibility and artifact availability**\n  - Where possible, provide research-only releases of code and evaluation scripts, and document how to request model weights under the stated access controls (Appendix: Intended use; Block #39).\n  - Release the exact policy-manual artifacts used for training (Appendix B.2), including versioning and dataset-specific manuals, to ease replication.\n  - Commit to releasing training seeds, logs, and checkpoints for baselines (Appendix C), or at least report mean/variance over multiple runs for Table 1 to improve statistical confidence.\n  - Include statistical significance tests (e.g., bootstrap confidence intervals) for main results (Sec. 4.3; Table 1) to complement point estimates.\n  - Annotate Table 1 with the specific evaluation splits used for each metric (e.g., HatefulMemes dev_unseen for fine-grained; Appendix B.3; Table 5) to ensure exact reproducibility.\n- **Tighten clarity and resolve inconsistencies**\n  - Reconcile the claim that gold rationales hurt warmup (Sec. 3.3) with the observation that SFT-R performs competitively after GRPO-CL+CDE (Table 3), by clarifying at which stage and why effects differ.\n  - Define RCDE vs. CDE consistently in the main text and tables (Table 1), and state whether RCDE denotes “CDE reported for reasoning experiments” or a variant estimator.\n  - Provide the explicit formula for r_acc for multi-label fine-grained tasks (Sec. 3.3), detailing partial credit and over-prediction penalties for replicability.\n  - Add quantitative sensitivity analyses for curriculum scheduling choices (Sec. 3.3), e.g., switching criteria, mixing ratios, and plateau detection, and summarize their impact.\n  - Correct the calibration comparison in Sec. 4.6 to explicitly state the GRPO baseline Brier (0.441) vs. ExPO-HM (0.283), while optionally noting the zero-shot value (0.590) for context (Table 12).\n  - Qualify the Sec. 4.5 statement about SFT-B being “below the no-warmup baseline” by metric (BF1 vs. fine-grained), aligning the narrative with Table 3.Score\n  - Overall (10): 8 — Strong empirical improvements across binary, fine-grained, and reasoning with clear ablations and analyses (Table 1; Table 2–3; Sec. 4.3–4.6), tempered by reliance on LLM judges and several CDE-related specification issues (Eq. 7–11; Appendix H.2).\n  - Novelty (10): 8 — The Conditional Decision Entropy as both metric and reward (Sec. 3.2; Eq. 7–11; Appendix H.2) and policy-manual warmup plus GRPO curriculum (Fig. 2; Sec. 3.3) present a distinctive, well-motivated approach.\n  - Technical Quality (10): 6 — Sound formulation and thorough experiments (Table 1; Fig. 3; Table 12; Appendix G–H), with notable gaps in CDE reward scaling/units clarity (Eq. 9–11; Eq. 20), fairness analyses, estimator variance, and explicit reward formulae (Sec. 3.3; Table 14).\n  - Clarity (10): 6 — Generally clear with helpful figures and equations (Fig. 2; Eq. 7–11) but minor-to-moderate inconsistencies (RCDE undefined; Sec. 4.6 comparison wording; Sec. 4.5 SFT-B statement; evaluation split annotation) reduce clarity (Table 1; Sec. 4.5–4.6; Appendix B.3).\n  - Confidence (5): 4 — Review based on a complete manuscript with detailed appendices and results (Sec. 4; Table 1–14), though artifact access constraints (Appendix: Intended use; Block #39) and specification issues limit independent verification."
}