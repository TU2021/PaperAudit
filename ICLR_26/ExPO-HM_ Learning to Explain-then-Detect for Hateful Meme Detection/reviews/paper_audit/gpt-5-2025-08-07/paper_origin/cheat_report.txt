Academic Integrity and Consistency Risk Report

Scope: This report flags high-impact, evidence-based inconsistencies and logical problems in the manuscript. Each point is anchored to specific parts of the paper.

1) CDE reward is double-weighted in the training objective
- Evidence:
  - Eq. 9 (Section 3.3): r(y_ig, d_i*) = r_format + r_acc + w r_CDE.
  - Eq. 11 (Section 3.3): r_CDE(h, δ) is defined piecewise with an explicit factor w inside the reward function (“CDE rewards contribute a maximum of weight w... r_CDE(...) = ... w ...”).
- Problem: With w multiplied both outside (Eq. 9) and inside r_CDE (Eq. 11), the effective weight becomes w^2, not w, contradicting “CDE rewards contribute a maximum of weight w” (Section 3.3). This materially affects optimization dynamics and reproducibility.
- Impact: Training stability, reward scaling, and comparisons to GRPO baseline depend critically on the reward’s magnitude; double weighting can inflate or suppress the intended contribution of CDE.
- Suggested fix: Either remove w inside Eq. 11 and keep w r_CDE in Eq. 9, or keep w inside Eq. 11 and change Eq. 9 to r = r_format + r_acc + r_CDE. Clarify how w=0 is handled for GRPO baseline (Section 3.3) to ensure the CDE term is truly null.

2) Entropy units mismatch (nats vs bits) leading to incorrect numeric thresholds and bounds
- Evidence:
  - Eq. 7 (Section 3.2) and Eq. 13 use “log” without specifying base (standard ML convention is natural log → nats).
  - Section 3.2 sets default entropy thresholds a = 0.1, b = 0.5 (implicitly in the same units as Eq. 7).
  - Appendix H.2 (Eq. 20) explicitly defines binary entropy H2 in bits and then computes p_a, p_b from a = 0.1, b = 0.5 (“It is easy to find p_a = 0.987, p_b = 0.110”).
- Problem: The thresholds a, b appear to be used in nats during training/evaluation (Eq. 7), but are treated as bits in the theoretical bound derivation. Without unit conversion, p_a and p_b are numerically inconsistent, which invalidates the derived bounds (Appendix H.2, Eqs. 23–31).
- Impact: Theoretical guarantees and calibration bounds (Appendix H.2) are quantitatively incorrect if unit conversion is not performed. This affects claims that ExPO-HM guarantees lower Brier losses under certain thresholds.
- Suggested fix: Specify the log base for all entropy computations. If training uses nats, convert a and b to bits before using H2 in Appendix H.2, or reformulate the derivation in nats. Recompute p_a, p_b and the bounds accordingly.

3) Misstatement of calibration improvement “compared to GRPO baseline”
- Evidence:
  - Section 4.6: “We observe that ExPO-HM consistently improves calibration compared to the GRPO baseline. Notably, for Qwen2.5-VL-3B, ExPO-HM reduces the Brier score from 0.590 to 0.283.”
  - Appendix H.1, Table 12: For Qwen2.5-VL-3B, Brier scores are Zero-shot: 0.590; SFT: 0.534; GRPO: 0.441; ExPO-HM: 0.283.
- Problem: The sentence states improvement “compared to the GRPO baseline,” but the cited reduction 0.590 → 0.283 is from zero-shot to ExPO-HM, not GRPO to ExPO-HM. The correct comparison to GRPO baseline is 0.441 → 0.283.
- Impact: Overstates the improvement versus GRPO baseline and confuses the reader about the source of the reported reduction.
- Suggested fix: Revise to: “compared to the GRPO baseline (0.441), ExPO-HM achieves 0.283 (vs. 0.590 for zero-shot).”

4) Overgeneralized claim about warmup performance being “below the no-warmup baseline”
- Evidence:
  - Section 4.5: “SFT-B … even below the no-warmup baseline.”
  - Table 3 (HatefulMemes, Qwen2.5-VL-7B): After GRPO-CL+CDE, No warmup (row 1) BF1 = 73.3; SFT-B (row 2) BF1 = 73.5; AttackF1 = 66.8 vs 69.3; TargetF1 = 70.1 vs 72.1.
- Problem: The statement is not accurate for binary F1 (SFT-B 73.5 > 73.3). It is below for AttackF1 and TargetF1, but not for BF1.
- Impact: Misleading summary of results may affect readers’ interpretation of warmup strategy efficacy.
- Suggested fix: Qualify the statement by metric: “SFT-B underperforms the no-warmup baseline on fine-grained metrics (AttackF1, TargetF1) and LLM-judge scores, while being marginally higher in binary F1.”

5) CDE/RCDE metric labeling inconsistency in Table 1
- Evidence:
  - Table 1 column headings: “HatefulMemes CDE ↓” vs “MAMI RCDE ↓” and “PrideMM RCDE ↓.”
- Problem: The manuscript defines CDE as both a metric and a reward (Sections 3.2–3.3), but does not define “RCDE.” It is unclear whether RCDE differs from CDE or is the same metric under reasoning setup.
- Impact: Ambiguity complicates replication and comparison across datasets, and may confuse readers about what is being measured.
- Suggested fix: Standardize the naming (e.g., always “CDE”) and explicitly define the metric once, including units and computation details for all datasets.

6) Missing evidence for claims about response length increase under GRPO-CL
- Evidence:
  - Section 4.4: “Standard GRPO produces short average responses (28 tokens) … GRPO-CL nearly doubles this (52 tokens).”
- Problem: No table/figure or appendix reference provides these token-length statistics.
- Impact: The claim is central to the argument that curriculum ordering improves reasoning depth; lack of evidence weakens the support for this conclusion.
- Suggested fix: Include a figure or table reporting average output lengths and variance for GRPO vs. GRPO-CL, anchored to the datasets and splits used.

7) Evaluation split clarity for fine-grained tasks
- Evidence:
  - Appendix B.1: “Note that we use the dev_unseen split for final evaluation” (fine-grained).
  - Table 4: Binary classification split details (dev_seen/test_seen described in text; table labels “Test”).
  - Table 1: Does not specify which splits are used per metric.
- Problem: While binary splits are described, the exact splits used for AttackF1/TargetF1 in Table 1 are not explicitly linked to dev_unseen in the table or main text.
- Impact: Reproducibility could be affected.
- Suggested fix: Explicitly annotate Table 1 with the evaluation split used for each metric (e.g., “HatefulMemes AttackF1/TargetF1 evaluated on dev_unseen; BF1 on test_seen”).

Summary
The most substantive integrity risks are:
- Reward definition inconsistency (double weighting of CDE) that can materially alter training dynamics and baselines.
- Entropy unit mismatch (nats vs bits) leading to incorrect theoretical bounds and thresholds.
- Misstatement of calibration improvements relative to the GRPO baseline.

Addressing these points with precise corrections and added clarifications would materially improve the manuscript’s correctness, reproducibility, and trustworthiness.

If the above are resolved, no further clear integrity-related or consistency problems were identified based on the manuscript.