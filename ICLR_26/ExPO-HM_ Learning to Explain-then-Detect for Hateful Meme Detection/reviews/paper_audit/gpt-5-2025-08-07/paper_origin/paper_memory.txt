# Global Summary
- Problem: Detecting hateful memes is challenging; most prior systems provide only binary predictions with no rationale, which is insufficient for moderation. Existing Explain-then-Detect systems (CoT prompting, agent frameworks) underperform strong supervised baselines, and naive RL post-training (e.g., GRPO) fails to improve reasoning.
- Approach: ExPO-HM (Explain-then-Detect Policy Optimization for Hateful Memes) trains LMMs to generate a rationale and then a decision. It combines (1) SFT warmup using policy-manual–augmented prompts derived from fine-grained labels and guidelines (SFT-PM), (2) GRPO with curriculum learning (first fine-grained, then binary), and (3) a Conditional Decision Entropy (CDE) metric used both for evaluation and as a reward to encourage confident correctness and caution when wrong.
- Evaluation: Benchmarked on HatefulMemes, MAMI, and PrideMM for binary detection and fine-grained classification (attack types, targets, stance). Reasoning quality evaluated on Hatred (human rationales for HatefulMemes) via LLM-as-a-judge; CDE reported and correlated with LLM judging. Human evaluations (coherence, helpfulness) and calibration metrics (ECE, Brier) included. Baselines: direct SFT, DPO, GRPO, RA-HMD (SOTA direct), agentic/CoT systems (LOREHM, U-CoT+).
- Key quantitative results (Qwen2.5-VL-7B unless noted): On HatefulMemes, ExPO-HM achieves "81.1" BF1, "75.6" AttackF1, "77.2" TargetF1, LLM-as-a-judge "6.2", CDE "0.03", surpassing RA-HMD (BF1 "80.2"). On MAMI: BF1 "82.3", AttackF1 "73.0", RCDE "0.04". On PrideMM: BF1 "78.7", StanceF1 "68.4", TargetF1 "65.1", RCDE "0.08". With Qwen2.5-VL-3B: HatefulMemes BF1 "74.7", AttackF1 "71.5", TargetF1 "73.7", LLM "5.1", CDE "0.16"; MAMI BF1 "80.7", AttackF1 "70.4", RCDE "0.08"; PrideMM BF1 "75.6", StanceF1 "66.5", TargetF1 "62.1", RCDE "0.12".
- Reported margins: Up to "15%" F1 improvement over GRPO and "17%" over DPO (varies by task/model; e.g., HatefulMemes AttackF1 +14.4 vs GRPO, MAMI AttackF1 +16.4 vs DPO; with 3B model, AttackF1 +20.2 vs DPO).
- CDE–LLM judge correlation: Pearson r "−0.78", Spearman ρ "−0.81" (p < "0.001").
- Human evaluation: Coherence "96%" (GRPO) vs "100%" (ExPO-HM); helpfulness averages "1.6" vs "2.2" (scaled to 0–10: "4.1" vs "5.5"); Krippendorff’s α_{ordinal} "0.71".
- Calibration: ExPO-HM improves ECE and Brier score (e.g., Qwen2.5-VL-3B Brier "0.590 → 0.283"; 7B ECE "0.160", Brier "0.214").
- Training and budget: LoRA rank "64", α "128"; vision module frozen; SFT/DPO/GRPO trained for "3" epochs; ExPO-HM and GRPO runtime about "4 hours" on "8" Nvidia H100 (80GB); DPO/SFT on "1" GPU; vLLM "0.9.2"; libraries: PyTorch "2.5.1", CUDA "12.4", HF Transformers "4.45.0".
- Caveats explicitly stated: Reasoning judged by LLMs (mainly GPT-4o mini) with reproducibility mitigations; dataset and annotator biases may propagate; outputs depend on underlying moderation policies; some closed-source LLM APIs blocked (~"30%") due to harmful content; content may be disturbing.

# Abstract
- Identifies the gap in hateful meme detection: binary-only outputs lack explanations needed for moderation; Explain-then-Detect methods using CoT or LMM agents underperform SFT, and GRPO fails to bridge the gap.
- Diagnosed issues: models fail to hypothesize policy-relevant cues (targets, attack types); binary reward signal too weak for guiding reasoning.
- Proposed method: ExPO-HM combining SFT warmup, GRPO with curriculum learning, and Conditional Decision Entropy (CDE) as metric and reward.
- Results summary: Across three benchmarks, ExPO-HM achieves SOTA on binary detection, fine-grained classification, and reasoning quality, with up to "15%" and "17%" F1 improvements over GRPO and DPO baselines.
- Emphasizes moving from binary alarms to explanation-driven detection for accurate, interpretable moderation support.
- Note: Demonstration content may be disturbing.

# Introduction
- Motivation: Moderators need context (attack type, targeted group) and reasons for harmfulness; users benefit from understanding explanations.
- Human analogy: Annotators trained/evaluated using policy manuals with fine-grained categories; binary-only training is impractical.
- Define Explain-then-Detect: generate rationale then label.
- Prior attempts: CoT prompting and agent-based frameworks perform worse than direct SFT; GRPO post-training still underperforms for hateful memes.
- Two challenges: explanations fail to identify correct violated policy/target; binary rewards provide insufficient learning signal.
- Contributions:
  - Paradigm: First Explain-then-Detect hateful meme detection system that outperforms direct detection.
  - Methods: Policy manual SFT warmup + GRPO curriculum + CDE reward.
  - Evaluation: Comprehensive setup covering binary, fine-grained categories, and reasoning (LLM-as-a-judge), with extensive baselines.
  - Results: New SOTA; up to "15%" and "17%" F1 improvements over GRPO/DPO.

# Related Work
- Direct detection: Most approaches treat hateful meme detection as binary classification using CLIP-based models and decoder LMMs; RA-HMD achieves SOTA with retrieval-augmented classification (Mei et al., 2025).
- Fine-grained classification: Less explored despite importance; datasets exist (Mathias et al., 2021; Dimitrov et al., 2021; Fersini et al., 2022; Shah et al., 2024). Some methods leverage fine-grained labels but often do not report fine-grained results; MemeCLIP trains separate classifiers. This paper extends evaluation to fine-grained tasks.
- Explain-then-Detect: Limited research; some systems use LLM debate or reasoning agents with retrieval/reflection, targeting binary classification. Only Hatred (Hee et al., 2023) provides human-written rationales; other datasets (e.g., Arabic ArMeme) not public. Existing explainable systems underperform direct detection. This paper benchmarks Explain-then-Detect on Hatred and introduces ExPO-HM surpassing both explainable and direct approaches.

# Preliminaries
- Problem setup: Given dataset D = {(I_i, c_i^*)}, with image I_i and binary label c_i^* ∈ {0,1}; additionally fine-grained labels z_i^* and gold explanations e_i^* when available. Tasks: (1) predict binary c_i; (2) fine-grained z_i; (3) generate explanation e_i. Textualized decision denoted d_i; ground-truth text d_i^*.
- LMM policy: Auto-regressive π_θ(y|x) over tokens with x = (I, p). Explain-then-Detect output format: y ≡ <think> e </think> <answer> d </answer>.
- Training: SFT maximizes likelihood of target sequence y^*.

# Method
- SFT objective: L_SFT(θ) = −∑_{t} log π_θ(y_t^* | y_{<t}^*, x).
- DPO baseline: On-policy preference pairs (y^+, y^−) chosen by decision correctness; objective uses β-weighted likelihood ratios vs reference model π_ref; β swept.
- GRPO baseline: Policy-gradient without critic; group sampling (group size shown as "G=8"); advantage normalized within group; clipped objective with KL regularization to π_ref.
- Conditional Decision Entropy (CDE):
  - Definition: H(d | e, x) = −E_{d∼π_θ(·|e,x)}[log π_θ(d|e,x)].
  - Estimation: Monte Carlo over validation with K = "16" sampled explanations per example; binary case can collapse vocabulary to {Yes, No}, though full vocabulary adopted for generality.
- ExPO-HM framework:
  - SFT-PM warmup: Convert fine-grained labels and dataset guidelines into structured policy manuals; use as input to teach policy knowledge. Target response y^* is fine-grained text label d_i^*. Gold explanations e^* are not used in warmup (off-policy, worse performance).
  - GRPO with Curriculum Learning (GRPO-CL): Two-stage schedule ("50/50/50"): first "50%" of steps on fine-grained data only; remaining "50%" on a "50/50" mix of fine-grained and binary data. Ordering (fine-grained before binary) is crucial.
  - Rewards: r(y_ig, d_i^*) = r_format + r_acc + w r_CDE. r_format ∈ {0,1} checks output template; r_acc ∈ [0,1] gives partial credit for multi-class fine-grained and exact match for binary. Baseline GRPO sets w = 0. CDE reward:
    - For each response: h_{ig} = H(d | e_{ig}, x_i); δ_{ig} = 1[d_{ig} = d_i^*].
    - r_CDE(h, δ) piecewise function (Eq. 11) rewarding confident correctness, tolerating uncertainty when wrong, penalizing confident errors.
    - Default hyperparameters: a = "0.1", b = "0.5", w = "0.2", ρ = "0.25". Stable ranges: 0.05 ≤ a ≤ 0.15; 0.4 ≤ b ≤ 0.6; 0.15 ≤ w ≤ 0.25; 0.1 ≤ ρ ≤ 0.5.

# Experiments
- Datasets and tasks:
  - HatefulMemes (12,000 memes; binary labels; fine-grained attack/target labels; Hatred subset has human rationales). Splits: Train "5450" benign/"3050" hate; Test "500" benign/"500" hate.
  - MAMI: Binary misogyny and types (objectification, shaming, stereotype, violence). Train "4500" benign/"4500" hate; Test "500"/"500". Sub-task B label distribution (train/test %): Shaming "25.48%"/"29.20%"; Stereotype "56.20%"/"70.00%"; Objectification "44.04%"/"69.60%"; Violence "19.06%"/"30.60%".
  - PrideMM: Binary hate speech; fine-grained target (undirected/individual/community/organization) and stance (neutral/support/oppose). Train "2581" benign/"2482" hate; Test "260"/"247". Fine-grained counts provided (e.g., Target Benign "2208", Undirected "666", Individual "219", Community "986", Organization "249"; Stance Neutral "1252", Support "1645", Oppose "1431"; test splits in Table 7).
- Metrics:
  - Classification: macro F1 (binary); micro F1 (fine-grained).
  - Reasoning: LLM-as-a-judge (GPT-4o mini; prompt specified), CDE, human evaluation (coherence and helpfulness).
  - Calibration: ECE (10 bins) and Brier score (on final answer token probability conditioned on explanation).
- Baselines and prior systems:
  - Direct SFT (Direct-SFT) vs CoT-SFT; Direct-SFT consistently stronger; best SFT reported.
  - DPO initialized from fine-grained SFT warmup (without policy manuals); β ∈ {"0.1", "0.3", "0.5", "0.7", "0.9"} swept.
  - GRPO baseline matched compute to ExPO-HM (same warmup and GRPO hyperparameters); group sampling.
  - Prior SOTA: RA-HMD (Qwen2.5-VL-7B), supports reasoning prompts; LLM-as-a-judge reported. Explain-then-Detect agentic systems: LOREHM (LLaVA-Next-34B) and U-CoT+ (Qwen2-VL-7B + Qwen2.5-14B) reported on binary only. Closed-source reasoning LLMs (OpenAI o-series) omitted due to ~"30%" API blocks.
- Main results (Table 1):
  - Direct SFT baselines: Qwen2.5-VL-7B SFT achieves HatefulMemes BF1 "75.0" and MAMI BF1 "78.1"; PrideMM BF1 "75.6".
  - RA-HMD: HatefulMemes BF1 "80.2", MAMI BF1 "81.0", PrideMM BF1 "77.8"; LLM-as-a-judge "5.4".
  - Explain-then-Detect baselines often underperform Direct SFT: e.g., U-CoT+ HatefulMemes BF1 "72.4".
  - ExPO-HM (Qwen2.5-VL-7B):
    - HatefulMemes: BF1 "81.1"; AttackF1 "75.6"; TargetF1 "77.2"; LLM "6.2"; CDE "0.03".
    - MAMI: BF1 "82.3"; AttackF1 "73.0"; RCDE "0.04".
    - PrideMM: BF1 "78.7"; StanceF1 "68.4"; TargetF1 "65.1"; RCDE "0.08".
  - ExPO-HM (Qwen2.5-VL-3B):
    - HatefulMemes: BF1 "74.7"; AttackF1 "71.5"; TargetF1 "73.7"; LLM "5.1"; CDE "0.16".
    - MAMI: BF1 "80.7"; AttackF1 "70.4"; RCDE "0.08".
    - PrideMM: BF1 "75.6"; StanceF1 "66.5"; TargetF1 "62.1"; RCDE "0.12".
  - Gains vs GRPO (Qwen2.5-VL-7B): AttackF1 +"14.4" (75.6 vs 61.2), TargetF1 +"12.7" (77.2 vs 64.5), LLM +"1.0" (6.2 vs 5.2), CDE reduced from "0.26" to "0.03". Similar gains across datasets.
- Observations (Sec. 4.3):
  - Baseline Explain-then-Detect post-training hurts classification vs Direct SFT and RA-HMD; larger agentic/CoT systems also fall short.
  - Naive post-training (SFT/DPO/GRPO) improves over zero-shot but fails to meaningfully improve reasoning; e.g., HatefulMemes LLM-as-a-judge "5.0" (zero-shot) → "4.9" (DPO) or "5.2" (GRPO).
  - ExPO-HM consistently outperforms across tasks and datasets, including reasoning quality; robustness shown with different LLM judges and prompts (Appendix E).
  - Strong negative correlation between LLM-as-a-judge and CDE: Pearson "−0.78", Spearman "−0.81".
- Ablation (Table 2, HatefulMemes Qwen2.5-VL-7B):
  - No SFT-PM/GRPO-CL/CDE: BF1 "74.5", AttackF1 "61.2", TargetF1 "64.5", LLM "5.2", CDE "0.263".
  - +SFT-PM: BF1 "75.8", AttackF1 "70.8", TargetF1 "70.2", LLM "5.6", CDE "0.092".
  - +GRPO-CL: BF1 "78.4", AttackF1 "74.3", TargetF1 "76.1", LLM "5.8", CDE "0.056".
  - +CDE (full ExPO-HM): BF1 "81.1", AttackF1 "75.6", TargetF1 "77.2", LLM "6.2", CDE "0.026".
- Warmup strategy comparison (Table 3, HatefulMemes Qwen2.5-VL-7B):
  - SFT stage vs after GRPO-CL+CDE:
    - No warmup: SFT BF1 "65.9" → "73.3".
    - SFT-B: SFT BF1 "74.1" → "73.5".
    - SFT-R: SFT BF1 "72.2" → "79.2".
    - SFT-FG: SFT BF1 "72.5" → "78.9".
    - SFT-PM: SFT BF1 "74.3" → "81.1" (AttackF1 "75.6", TargetF1 "77.2", LLM "6.2").
  - Insight: Binary-only warmup (SFT-B) does not transfer to strong RL reasoning; policy-manual augmentation yields best post-RL performance.
- CDE analysis:
  - Distribution separation (Fig. 3): ExPO-HM correct μ "0.019" vs wrong μ "0.048"; GRPO correct μ "0.278" vs wrong μ "0.226".
  - Calibration: ExPO-HM improves ECE/Brier on both model sizes (Table 12); e.g., 3B Brier "0.283" vs GRPO "0.441".
  - Policy entropy: Comparable to GRPO; CDE reward acts only on decision, does not collapse exploration.
  - Hyperparameters: Grid-searched on HatefulMemes; fixed across other datasets; performance stable within ranges (Appendix C.3).
- Human evaluation (Sec. 4.7):
  - Setup: Each example evaluated by 3 annotators (degree and meme familiarity required).
  - Coherence: GRPO "96%" vs ExPO-HM "100%".
  - Helpfulness: Average "1.6" (GRPO) vs "2.2" (ExPO-HM); normalized "4.1" vs "5.5"; alignment with LLM judge improvements.
- Extended reasoning evaluation (Appendix E):
  - Multiple judges (GPT-4o mini, GPT-5, Qwen3-4B-Instruct-2507, gpt-oss-20b, gemma-3-4b-it) and BERTScore; ExPO-HM best across all (e.g., Qwen2.5-VL-7B BERTScore "0.65" vs GRPO "0.59").
  - Prompt sensitivity analysis: Results largely unchanged under paraphrased prompt (e.g., ExPO-HM "6.2" → "6.3").
- Fine-grained per-class results (Appendix I):
  - HatefulMemes attack types and protected categories: ExPO-HM improves across all categories (e.g., Attack Dehumanizing F1 "62.1" vs GRPO "36.5"; Protected Nationality F1 "61.1" vs GRPO "23.1").
- Implementation details:
  - Software: PyTorch "2.5.1", CUDA "12.4", HF Transformers "4.45.0", TorchMetrics "1.0.1".
  - Fine-tuning: LLaMA-Factory "0.9.3" (Qwen2.5-VL), VeRL "v0.4.1" (GRPO).
  - Inference: vLLM "0.9.2".
  - Training: 3 epochs; best checkpoint selected by validation performance.

# Conclusion
- ExPO-HM integrates policy-manual SFT warmup, GRPO with curriculum learning, and a CDE reward to improve both classification and reasoning.
- Empirical results across three datasets show SOTA performance for binary detection, fine-grained classification, and reasoning quality, with significant F1 gains over GRPO/DPO baselines and improvements in calibration and human-judged rationale quality.

# Appendix
- Ethics and intended use: Aims to mitigate hate speech; explanations support moderators and end-users; strict access controls; restricted to detection/prevention; human oversight necessary due to potential biases and cultural subjectivity; outputs depend on underlying moderation policies.
- Dataset usage and licenses: HatefulMemes per Facebook license; MAMI under Apache 2.0; PrideMM license not specified; Hatred statistics: "2,982" train, "246" test hateful memes with explanations (total "3,228").
- Dataset details and prompts: Detailed Explain-then-Detect and policy-manual prompts provided for each dataset (attack types, protected groups, targets, stance).
- Policy manual construction: Converted prose guidelines into structured bullet-point manuals for instruction SFT (examples listed).
- Training environment: 8× H100 (80GB) for GRPO/ExPO-HM (~"4 hours" runtime); SFT/DPO on 1 GPU; LoRA rank "64", α "128"; vision module frozen.
- CDE derivations and estimators: Formal derivation (Eq. 12–14), alternative estimators without logits (Eq. 15); chain rule relation H(e,d|x) = H(e|x) + H(d|e,x).
- CDE and calibration theory: Under ideal ExPO-HM policy, Brier score upper-bounded by constants from entropy thresholds a, b (e.g., worst-case bound ≈ "0.79"; balanced 50% accuracy bound ≈ "0.40").
- Extended LLM judging: Cross-judge and BERTScore results (Table 9) confirm ExPO-HM’s consistent superiority; prompt sensitivity stable (Table 10).
- Human evaluation protocol: Randomized order; two tasks (coherence yes/no; helpfulness 0–4 Likert); average helpfulness "1.6" (GRPO) vs "2.2" (ExPO-HM); Krippendorff’s α_{ordinal} "0.71".
- Error analyses: Remaining failures involve implicit hate requiring complex reasoning or external knowledge; future directions include larger implicit hate CoT corpora and retrieval augmentation.

# References
- Core citations include: HatefulMemes dataset (Kiela et al., 2020), WOAH fine-grained annotations (Mathias et al., 2021a,b), Hatred rationales (Hee et al., 2023), MAMI (Fersini et al., 2022), PrideMM (Shah et al., 2024), RA-HMD (Mei et al., 2025), GRPO (Shao et al., 2024), DPO (Rafailov et al., 2023), agentic systems LOREHM (Huang et al., 2024), U-CoT+ (Pan et al., 2025), and Qwen2.5-VL reports (Bai et al., 2025; Qwen et al., 2025). Additional references include calibration and evaluation metrics (BERTScore), CoT prompting, and surveys on meme understanding.
- Not specified in this section: Error bars or statistical significance for main F1 improvements beyond the reported CDE–LLM correlations.