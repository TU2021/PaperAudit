Summary
The paper introduces Quadratic Direct Forecast (QDF), a training objective for multi-step time-series forecasting that replaces standard point-wise losses with a quadratic-form loss weighted by a learnable covariance proxy Σ. Motivated by a Gaussian likelihood (Eq. (2), Theorem 3.1), the authors argue two deficiencies of MSE/MAE: ignoring label autocorrelation across future steps and treating all steps equally. QDF learns Σ via a bilevel scheme (Definition 3.2, Algorithm 1) using splits of the training data, then trains a forecasting model with the learned quadratic loss (Algorithm 2). Extensive experiments across ETT, ECL, Weather, and PEMS datasets (Table 1, Table 2, Table 3) show consistent improvements when integrating QDF with several architectures (TQNet, PDF, FedFormer, iTransformer). Ablations isolate heterogeneous weights (diagonal Σ) and autocorrelation (off-diagonals) contributions.

Soundness
- Likelihood foundation: Theorem 3.1 correctly derives a Mahalanobis-weighted NLL when residuals are Gaussian; this motivates a quadratic objective with Σ−1. However, when optimizing Σ (outer loop), the paper continues to use the “reduced” NLL (Eq. (2)) that omits log|Σ|—a term independent of θ but not independent of Σ. This omission biases Σ learning away from a proper likelihood criterion and can favor rank-deficient/ill-conditioned Σ (Section 3.2, Algorithm 1), unless regularization or a determinant penalty is added. The text acknowledges “proxy Σ targeting generalization” but does not justify dropping log|Σ| for outer optimization.
- PSD and invertibility: Σ is reparameterized via Σ = LL⊤ with softplus diagonals (Section 3.2). Cholesky factorization typically requires positive definiteness (PD). The text claims Σ ⪰ 0 (PSD) but uses Σ−1 in the loss (Eq. (2)), which is undefined for singular PSD matrices. The paper needs a consistent statement that Σ is PD and clarifies numerical handling (e.g., jitter on diagonals or pseudo-inverses).
- Hypergradient mismatch: The narrative states the outer-loop gradient is “taken through θ to Σ” (Section 3.2), implying differentiation through the inner optimization (true bilevel optimization). Algorithm 1, however, updates Σ via ∇Σ LΣ(Xout, Yout; gθ) with θ held fixed, which does not include dθ/dΣ. Without a hypergradient (or a differentiable unrolled inner loop), the claimed bilevel effect is partially unrealized. Algorithmic clarity is needed (unrolling steps, implicit gradient).
- Statistical framing: The approach is closely related to generalized least squares and Mahalanobis-weighted regression. Learning a full Σ for a single task may overfit; the paper mitigates via data splits (K) and early stopping, but no explicit regularizer or structural prior on Σ (e.g., banded Toeplitz, low-rank plus diagonal) is used. Given limited data, full T×T covariance learning can be high-variance.
- Empirics generally support gains, but some margins are small, and variance reporting is limited (Table 9 rounds to 0.000 in several cells, suggesting display precision masks variability; seeds are only 5).

Presentation
- The manuscript is clearly structured: motivation with partial correlations (Fig. 1, Fig. 5), method (Alg. 1–2), broad experiments (Tables 1–4), ablations (Table 3), sensitivity (Fig. 4), and appendices. Anchors are consistent and helpful.
- A key ambiguity is the mismatch between the stated bilevel gradient-through-θ and the pseudocode in Algorithm 1; readers may misunderstand the implemented optimization.
- Some tables/labels are confusing: “QDF(Ours) MSE/MAE” columns in Table 1 mix training objective and evaluation metrics terminology. Table 5 domain tags look incorrect for ETT (“Health”), which may be a typographical oversight.
- Complexity results (Fig. 9) lack hardware details and batch-size context, making <2 ms numbers hard to interpret.
- Proof in Appendix B is standard; deeper theory (e.g., convergence or identifiability of learned Σ proxy) is not provided.

Contribution
- The paper elevates objective design in time-series forecasting by learning a quadratic weighting matrix that simultaneously encodes cross-step error correlations and heterogeneous task weights. This is a practical, architecture-agnostic enhancement.
- While the idea is related to classical GLS and loss-shaping/meta-learning, the specific workflow—learning Σ on held-out splits to target generalization and then using Σ−1 in a static objective for model training—provides a clear and useful recipe. The empirical breadth is a strong point.
- Novelty is moderate: learning a loss or its weighting via bilevel/meta-learning has precedents; the primary new angle is explicitly targeting conditional label autocorrelation across horizons in DF settings and demonstrating consistent gains across popular benchmarks and architectures.

Strengths
- Clear motivation with empirical evidence of label autocorrelation and heteroscedasticity across horizons (Fig. 1, Fig. 5; Appendix A).
- Simple, model-agnostic algorithm with small training-time overhead (Algorithm 2; Fig. 9).
- Extensive experiments and ablations showing consistent but varied gains across datasets and models (Table 1, Table 2, Table 3; Fig. 3).
- Sensitivity and flexibility studies (Fig. 4; Table 4) suggest robustness to hyperparameters and that multiple meta-learning schemes can improve over DF.

Weaknesses
- Dropping log|Σ| when optimizing Σ (Section 3.2, Eq. (2), Algorithm 1) is theoretically problematic for likelihood-based estimation of Σ; the paper should justify or correct this.
- Bilevel implementation ambiguity: Algorithm 1 does not include a hypergradient with dθ/dΣ, contrary to the text claim. Unrolling or implicit gradient details are missing.
- Overfitting risk and identifiability: learning a full dense Σ without structure or regularization may be high-variance; no spectral/condition-number control is discussed.
- Empirical reporting: Some gains are small; variance presentation is limited (Table 9 precision shows 0.000 sd in several places). Hardware and runtime measurement context are missing.
- Minor inconsistencies: PSD vs PD for invertibility of Σ−1; mislabeled domains in Table 5.

Questions
- When optimizing Σ in the outer loop, why is the log|Σ| term omitted even though it depends on Σ? Would including it change results materially, and is QDF best viewed as GLS rather than full NLL?
- Do you compute a hypergradient that accounts for dθ/dΣ (e.g., via unrolled inner steps or implicit differentiation)? If yes, please update Algorithm 1 to reflect the implementation and provide details (steps unrolled, memory cost).
- How do you ensure Σ−1 exists numerically? Is Σ forced to be PD (not PSD) via Cholesky with positive diagonals and a jitter term? What is the typical condition number of Σ?
- Have you tried structural priors (e.g., banded/Toeplitz Σ, low-rank-plus-diagonal, shrinkage) to reduce variance and improve interpretability?
- Can QDF generalize across datasets (learn a Σ on dataset A and apply to B)? If not, what is the scope of “static objective” across domains?
- Please provide hardware specs, batch sizes, and exact measurement protocol for Fig. 9; <2 ms for T=720 seems optimistic without context.

Rating
- Overall (10): 7 — Strong empirical motivation and breadth with a practical idea, but theoretical omissions (log|Σ| in outer loop) and algorithmic ambiguity (Algorithm 1 vs hypergradient claim in Section 3.2) weaken soundness.
- Novelty (10): 7 — Learning a covariance-weighted loss for DF is a useful twist; related to GLS and meta-learned losses, with the cross-horizon focus (Sections 3.1–3.3) being the main differentiator.
- Technical Quality (10): 6 — Correct likelihood foundation (Theorem 3.1) but incomplete treatment of Σ optimization and bilevel gradient; PSD vs PD and invertibility not fully resolved; limited regularization (Sections 3.2–3.3).
- Clarity (10): 8 — Clear motivation and organization (Fig. 1, Algorithm 2, Table 1), though Algorithm 1 and runtime context need clarification; minor inconsistencies in Table labeling (Table 5, Table 1).
- Confidence (5): 4 — Assessment based on careful reading of methods, proofs, and experiments across main text and appendices; residual uncertainty about implementation details (hypergradients, numerical handling of Σ).

---

Summary
The paper proposes QDF, a quadratic-form training objective for direct multi-step forecasting. QDF learns a weighting matrix Σ to capture cross-step label autocorrelation (off-diagonals) and heterogeneous task weights (diagonals). Σ is trained via a split-based bilevel scheme (Definition 3.2, Algorithm 1) and then fixed to train the forecasting model (Algorithm 2). Empirical results across multiple datasets and models (Table 1, Table 2, Fig. 3) show consistent improvements, and ablations (Table 3) isolate the contributions of autocorrelation and heterogeneous weighting.

Soundness
- The idea of learning a Mahalanobis metric for residuals is sound and aligns with GLS. The Gaussian residual assumption (Theorem 3.1) is a standard simplification for deriving a quadratic objective.
- The implementation treats Σ as a “generalization-targeted proxy,” which decouples it from exact covariance estimation; this is acceptable provided the goal is performance rather than statistical consistency.
- Two methodological caveats: (i) The outer-loop optimization of Σ still uses the reduced NLL (Eq. (2)), omitting log|Σ|, which is acceptable if QDF is framed as a learned quadratic weighting (not likelihood), but then the likelihood motivation (Section 3.1) should be framed as inspiration, not a precise objective. (ii) Algorithm 1 does not show differentiation through θ updates; if the actual code unrolls inner steps or uses implicit gradients, it should be documented.
- The PSD/PD issue is solvable in practice via Cholesky with positive diagonals (Section 3.2), but the text should state that Σ is PD to ensure Σ−1 exists.

Presentation
- The paper is well-written and easy to follow. Figures and tables are informative, especially Fig. 1/Fig. 5 for motivation, Table 3 for ablations, and Fig. 4 for sensitivity.
- Minor clarity issues: Algorithm 1 pseudocode vs text claim about hypergradients; evaluation column naming in Table 1; lack of hardware/runtime context in Fig. 9; slight domain label errors in Table 5.

Contribution
- Practical and broadly applicable: QDF is architecture-agnostic and provides a drop-in improvement to many DF models.
- The focus on horizon-wise autocorrelation and heterogeneous weighting in the objective is a useful contribution to a literature often concentrated on architectures.
- Relative novelty is moderate since it builds on known ideas (GLS, meta-learned loss), but the instantiation for multi-step DF with a learned Σ and strong empirical validation is valuable.

Strengths
- Good empirical coverage across datasets and models (Table 1, Table 2, Fig. 3).
- Clear ablation demonstrating that both diagonal and off-diagonal learning matter (Table 3).
- Sensitivity studies (Fig. 4) and comparison with meta-learning variants (Table 4) lend credibility and show robustness.

Weaknesses
- Theoretical gap in Σ learning (log-determinant omission) relative to the likelihood framing (Section 3.1–3.2).
- Ambiguity around the bilevel implementation (Algorithm 1) and whether hypergradients are used.
- No structural regularization on Σ; potential overfitting and interpretability concerns.
- Variance reporting limited; Table 9 displays almost zero standard deviations due to rounding precision.

Questions
- Can you clarify whether the implemented outer-loop uses unrolled differentiation through inner steps (Nin > 0)? If so, how many steps and what is the memory footprint?
- Why was log|Σ| omitted when optimizing Σ? If this is by design (GLS-style weighting), please reframe the theory to reflect that choice and discuss potential numerical consequences.
- Have you considered structured Σ (e.g., banded/chordal/Toeplitz) to reflect temporal locality and reduce parameter count?
- What is the typical conditioning of Σ−1 across datasets (e.g., eigenvalue spectra)? Any stabilization tricks used?
- Please add runtime measurement details (GPU/CPU model, batch sizes, precision, number of elements per forward/backward) for Fig. 9.

Rating
- Overall (10): 8 — Strong empirical gains and a practical method, with minor theoretical and implementation ambiguities (likelihood vs learned weighting; Algorithm 1 hypergradient claim).
- Novelty (10): 6 — Builds on established ideas (GLS/meta-learning of losses) but applies them thoughtfully to DF with learned cross-horizon covariance (Sections 3.1–3.3).
- Technical Quality (10): 7 — Solid empirical methodology and ablations (Table 3, Fig. 4), but theoretical rigor on Σ optimization and bilevel gradients could be improved (Section 3.2).
- Clarity (10): 7 — Generally clear and well-organized with small mismatches (Algorithm 1 vs text, Table labels, runtime context).
- Confidence (5): 4 — High confidence in experimental reading and identification of theoretical gaps; some uncertainty about exact implementation details of the outer-loop gradients.

---

Summary
The paper proposes learning a quadratic weighting matrix Σ to train direct multi-step forecasting models, capturing autocorrelation and heterogeneous task weights, and demonstrates gains across benchmarks. QDF employs a split-based bilevel update procedure (Definition 3.2; Algorithm 1) and then trains models using the learned quadratic objective (Algorithm 2). Ablations (Table 3) and generalization studies (Fig. 3) support the approach.

Soundness
- The idea is consistent with optimizing a learned Mahalanobis distance for residuals; it can be effective even without strict probabilistic grounds.
- However, the paper’s likelihood justification is incomplete for Σ learning: Eq. (2) removes log|Σ| because it is constant w.r.t θ; when optimizing Σ, that term should not be dropped. As stated, QDF is better viewed as learned quadratic weights rather than maximum-likelihood.
- Implementation ambiguity: Algorithm 1 does not differentiate through θ; the claim that “outer loop gradient is taken through θ to Σ” suggests hypergradient but is not reflected in pseudocode. If actual code uses unrolling or implicit differentiation, it should be stated.
- Practical PD requirement for Σ (since Σ−1 is used) is not clearly stated; PSD is insufficient.

Presentation
- Organization is good (Sections 3–4; Algorithms 1–2; Tables 1–4). Motivation from partial correlations is compelling (Fig. 1; Appendix A/Fig. 5).
- Several minor inconsistencies: Table 5 domain tags (“Health” for ETT); lack of runtime hardware details (Fig. 9); rounding in Table 9 conceals variability; confusing table headers (Table 1 uses “QDF(Ours) MSE/MAE”).
- The use of marking best/second-best results is clear, though many margins are modest.

Contribution
- Addresses a genuine gap: objectives that reflect cross-horizon dependencies and varied difficulty across steps.
- Empirical demonstration across architectures and datasets increases practical relevance.
- Novelty is incremental but meaningful in this application setting.

Strengths
- Strong empirical coverage and consistent improvements (Table 1; Table 2; Fig. 3).
- Ablation isolates the roles of diagonal vs off-diagonal learning (Table 3).
- Sensitivity and meta-learning comparisons enhance credibility (Fig. 4; Table 4).

Weaknesses
- Theoretical inconsistency in Σ optimization with respect to likelihood (omission of log|Σ|).
- Lack of explicit structural regularization on Σ; potential overfitting and ill-conditioning not analyzed.
- Algorithmic ambiguity regarding hypergradients and whether Σ updates account for θ’s dependence on Σ.
- Limited reporting of statistical significance and run-time context.

Questions
- Please clarify the exact optimization of Σ: Is the outer gradient computed with unrolled inner steps (Nin > 0) and backprop through θ? If not, how is “through θ” realized?
- Did you explore structured Σ (e.g., banded) or penalize off-diagonal magnitude to reduce variance?
- Are the improvements in Table 1 statistically significant across seeds? Provide CIs for headline tables, not only Fig. 3.
- Can the learned Σ be visualized (eigen-spectrum, sparsity patterns) to support the heterogeneous/auto-correlated claims?

Rating
- Overall (10): 6 — Useful empirical contribution with practical gains, but theoretical framing of Σ learning and algorithmic clarity need tightening.
- Novelty (10): 7 — Learning horizon-wise covariance weights in DF is a focused contribution; related to known GLS/meta-loss ideas (Sections 3.1–3.3).
- Technical Quality (10): 5 — Core idea is sound, but likelihood-consistent Σ optimization and bilevel gradient implementation are insufficiently specified (Section 3.2; Algorithm 1).
- Clarity (10): 6 — Generally readable; several ambiguities and missing details (Algorithmic hypergradients, runtime context, table labeling).
- Confidence (5): 3 — Moderate confidence; conclusions rely on interpreting ambiguous implementation details.

---

Summary
This paper proposes Quadratic Direct Forecast (QDF), a training scheme that learns a covariance-like weighting matrix Σ to capture autocorrelation among forecast horizons and heterogeneous difficulty across steps, then uses Σ−1 in a quadratic loss to train models. QDF is applied as a model-agnostic enhancement with empirical gains across benchmarks (Table 1, Table 2, Fig. 3), and ablations (Table 3) support its components.

Soundness
- Conceptually aligns with learning a Mahalanobis metric for residuals; empirically valid even if not strictly likelihood-based.
- Methodological concerns:
  - The outer optimization of Σ uses a reduced NLL (Eq. (2)) without log|Σ|, which is acceptable only if the goal is learned weighting, not likelihood-consistent estimation. The text presents likelihood motivation (Section 3.1) but does not justify the deviation in outer optimization (Section 3.2).
  - Algorithm 1’s update of Σ appears to ignore dθ/dΣ, conflicting with the narrative claim of gradients “taken through θ” (Section 3.2). Without unrolling/implicit differentiation, the method reduces to alternating minimization, not bilevel meta-learning.
  - The PSD vs PD contradiction: Σ−1 needs PD; the paper states Σ ⪰ 0 while using Σ−1 (Eq. (2)). Numerical stabilization is unstated.
- Complexity claims (<2 ms at T=720, Fig. 9) lack hardware and batch details; these numbers are likely for loss computation, not full model forward/backward, limiting interpretability.

Presentation
- Generally clear layout and comprehensive experimental section. Motivation figures (Fig. 1; Appendix A/Fig. 5) substantiate the problem.
- Inconsistencies in tables (Table 1 headers; Table 5 domain labels) and algorithmic description (Algorithm 1 vs text) detract from clarity.
- Sensitivity plots (Fig. 4) are helpful; however, standard deviations in Table 9 are rounded to zero, which understates variability.

Contribution
- Practical, architecture-agnostic enhancement to objective design for DF; emphasizes label autocorrelation and heterogeneous weights, which are often neglected.
- Novelty is moderate; the idea builds on classical GLS and learned-loss paradigms but is instantiated with clear focus on multi-horizon DF and supported by broad experiments.

Strengths
- Consistent improvements across strong baselines (Table 1, Table 2).
- Ablation clearly shows the value of both diagonal and off-diagonal learning (Table 3).
- Flexibility study: meta-learning variants also improve over DF, situating QDF among related approaches (Table 4).

Weaknesses
- Theoretical gap re: Σ optimization versus likelihood (log|Σ| omission).
- Ambiguous bilevel implementation; lack of hypergradient details.
- No structural priors/regularizers on Σ; risk of overfitting or ill-conditioned Σ−1 not analyzed.
- Runtime context missing; difficult to assess overhead claims.
- Minor inaccuracies (Table 5 domains), rounding concealing variance (Table 9).

Questions
- Is QDF implemented with unrolled inner updates and backprop through them when updating Σ? If so, please specify Nin used in hypergradient and any truncation strategies.
- Did you examine conditioning/eigenvalue spectra of learned Σ across datasets to ensure numerical stability? Any diagonal jitter or spectral clipping used?
- Could a structured Σ (banded, Toeplitz, low-rank+diag) achieve similar gains with fewer parameters?
- What is the impact of learning Σ per-horizon group (e.g., blocks) compared to full dense Σ?

Rating
- Overall (10): 5 — Solid idea with broad empirical support, but key theoretical and algorithmic details are underspecified or inconsistent (Sections 3.1–3.2; Algorithm 1; Eq. (2)), limiting confidence.
- Novelty (10): 6 — Incremental but meaningful in DF objective design; related to GLS/learned loss with targeted cross-horizon weighting (Section 3).
- Technical Quality (10): 4 — Missing log|Σ| in Σ optimization, unclear bilevel gradients, and PD/PSD ambiguity weaken methodological rigor.
- Clarity (10): 6 — Good structure and visuals, but algorithmic mismatch and table inconsistencies require revision.
- Confidence (5): 4 — Reasonable confidence in identified issues based on careful reading of methods, algorithms, and appendices; exact implementation remains unclear.