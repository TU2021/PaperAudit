Summary
The paper proposes Quadratic Direct Forecast (QDF), a training objective for direct multi-step time-series forecasting that replaces standard point-wise losses with a quadratic-form loss weighted by a learned matrix Σ. Motivated by a Gaussian residual model, the method aims to address two deficiencies of MSE/MAE: ignoring autocorrelation across future horizons and treating all horizons equally. QDF learns Σ via a split-based bilevel scheme: an outer loop optimizes Σ on held-out splits to target generalization, and an inner loop fits the forecasting model parameters; the learned Σ is then fixed and used to train the final forecasting model with a Mahalanobis-weighted loss. Σ is parameterized through a factorization (e.g., Σ = LL⊤) and is intended to encode both heterogeneous per-horizon weights (diagonal entries) and cross-horizon correlations (off-diagonals). Experiments across multiple datasets (ETT, ECL, Weather, PEMS) and architectures (TQNet, PDF, FedFormer, iTransformer) show consistent, though sometimes modest, improvements. Ablations isolate the contributions of diagonal versus off-diagonal structure, sensitivity analyses suggest robustness to hyperparameters, and comparisons with meta-learning variants position QDF as a practical, architecture-agnostic enhancement to objective design.

Strengths
- Clear motivation grounded in empirical evidence of label autocorrelation and heterogeneous horizon difficulty, supported by informative figures and analyses.
- Practical, model-agnostic approach that can be integrated into diverse architectures with modest training-time overhead.
- Broad empirical evaluation across standard benchmarks and strong baselines, with generally consistent improvements.
- Ablation studies that disentangle the roles of diagonal weighting and cross-horizon autocorrelation, demonstrating that both components contribute to performance.
- Sensitivity and flexibility investigations indicating robustness to hyperparameter choices and demonstrating that related meta-learning schemes also yield improvements.
- Well-structured presentation with a coherent workflow (learn Σ on splits, then train with a fixed quadratic loss), making the recipe accessible to practitioners.

Weaknesses
- Theoretical inconsistency in the likelihood framing for Σ learning: the outer-loop optimization uses a reduced negative log-likelihood that omits the log|Σ| term, which depends on Σ. Some reviewers view this omission as acceptable if QDF is framed as learning a quadratic weighting rather than maximum-likelihood estimation, but the current text presents the likelihood motivation without adequately justifying the deviation, raising concerns about bias toward ill-conditioned or rank-deficient Σ.
- Ambiguity in the bilevel implementation: the narrative suggests gradients are propagated “through θ to Σ,” but the provided algorithm updates Σ without accounting for dθ/dΣ. Without unrolling or implicit differentiation, the procedure resembles alternating minimization rather than full bilevel meta-learning, and the implementation details needed to reconcile this mismatch are absent.
- Positive definiteness versus semi-definiteness: the method uses Σ−1 in the loss, which requires Σ to be positive definite. The text alternates between PSD and PD terminology and does not clearly describe numerical stabilization (e.g., enforcing strictly positive diagonals, adding jitter, or conditioning controls), leaving questions about invertibility and stability of Σ−1.
- Overfitting and identifiability risks when learning a full dense T×T matrix Σ without structural priors or regularization (e.g., banded/Toeplitz, low-rank-plus-diagonal, shrinkage). The paper relies on data splits and early stopping but does not analyze the variance or conditioning of the learned Σ, which may be high with limited data.
- Empirical reporting issues reduce interpretability: some gains are modest; variance reporting is limited (e.g., standard deviations rounded to zero in a table), and statistical significance is not systematically discussed. Runtime and complexity claims lack hardware, batch-size, and measurement protocol details, making timing numbers difficult to assess.
- Minor presentation inconsistencies, such as confusing table headers mixing objective and evaluation metric terminology, mislabeling of dataset domains, and a lack of deeper theoretical guarantees (e.g., convergence or identifiability of the learned Σ proxy), detract from clarity.
