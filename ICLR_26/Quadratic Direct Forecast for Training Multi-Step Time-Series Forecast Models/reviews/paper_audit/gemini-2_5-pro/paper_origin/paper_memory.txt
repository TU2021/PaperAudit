# Global Summary
This paper identifies two key issues with standard training objectives (like MSE) for multi-step time-series forecasting: they overlook the autocorrelation in the future label sequence, leading to a biased objective, and they assign equal weights to all future time steps, missing an opportunity for performance gains via heterogeneous weighting. To address this, the authors propose a novel quadratic-form weighted training objective, where the off-diagonal elements of a weighting matrix account for label autocorrelation and the diagonal elements provide heterogeneous task weights. They introduce the Quadratic Direct Forecast (QDF) algorithm to learn this weighting matrix adaptively. QDF uses a bilevel optimization scheme where the matrix is updated to improve the generalization performance of the forecast model on a holdout set. Experiments across eight benchmark datasets show that QDF, when applied to various state-of-the-art models (like TQNet), consistently improves their performance, achieving new state-of-the-art results. For example, on the PEMS08 dataset, QDF reduces both MSE and MAE by 0.019 compared to the base model. The method is model-agnostic and its effectiveness is validated through extensive ablations and sensitivity analyses.

# Abstract
The paper argues that common training objectives for time-series forecasting, like mean squared error (MSE), are suboptimal. It identifies two main problems: (1) they overlook label autocorrelation among future steps, causing a biased objective, and (2) they fail to assign heterogeneous weights to forecasting tasks for different future steps. The authors propose a quadratic-form weighted training objective to solve both issues simultaneously. The off-diagonal elements of the weighting matrix handle autocorrelation, while non-uniform diagonals set task-specific weights. A learning algorithm called Quadratic Direct Forecast (QDF) is introduced to train the forecast model using this adaptively updated quadratic weighting matrix. Experiments demonstrate that QDF improves the performance of various forecast models and achieves state-of-the-art results.

# Introduction
- The paper positions its work as focusing on the underexplored area of learning objective design for time-series forecasting, in contrast to the heavily researched area of neural architecture design (e.g., Transformers, linear models).
- It identifies two primary challenges with current learning objectives like MSE:
    1.  **Label autocorrelation effect:** MSE ignores the correlation within the label sequence, making it a biased objective.
    2.  **Heterogeneous task weights:** MSE treats the prediction of each future step as an equally weighted task, which is a missed opportunity for optimization.
- The core contribution is a novel quadratic-form weighted training objective designed to address both challenges. The off-diagonal elements of its weighting matrix model autocorrelation, while the diagonal elements allow for heterogeneous task weights.
- The paper proposes the Quadratic Direct Forecast (QDF) algorithm to apply this objective, which involves adaptively updating the weighting matrix.
- Comprehensive experiments are claimed to show that QDF enhances the performance of state-of-the-art models across diverse datasets.

# Preliminaries
- **Problem Definition:** The task is multi-step time-series forecasting. Given a historical sequence X of length H, the goal is to learn a model g_θ that predicts a future sequence Ŷ of length T to approximate the true label sequence Y. The paper focuses on the direct forecasting (DF) paradigm. For simplicity, derivations are shown for the univariate case (D=1).
- **Neural Architectures:** The paper briefly reviews common architectures like RNNs, CNNs, GNNs, and the more recent Transformer-based models (e.g., TQNet, PatchTST, iTransformer) and linear models (e.g., TimeMixer, DLinear).
- **Learning Objectives:** The standard objective is Mean Squared Error (MSE), defined as L_mse = ||Y - g_θ(X)||^2.
- **MSE Limitations:** MSE is described as a biased objective because it neglects autocorrelation in the label sequence.
- **Alternative Objectives:** Previous work has explored shape-level alignment or transforming labels into decorrelated components (e.g., FreDF, Time-o1) to mitigate bias, but these are noted to lack theoretical guarantees for complete bias elimination.

# Method
This section is divided into "Motivation" and the description of the QDF algorithm.

# Motivation
- The motivation starts from a likelihood maximization perspective. Assuming forecast errors follow a multivariate Gaussian distribution, the Negative Log-Likelihood (NLL) is a quadratic form: L_Σ = (Y - g_θ(X))ᵀ Σ⁻¹ (Y - g_θ(X)), where Σ is the conditional covariance of the label sequence.
- This formulation highlights two challenges:
    1.  **Autocorrelation effect:** Requires modeling the off-diagonal elements of Σ⁻¹.
    2.  **Heterogeneous weights:** Requires modeling the non-uniform diagonal elements of Σ⁻¹.
- Directly minimizing this NLL is infeasible because the true conditional covariance Σ is unknown and hard to estimate. MSE implicitly assumes Σ is an identity matrix.
- Prior methods like FreDF and Time-o1 only guarantee marginal, not conditional, decorrelation, thus failing to fully address the autocorrelation effect.
- **Case Study (ECL dataset, T=96):**
    - The partial correlation matrix of raw labels shows significant off-diagonal values (over 61.4% exceed 0.1), confirming strong autocorrelation.
    - Conditional variances differ considerably across future steps, supporting the need for heterogeneous weights.
    - Components extracted by FreDF and Time-o1 show reduced but still present off-diagonal correlations, indicating they do not completely eliminate autocorrelation.

# Experiments
- **Experimental Goals:** The experiments aim to evaluate QDF's performance, sources of its gains, versatility across models, flexibility with meta-learning methods, hyperparameter sensitivity, and computational complexity.
- **Setup:**
    - **Datasets:** ETT (ETTh1, ETTh2, ETTm1, ETTm2), Electricity (ECL), Weather, and PEMS (PEMS03, PEMS08). Datasets are chronologically split into train/validation/test.
    - **Baselines:** 10 methods including TQNet, PDF, Fredformer, iTransformer, FreTS, TimesNet, MICN, TiDE, PatchTST, and DLinear.
    - **Implementation:** Baselines reproduced from official codebases, trained with Adam optimizer on MSE loss. The "drop-last trick" is disabled for fair comparison.
- **Overall Performance (Table 1):**
    - QDF, using TQNet as the base model, achieves the best MSE and MAE on all 8 datasets.
    - On PEMS08, QDF reduces MSE and MAE by 0.019 compared to the second-best model (TQNet).
    - Qualitative examples (Fig. 2) show QDF captures subtle dynamics (trends, peaks) that the baseline DF model misses.
- **Learning Objective Comparison (Table 2):**
    - QDF is compared against Time-o1, FreDF, Koopman, Soft-DTW, and DF (MSE) when integrated into TQNet and PDF models.
    - QDF achieves the best performance across most settings.
- **Ablation Studies (Table 3):**
    - QDF† (heterogeneous weights only) consistently outperforms the DF baseline.
    - QDF‡ (autocorrelation modeling only) also consistently outperforms DF and is generally the second-best performer.
    - The full QDF model, combining both components, achieves the best results, demonstrating a synergistic effect.
- **Generalization Studies (Fig. 3):**
    - QDF is integrated into TQNet, PDF, FedFormer, and iTransformer.
    - It provides consistent performance gains for all models. For example, on the ECL dataset, it reduces MSE by 7.4% for FedFormer and 5.9% for TQNet.
- **Flexibility Studies (Table 4):**
    - The weighting matrix is optimized using meta-learning algorithms (MAML, iMAML, MAML++, Reptile).
    - All meta-learning methods outperform the DF baseline on the ECL dataset.
    - QDF's native implementation, which explicitly optimizes for out-of-sample generalization, performs better than these meta-learning alternatives. For T=96 on ECL, QDF achieves a 6.10% MSE reduction vs. DF, while MAML++ achieves 5.76%.
- **Hyperparameter Sensitivity (Fig. 4):**
    - `N_in` (inner updates): Performance improves significantly from 0 to 1, with marginal gains thereafter.
    - `K` (data splits): `K=3` achieves the best performance.
    - `η` (update rate): The method is robust to a wide range of `η > 0`.
- **Complexity (Appendix D.7):**
    - The running time for QDF's updates remains below 2 ms even with a forecast horizon T=720.
    - The additional computation is only in the training phase and adds no overhead to inference.

# Conclusion
- The paper identifies and addresses two key challenges in designing learning objectives for time-series forecasting: the label autocorrelation effect and heterogeneous task weights.
- It proposes a quadratic-form weighted training objective and the QDF learning algorithm to adaptively learn the weighting matrix.
- Experiments show QDF consistently improves the performance of various forecasting models.
- **Limitations & Future Work:**
    - The current QDF relies on a fixed quadratic objective. A potential enhancement is to use a hyper-network to generate a more flexible objective.
    - The principles of QDF could be extended to other domains with similar challenges, such as user rating prediction or dense image prediction.

# Appendix
- **Reproducibility:** An anonymous code link is provided. Proofs and dataset details are included in the appendix.
- **Label Autocorrelation Estimation:** Details the use of partial correlation coefficients to measure label autocorrelation while controlling for the historical sequence X. The high computational cost of this estimation justifies its use for case studies only, not for model training. Figure 5 extends the case study to four additional datasets, confirming the widespread presence of label autocorrelation.
- **Theoretical Justification:** Provides a standard proof for the NLL formulation under a multivariate Gaussian assumption (Theorem B.1).
- **Reproduction Details:**
    - Table 5 provides statistics for all datasets (number of variates, sample counts, frequency).
    - Forecast horizons are {96, 192, 336, 720} for most datasets and {12, 24, 36, 48} for PEMS.
    - Training details include Adam optimizer, learning rates from {1e-3, ..., 5e-5}, and early stopping with patience of 3.
- **Additional Experimental Results:**
    - The appendix includes detailed tables and figures for overall performance (Table 6), qualitative examples (Figs 6, 7), objective comparisons (Table 7), and generalization studies (Fig. 8).
    - **Varying History Lengths (Table 8):** QDF consistently improves TQNet and PatchTST on the Weather dataset for input lengths of 96, 192, 336, and 720.
    - **Random Seed Sensitivity (Table 9):** Results from 5 different random seeds show minimal standard deviation, indicating robustness.
    - **Complexity (Fig. 9):** Running time for forward and backward passes of the objective is measured, showing it stays below 2 ms even for T=720.

# References
This section contains a list of references cited in the manuscript.