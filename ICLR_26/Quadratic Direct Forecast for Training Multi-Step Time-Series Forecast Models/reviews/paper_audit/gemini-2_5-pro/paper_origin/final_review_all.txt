1) Summary
This paper addresses two limitations of the standard mean squared error (MSE) objective in multi-step time-series forecasting: its failure to account for autocorrelation in the forecast horizon and its uniform weighting of prediction tasks at different future steps. The authors propose a quadratic-form weighted training objective, where a learnable weighting matrix's off-diagonal elements model autocorrelation and its diagonal elements provide heterogeneous task weights. They introduce the Quadratic Direct Forecast (QDF) algorithm, a bilevel optimization procedure that learns this weighting matrix to improve the model's generalization performance on a holdout set. The learned objective is then used to train a variety of forecasting models. Experiments across several benchmark datasets show that QDF consistently improves the performance of state-of-the-art models, achieving new state-of-the-art results.2) Strengths
*   **Well-Motivated Problem Formulation**
    *   The paper clearly identifies and provides evidence for two fundamental challenges in designing learning objectives for time-series forecasting: the "label autocorrelation effect" and the need for "heterogeneous task weights" (Section 3.1).
    *   The motivation is grounded in the negative log-likelihood (NLL) formulation under a Gaussian assumption (Theorem 3.1), which provides a principled basis for the proposed quadratic-form objective (Equation 2).
    *   Empirical evidence from a case study on the ECL dataset effectively visualizes these challenges, showing significant off-diagonal partial correlations and varying conditional variances across the forecast horizon (Figure 1a). This analysis is extended to more datasets in the appendix (Figure 5).
    *   The paper also demonstrates that prior methods attempting to decorrelate labels, such as FreDF and Time-o1, do not fully eliminate conditional autocorrelation (Figure 1b, Section 3.1), strengthening the case for a more direct approach.*   **Novel and Principled Method**
    *   The core contribution, QDF, is a novel learning algorithm that treats the weighting matrix `Σ` of the quadratic loss as learnable parameters. This elegantly sidesteps the intractable problem of directly estimating the true conditional covariance matrix.
    *   The use of a bilevel optimization framework (Definition 3.2) to learn `Σ` by optimizing for generalization performance on a holdout split of the training data is a clever and sound approach.
    *   The technical implementation is well-considered, employing Cholesky factorization to enforce the positive semi-definite constraint on `Σ` (Section 3.2, "Re-parameterization") and detailing the update procedure clearly in Algorithm 1 and the overall workflow in Algorithm 2.
    *   The method is presented as model-agnostic, which significantly increases its potential impact as a general-purpose tool for improving existing and future forecasting models (Section 3.3).*   **Comprehensive and Rigorous Experimental Evaluation**
    *   The paper demonstrates state-of-the-art performance by applying QDF to a strong backbone model (TQNet) and comparing it against a wide range of 10 recent and competitive baselines across eight benchmark datasets (Table 1).
    *   A thorough ablation study is conducted to isolate the contributions of the two key components of the learned weighting matrix: heterogeneous weights (diagonals, QDF†) and autocorrelation modeling (off-diagonals, QDF‡). The results clearly show that both components contribute to performance, with their combination yielding the best results (Table 3, Section 4.4).
    *   The versatility of QDF is explicitly tested by applying it to four different modern forecasting architectures (TQNet, PDF, FedFormer, iTransformer), showing consistent performance improvements across the board (Figure 3, Section 4.5).
    *   The experiments include comparisons with other advanced learning objectives (Time-o1, FreDF, Koopman, Soft-DTW), where QDF consistently performs the best (Table 2, Section 4.3).
    *   The authors provide extensive supplementary experiments, including sensitivity analyses for key hyperparameters (Figure 4), studies on varying input lengths (Table 8), and analysis of computational complexity (Appendix D.7, Figure 9).3) Weaknesses
*   **Unclear Practical Computational Overhead**
    *   The complexity analysis focuses on the time for a single forward/backward pass of the loss function (Appendix D.7, Figure 9), which is shown to be minimal (under 2ms). However, this does not capture the full computational cost of the QDF training procedure.
    *   The overall workflow involves an outer loop to learn `Σ` (Algorithm 2) and an inner loop to update model parameters `θ` (Algorithm 1). The total training time depends heavily on the number of outer (`N_out`) and inner (`N_in`) updates required for convergence, which is not reported.
    *   Without a direct comparison of total wall-clock training time against a standard MSE-based training (DF), it is difficult to assess the practical feasibility and overhead of adopting QDF, especially for large datasets or complex models.*   **Simplification of Conditional Covariance**
    *   The theoretical motivation (Theorem 3.1) introduces the conditional covariance matrix `Σ`, which is conditioned on the historical sequence `X`.
    *   However, the proposed method learns a single, global weighting matrix `Σ` that is applied to all samples in the dataset (Algorithm 2, steps 6-7). This is a significant simplification, as it assumes the error covariance structure is stationary and does not depend on the specific input `X`.
    *   The paper does not explicitly discuss or justify this simplification. This assumption may not hold for non-stationary time series where the volatility and correlation structure of forecast errors could change over time or depend on the input's regime. The potential limitations of using a global `Σ` are not addressed.*   **Insufficient Detail in Meta-Learning Comparison**
    *   Section 4.6 and Table 4 compare QDF with several established meta-learning algorithms (MAML, iMAML, MAML++, Reptile). The paper claims QDF has a "distinct advantage" because it "explicitly optimize[s] the weighting matrix for out-of-sample generalization."
    *   The manuscript provides very little detail on how these meta-learning baselines were adapted to the task of learning the weighting matrix `Σ`. It is unclear if they were set up to solve the exact same bilevel optimization problem as QDF or a different formulation.
    *   The performance improvements of QDF over the meta-learning methods are relatively small (e.g., on ECL T=96, QDF's MSE reduction is 6.10% vs. 5.76% for MAML++, as shown in Table 4). This small margin, combined with the lack of implementation details, makes it difficult to fully appreciate the claimed "distinct advantage" of the proposed update rule.*   **Inconsistencies and Inaccuracies in Presentation**
    *   The structure of some experimental tables is confusing. For example, Table 8 (Appendix D.5) presents results for TQNet and PatchTST but also includes an unexplained, separate "DF MSE" column, which makes the comparison difficult to interpret.
    *   Annotations in tables are inconsistent. The note for Table 3 states that second-best results are "underlined", but no results are underlined in the table. Furthermore, special formatting (`***`) is used for the ECL dataset results without any explanation in the caption.
    *   There is a mismatch between figure captions and their layout. The consolidated caption for Figure 3 provided in the text of Section 4.4 does not match the ordering of the sub-figures as they are presented in the manuscript.
    *   The supplementary material contains factual inaccuracies. The LLM usage statement in Appendix E claims the use of "GPT-5" and "Google Gemini 2.5", which are non-existent models as of the time of this review. This suggests a lack of care in the manuscript's preparation.4) Suggestions for Improvement
*   **Provide Comprehensive Training Time Analysis**
    *   To address the unclear computational overhead, please report the total wall-clock training time for QDF compared to the standard Direct Forecast (DF) baseline.
    *   This comparison should be provided for a few representative experimental settings (e.g., on a large dataset like ECL with a long forecast horizon like T=720).
    *   Please also report the typical number of outer loop iterations (`N_out` in Algorithm 2) required for `Σ` to converge, as this is a key factor in the overall training time.*   **Acknowledge and Justify the Global Weighting Matrix**
    *   In Section 3.3 or the discussion, please explicitly acknowledge that the method learns a single, global `Σ` as a simplification of the input-dependent conditional covariance `Σ(X)`.
    *   Discuss the rationale for this design choice (e.g., tractability, parameter efficiency) and its potential limitations, particularly for non-stationary data.
    *   Briefly mentioning that an input-dependent `Σ` (e.g., via a hypernetwork) is a direction for future work, as noted in the conclusion (Section 5), would strengthen this discussion by showing awareness of the limitation.*   **Elaborate on Meta-Learning Baseline Implementations**
    *   In Section 4.6, please provide more specific details on how the meta-learning baselines (MAML, iMAML, etc.) were implemented for the comparison in Table 4.
    *   Clarify whether these methods were also used to learn the parameters of the weighting matrix `Σ` and how the "meta-tasks" were constructed from the training data splits.
    *   A more detailed explanation would make the comparison more transparent and allow readers to better understand the empirical differences between QDF's update rule and standard first-order meta-learning algorithms.*   **Improve Clarity and Correct Inaccuracies in Presentation**
    *   Please clarify the structure of Table 8 by explaining what the "DF MSE" column represents or by correcting the table to remove the ambiguity.
    *   Ensure all table annotations are consistent with the content. For Table 3, either add the underlining for second-best results or remove the statement from the note, and explain any special formatting used.
    *   Correct the text caption for Figure 3 in Section 4.4 to match the actual layout and labels of the sub-figures.
    *   Thoroughly review all supplementary material for factual accuracy, including the LLM usage statement in Appendix E, to bolster the manuscript's overall credibility.5) Score
*   Overall (10): 7 — The paper presents a well-motivated and novel method with strong empirical results (Table 1), but its contribution is somewhat undermined by several presentation issues and reporting inconsistencies (Table 3, Table 8).
*   Novelty (10): 8 — The formulation of learning a quadratic loss via bilevel optimization for generalization is a novel and impactful contribution to time-series forecasting objectives (Section 3.2).
*   Technical Quality (10): 7 — The method is technically sound, but the experimental reporting contains inconsistencies (e.g., Table 8) and the manuscript has factual inaccuracies (Appendix E) that detract from its overall rigor.
*   Clarity (10): 7 — While the core method is explained well (Section 3), the paper suffers from several clarity issues in its experimental reporting, including inconsistent table annotations (Table 3) and mismatched figure captions (Section 4.4).
*   Confidence (5): 5 — I am highly confident in my assessment, as my expertise aligns well with time-series forecasting and meta-learning.