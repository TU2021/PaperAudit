# Innovation & Motivation Report

## 1. The Authors' Claimed Contribution
- Core Problem: Multi-step time-series forecasting commonly trains with MSE, which treats future steps independently and equally, misaligning training with the true dependency and heterogeneity across horizons.
- Claimed Gap: "MSE is prevalent but biased due to label autocorrelation and equal weighting across future steps." (Introduction)
- Proposed Solution: A quadratic-form negative log-likelihood objective L_Σ with a learnable weighting matrix Σ whose off-diagonal entries encode label autocorrelation and diagonal entries encode heterogeneous task weights. The Quadratic Direct Forecast (QDF) algorithm uses bilevel optimization to learn Σ on disjoint chronological splits, then trains base models with L_Σ. The formulation is grounded in: "Under multivariate Gaussian forecast errors, NLL is quadratic with weight Σ^{-1}: L_Σ(X,Y;g_θ) = (Y − g_θ(X))^⊤ Σ^{-1} (Y − g_θ(X))." (Method, Theoretical formulation)

## 2. Comparative Scrutiny (The "Trial")
Here, we analyze how the paper stands against the identified similar works.

### vs. MQRetNN: Multi-Horizon Time Series Forecasting with Retrieval Augmentation
- Identified Overlap: Both exploit overlooked dependencies to improve multi-horizon forecasts—QDF across time steps via a learned covariance-weighted objective; MQRetNN across entities via cross-entity attention and retrieval.
- Manuscript's Defense:
  - Citation status: MQRetNN is not cited in the provided manuscript summary.
  - General differentiation: The manuscript positions its contribution at the objective level, not architectural: "Conceptually related to meta-learning but differs in goal (static objective) and validation (holdout from same task)." (Method, 3.3 Workflow) It explicitly critiques MSE’s independence and uniform weighting: "MSE implicitly assumes Σ = I (identity), thus not modeling either effect." (Motivation) It also distinguishes from label-transform objectives: "Prior label-transform methods (FreDF, Time-o1) provide only marginal decorrelation, not conditional decorrelation; equal component weights persist." (Motivation)
- Reviewer's Assessment: Overlap is conceptual (learned coupling), but the dependency axis differs (cross-horizon vs. cross-entity) and the mechanism differs (loss shaping vs. attention/retrieval). QDF’s objective-level approach is model-agnostic and incurs no inference overhead; MQRetNN’s gains hinge on architecture and population-wide retrieval. The distinction is substantive enough; MQRetNN does not diminish QDF’s novelty in loss-function design for cross-horizon coupling.

### vs. LLM-Mixer: Multiscale Mixing in LLMs for Time Series Forecasting
- Identified Overlap: Both encode cross-horizon structure—LLM-Mixer via multiscale decomposition and LLM mixing; QDF via a learned covariance in the training objective.
- Manuscript's Defense:
  - Citation status: LLM-Mixer is not cited in the provided manuscript summary.
  - General differentiation: The manuscript frames an NLL-driven, covariance-weighted objective rather than a multiscale architecture: "Under multivariate Gaussian forecast errors, NLL is quadratic with weight Σ^{-1}..." (Method) It argues existing shape-level losses (e.g., Soft-DTW) "emphasize autocorrelation but lack guarantees" (Preliminaries) and empirically shows QDF improves over Soft-DTW (Table 2).
- Reviewer's Assessment: QDF targets training objective misspecification under correlated horizons; LLM-Mixer targets representation with a frozen LLM and decomposition. The methods operate on different layers (objective vs. architecture). QDF’s probabilistic grounding and bilevel learning provide a principled alternative to architectural mixing; the resemblance does not erode QDF’s claimed gap.

### vs. Latent Gaussian Count Time Series
- Identified Overlap: Both leverage Gaussian-based likelihoods and covariance structures to encode temporal dependence.
- Manuscript's Defense:
  - Citation status: Not cited in the provided manuscript summary.
  - General differentiation: QDF’s focus is continuous-valued multi-horizon forecasting with a learned Σ deployed as a static training objective; it states: "Σ is the conditional covariance given X" but "Σ [is] unknown and intractable to estimate per X" and thus learned via bilevel optimization over chronological splits (Method, 3.2 Learning weighting matrix). It contrasts this with prior label transforms that only partially decorrelate. (Motivation)
- Reviewer's Assessment: The overlap is at the probabilistic principle (Gaussian quadratic forms), but the application domains and training strategies differ: the similar work addresses stationary count series and latent processes; QDF designs a learnable training objective for direct multi-horizon forecasting. The distinction is clear; this similarity does not undermine the novelty in learning Σ for cross-horizon training.

### vs. SCINet: Time Series Modeling and Forecasting with Sample Convolution and Interaction
- Identified Overlap: Both operationalize inter-step dependencies—SCINet architecturally via downsample–convolve–interact; QDF through objective-level covariance coupling.
- Manuscript's Defense:
  - Citation status: SCINet appears in the references list in the manuscript summary ("linear/SSM models (SCINet, TimeMixer, DLinear)").
  - General differentiation: The manuscript situates itself as objective-centric rather than architecture-centric: "Learning objectives are underexplored; MSE is prevalent but biased..." (Introduction) and proposes a model-agnostic loss with learnable Σ. It empirically tests across transformer and non-transformer baselines (Experiments 4.2).
- Reviewer's Assessment: While both address temporal dependence, QDF’s contribution is orthogonal to SCINet’s architecture. The defense (model-agnostic objective; consistent gains across models) is credible and backed by cross-model improvements. The overlap does not weaken QDF’s motivation.

### vs. A Multi-Horizon Quantile Recurrent Forecaster (MQ-RNN)
- Identified Overlap: Both are direct multi-horizon probabilistic forecasting frameworks; MQ-RNN uses quantile losses; QDF uses Gaussian NLL with learned covariance across horizons.
- Manuscript's Defense:
  - Citation status: MQ-RNN is not cited in the provided manuscript summary.
  - General differentiation: QDF makes cross-horizon coupling explicit in the objective via Σ’s off-diagonals and horizon-wise heterogeneity via non-uniform diagonals, aiming to correct "biased training when ignored" (Motivation). It empirically compares against alternative objectives (Time-o1, FreDF, Soft-DTW) and shows consistent improvements (Table 2).
- Reviewer's Assessment: MQ-RNN and QDF differ in the probabilistic scoring rule and in how horizon coupling is handled (implicit via sequence states vs. explicit via Σ). Absence of explicit citation is a weakness, but the technical distinction is genuine. The resemblance suggests QDF should more directly position itself relative to widely used quantile losses; nonetheless, the proposed loss-learning approach remains novel within direct multi-horizon training.

### vs. Mean-Variance Optimization and Algorithm for Finite-Horizon MDPs
- Identified Overlap: Both recast horizon-dependent training as a bilevel optimization where outer parameters shape inner learning.
- Manuscript's Defense:
  - Citation status: Not cited in the provided manuscript summary.
  - General differentiation: QDF’s upper level learns Σ subject to PSD constraints via Cholesky ("Σ = LL^⊤ with lower-triangular L and positive diagonals (softplus)"), and uses chronological splits: "min_{Σ ⪰ 0} L_Σ(D_out; g_{θ*}) where θ* = arg min_θ L_Σ(D_in; g_θ)." (Method, 3.2) It also clarifies its relationship to meta-learning: "Conceptually related to meta-learning but differs in goal (static objective) and validation (holdout from same task)." (Method, 3.3)
- Reviewer's Assessment: The overlap is methodological (bilevel) rather than topical; the defense is implicit in scope and formulation. QDF’s bilevel design is a reasonable instantiation for loss learning in forecasting; this similarity does not detract from its claimed gap in objective misspecification.

## 3. Novelty Verdict
- Innovation Type: Incremental
- Assessment:
  The manuscript identifies a clear and under-addressed gap in multi-horizon forecasting objectives—independence and uniform weighting across future steps—and proposes a principled, Gaussian NLL-based quadratic objective with a learned covariance/weight matrix Σ. While the quadratic form and generalized least squares rationale are standard, the specific instantiation—learning Σ via a bilevel procedure to encode both cross-horizon autocorrelation (off-diagonals) and heterogeneous step weights (diagonals), and deploying it as a static, model-agnostic training objective—constitutes a focused, practically meaningful innovation. The paper strengthens its defense by:
  - Anchoring the objective in likelihood ("Under multivariate Gaussian forecast errors, NLL is quadratic with weight Σ^{-1}...").
  - Arguing and demonstrating that label-transform and shape-alignment objectives are insufficient ("Prior label-transform methods (FreDF, Time-o1) provide only marginal decorrelation..."; Table 2 shows QDF outperforming Time-o1, FreDF, Soft-DTW).
  - Showing generalization across diverse models and datasets (Tables 1–2; Fig. 3).
  The most similar works provided either adapt architectures (SCINet, ProNet, LLM-Mixer) or address different dependency axes (cross-entity, hierarchical, count series), and do not directly contest the loss-level coupling proposed here. A weakness is the absence of explicit comparison or citation of well-known direct probabilistic frameworks like MQ-RNN; positioning against quantile-based objectives would strengthen the motivation. Overall, the paper survives the comparison: its contribution is an objective-level, model-agnostic mechanism for encoding cross-horizon dependence and heterogeneity with consistent empirical gains.
  - Strength:
    - Clear, theoretically motivated objective that addresses a tangible source of bias (autocorrelation and heterogeneity).
    - Bilevel learning of Σ with PSD guarantees, no inference overhead, and cross-model applicability.
    - Empirical superiority over relevant alternative objectives (Time-o1, FreDF, Soft-DTW, DF) across multiple benchmarks.
  - Weakness:
    - Reliance on Gaussian error assumption; static Σ learned globally may limit adaptability to conditioning on X.
    - Missing explicit positioning against widely used quantile-loss direct forecasters (e.g., MQ-RNN).
    - The core mathematical form (quadratic NLL) is standard; novelty resides primarily in the practical loss-learning instantiation.

## 4. Key Evidence Anchors
- Introduction: "MSE is prevalent but biased due to label autocorrelation and equal weighting across future steps."
- Method, Theoretical formulation: "Under multivariate Gaussian forecast errors, NLL is quadratic with weight Σ^{-1}: L_Σ(X,Y;g_θ) = (Y − g_θ(X))^⊤ Σ^{-1} (Y − g_θ(X))."
- Method, 3.2 Learning weighting matrix: "min_{Σ ⪰ 0} L_Σ(D_out; g_{θ*}) where θ* = arg min_θ L_Σ(D_in; g_θ)." and "Σ = LL^⊤ with lower-triangular L and positive diagonals (softplus)."
- Motivation: "MSE implicitly assumes Σ = I (identity), thus not modeling either effect." and "Prior label-transform methods (FreDF, Time-o1) provide only marginal decorrelation, not conditional decorrelation; equal component weights persist."
- Method, 3.3 Workflow: "Conceptually related to meta-learning but differs in goal (static objective) and validation (holdout from same task)."
- Experiments, Table 2: Empirical comparison showing QDF > Time-o1, FreDF, Soft-DTW, DF across datasets on TQNet and PDF.
- Experiments, Table 1: Cross-model dataset-wide performance where QDF paired with TQNet achieves best averages.
- Motivation/Appendix A: Case-study evidence of substantial off-diagonal partial correlations ("over 61.4% exceeding 0.1") and heterogeneous conditional variances across future steps.