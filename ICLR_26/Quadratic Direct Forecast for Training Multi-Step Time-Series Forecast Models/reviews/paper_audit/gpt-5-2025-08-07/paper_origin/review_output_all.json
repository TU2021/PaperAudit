{
  "baseline_review": "Summary\n- The paper addresses the design of learning objectives for multi-step direct time-series forecasting. It argues that conventional MSE treats future steps as independent and equally weighted, overlooking label autocorrelation and heterogeneous task difficulty. The authors propose a quadratic-form objective derived from Gaussian NLL, weighted by the inverse conditional covariance Σ−1 (Theorem 3.1; Eq. (2), Section 3.1). They learn a proxy Σ via a bilevel procedure that updates model parameters θ in an inner loop and Σ in an outer loop, with PSD enforced by a Cholesky reparameterization (Definition 3.2; Algorithm 1; Section 3.2). The resulting Quadratic Direct Forecast (QDF) trains models using the learned Σ (Algorithm 2; Section 3.3). Experiments across ETT, ECL, Weather, and PEMS demonstrate consistent improvements over baselines and alternative objectives (Table 1, Table 2), supported by ablation (Table 3), generalization to different architectures (Fig. 3, Fig. 8), meta-learning variants (Table 4), sensitivity (Fig. 4), and complexity (Appendix D.7; Fig. 9).Strengths\n- Bold motivation grounded in empirical evidence\n  • The paper diagnoses two challenges—label autocorrelation and heterogeneous task weights—with concrete measurements: substantial off-diagonal partial correlations and non-uniform conditional variances (Fig. 1(a); Section 3.1). This matters because it justifies why point-wise losses like MSE (Eq. (1), Section 2.3) can be biased and motivates a structured objective.  \n  • The comparison of transformed components from FreDF and Time-o1 shows residual autocorrelation (Fig. 1(b); Appendix A, Fig. 5), supporting the claim that prior decorrelation approaches do not ensure conditional decorrelation. This matters for novelty in the loss design focus.  \n  • The case study clearly links these diagnostics to the proposed solution (Section 3.1), improving narrative clarity and problem framing.- Clear probabilistic formulation of the objective\n  • The quadratic objective follows directly from Gaussian NLL (Theorem 3.1; Eq. (2), Section 3.1; Appendix B), giving a principled foundation for the weighting matrix Σ−1. This matters for technical soundness.  \n  • Explicitly associating off-diagonal entries with autocorrelation and diagonal entries with task-specific weights (Section 3.1; Section 3.3) increases interpretability and impact.  \n  • The reparameterization via Cholesky to enforce Σ ⪰ 0 (Section 3.2) addresses a key constraint and aids stable optimization.- Bilevel learning strategy for Σ with practical algorithms\n  • The bilevel formulation to target out-of-sample generalization (Definition 3.2; Eq. (3), Section 3.2) is a reasonable design choice that matters for performance.  \n  • Algorithm 1 and Algorithm 2 provide concrete training procedures, including chronological K-way splitting for robustness (Algorithm 2; Section 3.3). This matters for reproducibility and clarity.  \n  • The workflow emphasizes no test leakage (Section 3.3), which is impactful for fair evaluation.- Comprehensive experimental evaluation\n  • Overall performance shows consistent gains across datasets on MSE and MAE (Table 1; Section 4.2). This matters for empirical rigor and impact.  \n  • Comparisons with alternative objectives (Time-o1, FreDF, Koopman, Soft-DTW) demonstrate QDF’s superiority in most settings (Table 2; Section 4.3; Table 7 in Appendix D). This matters for positioning relative to literature.  \n  • Ablations isolate heterogeneous weights (QDF†) and autocorrelation (QDF‡) contributions, with the full QDF performing best (Table 3; Section 4.4), which matters for technical soundness.  \n  • Generalization across architectures (TQNet, PDF, FedFormer, iTransformer) with percentage error reductions (Fig. 3; Fig. 8; Section 4.5) supports versatility and impact.  \n  • Meta-learning variants outperform DF and QDF performs best (Table 4; Section 4.6), highlighting flexibility.  \n  • Sensitivity to N_in, K, and η is explored (Fig. 4; Section 4.7), and seed robustness is reported (Table 9; Appendix D.6), strengthening experimental rigor.- Practical considerations and efficiency\n  • The claim that QDF adds no inference-time overhead and modest training overhead is supported by timing plots across horizons (Appendix D.7; Fig. 9). This matters for practical adoption.  \n  • The Reproducibility Statement and code link (Appendix E; Appendix Reproducibility Statement) and implementation details (Appendix C.2) improve replicability and clarity.Weaknesses\n- Limited theoretical guarantees for the learned Σ and bias correction\n  • Theorem 3.1 (Eq. (2); Appendix B) restates standard Gaussian NLL; there is no formal analysis showing that the proxy Σ learned via the bilevel procedure consistently reduces estimation bias or converges to the conditional covariance structure. No direct evidence found in the manuscript. This matters for novelty and technical strength.  \n  • Section 3.2 states the goal is to learn Σ “targeting generalization” (Definition 3.2), but provides no convergence or generalization bound for the procedure or its hypergradient approximation. No direct evidence found in the manuscript. This matters for technical rigor.  \n  • The bias critique of MSE (Section 2.3; 3.1) is valid empirically, but the paper lacks a formal demonstration that QDF’s learned Σ−1 objectively corrects that bias in a statistical sense across datasets. No direct evidence found in the manuscript. This matters for theoretical soundness.- Ambiguity about hypergradient computation in the bilevel update\n  • Section 3.2 claims “the outer loop gradient is taken through the model parameter θ to Σ,” yet Algorithm 1 Step 7 implements Σ ← Σ − η ∇_Σ L_Σ(X_out, Y_out; g_θ), which appears to treat θ as fixed and does not explicitly unroll or differentiate through inner updates. This mismatch matters for correctness.  \n  • Algorithm 2 repeatedly applies Algorithm 1 across splits (Steps 2–5) but still does not describe hypergradient computation through N_in inner steps; Appendix C.2 does not specify hypergradient implementation. No direct evidence found in the manuscript. This matters for reproducibility and technical clarity.  \n  • Without explicit hypergradient details (e.g., implicit differentiation or unrolled backprop through inner updates), the bilevel optimization may reduce to a simpler bi-stage procedure that does not optimize for generalization via θ’s dependence on Σ. This matters for claimed advantages.- Restrictive treatment of multivariate forecasting\n  • Section 2.1 footnote: “In the multivariate case, each variable can be treated as a separate univariate case when computing the learning objectives.” This ignores cross-variable label covariance, so Σ is T×T rather than (T·D)×(T·D). This matters for modeling fidelity in multivariate datasets (e.g., ECL, Weather; Appendix C.1; Table 5).  \n  • Eq. (2) and Theorem 3.1 are written for Y ∈ R^T, consistent with the univariate assumption; there is no extension to jointly weighted multivariate sequences. This matters for generality and impact.  \n  • Experiments include multivariate datasets (Table 1; Table 5), but the paper does not analyze the potential performance gap from ignoring inter-variable correlations or test a multivariate Σ formulation. No direct evidence found in the manuscript. This affects completeness.- Evaluation scope and fairness of SOTA claims\n  • Table 1 reports QDF with TQNet as the underlying model while other baselines use their own architectures; this mixes architecture improvements with objective changes. Although Section 4.5 shows gains for several models (Fig. 3, Fig. 8), Table 1’s SOTA claim is not a fully apples-to-apples comparison. This matters for fairness.  \n  • Generalization studies do not include all baselines (e.g., DLinear, TiDE, TimesNet, MICN; Table 1), and the extent of improvement across simpler linear or CNN models is not fully documented in the main text. No direct evidence found in the manuscript. This limits generality.  \n  • PatchTST’s dependence on longer histories is acknowledged (Appendix D.5; Table 8), but other models that require different settings are not exhaustively covered; input length is fixed to 96 for core results (Table 1; Section 4.1). This matters for comprehensive benchmarking.- Efficiency claims lack critical details\n  • The timing results (Appendix D.7; Fig. 9) show sub-2 ms overhead up to T=720, but the paper does not report hardware, batch size, precision, or implementation specifics, making the claims hard to verify. No direct evidence found in the manuscript. This affects credibility.  \n  • Memory complexity of storing and inverting Σ (or solving with Σ via Cholesky) scales at least quadratically in T; the paper does not quantify memory usage or report peak RAM during training. No direct evidence found in the manuscript. This matters for practical deployment.  \n  • The statement “no additional complexity to model inference” (Appendix D.7) is plausible since Σ is learned offline, but training-time overhead relative to DF baselines is not quantified beyond the curves. No direct evidence found in the manuscript. This limits practical clarity.- Case-study estimation may underrepresent nonlinear autocorrelation and uses reduced data\n  • Appendix A estimates partial correlations via OLS residuals, which capture linear dependencies; nonlinear relationships may persist and affect Σ. No direct evidence found in the manuscript for nonlinear controls. This matters for completeness.  \n  • The case study subsamples 5,000 examples, limits H to 8, and T to 96 (Appendix A), which reduces computational cost but potentially biases the estimated statistics relative to training settings. This matters for external validity.  \n  • The reported statistic “over 61.4% exceeding 0.1” (Fig. 1(a)) lacks hypothesis testing or CI for significance; robustness across datasets is shown qualitatively (Appendix A; Fig. 5), but not quantified. This matters for statistical rigor.- Numerical stability and implementation details of Σ−1\n  • The objective requires Σ−1 (Eq. (2); Section 3.1). While Σ = LLᵀ ensures PSD (Section 3.2), the paper does not specify whether gradients are computed by explicitly inverting Σ, solving triangular systems, or using stabilization (e.g., adding εI). No direct evidence found in the manuscript. This matters for stability.  \n  • Softplus on L’s diagonal is mentioned (Section 3.2), but no regularization (e.g., conditioning penalties or spectral bounds) is reported to prevent ill-conditioning. No direct evidence found in the manuscript. This affects robustness.  \n  • The outer-loop update (Algorithm 1 Step 7) gives a direct gradient on Σ; without explicit constraints beyond PSD reparameterization, Σ could drift to poorly conditioned matrices. No direct evidence found in the manuscript. This matters for technical quality.Suggestions for Improvement\n- Strengthen theoretical analysis of the learned Σ and its relation to bias correction\n  • Provide formal results or bounds showing that optimizing (Definition 3.2; Eq. (3)) reduces estimation bias relative to MSE/MAE, ideally under assumptions on the data-generating process (Section 3.1; Appendix B).  \n  • Establish a convergence analysis for the bilevel procedure (Algorithm 1/2), clarifying conditions under which the proxy Σ approximates the conditional covariance or yields generalization gains.  \n  • Include a controlled synthetic study where the ground-truth Σ is known to quantify how well the learned Σ recovers autocorrelation structure and heterogeneous variances (anchoring to Eq. (2) and Section 4.4’s ablations).- Clarify and implement hypergradient computation in the bilevel update\n  • Explicitly unroll inner updates and show how the outer gradient ∇Σ incorporates the dependence of θ* on Σ (e.g., through backpropagation through optimization or implicit differentiation), aligning the text in Section 3.2 with Algorithm 1 Step 7.  \n  • Document the exact hypergradient method used (number of unrolled steps, memory trade-offs) in Appendix C.2 and update Algorithm 1 to reflect it (e.g., notation for θ*(Σ) and hypergradient paths).  \n  • Provide an ablation comparing: (i) direct gradient w.r.t. Σ with θ fixed, vs (ii) hypergradient through inner updates, and report generalization performance on a holdout split (Section 4.7; Fig. 4).- Extend QDF to multivariate weighting or discuss its limitations more thoroughly\n  • Consider a block-structured Σ over (T·D)×(T·D) that models both temporal and cross-variable correlations, or justify the univariate approximation with empirical tests on multivariate datasets (Section 2.1 footnote; Table 5; Table 1).  \n  • Add experiments comparing univariate Σ (current) vs multivariate Σ on ECL and Weather, quantifying gains and computational trade-offs (Section 4.5; Fig. 3).  \n  • If multivariate Σ is infeasible at scale, discuss structured parameterizations (e.g., Kronecker or low-rank forms) and report complexity/memory (Appendix D.7).- Broaden and align evaluation to isolate objective-level improvements\n  • Complement Table 1 with QDF integrated into more baselines (e.g., DLinear, TiDE, TimesNet, MICN), reporting consistent training protocols and the same input history length (Section 4.1), to make SOTA claims attributable to the objective.  \n  • Expand Section 4.5/Fig. 8 to include these models with 95% confidence intervals or multiple seeds (Appendix D.6), clarifying robustness.  \n  • Explore model-specific preferred history lengths (beyond PatchTST; Appendix D.5; Table 8) to ensure comparisons are fair across architectures and settings.- Report detailed efficiency and resource metrics\n  • Include hardware details (GPU/CPU model, RAM, batch size, precision, framework) and precise timing methodology for Fig. 9 in Appendix D.7.  \n  • Quantify memory usage for storing Σ and its gradients, and discuss whether solving with Σ−1 uses explicit inversion or triangular solves; provide per-epoch overhead relative to DF.  \n  • Add scaling plots in Appendix D.7 showing training time vs T for several batch sizes and architectures and a breakdown of inner vs outer loop time (Algorithm 1 Steps 4 and 7).- Strengthen the autocorrelation case study with nonlinear controls and full-scale data\n  • Augment Appendix A with nonlinear partial correlation estimates (e.g., kernel-based residualization) and compare to OLS residuals, to assess sensitivity to nonlinearity.  \n  • Re-estimate partial correlations on full training settings (larger H and actual dataset sizes), or provide sub-sampling robustness checks demonstrating stability of the observed patterns.  \n  • Report statistical significance (e.g., bootstrapped CIs or hypothesis tests) for the “61.4% exceeding 0.1” figure (Fig. 1(a)) and summarize across datasets (Appendix A; Fig. 5) with quantitative metrics.- Provide numerical stability details for handling Σ−1\n  • Document how Σ−1 is computed in practice (explicit inversion vs solves with L), and whether an εI term is added for conditioning; include this in Appendix C.2.  \n  • Consider regularizers (e.g., Frobenius norm on off-diagonals, spectral constraints, trace penalties) to prevent ill-conditioning and report their effect in ablations (Section 4.4; Table 3).  \n  • Monitor and report the condition number of Σ during training and ensure PSD is preserved via reparameterization (Section 3.2), adding diagnostics in Appendix D.Score\n- Overall (10): 7 — Strong empirical results across datasets with clear motivation and practical algorithms (Table 1; Table 2; Table 3; Fig. 3; Fig. 4), but limited theoretical guarantees and some methodological ambiguities (Section 3.2; Algorithm 1).\n- Novelty (10): 6 — The idea of learning a quadratic weighting matrix for DF is useful, but the core objective follows standard Gaussian NLL (Theorem 3.1; Appendix B) and the meta-learning connection is conceptual (Section 3.3); novelty lies mainly in the bilevel learning of Σ and its application.\n- Technical Quality (10): 6 — Sound formulation and careful experiments (Table 1–4; Fig. 3–4; Appendix D), but missing hypergradient details (Section 3.2; Algorithm 1), multivariate treatment (Section 2.1 footnote), and stability/efficiency specifics (Appendix D.7).\n- Clarity (10): 8 — The manuscript is generally clear with algorithms and extensive results (Algorithm 1–2; Section 4; Appendix C–D), though some critical implementation details (hypergradients, Σ−1 computation) need clarification (Section 3.2; Appendix C.2).\n- Confidence (5): 4 — High confidence in reading and interpretation based on detailed anchors across methodology and experiments; some reservations due to missing theoretical and implementation specifics (Section 3.2; Appendix D.7).",
  "final_review": "Summary\n- The paper addresses the design of learning objectives for multi-step direct time-series forecasting. It argues that conventional MSE treats future steps as independent and equally weighted, overlooking label autocorrelation and heterogeneous task difficulty. The authors propose a quadratic-form objective derived from Gaussian NLL, weighted by the inverse conditional covariance Σ−1 (Theorem 3.1; Eq. (2), Section 3.1). They learn a proxy Σ via a bilevel procedure that updates model parameters θ in an inner loop and Σ in an outer loop, with PSD enforced by a Cholesky reparameterization (Definition 3.2; Algorithm 1; Section 3.2). The resulting Quadratic Direct Forecast (QDF) trains models using the learned Σ (Algorithm 2; Section 3.3). Experiments across ETT, ECL, Weather, and PEMS demonstrate consistent improvements over baselines and alternative objectives (Table 1, Table 2), supported by ablation (Table 3), generalization to different architectures (Fig. 3, Fig. 8), meta-learning variants (Table 4), sensitivity (Fig. 4), and complexity (Appendix D.7; Fig. 9).Strengths\n- Bold motivation grounded in empirical evidence\n  • The paper diagnoses two challenges—label autocorrelation and heterogeneous task weights—with concrete measurements: substantial off-diagonal partial correlations and non-uniform conditional variances (Fig. 1(a); Section 3.1). This matters because it justifies why point-wise losses like MSE (Eq. (1), Section 2.3) can be biased and motivates a structured objective.  \n  • The comparison of transformed components from FreDF and Time-o1 shows residual autocorrelation (Fig. 1(b); Appendix A, Fig. 5), supporting the claim that prior decorrelation approaches do not ensure conditional decorrelation. This matters for novelty in the loss design focus.  \n  • The case study clearly links these diagnostics to the proposed solution (Section 3.1), improving narrative clarity and problem framing.\n- Clear probabilistic formulation of the objective\n  • The quadratic objective follows directly from Gaussian NLL (Theorem 3.1; Eq. (2), Section 3.1; Appendix B), giving a principled foundation for the weighting matrix Σ−1. This matters for technical soundness.  \n  • Explicitly associating off-diagonal entries with autocorrelation and diagonal entries with task-specific weights (Section 3.1; Section 3.3) increases interpretability and impact.  \n  • The reparameterization via Cholesky to enforce Σ ⪰ 0 (Section 3.2) addresses a key constraint and aids stable optimization.\n- Bilevel learning strategy for Σ with practical algorithms\n  • The bilevel formulation to target out-of-sample generalization (Definition 3.2; Eq. (3), Section 3.2) is a reasonable design choice that matters for performance.  \n  • Algorithm 1 and Algorithm 2 provide concrete training procedures, including chronological K-way splitting for robustness (Algorithm 2; Section 3.3). This matters for reproducibility and clarity.  \n  • The workflow emphasizes no test leakage (Section 3.3), which is impactful for fair evaluation.\n- Comprehensive experimental evaluation\n  • Overall performance shows consistent gains across datasets on MSE and MAE (Table 1; Section 4.2). This matters for empirical rigor and impact.  \n  • Comparisons with alternative objectives (Time-o1, FreDF, Koopman, Soft-DTW) demonstrate QDF’s superiority in most settings (Table 2; Section 4.3; Table 7 in Appendix D). This matters for positioning relative to literature.  \n  • Ablations isolate heterogeneous weights (QDF†) and autocorrelation (QDF‡) contributions, with the full QDF performing best (Table 3; Section 4.4), which matters for technical soundness.  \n  • Generalization across architectures (TQNet, PDF, FedFormer, iTransformer) with percentage error reductions (Fig. 3; Fig. 8; Section 4.5) supports versatility and impact.  \n  • Meta-learning variants outperform DF and QDF performs best (Table 4; Section 4.6), highlighting flexibility.  \n  • Sensitivity to N_in, K, and η is explored (Fig. 4; Section 4.7), and seed robustness is reported (Table 9; Appendix D.6), strengthening experimental rigor.\n- Practical considerations and efficiency\n  • The claim that QDF adds no inference-time overhead and modest training overhead is supported by timing plots across horizons (Appendix D.7; Fig. 9). This matters for practical adoption.  \n  • The Reproducibility Statement and code link (Appendix E; Appendix Reproducibility Statement) and implementation details (Appendix C.2) improve replicability and clarity.Weaknesses\n- Limited theoretical guarantees for the learned Σ and bias correction\n  • Theorem 3.1 (Eq. (2); Appendix B) restates standard Gaussian NLL; there is no formal analysis showing that the proxy Σ learned via the bilevel procedure consistently reduces estimation bias or converges to the conditional covariance structure. No direct evidence found in the manuscript. This matters for novelty and technical strength.  \n  • Section 3.2 states the goal is to learn Σ “targeting generalization” (Definition 3.2), but provides no convergence or generalization bound for the procedure or its hypergradient approximation. No direct evidence found in the manuscript. This matters for technical rigor.  \n  • The bias critique of MSE (Section 2.3; 3.1) is valid empirically, but the paper lacks a formal demonstration that QDF’s learned Σ−1 objectively corrects that bias in a statistical sense across datasets. No direct evidence found in the manuscript. This matters for theoretical soundness.\n- Ambiguity about hypergradient computation in the bilevel update\n  • Section 3.2 claims “the outer loop gradient is taken through the model parameter θ to Σ,” yet Algorithm 1 Step 7 implements Σ ← Σ − η ∇_Σ L_Σ(X_out, Y_out; g_θ), which appears to treat θ as fixed and does not explicitly unroll or differentiate through inner updates. This mismatch matters for correctness.  \n  • Algorithm 2 repeatedly applies Algorithm 1 across splits (Steps 2–5) but still does not describe hypergradient computation through N_in inner steps; Appendix C.2 does not specify hypergradient implementation. No direct evidence found in the manuscript. This matters for reproducibility and technical clarity.  \n  • Without explicit hypergradient details (e.g., implicit differentiation or unrolled backprop through inner updates), the bilevel optimization may reduce to a simpler bi-stage procedure that does not optimize for generalization via θ’s dependence on Σ. This matters for claimed advantages.\n- Restrictive treatment of multivariate forecasting\n  • Section 2.1 footnote: “In the multivariate case, each variable can be treated as a separate univariate case when computing the learning objectives.” This ignores cross-variable label covariance, so Σ is T×T rather than (T·D)×(T·D). This matters for modeling fidelity in multivariate datasets (e.g., ECL, Weather; Appendix C.1; Table 5).  \n  • Eq. (2) and Theorem 3.1 are written for Y ∈ R^T, consistent with the univariate assumption; there is no extension to jointly weighted multivariate sequences. This matters for generality and impact.  \n  • Experiments include multivariate datasets (Table 1; Table 5), but the paper does not analyze the potential performance gap from ignoring inter-variable correlations or test a multivariate Σ formulation. No direct evidence found in the manuscript. This affects completeness.\n- Evaluation scope and fairness of SOTA claims\n  • Table 1 reports QDF with TQNet as the underlying model while other baselines use their own architectures; this mixes architecture improvements with objective changes. Although Section 4.5 shows gains for several models (Fig. 3, Fig. 8), Table 1’s SOTA claim is not a fully apples-to-apples comparison. This matters for fairness.  \n  • Generalization studies do not include all baselines (e.g., DLinear, TiDE, TimesNet, MICN; Table 1), and the extent of improvement across simpler linear or CNN models is not fully documented in the main text. No direct evidence found in the manuscript. This limits generality.  \n  • The averaging note in Table 1 states T={96,192,336,720} for averages, but PEMS uses horizons {12,24,36,48} (Appendix C.1; Table 5); the mismatch is not clarified for PEMS rows, which affects interpretability of the reported PEMS averages. This matters for clarity.  \n  • Results for the “Koopman” objective on ECL are inconsistent across tables (Table 2 vs. Appendix Table 7), potentially affecting comparative conclusions. This matters for reliability.  \n  • The PEMS08 split reports a very small test set (Train/Val/Test = 10690/3548/265; Table 5), which may limit statistical validity; no justification is provided. This matters for fairness.\n- Efficiency claims lack critical details\n  • The timing results (Appendix D.7; Fig. 9) show sub-2 ms overhead up to T=720, but the paper does not report hardware, batch size, precision, or implementation specifics, making the claims hard to verify. No direct evidence found in the manuscript. This affects credibility.  \n  • Memory complexity of storing and inverting Σ (or solving with Σ via Cholesky) scales at least quadratically in T; the paper does not quantify memory usage or report peak RAM during training. No direct evidence found in the manuscript. This matters for practical deployment.  \n  • The statement “no additional complexity to model inference” (Appendix D.7) is plausible since Σ is learned offline, but training-time overhead relative to DF baselines is not quantified beyond the curves. No direct evidence found in the manuscript. This limits practical clarity.\n- Case-study estimation may underrepresent nonlinear autocorrelation and uses reduced data\n  • Appendix A estimates partial correlations via OLS residuals, which capture linear dependencies; nonlinear relationships may persist and affect Σ. No direct evidence found in the manuscript for nonlinear controls. This matters for completeness.  \n  • The case study subsamples 5,000 examples, limits H to 8, and T to 96 (Appendix A), which reduces computational cost but potentially biases the estimated statistics relative to training settings. This matters for external validity.  \n  • The reported statistic “over 61.4% exceeding 0.1” (Fig. 1(a)) lacks hypothesis testing or CI for significance; robustness across datasets is shown qualitatively (Appendix A; Fig. 5), but not quantified. This matters for statistical rigor.\n- Numerical stability and implementation details of Σ−1\n  • The objective requires Σ−1 (Eq. (2); Section 3.1). While Σ = LLᵀ ensures PSD (Section 3.2), the paper does not specify whether gradients are computed by explicitly inverting Σ, solving triangular systems, or using stabilization (e.g., adding εI). No direct evidence found in the manuscript. This matters for stability.  \n  • Softplus on L’s diagonal is mentioned (Section 3.2), but no regularization (e.g., conditioning penalties or spectral bounds) is reported to prevent ill-conditioning. No direct evidence found in the manuscript. This affects robustness.  \n  • The stated constraint “Σ ⪰ 0” (Definition 3.2) combined with Cholesky with positive diagonals implies Σ is positive definite in practice; this PD vs. PSD inconsistency is not clarified, despite Σ−1 requiring PD. This matters for technical clarity.\n- Non-equivalent likelihood objective when optimizing Σ\n  • Theorem 3.1 and Appendix B present the Gaussian NLL, where the term log|Σ| depends on Σ (Appendix B, Theorem B.1). However, the outer objective and updates use L_Σ = (Y − g_θ(X))ᵀ Σ⁻¹ (Y − g_θ(X)) without log|Σ| (Eq. (2); Definition 3.2; Algorithm 1 Step 7), which is not the full NLL when Σ is a decision variable. This matters for correctness of the likelihood grounding.  \n  • Dropping log|Σ| removes a stabilizing term that penalizes scaling of Σ, potentially leading to ill-posed optimization of Σ in the outer loop (Section 3.2; Algorithm 1). No direct evidence found in the manuscript that this surrogate avoids degeneracy. This matters for technical soundness.  \n  • The text repeatedly refers to training with “the corresponding NLL objective” (Section 3.3; Algorithm 2 Step 7), but the implemented objective for Σ-optimization omits log|Σ|; this inconsistency reduces interpretability of the probabilistic formulation. This matters for clarity.\n- Inconsistency between method and experiments regarding training objective\n  • Section 3.3 states the final phase trains the model by minimizing L_Σ (Algorithm 2 Step 7), whereas Section 4.1 says “We train all models with the Adam optimizer to minimize MSE on the training set,” leaving unclear whether QDF models are trained with L_Σ or MSE. This matters for reproducibility.  \n  • Appendix C.2 reiterates that “Models were trained to minimize the MSE loss” while QDF-specific parameters are tuned, without explicitly stating the use of L_Σ in training. This matters for methodological transparency.  \n  • The discrepancy affects attribution of improvements in Table 1–2: if MSE is used for training, the role of the learned Σ during model training (Algorithm 2 Step 7) is unclear. This matters for interpretation of results.Suggestions for Improvement\n- Strengthen theoretical analysis of the learned Σ and its relation to bias correction\n  • Provide formal results or bounds showing that optimizing (Definition 3.2; Eq. (3)) reduces estimation bias relative to MSE/MAE, ideally under assumptions on the data-generating process (Section 3.1; Appendix B).  \n  • Establish a convergence analysis for the bilevel procedure (Algorithm 1/2), clarifying conditions under which the proxy Σ approximates the conditional covariance or yields generalization gains.  \n  • Include a controlled synthetic study where the ground-truth Σ is known to quantify how well the learned Σ recovers autocorrelation structure and heterogeneous variances (anchoring to Eq. (2) and Section 4.4’s ablations).\n- Clarify and implement hypergradient computation in the bilevel update\n  • Explicitly unroll inner updates and show how the outer gradient ∇Σ incorporates the dependence of θ* on Σ (e.g., through backpropagation through optimization or implicit differentiation), aligning the text in Section 3.2 with Algorithm 1 Step 7.  \n  • Document the exact hypergradient method used (number of unrolled steps, memory trade-offs) in Appendix C.2 and update Algorithm 1 to reflect it (e.g., notation for θ*(Σ) and hypergradient paths).  \n  • Provide an ablation comparing: (i) direct gradient w.r.t. Σ with θ fixed, vs (ii) hypergradient through inner updates, and report generalization performance on a holdout split (Section 4.7; Fig. 4).\n- Extend QDF to multivariate weighting or discuss its limitations more thoroughly\n  • Consider a block-structured Σ over (T·D)×(T·D) that models both temporal and cross-variable correlations, or justify the univariate approximation with empirical tests on multivariate datasets (Section 2.1 footnote; Table 5; Table 1).  \n  • Add experiments comparing univariate Σ (current) vs multivariate Σ on ECL and Weather, quantifying gains and computational trade-offs (Section 4.5; Fig. 3).  \n  • If multivariate Σ is infeasible at scale, discuss structured parameterizations (e.g., Kronecker or low-rank forms) and report complexity/memory (Appendix D.7).\n- Broaden and align evaluation to isolate objective-level improvements\n  • Complement Table 1 with QDF integrated into more baselines (e.g., DLinear, TiDE, TimesNet, MICN), reporting consistent training protocols and the same input history length (Section 4.1), to make SOTA claims attributable to the objective.  \n  • Expand Section 4.5/Fig. 8 to include these models with 95% confidence intervals or multiple seeds (Appendix D.6), clarifying robustness.  \n  • Clarify averaging for PEMS results (Table 1) to reflect horizons {12,24,36,48} (Appendix C.1; Table 5), and reconcile inconsistent “Koopman” results across Table 2 and Appendix Table 7; justify the small PEMS08 test set or correct it if typographical.\n- Report detailed efficiency and resource metrics\n  • Include hardware details (GPU/CPU model, RAM, batch size, precision, framework) and precise timing methodology for Fig. 9 in Appendix D.7.  \n  • Quantify memory usage for storing Σ and its gradients, and discuss whether solving with Σ−1 uses explicit inversion or triangular solves; provide per-epoch overhead relative to DF.  \n  • Add scaling plots in Appendix D.7 showing training time vs T for several batch sizes and architectures and a breakdown of inner vs outer loop time (Algorithm 1 Steps 4 and 7).\n- Strengthen the autocorrelation case study with nonlinear controls and full-scale data\n  • Augment Appendix A with nonlinear partial correlation estimates (e.g., kernel-based residualization) and compare to OLS residuals, to assess sensitivity to nonlinearity.  \n  • Re-estimate partial correlations on full training settings (larger H and actual dataset sizes), or provide sub-sampling robustness checks demonstrating stability of the observed patterns.  \n  • Report statistical significance (e.g., bootstrapped CIs or hypothesis tests) for the “61.4% exceeding 0.1” figure (Fig. 1(a)) and summarize across datasets (Appendix A; Fig. 5) with quantitative metrics.\n- Provide numerical stability details for handling Σ−1\n  • Document how Σ−1 is computed in practice (explicit inversion vs solves with L), and whether an εI term is added for conditioning; include this in Appendix C.2.  \n  • Consider regularizers (e.g., Frobenius norm on off-diagonals, spectral constraints, trace penalties) to prevent ill-conditioning and report their effect in ablations (Section 4.4; Table 3).  \n  • Clarify the PD requirement and align constraints and parameterization (Definition 3.2; Section 3.2), and monitor/report Σ’s condition number during training (Appendix D).\n- Restore equivalence to the full NLL when optimizing Σ\n  • Include the log|Σ| term in the outer objective and updates (Section 3.2; Algorithm 1; Algorithm 2) or provide theoretical/empirical justification for using the truncated surrogate, demonstrating it does not lead to degeneracies.  \n  • If using the surrogate, add stabilizing constraints or regularizers tied to Σ’s scale (Appendix C.2; Section 4.4) and show ablations confirming stable behavior.  \n  • Revise text to accurately state whether the outer optimization maximizes full NLL or a surrogate, and update Theorem/Definition references accordingly (Theorem 3.1; Definition 3.2; Appendix B).\n- Reconcile the method–experiment training objective discrepancy\n  • Explicitly state whether QDF training uses L_Σ or MSE in Section 4.1 and Appendix C.2, and align Algorithm 2 Step 7 accordingly.  \n  • If models are trained with L_Σ, provide implementation details and controls; if trained with MSE, clarify how the learned Σ is used (e.g., only for selection/tuning) and adjust claims to reflect this.  \n  • Add an experiment comparing training with L_Σ vs MSE under otherwise identical settings (Table 1–2) to quantify the contribution of the learned quadratic objective.Score\n- Overall (10): 6 — Strong empirical gains and thorough experiments (Table 1–4; Fig. 3–4), but key methodological inconsistencies (Section 3.2; Algorithm 1; Section 4.1) and missing details (Appendix D.7; Table 2 vs. Appendix Table 7) materially affect correctness and clarity.\n- Novelty (10): 6 — The quadratic objective stems from standard Gaussian NLL (Theorem 3.1; Appendix B), with novelty primarily in bilevel learning of Σ and its application; scope beyond univariate weighting is limited (Section 2.1 footnote).\n- Technical Quality (10): 5 — Sound high-level formulation and solid empirical work, but hypergradient ambiguity (Section 3.2; Algorithm 1), truncated NLL for Σ (Eq. (2); Appendix B), and method–experiment mismatch (Section 3.3 vs. Section 4.1) undermine rigor.\n- Clarity (10): 7 — Generally clear algorithms and extensive results (Algorithm 1–2; Section 4; Appendix C–D), yet critical implementation and consistency details (log|Σ| term, training objective, PD vs PSD) need clarification (Section 3.2; Appendix C.2; Table 1/2).\n- Confidence (5): 4 — High confidence based on numerous anchors across methodology and experiments; reservations remain due to the identified internal inconsistencies and missing implementation specifics (Section 3.2; Section 4.1; Appendix D.7).",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 7,
        "novelty": 6,
        "technical_quality": 6,
        "clarity": 8,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 6,
        "novelty": 6,
        "technical_quality": 5,
        "clarity": 7,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "Summary\n- The paper addresses the design of learning objectives for multi-step direct time-series forecasting. It argues that conventional MSE treats future steps as independent and equally weighted, overlooking label autocorrelation and heterogeneous task difficulty. The authors propose a quadratic-form objective derived from Gaussian NLL, weighted by the inverse conditional covariance Σ−1 (Theorem 3.1; Eq. (2), Section 3.1). They learn a proxy Σ via a bilevel procedure that updates model parameters θ in an inner loop and Σ in an outer loop, with PSD enforced by a Cholesky reparameterization (Definition 3.2; Algorithm 1; Section 3.2). The resulting Quadratic Direct Forecast (QDF) trains models using the learned Σ (Algorithm 2; Section 3.3). Experiments across ETT, ECL, Weather, and PEMS demonstrate consistent improvements over baselines and alternative objectives (Table 1, Table 2), supported by ablation (Table 3), generalization to different architectures (Fig. 3, Fig. 8), meta-learning variants (Table 4), sensitivity (Fig. 4), and complexity (Appendix D.7; Fig. 9).Strengths\n- Bold motivation grounded in empirical evidence\n  • The paper diagnoses two challenges—label autocorrelation and heterogeneous task weights—with concrete measurements: substantial off-diagonal partial correlations and non-uniform conditional variances (Fig. 1(a); Section 3.1). This matters because it justifies why point-wise losses like MSE (Eq. (1), Section 2.3) can be biased and motivates a structured objective.  \n  • The comparison of transformed components from FreDF and Time-o1 shows residual autocorrelation (Fig. 1(b); Appendix A, Fig. 5), supporting the claim that prior decorrelation approaches do not ensure conditional decorrelation. This matters for novelty in the loss design focus.  \n  • The case study clearly links these diagnostics to the proposed solution (Section 3.1), improving narrative clarity and problem framing.\n- Clear probabilistic formulation of the objective\n  • The quadratic objective follows directly from Gaussian NLL (Theorem 3.1; Eq. (2), Section 3.1; Appendix B), giving a principled foundation for the weighting matrix Σ−1. This matters for technical soundness.  \n  • Explicitly associating off-diagonal entries with autocorrelation and diagonal entries with task-specific weights (Section 3.1; Section 3.3) increases interpretability and impact.  \n  • The reparameterization via Cholesky to enforce Σ ⪰ 0 (Section 3.2) addresses a key constraint and aids stable optimization.\n- Bilevel learning strategy for Σ with practical algorithms\n  • The bilevel formulation to target out-of-sample generalization (Definition 3.2; Eq. (3), Section 3.2) is a reasonable design choice that matters for performance.  \n  • Algorithm 1 and Algorithm 2 provide concrete training procedures, including chronological K-way splitting for robustness (Algorithm 2; Section 3.3). This matters for reproducibility and clarity.  \n  • The workflow emphasizes no test leakage (Section 3.3), which is impactful for fair evaluation.\n- Comprehensive experimental evaluation\n  • Overall performance shows consistent gains across datasets on MSE and MAE (Table 1; Section 4.2). This matters for empirical rigor and impact.  \n  • Comparisons with alternative objectives (Time-o1, FreDF, Koopman, Soft-DTW) demonstrate QDF’s superiority in most settings (Table 2; Section 4.3; Table 7 in Appendix D). This matters for positioning relative to literature.  \n  • Ablations isolate heterogeneous weights (QDF†) and autocorrelation (QDF‡) contributions, with the full QDF performing best (Table 3; Section 4.4), which matters for technical soundness.  \n  • Generalization across architectures (TQNet, PDF, FedFormer, iTransformer) with percentage error reductions (Fig. 3; Fig. 8; Section 4.5) supports versatility and impact.  \n  • Meta-learning variants outperform DF and QDF performs best (Table 4; Section 4.6), highlighting flexibility.  \n  • Sensitivity to N_in, K, and η is explored (Fig. 4; Section 4.7), and seed robustness is reported (Table 9; Appendix D.6), strengthening experimental rigor.\n- Practical considerations and efficiency\n  • The claim that QDF adds no inference-time overhead and modest training overhead is supported by timing plots across horizons (Appendix D.7; Fig. 9). This matters for practical adoption.  \n  • The Reproducibility Statement and code link (Appendix E; Appendix Reproducibility Statement) and implementation details (Appendix C.2) improve replicability and clarity.Weaknesses\n- Limited theoretical guarantees for the learned Σ and bias correction\n  • Theorem 3.1 (Eq. (2); Appendix B) restates standard Gaussian NLL; there is no formal analysis showing that the proxy Σ learned via the bilevel procedure consistently reduces estimation bias or converges to the conditional covariance structure. No direct evidence found in the manuscript. This matters for novelty and technical strength.  \n  • Section 3.2 states the goal is to learn Σ “targeting generalization” (Definition 3.2), but provides no convergence or generalization bound for the procedure or its hypergradient approximation. No direct evidence found in the manuscript. This matters for technical rigor.  \n  • The bias critique of MSE (Section 2.3; 3.1) is valid empirically, but the paper lacks a formal demonstration that QDF’s learned Σ−1 objectively corrects that bias in a statistical sense across datasets. No direct evidence found in the manuscript. This matters for theoretical soundness.\n- Ambiguity about hypergradient computation in the bilevel update\n  • Section 3.2 claims “the outer loop gradient is taken through the model parameter θ to Σ,” yet Algorithm 1 Step 7 implements Σ ← Σ − η ∇_Σ L_Σ(X_out, Y_out; g_θ), which appears to treat θ as fixed and does not explicitly unroll or differentiate through inner updates. This mismatch matters for correctness.  \n  • Algorithm 2 repeatedly applies Algorithm 1 across splits (Steps 2–5) but still does not describe hypergradient computation through N_in inner steps; Appendix C.2 does not specify hypergradient implementation. No direct evidence found in the manuscript. This matters for reproducibility and technical clarity.  \n  • Without explicit hypergradient details (e.g., implicit differentiation or unrolled backprop through inner updates), the bilevel optimization may reduce to a simpler bi-stage procedure that does not optimize for generalization via θ’s dependence on Σ. This matters for claimed advantages.\n- Restrictive treatment of multivariate forecasting\n  • Section 2.1 footnote: “In the multivariate case, each variable can be treated as a separate univariate case when computing the learning objectives.” This ignores cross-variable label covariance, so Σ is T×T rather than (T·D)×(T·D). This matters for modeling fidelity in multivariate datasets (e.g., ECL, Weather; Appendix C.1; Table 5).  \n  • Eq. (2) and Theorem 3.1 are written for Y ∈ R^T, consistent with the univariate assumption; there is no extension to jointly weighted multivariate sequences. This matters for generality and impact.  \n  • Experiments include multivariate datasets (Table 1; Table 5), but the paper does not analyze the potential performance gap from ignoring inter-variable correlations or test a multivariate Σ formulation. No direct evidence found in the manuscript. This affects completeness.\n- Evaluation scope and fairness of SOTA claims\n  • Table 1 reports QDF with TQNet as the underlying model while other baselines use their own architectures; this mixes architecture improvements with objective changes. Although Section 4.5 shows gains for several models (Fig. 3, Fig. 8), Table 1’s SOTA claim is not a fully apples-to-apples comparison. This matters for fairness.  \n  • Generalization studies do not include all baselines (e.g., DLinear, TiDE, TimesNet, MICN; Table 1), and the extent of improvement across simpler linear or CNN models is not fully documented in the main text. No direct evidence found in the manuscript. This limits generality.  \n  • The averaging note in Table 1 states T={96,192,336,720} for averages, but PEMS uses horizons {12,24,36,48} (Appendix C.1; Table 5); the mismatch is not clarified for PEMS rows, which affects interpretability of the reported PEMS averages. This matters for clarity.  \n  • Results for the “Koopman” objective on ECL are inconsistent across tables (Table 2 vs. Appendix Table 7), potentially affecting comparative conclusions. This matters for reliability.  \n  • The PEMS08 split reports a very small test set (Train/Val/Test = 10690/3548/265; Table 5), which may limit statistical validity; no justification is provided. This matters for fairness.\n- Efficiency claims lack critical details\n  • The timing results (Appendix D.7; Fig. 9) show sub-2 ms overhead up to T=720, but the paper does not report hardware, batch size, precision, or implementation specifics, making the claims hard to verify. No direct evidence found in the manuscript. This affects credibility.  \n  • Memory complexity of storing and inverting Σ (or solving with Σ via Cholesky) scales at least quadratically in T; the paper does not quantify memory usage or report peak RAM during training. No direct evidence found in the manuscript. This matters for practical deployment.  \n  • The statement “no additional complexity to model inference” (Appendix D.7) is plausible since Σ is learned offline, but training-time overhead relative to DF baselines is not quantified beyond the curves. No direct evidence found in the manuscript. This limits practical clarity.\n- Case-study estimation may underrepresent nonlinear autocorrelation and uses reduced data\n  • Appendix A estimates partial correlations via OLS residuals, which capture linear dependencies; nonlinear relationships may persist and affect Σ. No direct evidence found in the manuscript for nonlinear controls. This matters for completeness.  \n  • The case study subsamples 5,000 examples, limits H to 8, and T to 96 (Appendix A), which reduces computational cost but potentially biases the estimated statistics relative to training settings. This matters for external validity.  \n  • The reported statistic “over 61.4% exceeding 0.1” (Fig. 1(a)) lacks hypothesis testing or CI for significance; robustness across datasets is shown qualitatively (Appendix A; Fig. 5), but not quantified. This matters for statistical rigor.\n- Numerical stability and implementation details of Σ−1\n  • The objective requires Σ−1 (Eq. (2); Section 3.1). While Σ = LLᵀ ensures PSD (Section 3.2), the paper does not specify whether gradients are computed by explicitly inverting Σ, solving triangular systems, or using stabilization (e.g., adding εI). No direct evidence found in the manuscript. This matters for stability.  \n  • Softplus on L’s diagonal is mentioned (Section 3.2), but no regularization (e.g., conditioning penalties or spectral bounds) is reported to prevent ill-conditioning. No direct evidence found in the manuscript. This affects robustness.  \n  • The stated constraint “Σ ⪰ 0” (Definition 3.2) combined with Cholesky with positive diagonals implies Σ is positive definite in practice; this PD vs. PSD inconsistency is not clarified, despite Σ−1 requiring PD. This matters for technical clarity.\n- Non-equivalent likelihood objective when optimizing Σ\n  • Theorem 3.1 and Appendix B present the Gaussian NLL, where the term log|Σ| depends on Σ (Appendix B, Theorem B.1). However, the outer objective and updates use L_Σ = (Y − g_θ(X))ᵀ Σ⁻¹ (Y − g_θ(X)) without log|Σ| (Eq. (2); Definition 3.2; Algorithm 1 Step 7), which is not the full NLL when Σ is a decision variable. This matters for correctness of the likelihood grounding.  \n  • Dropping log|Σ| removes a stabilizing term that penalizes scaling of Σ, potentially leading to ill-posed optimization of Σ in the outer loop (Section 3.2; Algorithm 1). No direct evidence found in the manuscript that this surrogate avoids degeneracy. This matters for technical soundness.  \n  • The text repeatedly refers to training with “the corresponding NLL objective” (Section 3.3; Algorithm 2 Step 7), but the implemented objective for Σ-optimization omits log|Σ|; this inconsistency reduces interpretability of the probabilistic formulation. This matters for clarity.\n- Inconsistency between method and experiments regarding training objective\n  • Section 3.3 states the final phase trains the model by minimizing L_Σ (Algorithm 2 Step 7), whereas Section 4.1 says “We train all models with the Adam optimizer to minimize MSE on the training set,” leaving unclear whether QDF models are trained with L_Σ or MSE. This matters for reproducibility.  \n  • Appendix C.2 reiterates that “Models were trained to minimize the MSE loss” while QDF-specific parameters are tuned, without explicitly stating the use of L_Σ in training. This matters for methodological transparency.  \n  • The discrepancy affects attribution of improvements in Table 1–2: if MSE is used for training, the role of the learned Σ during model training (Algorithm 2 Step 7) is unclear. This matters for interpretation of results.Suggestions for Improvement\n- Strengthen theoretical analysis of the learned Σ and its relation to bias correction\n  • Provide formal results or bounds showing that optimizing (Definition 3.2; Eq. (3)) reduces estimation bias relative to MSE/MAE, ideally under assumptions on the data-generating process (Section 3.1; Appendix B).  \n  • Establish a convergence analysis for the bilevel procedure (Algorithm 1/2), clarifying conditions under which the proxy Σ approximates the conditional covariance or yields generalization gains.  \n  • Include a controlled synthetic study where the ground-truth Σ is known to quantify how well the learned Σ recovers autocorrelation structure and heterogeneous variances (anchoring to Eq. (2) and Section 4.4’s ablations).\n- Clarify and implement hypergradient computation in the bilevel update\n  • Explicitly unroll inner updates and show how the outer gradient ∇Σ incorporates the dependence of θ* on Σ (e.g., through backpropagation through optimization or implicit differentiation), aligning the text in Section 3.2 with Algorithm 1 Step 7.  \n  • Document the exact hypergradient method used (number of unrolled steps, memory trade-offs) in Appendix C.2 and update Algorithm 1 to reflect it (e.g., notation for θ*(Σ) and hypergradient paths).  \n  • Provide an ablation comparing: (i) direct gradient w.r.t. Σ with θ fixed, vs (ii) hypergradient through inner updates, and report generalization performance on a holdout split (Section 4.7; Fig. 4).\n- Extend QDF to multivariate weighting or discuss its limitations more thoroughly\n  • Consider a block-structured Σ over (T·D)×(T·D) that models both temporal and cross-variable correlations, or justify the univariate approximation with empirical tests on multivariate datasets (Section 2.1 footnote; Table 5; Table 1).  \n  • Add experiments comparing univariate Σ (current) vs multivariate Σ on ECL and Weather, quantifying gains and computational trade-offs (Section 4.5; Fig. 3).  \n  • If multivariate Σ is infeasible at scale, discuss structured parameterizations (e.g., Kronecker or low-rank forms) and report complexity/memory (Appendix D.7).\n- Broaden and align evaluation to isolate objective-level improvements\n  • Complement Table 1 with QDF integrated into more baselines (e.g., DLinear, TiDE, TimesNet, MICN), reporting consistent training protocols and the same input history length (Section 4.1), to make SOTA claims attributable to the objective.  \n  • Expand Section 4.5/Fig. 8 to include these models with 95% confidence intervals or multiple seeds (Appendix D.6), clarifying robustness.  \n  • Clarify averaging for PEMS results (Table 1) to reflect horizons {12,24,36,48} (Appendix C.1; Table 5), and reconcile inconsistent “Koopman” results across Table 2 and Appendix Table 7; justify the small PEMS08 test set or correct it if typographical.\n- Report detailed efficiency and resource metrics\n  • Include hardware details (GPU/CPU model, RAM, batch size, precision, framework) and precise timing methodology for Fig. 9 in Appendix D.7.  \n  • Quantify memory usage for storing Σ and its gradients, and discuss whether solving with Σ−1 uses explicit inversion or triangular solves; provide per-epoch overhead relative to DF.  \n  • Add scaling plots in Appendix D.7 showing training time vs T for several batch sizes and architectures and a breakdown of inner vs outer loop time (Algorithm 1 Steps 4 and 7).\n- Strengthen the autocorrelation case study with nonlinear controls and full-scale data\n  • Augment Appendix A with nonlinear partial correlation estimates (e.g., kernel-based residualization) and compare to OLS residuals, to assess sensitivity to nonlinearity.  \n  • Re-estimate partial correlations on full training settings (larger H and actual dataset sizes), or provide sub-sampling robustness checks demonstrating stability of the observed patterns.  \n  • Report statistical significance (e.g., bootstrapped CIs or hypothesis tests) for the “61.4% exceeding 0.1” figure (Fig. 1(a)) and summarize across datasets (Appendix A; Fig. 5) with quantitative metrics.\n- Provide numerical stability details for handling Σ−1\n  • Document how Σ−1 is computed in practice (explicit inversion vs solves with L), and whether an εI term is added for conditioning; include this in Appendix C.2.  \n  • Consider regularizers (e.g., Frobenius norm on off-diagonals, spectral constraints, trace penalties) to prevent ill-conditioning and report their effect in ablations (Section 4.4; Table 3).  \n  • Clarify the PD requirement and align constraints and parameterization (Definition 3.2; Section 3.2), and monitor/report Σ’s condition number during training (Appendix D).\n- Restore equivalence to the full NLL when optimizing Σ\n  • Include the log|Σ| term in the outer objective and updates (Section 3.2; Algorithm 1; Algorithm 2) or provide theoretical/empirical justification for using the truncated surrogate, demonstrating it does not lead to degeneracies.  \n  • If using the surrogate, add stabilizing constraints or regularizers tied to Σ’s scale (Appendix C.2; Section 4.4) and show ablations confirming stable behavior.  \n  • Revise text to accurately state whether the outer optimization maximizes full NLL or a surrogate, and update Theorem/Definition references accordingly (Theorem 3.1; Definition 3.2; Appendix B).\n- Reconcile the method–experiment training objective discrepancy\n  • Explicitly state whether QDF training uses L_Σ or MSE in Section 4.1 and Appendix C.2, and align Algorithm 2 Step 7 accordingly.  \n  • If models are trained with L_Σ, provide implementation details and controls; if trained with MSE, clarify how the learned Σ is used (e.g., only for selection/tuning) and adjust claims to reflect this.  \n  • Add an experiment comparing training with L_Σ vs MSE under otherwise identical settings (Table 1–2) to quantify the contribution of the learned quadratic objective.Score\n- Overall (10): 6 — Strong empirical gains and thorough experiments (Table 1–4; Fig. 3–4), but key methodological inconsistencies (Section 3.2; Algorithm 1; Section 4.1) and missing details (Appendix D.7; Table 2 vs. Appendix Table 7) materially affect correctness and clarity.\n- Novelty (10): 6 — The quadratic objective stems from standard Gaussian NLL (Theorem 3.1; Appendix B), with novelty primarily in bilevel learning of Σ and its application; scope beyond univariate weighting is limited (Section 2.1 footnote).\n- Technical Quality (10): 5 — Sound high-level formulation and solid empirical work, but hypergradient ambiguity (Section 3.2; Algorithm 1), truncated NLL for Σ (Eq. (2); Appendix B), and method–experiment mismatch (Section 3.3 vs. Section 4.1) undermine rigor.\n- Clarity (10): 7 — Generally clear algorithms and extensive results (Algorithm 1–2; Section 4; Appendix C–D), yet critical implementation and consistency details (log|Σ| term, training objective, PD vs PSD) need clarification (Section 3.2; Appendix C.2; Table 1/2).\n- Confidence (5): 4 — High confidence based on numerous anchors across methodology and experiments; reservations remain due to the identified internal inconsistencies and missing implementation specifics (Section 3.2; Section 4.1; Appendix D.7)."
}