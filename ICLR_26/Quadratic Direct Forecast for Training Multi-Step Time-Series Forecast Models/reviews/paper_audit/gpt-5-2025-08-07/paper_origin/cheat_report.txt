Academic integrity and consistency risk report

Summary
The manuscript proposes QDF, a quadratic-form training objective with a learned weighting matrix Σ for multi-step time-series forecasting. While the idea is clearly articulated, several high-impact internal inconsistencies and methodological gaps may materially affect correctness, reproducibility, and trustworthiness.

Major issues

1) Incorrect negative log-likelihood (NLL) formulation when optimizing Σ
- Evidence:
  - Appendix B (Block #50) derives the full conditional NLL under a Gaussian assumption:
    −log P(Y|X) = 0.5 [T log(2π) + log|Σ| + (Y − gθ(X))ᵀ Σ⁻¹ (Y − gθ(X))].
  - The main text’s Theorem 3.1 (Block #11) explicitly omits “constant terms” to define LΣ = (Y − gθ(X))ᵀ Σ⁻¹ (Y − gθ(X)). This omission is valid only when optimizing θ, because log|Σ| is constant with respect to θ.
  - However, Definition 3.2 (Block #16) sets a bilevel optimization that optimizes Σ itself, and Algorithm 1/2 then use LΣ to update Σ (Block #17 step 7; Block #18 step 7), with no log|Σ| term included anywhere in the outer objective.
- Problem:
  - When Σ is a decision variable, log|Σ| is not a constant. Dropping log|Σ| yields a non-equivalent objective and can make Σ-updates ill-posed (e.g., scaling Σ up inflates Σ but shrinks Σ⁻¹, trivially reducing LΣ without a counterbalancing term). This directly contradicts the claimed NLL grounding and undermines the theoretical justification for learning Σ.
- Impact:
  - This is a central methodological inconsistency that affects the validity of the optimization and the claimed likelihood-based rationale. It is likely to affect results and reproducibility.

2) PSD vs PD inconsistency for Σ and use of Σ⁻¹
- Evidence:
  - Definition 3.2 constrains Σ ⪰ 0 (positive semi-definite) (Block #16).
  - The objective throughout uses Σ⁻¹ (e.g., Theorem 3.1, Eq. (2) in Block #11; Appendix B, Eq. (6) in Block #50), which requires Σ to be invertible, i.e., positive definite (PD).
  - Re-parameterization uses Cholesky Σ = LLᵀ with “positive diagonals” enforced via softplus (Block #16), which implies PD, not merely PSD.
- Problem:
  - The text claims to “enforce Σ ⪰ 0” but simultaneously relies on Σ⁻¹ (requiring PD) and uses a Cholesky parameterization that enforces PD. This is an internal inconsistency in the stated constraints versus what is actually required and implemented.
- Impact:
  - Conceptually confusing and technically relevant for correctness; readers reproducing using PSD may encounter singular Σ and undefined Σ⁻¹.

3) Contradiction in training objective for QDF vs. experimental setup
- Evidence:
  - Method Section 3.3 states that with learned Σ, “the final phase is to train the forecast model gθ… by minimizing the corresponding NLL objective (LΣ)” (Block #19 steps 6–7).
  - Experiments Setup Section 4.1 states, “We train all models with the Adam optimizer to minimize MSE on the training set.” (Block #22).
- Problem:
  - It is unclear whether QDF is trained with LΣ (as claimed in Method) or with MSE (as stated in Experiments). The phrase “all models” in Block #22 appears to include QDF and contradicts the core methodological contribution.
- Impact:
  - This affects the interpretability of reported improvements and the reproducibility of the method.

4) Inconsistent results for the “Koopman” objective on ECL across tables
- Evidence:
  - Table 2 (Block #24) reports for TQNet on ECL: Koopman MSE 0.166, MAE 0.258.
  - Table 7 (Block #67) reports for TQNet on ECL: Koopman MSE 0.623, MAE 0.524 (values identical to Soft-DTW in both tables).
- Problem:
  - The Koopman results for ECL conflict between tables by a large margin and appear duplicated from Soft-DTW in Table 7.
- Impact:
  - This discrepancy materially affects comparative conclusions about objectives and undermines the reliability of experimental claims.

5) Questionable dataset split for PEMS08
- Evidence:
  - Table 5 (Block #52) lists PEMS08 splits as Train/Validation/Test = 10690 / 3548 / 265.
- Problem:
  - A test set of 265 samples is unusually small compared to training and validation and inconsistent with typical public splits. No explanation is provided for this extreme imbalance.
- Impact:
  - Potentially biases evaluation and limits statistical validity of reported PEMS08 results. Reproducibility and fairness are compromised without clarification.

6) Misleading averaging note in Table 1 for PEMS datasets
- Evidence:
  - Table 1 note (Block #21) states that “Avg indicates average results over forecast horizons: T=96, 192, 336 and 720.”
  - Appendix C.1 (Block #51) states that PEMS uses horizons {12, 24, 36, 48}.
- Problem:
  - The averaging note in Table 1 applies the {96,192,336,720} horizons to all datasets, but PEMS uses different horizons. It is unclear whether the PEMS rows are single-horizon results or averages over {12,24,36,48}. The note is inconsistent with the dataset description.
- Impact:
  - Confuses how PEMS results are computed and compared; hinders correct interpretation.

7) Missing critical details on hypergradient computation in bilevel optimization
- Evidence:
  - The text claims: “Notably: the outer loop gradient is taken through the model parameter θ to Σ” (Block #16).
  - Algorithm 1 (Block #17) shows θ updates in steps 2–5 using ∇θ LΣ on Din, then a Σ update in step 7 using ∇Σ LΣ(Xout, Yout; gθ), but does not specify differentiating through the inner-loop updates (e.g., retaining the computational graph or using implicit gradients).
- Problem:
  - Without explicit details (e.g., unrolling, create_graph/retain_graph, or implicit differentiation), the outer ∇Σ may not correctly capture dθ*/dΣ required for bilevel optimization. The pseudocode, as written, suggests a direct gradient on LΣ w.r.t. Σ with θ fixed, which contradicts the stated “taken through θ” claim.
- Impact:
  - This is a correctness-critical omission; the method’s effectiveness hinges on properly capturing the dependence of θ on Σ.

Minor but notable issues

- Naming inconsistencies: “Time-o1” vs. “Time-01” and “FreDF” vs. “FreqDF” across tables (Block #24 vs. Block #67) may indicate table preparation errors.
- Domain labels in Table 5 (Block #52) list ETT datasets under “Health,” which appears incorrect for electricity transformer data. No direct impact on results but suggests carelessness in dataset metadata.

Recommendations to address critical issues

- Restore the full NLL when optimizing Σ, i.e., include log|Σ| in the outer objective, or explicitly justify and analyze the alternative objective (e.g., with regularization or constraints that prevent degeneracy). Update Definition 3.2, Algorithm 1/2, and all references to “NLL” accordingly (Blocks #16–#19, #50).
- Clarify and correct the Σ constraint to PD (Σ ≻ 0) consistently wherever Σ⁻¹ is used, and ensure the re-parameterization discussion reflects this (Block #16).
- Explicitly state the training objective used for QDF in experiments and reconcile Section 3.3 with Section 4.1 (Blocks #19 and #22).
- Correct the ECL “Koopman” results discrepancy across Table 2 and Table 7 (Blocks #24 and #67).
- Provide precise, justified splits for PEMS08 and explain the atypical test size (Block #52), or correct the table if it is a typographical error.
- Fix the averaging note in Table 1 to accurately reflect PEMS horizons (Blocks #21 and #51), or annotate PEMS rows separately.
- Add implementation details for bilevel hypergradient computation (e.g., unrolling steps with retained graph, implicit differentiation) to ensure the outer gradient truly flows through θ to Σ (Blocks #16–#18).

Conclusion
The manuscript contains multiple substantive inconsistencies and missing details that directly affect the methodological validity and the credibility of empirical claims. The most critical is the use of a truncated NLL without log|Σ| when optimizing Σ, which contradicts the stated likelihood foundation and can make the outer optimization ill-posed. Correcting these issues and clarifying the experimental protocol are necessary to restore confidence in the results.