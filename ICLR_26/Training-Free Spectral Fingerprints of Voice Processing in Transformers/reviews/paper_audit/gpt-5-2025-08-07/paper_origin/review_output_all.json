{
  "baseline_review": "Summary\n- The paper proposes a training-free graph signal processing (GSP) framework to analyze transformer computations by viewing attention as dynamic token graphs and hidden states as signals on these graphs (Section 3.1; Figure 1; Equation (1)). The primary diagnostic is the early-window mean change in the Fiedler eigenvalue (Δλ2) between passive and active voice, prespecified for layers 2–5 (Section 3.2). Evaluations span three model families (Qwen2.5‑7B, Phi‑3‑Mini, LLaMA‑3.2‑1B) and 20 languages (Section 4). Results show family-specific spectral fingerprints: a large English-specific disruption for Phi‑3‑Mini (Δλ2[2,5] ≈ −0.446; Figure 2), smaller distributed negatives for Qwen2.5‑7B (Figure 3), and systematic but muted effects for LLaMA‑3.2‑1B (Figure 4). Δλ2 correlates with tokenizer fragmentation (Table 2) and with behavioral fit (NLL; Table 3). Targeted attention-head ablations modulate Δλ2 (Table 4). Extensions include preliminary reasoning-mode signatures (Table 5; Appendix F) and an anonymized hallucination detector (Section 7.3; Eq. (2)).Strengths\n- Bolded title: **Clear, principled spectral formulation over attention-induced token graphs**\n  - Evidence: Formal definition of symmetrized, head-aggregated graphs and Laplacians with attention matrices A(ℓ,h) (Section 3.1; Equation (1); Figure 1). Why it matters: Provides a coherent operator-based foundation to interpret attention as a graph and hidden states as graph signals (novelty/technical soundness).\n  - Evidence: Four diagnostics defined (Dirichlet energy, spectral entropy, HFER, Fiedler connectivity) with explicit matrix relationships (Section 3.1). Why it matters: Enables multi-faceted spectral analysis beyond attention visualization or probing (novelty/clarity).\n  - Evidence: Theoretical rationale for λ2 as algebraic connectivity linked to information flow and bottlenecks (Appendix A.1). Why it matters: Strengthens interpretability of Δλ2 as a compact, theoretically grounded endpoint (technical soundness/impact).- Bolded title: **Prespecified early-window endpoint and sensitivity checks**\n  - Evidence: Primary endpoint fixed to early window layers 2–5 (Section 3.2), motivated by “onset of multihead context integration.” Why it matters: Reduces researcher degrees of freedom and supports comparability across models (rigor/clarity).\n  - Evidence: Stability to window shifts (1–4, 3–6) and HFER cutoff sweeps (Appendix B.4; Appendix E.2, early window choice). Why it matters: Demonstrates robustness of conclusions to reasonable analysis choices (technical soundness).\n  - Evidence: Normalization and aggregation robustness (random-walk vs. symmetric Laplacians; uniform vs. mass-weighted head aggregation; directed variants), preserving signs and peak locations (Appendix B.1–B.3; Table 6). Why it matters: Reduces concern that results hinge on arbitrary operator choices (rigor/reliability).- Bolded title: **Cross-model, cross-lingual results revealing family-specific spectral fingerprints**\n  - Evidence: Phi‑3‑Mini shows a large, English-specific early-layer negative shift (Δλ2[2,5] ≈ −0.446; Figure 2a/b); others near zero. Why it matters: Illustrates architecture-specific specialization effects that the framework can detect (impact/novelty).\n  - Evidence: Qwen2.5‑7B displays small, distributed negatives across voice types and languages (Figure 3). Why it matters: Identifies a different, more uniform processing signature at modest scale (experimental coverage).\n  - Evidence: LLaMA‑3.2‑1B exhibits systematic but muted effects with similar ordering trends (Figure 4). Why it matters: Shows consistent but scaled-down behavior, supporting generality of the probe across models (experimental rigor).- Bolded title: **Tokenizer-stress analysis linking subword fragmentation to spectral effects**\n  - Evidence: Pieces/character and fragmentation entropy defined and used as covariates (Section 5), with model-family-specific correlations (Table 2; Method/Table 1; Section 6.2). Why it matters: Distinguishes subword confounds from architecture sensitivity, revealing complementary fingerprints (novelty/technical soundness).\n  - Evidence: Differential tokenization effects between active/passive variants (Section 5; “Differential tokenization effects,” Pearson r = 0.51 for LLaMA inconsistencies; Section 15). Why it matters: Highlights a realistic source of stress that can modulate processing signatures (impact/experimental rigor).\n  - Evidence: Mixed-effects regressions with random intercepts by language; covariates standardized within family (Section 5). Why it matters: Indicates an attempt to control for cross-language variability in fragmentation metrics (rigor).- Bolded title: **Functional relevance via spectral–behavioral correlations and targeted interventions**\n  - Evidence: Strong negative correlations between |Δλ2| and NLL differences across families (Table 3; Section 6.3), e.g., Phi‑3‑Mini r = −0.976. Why it matters: Links spectral connectivity changes to behavioral fit, supporting interpretive validity (impact/technical soundness).\n  - Evidence: Intervention validation via limited head ablations aligning spectral signs with behavioral shifts in controlled items (Section 6.3, “Intervention validation”). Why it matters: Strengthens functional claims beyond correlation (rigor/impact).\n  - Evidence: Causal validation through attention-head ablations with windowed Δλ2 changes revealing family-specific resilience vs. brittleness profiles (Section 6.4; Table 4). Why it matters: Demonstrates that early attention structure causally shapes spectral connectivity (novelty/technical soundness).- Bolded title: **Extended robustness and reproducibility efforts**\n  - Evidence: Expanded sample size experiments (up to n=50 paraphrases) replicate key findings, including the Phi‑3 English-specific disruption (Appendix C; Figures 8–13). Why it matters: Supports statistical reliability beyond small-n settings (experimental rigor).\n  - Evidence: Reproducibility details with ARPACK settings, bootstrap/permutation parameters, and BH–FDR (Appendix B.5; Appendix E; Appendix H). Why it matters: Enables independent verification and clarifies computational settings (clarity/reliability).\n  - Evidence: Clear definitions of derived metrics (RCI in Appendix F; hallucination detector z-score in Section 7.3; Eq. (2)). Why it matters: Facilitates re-use and critical examination (clarity/impact).Weaknesses\n- Bolded title: **Incomplete methodological specification and limited reproducibility artifacts**\n  - Evidence: The multilingual dataset and paraphrase generation are described at a high level (“at least 10 paraphrases per voice per language… template-matched,” Section 4) without item-level details, prompt templates, or full language lists for each model; many figures list languages only for specific families (Figure 2a), while others present group summaries (Figures 3–4). Why it matters: Limits independent replication and verification of language coverage, paraphrase matching, and selection biases (clarity/reproducibility).\n  - Evidence: The anonymized repository includes only one representative language and a separate 6-language dataset link (Appendix H), not the full 20-language set used in the main results. Why it matters: Hinders full end-to-end replication and stress-testing of statistical conclusions (reproducibility).\n  - Evidence: Tokenization controls are “light” (≤2-token drift “when feasible,” Section 5; E.3), with no comprehensive reporting of drift distributions across languages or models. Why it matters: Without full drift statistics, residual confounding may remain in Δλ2 and behavioral scores (technical rigor).- Bolded title: **Statistical analysis leaves room for overinterpretation and sensitivity to outliers**\n  - Evidence: Many effects are small with CIs overlapping zero (e.g., Qwen2.5‑7B per-language bars “several CIs overlapping zero,” Section 6.1; LLaMA “muted effects,” Figure 4), yet narrative emphasizes consistent signatures. Why it matters: Small, borderline effects risk overinterpretation if not accompanied by stringent, preregistered hypothesis tests (experimental rigor).\n  - Evidence: FDR corrections are applied “within each model family” (Section 6.1), but multiple summaries (per-language, per-type, cross-family) raise multiplicity concerns beyond within-family control (Appendix E.2, significance strategy). Why it matters: Multiplicity can inflate false positives, especially with layered aggregations (technical rigor).\n  - Evidence: Strong spectral–behavioral correlations could be dominated by a single large outlier (Phi‑3 English Δλ2 ≈ −0.446; Figure 2a) given r = −0.976 (Table 3), and Spearman vs. Pearson differences for tokenizer correlations (Table 2; “Spearman ρ” diverges, e.g., Qwen ρ = −0.11 while Pearson r = 0.51). Why it matters: Robust correlation analysis (e.g., Spearman, leave-one-out, mixed models) is needed to ensure conclusions are not driven by outliers (technical rigor).- Bolded title: **Theory–practice gap: mapping Δλ2 to syntactic computation remains heuristic**\n  - Evidence: Theoretical justification ties λ2 to connectivity robustness (Appendix A.1), but no formal derivation links attention symmetrization and Δλ2 changes to specific operations of voice alternation (agent–patient reassignment, auxiliary coupling) beyond qualitative prediction (Section 3.2). Why it matters: Without formal modeling, Δλ2 may capture unrelated structural shifts (novelty/technical soundness).\n  - Evidence: Symmetrization of inherently directed attention (Section 3.1) simplifies flow structure; directed variants are explored (Appendix B.2), but the final endpoints rely on Hermitian forms and partial phase choices, with remaining interpretive ambiguity. Why it matters: Directionality is central to syntactic dependencies; symmetrization may obscure mechanism (technical rigor).\n  - Evidence: Head aggregation relies on mass weighting by total attention (Section 3.1; Appendix B.3), but the justification for αh choices is empirical rather than theoretical, and learned convex combinations are tuned to reduce cross-condition error (Appendix B.3). Why it matters: Aggregation can change spectral structure; clearer principled criteria would strengthen claims (technical soundness).- Bolded title: **Potential confounds from tokenization, morphology, and length remain**\n  - Evidence: Tokenization drift is only lightly controlled (≤2 tokens “when feasible,” Section 5; E.3) and per-language drift statistics are not systematically reported; LLaMA shows sensitivity to drift (Section 15). Why it matters: Δλ2 and NLL can both be affected by token count and segmentation differences rather than linguistic computation alone (technical rigor).\n  - Evidence: Morphological complexity and voice-realization types (Section 4) are treated as groupings, but fragmentation may co-vary with morphology, domain, or sentence templates; mixed-effects regressions are described (Section 5) without model specifications or coefficients per family beyond correlation summaries (Table 2). Why it matters: Confounds can produce apparent family-specific patterns; stronger controls are needed (experimental rigor).\n  - Evidence: Behavioral metric is negative mean NLL without explicit normalization for length or tokenization differences (Section 6.3; Appendix E.3). Why it matters: NLL is sensitive to length and token frequency; unnormalized comparisons can bias correlations with |Δλ2| (technical soundness).- Bolded title: **Ablation-based causal validation is promising but under-specified**\n  - Evidence: Table 4 reports Δλ2 changes across early/mid/late windows for specific head ablations (Section 6.4), but the ablation mechanism (e.g., zeroing attention outputs vs. masking attention logits) and evaluation language/items are not detailed. Why it matters: Implementation details are essential to interpret causal claims and to replicate effects (technical rigor/reproducibility).\n  - Evidence: No statistical tests or CIs are provided for ablation effects in Table 4, and no behavioral measurements post-ablation are reported, aside from a brief note in Section 6.3 for controlled items. Why it matters: Without effect size uncertainty and behavioral validation, mechanistic conclusions remain tentative (experimental rigor).\n  - Evidence: Family-specific interpretations (e.g., “computational resilience” for Qwen; Section 6.4) are based on sign patterns without formal controls across languages or items. Why it matters: Qualitative narratives may overreach without quantitative backing (clarity/technical soundness).- Bolded title: **Generalization beyond voice alternation is preliminary and partly ad hoc**\n  - Evidence: Reasoning-mode signatures use a single model (phi‑3.5‑mini), a single task family (transitivity), and a bespoke RCI index (Table 5; Appendix F), with limited sample size and no cross-architecture validation (Section 7.1; 7.2 “preliminary”). Why it matters: Generality claims require broader tasks, models, and preregistered evaluation (novelty/impact).\n  - Evidence: Hallucination detector is reported on a small held-out set (n = 80) with an anonymized external manuscript; the detector threshold uses final-layer Fiedler z-score (Section 7.3; Eq. (2)) with no rigorous baseline comparison on public benchmarks. Why it matters: Application claims are suggestive but not yet substantiated at scale (impact/technical rigor).\n  - Evidence: Some figure captions and values are approximate or low-resolution (“~−0.04,” “Zoom: layers 2–5,” Figures 3–4), and several numerical summaries differ across sections (e.g., tokenization correlations presented in Section 5/Table 1 vs. Section 6/Table 2). Why it matters: Presentation contributes to uncertainty in interpreting the claimed generalization and performance alignment (clarity).Suggestions for Improvement\n- Bolded title: **Enhance methodological transparency and reproducibility artifacts**\n  - Provide item-level documentation for the multilingual dataset: release full language lists, per-language sentence templates, paraphrase generation rules, and matching criteria across models (Section 4; Figure 2a). This will enable exact reproduction and assessment of selection biases.\n  - Expand the anonymized repository to include the full 20-language set (or a representative large subset) and scripts to regenerate all main figures (Appendix H). If size limits are a concern, host on an archival platform with checksums and a data card.\n  - Report comprehensive tokenization drift statistics per language and model (histograms and summary metrics), including active–passive differences, and include drift-controlled analyses where drift is exactly matched (Section 5; Appendix E.3).- Bolded title: **Strengthen statistical rigor and robustness to outliers**\n  - Supplement per-language and per-type results with preregistered hypotheses and cross-family multiplicity control (e.g., hierarchical FDR across language, type, and family) to mitigate layered testing (Section 6.1; Appendix E.2).\n  - Report leave-one-language-out and robust correlation analyses (Spearman, Theil–Sen) for spectral–behavioral and tokenizer-stress relationships; explicitly show sensitivity of r to the English outlier in Phi‑3 (Figure 2a; Table 3; Table 2).\n  - Provide confidence intervals for all correlation estimates, mixed-effects coefficients, and effect sizes, and include power analyses for the main endpoints beyond Appendix C (Appendix C; Appendix E.1), clarifying detectable effect scales.- Bolded title: **Tighten the theory–practice link between Δλ2 and syntactic computation**\n  - Derive or empirically validate a mechanistic mapping (e.g., controlled synthetic sequences) linking agent–patient reassignment and auxiliary coupling to predictable Δλ2 patterns, beyond qualitative predictions (Section 3.2; Appendix A.1).\n  - Extend directed-operator analysis with task-specific phase calibration and compare Δλ2 under directed magnetic Laplacians to symmetrized variants on identical items (Appendix B.2), quantifying the impact of directionality on conclusions.\n  - Justify head aggregation choices with principled criteria (e.g., variance-explained or causal relevance metrics) and show that learned α(ℓ) generalize out-of-sample without overfitting to cross-condition differences (Appendix B.3).- Bolded title: **Control confounds from tokenization, morphology, and length more strictly**\n  - Implement exact length/token-count matching for active/passive pairs wherever possible and report analyses restricted to perfectly matched pairs (Section 5; Appendix E.3), quantifying how Δλ2 and NLL change under stricter control.\n  - Augment mixed-effects regressions with explicit morphology/type covariates and interaction terms (Section 4; Section 5), reporting full model specifications and family-wise coefficient tables to separate fragmentation from morphological complexity.\n  - Normalize behavioral NLL by length or use per-token NLL (Appendix E.3; Section 6.3), and report correlations between |Δλ2| and length-normalized behavioral differences to reduce length-induced artifacts.- Bolded title: **Expand and formalize ablation-based causal validation**\n  - Detail ablation mechanics (e.g., masking attention logits vs. zeroing output), affected heads/layers, and evaluation items/languages (Section 6.4; Table 4) to support precise replication.\n  - Report bootstrap CIs and permutation tests for ablation-induced Δλ2 changes across windows, and include behavioral measurements post-ablation to tie causal spectral changes to functional outcomes (Section 6.3; Table 4).\n  - Run cross-language ablation studies and compare families under matched interventions, providing quantitative resilience metrics rather than qualitative narratives (Section 6.4), and assess whether compensation effects persist under stronger or alternative ablations.- Bolded title: **Substantiate generalization claims with broader, preregistered studies**\n  - Extend reasoning-mode analyses beyond transitivity and beyond a single model, using public benchmarks and preregistered endpoints for Δλ2 and RCI, and report cross-architecture results (Section 7.1; Appendix F).\n  - Evaluate the hallucination detector on larger, public datasets with clearly defined baselines and ablations of the detector threshold (Section 7.3; Eq. (2)), including precision–recall trade-offs and calibration analysis.\n  - Improve figure resolution and numeric consistency across sections (Figures 3–4; Tables 1–2; 5), and provide a consolidated table enumerating languages, voice-type assignments, and per-language Δλ2 with CIs for each family to clarify generalization scope.Score\n- Overall (10): 7 — A well-motivated spectral framework with clear cross-family fingerprints and functional links (Sections 3.1–3.2; Figures 2–4; Table 3), tempered by correlational analyses and under-specified confound controls (Section 5; Appendix E.3; Table 2).\n- Novelty (10): 8 — Training-free, layer-resolved spectral fingerprints over attention graphs (Figure 1; Section 3.1–3.2; Appendix A) with family-specific signatures (Figure 2; Figure 3) and tokenizer-stress analysis (Section 5; Table 2).\n- Technical Quality (10): 6 — Solid operator design and robustness checks (Appendix B; Table 6) but limited formal theory, small effects in many languages (Figure 3–4), and preliminary causality/generalization (Section 6.4; Section 7.1–7.3).\n- Clarity (10): 7 — Clear metric definitions and endpoints (Section 3.1–3.2; Appendix F) with comprehensive robustness appendices, offset by incomplete dataset/prompt details and some low-resolution/inconsistent figures (Appendix H; Figures 3–4; Tables 1–2).\n- Confidence (5): 4 — High confidence in reading-based assessment with numerous manuscript anchors (Sections 3–7; Appendices B–F), moderate reservation due to missing artifacts for full replication (Appendix H) and preliminary nature of extensions.",
  "final_review": "Summary\n- The paper proposes a training-free graph signal processing framework that models attention as dynamic token graphs and hidden states as signals, defining a Laplacian per layer to compute spectral diagnostics, with a primary endpoint given by the early-window mean change in the Fiedler eigenvalue, Δλ2, between passive and active sentences (Section 3.1–3.2; Equation (1); Figure 1). The main evaluation considers three model families (Qwen2.5‑7B, Phi‑3‑Mini, LLaMA‑3.2‑1B) and 20 languages (Section 4). Results show a large English-specific disruption in Phi‑3‑Mini (Δλ2[2,5] ≈ −0.446; Figure 2), small mixed-direction shifts with category means near zero for Qwen2.5‑7B (Figure 3; Appendix Table 6), and systematic but muted effects for LLaMA‑3.2‑1B (Figure 4). Δλ2 correlates with tokenizer fragmentation (Table 2) and with behavioral fit (Table 3). Attention-head ablations modulate Δλ2 (Section 6.4; Table 4). Extensions include preliminary reasoning-mode signatures (Table 5; Appendix F) and a small-scale hallucination detector (Section 7.3; Eq. (2)).Strengths\n- Bolded title: **Clear, principled spectral formulation over attention-induced token graphs**\n  - Evidence: Formal definition of symmetrized, head-aggregated graphs and Laplacians with attention matrices A(ℓ,h) (Section 3.1; Equation (1); Figure 1). Why it matters: Provides a coherent operator-based foundation to interpret attention as a graph and hidden states as graph signals (novelty/technical soundness).\n  - Evidence: Four diagnostics defined (Dirichlet energy, spectral entropy, HFER, Fiedler connectivity) with explicit matrix relationships (Section 3.1). Why it matters: Enables multi-faceted spectral analysis beyond attention visualization or probing (novelty/clarity).\n  - Evidence: Theoretical rationale for λ2 as algebraic connectivity linked to information flow and bottlenecks (Appendix A.1). Why it matters: Strengthens interpretability of Δλ2 as a compact, theoretically grounded endpoint (technical soundness/impact).\n- Bolded title: **Prespecified early-window endpoint and sensitivity checks**\n  - Evidence: Primary endpoint fixed to early window layers 2–5 (Section 3.2), motivated by “onset of multihead context integration.” Why it matters: Reduces researcher degrees of freedom and supports comparability across models (rigor/clarity).\n  - Evidence: Stability to window shifts (1–4, 3–6) and HFER cutoff sweeps (Appendix B.4; Appendix E.2, early window choice). Why it matters: Demonstrates robustness of conclusions to reasonable analysis choices (technical soundness).\n  - Evidence: Normalization and aggregation robustness (random-walk vs. symmetric Laplacians; uniform vs. mass-weighted head aggregation; directed variants), preserving signs and peak locations (Appendix B.1–B.3; Table 6). Why it matters: Reduces concern that results hinge on arbitrary operator choices (rigor/reliability).\n- Bolded title: **Cross-model, cross-lingual results revealing family-specific spectral fingerprints**\n  - Evidence: Phi‑3‑Mini shows a large, English-specific early-layer negative shift (Δλ2[2,5] ≈ −0.446; Figure 2a/b); others near zero. Why it matters: Illustrates architecture-specific specialization effects that the framework can detect (impact/novelty).\n  - Evidence: Qwen2.5‑7B exhibits small, mixed-direction shifts with category means close to zero (Figure 3; Appendix Table 6 shows a small positive overall mean under the default variant). Why it matters: Identifies a different, more uniform and low-magnitude processing signature that the framework can still resolve (experimental coverage).\n  - Evidence: LLaMA‑3.2‑1B exhibits systematic but muted effects with similar ordering trends (Figure 4). Why it matters: Shows consistent but scaled-down behavior, supporting generality of the probe across models (experimental rigor).\n- Bolded title: **Tokenizer-stress analysis linking subword fragmentation to spectral effects**\n  - Evidence: Pieces/character and fragmentation entropy defined and used as covariates (Section 5), with model-family-specific correlations (Table 2; Section 6.2). Why it matters: Distinguishes subword confounds from architecture sensitivity, revealing complementary fingerprints (novelty/technical soundness).\n  - Evidence: Differential tokenization effects between active/passive variants are examined, with LLaMA showing the strongest relationship (r = 0.51, p = 0.069; Section 5; “Differential tokenization effects”). Why it matters: Highlights a realistic source of stress that can modulate processing signatures (impact/experimental rigor).\n  - Evidence: Mixed-effects regressions with random intercepts by language; covariates standardized within family (Section 5). Why it matters: Indicates an attempt to control for cross-language variability in fragmentation metrics (rigor).\n- Bolded title: **Functional relevance via spectral–behavioral correlations and targeted interventions**\n  - Evidence: Strong negative correlations between Δλ2 and behavioral deltas across families (Table 3; Section 6.3), e.g., Phi‑3‑Mini r = −0.976. Why it matters: Links spectral connectivity changes to behavioral fit, supporting interpretive validity (impact/technical soundness).\n  - Evidence: Intervention validation via limited head ablations aligning spectral signs with behavioral shifts in controlled items (Section 6.3, “Intervention validation”). Why it matters: Strengthens functional claims beyond correlation (rigor/impact).\n  - Evidence: Causal validation through attention-head ablations with windowed Δλ2 changes revealing family-specific resilience vs. brittleness profiles (Section 6.4; Table 4). Why it matters: Demonstrates that early attention structure causally shapes spectral connectivity (novelty/technical soundness).\n- Bolded title: **Extended robustness and reproducibility efforts**\n  - Evidence: Expanded sample size experiments (up to n=50 paraphrases) replicate key findings, including the Phi‑3 English-specific disruption (Appendix C; Figures 8–13). Why it matters: Supports statistical reliability beyond small-n settings (experimental rigor).\n  - Evidence: Reproducibility details with ARPACK settings, bootstrap/permutation parameters, and BH–FDR (Appendix B.5; Appendix E; Appendix H). Why it matters: Enables independent verification and clarifies computational settings (clarity/reliability).\n  - Evidence: Clear definitions of derived metrics (RCI in Appendix F; hallucination detector z-score in Section 7.3; Eq. (2)). Why it matters: Facilitates re-use and critical examination (clarity/impact).Weaknesses\n- Bolded title: **Incomplete methodological specification and limited reproducibility artifacts**\n  - Evidence: The multilingual dataset and paraphrase generation are described at a high level (“at least 10 paraphrases per voice per language… template-matched,” Section 4) without item-level details, prompt templates, or a complete accounting of layer indexing (e.g., fractional layer labels such as “2.5” appear without definition; Appendix, figure text near layer zooms). Why it matters: Limits independent replication and interpretability of layer-resolved summaries (clarity/reproducibility).\n  - Evidence: The anonymized repository includes only one representative language and a separate 6-language dataset link (Appendix H); behavioral validation relies on Qwen‑2.2‑7b while the main spectral analysis centers on Qwen2.5‑7B (Appendix E.3 vs. Section 4). Why it matters: Mixing model versions and partial artifact release hinders end-to-end replication and cross-section consistency (reproducibility/clarity).\n  - Evidence: Tokenization controls are “light” (≤2-token drift “when feasible,” Section 5; Appendix E.3), with no comprehensive reporting of drift distributions across languages or models; some exemplar values mentioned in the text (e.g., per-language pieces/char and |Δλ2| examples) are not directly tabulated. Why it matters: Without full drift statistics and documented exemplars, residual confounding may remain in Δλ2 and behavioral scores (technical rigor); No direct evidence found in the manuscript for all cited exemplar values.\n- Bolded title: **Statistical analysis leaves room for overinterpretation and sensitivity to outliers**\n  - Evidence: Many effects are small with CIs overlapping zero (e.g., Qwen2.5‑7B per-language/type bars show near-zero means with overlapping CIs; Figure 3; LLaMA “muted effects,” Figure 4), yet the text emphasizes consistent signatures. Why it matters: Small, borderline effects risk overinterpretation if not accompanied by stringent, preregistered hypothesis tests (experimental rigor).\n  - Evidence: FDR corrections are applied “within each model family” (Section 6.1; Appendix E.2), but per-language/type claims of “FDR–sig” are not accompanied by visible q-values in Figures 2–4; Appendix E.2 notes the procedure but does not show the specific adjusted values in the main figures. Why it matters: Multiplicity without transparent adjusted statistics can inflate false positives and impair interpretability (technical rigor/clarity).\n  - Evidence: The spectral–behavioral analysis correlates Δλ2 (passive − active) with NLL deltas (Table 3; Section 6.3), while the surrounding text sometimes describes magnitude relationships (|Δλ2|) and large negative Δλ2 outliers (e.g., Phi‑3 English, Figure 2a) may dominate Pearson r. Why it matters: Variable-definition ambiguity (Δλ2 vs. |Δλ2|) and outlier sensitivity can misstate functional linkage strength; robust alternatives (Spearman, leave-one-out) are needed (technical rigor).\n- Bolded title: **Theory–practice gap: mapping Δλ2 to syntactic computation remains heuristic**\n  - Evidence: Theoretical justification ties λ2 to connectivity robustness (Appendix A.1), but no formal derivation links attention symmetrization and Δλ2 changes to specific operations of voice alternation (agent–patient reassignment, auxiliary coupling) beyond qualitative prediction (Section 3.2). Why it matters: Without formal modeling, Δλ2 may capture unrelated structural shifts (novelty/technical soundness).\n  - Evidence: Symmetrization of inherently directed attention (Section 3.1) simplifies flow structure; directed variants are explored (Appendix B.2), but the final endpoints rely on Hermitian forms and partial phase choices, with remaining interpretive ambiguity. Why it matters: Directionality is central to syntactic dependencies; symmetrization may obscure mechanism (technical rigor).\n  - Evidence: Head aggregation relies on mass weighting by total attention (Section 3.1; Appendix B.3), but given post‑softmax, row-stochastic A^(ℓ,h), the proposed s_h = ∑_{i,j} A_{ij}^{(ℓ,h)} can collapse to a constant across heads, rendering the mass-weighted default effectively uniform unless further qualified; learned convex combinations are motivated empirically (Appendix B.3). Why it matters: Aggregation choices can alter spectral structure; potential degeneracy and empirical tuning add interpretive ambiguity (technical soundness).\n- Bolded title: **Potential confounds from tokenization, morphology, and length remain**\n  - Evidence: Tokenization drift is only lightly controlled (≤2 tokens “when feasible,” Section 5; Appendix E.3) and per-language drift statistics are not systematically reported; LLaMA shows sensitivity to drift (Section 5 “Differential tokenization effects”). Why it matters: Δλ2 and NLL can both be affected by token count and segmentation differences rather than linguistic computation alone (technical rigor).\n  - Evidence: Morphological complexity and voice-realization types (Section 4) are treated as groupings, but fragmentation may co-vary with morphology, domain, or sentence templates; mixed-effects regressions are described (Section 5) without model specifications or coefficients per family beyond correlation summaries (Table 2). Why it matters: Confounds can produce apparent family-specific patterns; stronger controls are needed (experimental rigor).\n  - Evidence: Behavioral metric is negative mean NLL without explicit normalization for length or tokenization differences (Section 6.3; Appendix E.3). Why it matters: NLL is sensitive to length and token frequency; unnormalized comparisons can bias correlations with |Δλ2| (technical soundness).\n- Bolded title: **Ablation-based causal validation is promising but under-specified**\n  - Evidence: Table 4 reports Δλ2 changes across early/mid/late windows for specific head ablations (Section 6.4), but the ablation mechanism (e.g., zeroing attention outputs vs. masking attention logits), whether applied only at inference, and how “Overall” is aggregated are not detailed. Why it matters: Implementation details are essential to interpret causal claims and to replicate effects (technical rigor/reproducibility).\n  - Evidence: No statistical tests or CIs are provided for ablation effects in Table 4, and no behavioral measurements post-ablation are reported, aside from a brief note in Section 6.3 for controlled items. Why it matters: Without effect size uncertainty and behavioral validation, mechanistic conclusions remain tentative (experimental rigor).\n  - Evidence: Family-specific interpretations (e.g., “computational resilience” for Qwen; Section 6.4) are based on sign patterns without formal controls across languages or items. Why it matters: Qualitative narratives may overreach without quantitative backing (clarity/technical soundness).\n- Bolded title: **Generalization beyond voice alternation is preliminary and partly ad hoc**\n  - Evidence: Reasoning-mode signatures use a single model (phi‑3.5‑mini), a single task family (transitivity), and a bespoke RCI index (Table 5; Appendix F), with limited sample size and no cross-architecture validation (Section 7.1; 7.2 “preliminary”). Why it matters: Generality claims require broader tasks, models, and preregistered evaluation (novelty/impact).\n  - Evidence: Hallucination detector is reported on a small held-out set (n = 80) with an anonymized external manuscript; the detector threshold uses final-layer Fiedler z-score (Section 7.3; Eq. (2)) with no rigorous baseline comparison on public benchmarks. Why it matters: Application claims are suggestive but not yet substantiated at scale (impact/technical rigor).\n  - Evidence: Some figure captions and values are approximate or low-resolution (“~−0.04,” “Zoom: layers 2–5,” Figures 3–4), and several numerical summaries differ across sections (e.g., Qwen overall mean in Table 6 vs. earlier narrative; Figures 3–4 vs. Appendix expanded results). Why it matters: Presentation contributes to uncertainty in interpreting the claimed generalization and performance alignment (clarity).Suggestions for Improvement\n- Bolded title: **Enhance methodological transparency and reproducibility artifacts**\n  - Provide item-level documentation for the multilingual dataset: release full language lists, per-language sentence templates, paraphrase generation rules, and matching criteria across models, and define any fractional layer indices used in plots (Section 4; Appendix, layer zooms). This will enable exact reproduction and clarify layer-resolved interpretations.\n  - Expand the anonymized repository to include the full 20-language set (or a representative large subset) and scripts to regenerate all main figures, and standardize model versions across spectral and behavioral analyses (Appendix H; Section 4; Appendix E.3). If size limits are a concern, host on an archival platform with checksums and a data card.\n  - Report comprehensive tokenization drift statistics per language and model (histograms and summary metrics), including active–passive differences, and include drift-controlled analyses where drift is exactly matched; tabulate per-language tokenizer metrics and exemplar values referenced in the text (Section 5; Appendix E.3).\n- Bolded title: **Strengthen statistical rigor and robustness to outliers**\n  - Supplement per-language and per-type results with preregistered hypotheses and cross-family multiplicity control (e.g., hierarchical FDR across language, type, and family), and report adjusted q-values alongside CIs in figures/tables (Section 6.1; Appendix E.2).\n  - Report leave-one-language-out and robust correlation analyses (Spearman, Theil–Sen) for spectral–behavioral and tokenizer-stress relationships; explicitly test the impact of the English outlier in Phi‑3 and clarify whether Δλ2 or |Δλ2| is the variable of interest in each correlation (Figure 2a; Table 3; Table 2).\n  - Provide confidence intervals for all correlation estimates, mixed-effects coefficients, and effect sizes, and include power analyses for the main endpoints beyond Appendix C, clarifying detectable effect scales (Appendix C; Appendix E.1).\n- Bolded title: **Tighten the theory–practice link between Δλ2 and syntactic computation**\n  - Derive or empirically validate a mechanistic mapping (e.g., controlled synthetic sequences) linking agent–patient reassignment and auxiliary coupling to predictable Δλ2 patterns, beyond qualitative predictions (Section 3.2; Appendix A.1).\n  - Extend directed-operator analysis with task-specific phase calibration and compare Δλ2 under directed magnetic Laplacians to symmetrized variants on identical items (Appendix B.2), quantifying the impact of directionality on conclusions.\n  - Justify head aggregation choices with principled criteria (e.g., variance-explained or causal relevance metrics), address potential degeneracy of attention-mass weighting under row-stochastic attention (Section 3.1), and show that learned α(ℓ) generalize out-of-sample without overfitting to cross-condition differences (Appendix B.3).\n- Bolded title: **Control confounds from tokenization, morphology, and length more strictly**\n  - Implement exact length/token-count matching for active/passive pairs wherever possible and report analyses restricted to perfectly matched pairs (Section 5; Appendix E.3), quantifying how Δλ2 and NLL change under stricter control.\n  - Augment mixed-effects regressions with explicit morphology/type covariates and interaction terms (Section 4; Section 5), reporting full model specifications and family-wise coefficient tables to separate fragmentation from morphological complexity.\n  - Normalize behavioral NLL by length or use per-token NLL (Appendix E.3; Section 6.3), and report correlations between |Δλ2| and length-normalized behavioral differences to reduce length-induced artifacts.\n- Bolded title: **Expand and formalize ablation-based causal validation**\n  - Detail ablation mechanics (e.g., masking attention logits vs. zeroing outputs), whether interventions are inference-time only, the items/languages used, and define how “Overall” is computed (Section 6.4; Table 4) to support precise replication.\n  - Report bootstrap CIs and permutation tests for ablation-induced Δλ2 changes across windows, and include behavioral measurements post-ablation to tie causal spectral changes to functional outcomes (Section 6.3; Table 4).\n  - Run cross-language ablation studies and compare families under matched interventions, providing quantitative resilience metrics rather than qualitative narratives (Section 6.4), and assess whether compensation effects persist under stronger or alternative ablations.\n- Bolded title: **Substantiate generalization claims with broader, preregistered studies**\n  - Extend reasoning-mode analyses beyond transitivity and beyond a single model, using public benchmarks and preregistered endpoints for Δλ2 and RCI, and report cross-architecture results (Section 7.1; Appendix F).\n  - Evaluate the hallucination detector on larger, public datasets with clearly defined baselines and ablations of the detector threshold (Section 7.3; Eq. (2)), including precision–recall trade-offs and calibration analysis.\n  - Improve figure resolution and numeric consistency across sections (Figures 3–4; Tables 1–2; 5), and provide a consolidated table enumerating languages, voice-type assignments, and per-language Δλ2 with CIs for each family to clarify generalization scope.Score\n- Overall (10): 7 — A well-motivated spectral framework with cross-family fingerprints and functional links (Sections 3.1–3.2; Figures 2–4; Table 3), tempered by correlational analyses, partial artifacts (Appendix H), and some internal inconsistencies and presentation gaps (Figure 3 vs. Appendix Table 6; Section 6.1; Appendix E.3).\n- Novelty (10): 8 — Training-free, layer-resolved spectral fingerprints over attention graphs (Figure 1; Section 3.1–3.2; Appendix A) with family-specific signatures (Figure 2; Figure 4) and tokenizer-stress analysis (Section 5; Table 2).\n- Technical Quality (10): 6 — Solid operator design and robustness checks (Appendix B; Table 6) but limited formal theory, small effects in many languages (Figures 3–4), potential aggregation degeneracy (Section 3.1; Appendix B.3), and preliminary causality/generalization (Section 6.4; Section 7.1–7.3).\n- Clarity (10): 6 — Clear metric definitions and endpoints (Section 3.1–3.2; Appendix F) offset by incomplete dataset/prompt details, undefined fractional layer notation (Appendix, layer zooms), model-version mixing (Appendix E.3 vs. Section 4), and low-resolution/inconsistent figures (Figures 3–4; Table 6 vs. Section 6.1).\n- Confidence (5): 4 — High confidence in reading-based assessment with numerous manuscript anchors (Sections 3–7; Appendices B–F, H), with some reservations due to missing artifacts for full replication (Appendix H) and internal consistency issues (Section 6.1; Appendix Table 6).",
  "scores": {
    "baseline": {
      "parsed": {
        "overall": 7,
        "novelty": 8,
        "technical_quality": 6,
        "clarity": 7,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    },
    "refined": {
      "parsed": {
        "overall": 7,
        "novelty": 8,
        "technical_quality": 6,
        "clarity": 6,
        "confidence": 4
      },
      "max": {
        "overall": 10,
        "novelty": 10,
        "technical_quality": 10,
        "clarity": 10,
        "confidence": 5
      },
      "missing": []
    }
  },
  "reviews": "Summary\n- The paper proposes a training-free graph signal processing framework that models attention as dynamic token graphs and hidden states as signals, defining a Laplacian per layer to compute spectral diagnostics, with a primary endpoint given by the early-window mean change in the Fiedler eigenvalue, Δλ2, between passive and active sentences (Section 3.1–3.2; Equation (1); Figure 1). The main evaluation considers three model families (Qwen2.5‑7B, Phi‑3‑Mini, LLaMA‑3.2‑1B) and 20 languages (Section 4). Results show a large English-specific disruption in Phi‑3‑Mini (Δλ2[2,5] ≈ −0.446; Figure 2), small mixed-direction shifts with category means near zero for Qwen2.5‑7B (Figure 3; Appendix Table 6), and systematic but muted effects for LLaMA‑3.2‑1B (Figure 4). Δλ2 correlates with tokenizer fragmentation (Table 2) and with behavioral fit (Table 3). Attention-head ablations modulate Δλ2 (Section 6.4; Table 4). Extensions include preliminary reasoning-mode signatures (Table 5; Appendix F) and a small-scale hallucination detector (Section 7.3; Eq. (2)).Strengths\n- Bolded title: **Clear, principled spectral formulation over attention-induced token graphs**\n  - Evidence: Formal definition of symmetrized, head-aggregated graphs and Laplacians with attention matrices A(ℓ,h) (Section 3.1; Equation (1); Figure 1). Why it matters: Provides a coherent operator-based foundation to interpret attention as a graph and hidden states as graph signals (novelty/technical soundness).\n  - Evidence: Four diagnostics defined (Dirichlet energy, spectral entropy, HFER, Fiedler connectivity) with explicit matrix relationships (Section 3.1). Why it matters: Enables multi-faceted spectral analysis beyond attention visualization or probing (novelty/clarity).\n  - Evidence: Theoretical rationale for λ2 as algebraic connectivity linked to information flow and bottlenecks (Appendix A.1). Why it matters: Strengthens interpretability of Δλ2 as a compact, theoretically grounded endpoint (technical soundness/impact).\n- Bolded title: **Prespecified early-window endpoint and sensitivity checks**\n  - Evidence: Primary endpoint fixed to early window layers 2–5 (Section 3.2), motivated by “onset of multihead context integration.” Why it matters: Reduces researcher degrees of freedom and supports comparability across models (rigor/clarity).\n  - Evidence: Stability to window shifts (1–4, 3–6) and HFER cutoff sweeps (Appendix B.4; Appendix E.2, early window choice). Why it matters: Demonstrates robustness of conclusions to reasonable analysis choices (technical soundness).\n  - Evidence: Normalization and aggregation robustness (random-walk vs. symmetric Laplacians; uniform vs. mass-weighted head aggregation; directed variants), preserving signs and peak locations (Appendix B.1–B.3; Table 6). Why it matters: Reduces concern that results hinge on arbitrary operator choices (rigor/reliability).\n- Bolded title: **Cross-model, cross-lingual results revealing family-specific spectral fingerprints**\n  - Evidence: Phi‑3‑Mini shows a large, English-specific early-layer negative shift (Δλ2[2,5] ≈ −0.446; Figure 2a/b); others near zero. Why it matters: Illustrates architecture-specific specialization effects that the framework can detect (impact/novelty).\n  - Evidence: Qwen2.5‑7B exhibits small, mixed-direction shifts with category means close to zero (Figure 3; Appendix Table 6 shows a small positive overall mean under the default variant). Why it matters: Identifies a different, more uniform and low-magnitude processing signature that the framework can still resolve (experimental coverage).\n  - Evidence: LLaMA‑3.2‑1B exhibits systematic but muted effects with similar ordering trends (Figure 4). Why it matters: Shows consistent but scaled-down behavior, supporting generality of the probe across models (experimental rigor).\n- Bolded title: **Tokenizer-stress analysis linking subword fragmentation to spectral effects**\n  - Evidence: Pieces/character and fragmentation entropy defined and used as covariates (Section 5), with model-family-specific correlations (Table 2; Section 6.2). Why it matters: Distinguishes subword confounds from architecture sensitivity, revealing complementary fingerprints (novelty/technical soundness).\n  - Evidence: Differential tokenization effects between active/passive variants are examined, with LLaMA showing the strongest relationship (r = 0.51, p = 0.069; Section 5; “Differential tokenization effects”). Why it matters: Highlights a realistic source of stress that can modulate processing signatures (impact/experimental rigor).\n  - Evidence: Mixed-effects regressions with random intercepts by language; covariates standardized within family (Section 5). Why it matters: Indicates an attempt to control for cross-language variability in fragmentation metrics (rigor).\n- Bolded title: **Functional relevance via spectral–behavioral correlations and targeted interventions**\n  - Evidence: Strong negative correlations between Δλ2 and behavioral deltas across families (Table 3; Section 6.3), e.g., Phi‑3‑Mini r = −0.976. Why it matters: Links spectral connectivity changes to behavioral fit, supporting interpretive validity (impact/technical soundness).\n  - Evidence: Intervention validation via limited head ablations aligning spectral signs with behavioral shifts in controlled items (Section 6.3, “Intervention validation”). Why it matters: Strengthens functional claims beyond correlation (rigor/impact).\n  - Evidence: Causal validation through attention-head ablations with windowed Δλ2 changes revealing family-specific resilience vs. brittleness profiles (Section 6.4; Table 4). Why it matters: Demonstrates that early attention structure causally shapes spectral connectivity (novelty/technical soundness).\n- Bolded title: **Extended robustness and reproducibility efforts**\n  - Evidence: Expanded sample size experiments (up to n=50 paraphrases) replicate key findings, including the Phi‑3 English-specific disruption (Appendix C; Figures 8–13). Why it matters: Supports statistical reliability beyond small-n settings (experimental rigor).\n  - Evidence: Reproducibility details with ARPACK settings, bootstrap/permutation parameters, and BH–FDR (Appendix B.5; Appendix E; Appendix H). Why it matters: Enables independent verification and clarifies computational settings (clarity/reliability).\n  - Evidence: Clear definitions of derived metrics (RCI in Appendix F; hallucination detector z-score in Section 7.3; Eq. (2)). Why it matters: Facilitates re-use and critical examination (clarity/impact).Weaknesses\n- Bolded title: **Incomplete methodological specification and limited reproducibility artifacts**\n  - Evidence: The multilingual dataset and paraphrase generation are described at a high level (“at least 10 paraphrases per voice per language… template-matched,” Section 4) without item-level details, prompt templates, or a complete accounting of layer indexing (e.g., fractional layer labels such as “2.5” appear without definition; Appendix, figure text near layer zooms). Why it matters: Limits independent replication and interpretability of layer-resolved summaries (clarity/reproducibility).\n  - Evidence: The anonymized repository includes only one representative language and a separate 6-language dataset link (Appendix H); behavioral validation relies on Qwen‑2.2‑7b while the main spectral analysis centers on Qwen2.5‑7B (Appendix E.3 vs. Section 4). Why it matters: Mixing model versions and partial artifact release hinders end-to-end replication and cross-section consistency (reproducibility/clarity).\n  - Evidence: Tokenization controls are “light” (≤2-token drift “when feasible,” Section 5; Appendix E.3), with no comprehensive reporting of drift distributions across languages or models; some exemplar values mentioned in the text (e.g., per-language pieces/char and |Δλ2| examples) are not directly tabulated. Why it matters: Without full drift statistics and documented exemplars, residual confounding may remain in Δλ2 and behavioral scores (technical rigor); No direct evidence found in the manuscript for all cited exemplar values.\n- Bolded title: **Statistical analysis leaves room for overinterpretation and sensitivity to outliers**\n  - Evidence: Many effects are small with CIs overlapping zero (e.g., Qwen2.5‑7B per-language/type bars show near-zero means with overlapping CIs; Figure 3; LLaMA “muted effects,” Figure 4), yet the text emphasizes consistent signatures. Why it matters: Small, borderline effects risk overinterpretation if not accompanied by stringent, preregistered hypothesis tests (experimental rigor).\n  - Evidence: FDR corrections are applied “within each model family” (Section 6.1; Appendix E.2), but per-language/type claims of “FDR–sig” are not accompanied by visible q-values in Figures 2–4; Appendix E.2 notes the procedure but does not show the specific adjusted values in the main figures. Why it matters: Multiplicity without transparent adjusted statistics can inflate false positives and impair interpretability (technical rigor/clarity).\n  - Evidence: The spectral–behavioral analysis correlates Δλ2 (passive − active) with NLL deltas (Table 3; Section 6.3), while the surrounding text sometimes describes magnitude relationships (|Δλ2|) and large negative Δλ2 outliers (e.g., Phi‑3 English, Figure 2a) may dominate Pearson r. Why it matters: Variable-definition ambiguity (Δλ2 vs. |Δλ2|) and outlier sensitivity can misstate functional linkage strength; robust alternatives (Spearman, leave-one-out) are needed (technical rigor).\n- Bolded title: **Theory–practice gap: mapping Δλ2 to syntactic computation remains heuristic**\n  - Evidence: Theoretical justification ties λ2 to connectivity robustness (Appendix A.1), but no formal derivation links attention symmetrization and Δλ2 changes to specific operations of voice alternation (agent–patient reassignment, auxiliary coupling) beyond qualitative prediction (Section 3.2). Why it matters: Without formal modeling, Δλ2 may capture unrelated structural shifts (novelty/technical soundness).\n  - Evidence: Symmetrization of inherently directed attention (Section 3.1) simplifies flow structure; directed variants are explored (Appendix B.2), but the final endpoints rely on Hermitian forms and partial phase choices, with remaining interpretive ambiguity. Why it matters: Directionality is central to syntactic dependencies; symmetrization may obscure mechanism (technical rigor).\n  - Evidence: Head aggregation relies on mass weighting by total attention (Section 3.1; Appendix B.3), but given post‑softmax, row-stochastic A^(ℓ,h), the proposed s_h = ∑_{i,j} A_{ij}^{(ℓ,h)} can collapse to a constant across heads, rendering the mass-weighted default effectively uniform unless further qualified; learned convex combinations are motivated empirically (Appendix B.3). Why it matters: Aggregation choices can alter spectral structure; potential degeneracy and empirical tuning add interpretive ambiguity (technical soundness).\n- Bolded title: **Potential confounds from tokenization, morphology, and length remain**\n  - Evidence: Tokenization drift is only lightly controlled (≤2 tokens “when feasible,” Section 5; Appendix E.3) and per-language drift statistics are not systematically reported; LLaMA shows sensitivity to drift (Section 5 “Differential tokenization effects”). Why it matters: Δλ2 and NLL can both be affected by token count and segmentation differences rather than linguistic computation alone (technical rigor).\n  - Evidence: Morphological complexity and voice-realization types (Section 4) are treated as groupings, but fragmentation may co-vary with morphology, domain, or sentence templates; mixed-effects regressions are described (Section 5) without model specifications or coefficients per family beyond correlation summaries (Table 2). Why it matters: Confounds can produce apparent family-specific patterns; stronger controls are needed (experimental rigor).\n  - Evidence: Behavioral metric is negative mean NLL without explicit normalization for length or tokenization differences (Section 6.3; Appendix E.3). Why it matters: NLL is sensitive to length and token frequency; unnormalized comparisons can bias correlations with |Δλ2| (technical soundness).\n- Bolded title: **Ablation-based causal validation is promising but under-specified**\n  - Evidence: Table 4 reports Δλ2 changes across early/mid/late windows for specific head ablations (Section 6.4), but the ablation mechanism (e.g., zeroing attention outputs vs. masking attention logits), whether applied only at inference, and how “Overall” is aggregated are not detailed. Why it matters: Implementation details are essential to interpret causal claims and to replicate effects (technical rigor/reproducibility).\n  - Evidence: No statistical tests or CIs are provided for ablation effects in Table 4, and no behavioral measurements post-ablation are reported, aside from a brief note in Section 6.3 for controlled items. Why it matters: Without effect size uncertainty and behavioral validation, mechanistic conclusions remain tentative (experimental rigor).\n  - Evidence: Family-specific interpretations (e.g., “computational resilience” for Qwen; Section 6.4) are based on sign patterns without formal controls across languages or items. Why it matters: Qualitative narratives may overreach without quantitative backing (clarity/technical soundness).\n- Bolded title: **Generalization beyond voice alternation is preliminary and partly ad hoc**\n  - Evidence: Reasoning-mode signatures use a single model (phi‑3.5‑mini), a single task family (transitivity), and a bespoke RCI index (Table 5; Appendix F), with limited sample size and no cross-architecture validation (Section 7.1; 7.2 “preliminary”). Why it matters: Generality claims require broader tasks, models, and preregistered evaluation (novelty/impact).\n  - Evidence: Hallucination detector is reported on a small held-out set (n = 80) with an anonymized external manuscript; the detector threshold uses final-layer Fiedler z-score (Section 7.3; Eq. (2)) with no rigorous baseline comparison on public benchmarks. Why it matters: Application claims are suggestive but not yet substantiated at scale (impact/technical rigor).\n  - Evidence: Some figure captions and values are approximate or low-resolution (“~−0.04,” “Zoom: layers 2–5,” Figures 3–4), and several numerical summaries differ across sections (e.g., Qwen overall mean in Table 6 vs. earlier narrative; Figures 3–4 vs. Appendix expanded results). Why it matters: Presentation contributes to uncertainty in interpreting the claimed generalization and performance alignment (clarity).Suggestions for Improvement\n- Bolded title: **Enhance methodological transparency and reproducibility artifacts**\n  - Provide item-level documentation for the multilingual dataset: release full language lists, per-language sentence templates, paraphrase generation rules, and matching criteria across models, and define any fractional layer indices used in plots (Section 4; Appendix, layer zooms). This will enable exact reproduction and clarify layer-resolved interpretations.\n  - Expand the anonymized repository to include the full 20-language set (or a representative large subset) and scripts to regenerate all main figures, and standardize model versions across spectral and behavioral analyses (Appendix H; Section 4; Appendix E.3). If size limits are a concern, host on an archival platform with checksums and a data card.\n  - Report comprehensive tokenization drift statistics per language and model (histograms and summary metrics), including active–passive differences, and include drift-controlled analyses where drift is exactly matched; tabulate per-language tokenizer metrics and exemplar values referenced in the text (Section 5; Appendix E.3).\n- Bolded title: **Strengthen statistical rigor and robustness to outliers**\n  - Supplement per-language and per-type results with preregistered hypotheses and cross-family multiplicity control (e.g., hierarchical FDR across language, type, and family), and report adjusted q-values alongside CIs in figures/tables (Section 6.1; Appendix E.2).\n  - Report leave-one-language-out and robust correlation analyses (Spearman, Theil–Sen) for spectral–behavioral and tokenizer-stress relationships; explicitly test the impact of the English outlier in Phi‑3 and clarify whether Δλ2 or |Δλ2| is the variable of interest in each correlation (Figure 2a; Table 3; Table 2).\n  - Provide confidence intervals for all correlation estimates, mixed-effects coefficients, and effect sizes, and include power analyses for the main endpoints beyond Appendix C, clarifying detectable effect scales (Appendix C; Appendix E.1).\n- Bolded title: **Tighten the theory–practice link between Δλ2 and syntactic computation**\n  - Derive or empirically validate a mechanistic mapping (e.g., controlled synthetic sequences) linking agent–patient reassignment and auxiliary coupling to predictable Δλ2 patterns, beyond qualitative predictions (Section 3.2; Appendix A.1).\n  - Extend directed-operator analysis with task-specific phase calibration and compare Δλ2 under directed magnetic Laplacians to symmetrized variants on identical items (Appendix B.2), quantifying the impact of directionality on conclusions.\n  - Justify head aggregation choices with principled criteria (e.g., variance-explained or causal relevance metrics), address potential degeneracy of attention-mass weighting under row-stochastic attention (Section 3.1), and show that learned α(ℓ) generalize out-of-sample without overfitting to cross-condition differences (Appendix B.3).\n- Bolded title: **Control confounds from tokenization, morphology, and length more strictly**\n  - Implement exact length/token-count matching for active/passive pairs wherever possible and report analyses restricted to perfectly matched pairs (Section 5; Appendix E.3), quantifying how Δλ2 and NLL change under stricter control.\n  - Augment mixed-effects regressions with explicit morphology/type covariates and interaction terms (Section 4; Section 5), reporting full model specifications and family-wise coefficient tables to separate fragmentation from morphological complexity.\n  - Normalize behavioral NLL by length or use per-token NLL (Appendix E.3; Section 6.3), and report correlations between |Δλ2| and length-normalized behavioral differences to reduce length-induced artifacts.\n- Bolded title: **Expand and formalize ablation-based causal validation**\n  - Detail ablation mechanics (e.g., masking attention logits vs. zeroing outputs), whether interventions are inference-time only, the items/languages used, and define how “Overall” is computed (Section 6.4; Table 4) to support precise replication.\n  - Report bootstrap CIs and permutation tests for ablation-induced Δλ2 changes across windows, and include behavioral measurements post-ablation to tie causal spectral changes to functional outcomes (Section 6.3; Table 4).\n  - Run cross-language ablation studies and compare families under matched interventions, providing quantitative resilience metrics rather than qualitative narratives (Section 6.4), and assess whether compensation effects persist under stronger or alternative ablations.\n- Bolded title: **Substantiate generalization claims with broader, preregistered studies**\n  - Extend reasoning-mode analyses beyond transitivity and beyond a single model, using public benchmarks and preregistered endpoints for Δλ2 and RCI, and report cross-architecture results (Section 7.1; Appendix F).\n  - Evaluate the hallucination detector on larger, public datasets with clearly defined baselines and ablations of the detector threshold (Section 7.3; Eq. (2)), including precision–recall trade-offs and calibration analysis.\n  - Improve figure resolution and numeric consistency across sections (Figures 3–4; Tables 1–2; 5), and provide a consolidated table enumerating languages, voice-type assignments, and per-language Δλ2 with CIs for each family to clarify generalization scope.Score\n- Overall (10): 7 — A well-motivated spectral framework with cross-family fingerprints and functional links (Sections 3.1–3.2; Figures 2–4; Table 3), tempered by correlational analyses, partial artifacts (Appendix H), and some internal inconsistencies and presentation gaps (Figure 3 vs. Appendix Table 6; Section 6.1; Appendix E.3).\n- Novelty (10): 8 — Training-free, layer-resolved spectral fingerprints over attention graphs (Figure 1; Section 3.1–3.2; Appendix A) with family-specific signatures (Figure 2; Figure 4) and tokenizer-stress analysis (Section 5; Table 2).\n- Technical Quality (10): 6 — Solid operator design and robustness checks (Appendix B; Table 6) but limited formal theory, small effects in many languages (Figures 3–4), potential aggregation degeneracy (Section 3.1; Appendix B.3), and preliminary causality/generalization (Section 6.4; Section 7.1–7.3).\n- Clarity (10): 6 — Clear metric definitions and endpoints (Section 3.1–3.2; Appendix F) offset by incomplete dataset/prompt details, undefined fractional layer notation (Appendix, layer zooms), model-version mixing (Appendix E.3 vs. Section 4), and low-resolution/inconsistent figures (Figures 3–4; Table 6 vs. Section 6.1).\n- Confidence (5): 4 — High confidence in reading-based assessment with numerous manuscript anchors (Sections 3–7; Appendices B–F, H), with some reservations due to missing artifacts for full replication (Appendix H) and internal consistency issues (Section 6.1; Appendix Table 6)."
}