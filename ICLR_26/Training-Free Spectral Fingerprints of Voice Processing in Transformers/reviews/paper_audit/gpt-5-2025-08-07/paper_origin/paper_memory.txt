# Global Summary
Problem: Reveal architecture-specific “computational fingerprints” in transformers by measuring how attention-induced token graphs reconfigure during syntactic transformations, focusing on active→passive voice alternation.

Core approach: Construct dynamic graphs from multihead attention, treat hidden states as graph signals, and track spectral diagnostics across layers. The prespecified primary endpoint is the early-window mean Δλ₂ (layers 2–5), where Δλ₂^(ℓ) = λ₂_passive^(ℓ) − λ₂_active^(ℓ). Models: Qwen2.5‑7B (28 layers), Phi‑3‑Mini (32 layers), LLaMA‑3.2‑1B (16 layers). Languages: 20 across diverse voice realization types. Uncertainty quantified via bootstrap CIs (2,000 resamples), paired permutation tests (10,000 shuffles), BH–FDR (q=0.05), and trimmed Hedges’ g (winsorization 1%, trimming 20%).

Evaluation scope: Crosslingual analysis (20 languages) of early-layer connectivity under voice alternation; tokenizer stress covariates (pieces/character and fragmentation entropy); spectral–behavioral correlations using negative mean NLL; targeted early-layer attention head ablations; preliminary generalization to reasoning strategies; small-scale hallucination detection proof-of-concept.

Key findings:
- Phi‑3‑Mini shows a dramatic English-specific early-layer disruption: Δλ₂[2,5] ≈ −0.446; French ≈ −0.134; remaining 19 languages tightly clustered near 0 (|Δλ₂| ≲ 0.02). The analytic voice type mean is dominated by English (−0.446).
- Qwen2.5‑7B exhibits small, distributed early negative shifts across types (effect sizes |g_trim| ≈ 0.2–0.4), with voice-type ordering patterns noted (several periphrastic/affixal negatives).
- LLaMA‑3.2‑1B shows systematic but muted effects; affixal languages show consistent negatives (≈ −0.030) and analytic types modest positives.
- Tokenizer stress differs by family: pieces/character correlates with |Δλ₂[2,5]| for Qwen (r = 0.51, 95% CI [0.15, 0.76]), Phi‑3 (r = −0.44, [−0.72, −0.08]), LLaMA (r = 0.29, [−0.17, 0.65]). Fragmentation entropy correlations: Phi‑3 0.36 [0.02, 0.66]; Qwen −0.23 [−0.58, 0.18]; LLaMA −0.25 [−0.62, 0.20].
- Spectral–behavioral correlations (early Δλ₂ vs. passive NLL) show consistent negative r: Phi‑3‑Mini r = −0.976 [−0.99, −0.89], Qwen2.5‑7B r = −0.627 [−0.85, −0.24], LLaMA‑3.2‑1B r = −0.143 [−0.62, 0.41].
- Head ablations demonstrate causal relevance and family-specific signatures (e.g., Qwen heavy early ablation: early +0.04538, mid −0.02696, late −0.00975, overall −0.00425).
- Generalization to reasoning: on 95 transitivity tasks with phi‑3.5‑mini, CoT (accuracy 0.695, positive Fiedler z) and Standard (0.600) show stronger connectivity; CoD (0.274) and ToT (0.253) show weaker connectivity; RCI aligns with performance.
- Hallucination detection prototype (final-layer Fiedler threshold) achieves 88.75% accuracy on n=80 (50 factual, 30 hallucinations).

Caveats explicitly stated: Effects are largely correlational; indirect provenance inference; tokenization/morphology/length can confound; English-specific Phi‑3 signature is “evidence consistent with an English emphasis,” not proof; early-window choice prespecified; robustness checks show sign stability across Laplacian normalizations and aggregation, but magnitudes vary; expanded n=50 for select languages confirms patterns.

# Abstract
- Problem and claim: Distinct transformer architectures implement the same linguistic computations via different connectivity patterns, yielding “computational fingerprints” detectable with spectral analysis.
- Method: Graph signal processing on attention-induced token graphs; primary endpoint is early-window mean Δλ₂ (layers 2–5) under voice alternation.
- Scope: 20 languages, 3 model families (Phi‑3‑Mini, Qwen2.5‑7B, LLaMA‑3.2‑1B).
- Key results: Phi‑3‑Mini shows English-specific early disruption (Δλ₂[2,5] ≈ −0.446); 19 other languages show minimal effects. Qwen2.5‑7B exhibits small shifts largest for morphologically rich languages; LLaMA‑3.2‑1B shows systematic but muted responses.
- Functional relevance: Spectral signatures correlate with behavioral differences (Phi‑3: r = −0.976) and change under targeted early attention head ablations, linking effects to early attention structure.
- Broader utility: Framework also differentiates reasoning modes; proposed as a simple, training-free diagnostic for architectural biases and reliability analysis.

# Introduction
- Motivation: Existing interpretability (attention visualization, probing, circuit analysis) reveals encoded information but not layer-wise evolution of syntactic computation; need training-free, scalable tracking of how architectures execute the same operation.
- Proposal: Treat attention as dynamic graphs and hidden states as signals; track algebraic connectivity via Fiedler value (λ₂). Report mean Δλ₂[2,5] as the prespecified endpoint aligned with early multihead context integration.
- Voice alternation probe: Active→passive requires agent–patient reassignment, auxiliary–participle coupling, reanchoring long-range dependencies, expected to produce connectivity signatures.
- Design for comparability: Head-aggregated attention; random-walk and symmetric Laplacians; tokenization drift limited; uncertainty via bootstrap and paired permutation tests; multiplicity via BH–FDR; tokenizer stress covariates (pieces/character, fragmentation entropy).
- Validation plan: 20 languages × 3 model families; focused manipulation (voice alternation) with crosslinguistic variability and behavioral significance.
- Contributions: (1) Spectral framework with Δλ₂ endpoint and early window; (2) uncertainty-aware statistics for matched contrasts; (3) family-specific spectral signatures; (4) robustness to normalization/aggregation; (5) functional relevance via spectral–behavioral correlations and head ablations; (6) preliminary generalization beyond linguistic processing.

# Related Work
- Interpretability and syntactic analysis: Attention visualization and probing emphasize encoded info; circuit analyses give mechanistic details; behavioral tests assess competence; training-free, layer-resolved tracking of executing the same syntactic operation remained missing.
- Graph-theoretic perspectives: Prior work treats attention as graphs; spectral methods in GNNs/CNNs inform connectivity analysis. This work formalizes attention-derived graphs, normalization, aggregation, and tracks algebraic connectivity across layers during controlled alternations, comparing Laplacian variants (random-walk vs. symmetric; directed).
- Multilingual/cross-architectural generalization: Method fixes the manipulation and compares spectral responses across architectures/languages, accounting for tokenization/length confounds with uncertainty and mixed-effects summaries.
- Computational fingerprints: Extends work on model-imprinted signatures by providing training-free, layer-resolved diagnostics tied to behavior and interventions, with transparent analysis choices (effect sizes, confidence intervals, permutation tests, multiple-testing control).

# Method
- Spectral framework: For each layer ℓ with H heads over N tokens, construct head-aggregated, symmetrized attention W^(ℓ) and Laplacian L^(ℓ) (combinatorial; checks with normalized L_sym). Hidden states X^(ℓ) are graph signals.
- Diagnostics: (i) Dirichlet energy E^(ℓ); (ii) spectral entropy SE^(ℓ); (iii) high-frequency energy ratio HFER^(ℓ)(K); (iv) Fiedler connectivity λ₂^(ℓ).
- Head aggregation: Default mass-weighted head aggregation α_h^(ℓ) ∝ ∑_{i,j} A_{ij}^{(ℓ,h)}; uniform weights used in robustness checks.
- Primary endpoint: Δλ₂^(ℓ) = λ₂_passive^(ℓ) − λ₂_active^(ℓ); report mean over layers 2–5 as Δλ₂[2,5]. λ₂ showed superior sensitivity and consistent directional effects across families.
- Theoretical prediction: Under passive morphology, reconfigured long-range dependencies should yield early-layer connectivity changes that (a) survive length controls, (b) are larger for argument-structure changes than tense/number, (c) correlate with behavior.
- Analysis scope: Observational analysis across 20 languages × 3 families (Qwen2.5‑7B, Phi‑3‑Mini, LLaMA‑3.2‑1B) plus causal validation via early attention head ablations; implementation variants (directed Laplacians, aggregation, normalization) in Appendix B.
- Models and languages: Qwen2.5‑7B (28L), Phi‑3‑Mini (32L), LLaMA‑3.2‑1B (16L). 20 languages grouped by voice type (analytic, periphrastic, affixal, particle, non‑concatenative, etc.). At least 10 paraphrases per voice per language; template-matched; tokenizer-level length control when feasible. Statistical methods (bootstrap, effect sizes) in Appendix E.
- Tokenizer stress metrics: Pieces/character φ(s,T) = |T(s)|/|s|; fragmentation entropy H_frag(s,T) with length-normalized Ĥ_frag = H_frag/|T(s)|. Length control |T(pass)| − |T(act)| ≤ 2 when feasible. Mixed-effects regressions (random intercept by language) regress |Δλ₂[2,5]| on standardized φ and Ĥ_frag; report bootstrap CIs. Family-specific patterns include Qwen r = 0.51 (pieces/char), Phi‑3 r = −0.44, LLaMA r = 0.29; example: Yoruba 0.57 pieces/char, |Δλ₂| = 0.054; English 0.20 pieces/char, |Δλ₂| = 0.446. LLaMA shows strongest correlation with tokenization differences between active/passive (r = 0.51, p = 0.069).

# Experiments
- Setup: Per language, average at least ten paraphrases per voice (active/passive), compute Δλ₂[2,5]. Error bars: nonparametric 95% bootstrap CIs (2,000 resamples). p‑values: paired permutation tests (10,000 shuffles) within paraphrase pairs; BH–FDR at q = 0.05. Effect sizes: trimmed Hedges’ g (winsorization 1%, trimming 20%).
- Phi‑3‑Mini (32L): English (EN) shows large negative Δλ₂[2,5] ≈ −0.446 (FDR significant; g_trim ≈ large). French (FR) ≈ −0.134 (marginal after FDR). All other languages cluster near 0 (|Δλ₂| ≲ 0.02; non‑sig). By voice type, analytic mean is strongly negative, driven by English; other types near zero.
- Per-language Phi‑3 early-window Δλ₂[2,5] highlights: EN −0.446; FR −0.134; RU −0.023; KO −0.015; ES −0.013; TL +0.008; YO +0.006; DE +0.005; VI −0.004; AR +0.004; TR −0.004; SW +0.004; PT +0.003; HE +0.003; ZH −0.002; JA +0.002; HI +0.001; FI +0.001; ID −0.001; PL +0.001.
- Phi‑3 by voice type Δλ₂[2,5]: analytic −0.446; periphrastic +0.011; unknown −0.005; non‑concatenative +0.003; affixal −0.002; particle approximately 0.
- Qwen2.5‑7B (28L): Small but consistent negatives; largest decreases for analytic and non‑concatenative; periphrastic, affixal, particle closer to zero; several CIs overlap zero. Effect sizes small (|g_trim| ≈ 0.2–0.4). Some figures report approximate early-window type means (analytic ~ −0.04; affixal ~ −0.01; particle ~ +0.01; non‑concatenative ~ +0.02; periphrastic ~ 0.00; unknown ~ 0.00).
- LLaMA‑3.2‑1B (16L): Systematic but muted patterns; affixal ≈ −0.030 (consistent negatives); analytic shows modest positives; ordering affixal < periphrastic < analytic appears at smaller scale.
- Tokenizer fragmentation correlations (Table 2): Pieces/character—Phi‑3 −0.44 [−0.72, −0.08]; Qwen 0.51 [0.15, 0.76]; LLaMA 0.29 [−0.17, 0.65]. Fragmentation entropy—Phi‑3 0.36 [0.02, 0.66]; Qwen −0.23 [−0.58, 0.18]; LLaMA −0.25 [−0.62, 0.20].
- Spectral–behavioral correlations (Table 3): Pearson r between Δλ₂[2,5] and behavior delta (negative mean NLL difference). Phi‑3‑Mini: n = 20, r = −0.976, 95% CI [−0.99, −0.89], p < 0.001. Qwen2.5‑7B: n = 20, r = −0.627, [−0.85, −0.24], p < 0.05. LLaMA‑3.2‑1B: n = 20, r = −0.143, [−0.62, 0.41]. Intervention validation on controlled alternations (≤ 2 token drift): predicted Δλ₂ sign appears in 6/7 items for LLaMA and 7/7 for Qwen, with corresponding behavioral shifts.
- Causal validation via attention head ablations (Table 4):
  - LLaMA‑3.2‑1B: L2&L3 H0–7 → Early +0.01472, Mid +0.07566, Late +0.00200, Overall +0.02795; L2 H0–3 → Early −0.00478, Mid −0.00907, Late +0.00270, Overall −0.00319; L3&L4 H0–3 → Early −0.00475, Mid +0.03023, Late +0.02691, Overall +0.01667.
  - Phi‑3‑Mini: L2&L3 H0–7 → Early −0.00736, Mid −0.00714, Late +0.00176, Overall −0.00088; L2 H0–3 → Early −0.00624, Mid +0.00130, Late +0.00170, Overall +0.00054; L3&L4 H0–3 → Early −0.01220, Mid +0.00200, Late +0.00439, Overall +0.00167.
  - Qwen2.5‑7B: L2&L3 H0–7 → Early +0.04538, Mid −0.02696, Late −0.00975, Overall −0.00425; L2 H0–3 → Early +0.00630, Mid −0.00344, Late −0.00266, Overall −0.00133; L3&L4 H0–3 → Early +0.01354, Mid −0.01013, Late −0.01045, Overall −0.00622.
- Beyond voice alternation: Reasoning strategies (phi‑3.5‑mini, 95 transitivity tasks). Strategy → Accuracy, RCI, z-scores (Energy, Entropy, HFER, Fiedler):
  - CoT: 0.695; RCI +1.307; Energy +0.790; Entropy +0.898; HFER −0.744; Fiedler +0.455.
  - Standard: 0.600; RCI +1.738; Energy +0.596; Entropy +0.127; HFER −0.921; Fiedler +1.286.
  - CoD: 0.274; RCI −2.996; Energy −1.708; Entropy −1.664; HFER +1.611; Fiedler −1.429.
  - ToT: 0.253; RCI −0.049; Energy +0.322; Entropy +0.639; HFER +0.054; Fiedler −0.312.
  Limitations: Preliminary; extend to more reasoning types and architectures.
- Safety application (hallucination detection): Final-layer Fiedler threshold detector SHD(x) = 1[z_fid(x) > τ_d] achieved 88.75% accuracy on n = 80 (50 factual, 30 hallucinations). Baselines (Perplexity, SelfCheckGPT-style) performed lower on this set; treated as preliminary.

# Conclusion
- Discussion and limitations: Spectral fingerprints enable computational provenance—inferring developmental pressures from present connectivity imprints. Observed English-specific early-layer disruption in Phi‑3‑Mini aligns with its public English focus, consistent with training emphasis leaving spectral traces. Potential audits for language coverage/brittleness and indirect evaluation of training claims when datasets are proprietary.
- Limitations: Inferences are indirect, effects correlational outside controlled interventions, voice alternation is a focused probe, multiple factors (tokenizer fragmentation, morphology, domain mix, architecture) can produce similar patterns. English-specific signature is consistent evidence, not definitive causal attribution.
- Conclusion: The training-free spectral framework surfaces family-specific signatures tied to functional outcomes. Across 20 languages, early-layer Δλ₂ tracks syntactic reconfiguration, aligns with tokenizer stress in family-specific ways, correlates with behavior, and responds to head ablations. Next steps include expanding beyond voice alternation, analyzing multi-eigenvector structure, preregistered evaluations on public benchmarks, and releasing a reproducible toolkit.

# References
- Cited works span attention visualization and probing (Clark et al., 2019; Rogers et al., 2020; Tenney et al., 2019; Hewitt & Manning, 2019), mechanistic circuits (Elhage et al., 2021; Wang et al., 2022), multilingual transfer (Devlin et al., 2019; Conneau et al., 2020; Zhao et al., 2020; Müller et al., 2023), spectral graph theory and signal processing (Chung, 1997; von Luxburg, 2007; Shuman et al., 2013; Sandryhaila & Moura, 2013), computational fingerprints (Didolkar et al., 2025), and reasoning prompts (Wei et al., 2022; Yao et al., 2024; Xu et al., 2025). BLiMP (Warstadt et al., 2020) and other linguistic benchmarks are referenced for behavioral contexts.

# Appendix
- A. Metric selection rationale: Theoretical and empirical justification for λ₂ (algebraic connectivity) as primary diagnostic; voice alternations produce reliable λ₂ differences; λ₂ relates to robustness of graph connectivity and bottlenecks.
- B. Normalization, directionality, aggregation:
  - Random-walk vs. symmetric Laplacians: Signs and peak-layer timing for Δλ₂^(ℓ) coincide; magnitudes shift slightly within bootstrap bands; high correlation between Δλ₂[2,5] across normalizations.
  - Directed attention graphs: Left random-walk on directed graphs and magnetic Laplacian (θ = 0.2) preserve sign and early-window peak; magnitudes differ within ~15%.
  - Head aggregation schemes: Uniform vs. mass-weighted agree on signs/timing; learned α^(ℓ) yields smoother trajectories; mass-weighted used by default.
  - HFER cutoff sweep: Directional conclusions unchanged across cutoffs (10–40% mass); early-window averages shift by < 15%; adjacent windows (1–4, 3–6) preserve signs and peak locations.
  - Numerical details: Eigenpairs via ARPACK (tolerance 1e−6, maxit 10^4); bootstrap CIs (2,000 resamples); permutation tests (10,000 shuffles); BH–FDR q = 0.05. ΔEnergy analyses confirm connectivity focus.
- C. Extended statistical validation (expanded sample size):
  - Dataset: Six languages (EN, DE, ES, FR, AR, TR), 50 paraphrases per voice, standard methodology.
  - Phi‑3‑Mini: English effect replicates (Δλ₂[2,5] = −0.444, p = 0.008, bootstrap 95% CI); other languages near zero.
  - Qwen2.5‑7B: Early-window means range −0.067 to +0.016; confirms small distributed effects.
  - LLaMA‑3.2‑1B: Systematic moderate negatives (range −0.044 to −0.007).
  - Statistical robustness: Patterns match n=10 results; tighter CIs; supports computational fingerprints rather than small-sample artifacts.
- D. Ablations and robustness:
  - Cutoffs: HFER 10–30% stable; effect magnitudes vary ≤ 15%.
  - Normalization/aggregation variants (Table 6, means across languages ± bootstrap SE): Default L_rw + mass-weighted—Qwen +0.019 ± 0.006; Phi‑3 −0.112 ± 0.031; LLaMA −0.012 ± 0.005. L_sym + mass-weighted—Qwen +0.018 ± 0.006; Phi‑3 −0.105 ± 0.029; LLaMA −0.013 ± 0.005. L_rw + uniform—Qwen +0.017 ± 0.006; Phi‑3 −0.109 ± 0.030; LLaMA −0.011 ± 0.005. Directed L_→—Qwen +0.017 ± 0.007; Phi‑3 −0.101 ± 0.033; LLaMA −0.010 ± 0.006. Magnetic L_mag (θ = 0.2)—Qwen +0.018 ± 0.006; Phi‑3 −0.107 ± 0.031; LLaMA −0.011 ± 0.005.
  - Winsorization/trimming stabilize effect sizes; multiple seeds (3) yield overlapping 95% bootstrap CIs; seed variance ≪ prompt variance; conclusions unchanged.
  - Scope notes: Causal claims limited to attention structure→spectral effects; inference centered on language-type and family aggregates; early window prespecified and sign-stable in adjacent windows; sanity controls (tense/number, length matching) do not reproduce voice signatures.
- E. Metrics and validation details:
  - Primary endpoint: Δλ₂[2,5]; default normalization L_rw = I − D̄⁻¹ W̄; directed and L_sym variants reported.
  - Sample size/power: 10 paraphrases per voice adequate for CIs; per-language hypothesis power limited; hierarchical inference strategy using type/family aggregates; permutation tests with BH–FDR; bootstrap CIs.
  - Effect sizes: Trimmed Hedges’ g with practical Δ_sym percentage change thresholds (small ≈ 25%, medium ≈ 50%, large ≥ 100%).
  - Behavioral validation: Negative mean NLL; controlled pairs with ≤ 2 token drift; methodology for computing scores and deltas.
- F. RCI definition: RCI = (z_Entropy + z_Fiedler) − (z_Energy + z_HFER). Higher RCI indicates stronger low-frequency connectivity and dispersion, penalizing high smoothness and high-frequency mass.
- G. Statement on LLM usage: Assistance used for grammar, clarity, and brainstorming; core concepts, design, analysis, and conclusions are the authors’.
- Ethics Statement: Scope and intended use (auditing and scientific understanding); no personal data; caution against misinterpretation; fairness considerations; dual-use concerns mitigated.
- H. Reproducibility and code release: Anonymized repository and dataset links; seeds and scripts provided; release plan with permissive license and guidance.
- I. Supplementary material: Anonymized artifact link for concurrent manuscript.