1) Summary
This paper introduces a training-free framework based on graph signal processing (GSP) to analyze how different transformer architectures implement syntactic computations. The method constructs dynamic graphs from attention weights and uses the Fiedler value (algebraic connectivity, λ₂) as a diagnostic. The primary endpoint is the change in λ₂ (Δλ₂) in early layers (2–5) when processing active versus passive voice alternations. The study analyzes three model families across 20 languages, revealing distinct "spectral fingerprints." For instance, Phi-3-Mini shows a large, English-specific connectivity disruption, while Qwen2.5-7B exhibits smaller, distributed shifts. These spectral signatures are shown to be functionally relevant, correlating strongly with behavioral performance (NLL changes) and being causally influenced by targeted attention head ablations. The framework is also preliminarily extended to differentiate reasoning strategies.2) Strengths
*   **Novel and Well-Motivated Methodological Framework**
    *   The application of graph signal processing, and specifically the Fiedler value (λ₂), to probe the computational mechanisms of transformers is a novel and insightful approach (Section 3.1). It provides a compact, interpretable, and theoretically grounded metric for analyzing complex attention patterns.
    *   The framework is training-free, making it a lightweight and widely applicable diagnostic tool for auditing pretrained models without needing access to training data or requiring expensive fine-tuning (Abstract, Section 1).
    *   The choice of the Fiedler value is well-justified both theoretically as a measure of graph connectivity (Appendix A.1) and empirically as the most sensitive diagnostic for the chosen task compared to alternatives like Dirichlet energy or spectral entropy (Section 3.2, Appendix A.2, Figure 5).*   **Comprehensive and Rigorous Experimental Design**
    *   The study's breadth is a significant strength, covering three distinct model families (Phi-3-Mini, Qwen2.5-7B, LLaMA-3.2-1B) and 20 languages with diverse typological properties (Section 4). This allows for robust cross-architectural and cross-linguistic comparisons.
    *   The use of a controlled linguistic probe—voice alternation—is a methodologically sound choice, as it involves systematic syntactic reconfiguration and allows for clear theoretical predictions about attention changes (Section 1, Section 3.2).
    *   The multi-faceted validation strategy strengthens the paper's claims by combining observational analysis (Section 6.1), correlation with external variables (tokenizer stress in Section 6.2, behavior in Section 6.3), and causal interventions (head ablations in Section 6.4).*   **Strong Empirical Findings with Clear Functional Grounding**
    *   The paper uncovers clear, model-specific "spectral fingerprints." The finding that Phi-3-Mini exhibits a massive, English-specific disruption (Δλ₂ ≈ -0.446) is a striking result that aligns with the model's documented focus on English (Section 6.1, Figure 2).
    *   The functional relevance of the spectral metric is convincingly established through strong correlations with behavioral performance. The near-perfect correlation for Phi-3-Mini (Pearson r = -0.976, Table 3) provides compelling evidence that Δλ₂ captures computationally meaningful processing differences.
    *   The causal validation via head ablations (Section 6.4, Table 4) confirms that the observed spectral signatures are mechanistically tied to the function of early-layer attention heads, rather than being mere statistical artifacts. The different responses to ablation across models (e.g., Qwen's compensatory reaction) further highlight architectural differences.*   **Commendable Attention to Statistical Detail**
    *   The authors employ a sophisticated statistical toolkit, including nonparametric bootstrap for confidence intervals, paired permutation tests for significance, and Benjamini-Hochberg for FDR control, which demonstrates a commitment to rigorous validation (Section 6.1, Appendix E).
    *   The paper includes a commendable set of robustness checks for methodological choices like Laplacian normalization, graph directionality, and head aggregation schemes (Appendix B, Table 6).
    *   Potential confounds, such as tokenizer fragmentation, are actively investigated (Section 5, Table 2), reflecting a thoughtful approach to the analysis. The inclusion of an expanded sample size validation (Appendix C) is also a positive step, though its results raise some questions (see Weaknesses).3) Weaknesses
*   **Clarity and Consistency of Figures and Tables**
    *   There is redundancy and inconsistency in the presentation of data. For example, Table 1 and Table 2 both report a Pearson correlation between |Δλ₂| and "pieces per character," creating confusion. Table 1 appears in Section 5, while the more detailed Table 2 is in Section 6.2.
    *   The format of figures in the main text is inconsistent and sometimes difficult to parse. Figure 2 presents results as a bar chart, a text-based table, and another bar chart, while Figure 3 uses line plots and a bar chart. This makes direct visual comparison of the core results across the three models less intuitive.
    *   Several figures embed crucial quantitative results as text tables within the figure block or caption (e.g., the per-language values in Figure 2a, the group means in Figure 3), which is an unconventional format that hinders readability and proper referencing. The actual plots are often small and lack detail (e.g., the line plots in Figure 3).*   **Limited Scope and Justification of Generalization and Causal Claims**
    *   The causal intervention experiments are insightful but lack justification for the specific heads chosen for ablation (e.g., "L2 & L3 H0–7" in Table 4). Without this context, it is unclear if these heads are particularly important for the task or were chosen arbitrarily as examples, which limits the depth of the causal conclusion.
    *   The generalization of the framework to reasoning strategies (Section 7.1) is based on a single model (phi-3.5-mini) and one task (transitivity). Claiming a general "performance-connectivity relationship" based on this limited experiment (Table 5) may be an overstatement, especially since the "Standard" strategy has lower accuracy but a higher Fiedler z-score than "CoT".
    *   The application to hallucination detection (Section 7.3) feels underdeveloped. It relies on a small dataset (n=80) and cites a concurrent anonymous manuscript, making it difficult to evaluate. While interesting, its inclusion as a main section feels premature and slightly distracts from the paper's core contribution.*   **Ambiguity in Key Experimental Details**
    *   The methodology for generating the sentence stimuli is not sufficiently detailed. The paper states it uses "(at least) 10 paraphrases per voice" for 20 languages (Section 4, Section 6.1), but the origin of these paraphrases (e.g., human-authored, template-based, machine-translated) and any quality control process are not described. This information is critical for assessing the validity of the cross-linguistic comparisons.
    *   The mapping of the 20 languages to the "voice type" categories (e.g., analytic, periphrastic) used for aggregated analysis is not explicitly provided in a table. While some counts are given in figure legends (e.g., Figure 3), a clear, comprehensive list is missing, making it difficult to fully scrutinize the group-level results (e.g., Figure 2b).
    *   The Reconfiguration Change Index (RCI) is introduced and used in Table 5, but its definition is deferred entirely to Appendix F. A brief, intuitive explanation of its components in the main text (Section 7.1) would improve the self-containedness of the reasoning analysis.*   **Internal Inconsistencies and Contradictory Reporting**
    *   The expanded statistical validation in Appendix C presents results that appear to contradict the main findings. For LLaMA-3.2-1B, the main text reports a positive Δλ₂ for analytic languages (Figure 4), while the expanded analysis reports a negative value (Figure 12). Similarly, for Qwen2.5-7B, the main text shows mixed positive and negative effects by voice type (Figure 3), whereas the appendix shows uniformly negative effects for all reported types (Figure 10). This undermines the claim that the appendix "confirm[s] the robustness of our main findings" (Appendix C).
    *   The manuscript provides conflicting information regarding the sample size. Sections 4 and 6.1 state "at least 10 paraphrases," and Appendix E.1 specifies "10 paraphrases," but Appendix D refers to an analysis with "three paraphrases per voice per language." This ambiguity makes it difficult to assess the statistical power and reproducibility of the primary results.
    *   The reporting of tokenizer stress correlations is inconsistent. Table 1 presents both Pearson and Spearman correlations, which show conflicting trends for Qwen2.5-7B (r=0.51 vs ρ=-0.11). However, the more detailed Table 2 and the discussion in Section 5 and 6.2 omit the Spearman correlations, potentially presenting an incomplete picture of the relationship.
    *   There are numerical discrepancies within figures. In Figure 2, the data table for Phi-3-Mini reports a positive Δλ₂ of +0.011 for the "periphrastic" voice type, but the corresponding bar in the chart appears negative, and a manual average of the per-language data for this type also yields a negative value.4) Suggestions for Improvement
*   **Standardize and Improve Data Visualization**
    *   Consolidate the redundant information from Table 1 into the more comprehensive Table 2 and present it once in the main results section to avoid confusion.
    *   Redesign the main results figures (Figures 2, 3, 4) to use a consistent and standardized format. For example, a multi-panel figure with one row per model, each containing a bar chart of per-language Δλ₂, would greatly facilitate cross-model comparison.
    *   Convert the text-based tables currently embedded in figure captions or blocks into properly formatted and numbered tables within the main text or appendix. This would make the quantitative results easier to read, cite, and interpret.*   **Refine the Framing of Generalization and Causal Claims**
    *   In Section 6.4, add a sentence or two explaining the rationale for selecting the specific sets of attention heads for ablation. For instance, clarify if they were chosen based on prior work identifying their function, pilot experiments, or simply as illustrative probes.
    *   In Section 7.1, explicitly frame the reasoning experiment as a preliminary case study. Acknowledge the limitations (one model, one task) and moderate the claims, suggesting this as a promising direction for future work to verify the spectral-performance link in more complex cognitive tasks.
    *   Consider moving the hallucination detection section (Section 7.3) to an appendix or reframing it as a brief "Future Application" subsection within the discussion. This would strengthen the paper's focus on the well-supported analysis of syntactic processing.*   **Provide Greater Clarity on Experimental Setup**
    *   Add a dedicated subsection in the appendix that details the data generation process. Please specify how the paraphrases for the 20 languages were created and what steps were taken to control for length and other potential confounds.
    *   Include a table in the appendix that explicitly lists all 20 languages studied and maps each one to its designated "voice type" category. This will make the aggregated analyses in figures like Figure 2b and Figure 3 fully transparent and reproducible.
    *   In Section 7.1, when the RCI metric is introduced, add a brief parenthetical explanation of its construction (e.g., "a composite score rewarding Fiedler connectivity and entropy while penalizing energy and high-frequency signals") before directing the reader to Appendix F for the full formula.*   **Resolve Internal Inconsistencies and Clarify Reporting**
    *   Please reconcile the conflicting results between the main text (Figures 3, 4) and the expanded validation in Appendix C (Figures 10, 12). The authors should either correct the data, explain the source of the discrepancy, or revise the claim that the appendix confirms the main findings.
    *   Clarify the exact number of paraphrases used for the main experiments. The conflicting statements in Section 4, Appendix D, and Appendix E.1 should be resolved to state a single, consistent sample size.
    *   Provide a more complete analysis of the tokenizer stress correlations. The authors should either include and discuss the Spearman correlations from Table 1 in the main results section or provide a clear justification for relying solely on the Pearson correlation.
    *   Please double-check and correct the numerical values presented in figures and their associated data tables, particularly for the "periphrastic" voice type in Figure 2.5) Score
- Overall (10): 6 — The paper presents a highly novel framework, but significant internal inconsistencies in the reported results undermine the reliability of its conclusions.
- Novelty (10): 9 — The application of GSP and the Fiedler value to create training-free "spectral fingerprints" of architectural behavior is a significant and original contribution to interpretability.
- Technical Quality (10): 5 — The methodology is promising, but the execution is marred by contradictory results between the main text and appendix (Figures 3/4 vs 10/12), ambiguous reporting of key experimental details like sample size, and numerical errors.
- Clarity (10): 6 — While core concepts are well-explained (Section 3), the presentation is hindered by inconsistent figure formatting (Figure 2, 3) and internal contradictions that make the results difficult to trust.
- Confidence (5): 5 — I am highly confident in my assessment; the paper provides extensive evidence and detailed appendices that allow for a thorough evaluation, which revealed several critical inconsistencies.