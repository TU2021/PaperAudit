# Global Summary
This paper introduces a training-free method to analyze transformer models using graph signal processing (GSP). The core idea is to treat attention weights as dynamic graphs over tokens and track changes in their algebraic connectivity, measured by the Fiedler value (λ₂), during syntactic transformations. The primary endpoint is the change in λ₂ (Δλ₂) in an early-layer window (layers 2-5) when processing active vs. passive voice sentences. The method is applied to three model families (Phi-3-Mini, Qwen2.5-7B, LLaMA-3.2-1B) across 20 languages.

Key findings reveal distinct "computational fingerprints" for each model family. Phi-3-Mini shows a dramatic, English-specific disruption (Δλ₂ ≈ -0.446), while its response in 19 other languages is minimal, consistent with its documented English focus. Qwen2.5-7B exhibits small, distributed shifts, and LLaMA-3.2-1B shows systematic but muted responses. These spectral signatures are shown to be functionally relevant, correlating strongly with behavioral performance (e.g., Pearson's r = -0.976 for Phi-3-Mini) and being causally linked to early-layer attention structure via head ablation experiments. The framework is also shown to generalize beyond voice alternation to distinguish reasoning strategies and has a preliminary application in hallucination detection. The authors propose this as a lightweight diagnostic tool for auditing model provenance, specialization, and brittleness.

# Abstract
The paper proposes using spectral analysis of attention-induced token graphs to find "computational fingerprints" in transformers. The method tracks changes in the Fiedler value (algebraic connectivity, Δλ₂) during active-to-passive voice transformations across 20 languages and three model families (Phi-3-Mini, Qwen2.5-7B, LLaMA-3.2-1B). A prespecified early window of layers 2-5 is used for analysis.

Key findings include:
- Phi-3-Mini exhibits a large, English-specific disruption in early layers (Δλ₂[2,5] ≈ -0.446), with minimal effects in other languages, aligning with its public positioning as an English-focused model.
- Qwen2.5-7B shows small, distributed shifts, largest for morphologically rich languages.
- LLaMA-3.2-1B displays systematic but muted responses.
- These spectral signatures correlate strongly with behavioral differences (for Phi-3, r = -0.976).
- Targeted attention head ablations confirm the effect is linked to early attention structure.
- The framework is presented as a training-free diagnostic for architectural biases and can differentiate reasoning modes.

# Introduction
The paper addresses the challenge of understanding *how* transformers perform syntactic computations, moving beyond simply identifying *what* linguistic information they encode. It proposes a method based on graph signal processing (GSP), where attention patterns form dynamic graphs over tokens. The primary metric is the Fiedler value (λ₂), a measure of algebraic connectivity, which is tracked across layers.

- The core approach is to measure the change in λ₂ (Δλ₂) when a model processes active vs. passive voice sentences.
- A prespecified early window (layers 2–5) is used to define the primary endpoint, denoted as Δλ₂[2,5].
- Voice alternation is chosen as a computational probe because it requires systematic attention reconfiguration.
- The study design emphasizes comparability through controlled stimuli, statistical rigor (bootstrap, permutation tests, FDR control), and analysis of potential confounds like tokenizer stress.
- Contributions are: (1) a spectral framework with Δλ₂ as a compact endpoint; (2) uncertainty-aware statistics; (3) discovery of family-specific spectral signatures; (4) robustness checks; (5) establishing functional relevance via correlations and ablations; (6) preliminary generalization to reasoning strategies.

# Related Work
This work is positioned relative to several areas of transformer interpretability.
- It is presented as a complement to attention visualization, probing, and circuit-level analysis, offering a scalable, training-free method to track the dynamics of a specific linguistic operation.
- It builds on graph-theoretic approaches but uses GSP as a diagnostic lens, focusing on algebraic connectivity during syntactic transformations. It emphasizes robustness to choices like Laplacian normalization.
- In the context of multilingual models, the methodology is designed to separate universal tendencies from model-family idiosyncrasies by fixing the linguistic task and comparing responses across architectures and languages.
- The work aligns with the concept of "computational fingerprints" (Didolkar et al., 2024), proposing a layer-resolved method to reveal such signatures and link them to behavior and interventions with quantified uncertainty.

# Method
The method treats transformer layers as a graph signal processing system.
- **Graph Construction**: For each layer, post-softmax attention matrices (A) are symmetrized and aggregated across heads to form a weighted adjacency matrix (W). The default is mass-weighted head aggregation. A combinatorial Laplacian (L = D - W) is then computed.
- **Spectral Diagnostics**: Four diagnostics are considered: Dirichlet energy, spectral entropy, high-frequency energy ratio (HFER), and the Fiedler value (λ₂), which is the second-smallest eigenvalue of L and measures algebraic connectivity.
- **Primary Endpoint**: The main metric is Δλ₂(ℓ) = λ₂(pass) - λ₂(act) for each layer ℓ. The primary endpoint is the mean over a prespecified early window: Δλ₂[2,5]. λ₂ was chosen for its superior sensitivity to voice alternations.
- **Models and Languages**: The analysis covers three models: Qwen2.5-7B (28L), Phi-3-Mini (32L), and LLaMA-3.2-1B (16L). It uses data from 20 languages, with at least 10 paraphrases per voice, grouped by voice type (analytic, periphrastic, etc.).
- **Tokenizer Stress Analysis**: The paper investigates the relationship between spectral changes and tokenization. Metrics include tokens per character (φ) and fragmentation entropy (H_frag). It finds opposing correlations between |Δλ₂[2,5]| and φ for Qwen2.5-7B (Pearson r = 0.51) and Phi-3-Mini (r = -0.44). LLaMA-3.2-1B shows a correlation (r = 0.51) between spectral effects and the *difference* in tokenization between active/passive pairs.

# Experiments
This section presents the main results of the study.
- **Early-window Δλ₂[2,5]**:
    - **Phi-3-Mini**: Shows a large negative effect for English (Δλ₂[2,5] ≈ -0.446), which is an outlier. All other 19 languages show minimal effects (|Δλ₂[2,5]| ≲ 0.02). French is second largest at ≈ -0.134.
    - **Qwen2.5-7B**: Displays small but consistent negative shifts across voice types, with analytic and non-concatenative types showing the largest decreases.
    - **LLaMA-3.2-1B**: Exhibits systematic patterns at a smaller magnitude, with affixal languages showing negative shifts (≈ -0.030) and analytic types showing modest positive effects.
- **Tokenizer Fragmentation Correlations**: Table 2 shows model-specific correlations between |Δλ₂[2,5]| and tokenizer metrics.
    - Phi-3-Mini: Negative correlation with pieces/character (r = -0.44) and positive with fragmentation entropy (r = 0.36).
    - Qwen2.5-7B: Positive correlation with pieces/character (r = 0.51) and negative with fragmentation entropy (r = -0.23).
    - LLaMA-3.2-1B: Weaker positive correlation with pieces/character (r = 0.29).
- **Spectral-Behavioral Correlations**: The change in spectral connectivity (Δλ₂) is correlated with behavioral performance (change in negative mean log-likelihood, NLL).
    - Phi-3-Mini: Very strong negative correlation (Pearson's r = -0.976, p < 0.001).
    - Qwen2.5-7B: Moderately strong negative correlation (r = -0.627, p < 0.05).
    - LLaMA-3.2-1B: Minimal correlation (r = -0.143).
- **Causal Validation (Head Ablations)**: Ablating early-layer attention heads reveals family-specific causal profiles.
    - LLaMA-3.2-1B: Shows sustained positive shifts in Δλ₂, suggesting disruption propagates.
    - Phi-3-Mini: Shows only small effects, suggesting robustness or redundancy.
    - Qwen2.5-7B: Exhibits a compensatory pattern with early positive responses followed by mid/late negative responses, suggesting active redistribution of computation.
- **Generalization**:
    - **Reasoning Strategies**: The framework distinguishes prompting strategies (Standard, CoT, CoD, ToT) on transitivity tasks. CoT (69.5% accuracy) and Standard (60.0%) show positive Fiedler shifts, while CoD and ToT show negative shifts and lower accuracy.
    - **Hallucination Detection**: A simple detector based on thresholding the final-layer Fiedler value achieved 88.75% accuracy on a small held-out set (n=80).

# Conclusion
The paper concludes that the proposed training-free spectral framework can reveal family-specific "computational fingerprints" in transformers. The method successfully links these spectral patterns to functional outcomes like behavioral performance and tokenizer sensitivity. The authors suggest the framework can be used for "computational provenance," inferring aspects of a model's training from its connectivity patterns, as exemplified by the English-specific signature in Phi-3-Mini. This could support audits for language coverage and brittleness.

Limitations are acknowledged: inferences about training are indirect, voice alternation is not an exhaustive probe, most findings are correlational outside of the ablation experiments, and multiple factors can produce similar spectral patterns. Future work will extend the analysis beyond voice alternation and release a reproducible toolkit.

# References
This section contains a list of academic references cited throughout the manuscript.

# Appendix
The appendix provides extensive details on methodology, robustness checks, and statistical validation.
- **A. Metric Selection Rationale**: Justifies choosing the Fiedler value (λ₂) based on its theoretical grounding in graph connectivity and its empirical superiority in discriminating syntactic conditions compared to other spectral metrics.
- **B. Normalization, Directionality, and Aggregation**: Reports that the main findings are robust to different choices of Laplacian normalization (random-walk vs. symmetric), graph construction (undirected vs. directed), and head aggregation (mass-weighted vs. uniform).
- **C. Extended Statistical Validation**: Confirms the main findings with an expanded sample size (n=50 paraphrases) for 6 key languages. The Phi-3-Mini English-specific effect is replicated (Δλ₂[2,5] = -0.444).
- **D. Ablations and Robustness**: Details robustness to HFER cutoffs, multiple seeds, and trimming/winsorization. Clarifies the scope of causal claims and inference.
- **E. Metric and Robustness**: Provides details on numerical computation (ARPACK), statistical procedures (bootstrap, permutation tests, FDR), effect size reporting (Hedges' g), and behavioral validation methodology (NLL).
- **F. RCI Metric Definition**: Defines the Reconfiguration Change Index (RCI) used in the reasoning strategy experiment as a z-scored combination of the four spectral diagnostics.
- **G. Statement on LLM Usage**: Acknowledges the use of LLMs for grammar, code refinement, and brainstorming.
- **H. Reproducibility and Code Release**: Provides links to an anonymized code repository and dataset.
- **I. Supplementary Material**: Provides a link to an anonymized concurrent manuscript on hallucination detection.
- The appendix also includes numerous figures and tables supporting the robustness checks and expanded validation experiments.