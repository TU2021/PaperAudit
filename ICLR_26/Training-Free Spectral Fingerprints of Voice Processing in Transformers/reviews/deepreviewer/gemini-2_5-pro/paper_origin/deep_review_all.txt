### Review 1

**Summary**
This paper introduces a novel, training-free framework for analyzing transformer internals by applying graph signal processing (GSP) to attention-induced token graphs. The authors propose using the Fiedler value (λ₂), a measure of algebraic connectivity, as a compact diagnostic to track how models reconfigure attention during syntactic transformations. By comparing active and passive voice sentences across 20 languages and three model families (Phi-3, Qwen2, LLaMA-3), they uncover distinct "spectral fingerprints," most notably a dramatic, English-specific connectivity disruption in Phi-3-Mini's early layers. These spectral signatures are shown to correlate strongly with behavioral performance and are causally linked to early-layer attention heads through ablation studies.

**Soundness**
The methodology is sound and well-motivated. Grounding the analysis in established spectral graph theory and using the Fiedler value as a principled measure of graph connectivity is a strong choice. The experimental design is comprehensive, leveraging a controlled linguistic phenomenon (voice alternation) across a diverse set of models and languages. The statistical analysis is rigorous, employing bootstrapping, permutation tests, and FDR correction to ensure the reliability of the findings. The authors are commendably thorough in the appendix, providing extensive robustness checks for normalization choices, head aggregation schemes, and even expanding the sample size (Appendix C) to confirm the stability of their results. The causal claims are appropriately scoped, supported by targeted head ablation experiments (Section 6.4).

**Presentation**
The paper is generally well-written and the core ideas are communicated with clarity. Figure 1 provides an excellent conceptual overview of the GSP framework. The main finding regarding Phi-3 is presented with compelling clarity in Figure 2, making the paper's central claim immediately understandable. The extensive appendices are a major asset, proactively addressing potential questions and demonstrating the robustness of the work.

**Contribution**
The contribution is significant and novel. The paper introduces a genuinely new perspective for transformer interpretability, moving beyond standard probing or attention visualization. The concept of "spectral fingerprints" as a lightweight, training-free diagnostic for uncovering architectural biases and potential brittleness is a powerful one. The demonstration that this method can surface a known, documented model characteristic (Phi-3's English focus) without access to training data (Section 7.2) is a compelling proof-of-concept for its utility in model auditing. This work opens up a promising new direction for scalable model analysis.

**Strengths**
1.  **Novel and Principled Framework:** The application of GSP, and specifically the Fiedler value (Δλ₂), to attention graphs is a highly novel and theoretically grounded approach to interpretability.
2.  **Striking Empirical Results:** The finding of a massive, English-specific spectral disruption in Phi-3-Mini (Δλ₂ ≈ -0.446, Figure 2) is a clear and powerful result that validates the entire approach.
3.  **Strong Functional Grounding:** The paper successfully links the abstract spectral signatures to concrete functional outcomes through strong spectral-behavioral correlations (r = -0.976 for Phi-3, Table 3) and causal head ablation experiments (Table 4).
4.  **Methodological Rigor:** The work is characterized by excellent methodological and statistical practices, including prespecifying an analysis window, comprehensive robustness checks in the appendix (Appendix B, D), and appropriate use of uncertainty quantification.

**Weaknesses**
1.  **Cluttered Figures:** Some figures, particularly the multi-line plots like Figure 3, are cluttered and difficult to parse due to overlapping lines and small legends. A clearer presentation would strengthen the paper's impact.
2.  **Preliminary Generalization:** The section on generalizing beyond voice alternation (Section 7) feels somewhat preliminary compared to the depth of the main analysis. While promising, the results on reasoning and hallucination detection could be framed more cautiously as avenues for future work.

**Questions**
1.  The compensatory effect observed in Qwen2.5-7B during head ablations (Table 4) is fascinating. Do you have any hypotheses about the specific mechanisms (e.g., later heads taking over specific roles) that enable this resilience, and could your spectral framework be used to identify these compensatory pathways?
2.  You focus on the mean Δλ₂ in a pre-specified window. Have you considered the trajectory or variance of λ₂ across layers as another potential source for fingerprints? For instance, does a model that shows high variance in λ₂ from layer to layer exhibit different properties than one with a smooth trajectory?

**Rating**
- Overall (10): 9 — This is a strong, novel paper with compelling results and rigorous methodology that opens a new avenue for interpretability research.
- Novelty (10): 10 — The use of spectral graph theory, specifically the Fiedler value, as a training-free diagnostic for transformer computation is highly original.
- Technical Quality (10): 9 — The technical execution is excellent, with strong statistical analysis and thorough robustness checks (Appendices B, C, D), though some figure presentation could be improved.
- Clarity (10): 8 — The core ideas are clear, but the organization of some sections (e.g., Sec 5 vs 6.2) and the clutter in some figures (Fig 3) slightly detract from overall clarity.
- Confidence (5): 5 — I am highly confident in my evaluation, as my expertise aligns well with the paper's methods and claims.

---
### Review 2

**Summary**
This paper proposes a method for analyzing transformer models by treating attention matrices as graphs and computing their spectral properties. The primary metric is the change in the Fiedler eigenvalue (Δλ₂) of the graph Laplacian when comparing active and passive voice sentences. The authors apply this method to three model families across 20 languages, finding model-specific "spectral fingerprints." They report a large effect in Phi-3-Mini for English, correlate these spectral changes with behavioral metrics and tokenizer properties, and perform head ablations to support causality.

**Soundness**
The paper's methodological soundness has both strong and weak points. On the positive side, the use of established statistical techniques like bootstrapping, permutation tests, and FDR correction is commendable (Section 6.1, Appendix E). The decision to prespecify an analysis window (layers 2-5) is good scientific practice. The extensive robustness checks in the appendix regarding Laplacian normalization and head aggregation (Appendix B) are thorough and appreciated.

However, several points raise concern. First, the initial sample size of "at least 10 paraphrases" (Section 4) is small for drawing robust conclusions, especially for per-language claims. While Appendix C commendably adds an n=50 validation for 6 languages, this feels more like a post-hoc patch than a feature of the original design, and the main body of results still relies on the smaller sample. Second, the interpretation of the tokenizer stress analysis (Section 5) is ambiguous. While the authors frame the correlations as another "fingerprint," it is not clear that they have successfully ruled it out as a significant confound. The opposing correlations are interesting but lack a deep mechanistic explanation, making them hard to interpret. Finally, the causal claims should be tempered. The head ablations (Section 6.4) show that early heads *influence* the spectral signature, but the broader claim linking the signature to "training emphasis" (Section 8) is an abductive inference, not a direct causal conclusion.

**Presentation**
The presentation is a significant weakness. The paper's organization is confusing. For example, results on tokenizer fragmentation are presented in two different places (Table 1 in Section 5 and Table 2 in Section 6.2) with slightly different information, which is disorienting. The figures are often poorly executed. Figure 3 is described in the text (Block 19) but the corresponding images (Blocks 23, 24, 25) are scattered and difficult to connect to the caption. The line plot in Block 23 is nearly unreadable due to visual clutter. Many figures lack clear axis labels or have tiny, illegible legends. The relationship between text, captions, and figures is frequently broken, suggesting a hastily assembled manuscript. For example, the text for Figure 2 (Block 19) describes parts (a) and (b), but the images are separate (Blocks 21, 22) and not labeled as such. This lack of polish severely hinders the reader's ability to verify the claims.

**Contribution**
The core idea of using GSP for interpretability is novel. If the method proves robust, it could offer a valuable, lightweight alternative to more intensive techniques. The finding about Phi-3 is the paper's strongest point and serves as a good proof of concept. However, the overall contribution is undermined by the presentation issues and the somewhat preliminary nature of the generalization claims in Section 7. The reliance on an anonymous concurrent paper (Section 7.3) is also problematic for a self-contained evaluation.

**Strengths**
1.  **Rigorous Statistical Framework:** The paper correctly applies modern statistical methods for uncertainty quantification and hypothesis testing (bootstrapping, permutation tests, FDR control).
2.  **Thorough Robustness Checks:** The appendix contains a comprehensive set of analyses (Appendix B, D) that test the sensitivity of the results to various methodological choices (e.g., Laplacian type, aggregation method), which strengthens confidence in the core findings.
3.  **Clear Motivating Example:** The Phi-3 result for English is a very strong, clear finding that effectively demonstrates the potential of the proposed method.

**Weaknesses**
1.  **Poor Presentation and Organization:** The paper is difficult to follow due to confusing organization (e.g., Sec 5 vs. Sec 6.2), cluttered and poorly labeled figures (e.g., Fig 3), and a disconnect between text, captions, and images.
2.  **Ambiguous Interpretation of Confounds:** The analysis of tokenizer stress (Section 5) is not fully convincing in distinguishing a "fingerprint" from a simple confound that drives the spectral changes.
3.  **Overstated Generalization:** The extensions to reasoning and hallucination detection (Section 7) are underdeveloped and feel tacked on. The RCI metric is introduced abruptly and seems ad-hoc. These claims dilute the focus of the paper.
4.  **Inconsistent Data Reporting:** The presence of two tables (Table 1, Table 2) reporting on the same correlation analysis is confusing and suggests poor editing.

**Questions**
1.  In Section 5, you present correlations between |Δλ₂| and tokenizer fragmentation. Why use the absolute value of Δλ₂ here, when the sign of the change seems functionally important in the rest of your analysis (e.g., the consistent negative shifts in Qwen)? Does this not discard important information?
2.  Could you clarify the discrepancy between Table 1 and Table 2? They appear to report the same Pearson correlations but are presented in different sections with different accompanying information. Why was the paper structured this way?
3.  The choice of λ₂ is justified by its relation to graph connectivity. However, other eigenvalues (or the whole spectrum) also contain structural information. Did you investigate whether other parts of the Laplacian spectrum (e.g., λ₃, or the spectral gap between λ₂ and λ₃) yield similar or complementary fingerprints?

**Rating**
- Overall (10): 6 — A novel idea with a strong core finding, but significant issues with presentation, organization, and the strength of some secondary claims prevent a higher rating.
- Novelty (10): 8 — The application of GSP and the Fiedler value to this problem is novel, though graph-based analyses of attention are not entirely new.
- Technical Quality (10): 7 — The statistical analysis is strong, but the experimental design has some minor weaknesses (initial sample size) and the interpretation of confounds is not fully resolved.
- Clarity (10): 4 — The paper is very difficult to read due to poor organization, confusing figure presentation, and inconsistencies in reporting.
- Confidence (5): 5 — I am highly confident in my assessment, particularly regarding the methodological and presentational shortcomings.

---
### Review 3

**Summary**
This paper introduces a "spectral fingerprinting" method to characterize how different transformer architectures implement syntactic operations. The authors model token-level attention as a dynamic graph and track its algebraic connectivity (the Fiedler value, λ₂) across layers. Their primary probe is the active-to-passive voice transformation, which they test on three model families in 20 languages. The key findings are that models exhibit family-specific spectral patterns: Phi-3-Mini shows an extreme, English-specific connectivity drop, Qwen2.5-7B shows diffuse, small changes, and LLaMA-3.2-1B is intermediate. These spectral changes are linked to behavioral performance and causally influenced by early-layer attention heads.

**Soundness**
The work appears methodologically sound. The choice to use a controlled syntactic alternation (active/passive) is a classic and effective strategy in linguistic probing. Framing the analysis with Graph Signal Processing provides a solid theoretical foundation, and the Fiedler value is a well-understood metric for graph connectivity. The statistical approach, including bootstrapping and permutation testing, is appropriate for the claims being made. The authors also do a good job of validating the functional relevance of their metric by correlating it with model behavior (NLL changes, Section 6.3) and performing interventional experiments (head ablations, Section 6.4). This combination of observational, correlational, and causal evidence makes for a convincing argument.

**Presentation**
The paper is reasonably well-written, but its structure could be improved. The core narrative is compelling, especially the story around Phi-3. However, the flow is sometimes interrupted by confusingly placed results (e.g., the tokenizer analysis in Section 5 appearing before the main results in Section 6) and figures that are not well-integrated with the text. For instance, Figure 3 is a composite of several plots and text descriptions that are hard to piece together. While the appendices are commendably detailed, the main text would benefit from a more streamlined and polished presentation of its key figures and tables.

**Contribution**
This paper makes a valuable and novel contribution to the field of transformer interpretability. It carves out a useful niche between high-level probing (which tells you *what* information is present) and low-level circuit analysis (which tells you *how* a specific computation is implemented in detail). This method provides a meso-level view, revealing architectural *strategies* for implementing a computation without getting bogged down in individual neuron activations. The idea of "computational provenance"—inferring model properties from these spectral signatures—is particularly exciting. It suggests a new class of lightweight, training-free auditing tools. The Phi-3 result is a powerful demonstration of this, connecting a low-level computational artifact to a high-level, documented fact about the model's training. This is a significant step towards making interpretability methods useful for real-world model evaluation.

**Strengths**
1.  **Novel Interpretability Lens:** The paper successfully positions GSP as a new tool for understanding transformers, offering a unique perspective on computational strategies that complements existing methods like probing and circuit analysis.
2.  **Connects Computation to Provenance:** The central claim that training emphasis leaves detectable "spectral imprints" is a powerful and potentially very useful idea for model auditing and governance. The Phi-3 finding is a stellar example.
3.  **Multi-faceted Validation:** The work is strengthened by its three-pronged validation approach: finding model-specific observational patterns, showing strong correlations with behavior, and demonstrating causality via interventions.
4.  **Scalability:** The proposed method is training-free and computationally lightweight compared to circuit discovery, suggesting it could be scaled for broad audits across many models and tasks.

**Weaknesses**
1.  **Limited Scope of Probe:** The analysis relies entirely on the active/passive alternation. While this is a clean probe, it's a single syntactic phenomenon. It's unclear how well these "fingerprints" would generalize to other syntactic transformations (e.g., question formation, negation) or semantic tasks.
2.  **Underdeveloped Generalization Section:** Section 7, which attempts to show generalization to reasoning and safety, feels rushed. The connection between the spectral signatures found in the voice task and those in reasoning tasks is not clearly established. This section weakens the paper's focus.
3.  **Clarity of Figures:** Several key figures are difficult to interpret due to clutter and poor labeling, which detracts from the paper's overall impact. A professional redrawing of figures like Figure 3 is needed.

**Questions**
1.  You frame this work as a complement to circuit analysis. Could you elaborate on how these two approaches might be combined? For example, could a large Δλ₂ signature be used as a guide to pinpoint which layers or heads are most critical, thereby focusing the search for a specific circuit?
2.  The paper focuses on the Fiedler value (λ₂). In spectral clustering, the eigenvector corresponding to λ₂ (the Fiedler vector) is used to partition the graph. Have you analyzed the Fiedler vectors in your experiments? They might reveal *which* tokens are being separated or clustered during the syntactic transformation, providing a more mechanistic story than the scalar λ₂ value alone.
3.  How sensitive are the spectral fingerprints to the specific templates used for generating the active/passive sentences? If you were to use more complex sentences with subordinate clauses, would you expect the same architectural signatures to hold?

**Rating**
- Overall (10): 8 — A very strong paper with a novel and significant contribution to interpretability, held back slightly by presentation issues and a weak generalization section.
- Novelty (10): 9 — The framework is highly novel and provides a new conceptual lens for analyzing transformer computations.
- Technical Quality (10): 8 — The core analysis is technically excellent, but the preliminary nature of the generalization experiments and some presentation flaws are minor drawbacks.
- Clarity (10): 7 — The main ideas are clear, but the paper's structure and the quality of some figures could be significantly improved.
- Confidence (5): 4 — I am confident in my assessment of the paper's contribution to the interpretability field, though I am not an expert in GSP theory itself.

---
### Review 4

**Summary**
The paper presents a method to "fingerprint" transformer models using spectral analysis of their attention graphs. The core idea is to measure the change in graph connectivity (using the Fiedler value, Δλ₂) when a model processes a sentence and its passive-voice equivalent. The authors test this on three popular models (Phi-3, Qwen2, LLaMA-3) across 20 languages. They find that each model family has a unique spectral response, with Phi-3 showing a particularly large, English-specific effect. The authors argue this method can be used as a lightweight, training-free diagnostic for model auditing, and show preliminary extensions to reasoning tasks and hallucination detection.

**Soundness**
The technical approach seems sound. The use of graph theory to analyze attention is a reasonable idea, and the authors perform a number of statistical checks and robustness analyses in the appendix that build confidence in the results. The correlation with behavioral performance (Section 6.3) and the head ablation study (Section 6.4) are important steps to show that this metric isn't just a mathematical curiosity but is tied to the model's actual processing. The validation on an expanded dataset (Appendix C) is also a strong point, confirming the main findings aren't a small-sample artifact.

**Presentation**
The paper is difficult to read from a practical standpoint. An engineer looking to implement this method would struggle with the current presentation. The organization is confusing, with key results scattered and tables/figures being duplicated or poorly explained (e.g., Table 1 vs. Table 2). The figures are a major problem; many are cluttered (Fig 3), poorly labeled, and not clearly connected to their captions, making it hard to quickly grasp the results. The Mermaid diagram in Block 9 is a nice idea but less informative than the proper Figure 1 in Block 12. The paper needs a thorough pass for clarity and organization before it can be considered a useful guide for practitioners.

**Contribution**
The main contribution is a potentially useful diagnostic tool. The idea of a simple, training-free metric that can reveal model-specific biases (like the English-centricity of Phi-3) is very appealing for practical applications like model selection, red-teaming, or monitoring. The "computational fingerprint" is a good analogy. The paper's value lies in this potential for a practical audit tool. However, the extensions in Section 7 feel like over-selling. The reasoning analysis is on a small set of tasks, and the hallucination detector (Section 7.3) is presented with minimal detail, referencing an anonymous paper and tested on a tiny dataset (n=80). These preliminary claims detract from the more solid, and already interesting, core contribution on syntactic analysis.

**Strengths**
1.  **Practical Potential:** The method is training-free and based on a single, interpretable metric (Δλ₂), making it a potentially valuable and lightweight tool for auditing LLMs.
2.  **Clear Proof of Concept:** The Phi-3 result is a "killer app" for this method. It demonstrates a real, practical use case: detecting potential brittleness or specialization in a model without needing access to its training data.
3.  **Good Grounding in Behavior:** The strong correlation between the spectral metric and a behavioral metric (NLL) in Table 3 is crucial for demonstrating the practical relevance of the findings.
4.  **Reproducibility Efforts:** The authors state they will release code and have provided links (Appendix H), which is essential for a method intended for practical use.

**Weaknesses**
1.  **Unclear Practicality of Generalization:** The claims about using this for reasoning (Section 7.1) and hallucination detection (Section 7.3) are not well-supported. The RCI metric seems complex and ad-hoc, and the hallucination results are too preliminary to be convincing. This makes it hard to judge the true breadth of the tool's utility.
2.  **Poor Presentation for Implementation:** The paper is not structured in a way that makes the method easy to understand or replicate. The confusing figures and disorganized sections are a significant barrier for anyone wanting to apply this technique.
3.  **The "Why" is Missing:** While the paper shows *that* different models have different fingerprints, the practical implications are not fully explored. For example, is the "resilient" signature of Qwen2 (Section 6.4) always better? Does the "muted" LLaMA signature imply anything about its generalization capabilities? The paper identifies patterns but offers limited guidance on how to act on them.

**Questions**
1.  From a practical perspective, how computationally expensive is it to compute the Fiedler value for each layer of a large model on a long sequence? Is this something that can be run "at scale" for continuous monitoring, or is it more of a one-off analysis tool?
2.  The hallucination detector in Section 7.3 uses the *final-layer* Fiedler value, whereas the main analysis focuses on an *early-window* Δλ₂. Why the change in metric and layer window? This seems inconsistent and undermines the idea of a single, unified framework.
3.  For the tokenizer stress analysis (Section 5), you show opposing correlations for different model families. From an engineering perspective, what is the actionable insight here? Does this suggest we should prefer one model over another for languages with high fragmentation, or does it point to a specific type of fine-tuning that might be needed?

**Rating**
- Overall (10): 7 — A promising and potentially practical auditing method with a great proof-of-concept, but the paper is hampered by poor presentation and overstated, preliminary generalization claims.
- Novelty (10): 8 — The specific application of GSP and λ₂ as a diagnostic tool is a novel and clever idea in the applied space.
- Technical Quality (10): 8 — The core experiments are well-executed and validated, but the extensions in Section 7 are weak and the presentation is flawed.
- Clarity (10): 5 — The paper is confusingly organized and the figures are often unreadable, making it difficult for a practitioner to understand and apply the method.
- Confidence (5): 4 — I am confident in my assessment of the paper's practical utility and its presentational weaknesses, based on my experience with applied ML research.