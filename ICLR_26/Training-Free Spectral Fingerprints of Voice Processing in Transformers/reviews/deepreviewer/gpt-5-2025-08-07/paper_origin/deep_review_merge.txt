Summary
The paper introduces a training-free graph signal processing framework to interpret transformer computations by treating attention matrices as dynamic token graphs and hidden states as graph signals. Using Laplacian operators built from attention, the method tracks spectral connectivity via the Fiedler value (λ2) and focuses on the early-layer window (layers 2–5). The primary endpoint is the mean early-window contrast Δλ2 = λ2(passive) − λ2(active) for voice alternation across 20 languages and three model families (Phi‑3‑Mini, Qwen2.5‑7B, LLaMA‑3.2‑1B). The study reports family-specific fingerprints: a large, English-specific disruption in Phi‑3‑Mini (strong negative Δλ2 in early layers), smaller and more distributed negative shifts for Qwen, and muted but systematic effects for LLaMA. Robustness checks cover normalization choices (e.g., Lrw vs Lsym), directed/magnetic Laplacians, and head aggregation variants. The spectral signatures are linked to behavioral differences (negative log-likelihood contrasts), including very high correlations (e.g., r near −0.98 for Phi‑3) with n=20 languages. Targeted early attention head ablations provide causal support for the mechanistic relevance of early connectivity changes. The framework is tentatively extended beyond syntax to reasoning via a composite Reconfiguration Change Index (RCI), and tokenizer-stress covariates are used to probe subword fragmentation effects.

Strengths
- Conceptually clear, theoretically grounded use of spectral graph theory and the Fiedler value as an interpretable measure of global connectivity, with explicit operator definitions and derivations.
- Preregistered focus on an early-layer window (layers 2–5) with matched contrasts, bootstrap confidence intervals, permutation tests, and FDR control providing uncertainty quantification and statistical rigor.
- Robustness analyses spanning normalization schemes, directed/magnetic Laplacians, and head aggregation choices, demonstrating stability of the main qualitative findings.
- Causal relevance supported by targeted attention head ablations that align with family-specific fingerprints, strengthening the link between early attention structure and spectral connectivity changes.
- Multilingual scope across 20 languages and inclusion of tokenizer-stress covariates that help separate subword fragmentation from syntactic sensitivity, improving the cross-lingual interpretability of results.
- Practical, training-free diagnostic that scales across models and languages, offering a middle ground between probing and circuit-level analysis, with preliminary evidence of broader applicability to reasoning strategies.

Weaknesses
- The “mass-weighted” head aggregation may collapse to uniform weighting if attention is strictly row-stochastic post-softmax, making the default aggregation potentially vacuous unless masking/padding materially changes row sums; clearer justification, alternative weighting (e.g., pre-softmax magnitude or variance), or empirical evidence of non-uniformity is needed.
- Sample-size reporting is inconsistent across sections and appendices (e.g., “at least 10 paraphrases,” “three paraphrases,” and expanded n=50 subsets), hindering assessment of statistical power and comparability across figures; standardized n and explicit annotations per analysis would strengthen claims.
- Extremely strong spectral–behavioral correlations (e.g., r ≈ −0.98 with n=20) risk overinterpretation without multivariate controls; potential confounds include tokenization drift, morphology, language-type, sentence length, and prompt variations. Partial correlations or mixed-effects models would increase robustness.
- Narrative statements about Qwen’s “uniformly negative” early shifts are not perfectly aligned with figures showing small magnitudes and confidence intervals that often overlap zero; more cautious language and explicit reporting of the proportion of FDR-significant language-level effects would help.
- Ablation methodology is under-specified (e.g., whether logits or values were zeroed, how residual connections were handled, the exact hooks used), and statistical significance/uncertainty of ablation-induced changes is not consistently reported; replication across seeds and languages is unclear.
- Directed/magnetic Laplacian sensitivity (e.g., dependence on θ, potential sign reversals or notable magnitude changes) is only briefly addressed; more systematic quantification of deviations relative to undirected operators would clarify robustness.
- The large English-specific Phi‑3 effect could be influenced by prompt templates or tokenizer-specific artifacts; stratified analyses by passive subtype (e.g., by-phrase vs agentless, short vs long passives) and tokenization drift bins would help rule out artifacts.
- Several figures are low resolution and visually crowded, making small effects difficult to parse; cross-referencing conventions and effect-size sign conventions occasionally create confusion.
- The reasoning extension via RCI is preliminary, uses a different model (phi‑3.5‑mini), and is described briefly; more methodological detail and broader task coverage are needed before strong conclusions can be drawn.
