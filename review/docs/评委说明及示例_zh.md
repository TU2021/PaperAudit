# **AI科学家挑战赛——评测员指南（2025年11月12日版）**

## **1. 概述**

**AI科学家挑战赛**分为四个赛道：

1. **文献综述** —— 自动化地进行学术调研与综合  
2. **论文问答（Paper QA）** —— 基于一篇研究论文的科学问题回答  
3. **创意生成（Ideation）** —— 生成新颖且可行的研究思路  
4. **论文评审（Paper Review）** —— 对论文进行自动化批判性评审与打分  

与依赖静态测试数据集的传统基准不同，本次竞赛通过 **AI科学家挑战赛竞技场（Arena）** 进行动态评估。该交互式平台旨在在真实场景下评估模型的表现。**竞技场平台**具备以下特性：

- **双盲成对评估**：每位评测员首先通过竞技场界面**输入一个自定义查询**（例如一个文献综述主题或创意生成请求）。系统会自动将该查询发送给**两个匿名模型**（模型A和模型B），并将它们的输出并排展示。  
  评测员需根据回答质量**投票选择**更优的一方——**A**、**B**、**平局**或**两者都不好**。
- **基于Elo的排名机制**：系统分数会根据成对比较的结果动态更新，生成实时变化的排行榜。

评测员可通过以下链接访问竞技场：  
👉 **http://39.97.229.86/leaderboard**

所有评测员均已分配唯一的访问令牌，用于安全登录与追踪。

## **2. 评测指南**

### 2.0 **适用于所有赛道的通用说明**：

1. **时间限制与输出截断**  
   每个赛道有各自的生成时间限制：

   - 文献综述 – **15分钟**  
   - 论文问答 – **15分钟**  
   - 创意生成 – **10分钟**  
   - 论文评审 – **20分钟**  

   当达到时间限制时，竞技场前端将**自动停止显示新输出**，可能导致响应被截断。

2. **持久化聊天窗口与并行评测**  
   每个竞技场聊天窗口是**持久化的**，即一旦提交查询，模型会在后台继续生成响应，即使你离开页面也不会中断。  
   评测员**无需停留在当前聊天标签页**等待生成完成——你可以立即打开**新的聊天窗口**提交下一个查询。  
   这允许你**并行运行多个查询**，节省时间。待所有响应生成完毕后，再返回各标签页进行评审和打分。

---

### **2.1 赛道A：文献综述**

#### **目标**

评估模型针对指定研究主题进行全面、准确文献综述的能力。

#### **示例查询**

- 请全面综述多模态大语言模型的最新进展。  
- 总结AI科学家领域的主要方法与挑战。  
- 概述世界模型（world models）的近期趋势。  
- 请总结将强化学习与大语言模型推理智能体结合的最新工作。  
- 综述AI技术在气候建模与可持续性研究中的应用。  
- 总结面向大语言模型的强化学习算法的发展与演进。  
- 综述2024年以来符号回归（symbolic regression）领域的最新研究。  
- 汇总过去五年发表在《Nature》《Science》及其子刊中，聚焦AI与化学交叉领域的所有论文。  
- 追踪过去两年在三大顶级机器学习会议上发表的关于测试时扩展（test-time scaling）的研究进展。  
- 总结AI辅助药物发现与分子设计领域的最新技术水平。

#### **评测维度**

| 维度 | 描述 |
|------|------|
| **覆盖广度** | 是否涵盖了该领域的主要工作与研究方向？ |
| **准确性** | 引用的文献和摘要是否事实正确？ |
| **组织结构** | 综述是否逻辑清晰（例如按主题或时间线组织）？ |
| **洞察深度** | 是否提炼出关键趋势或洞见，而非仅做表面总结？ |
| **清晰度** | 文字是否可读、连贯、格式良好？ |

---

### **2.2 赛道B：论文问答（Paper QA）**

#### **目标**

评估模型对一篇科学论文的理解能力，以及回答基于该论文的技术问题的能力。

#### **示例查询**

- 请仔细分析并解释本文所采用的强化学习训练方法。  
- 本文的主要贡献和局限性是什么？  
- 解释本文的实验设计如何支持其结论。  
- 请总结本文的理论框架及其关键假设。  
- 分析所使用的数据收集与预处理方法，并讨论其对结果的潜在影响。  
- 解释所提出算法或模型架构相比先前方法的改进之处。  
- 识别并评估实验中主要的不确定性来源或潜在偏差。  
- 描述作者如何验证其假设，证据是否充分？  
- 总结所使用的定量结果和评估指标，并讨论其是否支持结论。  
- 分析所提方法与相关工作的核心区别。  
- 解释本文引入的任何新型评估协议及其意义。  
- 评估图表和表格是否有效地传达了研究结果。

#### **评测维度**

| 维度 | 描述 |
|------|------|
| **理解能力** | 模型是否正确解读了论文内容？ |
| **深度** | 是否超越表面复述，提供推理或批判性分析？ |
| **相关性** | 回答是否直接针对所提问题？ |
| **忠实性** | 所有陈述是否基于论文原文（无幻觉）？ |
| **清晰度** | 回答是否行文流畅、逻辑清晰？ |

---

### **2.3 赛道C：创意生成（Ideation）**

#### **目标**

评估系统在给定主题下提出**新颖科学创意**的创造力与可行性。

#### **示例查询**

- 提出一个将图神经网络与蛋白质折叠模拟相结合的新颖研究思路。  
- 建议一个新的基准框架，用于评估大语言模型生成科学假设的创造力。  
- 提出一种利用多模态推理提升分子性质预测数据效率的方法。  
- 设想一个能同时执行实验操控与理论推理的未来AI科学家，并概述其核心架构。  
- 如何在用于机器人控制与仿真的世界模型中更好地融入物理世界约束？  
- 提出一种方法，以提升多模态大语言模型在同步处理视频与文本流时的时序一致性。  
- 如何将符号推理与大语言模型结合，以增强科学预测中的数学可解释性？  
- 建议一种利用强化学习与世界模型预测实现实时自适应语音合成的技术。  
- 提出一个利用多模态世界模型融合本体感知反馈与视觉输入的机器人导航框架。  
- 如何将因果结构学习融入世界模型，以提升在动态物理环境中长期预测的准确性？  
- 建议一种跨模态对齐方法，使大语言模型能在文本、音频和视频信号上联合推理，生成科学假设。  
- 如何在强化学习智能体中编码能效约束，以适用于真实世界机器人实验？  
- 提出一种利用符号或规则表示增强多模态AI系统中所学物理模型可解释性的方法。  
- 如何设计分层世界模型，使其能同时对微观尺度模拟（如分子）和宏观尺度现象（如材料或生态系统）进行推理？

#### **评测维度**

| 维度 | 描述 |
|------|------|
| **原创性** | 该想法是否非平凡且具有创造性？ |
| **可行性** | 该想法是否可实际实施或验证？ |
| **科学价值** | 是否能推动方法论或认知的进步？ |
| **清晰性与结构** | 提案是否逻辑连贯、动机明确？ |
| **知识整合** | 是否有效融合了相关领域先验知识与推理？ |

---

### **2.4 赛道D：论文评审（Paper Review）**

#### **目标**

评估模型扮演**审稿人**的能力，提供结构化批评与量化评分。

#### **注意：**

**在论文评审赛道中，评测员只需上传PDF文件，无需输入用户查询。**  
评审输出格式严格如下：

- 摘要（Summary）  
- 优点（Strengths）  
- 缺点/关切（Weaknesses / Concerns）  
- 给作者的问题（Questions for Authors）  
- 评分：
  - 总体（满分10分）
  - 新颖性（满分10分）
  - 技术质量（满分10分）
  - 清晰度（满分10分）
  - 评审信心（满分5分）

#### **评测维度**

| 维度 | 描述 |
|------|------|
| **专业性** | 评审是否展现出对该研究领域的扎实理解，并提供专业反馈？ |
| **评分一致性** | 数值评分是否与文字评述逻辑一致？ |
| **平衡性** | 是否公平地指出了优点与不足？ |
| **清晰度** | 评审意见是否清晰、结构良好、易于理解？ |