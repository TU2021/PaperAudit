#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Review Alignment Evaluator

Batch review alignment evaluator that compares AI-generated reviews with human reviews
based on COVERAGE alignment (not score alignment). The evaluator focuses on how well
AI reviews cover the substantive points raised in human reviews.

The evaluator:
1. Compares human review summary with AI review text
2. Handles multiple AI reviews by consolidating them into one review
3. Extracts key points (strengths and weaknesses) from both reviews
4. Computes coverage metrics: recall, extra points rate, and similarity

Usage Examples:
    # Basic alignment evaluation
    python eval_alignment.py \
        --input_dir /path/to/papers \
        --judge_model gemini-2.5-pro \
        --review_agent paper_audit \
        --ai_model gpt-5-2025-08-07 \
        --ai_review_file review_output_all.json \
        --model_tag v1 \
        --jobs 10

Key Arguments:
    --input_dir: Root folder containing paper subfolders. Default: /mnt/parallel_ssd/home/zdhs0006/ACL/data_test
    --judge_model: LLM model used for alignment judgment. Default: gemini-2.5-pro
    --review_agent: Review agent name (folder under reviews/). Required
    --ai_model: AI model that generated the review. Required
    --ai_review_file: Filename of AI review file. Required
    --model_tag: Tag for output alignment file. Default: default
    --jobs: Number of concurrent evaluations. Default: 1
    --overwrite: Force re-run and overwrite existing outputs. Default: False

Input:
    - Human review: <paper_dir>/review_origin.txt
    - AI review: <paper_dir>/reviews/{review_agent}/{ai_model}/{paper_origin}/{ai_review_file}

Output:
    - Alignment results: <paper_dir>/reviews/alignment_judge/{judge_model}/{paper_origin}/alignment_{model_tag}.json
    - The JSON contains:
      - paper: Paper identifier
      - runs: List of evaluation runs (one per config)
        - config: Configuration used for this run
        - config_key: Unique key for this configuration
        - inputs: Input review texts
        - multi_review: Information about multiple reviews (if applicable)
        - ai_review_consolidated: Consolidated AI review text
        - alignment: Alignment metrics
          - strength_coverage_recall: How well AI covers human strengths (0-1)
          - weakness_coverage_recall: How well AI covers human weaknesses (0-1)
          - ai_extra_major_points_rate: Rate of AI points not in human review (0-1)
          - symmetric_coverage_similarity: Jaccard similarity of all points (0-1)
          - note: Additional notes from judge
        - generated_at: Timestamp

Metrics Explanation:
    - Strength Coverage Recall: Fraction of human strength points covered by AI
    - Weakness Coverage Recall: Fraction of human weakness points covered by AI
    - AI Extra Major Points Rate: Fraction of AI major points not in human review
    - Symmetric Coverage Similarity: Jaccard similarity of all matched points
"""

from __future__ import annotations

import argparse
import asyncio
import json
import re
import sys
import time
import threading
import hashlib
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

try:
    from tqdm import tqdm
except Exception:
    tqdm = None

from dotenv import load_dotenv

load_dotenv()
_PRINT_LOCK = threading.Lock()

# ---------------- prompts ----------------

AI_MULTI_REVIEW_SUMMARY_PROMPT = """You are an expert meta-reviewer.

You are given MULTIPLE peer reviews for the SAME paper. Your task is to
merge and summarize them into ONE consolidated review.

Constraints:
- Focus on review CONTENT quality, not numerical scores.
- Preserve the substance across reviews: key motivations/contributions/strengths, key weaknesses/issues,
  and actionable suggestions.
- De-duplicate repeated points, but keep important disagreements if any.
- Output plain text ONLY (no JSON), with a clear structure:
  1) Summary
  2) Strengths
  3) Weaknesses / Concerns
  4) Suggestions

Output requirements:
- Keep it concise (ideally <= 250-400 words total).
- Do NOT include any numerical scores.

Now consolidate the following reviews:
"""

COVERAGE_ALIGNMENT_JUDGE_PROMPT = """You are an expert meta-reviewer.

You are given TWO reviews of the SAME paper:
- Review A: written by a human reviewer (summary form)
- Review B: generated by an AI system (or a consolidated summary of multiple AI reviews)

Your task is to evaluate COVERAGE ALIGNMENT: how well Review B covers the
substantive points raised in Review A, and how similar the two reviews are
in terms of the sets of key points they discuss.

Focus ONLY on *what points are mentioned*, not writing style, tone, or length.

────────────────────────────────
STEP 1: Extract key point sets
────────────────────────────────

From EACH review, extract two sets of atomic, de-duplicated points:

1) Strength points
2) Weakness / Concern points

Guidelines:
- Each point should be a short phrase (not a full sentence).
- Only include substantive points (e.g., concrete contributions, limitations, methodological concerns).
- Exclude generic praise or boilerplate (e.g., "well-written", "interesting paper").
- Extract 5–12 points per set if available; otherwise extract all that exist.

────────────────────────────────
STEP 2: Match and count coverage
────────────────────────────────

Treat two points as matching if they express the SAME underlying idea,
even if phrased differently.

Compute the following FOUR metrics:

A. Strength Coverage Recall (Human → AI)
   = |HumanStrength ∩ AIStrength| / |HumanStrength|

B. Weakness Coverage Recall (Human → AI)
   = |HumanWeakness ∩ AIWeakness| / |HumanWeakness|

C. AI Extra Major Points Rate
   = |AI_major_points NOT mentioned by Human| / |AI_major_points|

   Notes:
   - "AI_major_points" are the central, emphasized points in Review B,
     not minor nitpicks or formatting issues.
   - This metric reflects coverage mismatch, not necessarily an error.

D. Symmetric Coverage Similarity (Jaccard)
   = |AllMatchedPoints| / |AllUniquePoints|
   where:
     AllMatchedPoints = (HumanStrength ∩ AIStrength) ∪ (HumanWeakness ∩ AIWeakness)
     AllUniquePoints  = (HumanStrength ∪ AIStrength ∪ HumanWeakness ∪ AIWeakness)

────────────────────────────────
STEP 3: Map to final scores
────────────────────────────────

For EACH metric (A–D), output a score in [0, 1], rounded to TWO decimal places.

Interpretation guidelines:
- 0.00–0.20: almost no overlap in covered points
- 0.20–0.40: low overlap; only a few generic points match
- 0.40–0.60: moderate overlap; some key points match, many are missed
- 0.60–0.80: high overlap; most key points are covered
- 0.80–1.00: very high overlap; near-complete coverage alignment

Do NOT force scores to cluster near the middle; use the full range when appropriate.

────────────────────────────────
OUTPUT FORMAT (STRICT JSON ONLY)
────────────────────────────────

{
  "strength_coverage_recall": float,
  "weakness_coverage_recall": float,
  "ai_extra_major_points_rate": float,
  "symmetric_coverage_similarity": float,
  "note": "1–3 sentences summarizing the most important missing or extra points affecting coverage."
}

Rules:
- All floats must be in [0, 1] and rounded to TWO decimal places.
- Do NOT mention any numerical reviewer scores from the reviews.
- Do NOT evaluate correctness or quality of the paper itself.
- Judge ONLY coverage alignment between the two reviews.
"""


# ---------------- LLM client (self-contained) ----------------

@dataclass
class LLMConfig:
    model: str
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    temperature: float = 0.0
    max_tokens: int = 1200
    timeout_s: int = 120


class AsyncChatLLM:
    """
    Minimal async chat wrapper using OpenAI-compatible SDK.

    Env vars supported by default:
      - OPENAI_API_KEY
      - OPENAI_BASE_URL (optional)
    """
    def __init__(self, cfg: LLMConfig):
        self.cfg = cfg
        try:
            from openai import AsyncOpenAI  # type: ignore
        except Exception as e:
            raise RuntimeError(
                "Missing dependency `openai`. Please `pip install openai` (OpenAI-compatible SDK)."
            ) from e

        api_key = cfg.api_key
        if not api_key:
            import os
            api_key = os.environ.get("OPENAI_API_KEY")

        base_url = cfg.base_url
        if not base_url:
            import os
            base_url = os.environ.get("OPENAI_BASE_URL") or None

        if not api_key:
            raise RuntimeError("OPENAI_API_KEY is not set (and --api_key not provided).")

        self.client = AsyncOpenAI(api_key=api_key, base_url=base_url)

    async def complete_text(self, prompt: str) -> str:
        resp = await self.client.chat.completions.create(
            model=self.cfg.model,
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt},
            ],
            temperature=self.cfg.temperature,
            max_tokens=self.cfg.max_tokens,
            timeout=self.cfg.timeout_s,
        )
        return (resp.choices[0].message.content or "").strip()


# ---------------- CLI ----------------

def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Batch review alignment judge (COVERAGE alignment)")
    p.add_argument(
        "--input_dir",
        default="/mnt/parallel_ssd/home/zdhs0006/ACL/data/ICLR_26",
        help="Root folder containing paper subfolders (each subfolder is one paper).",
    )

    p.add_argument(
        "--judge_model",
        default="gemini-2.5-pro",
        help="Judge LLM model for summarization (if needed) + coverage scoring.",
    )
    p.add_argument("--api_key", default=None, help="Override OPENAI_API_KEY")
    p.add_argument("--base_url", default=None, help="Override OPENAI_BASE_URL")
    p.add_argument("--temperature", type=float, default=0.0)
    p.add_argument("--max_tokens", type=int, default=20000)
    p.add_argument("--timeout_s", type=int, default=180)

    p.add_argument(
        "--review_agent",
        default="deepreviewer",
        help="Folder name under reviews/, e.g., deepreviewer or paper_audit",
    )
    p.add_argument(
        "--ai_model",
        default="gpt-5-2025-08-07",
        help="Model folder under reviews/{review_agent}/",
    )
    p.add_argument(
        "--ai_review_file",
        default="deep_review_merge.txt",
        help="Filename under .../paper_origin/ for AI review, e.g. deep_review_all.txt",
    )

    p.add_argument(
        "--model_tag",
        default="v3",
        help="Tag used in output filenames, e.g., v1 / exp01",
    )

    p.add_argument(
        "--jobs", "-j",
        type=int,
        default=10,
        help="Number of in-flight papers (threadpool under the hood).",
    )
    p.add_argument(
        "--overwrite",
        default=False,
        action="store_true",
        help="Overwrite existing alignment outputs (same config_key).",
    )

    return p.parse_args()


# ---------------- utils ----------------

def sanitize_filename(name: str) -> str:
    return "".join(c if c.isalnum() or c in ("-", "_", ".") else "_" for c in name)

def load_text(path: Path) -> str:
    return path.read_text(encoding="utf-8", errors="ignore").strip()

def save_json(path: Path, obj: Dict[str, Any]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding="utf-8")

def load_json(path: Path) -> Dict[str, Any]:
    return json.loads(path.read_text(encoding="utf-8", errors="ignore") or "{}")

def alignment_dir(paper_dir: Path, judge_model: str) -> Path:
    return paper_dir / "reviews" / "alignment_judge" / sanitize_filename(judge_model) / "paper_origin"

def alignment_out(paper_dir: Path, judge_model: str, tag: str) -> Path:
    return alignment_dir(paper_dir, judge_model) / f"alignment_{sanitize_filename(tag)}.json"

def find_paper_dirs(root_dir: Path) -> List[Path]:
    return [p for p in sorted(root_dir.iterdir()) if p.is_dir()]

def stable_json_dumps(obj: Any) -> str:
    return json.dumps(obj, ensure_ascii=False, sort_keys=True, separators=(",", ":"))

def make_config(args: argparse.Namespace) -> Dict[str, Any]:
    # Config fields that define "same experiment"
    return {
        "judge_model": args.judge_model,
        "review_agent": args.review_agent,
        "ai_model": args.ai_model,
        "ai_review_file": args.ai_review_file,
        "temperature": float(args.temperature),
        "max_tokens": int(args.max_tokens),
        "timeout_s": int(args.timeout_s),
        "metric": "coverage_v1",  # <- explicit
    }

def make_config_key(config: Dict[str, Any]) -> str:
    s = stable_json_dumps(config).encode("utf-8")
    return hashlib.sha1(s).hexdigest()  # short & stable

def get_runs_container(existing: Dict[str, Any]) -> List[Dict[str, Any]]:
    runs = existing.get("runs")
    if isinstance(runs, list):
        return runs
    return []

def has_run_with_key(existing: Dict[str, Any], config_key: str) -> bool:
    for r in get_runs_container(existing):
        if isinstance(r, dict) and r.get("config_key") == config_key:
            return True
    return False


# ---------------- multi-review splitting ----------------

_SPLIT_PATTERNS = [
    r"(?im)^\s*(reviewer\s*\#?\s*\d+)\s*[:\-—]\s*$",
    r"(?im)^\s*(review\s*\#?\s*\d+)\s*[:\-—]\s*$",
    r"(?im)^\s*(meta[-\s]*review)\s*[:\-—]\s*$",
    r"(?im)^\s*#{1,6}\s*(reviewer\s*\#?\s*\d+|review\s*\#?\s*\d+)\s*$",
    r"(?m)^\s*[-=_]{8,}\s*$",
]

def split_into_reviews(ai_text: str, min_chunk_chars: int = 500) -> List[str]:
    text = (ai_text or "").strip()
    if not text:
        return []

    sep = re.compile(_SPLIT_PATTERNS[-1])
    parts = [p.strip() for p in sep.split(text) if p.strip()]
    parts = [p for p in parts if len(p) >= min_chunk_chars]
    if len(parts) >= 2:
        return parts

    for pat in _SPLIT_PATTERNS[:-1]:
        rgx = re.compile(pat)
        matches = list(rgx.finditer(text))
        if len(matches) >= 2:
            idxs = [m.start() for m in matches] + [len(text)]
            chunks = []
            for i in range(len(idxs) - 1):
                chunk = text[idxs[i]:idxs[i+1]].strip()
                if len(chunk) >= min_chunk_chars:
                    chunks.append(chunk)
            if len(chunks) >= 2:
                return chunks

    marker = re.compile(r"(?im)^\s*#{2,6}\s*(summary|strengths|weaknesses|overall)\b")
    m = list(marker.finditer(text))
    if len(m) >= 4:
        sum_pat = re.compile(r"(?im)^\s*#{2,6}\s*summary\b")
        ms = list(sum_pat.finditer(text))
        if len(ms) >= 2:
            idxs = [x.start() for x in ms] + [len(text)]
            chunks = []
            for i in range(len(idxs) - 1):
                chunk = text[idxs[i]:idxs[i+1]].strip()
                if len(chunk) >= min_chunk_chars:
                    chunks.append(chunk)
            if len(chunks) >= 2:
                return chunks

    return [text]


# ---------------- JSON extraction (robust) ----------------

def extract_first_json_object(s: str) -> Dict[str, Any]:
    raw = (s or "").strip()
    if not raw:
        raise ValueError("Empty LLM response.")

    # direct parse
    try:
        obj = json.loads(raw)
        if isinstance(obj, dict):
            return obj
    except Exception:
        pass

    # search spans
    starts = [m.start() for m in re.finditer(r"\{", raw)]
    for st in starts:
        for ed in range(len(raw), st + 2, -1):
            if raw[ed - 1] != "}":
                continue
            cand = raw[st:ed]
            try:
                obj = json.loads(cand)
                if isinstance(obj, dict):
                    return obj
            except Exception:
                continue
    raise ValueError("Could not extract a valid JSON object from LLM response.")


def clamp01(x: Any) -> float:
    try:
        v = float(x)
    except Exception:
        return 0.0
    if v < 0.0:
        v = 0.0
    if v > 1.0:
        v = 1.0
    return round(v, 2)


def normalize_alignment_obj(obj: Dict[str, Any]) -> Dict[str, Any]:
    """
    Enforce schema + clamp/round to 2 decimals.
    """
    out: Dict[str, Any] = {}
    out["strength_coverage_recall"] = clamp01(obj.get("strength_coverage_recall"))
    out["weakness_coverage_recall"] = clamp01(obj.get("weakness_coverage_recall"))
    out["ai_extra_major_points_rate"] = clamp01(obj.get("ai_extra_major_points_rate"))
    out["symmetric_coverage_similarity"] = clamp01(obj.get("symmetric_coverage_similarity"))
    note = obj.get("note")
    out["note"] = str(note).strip() if note is not None else ""
    return out


# ---------------- per-paper worker ----------------

def process_one_paper_sync(paper_dir: Path, args: argparse.Namespace) -> Tuple[Path, bool, str]:
    human_path = paper_dir / "review_origin.txt"
    ai_path = (
        paper_dir
        / "reviews"
        / args.review_agent
        / args.ai_model
        / "paper_origin"
        / args.ai_review_file
    )

    if not human_path.exists():
        return paper_dir, True, "skip_missing_human"
    if not ai_path.exists():
        return paper_dir, True, "skip_missing_ai"

    outp = alignment_out(paper_dir, args.judge_model, args.model_tag)

    config = make_config(args)
    config_key = make_config_key(config)

    # Load existing container (if any) and decide skip/append
    existing: Dict[str, Any] = {}
    if outp.exists():
        try:
            existing = load_json(outp)
        except Exception:
            existing = {}

    if (not args.overwrite) and outp.exists() and has_run_with_key(existing, config_key):
        return paper_dir, True, "skip_done_same_config"

    try:
        outp.parent.mkdir(parents=True, exist_ok=True)

        human_review = load_text(human_path)
        ai_review_raw = load_text(ai_path)

        # Split AI review if multi-review present
        chunks = split_into_reviews(ai_review_raw)
        is_multi = len(chunks) >= 2

        # Private event loop (thread-safe)
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        llm = AsyncChatLLM(
            LLMConfig(
                model=args.judge_model,
                api_key=args.api_key,
                base_url=args.base_url,
                temperature=args.temperature,
                max_tokens=args.max_tokens,
                timeout_s=args.timeout_s,
            )
        )

        consolidated_ai_review = ai_review_raw
        summarize_used = False

        if is_multi:
            summarize_used = True
            joined = []
            for i, c in enumerate(chunks, 1):
                joined.append(f"[Review {i}]\n{c}")
            multi_blob = "\n\n".join(joined)

            summary_prompt = AI_MULTI_REVIEW_SUMMARY_PROMPT + "\n\n" + multi_blob
            consolidated_ai_review = loop.run_until_complete(llm.complete_text(summary_prompt)).strip()

        # Coverage alignment judging
        judge_prompt = (
            COVERAGE_ALIGNMENT_JUDGE_PROMPT
            + "\n\n[Review A — Human]\n"
            + human_review
            + "\n\n[Review B — AI]\n"
            + consolidated_ai_review
        )
        judge_resp = loop.run_until_complete(llm.complete_text(judge_prompt)).strip()
        loop.close()

        alignment_raw = extract_first_json_object(judge_resp)
        alignment_obj = normalize_alignment_obj(alignment_raw)

        run_obj: Dict[str, Any] = {
            "config": config,
            "config_key": config_key,
            "inputs": {
                "human_review": str(human_path.relative_to(paper_dir)),
                "ai_review": str(ai_path.relative_to(paper_dir)),
            },
            "multi_review": {
                "detected": is_multi,
                "num_reviews": len(chunks),
                "summarize_used": summarize_used,
            },
            "ai_review_consolidated": consolidated_ai_review if summarize_used else None,
            "alignment": alignment_obj,
            "generated_at": time.strftime("%Y-%m-%dT%H:%M:%S", time.localtime()),
        }

        # Build/append container
        container: Dict[str, Any]
        if isinstance(existing, dict) and existing:
            container = existing
        else:
            container = {"paper": paper_dir.name, "runs": []}

        if "paper" not in container:
            container["paper"] = paper_dir.name

        runs = container.get("runs")
        if not isinstance(runs, list):
            runs = []

        # If overwrite, remove existing runs with same config_key
        if args.overwrite:
            runs = [r for r in runs if not (isinstance(r, dict) and r.get("config_key") == config_key)]

        runs.append(run_obj)
        container["runs"] = runs

        save_json(outp, container)
        return paper_dir, True, "ok"

    except Exception as e:
        return paper_dir, False, f"error: {repr(e)}"


# ---------------- async scheduler ----------------

async def bounded_worker(
    sem: asyncio.Semaphore,
    loop: asyncio.AbstractEventLoop,
    paper_dir: Path,
    args: argparse.Namespace,
):
    async with sem:
        return await loop.run_in_executor(None, process_one_paper_sync, paper_dir, args)

async def run_batch(root_dir: Path, jobs: int, args: argparse.Namespace) -> Dict[str, Any]:
    paper_dirs = find_paper_dirs(root_dir)
    if not paper_dirs:
        return {"total": 0, "ok": 0, "fail": [], "skipped_done": 0}

    planned: List[Path] = paper_dirs[:]  # rely on per-paper skip logic

    sem = asyncio.Semaphore(jobs)
    loop = asyncio.get_running_loop()
    tasks = [bounded_worker(sem, loop, d, args) for d in planned]

    results: List[Tuple[Path, bool, str]] = []
    iterator = asyncio.as_completed(tasks)
    if tqdm is not None:
        iterator = tqdm(iterator, total=len(tasks), desc="Coverage alignment judging")

    skipped_done = 0
    for coro in iterator:
        d, ok, msg = await coro
        results.append((d, ok, msg))

        with _PRINT_LOCK:
            if msg == "ok":
                outp = alignment_out(d, args.judge_model, args.model_tag)
                rel = outp.relative_to(d)
                print(f"[DONE] {d.name} -> {rel}")
            elif msg.startswith("skip_"):
                skipped_done += 1
                print(f"[SKIP] {d.name} ({msg})")
            else:
                print(f"[FAIL] {d.name} :: {msg}", file=sys.stderr)

    ok_cnt = sum(1 for (_, ok, msg) in results if ok and msg == "ok")
    fail = [(str(d), msg) for (d, ok, msg) in results if not ok]

    return {
        "total": len(results),
        "ok": ok_cnt,
        "fail": fail,
        "skipped_done": skipped_done,
    }


def main() -> None:
    args = parse_args()
    root_dir = Path(args.input_dir).expanduser().resolve()
    if not root_dir.exists() or not root_dir.is_dir():
        raise SystemExit(f"Input directory not found: {root_dir}")

    print(
        f"[INFO] root={root_dir} jobs={args.jobs} "
        f"judge_model={args.judge_model} review_agent={args.review_agent} "
        f"ai_model={args.ai_model} ai_review_file={args.ai_review_file} "
        f"overwrite={args.overwrite} tag={args.model_tag}"
    )

    summary = asyncio.run(run_batch(root_dir=root_dir, jobs=args.jobs, args=args))

    print(
        f"[DONE] total={summary['total']} ok={summary['ok']} "
        f"fail={len(summary['fail'])} skipped_done={summary.get('skipped_done', 0)}"
    )
    if summary["fail"]:
        print("[FAILED ITEMS]")
        for i, (path, msg) in enumerate(summary["fail"], 1):
            print(f"  {i}. {path} :: {msg}")


if __name__ == "__main__":
    main()
