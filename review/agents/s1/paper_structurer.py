from typing import List, Dict, Any
import re
from ..base_agent import BaseAgent
from ..logger import get_logger

logger = get_logger(__name__)

class PaperStructurer(BaseAgent):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    """
    ç”¨ LLM å¯¹åŸå§‹ PDF æ–‡æœ¬åšâ€œç»“æ„åŒ–åˆ‡ç‰‡â€ï¼Œä½†ä¸è®© LLM é‡å†™æ­£æ–‡ï¼š
    - åªè®© LLM æ‰¾å‡ºè‹¥å¹²ä¸ªå¤§çš„ç« èŠ‚å—ï¼ˆPart 1, Part 2, ...ï¼‰çš„â€œèµ·ç‚¹ä½ç½®â€
    - æ¯ä¸ªå—åŒ…å«ï¼š
        * ä¸€ä¸ªç®€çŸ­æ ‡é¢˜ï¼ˆtitleï¼‰
        * è¯¥å—åœ¨åŸæ–‡ä¸­çš„ç¬¬ä¸€å¥è¯ï¼ˆanchor_sentenceï¼‰ï¼Œå¿…é¡»ä¸åŸæ–‡é€å­—ä¸€è‡´
    - ç„¶ååœ¨æœ¬åœ°æ ¹æ® anchor_sentence åœ¨åŸå§‹ pdf_text ä¸­åšå­—ç¬¦ä¸²åŒ¹é…ï¼Œ
      ç”¨è¿™äº› anchor æŠŠå…¨æ–‡åˆ‡æˆè¿ç»­çš„è‹¥å¹²å¤§å—ã€‚

    æœ€ç»ˆè¿”å›ä¸€ä¸ª sections åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ï¼š
    - title: ç« èŠ‚æ ‡é¢˜ï¼ˆå¦‚ "Part 1: Introduction and Motivation"ï¼‰
    - content: è¯¥ç« èŠ‚åœ¨åŸæ–‡ä¸­çš„åŸå§‹ç‰‡æ®µï¼ˆæœªè¢« LLM æ”¹å†™ï¼‰
    """

    # ğŸ”¢ æœ€å¤šå‡ å¤§å—ï¼ˆå¯æŒ‰éœ€è°ƒæ•´ï¼‰
    MAX_SECTIONS = 6

    # ç°åœ¨çš„ SYSTEM_PROMPTï¼šè®©æ¨¡å‹åªè¾“å‡ºâ€œåˆ‡ç‰‡ç´¢å¼• + é”šç‚¹å¥å­â€ï¼Œä¸é‡å†™æ­£æ–‡
    SYSTEM_PROMPT = (
        "You are a meticulous segmenter for scientific papers. "
        "Your job is NOT to rewrite the paper, but ONLY to identify a small number "
        "of major contiguous segments (high-level parts) in the manuscript.\n\n"
        "You will see the full (possibly noisy) plain-text manuscript. "
        "Your task is to propose AT MOST {max_parts} major parts that together cover "
        "the whole paper from beginning to end.\n\n"
        "CRITICAL: You MUST NOT rewrite, summarize, or modify the manuscript text. "
        "You ONLY choose segmentation points.\n\n"
        "For each part, you must output:\n"
        "- a short human-readable title describing that part, and\n"
        "- the EXACT FIRST SENTENCE of that part, copied VERBATIM from the manuscript.\n\n"
        "The first sentence (anchor) must:\n"
        "- **be copied exactly as it appears in the RAW MANUSCRIPT** (same wording, same order),\n"
        "- be uniquely identifiable (avoid very short or generic phrases),\n"
        "- correspond to the first sentence of that segment.\n\n"
        "You **MUST NOT paraphrase or clean the anchor sentence.** "
        "Do NOT fix grammar, do NOT delete words, do NOT change line breaks inside it. "
        "We will use this anchor sentence to locate the segment in the original text "
        "via exact string matching.\n\n"
        "================ OUTPUT FORMAT (STRICT) ================\n"
        "Output ONE line per part, in order from the beginning of the paper to the end.\n"
        "Each line MUST follow this exact pattern:\n"
        "    Part <k> | <short title> | <EXACT first sentence of this part>\n"
        "Where:\n"
        "    - <k> is 1, 2, 3, ... (no gaps, strictly increasing)\n"
        "    - <short title> is a brief descriptor (e.g., 'Introduction and Motivation')\n"
        "    - <EXACT first sentence> is copied verbatim from the manuscript.\n\n"
        "Do NOT output anything else: no explanations, no JSON, no bullet points.\n"
        "If you think fewer parts are sufficient, use fewer. "
        "NEVER exceed {max_parts} parts.\n"
    )

    USER_PROMPT_TEMPLATE = (
        "Here is the full manuscript extracted from a PDF (with possible noise such as page headers, "
        "footers, and line breaks).\n"
        "Your job is ONLY to propose at most {max_parts} major segments.\n\n"
        "Important:\n"
        "- Do NOT rewrite or summarize the text.\n"
        "- Do NOT clean or modify the anchor sentences.\n"
        "- The anchor sentences MUST be copied verbatim from the RAW MANUSCRIPT below.\n\n"
        "Again, for each part, output exactly one line in this format:\n"
        "    Part <k> | <short title> | <EXACT first sentence of this part>\n\n"
        "=== BEGIN RAW MANUSCRIPT ===\n"
        "{text}\n"
        "=== END RAW MANUSCRIPT ===\n"
    )

    @staticmethod
    def _extract_text_from_message_content(raw_content: Any) -> str:
        """
        å…¼å®¹ OpenAI é£æ ¼çš„ message.contentï¼š
        - å¯èƒ½æ˜¯ str
        - ä¹Ÿå¯èƒ½æ˜¯ list[TextPart] æˆ– list[dict(text=...)]
        """
        if isinstance(raw_content, str):
            return raw_content

        if isinstance(raw_content, list):
            parts: List[str] = []
            for part in raw_content:
                if isinstance(part, dict):
                    t = part.get("text") or part.get("content")
                    if t:
                        parts.append(str(t))
                else:
                    t = getattr(part, "text", None) or getattr(part, "content", None)
                    if t:
                        parts.append(str(t))
            return "".join(parts)

        return str(raw_content)

    async def run(self, pdf_text: str) -> List[Dict[str, Any]]:
        """
        æ–°é€»è¾‘ï¼š
        1. è°ƒç”¨ LLMï¼Œè¯·å®ƒè¾“å‡ºè‹¥å¹²è¡Œï¼š
              Part k | <title> | <EXACT first sentence>
        2. è§£æè¿™äº›è¡Œï¼Œå¾—åˆ° (k, title, anchor_sentence) åˆ—è¡¨
        3. åœ¨åŸå§‹ pdf_text ä¸­ç”¨ anchor_sentence åšå­—ç¬¦ä¸²æŸ¥æ‰¾ï¼Œå¾—åˆ°æ¯ä¸ªå—çš„èµ·å§‹ä½ç½®
        4. æŒ‰èµ·å§‹ä½ç½®æ’åºï¼ŒæŠŠ pdf_text åˆ‡æˆè¿ç»­çš„è‹¥å¹²å¤§æ®µ
        5. è¿”å› sections åˆ—è¡¨ï¼Œæ¯æ®µå®Œå…¨æ¥è‡ªåŸå§‹ pdf_textï¼ˆä¸ç» LLM æ”¹å†™ï¼‰
        """
        user_prompt = self.USER_PROMPT_TEMPLATE.format(
            text=pdf_text,
            max_parts=self.MAX_SECTIONS,
        )

        system_prompt = self.SYSTEM_PROMPT.format(max_parts=self.MAX_SECTIONS)

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        logger.info("Calling LLM to propose segment anchors (stream=False)...")

        try:
            resp = await self._call_llm_with_retry(
                model=self.model,
                messages=messages,
                stream=False,
                temperature=self.config.get("agents.paper_structurer.temperature", None),
            )
        except Exception as e:
            logger.error(f"Failed to get response from LLM: {e}")
            return [{"title": "Full Paper (fallback)", "content": pdf_text}]

        # -------- è§£æéæµå¼è¿”å›ï¼Œå¾—åˆ°çº¯æ–‡æœ¬ --------
        try:
            choices = getattr(resp, "choices", None)
            if not choices:
                logger.warning("No choices in response, fallback.")
                return [{"title": "Full Paper (fallback)", "content": pdf_text}]

            first = choices[0]
            message = getattr(first, "message", None)
            if message is None:
                logger.warning("No message in first choice, fallback.")
                return [{"title": "Full Paper (fallback)", "content": pdf_text}]

            raw_content = getattr(message, "content", "")
            llm_text = self._extract_text_from_message_content(raw_content).strip()
        except Exception as e:
            logger.error(f"Error parsing LLM response: {e}")
            return [{"title": "Full Paper (fallback)", "content": pdf_text}]

        if not llm_text:
            logger.warning("Empty content from LLM, fallback.")
            return [{"title": "Full Paper (fallback)", "content": pdf_text}]

        logger.info("LLM segment proposal raw text:")
        logger.info(llm_text[:500] + ("\n..." if len(llm_text) > 500 else ""))

        # ---------- 1) è§£æ LLM è¾“å‡ºæˆ parts_info ----------

        parts_info = self._parse_parts_from_llm(llm_text)
        if not parts_info:
            logger.warning("Failed to parse any 'Part k | title | anchor' lines, fallback.")
            return [{"title": "Full Paper (fallback)", "content": pdf_text}]

        logger.info(f"Parsed {len(parts_info)} parts from LLM output.")

        # ---------- 2) æ ¹æ® anchor åœ¨åŸæ–‡ä¸­åˆ‡ç‰‡ ----------

        sections = self._segment_by_anchors(pdf_text, parts_info)

        if not sections:
            logger.warning("Segmentation by anchors failed, fallback to full paper.")
            return [{"title": "Full Paper (fallback)", "content": pdf_text}]

        logger.info(f"Successfully segmented into {len(sections)} sections.")
        return sections

    @staticmethod
    def _parse_parts_from_llm(llm_text: str) -> List[Dict[str, Any]]:
        """
        è§£æ LLM è¾“å‡ºçš„è‹¥å¹²è¡Œï¼š
            Part k | <title> | <anchor_sentence>
        è¿”å›ï¼š
            [
                {"index": k, "title": <title>, "anchor": <anchor_sentence>},
                ...
            ]
        """
        parts_info: List[Dict[str, Any]] = []

        line_pattern = re.compile(
            r"^Part\s+(\d+)\s*\|\s*(.*?)\s*\|\s*(.+)$", re.IGNORECASE
        )

        for line in llm_text.splitlines():
            line = line.strip()
            if not line:
                continue
            m = line_pattern.match(line)
            if not m:
                continue
            idx_str, title, anchor = m.group(1), m.group(2), m.group(3)
            try:
                idx = int(idx_str)
            except ValueError:
                continue

            title = title.strip() or f"Part {idx}"
            anchor = anchor.strip()
            if not anchor:
                continue

            parts_info.append(
                {
                    "index": idx,
                    "title": title,
                    "anchor": anchor,
                }
            )

        # æŒ‰ index æ’åºï¼Œé¿å…ä¹±åº
        parts_info.sort(key=lambda x: x["index"])
        return parts_info

    @staticmethod
    def _build_normalized_index(text: str) -> tuple[str, List[int]]:
        """
        å°†åŸæ–‡ text ä¸­æ‰€æœ‰ç©ºç™½å­—ç¬¦æŠ˜å ä¸ºå•ä¸ªç©ºæ ¼ï¼Œç”¨äºâ€œæ¨¡ç³ŠåŒ¹é…â€ï¼ˆå¿½ç•¥å¤šç©ºæ ¼ / æ¢è¡Œï¼‰ã€‚
        è¿”å›:
            norm_text: æŠ˜å åçš„å­—ç¬¦ä¸²
            norm_to_orig: é•¿åº¦ä¸ norm_text ç›¸åŒçš„åˆ—è¡¨ï¼Œnorm_to_orig[i] = åŸæ–‡å¯¹åº”çš„å­—ç¬¦ä¸‹æ ‡
        """
        norm_chars: List[str] = []
        norm_to_orig: List[int] = []
        prev_wspace = False

        for i, ch in enumerate(text):
            if ch.isspace():
                # è¿ç»­ç©ºç™½åªä¿ç•™ä¸€ä¸ªç©ºæ ¼
                if prev_wspace:
                    continue
                norm_chars.append(" ")
                norm_to_orig.append(i)
                prev_wspace = True
            else:
                norm_chars.append(ch)
                norm_to_orig.append(i)
                prev_wspace = False

        return "".join(norm_chars), norm_to_orig

    @staticmethod
    def _normalize_for_matching(s: str) -> str:
        """
        å¯¹ anchor å¥å­åšåŒæ ·çš„ç©ºç™½æŠ˜å å¤„ç†ï¼š
        - æ‰€æœ‰ç©ºç™½ï¼ˆç©ºæ ¼/æ¢è¡Œ/tabï¼‰æŠ˜å ä¸ºå•ç©ºæ ¼
        - å»æ‰é¦–å°¾ç©ºç™½
        """
        out_chars: List[str] = []
        prev_wspace = False
        for ch in s:
            if ch.isspace():
                if prev_wspace:
                    continue
                out_chars.append(" ")
                prev_wspace = True
            else:
                out_chars.append(ch)
                prev_wspace = False
        return "".join(out_chars).strip()

    @staticmethod
    def _fuzzy_find_norm_pos(norm_text: str, norm_anchor: str, min_ratio: float = 0.5) -> int:
        """
        åœ¨å½’ä¸€åŒ–åçš„å…¨æ–‡ norm_text ä¸­ï¼Œæ¨¡ç³ŠæŸ¥æ‰¾ norm_anchorï¼š
        - ä½¿ç”¨ difflib.SequenceMatcher.find_longest_match
        - å¦‚æœæœ€é•¿å…¬å…±å­ä¸²é•¿åº¦ / norm_anchor é•¿åº¦ >= min_ratioï¼Œåˆ™è¿”å›è¯¥ä½ç½®
        - å¦åˆ™è¿”å› -1
        """
        from difflib import SequenceMatcher

        if not norm_anchor:
            return -1

        # anchor è¿‡çŸ­æ—¶ï¼ŒåŒ¹é…æœ¬èº«å°±ä¸ç¨³å®šï¼Œè¿™é‡Œç›´æ¥è¿”å› -1ï¼Œäº¤ç»™ä¸Šå±‚å¤„ç†
        if len(norm_anchor) < 10:
            return -1

        sm = SequenceMatcher(None, norm_anchor, norm_text, autojunk=True)
        match = sm.find_longest_match(0, len(norm_anchor), 0, len(norm_text))

        if match.size == 0:
            return -1

        ratio = match.size / len(norm_anchor)
        if ratio >= min_ratio:
            # match.b æ˜¯ norm_text ä¸­çš„èµ·å§‹ä½ç½®
            return match.b
        return -1

    @staticmethod
    def _segment_by_anchors(
        text: str, parts_info: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        æ ¹æ® anchor_sentence åœ¨åŸå§‹ text ä¸­çš„ä½ç½®ï¼ŒæŠŠå…¨æ–‡åˆ‡æˆè‹¥å¹²è¿ç»­å¤§å—ï¼š
        - å…ˆç”¨â€œç©ºç™½æŠ˜å  + ç²¾ç¡®åŒ¹é…â€
        - åŒ¹é…å¤±è´¥æ—¶å†ç”¨â€œç©ºç™½æŠ˜å  + ç›¸ä¼¼åº¦ â‰¥ 0.7 çš„æ¨¡ç³ŠåŒ¹é…â€
        - anchors æŒ‰åœ¨ text ä¸­çš„èµ·å§‹ä½ç½®æ’åº
        - æ¯ä¸ªå—ä»è¯¥ anchor å¼€å§‹ï¼Œåˆ°ä¸‹ä¸€ä¸ª anchor ä¹‹å‰ä¸ºæ­¢ï¼ˆæœ€åä¸€å—åˆ°æ–‡æœ«ï¼‰
        """
        # å…ˆæ„å»ºå½’ä¸€åŒ–åçš„æ•´ç¯‡æ–‡æœ¬åŠå…¶æ˜ å°„
        norm_text, norm_to_orig = PaperStructurer._build_normalized_index(text)

        positions: List[Dict[str, Any]] = []

        for p in parts_info:
            raw_anchor = p["anchor"]
            norm_anchor = PaperStructurer._normalize_for_matching(raw_anchor)

            if not norm_anchor:
                logger.warning(f"WARNING: empty normalized anchor for part '{p['title']}'")
                continue

            # 1) å…ˆå°è¯•ç²¾ç¡®åŒ¹é…ï¼ˆåœ¨å½’ä¸€åŒ–æ–‡æœ¬ä¸Šï¼‰
            pos_norm = norm_text.find(norm_anchor)

            # 2) å¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå†å°è¯•æ¨¡ç³ŠåŒ¹é…
            if pos_norm == -1:
                fuzzy_pos = PaperStructurer._fuzzy_find_norm_pos(norm_text, norm_anchor, min_ratio=0.7)
                if fuzzy_pos != -1:
                    logger.info(
                        f"INFO: fuzzy matched anchor for part "
                        f"'{p['title']}' at norm_pos={fuzzy_pos}"
                    )
                    pos_norm = fuzzy_pos

            if pos_norm == -1:
                # æ‰¾ä¸åˆ°ï¼šæç¤º warningï¼Œç„¶åè·³è¿‡è¿™ä¸ª part
                logger.warning(
                    f"WARNING: anchor not found (even with fuzzy match) for part "
                    f"'{p['title']}'. Raw anchor preview: {raw_anchor[:120]!r}"
                )
                continue

            # æ˜ å°„å›åŸæ–‡ä¸­çš„èµ·å§‹ä½ç½®
            orig_start = norm_to_orig[pos_norm]

            positions.append(
                {
                    "start": orig_start,
                    "index": p["index"],
                    "title": p["title"],
                    "anchor": raw_anchor,
                }
            )

        if not positions:
            return []

        # æŒ‰åœ¨åŸæ–‡ä¸­çš„ä½ç½®æ’åºï¼Œä¿è¯ä»å‰åˆ°å
        positions.sort(key=lambda x: x["start"])

        sections: List[Dict[str, Any]] = []
        n = len(positions)

        for i, info in enumerate(positions):
            start = info["start"]
            end = positions[i + 1]["start"] if i + 1 < n else len(text)
            content = text[start:end].strip()
            title = f"Part {info['index']}: {info['title']}"
            sections.append(
                {
                    "title": title,
                    "content": content,
                }
            )

        return sections


# ===================== æœ¬åœ°æµ‹è¯•å…¥å£ =====================

if __name__ == "__main__":
    import asyncio
    import base64
    from pathlib import Path

    async def main():
        PDF_FILE = "attention_is_all_you_need.pdf"

        pdf_path = Path(PDF_FILE)
        if not pdf_path.exists():
            print(f"[PaperStructurer __main__] PDF file not found: {pdf_path.resolve()}")
            return

        try:
            with open(pdf_path, "rb") as f:
                pdf_base64 = base64.b64encode(f.read()).decode("utf-8")
        except Exception as e:
            print(f"[PaperStructurer __main__] è¯»å– PDF å¤±è´¥: {e}")
            return

        structurer = PaperStructurer()
        raw_text = structurer.extract_pdf_text_from_base64(pdf_base64)
        print(f"[PaperStructurer __main__] Raw PDF text length: {len(raw_text)} characters")

        sections = await structurer.run(raw_text)

        print("\n================ STRUCTURED SECTIONS ================\n")
        print(f"Total sections: {len(sections)}\n")
        for i, sec in enumerate(sections, start=1):
            title = sec.get("title", "")
            content = sec.get("content", "") or ""
            print(f"[Section {i}] {title}")
            print(f"  Content length: {len(content)} characters")
            print("  Preview (first 300 chars):")
            print("  " + content[:300].replace("\n", " ") + ("..." if len(content) > 300 else ""))
            print("------------------------------------------------\n")

    asyncio.run(main())
