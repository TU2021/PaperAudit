Summary  
The paper investigates whether and how contemporary vision–language models (VLMs) represent spatial relations under frame‐of‐reference (FoR) ambiguities. To this end, the authors introduce COMFORT, a synthetic evaluation protocol comprising two datasets (COMFORT-BALL and COMFORT-CAR) of Blender‐rendered 3D scenes with objects rotating through 36 angles around a reference. Textual queries ask whether a target is “to the left of,” “to the right of,” “in front of,” or “behind” a reference, with or without explicit FoR prompts (camera, addressee, or object viewpoint). The paper defines multiple metrics—accuracy, region parsing error against hemisphere and cosine reference curves (Section 3.4), robustness metrics (standard deviation σ and prediction noise η, Section 3.5), and consistency metrics (symmetry c^sym and opposition c^opp, Section 3.6). Nine open‐source and commercial VLMs (e.g., InstructBLIP, LLaVA-1.5, mBLIP-BLOOMZ, GLaMM, XComposer2, GPT-4o) are evaluated inError: AttributeError: 'NoneType' object has no attribute 'content'