Summary  
This paper introduces COMFORT, a synthetic evaluation framework designed to probe how vision–language models (VLMs) handle spatial language under frame-of-reference (FoR) ambiguities. COMFORT consists of two datasets—COMFORT-BALL (single red/blue ball pairs) and COMFORT-CAR (20 object combinations with an optional addressee)—rendered in Blender with the referent traversing a circular trajectory around the relatum. Test cases combine image, spatial-relation query (to the left/right of, in front of, behind), optional FoR prompt (none, camera-relative, addressee-relative, relatum-intrinsic), and language (109 languages for COMFORT-CAR). The paper defines a suite of metrics: accuracy, region parsing error against hemisphere and cosine references, robustness (standard deviation, prediction noise), and consistency (spatial symmetry and spatial opposition). Nine state-of-the-art VLMs—including InstructBLIP, LLaVA, XComposer2, MiniCPM-V, GLaMM, mBLIP-BLOOMZ-7B, and GPT-4o—are evaluated. Main findings are: (1) most models default to an egocentric relative FoR with a reflected coordinate transformation (the “English” convention) when FoR is unspecified; (2) models struggle to follow explicitly specified alternative FoRs; (3) spatial judgments lack robustness and consistency across continuous angles; and (4) in a cross-lingual probe (GPT-4o in 109 languages), the English relative FoR convention predominates, overriding known human-language preferences. The authors argue for greater attention to FoR ambiguities and cultural diversity in VLM spatial reasoning.Strengths  
• Focus on FoR Ambiguity: The paper tackles the under-studied problem of ambiguous spatial frames of reference in VLMs, expanding beyond existing benchmarks that assume a single “ground-truth” interpretation.  
• Controlled Continuous Variation: By placing the referent on a circular trajectory and sampling every 10°, COMFORT-BALL and COMFORT-CAR capture continuous variation rather than fixed canonical axes (Section 3.3, Fig. 4), enabling fine-grained analysis of model sensitivity.  
• Comprehensive Metrics: In addition to standard accuracy, the introduction of region parsing error (⊓hemi, ⊓cos), robustness (σ, η), and consistency (c^sym, c^opp) provides multiple lenses on model behavior (Section 3.4–3.6, Figs. 5–6).  
• Broad Model Suite: The evaluation covers a diverse set of nine open-source and closed-source VLMs, spanning supervised instruction-tuned, RL-aligned, and mechanistically grounded architectures, which illustrates that the observed patterns are not confined to a single model family (Section 4, Tables 2–5).  
• Large-Scale Multilingual Probe: Extending COMFORT-CAR to 109 languages and 170 regions (Section 4.5, Table 10, Fig. 8) is an ambitious effort to study cross-cultural FoR conventions at scale.  
• Synthetic & Real-World Case Studies: The paper verifies that key trends (e.g., noise and inconsistency) also manifest in real-world images (Appendix C), suggesting that the synthetic setup is not overly trivial.Weaknesses / Concerns  
1. Ground-Truth for Ambiguous Prompts  
   • When FoR is unspecified (nop), the “correct” interpretation is assumed to be the English egocentric-reflected convention (Section 4.2, Table 3). No human judgments on the synthetic images are reported to justify this gold standard or to quantify human agreement; thus it is unclear whether the model deviations reflect genuine reasoning failures or legitimate alternate interpretations.  
2. Absence of Geometric-Solver Upper Bound  
   • A rule-based baseline that deterministically maps 2D object coordinates to spatial relations under each FoR would achieve zero region parsing error (⨯hemi/⨯cos) in these noise-free renders. Without such an “oracle” baseline, it is difficult to interpret whether even the best VLM (e.g., XComposer2, GPT-4o) is close to the maximal attainable performance.  
3. Dependence on Synthetic Scenes  
   • COMFORT uses simple Blender renders without occlusion, complex backgrounds, or varied camera poses (Section 5.1, Limitations). While Appendix C reports more noise on real-world balls and laptop scenes, the synthetic stimuli may still lack the visual challenges of in-the-wild images (lighting, texture, viewpoint variation). It remains unclear how these results would generalize to standard VQA or embodied navigation benchmarks.  
4. Prompt and Translation Details Omitted  
   • The paper describes four prompt templates (Table 1), but does not include the exact natural-language wording used for non-English prompts, nor any human verification of Google-translated queries. Without sample multilingual prompts or a qualitative analysis of translation fidelity, the cross-lingual results (Section 4.5) risk reflecting translation artifacts rather than true FoR reasoning differences.  
5. Interpretation of Region Parsing Error  
   • Two references are proposed: 180° hemisphere (⨯hemi) and cosine-based (⨯cos). However, the choice of threshold (θ ∈ (–90°, 90°) vs. cos(θ)+1/2) is treated as a proxy for human acceptability without psycholinguistic validation. It is not clear why ⨯cos, which imposes a linear cosine curve, is more plausible than other smoothly decaying templates.  
6. Limited Scope of Spatial Relations  
   • The benchmark focuses only on four canonical directional relations (left/right/front/back). Other common relations (near/far, above/below, between) and absolute (cardinal) FoRs are not included, which limits COMFORT’s coverage of spatial language phenomena (Section B.1).  
7. No Analysis of Prompt Sensitivity  
   • In Table 4, accuracy sometimes decreases when specifying perspective explicitly (e.g., InstructBLIP-7B). The paper does not probe whether alternative prompt wording (paraphrases) or prompt engineering could yield better adherence to specified FoRs.  
8. Cross-lingual Claims on a Single Model  
   • The multilingual evaluation is conducted on GPT-4o only, yet the paper generalizes to “multilingual VLMs” at large. It is therefore unclear whether the observed English dominance holds for other multilingual models with stronger non-English pretraining.  
9. Reproducibility Gaps  
   • While the paper reports dataset-generation statistics (Appendix A.1), there is no public release link to the synthetic scenes, prompts, model-evaluation scripts, or seeds for replicating the region parsing computations. Key details such as how p(Yes) vs. p(No) is extracted from closed-source GPT-4o and how F1 for object hallucination is computed are missing.Questions for Authors  
1. How were the “correct” answers determined for ambiguous (nop) prompts? Did you collect human judgments on a subset of COMFORT-BALL or ‑CAR to validate that English-relative/-reflected FoR is the majority choice?  
2. Can you provide or cite a simple geometric solver baseline (e.g., analytically thresholded angles) to establish the maximal attainable accuracy and region parsing error on COMFORT?  
3. For the multilingual prompts, could you include example translations for three typologically diverse languages (e.g., Chinese, Arabic, Hausa) and describe any back-translation checks or human spot-checks you performed?  
4. How robust are your findings to prompt variations (e.g., “Is the ball to the left of the car from my viewpoint?” vs. “From the camera’s perspective, is the ball to the left of the car?”)?  
5. Why did you choose the hemisphere and cosine references specifically? Have you considered other spatial-template shapes (e.g., Gaussian or empirically measured human acceptability curves)?  
6. Could you extend COMFORT to include additional relations (near/far, above/below) or absolute FoRs (north/south)? What challenges would that entail?  
7. In Table 4 some models perform worse when given an explicit FoR prompt. Do you have hypotheses or diagnostic analyses (e.g., attention maps or ablations) to explain why models ignore or misapply the instruction?  
8. For GPT-4o, how did you extract p(Yes) and p(No)? Did you use the OpenAI log-probs API, multiple-choice prompting, or another strategy? Please clarify.  
9. Will you publicly release the COMFORT dataset (Blender scenes, prompt templates, evaluation code)? If so, please include links and versioning information to ensure reproducibility.Score  
- Overall (10): 6 — Proposes a novel FoR-ambiguity framework with rich metrics (Sec. 3; Tables 2–5) but lacks human baselines, geometric solvers, and full reproducibility  
- Novelty (10): 7 — First to systematically combine continuous spatial variation, explicit/ambiguous FoR prompting, and robustness/consistency metrics in VLM evaluation (Sec. 3–4)  
- Technical Quality (10): 6 — Well-engineered synthetic setup and metrics, but some methodological choices (region references, ground truth for ambiguity) are under-justified, and prompt/translation details are sparse  
- Clarity (10): 6 — The framework and metrics are clearly described, yet the paper is heavy on tables, and key design decisions (translation quality, computation of probabilities) are left implicit  
- Confidence (5): 4